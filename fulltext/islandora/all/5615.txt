EXAMINING EMOTION DISCRIMINATION IN 7-MONTH-OLD INFANTS AND ADULTS USING FAST PERIODIC VISUAL STIMULATION (FPVS) by Alexandra Rose Marquis Bachelor of Arts Honours, University of Guelph, 2012 A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Arts in the Program of Psychology Toronto, Ontario, Canada, 2016 © (Alexandra R. Marquis) 2016

AUTHOR'S DECLARATION I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my thesis may be made electronically available to the public.

ii

Abstract Examining Emotion Discrimination in 7-month-old infants and adults using Fast Periodic Visual Stimulation (FPVS) Alexandra R. Marquis Master of Arts, 2016 Psychology Ryerson University The ability to discriminate facial expressions of emotion is important for human communication and interaction. When this ability develops is largely unknown, with the origins believed to lie in infancy. Behavioural and brain-based evidence suggests that infants are capable of differentiating positive and negative facial expressions (i.e., sad vs. happy, surprised vs. angry), however there is little research examining whether infants can make more fine-grained discriminations among negative facial expressions (e.g., fearful vs. angry). In the present paper, two experiments use a novel technique known as Fast Periodic Visual Stimulation (FPVS) to assess discrimination of facial expressions by adults (n = 33) and 7-month-old infants (n = 33). Adults discriminated facial expressions, but 7-month-old infants did not. Reasons why infants did not show a discrimination response are explored and the potential benefits of FPVS are discussed.

Keywords: face perception, emotion discrimination, infancy, fast periodic visual stimulation, EEG

iii

Acknowledgements Thank you to my family for your support. I wouldn't be where I am today if it weren't for all four of you. Thank you to my joon Koosha Vakili for being there for me every step of the way. You are selfless in every sense of the word and always encouraging me to succeed.

Tremendous gratitude goes towards my supervisor Dr. Margaret Moulson for believing in me throughout this entire process. Your patience, flexibility, and unwavering assistance throughout graduate school have been invaluable.

I would like to extend a warm thank you to Dr. Julia Spaniol and Dr. Jean-Paul Boudreau for being an integral part of my examining committee.

iv

Table of Contents Abstract .............................................................................................................................. iii Acknowledgements ............................................................................................................ iv List of Tables .................................................................................................................... vii List of Figures .................................................................................................................. viii List of Appendices ............................................................................................................. ix Introduction ..................................................................................................................... 1 Emotion Processing in Adults................................................................................. 1 Theory of Basic Emotions ...................................................................................... 3 Structural Theory of Emotion ................................................................................. 5 Emotion Discrimination in Infancy ........................................................................ 6 Fast Periodic Visual Stimulation (FPVS) ............................................................. 14 Study 1: Emotion Discrimination in Adults ..................................................................... 18 Method........................................................................................................................... 19 Participants ............................................................................................................ 19 Stimuli ................................................................................................................... 19 Procedure .............................................................................................................. 20 Results ........................................................................................................................... 26 Base Stimulation Frequency (6 Hz) ...................................................................... 26 Oddball Discrimination Frequency (1.2 Hz) ........................................................ 33 Emotion Discrimination ........................................................................................ 36 Study 1 Discussion ........................................................................................................ 43 Study 2: Emotion Discrimination in Infants ..................................................................... 48 Hypotheses .................................................................................................................... 48 Method........................................................................................................................... 49 Participants ............................................................................................................ 49 Stimuli ................................................................................................................... 50 Procedure .............................................................................................................. 50 Questionnaires....................................................................................................... 51 Results ........................................................................................................................... 52 Base Stimulation Frequency (6 Hz) ...................................................................... 52 v

Oddball Discrimination Frequency (1.2 Hz) ........................................................ 61 Individual Differences .......................................................................................... 67 Study 2 Discussion............................................................................................................ 70 General Discussion ........................................................................................................... 76 Limitations ............................................................................................................ 78 Using Fast Periodic Visual Stimulation to Study Emotion Discrimination .......... 80 Future Directions .................................................................................................. 80 Conclusion ............................................................................................................ 82 References ....................................................................................................................... 107

vi

List of Tables Study 1 Table 1. Group Level Z-scores at the Base Frequency (6 Hz) and Harmonics.............27 Table 2. Group Level Z-scores at the Oddball Frequency (1.2 Hz) and Harmonics.......34 Table 3. Individual Z-sores at 1.2 and 2.4 Hz Oddball Harmonics...........................35 Table 4. Descriptive Statistics showing Effect of Emotion on Oddball Discrimination Response.............................................................................................40 Table 5. Descriptive Statistics showing Effect of Orientation on Oddball Discrimination Response.............................................................................................41 Study 2 Table 6. Group Level Z-scores at the Base Frequency (6 Hz) and Harmonics..............54 Table 7. Group Level Z-scores at the Oddball Frequency (1.2 Hz) and Harmonics.......62 Table 8. Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics across Emotion Conditions............................................................................................63 Table 9. Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Happy Condition.............................................................................................64 Table 10. Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Angry Condition.............................................................................................65 Table 11. Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Sad Condition.............................................................................................66 Table 12. Descriptive Statistics of Infant Questionnaire Data.................................68 Table 13. Pearson Correlations Between Signal-to-Noise ratio (SNR) of 1.2 Hz at Right Occipital Region and Questionnaire Data........................................................69 vii

List of Figures Study 1 Figure 1. Diagram of the experimental design.................................................23 Figure 2. Sample stimuli exhibiting the fast oddball technique. ............................24 Figure 3. Fast Fourier Transform (FFT) of 6 Hz and harmonics............................ 28 Figure 4. Signal-to-Noise Ratio (SNR) of 6 Hz and harmonics..............................29 Figure 5. Significant main effect of region on the baseline-subtracted amplitudes for upright and inverted conditions..................................................................32 Figure 6. Topographic maps of Signal-to-Noise Ratio (SNR) by emotion condition at the oddball response (1.2 Hz) .........................................................................39 Figure 7. Interaction between orientation and emotion condition on the sum of baselinesubtracted amplitudes across three regions of interest.........................................42 Study 2 Figure 8. Fast Fourier Transform (FFT) of 6 Hz and harmonics ............................55 Figure 9. Signal-to-Noise Ratio (SNR) of 6 Hz and harmonics..............................56 Figure 10. Topographic maps of Signal-to-Noise Ratio (SNR) by emotion condition at 6 Hz. ...................................................................................................57 Figure 11. Baseline-subtracted amplitudes by region and emotion condition at the base frequency (6 Hz) and harmonics..................................................................60

viii

List of Appendices Appendix A: Consent Form: Study 1 Adults....................................................84 Appendix B: Consent Form: Study 2 Infants....................................................88 Appendix C: Demographic Questionnaire.......................................................93 Appendix D: Motor Development Questionnaire ...............................................94 Appendix E: Self-Expressiveness in the Family Questionnaire (SEFQ) ....................95 Appendix F: Infant Behavior Questionnaire (IBQ-R) .........................................97

ix

Examining Emotion Discrimination in 7-month-old infants and adults using Fast Periodic Visual Stimulation (FPVS) Humans are highly social beings that are skilled at recognizing different emotions. Everyday social interactions require us to rapidly and efficiently identify how an individual is feeling. The face acts as a point of reference for communicating information that facilitates understanding of others' emotions. In turn, facial expressions play a critical role in the development and regulation of interpersonal relationships (Ekman, 1999). Facial expressions may be particularly important early in life. Preverbal infants who are unable to use language to express emotions might especially rely on facial expressions as a form of communicating and maintaining relationships (de Haan & Nelson, 1996). Given the importance of facial expressions for early communication, it is essential for an infant to be able to discriminate between familiar and unfamiliar faces and to extract information from faces about a person's internal state (Leppänen & Nelson, 2009). The goal of the current studies was to add to our understanding of the development of this critical social ability in the first year of life. Specifically, two studies investigated the ability of both adults and infants to discriminate facial expressions of emotion using a novel electrophysiological technique called Fast Periodic Visual Stimulation (FPVS). Emotion Processing in Adults Emotion discrimination is well developed by adulthood. Supporting evidence comes from many decades of research (Adolphs, 2002; Calder et al., 2003; Calder, Young, Perrett, Etcoff, & Rowland, 1996; for reviews, see Elfenbein & Ambady, 2002; Mcclure, 2000). In addition, the ability to discriminate between facial expressions 1

remains intact even under suboptimal conditions. Discrimination is present when there is little facial information (Seirafi, Weerd, & Gelder, 2013), faces are presented rapidly (Kiss & Eimer, 2008), or unconsciously (Smith, 2012), and when stimuli are very basic (i.e., schematic drawings; Etcoff & Magee, 1992). In eye-tracking studies, adults adopt different scanning patterns when viewing certain facial expressions, affirming that adults process expressions differentially (Hunnius, de Wit, Vrins, & von Hofsten, 2011). The neural underpinnings of this sophisticated skill have been explored in eventrelated potential (ERP), positron emission tomography (PET), and functional magnetic resonance imaging (fMRI) studies. Experiments using ERP demonstrate differences in face-specific components in response to facial expressions of emotions, such as disgust, anger, fear, happiness, and neutrality (Batty & Taylor, 2003; Caharel, Courtay, Bernard, Lalonde, & Rebaï, 2005; Eimer & Holmes, 2002; Eimer, Holmes, & McGlone, 2003; Holmes, Vuilleumier, & Eimer, 2003; Krombholz, Schaefer, & Boucsein, 2007; Leppänen, Moulson, Vogel-Farley, & Nelson, 2007). The N170 is a negative-going component, occurring approximately 200ms after presentation of a stimulus, that shows substantial specificity in response to faces compared to other objects (Bentin, Allison, Puce, Perez, & McCarthy, 1996). In a recent meta-analytic review, researchers concluded that the N170 is sensitive to facial expressions (see Hinojosa, Mercado, & Carretié, 2015). Evidence from neuroimaging studies suggests that the adult brain activates partially dissociable regions in response to different facial expressions of emotion (Breiter et al., 1996; Kesler-West et al., 2001; Morris et al., 1996; Thomas et al., 2001; Whalen et al., 1998). These responses are typically localized to the right hemisphere, (for

2

a meta-analytic review, see McClure, 2000). It is clear that adults show a reliable ability to discriminate facial expressions of emotion. Researchers have been investigating when the ability to differentiate between facial expressions appears during development. In general, it is agreed that by around 7 months of age, emotion discrimination is robust for positive versus negative expressions. Less is known about when and if infants can discriminate two negative expressions, such as fearful and angry. The current methodologies used to investigate emotion discrimination in infancy pose several challenges to answering this question. In the following literature review, two competing theoretical perspectives, theory of basic emotions and structural theory of emotions and their predictions about infants' ability to discriminate emotions will be considered. Behavioural and neuroimaging research on emotion discrimination in infancy will be reviewed and the limitations of these approaches will be discussed. Finally, the FPVS technique and its applicability to emotion discrimination in infancy will be introduced. Theory of Basic Emotions The first theoretical perspective was influenced by Darwin, who proposed that facial expressions are universally understood and produced. Through a series of judgment studies, Ekman and colleagues (1976) provided partial evidence in support of universality. Researchers showed people in different cultures a series of photographs of Americans posing various facial expressions and asked them to judge the emotion based on a given list of ten labels. In the 21 countries with high literacy rates studied, the majority of people were in agreement about six facial expressions: happiness, anger, fear, sadness, disgust, and surprise. These findings suggest that basic emotions are universally 3

expressed and understood (Ekman & Friesen, 1976). Further, Ekman and colleagues (1969) found that people in preliterate cultures (e.g., New Guinea) were able to identify and distinguish the six basic emotions although they had difficulty distinguishing fear and surprise from each other. Further evidence for the universality of emotions comes from studies on infants' ability to produce facial expressions in the first year of life. Izard and colleagues (1995) observed structured, face-to-face mother-infant interactions and coded the facial expressions produced by infants at various ages between 2.5 and 9.5 months. At 2.5 months of age, infants demonstrated the ability to express interest, joy, sadness, and anger. Patterns of muscle movements of facial expressions adopted by infants were stable throughout the first 9 months of life. Additionally, Camras and colleagues (1992) examined the facial expressions of 5- and 12-month-old Japanese and American infants in response to a non-painful arm restraint procedure. Infants from both cultures and in both age groups produced similar facial expressions in response to the arm restraint. That infants early in the first year of life and from different cultures reliably produce similar facial expressions suggests that these expressions may indeed be universal. Based on these cross-cultural and developmental studies, Ekman (1999) outlined a theory of basic emotions and their associations with facial expressions. He posited that each emotion is a discrete affective state whose primary function is to prepare the organism, via physiological changes, to respond appropriately. A person either expresses one emotion or another; there is no in-between. Ekman proposed that facial expressions yield relevant information to conspecifics about earlier events, current responses, and future behaviour (Ekman, 1999). This theoretical perspective posits that infants are born with the ability to express and perceive at least basic-level emotions (Izard, 1994). Drawing from this 4

theoretical perspective, one would predict that infants would be able to differentiate between facial expressions representing the basic emotions. Structural Theory of Emotion An alternative perspective on emotion proposes they are not discrete but related. Russell (1980) proposed a structural theory of emotion that places emotions in a twodimensional space, with pleasure-displeasure and degree of arousal on either axis. This theoretical model posits that humans perceive emotions as systematically interrelated (Russell & Bullock, 1985) and the mental representation of emotions is based on two continuous underlying dimensions. Russell (1980) asked adults to rate emotion terms and instructed participants to group together emotional states that were most similar. For the majority of participants, the emotion words spread out in a systematic order, around a multidimensional scale of pleasure-displeasure and degree of arousal; unknowingly, adults organized their knowledge of emotions along this mental map. In this model, emotion categories can be viewed as "fuzzy sets," which consist of overlap and continuity (Russell & Bullock, 1985). For example, anger and fear are located close in space because their boundaries overlap in degree of arousal and pleasure. Russell (1980) suggested that in everyday social interactions, a given facial expression is interpreted by first evaluating it in terms of pleasure-displeasure and arousal, then choosing a label to categorize it based on the context. This theoretical perspective would predict that infants are initially not able to discriminate emotions and that this ability develops slowly in concert with their verbal abilities and capacity to label emotions. Based on this perspective, emotion discrimination would first appear along the pleasure-displeasure axis, so that infants are able to detect differences between emotions that fall at opposite 5

ends of this axis (e.g., happiness compared to sadness), followed by the more difficult, within-boundary discriminations (e.g., anger compared to fear). Emotion Discrimination in Infancy A wealth of behavioural studies have examined infants' perception of facial expressions during the first year of life. The majority of these behavioural studies rely on looking time paradigms, including spontaneous visual preference and habituation/familiarization tasks. In spontaneous visual preference tasks, infants view two stimuli presented simultaneously and the looking time to each stimulus is measured. When an infant looks longer at one stimulus, this provides evidence of the infant's ability to discriminate the two stimuli and suggests an attentional preference for that stimulus (although the reason for this attentional preference is often difficult to discern). In habituation/familiarization tasks, infants view a stimulus repeatedly, either for a fixed period of time or until the infant loses interest and decreases looking to a pre-specified criterion. Following habituation/familiarization, researchers employ one of two ways to test infants' discrimination. A dishabituation test involves presenting a novel and familiar face sequentially, one after the other, while a visual-paired comparison (VPC) test shows novel and familiar faces side-by-side. If the infant looks longer at the novel stimulus in comparison to the previously seen one, this is seen as evidence of discrimination. Both of these paradigms can provide evidence that infants are able to detect a difference between two stimuli; in practice, however, interpreting findings from these paradigms can be difficult. Next, findings from experiments investigating emotion discrimination at different ages will be discussed.

6

Very early in the first year of life, discrimination is evident although not fully developed. Newborns fewer than 3 days old prefer to look at a happy face more than a fearful face, but show no differential looking towards a fearful versus neutral face (Farroni, Menon, Rigato, & Johnson, 2007). Similarly, using a habituation-dishabituation task, Field, Woodson, Greenberg, and Cohen, (1982) found evidence that newborns are capable of discriminating and imitating photographs of happy, sad, and surprised facial expressions of a female stranger by 36 hours old. In an attempt to replicate these findings using live dynamic models, Kaitz and colleagues (1988) found that newborns did not imitate the models' happy, sad, or surprised expressions. Although the findings are somewhat mixed, there is evidence that newborns discriminate at least some facial expressions and prefer happy facial expressions. Three-month-olds continue to show a preference for happy over neutral faces, and they exhibit a stronger preference as the smile became more prominent, indicating sensitivity to the intensity of an emotional expression. In a series of studies, Barrera and Maurer (1981) used a habituation paradigm to test 3-month-old infants' ability to discriminate photographs of happy and sad faces posed by their mother or a female stranger. Regardless of identity, infants recognized the novel expression at test, however, more infants showed this discrimination when presented by mom, pointing to the importance of experience and familiarity early in life. Similarly, using a habituationdishabituation procedure, Young-Browne and colleagues (1977) found that 3-month-olds could discriminate between happy and surprised faces. They could also discriminate between surprised and sad faces after habituation to sad, but did not demonstrate

7

discrimination after habituation to surprised. These "order effects" are common across all ages; potential reasons for these effects are discussed below. Researchers investigate infants' discrimination in tightly controlled experimental conditions using static photographs, but they can also do so in natural contexts. Montague and Walker-Andrews (2001) found that 4-month-old infants' reactions differ based on the expression presented in peek-a-boo games. Infants played typical peek-a-boo games, in which the experimenter expressed happiness, followed by a game that involved a change in facial expression, to either sadness, anger, or fear. Infants' looking behaviour differed for each of the conditions, demonstrating discrimination in natural and less controlled settings. It is rare for infants to see static facial expressions in their environment, with some arguing that crucial affective information is lost in this method (Ekman & Friesen, 1978). When using dynamic stimuli, researchers have shown infants at this age can discriminate positive from negative expressions (Caron, Caron, & Maclean, 1988; Flom & Bahrick, 2007). Beyond simple discrimination, researchers have proposed that emotion categorization is a necessary step towards understanding the meaning behind facial expressions. Nelson (1987) provides an example: Infants need to recognize that a happy facial expression is the same across different settings and different people in order to comprehend the meaning behind it. To investigate categorization, researchers modify the typical habituation/familiarization task. First, the infant views multiple exemplars in a category (e.g., different faces expressing happiness). Then, during the test phase, s/he sees a new exemplar from the familiar category (e.g., happiness) and a new exemplar of a different category (e.g., sadness). If infants look longer at the face from the new emotion 8

category, this suggests that they were able to generalize across the various happy faces presented, extend that generalization to a new exemplar of happy, and discriminate it from the exemplar from the new category. Kaneshige and Haryu (2015) demonstrated that 4-month-old infants categorized happy and angry expressions; similarly, Bornstein and Arterberry (2003) found that 5-month-olds categorized happy and fearful expressions. However, other studies have found the ability to categorize emerges later than 7 months of age (Ludemann & Nelson, 1988; Ludemann, 1991) or fail to find evidence for this ability altogether (Phillips, Wagner, Fells, & Lynch, 1990). Additionally, puzzling order effects emerge in many studies of categorization. For example, in many studies infants demonstrate categorization of happy and fearful faces if they are first habituated to the happy face (i.e., they show a novelty preference for a fearful face following habituation to happy) but do not demonstrate categorization if they are first habituated to the fearful face (i.e., they show no novelty preference for a happy face following habituation to fearful; Caron, Caron, & Myers, 1985; Ludemann & Nelson, 1988; Nelson & Dolgin, 1985; Schwartz, Izard, & Ansul, 1985; Young-Browne et al., 1977). These order effects may be driven by infants' spontaneous preferences for certain expressions. Very early in life, young infants demonstrate a preference for happy facial expressions (Farroni et al., 2007), but preferential attention to fearful (Peltola, Leppänen, & Hietanen, 2011) and angry faces (Lobue & Deloache, 2010) compared to neutral and happy faces, emerges by 7 months of age. These spontaneous preferences may interfere with our ability to determine whether infants can discriminate and categorize particular emotions by influencing the novelty preference following habituation/familiarization. 9

Most studies of infant emotion processing examine infants' ability to discriminate or categorize positive versus negative emotions. There are only two behavioural studies that investigate infants' ability to discriminate and categorize different negative emotions and the results are inconclusive. Schwartz, Izard, and Ansul (1985) conducted a study assessing discrimination of negative emotions in 5-month-old infants. Infants saw fearful, angry, and sad facial expressions in a familiarization paradigm. Infants were able to discriminate fearful from sad regardless of which facial expression served as the familiar stimulus. Infants were also able to discriminate between angry and sad, and angry and fearful, but not when angry was the novel stimulus. It is possible that the angry expression was not successful in maintaining the infant's attention. The researchers also measured how long infants looked at each expression and found support for this lack of attention hypothesis: Infants showed a tendency to look less at the angry facial expressions. To further complicate the picture, Serrano, Iglesias, and Loeches (1992) used a categorization task. Four- to 6-month-old infants were habituated to angry, fearful, or surprised faces expressed by multiple models. Irrespective of the habituated expression, infants reliably looked longer at the novel expression, suggesting that they discriminated amongst negative emotions. It was theorized that infants were able to discriminate as they had more affective information from the different models than if they were habituated to a single model. Thus, there is limited evidence to suggest that infants may be able to discriminate and categorize negative emotions. The two studies that have examined this question partially conflict with each other and the ability has not been investigated in more recent studies. Other researchers have examined emotion discrimination using brain-based and physiological methods. In order to better understand 10

the neurocognitive processes that occur during face perception, it is important to consider the electrophysiological responses of infants when viewing facial expressions (Kobiella, Grossmann, Reid, & Striano, 2008). Research has shown that 7-month-old infants process negative and positive emotions differently, although the findings are somewhat mixed. Some studies have found evidence that negative facial expressions of emotion (e.g., fearful) generate a larger Nc amplitude (Negative component) than positive facial expressions of emotions (e.g., happy) (de Haan, Belsky, Reid, Volein, & Johnson, 2004; Nelson & de Haan, 1996; Peltola, Leppänen, Mäki, & Hietanen, 2009). The Nc is a negative-going deflection occurring between 400 and 800ms after stimulus presentation over frontocentral leads and is thought to reflect the allocation of attention (de Haan, Johnson, & Halit, 2003). In contrast, other studies have shown that happy facial expressions elicit a larger Nc amplitude compared to angry facial expressions in 7-monthold infants, while 12-month-olds responded similarly to adults and showed an enhanced Nc response to angry faces compared to happy faces (Grossmann, Striano, & Friederici, 2007; Schupp et al., 2004). Recent work by Jessen and Grossmann (2015) investigated how rapidly the infant brain can discriminate positive from negative emotions. Sevenmonth-old infants showed differential neural responses as measured by ERPs to subliminally (50 and 100ms) and supraliminally (500ms) presented happy and fearful faces. Additionally, Leppänen and colleagues (2009) found categorical perception of happy and sad facial expressions in 7-month-old infants both behaviourally and using ERP. The ability to discriminate between positive and negative emotions is also substantiated by functional Near-Infrared Spectroscopy (fNIRS) research. This method is 11

a non-invasive measure of brain activity that monitors changes in blood oxygenation and volume with the use of near-infrared light (Boas, Elwell, Ferrari, & Taga, 2014). The hemodynamic response in 6- to 7-month-old infants differed when presented with angry and happy facial expressions posed by different models (Nakato, Otsuka, Kanazawa, Yamaguchi, & Kakigi, 2011). The response to happy expressions increased slowly, while the response elicited by angry expressions peaked quickly and decreased rapidly until the face was no longer presented. The rapid decrease in response to angry expressions may have been due to the infant avoiding the angry face due to an understanding of its threatening meaning (Nakato et al., 2011) There is also evidence from physiological measures that infants can discriminate between positive and negative emotions. Peltola, Leppänen, and Hietanen (2011) recorded the heart rate of 7-month-old infants while viewing images of two female faces expressing both happiness and fear. They found a significantly larger heart rate deceleration after presentation of fearful faces compared to happy ones. A larger heart rate deceleration to emotionally negative stimuli has also been seen in adult samples (Bradley, Lang, & Cuthbert, 1993; Kolassa & Miltner, 2006). In summary, similar to the behavioural literature, there is a fair amount of evidence from ERP, fNIRS, and psychophysiological studies to suggest that infants, by 7 months of age, respond differently to positive and negative emotions. Thus far, only two published studies have investigated whether the infant brain responds differently to different negative emotions. Nelson and de Haan (1996) found that 7-month-old infants did not show different ERP responses to an angry versus a fearful facial expression. The authors proposed that infants may not have shown different neural responses for the two emotions because they were unfamiliar with both 12

expressions and/or the infants perceived the social value as negative for both expressions, but did not perceive them as different in meaning. In contrast, Kobiella and colleagues (2008) found ERP differences in response to angry and fearful female faces in 7-monthold infants. Specifically, the angry facial expression elicited a larger Nc and a larger N290 than the fearful face, whereas the fearful face elicited a larger P400 than the angry face. Both the N290 and P400 components are thought to be developmental precursors to the face-sensitive N170 seen in adults (de Haan et al., 2003). One challenge to developmental studies using ERP is that although they can inform us about the timing of a process, the criteria used to detect a difference in response to two stimuli (e.g., a fearful vs. an angry face) are often subjective. ERP analysis relies on identification of peaks or components of interest within a specific time frame (Farzin, Hou, & Norcia, 2012). Studies report differences in response timing, polarity, latency, lateralization, and/or amplitude, which leads to little consistency across developmental studies. For example, the two studies measuring ERPs while infants viewed two negative facial expressions reported different time windows, different components, and varying electrode locations. Unlike the adult literature, it is currently unknown when and where emotion discrimination occurs in the infant brain. This makes it difficult to make comparisons across developmental studies as well as to draw connections to the adult literature. A method that is more sensitive and appropriate for both age groups is necessary to further our understanding of the neural basis of this ability. Taken together, the behavioural and brain-based research with infants demonstrates that within the first year of life, infants are able to discriminate positive and 13

negative facial expressions. However, it is less clear when the ability to discriminate among negative facial expressions emerges. The methods currently being used lack standardization; as such, findings to date are characterized by variability and order effects. Our understanding of the neural underpinnings of emotion discrimination in infancy is also unclear and the variability across studies makes it difficult to draw conclusions. There are questions about lateralization, localization, and timing of emotion discrimination in the infant brain that remain largely unanswered. In order to address these issues, the field needs a measure that is: 1) objective; 2) immune to infants' spontaneous preferences for certain expressions; 3) able to be used across different ages; and 4) brain-based in order to answer questions about the neural basis of discrimination. Due to behavioural limitations (e.g., inability to use the same paradigms on infants and adults) and differences in ERP components (e.g., the N170 exists in adults but not in infants), there are no measures that allow for direct comparisons between infant and adult performance on emotion recognition tasks. Thus, it is difficult to track the emergence of emotion discrimination over developmental time. The current studies used an objective and sensitive measure called Fast Periodic Visual Stimulation (FPVS) to assess the ability to discriminate facial expressions in both adults and 7month-old infants. This measure offers significant advantages over traditional behavioural and electrophysiological measures of emotion processing in infancy. Fast Periodic Visual Stimulation (FPVS) A typical ERP study presents stimuli at slow rates (e.g., one face every 2s) so that the responses to following stimuli do not overlap (Rossion & Jacques, 2012). FPVS involves a different approach: stimulus presentation occurs at a rapid fixed frequency that 14

is defined by the researcher, which results in electrophysiological responses at the same frequency (Rossion, 2014a). Visual stimuli, when presented repetitively at a fixed rate, create a periodic change of voltage amplitude in the electrical activity at the human scalp (Rossion, 2014a). Contrary to ERP, which measures responses in the time domain, the periodic responses of FPVS occur at the specific frequency determined by the researcher and are subsequently analyzed in the frequency domain. This response can be quantified by comparing the signal at the stimulation frequency to the signal at the neighbouring frequencies (Rossion, 2014a). The FPVS approach results in few artifacts and provides a high signal-to-noise ratio, which means that fewer trials are needed to obtain a reliable signal. This makes this technique practical for experiments with infants, who have limited ability to sit through long experimental sessions typical of ERP studies. Previous uses of FPVS include numerous studies investigating low-level vision (e.g., sensitivity to contrast; see Vialatte et al., 2010, for a review). Within the past 10 years, researchers have demonstrated that periodic presentation of high-level visual stimuli, such as faces, elicits a robust FPVS signal that reflects high-level visual processes. Rossion and Boremanse (2011) assessed the sensitivity of the human brain to individual faces using FPVS. Adult participants were shown 90s long presentations of faces at a periodic rate (3.5 faces per second). This study included two conditions: the first presented the same face repeatedly while the second presented faces whose identities were different. Results showed a large response at the stimulation frequency of 3.5Hz over posterior electrode sites. The response was larger over right occipito-temporal sites during presentation of different faces than when an identical face was repeated. In addition, the difference between conditions disappeared when the faces were inverted, 15

suggesting that participants were not relying on low-level features to discriminate amongst the presented faces, but rather, were engaging in holistic processing (Rossion & Boremanse, 2011). The results of this particular study and others (Boremanse, Norcia, & Rossion, 2013; Liu-Shuang, Norcia, & Rossion, 2014; Rossion et al, 2015; Rossion, 2014b) suggest that FPVS is an objective tool for studying face processing in the adult brain. Surprisingly, there have only been two studies that use FPVS with infants to study high-level vision (de Heering & Rossion, 2015; Farzin et al., 2012). de Heering and Rossion (2015) recorded scalp electrical brain activity in 4- to 6-month-old infants while they watched photographs of objects embedded in natural backgrounds presented at a rate of 6 Hz. Every fifth stimulus was a face, creating a secondary frequency of 1.2 Hz (reflecting the frequency at which a face appeared). Results indicated a face-selective response over the right hemisphere at 1.2 Hz. This response was not apparent when the images were scrambled, suggesting that infants were not relying on low-level visual information to discriminate faces from objects. This study demonstrates that infants as young as 4 months are capable of discriminating faces and objects in natural photographs. Secondly, Farzin and colleagues (2012) tested 4- and 6-month-old infants and adults using FPVS. Participants were presented with both scrambled and intact greyscale photos of objects and faces. The localization of face responses differed between adults and infants, such that adult responses were localized over occipito-temporal areas while infants' responses localized over temporal regions. Similar to de Heering and Rossion's work, infants' FPVS responses demonstrated face category specificity.

16

The studies published to date reveal three main advantages to using FPVS to study emotion perception in infants. The first advantage is that this method requires short recording times. Because it results in a high number of responses within a short time frame, it is practical for use with infants. Second, the researcher objectively defines the responses. This is an important factor because the current behavioural and ERP studies have been inconsistent and resulted in an unclear picture of the emergence of emotion discrimination in infants. Third, this method is appropriate for different age groups, and allows for direct comparisons between infant and adult ability.

17

Study 1: Emotion Discrimination in Adults The aim of Study 1 was to assess adults' ability to discriminate between facial expressions of emotion using FPVS. It was necessary to establish this ability in adults before testing infants because they served as a control group. With an adult sample completing the same task, we were able to make direct comparisons between both age groups, which helps us to better understand this ability in infants. All adults saw both between-boundary emotion comparisons (e.g., happiness vs. fear) and within-boundary emotion comparisons (e.g., anger vs. fear). To control for discrimination based on lowlevel feature differences, half of the sample saw inverted faces. Face orientation was manipulated as a between-subjects factor because we wanted to decrease the duration of the experiment. Pilot testing indicated that including both upright and inverted conditions in the same experiment (thus making it longer) made it difficult for adults to pay attention for the entirety of the experiment. We predicted adults would evidence discrimination of both between-boundary emotions and within-boundary emotions, showing a better ability for the former. We also hypothesized differences in orientation, such that participants in the upright condition would evidence a larger discrimination response than those in the inverted condition. This is because when faces are presented upright, humans engage in holistic processing, which enhances face and emotion recognition (Bombari et al., 2013). Finally, we predicted the right hemisphere would show greater discrimination of emotional stimuli than the left or midline regions (Dzhelyova, Jacques, & Rossion, under review; Junghöfer et al., 2006; Leppänen et al., 2007; Smith, 2012).

18

Method Participants Thirty-four healthy adult participants took part in the current study. Participants were recruited from Ryerson's Psychology Research Pool (n = 22) and received one credit towards their undergraduate Psychology course. The remaining 12 participants were community members who were interested in research. They were recruited through word of mouth and did not receive any compensation for their participation. One participant was excluded from analysis for being left-handed. The remaining 33 participants were right-handed with normal or corrected-to-normal vision, no history of seizures, or diagnosis of seizure-related disorders. The average age was 22.76 (SD = 5.14, range: 18 - 40), and the majority of participants were female (82%, n = 27). Almost half of the participants were Caucasian (45%, n = 15); the other participants self-identified as: Asian (30%, n = 10; including South and East Asian), Middle Eastern (9%, n = 3), Arab (9%, n = 3), and Hispanic (6%, n = 2). Written informed consent was obtained from all participants prior to participation in the experiment (see Appendix A for the consent form).

Stimuli The current study sourced nine female face stimuli from the Karolinska Directed Emotional Faces (KDEF; Lundqvist, Flykt, & Öhman, 1998) dataset, each expressing happiness, sadness, fear, and anger, for a total of 36 faces. All of the face stimuli were unfamiliar to the participants. The stimuli were colour photographs. The external 19

contours of the face, including hair, remained intact, but the background of each photo was replaced with a uniform grey colour (128R, 128G, 128B). The stimuli were not matched for contrast or luminance as the photographs were taken under standardized conditions to ensure uniformity. Therefore, any differences (e.g., faces with dark eye and hair colour) represent typical differences seen in daily life that could be used as diagnostic cues. All of the faces were presented in frontal view, and the facial expressions reflected various intensities of the emotions (e.g., happy expressions with wide open vs. slightly open mouth). All of the photographs had previously been validated by untrained research participants for facial expression of emotion (Goeleven, De Raedt, Leyman, & Verschuere, 2008). Model numbers of the identities that were used were: F01, F09, F14, F16, F22, F28, F30, F31, F33. All of the models were Caucasian. Procedure Following EEG cap placement, participants were seated in a dark room, at a viewing distance of 100-110cm from the computer screen. Stimuli were presented using a custom-made script in Presentation (Neurobehavioral Systems, 2016). Each stimulation sequence lasted for 34s. The first 2s contained one face image fading in and out, to warn participants that stimulation was starting; the last 2s repeated this to let participants know that stimulation was ending. The 30s between these periods consisted of the fast periodic visual stimulation, where 6 faces were presented per second (stimulation rate = 6Hz). During FPVS, each face gradually increased in contrast in a series of steps from 0% (uniform grey background) to 50% to 100% (full contrast) and back to 50% then 0%. Each `step' lasted a total of 33.33 milliseconds. In order to achieve this, we used a Dell 24-inch Ultrasharp monitor with a refresh rate of 60 Hz. Within the Presentation script, 20

we modified the pixels to be 1024 x 768 pixels and used a Dell Optiplex 7010 with 1GB Radeon graphics card running on Windows 7. The size of each image randomly increased or decreased in size across presentations between 82% and 118% of the original size, in increments of 2%. At the original size, the area of the face subtended a visual angle of 5.73° x 4.95°. Changing the size of each face image makes it more difficult for participants to discriminate the emotions based on low-level image features alone. After each sequence, participants were encouraged to take a break and inform the researcher when they were ready to proceed. Each participant completed a total of 18 sequences: Six sequences in each of three emotion conditions (happy, sad, angry), in one of two orientations (upright or inverted). Each individual sequence involved the presentation of a single model. Within a single emotion condition, participants saw three unique females two times each. A block design was used, where all sequences in a given condition (happy, sad, angry) were shown before moving on to the next condition. Each participant was randomly assigned the order of emotion conditions and the version, which specified the models seen in each condition (Version A, consisting of Model 1, 2, and 3; version B, models 4, 5, 6 and version C, models 7, 8, 9). Participants were instructed to pay attention to the emotion each woman was expressing by fixating on the monitor for the entirety of the experiment. For every emotion condition, there was one frequent emotion and one infrequent "oddball" emotion presented every fifth face. Frequent emotions were happy, sad, or angry; the oddball emotion was always fearful. There were several reasons why fearful was chosen as the oddball emotion in the current study: 1) the majority of work in infancy examines infants' processing of this emotion compared to others; 2) at 7 months, 21

infants show a spontaneous preference for fearful over happy (Ludemann & Nelson, 1988), a limitation to infant looking work that FPVS can overcome; and 3) this emotion has also been extensively studied in adults and animals in recent years (Davis & Whalen, 2001). For example: Ahappy Ahappy Ahappy Ahappy Bfearful Ahappy (see Figure 1 for a diagram of the experimental design; Figure 2 shows sample stimuli from the happy-fearful condition). This fast oddball technique elicits a response at the base frequency of 6 Hz, but also elicits a discrimination response at 1.2 Hz (the presentation rate of the oddball stimulus), only if the participant recognizes a difference between the frequent and oddball stimuli. Including consent, EEG capping, and breaks between sequences, the entire experiment took approximately 45 minutes.

22

Emotion Condition 1a (e.g., Happy vs. Fearful) Model 1b Time(s) 30s Model 2 30s Model 3 30s Model 1 30s Model 2 30s Model 3 30s Version A c (Model 1, 2, 3)

Emotion Condition 2 (e.g., Sad vs. Fearful) Model 4 Time(s) 30s Model 5 30s Model 6 30s Model 4 30s Model 5 30s Model 6 30s Version B (Model 4, 5, 6)

Emotion Condition 3 (e.g., Angry vs. Fearful) Model 7 Time(s) 30s Model 8 30s Model 9 30s Model 7 30s Model 8 30s Model 9 30s Version C (Model 7, 8, 9)

Figure 1. Diagram of the experimental design.
a

Order of presentation of emotion condition was counterbalanced by the researcher. b Order of presentation of the models was

randomized. c Order of version presentation was counterbalanced by the researcher

23

Figure 2. Sample stimuli exhibiting the fast oddball technique. This is an example of the upright happy-fearful condition. A total of six faces were presented every 1s, with every 5th face serving as an oddball expression (fearful). Each sequence lasted a total of 30s. The model seen here is from the Karolinska Directed Emotional Faces dataset (KDEF; Lundqvist et al., 1998).

24

EEG Acquisition Continuous EEG was recorded using a 128-channel Hydrocel Geodesic Sensor Net (Electrical Geodesics, Inc). Eye movements were monitored through four electrodes. The sampling rate was 500 Hz and electrode impedances were verified to be below 40 k at the start of the experiment. For participants with relatively thick hair, the experiment was paused after the first two emotion conditions and impedances were examined a second time and any dried sponges were re-moistened. EEG Pre-Processing All EEG processing steps were carried out using NetStation 5 (EGI), Letswave 6 (http://nocions.webnode.com/letswave) and MatLab 2012b (MathWorks). In NetStation, each recording was bandpass filtered with a low cut-off value of 0.1 and a high cut-off value of 100 Hz. Next, continuous recordings were segmented into 30s sequences following the onset of stimulation. Using Letswave, Independent Components Analysis (ICA) was performed for each participant to identify and exclude eye blinks. Following eye blink correction, the sequences were examined in the time domain for possible channel artifacts. Noisy channels were interpolated using three neighbouring electrodes. Sequences were removed when more than 10% of the electrodes (12 of 128 electrodes) required interpolation. This criterion led to 1 or 2 sequences being excluded per adult. For the sequences that were included in the analyses, an average of 5 channels were interpolated (range: 2 - 10). After channel interpolation, data were re-referenced to the average reference (excluding eye channels). Sequences were averaged in the time-domain for each adult and emotion condition separately. A Fast Fourier Transform (FFT) was applied to the data at the frequency resolution of .03 Hz (1/30s). 25

Next, grand-averaged spectra were computed by averaging across each emotion condition and orientation (i.e., happy upright, happy inverted, etc.). SNR was computed for each individual spectrum as the ratio of the amplitude at each frequency of interest and the average amplitude of the 20 surrounding frequency bins (10 bins on either side, excluding the bins on either side of the frequency of interest). On grand-averaged data, zscores were computed for the stimulation frequency (6 Hz) and the oddball frequency (1.2 Hz) and harmonics. The threshold for significance was placed at a one-tailed z-score (z > 1.64, p < .05) as the hypothesis is that the signal will be above the noise. Finally, based on a recent FPVS study with infants and adults (Farzin et al., 2012), three regions of interest were included in the analyses. Electrodes over the right (channels 89, 90, 91, 95, 96), middle (channels 70, 74, 75, 82, 83), and left (channels 58, 59, 64, 65, 69) occipital regions were used. Results Base Stimulation Frequency (6 Hz) There were large responses present at the base stimulation frequency and its harmonics in all of the conditions. When grand averaging across all conditions and pooling across all channels, the responses remained significant until the 10th harmonic (72 Hz; see Table 1 for z-scores; z > 1.64, p < .05, z-score range: 4.17 - 188.47). The amplitude spectra showing responses at 6 Hz over the pooled channels can be seen in Figure 3. Topographically, the 6 Hz response was localized over the medial occipital region. The SNR averaged across all conditions also suggested large responses at the base stimulation frequency. The signal at 6 Hz is 6.67 times larger than surrounding frequency bins (see Figure 4). The high SNR can also be seen in the harmonics (12 and 18 Hz). 26

Table 1 Study 1: Group Level Z-scores at the Base Frequency (6 Hz) and Harmonics Harmonic 1F = 6 Hz 2F = 12 Hz 3F = 18 Hz 4F = 24 Hz 5F = 30 Hz 6F = 36 Hz 7F = 42 Hz 8F = 48 Hz 9F = 54 Hz 10F = 60 Hz 11F = 66 Hz 12F = 72 Hz z-score 188.47** 145.02** 167.00** 87.98** 72.81** 32.66** 36.43** 16.85** 4.17** 29.41** 5.31** 1.50

Note. These responses were calculated by averaging across all participants, conditions, and channels. The responses remained significant up until the 10th harmonic ** p < .001 (z > 3.10)

27

Figure 3. Study 1: Fast Fourier Transform (FFT) of 6 Hz and harmonics (12 and 18 Hz). These responses were calculated by averaging all participants, conditions, and channels.

28

Figure 4. Study 1: Signal-to-Noise Ratio (SNR) of 6 Hz and harmonics (12 and 18 Hz). These responses were calculated by averaging all participants, conditions, and channels. The topographic maps show the base frequency responses are localized to the medial occipital region.

29

To determine whether the base frequency responses differed by emotion condition, orientation, or region, we calculated baseline-subtracted amplitudes for each participant, similar to previous work (Dzhelyova et al., under review; Liu-Shuang et al., 2014). This was calculated in two steps. First, the baseline-subtracted amplitude was calculated for each harmonic as the amplitude at the harmonic minus the average amplitude at the 20 surrounding frequency bins. Then, the baseline-subtracted amplitudes for the 10 harmonics that were found to be significant in the grand-averaged data were summed for each emotion condition and region, to give an overall picture of the size of the response. Tests for normality were assessed in three ways. The P-P plots of the baseline-subtracted values were generally normally distributed, while the histograms suggested all of the variables were positively skewed. Next, skewness and kurtosis were examined, and only one variable, happy medial region was of concern for violating normality (skewness = 1.74, SE = .41; kurtosis = 3.90, SE = .80). Shapiro-Wilk tests were assessed for significance. Only one of the variables was non-significant (sad medial region) indicating non-normality for the majority of variables. A repeated-measures ANOVA was performed on the baseline-subtracted amplitude values with orientation (2: upright, inverted) as a between-subjects variable and emotion condition (3: happy, angry, sad) and region (3: left, medial, right) as withinsubjects variables. Mauchly's test of Sphericity was non-significant for emotion condition, 2 (2) = .96, p = .566, therefore no corrections were used. However, sphericity was violated for region (2 (2) =.66, p = .003), and Emotion Condition X Region (2 (2) = .15, p < .001), and consequently, Greenhouse-Geisser correction was applied. Levene's test of homogeneity of variance was non-significant. There were no main effects of 30

emotion condition, F(2, 58) = .85, p = .431, 2 = .03, or orientation, F(1, 29) = .74, p = .398, 2 = .03. Thus, the 6 Hz response did not differ depending on emotion condition or whether the faces were presented upright or inverted. There was a significant main effect of region, F(1.49, 43.15) = 41.69, p > .001, 2 = .59 as the response was localized to the medial-occipital region (M = 3.38, SE = .36), which was significantly larger than the right (M = 1.71, SE = .18) and left regions (M = 1.40, SE = .13, see Figure 5).

31

4.5 4 Upright Inverted

Baseline Subtracted Amplitude

3.5

3
2.5 2 1.5 1 0.5 0 Left Medial Hemisphere Right

Figure 5. Study 1: Significant main effect of region on the baseline-subtracted amplitudes for upright and inverted conditions. The baseline-subtracted amplitude was calculated for each harmonic as the amplitude at the harmonic minus the average amplitude at the 20 surrounding frequency bins. Following this, baseline-subtracted amplitudes for the 10 harmonics were summed at all three areas of interest to give an overall picture of the size of the response. Sixteen participants were included in the upright condition and 15 participants in the inverted condition. Error bars represent standard error.

32

Oddball Discrimination Frequency (1.2 Hz) Given that the response at the base frequency was present, the next step was to examine the response at the discrimination frequency (e.g., the fearful oddball face occurring every 5th face). Across all conditions and pooling across all channels, robust responses were present at the oddball frequency and its harmonics. On grand-averaged data, z-scores were calculated to determine the significance of the response at 1.2 Hz and its harmonics (e.g., 2.4, 3.6, and 4.8 Hz). The threshold for significance was placed at a one-tailed z-score (z > 1.64, p < .05) as we hypothesized the signal would be above the noise. Oddball discrimination responses were significant up until the 7th harmonic (10.8 Hz; see Table 2; z > 1.64, p < .05, z-score range: 1.67 ­ 13.51). To ensure this response was not driven by a few individuals, inspection of each individual's data showed that the majority of participants had significant z-scores over the pooled right hemisphere channels, where the effect was expected to be the strongest. At 1.2 Hz, 14 participants had a significant z-score, whereas at the first harmonic (2.4 Hz) 22 of the 33 participants had significant z-scores at the p < 0.05 level (see Table 3).

33

Table 2 Study 1: Group Level Z-scores at the Oddball Frequency (1.2 Hz) and Harmonics Harmonic z-score

1f/6 = 1.2 2f/6 = 2.4 3f/6 = 3.6 4f/6 = 4.8 6f/6 = 7.2 7f/6 = 8.4 8f/6 = 9.6 9f/6 = 10.8 11f/6 = 13.2

1.67* 5.12** 6.83** 12.95** 13.51** 4.35** 4.02** 6.28** 1.08

Note. These responses were calculated by averaging across all participants, conditions, and channels. The responses remained significant up until the 7th harmonic. * p < .05 (z > 1.64). ** p < .001 (z > 3.10).

34

Table 3 Study 1: Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics
1.2 Hz Participant 0 1 2 3 4 5 6 7 8 9 11 12 14 15 17 18 19 20 21 22 23 24 25 26 27 28 29 30 32 33 34 35 R 2.41* 1.23 1.47 -0.71 0.09 4.97* 3.39* 0.88 0.56 2.98* -1.09 1.22 10.65* 3.70* 4.70* 1.67* 0.34 4.75* 0.99 -0.66 1.19 2.54* -0.17 1.84* 6.60* -0.50 -0.42 2.57* 1.46 -0.99 4.42* -0.14 L 0.48 5.27* 3.27* -1.11 0.10 0.86 3.54* 0.44 0.26 1.11 1.67* 0.76 7.67* 9.93* 4.38* -0.47 2.98* -0.52 6.47* 1.41 0.73 1.03 0.57 3.42* 2.47* 3.49* 0.07 3.70* -1.01 0.61 3.80* 0.62 M -1.15 2.62* 2.47* -0.29 0.75 1.17 2.03* -1.15 0.21 1.35 0.80 0.41 7.55* 7.44* 3.74* 2.53* 1.89* 0.58 2.59* -0.09 0.20 2.24* -1.35 0.33 4.73* 1.15 0.86 1.97* -1.92 0.48 1.70* 0.58 R 2.39* 11.43* 3.70* 3.31* 2.25* 6.14* 4.36* 6.27* 2.08* 6.50* -0.02 -1.08 14.67* -0.25 1.59 3.59* -0.23 1.99* 9.35* 0.57 1.65* -2.22 3.43* 2.05* 0.57 2.17* 3.89* 0.22 0.89 4.12* 4.88* -0.56 2.4 Hz L 0.24 11.35* 4.04* 2.43* 5.21* 2.45* -0.89 1.22 0.62 -0.76 -0.57 1.77* 8.98* 7.56* 4.29* 5.00* -0.10 2.27* 6.86* -0.65 0.15 1.10 9.11* 0.49 0.20 3.34* 5.97* 0.09 1.33 0.29 15.17* -2.44 M 1.99* 7.09* 1.22 0.37 2.50* 3.13* 2.03* 1.65* -0.11 2.36* 2.00* 1.07 10.34* 5.42* 0.99 1.71* -0.08 1.22 9.08* -0.09 1.99* -1.26 2.47* 1.29 0.49 1.02 4.69* -0.55 0.62 1.34 6.62* -0.10

Note. Z-scores were based on Fast Fourier Transformed (FFT) data at the oddball frequency (1.2 Hz) and the first harmonic (2.4 Hz) averaged across emotion condition at three regions of interest. R = right occipital channels; L = left occipital channels; M = medial occipital channels. * p < .05 level (z > 1.64). 35

Emotion Discrimination Visual inspection of the data suggests that the strength of the discrimination response differed based on orientation, emotion, and region (Figure 6). To determine whether responses differed significantly, we calculated each participant's baselinesubtracted amplitudes (same as above) for each condition and region summing across the significant harmonics. Before testing for statistical significance, tests for normality were assessed in three manners. The first was inspection of the baseline-subtracted values in P-P plots. When examining emotion condition by orientation, the P-P plots suggested the values were normally distributed. Next, skewness and kurtosis were examined, and those of concern were in the inverted angry condition in the left (skewness = 2.18, SE = .58; Kurtosis = 5.29, SE = 1.12) and medial region (skewness = .65, SE = .58; Kurtosis = 3.02, SE = 1.12). Shapiro-Wilk tests were assessed for significance. The majority of the variables (15) were non-significant indicating normality, with the exception of inverted angry left region (Shapiro-Wilk = .76, p = .001), inverted happy medial region (ShapiroWilk = .87, p = .029), and upright sad medial region (Shapiro-Wilk = .88, p = .041). A mixed-factorial Analysis of Variance (ANOVA) was run using SPSS to test the effects of emotion condition (3: happy, angry, sad) and region (3: left, right, medial) as within-subjects factors and orientation (2: inverted or upright) as a between-subjects factor. Sphericity was not violated for Emotion as Mauchly's test of Sphericity was nonsignificant, 2 (2) = .95, p = .478, nor region, 2 (2) =.82, p = .066. However, sphericity was violated for Emotion X Region, 2 (9) = .48, p = .016, consequently, GreenhouseGeisser correction is reported. Levene's test of homogeneity of variance was significant 36

for the sad condition in all three regions (left: F(1, 29) = 7.22, p = .012, right: F(1, 29) = 5.97, p = .021, and medial: F(1, 29) = 12.32, p = .001), while it was non-significant for the remaining conditions. The ANOVA revealed a significant effect of Emotion, F(2, 58) = 9.46, p < .001, p2 = .25. Specifically, the oddball discrimination response was largest in the happy condition, followed by angry, then sad condition (see Table 4 for descriptive statistics). Pairwise comparisons using a Bonferroni correction for multiple comparisons revealed a significant difference between the means in the sad and happy conditions. There were no significant differences between the means of happy and angry conditions and sad and angry conditions. There was also a significant effect of orientation, F(1, 29) = 6.84, p = .014, p2 = .19 (see Table 5). Participants' oddball discrimination response was larger when faces were presented upright than when they were inverted. Contrary to our hypothesis, the effect of region was not significant, F(2, 58) = 1.79, p = .177, p2 = .058. The interaction between emotion and orientation was significant, F(2, 58) = 3.66, p = .032, p2 = .112 (see Figure 7). To follow up on this, two separate ANOVAs were run to test whether there was differential performance across emotion condition and orientation. The sum of the discrimination responses was calculated across all three regions for each emotion condition, to create an overall occipital region score. In the upright condition, the discrimination response was significantly larger in the happy condition, than in the sad and angry conditions, while angry and sad conditions did not differ from each other. In contrast, in the inverted condition, happy was not significantly different from angry or sad. Angry was significantly larger than sad. A Bonferroni adjustment was used for multiple comparisons. 37

There were no significant interactions between region and orientation, F(1, 58) = 1.79, p = .18, p2 = .06, nor between emotion and region, F(3.08, 89.20) = 1.10, p = .35, p2 = .04. Finally, the three-way interaction between Emotion X Region X Orientation was not significant, F(2, 58) = 1.76, p = .18, p2 = .06.

38

Upright Angry

Inverted

Happy

Sad

Figure 6. Topographic maps of Signal-to-Noise Ratio (SNR) by emotion condition at the oddball response (1.2 Hz).

39

Table 4 Study 1: Descriptive Statistics showing Effect of Emotion Condition on Oddball Discrimination Response (as measured by baseline-subtracted amplitudes summed across significant harmonics) 95 % CI Condition Angry Happy Sad M .46 .58 .33 SE .06 .07 .05 Lower .33 .43 .24 Upper .59 .73 .42

Note. Two participants were removed from the individual analysis as they did not provide data for more than one emotion condition and thus their data was unable to be averaged (n=31). The baseline-subtracted amplitude was calculated for each harmonic as the amplitude at the harmonic minus the average amplitude at the 20 surrounding frequency bins. Following this, baseline-subtracted amplitudes for the 10 harmonics were summed at all three areas of interest to give an overall picture of the size of the response.

40

Table 5 Study 1: Descriptive Statistics showing Effect of Orientation on Oddball Discrimination Response (as measured by baseline-subtracted amplitudes summed across significant harmonics) 95 % CI Orientation Upright Inverted M .59 .32 SE .07 .07 Lower .44 .17 Upper .74 .47

Note. Two participants were removed from the individual analysis as they did not provide data for more than one emotion condition and thus their data was unable to be averaged. The baseline-subtracted amplitude was calculated for each harmonic as the amplitude at the harmonic minus the average amplitude at the 20 surrounding frequency bins. Following this, baseline-subtracted amplitudes for the 10 harmonics were summed at all three areas of interest to give an overall picture of the size of the response. Sixteen participants were included in the upright condition and 15 participants in the inverted condition.

41

1

Baseline Subtracted Amplitude

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Angry Happy Emotion

Upright

Inverted

Sad

Figure 7. Study 1: Interaction between orientation and emotion condition on the sum of baseline-subtracted amplitudes across three regions of interest. Error bars represent standard error.

42

Study 1 Discussion Study 1 used Fast Periodic Visual Stimulation to assess emotion discrimination in adults. This technique involves the rapid presentation of stimuli at a rate of 6 Hz (6 faces/second), with every fifth face displaying a new emotional expression, a paradigm known as the fast oddball (Rossion, 2014a). This study provided an implicit measure of emotion discrimination in the adult brain for facial expressions that cross boundaries (i.e., positive-negative) and facial expressions within boundaries (i.e., negative-negative). Our results found a clear discrimination response in all three emotion conditions (angry, happy, sad) to the fearful oddball emotion. This discrimination response was robust, as measured by a high SNR, and present at the individual level. Orientation affected the manner in which adults processed the faces, showing better discrimination when faces were presented upright rather than inverted. Surprisingly, although the discrimination response was expected to be largest in the right region, the discrimination response was localized generally to the occipital regions. Through the use of FPVS, a response to brief changes in facial expressions of emotion was detected in the adult brain. The current findings are unique because they demonstrate rapid discrimination of brief changes of facial expression using an objective method. They replicate past work by underscoring adults' ability to discriminate facial expressions of emotion. The face provides a wealth of information regarding a social partner's mental state and allows us to make inferences about their intentions. Undoubtedly, this ability is important for social interactions, and is highlighted by emotion recognition deficits seen in Autism Spectrum Disorder, in which individuals have difficulty in social settings (Baron-Cohen, 1995).

43

When faces were presented upright, the current study found evidence for a greater discrimination response for happy versus fearful facial expressions compared to angry versus fearful or sad versus fearful. This finding is consistent with previous work on categorical perception. Adults tend to show a reduced sensitivity for within-category differences and better detection of between-category differences (Harnad, 1987). This is also seen in work on colour perception (Bornstein & Korda, 1984). Similarly, amplitude differences have been found in ERP components in response to within- and betweencategory emotional expressions (Campanella, Quinet, Bruyer, Crommelinck, & Guerit, 2002). The N170 amplitude was reduced for within-category pairs of emotional expressions relative to between-category pairs in bilateral occipito-temporal regions. In order to succeed at the FPVS oddball paradigm, both recognition of the frequent emotion and detection of the oddball emotion are required. The current findings are in accordance with past work that found a bias towards better recognition and faster categorization of positive facial expressions than negative ones (Bombari et al., 2013; Leppänen & Hietanen, 2004; Tottenham, Hare, & Casey, 2011). In a meta-analysis on the universality of emotion recognition in adults, happy was the most accurately recognized emotion across in- and out-groups (Elfenbein & Ambady, 2002). Other research shows there is a processing advantage for negative (threatening) faces over positive faces (Fox et al., 2000; Öhman, Lundqvist, & Esteves, 2001). According to this work, adults are faster at detecting negative facial expressions, as threatening faces are superior at grabbing the viewer's attention. The oddball was detected in all three conditions, showing support for superior processing of negative emotions, while the advantage adults show in recognizing happy more easily can account for why we saw smaller oddball responses in 44

sad and angry conditions. It is plausible that both of these emotions are more difficult to discern based on facial expression alone. In our visual world, a face is rarely an isolated stimulus, but is integrated in an environment, accompanied by vocal changes and bodily cues. Context provides cues as to a social partner's emotional state and intentions (e.g., sad expression at a funeral or fearful expression on a roller coaster). Indeed research has found that anger and sadness are most accurately recognized from the voice (for a metaanalytic review, see Elfenbein & Ambady, 2002). Work assessing emotion recognition as expressed by the body found that anger was detected faster than fear or neutral (Zhan, Hortensius, & De Gelder, 2015). Angry bodily expressions pose a greater threat to the viewer compared to neutral or fearful ones. In the context of the current study, adults may have required more information in addition to the face to process the emotion. The current findings extend work by Dzhelyova, Jacques, and Rossion (under review). This study assessed emotion discrimination of happy, disgust, and fearful from neutral facial expressions. Oddball discrimination was seen in all emotion conditions, showing detection of expression changes, with the largest response seen in the disgust condition, followed by happy then fearful. The authors did not find consistent enhanced processing of negative facial expressions, and this is possibly due to smaller differences between neutral and negative expressions than neutral and happy (Dzhelyova, et al., under review; Leppänen & Hietanen, 2004). These researchers propose that future work should directly compare different expressions. Our work extends this research, as we directly contrasted expressions. We found the largest discrimination response to fearful expressions in the happy condition, although we did not include a disgusted condition. Future studies should examine additional emotion comparisons, in order to determine 45

which emotions are most easily discriminated from one another, and whether this rapid discrimination response in the brain is related to behavioural measures of emotion discrimination and recognition. The oddball discrimination response to the inverted faces was significantly lower than that in the upright condition. This shows that adults were relying on high-level processing to discriminate facial expressions during rapid presentation. This difference between upright and inverted faces cannot be explained simply by image characteristics, since the images were identical in the two orientations. Nor can this difference be explained by attentional factors (e.g., participants paid more attention in the upright than the inverted condition), because the response at the base rate frequency (6 Hz) did not differ between the two orientations. Further, there were differences in the oddball discrimination responses to different emotions in the two orientations, such that the happy condition had the highest oddball discrimination response, but only when the faces were presented upright. This was not true when stimuli were inverted, suggesting the magnitude of this inversion effect equally impaired recognition of the fearful oddball in the happy, angry, and sad conditions. Further, the localization of the oddball discrimination response to upright faces was lateralized to the right-occipital region, whereas it was more widespread across the occipital region to inverted faces (Figure 6). Inverting faces has been shown to produce deficits in recognizing invariant face features such as identity (Valentine, 1988) and variant facial expressions (Bombari et al., 2013; Dzhelyova et al.,under review) and is thought to impair holistic processing (Maurer, Le Grand, & Mondloch, 2002)

46

Based on past work, it was predicted that the oddball discrimination response would show right hemispheric localization (Bentin et al., 1996; de Heering & Rossion, 2015; Farzin et al., 2012; Kanwisher, Tong, & Nakayama, 1998; Mcclure, 2000; Rossion, 2014a; Rossion et al., 2015). Indeed, the topographic maps and oddball discrimination responses were highest over the occipital sites, specifically the right region for upright faces. This response was robust (high SNR) and significant at the individual level. This distribution over occipito-temporal regions is consistent with other FPVS experiments when processing identity and facial expressions (Dzhelyova et al., under reivew; LiuShuang et al., 2014) and in ERP experiments (Batty & Taylor, 2003; Mcclure, 2000). The current study demonstrates that the adult brain is capable of rapid emotion discrimination. The developmental trajectory of emotion discrimination is currently unclear. Thus, the purpose of Study 2 was to use the same paradigm with infants to investigate the origins of emotion discrimination.

47

Study 2: Emotion Discrimination in Infants The second study seeks to extend previous research on emotion discrimination in infancy. Indeed, there is a large body of evidence to suggest that infants are capable of discriminating positive from negative facial expressions, however it is unknown whether they can discriminate within-category expressions. Due to limitations of the typical methods used, it is also unclear exactly when during infancy this ability emerges. FPVS is an objective method used in the current study to address this question. Since this method is an implicit measure of discrimination, it is not susceptible to the order effects observed in behavioural research. In Study 1, FPVS was shown to be an appropriate method of studying emotion discrimination in adults; thus, it was deemed useful to apply to an infant sample. In order to establish whether they show the ability, all infants saw upright faces. If infants showed a discrimination response to upright faces, we planned to test an inverted condition to control for low-level features. Hypotheses For all comparisons, it was predicted that there would be significantly larger SNR at the base frequency (6 Hz) and its harmonics (12 Hz, 18 Hz etc.) than at surrounding frequencies. Based on previous literature, we hypothesized that the infants would show a robust response at the oddball frequency (1.2 Hz) and its harmonics (2.4 Hz, 3.6 Hz) during the happy-fearful condition, providing evidence for discrimination, but we remained agnostic whether they would be able to discriminate between angry-fearful and sad-fearful. Based on past research assessing face recognition using FPVS in adults and infants and work assessing emotion processing (Bentin et al., 1996; de Heering & Rossion, 2015; Farzin et al., 2012; Kanwisher et al., 1998; Mcclure, 2000; Rossion, 48

2014a; Rossion et al., 2015), we predicted that the response to the oddball faces would likely be largest in the right hemisphere. Parents completed questionnaires (described in the Methods section) on their emotional expressivity, and their infant's temperament and motor development. We hypothesized that infants' ability to crawl would be positively correlated with their ability to discriminate negative facial expressions. Previous work has shown that mothers' expressions of negative expressions towards their infant increase when the infant begins to locomote (Campos et al., 2000). Other work found 7-month-old infants with high perceptual sensitivity showed differences in ERP components to fearful faces than those with low perceptual sensitivity (Jessen & Grossmann, 2015). Drawing from this, it was hypothesized that infants with high perceptual sensitivity would show a larger oddball discrimination response than those with low perceptual sensitivity. It was also predicted that infants of families who scored high on negative expressivity would be better at discrimination than infants of families who scored low on negative expressivity. Method Participants Participants were recruited from the Ryerson Infant and Child Database (RIC-D). This database consists of families recruited from local libraries and Toronto's BabyTime Show. Eligibility requirements included full-term 7-month-old infants (37-42 weeks gestation). Exclusion criteria included parent-reported visual impairment, neurological impairment, or a history of seizures. The final sample included in the present study consisted of 33 infants (M = 214.9 days, range: 195 ­ 225 days, SD = 8.3, 55% female). An additional eight infants were tested but excluded from analyses because they were 49

unable to provide EEG data (n = 1), they were not full-term (n = 2), excessive movement during EEG recording or failure to contribute at least two artifact-free segments (n = 5). Although not all infants completed all three conditions, they contributed on average four angry segments, four happy segments, and five sad segments. A total of 17 infants provided usable data for the angry condition, 21 provided data for the happy condition, and 18 provided data for the sad condition. Stimuli The stimuli used in the current study were identical to those from Study 1. Infants only saw faces in the upright orientation. Procedure Infants were screened for eligibility either by telephone or email. Once eligible, families were invited to the Brain and Early Experiences (BEE) lab. Parents gave written informed consent (see Appendix B) and were given the option to complete questionnaires or have their infant complete the EEG experiment first. During the EEG experiment, infants were seated 80cm from the screen on their parent's lap. This resulted in a visual angle of 7.87° by 6.80° for the area of the face, similar to other work with infants (de Heering & Rossion, 2015; Kobiella et al., 2008). The cap was placed on the infant's head while sitting on their parent's lap and impedances were below 40 k before recording. The order of emotion condition presentation was randomized for each infant. Lullaby music quietly played in the background during the entirety of the experiment. The experiment began whenever the parent and researcher believed the infant was awake and ready. One researcher remained in the experiment room and directed infants' attention to the screen using a squeaky toy whenever they lost interest for approximately 3s or longer, 50

while the second researcher controlled the experiment in a separate room. The researcher controlled the stimuli by watching the infants' gaze through a live video feed from a camera mounted on the computer screen. In between sequences, a small attention grabber played on the screen to help direct the infants' attention back to the screen (e.g., attractive yellow ball that expanded and decreased in size). Whenever infants became too fussy or lost interest, the experiment ended. The entire visit took approximately 1.5 hours from start to finish. Questionnaires. Parents completed four questionnaires (Demographics, Motor Development, SEFQ, and the IBQ-R; see Appendices C-F). The first questionnaire collected information on the infant's ethnicity, due date, and date of birth. Parents completed a short questionnaire assessing their infant's gross motor skills, in order to assess their milestones in motor development. Parents completed the Self-Expressiveness in the Family Questionnaire (SEFQ; Halberstadt, Parke, Cassidy, Stifter, & Fox, 1995). This 40-item instrument seeks to measure the degree of expressiveness people show in their families and has been validated in four separate studies with 499 mothers and 362 fathers (Halberstadt et al., 1995). There is no published work using this questionnaire in children younger than 54 months of age, thus the current study seeks to collect information from this population. Finally, parents provided information about their child's temperament via the Infant Behavior Questionnaire-Revised (IBQ-R; Garstein & Rothbart, 2003). This 191-item measure assesses infant temperament across 14 different subscales. EEG Acquisition Continuous EEG was recorded using a 128-channel Hydrocel Geodesic Sensor 51

Net (Electrical Geodesics, Inc) with four eye channels removed. The sampling rate was 500Hz and electrode impedances were verified to be below 40 k at the start of the experiment. Pre-processing The EEG processing steps were identical to those taken in Study 1, except for the following differences. First, Independent Components Analysis was not performed on the infant data as there are no eye electrodes on the infant caps, and infants blink less frequently than adults. As with the adult data, noisy channels were interpolated using three neighbouring electrodes and sequences were removed when more than 10% of the electrodes (12 of 124 electrodes) required interpolation. In an attempt to clean up the infant data further, sequences were removed if the SNR was below 2 at the base frequency (6 Hz) at all medial occipital electrodes (channels 70, 75, 83; de Heering & Rossion, 2015). This resulted in 1 to 6 sequences removed per infant. For the sequences that were included in the analyses, an average of 11 channels were interpolated (range: 812). After channel interpolation, the processing steps were the same as in Study 1. Results Base Stimulation Frequency (6 Hz) Similar to the adults, there were large responses present at the base stimulation frequency and its harmonics in all of the conditions. Grand averaging across all conditions and pooling across all channels, the responses were significant up until the 5th harmonic (36 Hz; see Table 6, z > 1.64, p < .05, z-score range: 3.45 ­ 27.23). The FFT amplitude spectrum (Figure 8) shows the responses at 6 Hz and its harmonics over all pooled channels. The SNR also suggested large responses at the base stimulation 52

frequency. The signal at 6 Hz is 2.2 times larger than surrounding frequency bins (see Figure 9). The high SNR can also be seen in the harmonics (12 Hz, 18 Hz). Topographically, this response was localized to the medial occipital region in all three emotion conditions (see Figure 10).

53

Table 6 Study 2: Group Level Z-scores at the Base Frequency (6 Hz) and Harmonics Harmonic 1F = 6 Hz 2F = 12 Hz 3F = 18 Hz 4F = 24 Hz 5F = 30 Hz 6F = 36 Hz 7F = 42 Hz z-score 18.23** 21.69** 27.23** 4.97** 3.94** 3.45** 1.10

Note. These responses were calculated by averaging across all participants, conditions and channels. The responses remained significant up until the 5th harmonic. ** p < .001 (z > 3.10)

54

Figure 8. Study 2: Fast Fourier Transform (FFT) of 6 Hz and harmonics (12 and 18 Hz). These responses were calculated by averaging all participants, conditions, and channels.

55

Figure 9. Study 2: Signal-to-Noise Ratio (SNR) of 6 Hz and harmonics (12 and 18 Hz). These responses were calculated by averaging all participants, conditions, and channels. The topographic map shows the base frequency response is localized to the medial occipital region, similar to the adult participants.

56

Condition

Angry

Happy

Sad

Figure 10. Study 2: Topographic maps of Signal-to-Noise Ratio (SNR) by emotion condition at 6 Hz.

57

Baseline-subtracted amplitude values were calculated for each participant by summing the five harmonics that were found to be significant. Because most infants only completed one or two emotion conditions, we elected to conduct separate ANOVAs for each emotion condition on the baseline-subtracted amplitude values. This allowed us to retain all of the infants who completed each emotion condition. Normality was assessed for each of the variables: examination of P-P plots indicated the values were normally distributed. Skewness and kurtosis indicated nonnormality for two variables, sad left region (skewness = 2.33, SE = .54; kurtosis = 7.52, SE = 1.04) and sad medial region (skewness = 1.96, SE = .54; kurtosis = 4.95, SE = 1.04). Finally, Shapiro-Wilk tests of normality were significant for the same variables: sad left region (Shapiro-Wilk = .78, p = .001) and sad medial region (Shapiro-Wilk = .82, p = .003). The baseline-subtracted amplitudes for the infants who completed the angry condition (n = 17) were entered into a repeated measures ANOVA on baseline-subtracted values with region (3: left, medial, right) as the within-subjects factor. Mauchly's test of Sphericity was violated, 2 (2) =.56, p = .013, and thus Greenhouse-Geisser correction was used. There was a significant main effect for region, F(1.39, 22.26) = 28.28, p < .001, p2 = .64, with the 6 Hz response being highest in the medial occipital region (see Figure 11). Pairwise comparisons indicated the response at the medial region (M = 4.51, SE = .56) was significantly larger than the right (p < .001, M = 1.63, SE = .22) and left (p < .001, M = 1.50, SE = .21) Next, the baseline-subtracted amplitude values for the infants who completed the happy condition (n = 21) were entered into a repeated measures ANOVA with region (3: 58

left, medial, right) as within-subjects factor. Mauchly's test of Sphericity was violated, 2 (2) = .59, p = .006, and thus Greenhouse-Geisser correction was used. Again, there was a significant main effect for region, F(1.41, 28.26) = 50.04, p < .001, p2 = .71. Pairwise comparisons indicated the response at the medial region (M = 4.90, SE = .52) was significantly larger than the right (p < .001, M = 1.67, SE = .22) and left (p < .001, M = 1.17, SE = .24) (see Figure 11). Finally, the baseline-subtracted amplitude values for infants who completed the sad condition (n = 18) were entered into a repeated measures ANOVA with region (3: left, medial, right) as within-subjects factor. Mauchly's test of Sphericity was violated, 2 (2) = .63, p = .024, and thus Greenhouse-Geisser correction was used. Similarly, there was a significant main effect of region, F(1.46, 24.79) = 32.60, p < .001, p2 = .66. Pairwise comparisons indicated the response at the medial region (M = 5.13, SE = .63) was significantly larger than the right (p < .001, M = 1.94, SE = .23) and left (p < .001, M = 1.64, SE = .35; see Figure 11). Bonferroni adjustments were used to adjust for multiple comparisons.

59

7

Angry

Happy

Sad

Baseline Subtracted Amplitude

6 5 4 3 2 1 0 Left Medial Region Right

Figure 11. Study 2: Baseline-subtracted amplitudes by region and emotion condition at the base frequency (6 Hz) and harmonics. Error bars represent standard error.

60

Oddball Discrimination Frequency (1.2 Hz) Across all conditions and pooling across all channels, the response at the discrimination frequency was not as clear as the discrimination response in adults. Again, z-scores were calculated to determine significance, which indicated no significant responses (Table 7). Thus, at the group level, infants were not able to discern the fearful oddball face. Inspection at the individual level also indicated this was the case. Across all conditions, only 3 of 19 infants show a significant response over the right region and 5 infants show a significant response at the medial-region at 1.2 Hz (see Table 8). Since we predicted infants might be better at cross-boundary discrimination, individual z-scores were calculated for the happy condition alone. Again, the responses at 1.2 Hz and its harmonics were not consistently significant. Similarly, the individual z-scores for angry and sad conditions yielded non-significant results (see Tables 9-11).

61

Table 7 Study 2: Group Level Z-scores at the Oddball Frequency (1.2 Hz) and Harmonics

Harmonic 1f/6 = 1.2 2f/6 = 2.4 3f/6 = 3.6 4f/6 = 4.8 6f/6 = 7.2 7f/6 = 8.4

z-score 0.02 -0.17 -0.95 0.67 -1.08 -0.87

Note. These responses were calculated by averaging across all participants, conditions and channels. None of the responses at any of the frequencies were significant.

62

Table 8 Study 2: Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics across Emotion Conditions
1.2Hz Participant 7 9 11 15 16 17 19 22 23 25 26 28 29 32 39 44 45 46 48 R 0.88 2.11* 1.98* 0.53 -0.02 1.45 -0.77 -0.10 1.37 0.70 -0.12 0.93 -0.58 0.18 2.36* 0.05 0.14 -0.78 -0.07 L -0.30 1.95* 0.04 -0.36 1.03 1.30 0.90 0.56 -1.19 -0.94 -0.76 2.25* 0.82 1.25 0.17 0.78 1.42 -0.48 -1.91 M 0.40 1.93* 1.70* 1.15 -0.31 -0.01 0.63 2.54* 1.88* -1.26 -0.85 1.69* -0.33 -0.18 0.67 0.43 0.79 -1.15 -0.17 R 1.52 -1.37 0.40 -0.36 -1.21 -1.24 3.27* 0.51 -0.20 2.85* 0.97 0.60 0.45 0.16 -0.03 -1.30 0.28 -1.60 0.90 2.4Hz L 2.12* -0.38 -1.01 -0.48 -1.27 1.17 -0.06 1.52 -1.35 3.66* -1.05 0.06 -0.63 -0.20 -0.64 -0.32 -0.57 -0.73 -0.48 M 3.26* -0.72 0.66 0.25 -0.79 -1.26 3.49* 0.77 1.07 -0.38 -1.27 0.27 0.80 0.51 -1.59 0.09 -1.17 -1.06 1.46

Note. A total of 19 infants were included in this analysis as they provided data for more than one condition. Infants who only completed one condition were unable to be averaged across conditions. Z-scores were based on Fast Fourier Transformed (FFT) data at the oddball frequency (1.2 Hz) and the first harmonic (2.4 Hz) averaged across emotion condition at three regions of interest. R = right occipital channels; L = left occipital channels; M = medial occipital channels. * p < .05 level (z > 1.64).

63

Table 9 Study 2: Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Happy Condition
1.2 Hz Participant 7 8 9 11 13 15 16 17 18 19 20 21 22 23 24 25 26 28 29 44 48 R 0.62 0.28 1.27 1.91* -0.09 0.46 -0.36 1.26 1.13 0.30 -0.44 -0.42 1.91* 0.87 4.69* -0.85 1.48* 1.04 -0.38 0.24 -0.77 L 0.02 -1.44 1.06 1.41 -0.62 0.31 0.45 1.33 -1.00 0.46 -0.76 0.33 0.75 -0.35 0.53 0.34 0.03 2.66* 2.92* 0.55 -2.05 M 0.72 1.37 1.43 1.95* 0.40 -0.48 -1.20 1.23 -0.23 1.65* -1.19 0.35 1.31 0.32 0.69 -0.26 -0.80 2.91* -1.03 0.78 0.52 R 0.71 0.49 -0.68 1.00 1.09 -1.49 -0.70 -1.07 -0.50 1.16 -1.73 0.32 0.53 0.01 7.07* 0.37 -0.54 1.08 0.87 -1.48 0.58 2.4 Hz L 2.95* 1.34 0.17 -0.43 -0.10 0.16 -1.14 -0.01 -0.07 -0.35 -0.30 -0.98 1.86* -0.77 7.54* 3.56* 0.52 -0.57 -2.51 -0.24 -0.74 M 3.01* -0.47 0.11 0.47 0.84 0.12 -1.30 -1.09 0.47 2.97* -1.86 -0.79 1.31 0.19 4.85* -0.53 -0.46 1.02 -0.97 0.42 1.23

Note. A total of 21 of 33 infants provided data included in the analysis for the happy condition. Z-scores were based on Fast Fourier Transformed (FFT) data at the oddball frequency (1.2 Hz) and the first harmonic (2.4 Hz) averaged across emotion condition at three regions of interest. R = right occipital channels; L = left occipital channels; M = medial occipital channels. * p < .05 level (z > 1.64).

64

Table 10 Study 2: Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Angry Condition
1.2Hz Participant 7 15 16 17 23 26 29 32 33 36 38 39 44 45 46 47 48 R 0.98 0.01 0.26 1.56 1.23 -0.86 1.81* 0.14 -2.10 1.09 -0.20 2.86* -1.69 -0.29 0.61 0.69 -0.09 L -0.45 -0.74 1.00 0.58 -1.37 -1.02 -0.53 2.65* -0.96 0.75 -1.06 -0.13 -1.43 0.51 -0.22 0.42 2.05* M 0.05 1.29 0.31 -1.40 2.45* -0.64 0.97 1.87* -0.95 -1.31 -0.98 -0.34 -1.68 0.51 -0.58 1.16 -0.43 R 2.33* -0.65 -0.74 -0.46 -0.07 2.08* 0.75 -0.85 -1.27 0.95 -1.11 -0.39 -0.24 1.06 -1.26 1.41 0.00 2.4Hz L 0.55 -0.29 -0.58 2.55* -1.22 -1.48 1.86* -1.17 1.33 -0.24 -0.90 -1.67 0.08 0.39 -0.69 0.58 0.18 M 1.28 0.49 0.44 -0.72 1.22 -1.87 2.93* -0.67 -1.64 -0.70 -1.58 -1.16 -0.29 -0.88 -0.78 0.93 -0.07

Note. A total of 17 of 33 infants provided data included in the analysis for the angry condition. Z-scores were based on Fast Fourier Transformed (FFT) data at the oddball frequency (1.2 Hz) and the first harmonic (2.4 Hz) averaged across emotion condition at three regions of interest. R = right occipital channels; L = left occipital channels; M = medial occipital channels. * p < .05 level (z > 1.64).

65

Table 11 Study 2: Individual Z-scores at 1.2 and 2.4 Hz Oddball Harmonics in the Sad Condition
1.2Hz Participant 9 11 15 19 22 25 28 29 32 34 35 37 39 40 44 45 46 48 R 2.08* 1.45 2.51* -1.07 -1.52 1.10 0.81 -1.93 0.23 1.87* -0.32 1.19 1.30 0.33 1.67* 0.56 -1.39 0.56 L 1.25 -1.32 0.61 0.81 0.25 -1.95 1.30 -0.95 -0.98 -2.01 0.53 -1.61 0.50 -1.10 3.40* 1.91* -0.62 -1.46 M 1.37 1.36 1.22 -0.41 3.30* -1.38 0.13 -0.10 -0.92 -0.67 0.18 -0.28 1.65* 0.74 1.37 0.96 -1.32 -0.71 R -1.17 -0.29 1.46 4.12* 0.47 2.54* -0.05 -1.13 0.80 0.24 -0.34 -2.34 0.66 -0.17 -0.78 -1.07 -1.19 0.85 2.4Hz L -0.96 -0.76 -0.96 0.52 0.70 3.14* 1.26 -0.72 1.37 -0.12 -0.03 -0.81 0.63 0.86 -0.38 -0.88 -0.48 0.16 M -1.32 0.62 -0.29 3.44* 0.10 -0.04 -0.67 -0.31 1.01 -0.98 0.01 0.94 -0.95 -0.15 -0.29 -0.86 -0.54 2.50*

Note. A total of 18 of 33 infants provided data included in the analysis for the angry condition. Z-scores were based on Fast Fourier Transformed (FFT) data at the oddball frequency (1.2 Hz) and the first harmonic (2.4 Hz) averaged across emotion condition at three regions of interest. R = right occipital channels; L = left occipital channels; M = medial occipital channels. * p < .05 level (z > 1.64).

66

Individual Differences Past literature has assessed whether individual differences affect infants' ability to discriminate emotional facial expression. An SNR response was calculated for each infant for each emotion condition at 1.2 Hz at the right region of interest. This number was correlated with infant's perceptual sensitivity (as measured by the IBQ-R) and parents' positive and negative expressiveness (as measured by the SEFQ). Descriptive statistics can be seen in Table 12. Neither perceptual sensitivity nor parental expressiveness correlated with any of the discrimination responses in any of the emotion conditions (for correlation table see Table 13). In terms of motor development, the majority (76%) of the sample was not crawling (n = 25) so we were unable to analyze the relation between crawling status and discrimination.

67

Table 12 Study 2: Descriptive Statistics of Infant Questionnaire Data Scale n Range of scores M SD

Perceptual Sensitivity 1 Positive Expressiveness 2 Negative Expressiveness 2

33 33 33

2.55 - 5.86 5.50 - 8.61 1.12 - 6.29

4.23 6.73 3.63

0.94 0.88 1.11

1

Perceptual Sensitivity refers to the detection of low intensity stimuli (e.g., "how often

did the baby notice fabrics with scratchy texture (e.g., wool)?"). Higher scores indicate higher perceptual sensitivity.
2

Expressiveness refers to the predominant style of exhibiting verbal and nonverbal

expressions within a family (e.g., Positive expressiveness: "how frequently do you express gratitude for a favour?" Negative expressiveness: "how frequently do you show dislike for someone?"). Higher scores indicate greater expressivity.

68

Table 13 Study 2: Pearson Correlations Between Signal-to-Noise ratio (SNR) of 1.2 Hz at Right Occipital Region and Questionnaire Data 1 1. Perceptual Sensitivity 2. Positive Expressiveness 3. Negative Expressiveness 4. SNR Angry 5. SNR Happy 6. SNR Sad -.21 -.16 .21 -.15 -.17 .26 -.11 .07 -.30 -.34 .03 .24 2 3

69

Study 2 Discussion One of our first hypotheses was that infants would show a robust response at the stimulus presentation rate (base frequency of 6 Hz and harmonics). Indeed, infants were sensitive to the rate of presentation, showing a high Signal-to-Noise Ratio (SNR) at the same frequency at which the stimuli were presented. This response was predicted to be localized to the occipital regions of the infant brain. We found the strongest 6 Hz response over the medial occipital region in all three conditions. Next, we predicted infants would show a discrimination response in the happy-fearful condition, showing an ability to differentiate between positive and negative emotions. We were agnostic as to whether infants would show an oddball discrimination response to the sad-fearful and angry-fearful conditions. To our surprise, inspection at both the group level and individual level showed that the oddball discrimination responses were not consistently present in any of the emotion conditions. We expected any oddball discrimination response to be localized to the right region; however, we did not find any evidence for this. The major aim of the current study was to examine infants' capacity to discriminate facial expressions of emotion using FPVS. The most surprising finding was the lack of discrimination response for positive and negative emotions. Past behavioural, ERP, fNIRS, and psychophysiological work has found that infants at 7 months are likely capable of both discrimination and categorization of positive and negative expressions (Bornstein & Arterberry, 2003; Kaneshige & Haryu, 2015; Kotsoni, de Haan, & Johnson, 2001; Lobue & Deloache, 2010; Nakato et al., 2011; Peltola, Leppänen, & Hietanen, 2011a). 70

There are several possible reasons why infants did not show an ability to discriminate facial expressions. First, it is possible that the stimulation frequency was too fast for infants to process emotional expressions. Past work assessing infants processing of high-level stimuli using FPVS have used presentation rates of 3.5 Hz (Farzin et al., 2012) and 6 Hz (de Heering & Rossion, 2015). Both studies examined 4- to 6-montholds' responses to face categories by comparing them to objects. de Heering and Rossion (2015) argue that the oddball discrimination response arises from a face-specific population of neurons that respond differentially to faces rather than objects. Faces are a common visual category in infants' visual world, who spend 25% of their time awake exposed to faces (Sugden, Mohamed-Ali, & Moulson, 2014). In addition, infants consistently prefer to look at face or face-like stimuli over other objects (Fantz, 1958). Given the frequency and social importance of faces, object-face discrimination is an easier task than emotion discrimination. There are larger visual differences between faces and objects, whereas the stimuli in the current study consist solely of faces, with very slight differences in features. It is plausible that in order for infants to process the frequent emotion and subsequently detect the brief difference in expression, they require a longer time with the face. In ERP studies, approximately 300ms (negative-going component N290) after stimulus presentation is when the infant brain can differentiate between two expressions. Past ERP work with 7-month-olds examining brain responses to subliminal (50 and 100ms) and supraliminal (500ms) presentations of happy and fearful facial expressions revealed differences to both emotions regardless of stimulus duration (Jessen & Grossmann, 2015). Thus, infants show the capacity to discriminate expressions presented subliminally. In contrast, in the current study, the stimulus was 71

presented (at full contrast) for approximately 33ms before being replaced by the following face. The infant brain may require more time with each face before being replaced by the next. Furthermore, the discrimination ability in the current study is measured as the detection of an oddball fearful face. This requires the viewer to rapidly process and recognize the frequent expression and discern this from the oddball expression. Previous behavioural and ERP studies, however, involve presenting stimuli centrally and slowly. Future work should address this by decreasing the presentation frequency (subsequently increasing presentation time) and investigating whether this produces a discrimination response. Other modifications to the experimental design may also help to detect a discrimination response. Each sequence in the current study was 30 seconds long, which was longer than previous studies using FPVS with infants. In this light, the sequence length may have been too long to hold infants' attention for the entirety of the presentation. By adopting a shorter sequence time, it is possible infants would provide cleaner data. Through adult pilot testing, we found a lack of attention resulted in no oddball discrimination response. Research investigating the role of attention on emotion discrimination found that successful detection and analysis of rapidly presented facial expressions requires focal attention (Eimer et al., 2003). Based on the current study, by decreasing the demands on the infant, making changes to the experimental design may help to find a discrimination response. For future FPVS studies with infants in our lab, we plan to make modifications to the testing setup to encourage infants' attention to stimuli (e.g., rather than having the experimenter in a separate room, placing the experimenter hidden behind the screen to attract infants' attention to the presentation by 72

calling for them). Because neural responses in FPVS are objectively defined by the base frequency, an advantage to this method is that they are not affected by any vocalizations, as is the case with ERP studies. This is an important advantage of FPVS that we will incorporate into our design of future studies. Another potential explanation for why infants did not detect the oddball expression is that the stimuli affected the infant's ability to discriminate. Discrimination and categorization are highly task-dependent (Madole & Oakes, 1999). The stimuli used in the current study consisted of nine different models, all with varying intensities of emotional expressions. Most studies tend to use high intensity stimuli. In an early study, 7- and 9.5-month-olds were unable to discriminate happy and angry expressions when salient features (e.g., toothiness) were controlled for (Caron, Caron, & Myers, 1985). This suggests infants might rely on the features of an expression rather than the expression as a whole. Other work using eight different models found no evidence for categorization of positive and negative facial expressions in 7-month-olds (Phillips et al., 1990). Variations in stimuli and presentation methods used in a study can also affect the direction of infants' discrimination. Geangu, Hauf, Bhardwaj, and Bentz (2011) showed infants dynamic videos of other infants in distress or expressing happiness. Both 6- and 12-month-olds exhibited larger pupil diameter in response to another baby in distress. In contrast, Jessen, Altvater-Mackensen, and Grossmann (2016) presented static happy and fearful faces subliminally and supraliminally. Seven-month-old infants showed greater pupil dilation to happy expressions. Although the findings are not in agreement, they show infants' autonomic system can discriminate between positive and negative

73

emotions. In conclusion, however, the materials, stimulus duration, and modality used all play a role in infant experiments on emotion perception. We failed to see discrimination between negative emotions. A possible explanation for this is that the ability is not yet developed at 7 months. There are more similarities of features for within-category expressions than between-category ones, and the emotions are more conceptually similar as well, making it a more difficult task for infants. The representation of happy faces may be well-established by 7 months, whereas representations of negative expressions may need more time to develop (Leppänen & Nelson, 2006). Indeed, age-related improvements on discrimination and categorization are seen in preschool samples (Widen & Russell, 2003). Experience may also play a role in infant's discrimination ability. Infants experience more positive than negative expressions in their natural environment (Sugden, Festa, Vascotto, & Moulson, 2016; Moulson, Sugden, & Patel, 2015). In earlier work, Campos and colleagues (2000) proposed that exposure to negative emotions as expressed by parents might increase as a result of self-locomotion. Given that the majority of infants in the current sample were not crawling yet, according to this view, they likely had little experience with negative facial expressions. Very few published studies have investigated discrimination of negative facial expressions. This is in contrast to the many studies that show positivenegative emotion discrimination, leaving one to speculate whether a publication bias is the leading reason why many of the studies finding null results go unpublished. Testing older infants might help to resolve this question. Discrimination and categorization of facial expressions do not imply an understanding of the emotional meaning of those facial expressions. Based on findings 74

from looking time paradigms, we cannot conclude that infants understand the emotions being expressed. Researchers have observed infant behaviours during habituation tasks to gain insight into their understanding of positive and negative emotions (Kaneshige & Haryu, 2015; Serrano, Iglesias, & Loeches, 1995). Both studies purported that if infants displayed negative affect (i.e., avoidance behaviour, leaning backward and frowning) towards negatively-valenced facial expressions and positive affect (i.e., smiling, movement toward monitor) towards positively-valenced facial expressions, this is evidence that infants are capable of understanding the emotion being presented. Using both happy and angry facial expressions and similar coding schemes, researchers found conflicting evidence. The earlier study found that infants would react according to the affect being presented on screen, while the more recent study found no behavioural differences. Investigation into infants' behaviour during the rapid presentation would likely help to uncover whether infants understood the frequently presented expressions. While their neural response as measured by FPVS shows no detection of the oddball discrimination, we might be able to disentangle their ability to understand the frequently presented emotion in this manner.

75

General Discussion In sum, we found evidence in the adult brain for rapid discrimination of fearful expressions from happy, sad, and angry using a novel technique known as Fast Periodic Visual Stimulation (FPVS). This response was localized to the occipital region and was seen on an individual level. This is one of the first studies to have shown the adult brain is capable of emotion processing at a fixed rapid rate using FPVS. We saw a high signalto-noise ratio in both adults and infants in response to the rate at which faces were presented. However, there was no evidence that infants detected the oddball expression in any of the emotion conditions, indicating 7-month-old infants were unable to discriminate facial expressions of emotion when presented rapidly. From a practical standpoint, FPVS is an appropriate measure to use for investigating emotion processing in adults, however, requires additional testing to determine its use in infancy. In terms of theoretical perspectives, there are two competing views that predict different outcomes for infant's ability to discriminate emotions. The structural theory of emotions predicts emotions are related, with the ability to discriminate amongst them emerging with other abilities (e.g., language) at an older age (Russell, 1980; Widen & Russell, 2008). On the surface, the findings from the current study provide preliminary evidence for this, however, further testing is required to determine whether these findings were due to methodological parameters or a lack of attention on the infant's part. If we were to have found that infants were capable of detecting the oddball stimulus, this would be seen as evidence for the basic theory of emotions. From this perspective, emotional expressions are recognized universally, suggesting a strong biological mechanism

76

(Ekman, 1992, 1999; Ekman & Friesen, 1978). Discrimination of expressions in the current paradigm would be taken as evidence for an innate ability. Not only does this technique show emotion discrimination on a group level, it is ideal for examining individual differences in emotion processing. Hamann and Canli (2004) report individual differences in adults' emotion processing in terms of personality, biological sex, and genotype. When trying to understand the neural bases of emotion perception, it is important to consider these individual differences. Two people viewing the same emotional stimulus can have a wide range of emotional responses, which suggests that the corresponding neural bases might differ. For example, amygdala responses to happy faces varied greatly as a function of extraversion (Canli, Sivers, Whitfield, Gotlib, & Gabrieli, 2002). Future work assessing individual variables and emotion discrimination by FPVS would prove to be a worthwhile avenue of research, as this is a strength of this method. Not only do adults show individual differences in emotion perception, but infants do as well (Jessen & Grossmann, 2015; Ravicz, Perdue, Westerlund, Vanderwert, & Nelson, 2015). We hypothesized that infants high in perceptual sensitivity would evidence a greater oddball discrimination response. In addition, we predicted positive and negative expressiveness in the family environment would be related to their discrimination ability. Contrary to past work, we found no correlation between any individual differences and infant discrimination in the current study. However, these predictions were based on the assumption that infants would evidence discrimination. Our failure to find relations between these variables was likely due to the lack of variation in a measurable discrimination response on the infants' part. 77

Limitations One of the strengths of electrophysiological studies is that of high temporal resolution at the level of milliseconds. With this, however, it is limited in its spatial resolution. The topographic localizations in the current studies are evidence of cortical regions, but no specific locations in the brain can be identified through this method. A more specific analysis to determine the underlying source is needed to answer questions on the origin of neural activity. Because infants showed no evidence of discrimination, the present data cannot answer questions about the neural underpinnings of emotion discrimination in infancy. Through past work, we can infer that in adults the neural origins of these responses are located in the superior temporal sulcus, amygdala, orbitofrontal cortex, and inferior occipital gyrus (for a review on brain mechanisms, see Vuilleumier & Pourtois, 2007). Another limitation of the current studies is that only fearful expressions were used as the oddball stimulus. Thus, the findings are limited to emotion discrimination of fearful from happy, sad, and angry. Although fear has been one of the most widely studied emotions in humans and animals (Davis & Whalen, 2001; Vuilleumier & Pourtois, 2007), there are other basic emotions (Ekman, 1999) such as disgust and surprise that were not examined in the current studies and warrant further investigation using this method. Testing more facial expressions using the current technique can inform us about differential processing for certain emotions. We can also examine the developmental progression of emotion processing, which would provide insight for theoretical perspectives on emotion (e.g., universality of emotions or structural theory of emotions). 78

The photographs used in the current studies originated from a validated stimulus set of directed facial expressions (Lundqvist et al., 1998). The photographs chosen ranged from low to high intensity expressions, which are likely to be seen in daily interactions. Due to rapid change in expression, the stimuli give an illusion of a dynamic expression, making them more realistic than static images, however, they were presented against a plain grey background. Emotion recognition in a real life setting requires the viewer to segregate the face from its natural background and detect subtle changes in the expression at different viewpoints, varying luminance, and at varying distances. Although it may be difficult to source, using natural images where the viewer has to segregate the face and subsequently recognize the expression are better representations of emotion discrimination. Further, these findings can only be generalized to discrimination of expressions when expressed by female models. Research suggests that women and men express emotions differently. In general, women are better at facially expressing most emotions than men (Hall, 1984; Hamann & Canli, 2004) and sex differences are seen in smiling (LaFrance, Hecht, & Paluck, 2003). Similarly, in a meta-analysis on children, Chaplin and Aldao (2013) found that females show more emotion in facial, vocal, and behavioural expressions than males. This work is also limited to emotion discrimination. Moving beyond discrimination, it is unknown whether adults would show an ability to categorize expressions when expressed by different models. Some of our unpublished testing failed to find an oddball discrimination response when adults were asked to categorize across different models.

79

Using Fast Periodic Visual Stimulation to Study Emotion Discrimination As hypothesized, both the infant and adult brain showed evidence of processing high-level visual stimuli at the rate of 6 Hz. The response was strongest at the medialoccipital regions in both infants and adults. Thus, the infant visual cortex is sensitive to the functional organization of facial expressions during rapid presentation and shows a similar topographical pattern as adults. In the current studies, adults and infants completed the same paradigm, which allows for direct comparisons and helps us to understand neural development. Although an oddball discrimination response was not found in the infant sample, further testing is warranted to explore emotion discrimination using this method. In terms of feasibility, the time required to obtain a discrimination response in adults using FPVS is much shorter than in ERP studies, which require a greater amount of repetition in order to detect changes in components. A robust oddball discrimination response was seen in adults after 3 minutes of stimuli presentation. This method also results in few artefacts (such as eye blinks and movement) and produces an objective response that is predefined by the researcher. FPVS is also suitable for infants as the flickering nature of the stimuli is likely foreign and attractive to them, holding their gaze to the monitor. Future Directions Future work is necessary to determine whether infants evidence emotion discrimination using FPVS. Modifications to the current design may help to clarify an oddball discrimination response, such as decreasing the presentation rate, decreasing the length of each sequence, and changes to the testing environment all in hopes of 80

maintaining infant attention. In addition, evaluating discrimination between negative expressions with an older sample of infants warrants further investigation. If we are able to find a discrimination response in infants, investigating what this response translates to with regards to higher-order abilities is of interest. There is currently no research examining how early perceptual discrimination abilities evolve into later emotional development. This gap in research continues to grow larger as researchers focus on one of the two areas, with no one attempting to bridge them. A line of longitudinal research examining infants' early discrimination capacity using FPVS and relating this to meaningful emotion comprehension tasks would provide considerable insight into emotion development. An example of an experimental task is one that assesses the toddler's ability to understand the meaning behind a facial expression and whether they guide their behaviour based on it (e.g., social referencing; Klinnert, 1984). This would provide a solid understanding of the progression of emotional development. Understanding the development of emotion perception in typical infants has important practical implications. Individuals with certain developmental disorders, like Autism Spectrum Disorder (ASD), show deficits in emotion recognition and social communication (Baron-Cohen, 1995). By understanding the course of typical development, we can offer recommendations for interventions that might facilitate social communication in infants at risk for these disorders (e.g., infants with a sibling who has ASD). In addition, the adult study provides findings on emotion discrimination in typical adults (to our knowledge, as we did not screen for any clinical diagnoses). Individuals with anxiety and/or depression show differences in emotion processing abilities (Fox, 2002; Leppanen, Milders, Bell, Terriere, & Hietanen, 2004). Using FPVS to objectively 81

measure the emotion discrimination responses to certain expressions would shed light on implicit processing in the context of clinical disorders. The current studies have both theoretical and practical implications. This research used a novel method to address a theoretical question about the development of emotion recognition in infancy. There is a great deal of evidence demonstrating adults' ability to discriminate facial expressions, however little is known about when exactly it develops, partly because traditional behavioural and ERP paradigms are not ideal for addressing this question. There are only a handful of studies that assess infants' ability to discriminate within-category expressions and all yield different results. Considering the importance of emotion recognition in social environments, the current study is an important addition to the literature. A fruitful line of research will inevitably emerge from the current work in an attempt to characterize this ability using FPVS. Other avenues assessing face perception in infancy, children, and adults are warranted, such as exploration of identity, race, age, and familiarity. Conclusion In sum, FPVS offers a new tool to measure emotion discrimination responses in adults and holds potential for detecting this ability in infants. Consistent with past research, the current study shows the adult brain's ability to rapidly detect brief changes to facial expressions. Although discrimination was not seen in infants, further testing is required to assess whether infants require more time to process the complex patterns of emotion expressed in a face. There are currently only two published studies using FPVS to examine high-level vision in infants, examining the ability to categorize faces and

82

objects. The current study furthers the application of this method to investigating social cues of emotion as expressed through the face.

83

Appendix A Ryerson University Adult Consent Agreement Emotion Categorization in Adults

You are being asked to participate in a research study. Before signing this consent form, it is important that you read and understand the following information. You may ask as many questions as necessary to be sure that you understand what the study entails.

INVESTIGATORS: Dr. Margaret Moulson, Department of Psychology, Ryerson University Alexandra Marquis, Master's Student in Psychology, Ryerson University

PURPOSE OF THE STUDY: The purpose of this perceptual study is to evaluate a technique on adults that can be used on infants in the future, to see if they are able to distinguish different types of negative emotions. Figuring out what other people are feeling is an important part of communicating with them. The main way in which humans communicate feelings is through facial expressions. Behavioural and brain based studies have shown that adults are able to categorize emotions in order for them to process them quickly in social settings. For example, adults are able to group angry faces together and treat these faces differently than happy faces or sad faces, however it has never been examined using steady state visual evoked Potentials (ssVEP). The goal of the current study is to examine this ability by measuring natural brain activity using a special technique called EEG. We are recruiting approximately 20 students currently enrolled in PSY 102 or PSY 202.

WHAT PARTICIPATION MEANS: If you volunteer to participate in this study you will be asked to do the following things. Please note the times are approximations: Read and sign a consent form and short questionnaire [~5 minutes], have a special EEG cap placed on your head with special sensors [~10 minutes], attend to an on-screen stimulus [~30minutes], and read a debrief form [~5 minutes]. The on-screen stimulus will involve the rapid presentation of various emotional faces and objects. The visit will last approximately 60 minutes. All participation is voluntary and doing a `walk-through' of the study is optional. If you feel uncomfortable with any aspect of the study, please notify the researcher and you will be given a simulation of the real experiment with no data collected. 84

POTENTIAL RISKS: You may find the EEG cap slightly uncomfortable (it is damp and snug, like a swim cap), and may experience mild boredom during the experiment. Participants may choose to refuse to participate in any aspect of the research. Any risk or discomfort will be minimal and is not expected to last beyond the day of the study. If any aspect of this study makes you feel uncomfortable, you may temporarily or permanently discontinue your participation without penalty or loss of your course participation credit. In addition, because of the fast presentation of faces and objects during the video we present, there is a risk of seizures from undiagnosed photosensitive epilepsy. Photosensitive epilepsy is extremely rare, occurring in less than 3% of individuals who have epilepsy (Epilepsy Society, 2014) and is typically diagnosed before the first year of life. Although there are many research projects that have used the method of presentation with adults and infants, we have also adjusted the contrast and brightness of the images in order to reduce the likelihood of a seizure. BENEFITS OF THE STUDY: There is no direct benefit to participants in this study although the information gained from the overall study may improve research in adult and infant face perception and emotion recognition. When the session is over, we will describe the purpose and hypotheses of the study to you in more detail.

CONFIDENTIALITY: Any information obtained in connection with this study will remain confidential. All participants in this study will have their information protected using a participant ID number. This participant ID is assigned by SONA and cannot be matched to your student number in any way. Any written notes and data files will be identified only by this participant ID number, will be stored separately from the participant consent forms, and will be accessed only by those individuals directly involved in the study. Paper copies of the data will be stored in locked filing cabinets in the BEE Lab; Electronic data files will be stored on password-protected media in the BEE Lab. The BEE Lab is locked and only accessible to members of the lab. All data will be pooled and published in aggregate form only. There are no limits to the use, disclosure, and retention of this information. Since most journals require data to be stored for five years post-publication, this data will be stored for at least this period of time prior to being destroyed.

INCENTIVE FOR PARTICIPATION: You will be receiving 1 credit for your participation in this study towards your class PSY 102 or 202. 85

VOLUNTARY PARTICIPATION AND WITHDRAWAL: Participation in this study is voluntary. This means that you can choose whether or not you will be in this study. Your choice of whether or not to take part in this study will not influence future relations with Ryerson University. If you decide to participate, you are free to withdraw consent and to stop participation at any time without penalty or loss of benefits. At any particular point in the study, you may refuse to answer any question(s) or stop participation altogether.

Questions: If you have any questions about the research now, please ask. If you have questions later about the research, please contact:

Principal Investigator

Dr. M. Moulson

(416)979-5000 ext2661 mmoulson@psych.ryerson.ca

Student Researcher

Alexandra Marquis

(416)979-5000 ext2189 alexandra.marquis@psych.ryerson.ca

If you have questions regarding your rights as a human subject and participant in this study, you may contact the SONA Ryerson University Research Ethics Board for information:

Ryerson REB chair: Lynn Lavallée, Ph.D., Associate Professor (lavallee@ryerson.ca), Research Ethics Board c/o Office of the Vice President, Research and Innovation, Ryerson University, 350 Victoria Street, Toronto, ON, M5B 2K3, Telephone: (416) 979 5000 x 4791

Agreement: Your signature below indicates that you have read the information in this agreement and have had a chance to ask any questions you have about the study. Your signature also indicates that you agree to be in the study and have been told that you can change your mind any time during the study and withdraw from it. You have been given a copy of this agreement.

86

You have been told that by signing this consent agreement, you are not giving up any of your legal rights.

___________________________ Name of Participant (please print)

___________________________ Signature of Participant

______________________ Date

___________________________ Signature of Investigator

______________________ Date

87

Appendix B Ryerson University - Parental Consent Agreement Examining Emotion Categorization in 5- and 7-month-old infants using Steady State Visual Evoked Potentials (ssVEP) Your child is being asked to participate in a research study. Before you give your consent for your child to be a volunteer, it is important that you read the following information and ask as many questions as necessary to be sure you understand what your child will be asked to do. Investigators: Dr. Margaret Moulson, PhD, Department of Psychology, Ryerson University Alexandra Marquis, BA, Master's student in Psychology, Ryerson University.

Purpose of the Study: Figuring out what other people are feeling is an important part of communicating with them. The main way in which humans communicate feelings is through facial expressions. Studies have shown that adults are able to categorize emotions in order for them to process them quickly in social settings. For example, adults are able to group angry faces together and treat these faces differently than happy faces or sad faces. We know that infants can group positive emotions separately from negative emotions, but we don't know if they can put negative emotions into different groups (for example, telling the difference between angry faces and sad faces). The goal of the current study is to examine this ability by measuring natural brain activity using a special technique called EEG. We are recruiting a total of 60 infants between the ages of 5- and 7-months for this study. Infants who have vision impairments or corrective eyewear and infants who have a history of seizure(s) or seizure-like symptoms will not be eligible to participate in the current study. The results from the current study will contribute to the completion of the student researcher's Master's thesis project.

Description of the Study: To participate in this study, you and your child will be asked to visit Ryerson University one time for approximately an hour and a half. You will remain with your child at all times throughout the study. If you decide to participate in the current study, the following will occur: While your child sits on your lap, a researcher will place a special sensor cap on your child's head. This cap contains tiny sensors that detect the brain signals that your baby is naturally producing. While wearing the cap and sitting on your lap, your baby will watch a computer screen that will display adult female faces expressing different emotions (e.g., sad, happy, angry, surprised) at a very fast rate (6 faces per second). Your baby will watch several video clips showing these faces while the sensor cap collects information on how his/her brain processes the faces. 88

You will also be asked to complete two questionnaires. The first questionnaire is titled the Infant Behavior Questionnaire (IBQ), which measures infant's temperament and includes questions like, "How often does your baby enjoy being sung to?" and "How often does your baby notice a bird or a squirrel up in a tree?" The second questionnaire is titled Self-Expressiveness in the Family Questionnaire (SEFQ) which aims to measure how expressive the family is and features questions like, "How often do you praise someone for good work" and "How often do you exclaim over a beautiful day?" Approximate Breakdown of Timing: Consent and Questionnaire completion: 1 hour EEG session: ½ hour POTENTIAL BENEFITS: While there is no direct benefit to you or your child, this study has the potential to tell us more about how emotion recognition develops. This is very important because it helps us understand how people communicate and interact with each other. WHAT ARE THE POTENTIAL RISKS TO YOU/YOUR CHILD AS A PARTICIPANT? Because of the fast presentation of objects during the video we present, there is a risk of seizures from undiagnosed photosensitive epilepsy. Photosensitive epilepsy is extremely rare, occurring in less than 3% of individuals who have epilepsy (Epilepsy Society, 2014) and is typically diagnosed before the first year of life. Although the faces we will show your baby have been presented to infants like yours in other research projects, it is possible that your infant may become distressed or upset during the presentation of angry or fearful faces. It is possible that your child may find the sensor cap uncomfortable or he/she may become bored while watching the video clips. To manage these risks, if you feel like your child is too bored or uncomfortable to keep going, you can decide to take a break or stop participating. CONFIDENTIALITY: Any information obtained in connection with this study will remain confidential. All participants in this study will have their information protected using a participant ID number. Any written notes and data files will be identified only by this participant ID number, will be stored separately from the participant consent forms, and will be accessed only by those individuals directly involved in the study. Paper copies of the data will be stored in locked filing cabinets in the BEE Lab; Electronic data files will be stored on password-protected media in the BEE Lab. The BEE Lab is locked and only accessible to members of the lab. All data will be pooled and published in aggregate form only. Since most journals require data to be stored for five years post-publication, this data will be stored for at least this period of time prior to being destroyed. INCENTIVES FOR PARTICIPATION: For participating in this study, your child will receive a small gift (e.g., a toy) at the end of the study session. If you decide not to participate in this study or discontinue participation during the study, your child will still receive the gift as a token of our appreciation for your coming to the lab. In addition, you will receive $10 for your participation, whether or not your child completes the study. 89

VOLUNTARY PARTICIPATION AND WITHDRAWAL: Participation in this study is voluntary. This means that you can choose whether or not you and your child will be in this study. Your choice of whether or not to take part in this study will not influence future relations with Ryerson University. If you decide to participate, you are free to withdraw consent and to stop participation at any time without penalty or loss of benefits. In addition, your infant's success or interest in the task does not imply anything negative or positive about your child's development. At any particular point in the study, you may refuse to answer any question(s) or stop participation altogether.

Questions: If you have any questions about the research now, please ask. If you have questions later about the research, please contact:

Principal Investigator

Dr. M. Moulson

(416)979-5000 ext2661 mmoulson@psych.ryerson.ca

Student Researcher

Alexandra Marquis

(416)979-5000 ext2189 alexandra.marquis@psych.ryerson.ca

If you have questions regarding your rights as a human subject and participant in this study, you may contact the Ryerson University Research Ethics Board for information: Research Ethics Board c/o Office of the Vice President, Research and Innovation Ryerson University 350 Victoria Street, Toronto, ON M5B 2K3 Email: REBChair@Ryerson.ca Tel: 416-979-5042 Agreement: Your signature below indicates that you have read the information in this agreement and have had a chance to ask any questions you have about the study. Your signature also indicates that you are consenting for your child to be in the study and have been told that you can change your mind and withdraw your consent to participate at any time. You have been given a copy of this agreement. You have been told that by signing this consent agreement you are not giving up any of your legal rights.

Name of Parent/Guardian of Participant (please print)

Name of Child (please print)

90

Signature of Parent/Guardian

Date

Signature of Investigator

Date

91

Consent to Video Tape

Your signature below indicates that you agree to have your infant videotaped during the study session. Videotaping during the study session can be stopped at anytime.

Name of Parent/Guardian of Participant (please print)

Name of Child (please print)

Signature of Parent/Guardian

Date

Signature of Investigator

Date

92

Appendix C Demographic Information Questionnaire Please fill out the following demographic information: Infant's birthdate (D/M/Y) ____________________ Infant's due date (D/M/Y) _____________________ Infant's race/ethnicity (please check all applicable):  Aboriginal/Metis/Inuit  Black  East Asian (e.g., Chinese, Korean, Japanese, Filipino, etc.)  Latin American  South Asian (e.g., East Indian, Pakistani, Sri Lankan, etc.)  Southeast Asian (e.g., Vietnamese, Cambodian, Malaysian, Laotian, etc.)  West Asian (e.g., Iranian, Afghan, etc.)  White  Other ­ Specify: ___________________

93

Appendix D Motor Development Questionnaire Can your child crawl?  Yes  No

If yes, please provide the exact start date: ______________ Can they move forward more than three crawling steps?  Yes  No

How does your child crawl? (circle all that apply)  Belly crawling-army crawl  Hands and knees  Hands and feet (Bear crawl) Other:______________________________________________________

94

Appendix E Participant #:______ Date(Y/M/D): _____________ SELF-EXPRESSIVENESS IN THE FAMILY QUESTIONNAIRE This is a questionnaire about the degree of expressiveness people show in their families. To answer the questionnaire, try to think of how frequently you express yourself during each of the following situations with family members. Using the scale shown below, write in the number that best indicates how frequently you express yourself in that situation when it occurs. Thus, if you never or rarely express those feelings, write down a 1, 2, or 3 in the space beside the statement. If you express those feelings with some or moderate frequency, write down a 4, 5, or 6. And if you express those feelings very frequently, write down a 7, 8 or 9. Some items may be difficult to judge. However, it is important to answer every item. Try to respond quickly and honestly about yourself. There are no right or wrong answers, and we don't believe that any answer is better than another. +-------------+-------------+------------+------------+-------------+-------------+------------+-------------+ 1 not at all frequently 2 3 4 5 6 7 8 9 very frequently somewhat frequently

Showing forgiveness to someone who broke a favorite possession. ______ Thanking family members for something they have done. ______ Exclaiming over a beautiful day. ______ Showing contempt for another's actions. ______ Expressing dissatisfaction with someone else's behavior. ______ Praising someone for good work. ______ Expressing anger at someone else's carelessness. ______ Sulking over unfair treatment by a family member. ______ Blaming one another for family troubles. ______ Crying after an unpleasant disagreement. ______ Putting down other people's interests. ______ Showing dislike for someone. ______ Seeking approval for an action. ______ Expressing embarrassment over stupid mistakes. ______ Going to pieces when tension builds up. ______ Expressing exhilaration after an unexpected triumph. ______ Expressing excitement over one's future plans. ______ Demonstrating admiration. ______
(OVER)

95

Please rate how frequently you express yourself in these situations with your family. Use the scale shown below, and write in the number that fits best for you beside the item. +-------------+-------------+------------+------------+-------------+-------------+------------+-------------+ 1 not at all frequently 2 3 4 5 6 7 8 9 very frequently somewhat frequently

Expressing sorrow when a pet dies. ______ Expressing disappointment over something that didn't work out. ______ Telling someone how nice they look. ______ Expressing sympathy for someone's troubles. ______ Expressing deep affection or love for someone. ______ Quarreling with a family member. ______ Crying when a loved one goes away. ______ Spontaneously hugging a family member. ______ Expressing momentary anger over a trivial irritation. ______ Expressing concern for the success of other family members. ______ Apologizing for being late. ______ Offering to do somebody a favor. ______ Snuggling up to a family member. ______ Showing how upset you are after a bad day. ______ Trying to cheer up someone who is sad. ______ Telling family members how hurt you are. ______ Telling family members how happy you are. ______ Threatening someone. ______ Criticizing someone for being late. ______ Expressing gratitude for a favor. ______ Surprising someone with a little gift or favor. ______ Saying "I'm sorry" when one realizes one was wrong. ______

Thank you for your time!

96

Appendix F The authors have provided permission to reprint the IBQ-R. © 2000 Mary K. Rothbart Appendix F

Maria A. Gartstein All Rights Reserved

Infant Behavior Questionnaire - Revised Date of Baby's Birth ______ ____ month. day Today's Date _______________ Age of Child year

Subject No.

_______________

_____ _____ mos. weeks

Sex of Child _______________

INSTRUCTIONS: Please read carefully before starting: As you read each description of the baby's behavior below, please indicate how often the baby did this during the LAST WEEK (the past seven days) by circling one of the numbers in the left column. These numbers indicate how often you observed the behavior described during the last week. (2) (1) Never Very Rarely (3) Less Than Half the Time (4) About Half the Time (5) More Than Half the Time (6) Almost Always (7) Always (X) Does Not Apply

The "Does Not Apply" (X) column is used when you did not see the baby in the situation described during the last week. For example, if the situation mentions the baby having to wait for food or liquids and there was no time during the last week when the baby had to wait, circle 97

the (X) column. "Does Not Apply" is different from "Never" (1). "Never" is used when you saw the baby in the situation but the baby never engaged in the behavior listed during the last week. For example, if the baby did have to wait for food or liquids at least once but never cried loudly while waiting, circle the (1) column. Please be sure to circle a number for every item.

Feeding During feeding, how often did the baby: 1 2 3 4 5 6 7 X . . . . (1) lie or sit quietly? 1 2 3 4 5 6 7 X . . . . (2) squirm or kick? 1 2 3 4 5 6 7 X . . . . (3) wave arms? 1 2 3 4 5 6 7 X . . . . (4) notice lumpy texture in food (e.g., oatmeal)? In the last week, while being fed in your lap, how often did the baby: 1 2 3 4 5 6 7 X . . . . (5) seem to enjoy the closeness? 1 2 3 4 5 6 7 X . . . . (6) snuggle even after she was done? 1 2 3 4 5 6 7 X . . . . (7) seem eager to get away as soon as the feeding was over? How often did your baby make talking sounds: 1 2 3 4 5 6 7 X . . . . (8) while waiting in a high chair for food? 1 2 3 4 5 6 7 X . . . . (9) when s/he was ready for more food? 1 2 3 4 5 6 7 X . . . . (10) when s/he has had enough to eat?

Sleeping Before falling asleep at night during the last week, how often did the baby: 1 2 3 4 5 6 7 X . . . . (11) show no fussing or crying? During sleep, how often did the baby: 1 2 3 4 5 6 7 X . . . . (12) toss about in the crib? 1 2 3 4 5 6 7 X . . . . (13) move from the middle to the end of the crib? 1 2 3 4 5 6 7 X . . . . (14) sleep in one position only? After sleeping, how often did the baby: 1 2 3 4 5 6 7 X . . . . (15) fuss or cry immediately? 1 2 3 4 5 6 7 X . . . . (16) play quietly in the crib? 1 2 3 4 5 6 7 X . . . . (17) cry if someone doesn't come within a few minutes? How often did the baby: 1 2 3 4 5 6 7 X . . . . (18) seem angry (crying and fussing) when you left her/him in the crib? 1 2 3 4 5 6 7 X . . . . (19) seem contented when left in the crib? 98

1 2 3 4 5 6 7 X . . . . (20) cry or fuss before going to sleep for naps? When going to sleep at night, how often did your baby: 1 2 3 4 5 6 7 X . . . . (21) fall asleep within 10 minutes? 1 2 3 4 5 6 7 X . . . . (22) have a hard time settling down to sleep? 1 2 3 4 5 6 7 X . . . . (23) settle down to sleep easily?

When your baby awoke at night, how often did s/he: 1 2 3 4 5 6 7 X . . . . (24) have a hard time going back to sleep? 1 2 3 4 5 6 7 X . . . . (25) go back to sleep immediately? When put down for a nap, how often did your baby: 1 2 3 4 5 6 7 X . . . . (26) stay awake for a long time? 1 2 3 4 5 6 7 X . . . . (27) go to sleep immediately? 1 2 3 4 5 6 7 X . . . . (28) settle down quickly? 1 2 3 4 5 6 7 X . . . . (29) have a hard time settling down? When it was time for bed or a nap and your baby did not want to go, how often did s/he: 1 2 3 4 5 6 7 X . . . . (30) whimper or sob? 1 2 3 4 5 6 7 X . . . . (31) become tearful?

Bathing and Dressing When being dressed or undressed during the last week, how often did the baby: 1 2 3 4 5 6 7 X . . . . (32) wave her/his arms and kick? 1 2 3 4 5 6 7 X . . . . (33) squirm and/or try to roll away? 1 2 3 4 5 6 7 X . . . . (34) smile or laugh? 1 2 3 4 5 6 7 X . . . . (35) coo or vocalize? When put into the bath water, how often did the baby: 1 2 3 4 5 6 7 X . . . . (36) smile? 1 2 3 4 5 6 7 X . . . . (37) laugh? 1 2 3 4 5 6 7 X . . . . (38) splash or kick? 1 2 3 4 5 6 7 X . . . . (39) turn body and/or squirm? When face was washed, how often did the baby: 1 2 3 4 5 6 7 X . . . . (40) smile or laugh? 1 2 3 4 5 6 7 X . . . . (41) fuss or cry? 1 2 3 4 5 6 7 X . . . . (42) coo? When hair was washed, how often did the baby: 1 2 3 4 5 6 7 X . . . . (43) smile? 1 2 3 4 5 6 7 X . . . . (44) fuss or cry? 1 2 3 4 5 6 7 X . . . . (45) vocalize? 99

Play How often during the last week did the baby: 1 2 3 4 5 6 7 X . . . . (46) look at pictures in books and/or magazines for 2-5 minutes at a time? 1 2 3 4 5 6 7 X . . . . (47) look at pictures in books and/or magazines for 5 minutes or longer at a time? 1 2 3 4 5 6 7 X . . . . (48) stare at a mobile, crib bumper or picture for 5 minutes or longer? 1 2 3 4 5 6 7 X . . . . (49) play with one toy or object for 5-10 minutes? 1 2 3 4 5 6 7 X . . . . (50) play with one toy or object for 10 minutes or longer? 1 2 3 4 5 6 7 X . . . . (51) spend time just looking at playthings? 1 2 3 4 5 6 7 X . . . . (52) repeat the same sounds over and over again? 1 2 3 4 5 6 7 X . . . . (53) laugh aloud in play? 1 2 3 4 5 6 7 X . . . . (54) repeat the same movement with an object for 2 minutes or longer (e.g., putting a block in a cup, kicking or hitting a mobile)? 1 2 3 4 5 6 7 X . . . . (55) pay attention to your reading during most of the story when looking at picture books? 1 2 3 4 5 6 7 X . . . . (56) smile or laugh after accomplishing something (e.g., stacking blocks, etc.)? 1 2 3 4 5 6 7 X . . . . (57) smile or laugh when given a toy? 1 2 3 4 5 6 7 X . . . . (58) smile or laugh when tickled? How often during the last week did the baby enjoy: 1 2 3 4 5 6 7 X . . . . (59) being sung to? 1 2 3 4 5 6 7 X . . . . (60) being read to? 1 2 3 4 5 6 7 X . . . . (61) hearing the sound of words, as in nursery rhymes? 1 2 3 4 5 6 7 X . . . . (62) looking at picture books? 1 2 3 4 5 6 7 X . . . . (63) gentle rhythmic activities, such as rocking or swaying? 1 2 3 4 5 6 7 X . . . . (64) lying quietly and examining his/her fingers or toes? 1 2 3 4 5 6 7 X . . . . (65) being tickled by you or someone else in your family? 1 2 3 4 5 6 7 X . . . . (66) being involved in rambunctious play? 1 2 3 4 5 6 7 X . . . . (67) watching while you, or another adult, playfully made faces? 1 2 3 4 5 6 7 X . . . . (68) touching or lying next to stuffed animals? 1 2 3 4 5 6 7 X . . . . (69) the feel of soft blankets ? 1 2 3 4 5 6 7 X . . . . (70) being rolled up in a warm blanket? 1 2 3 4 5 6 7 X . . . . (71) listening to a musical toy in a crib?

100

When playing quietly with one of her/his favorite toys, how often did your baby: 1 2 3 4 5 6 7 X . . . . (72) show pleasure? 1 2 3 4 5 6 7 X . . . . (73) enjoy lying in the crib for more than 5 minutes? 1 2 3 4 5 6 7 X . . . . (74) enjoy lying in the crib for more than 10 minutes? When something the baby was playing with had to be removed, how often did s/he: 1 2 3 4 5 6 7 X . . . . (75) cry or show distress for a time? 1 2 3 4 5 6 7 X . . . . (76) seem not bothered? When tossed around playfully how often did the baby: 1 2 3 4 5 6 7 X . . . . (77) smile? 1 2 3 4 5 6 7 X . . . . (78) laugh? During a peekaboo game, how often did the baby: 1 2 3 4 5 6 7 X . . . . (79) smile? 1 2 3 4 5 6 7 X . . . . (80) laugh? How often did your baby enjoy bouncing up and down: 1 2 3 4 5 6 7 X . . . . (81) while on your lap? 1 2 3 4 5 6 7 X . . . . (82) on an object, such as a bed, bouncer chair, or toy? How often did the infant look up from playing: 1 2 3 4 5 6 7 X . . . . (83) when the telephone rang? 1 2 3 4 5 6 7 X . . . . (84) when s/he heard voices in the next room? When your baby saw a toy s/he wanted, how often did s/he: 1 2 3 4 5 6 7 X . . . . (85) get very excited about getting it? 1 2 3 4 5 6 7 X . . . . (86) immediately go after it? When given a new toy, how often did your baby: 1 2 3 4 5 6 7 X . . . . (87) get very excited about getting it? 1 2 3 4 5 6 7 X . . . . (88) immediately go after it? 1 2 3 4 5 6 7 X . . . . (89) seem not to get very excited about it? Daily Activities How often during the last week did the baby: 1 2 3 4 5 6 7 X . . . . (90) cry or show distress at a change in parents' appearance, (glasses off, shower cap on, etc.)? 1 2 3 4 5 6 7 X . . . . (91) when in a position to see the television set, look at it for 2 to 5 minutes at a time?

101

How often during the last week did the baby: 1 2 3 4 5 6 7 X . . . . (92) when in a position to see the television set, look at it for 5 minutes or longer? 1 2 3 4 5 6 7 X . . . . (93) protest being placed in a confining place (infant seat, play pen, car seat, etc)? 1 2 3 4 5 6 7 X . . . . (94) startle at a sudden change in body position (for example, when moved suddenly)? 1 2 3 4 5 6 7 X . . . . (95) appear to listen to even very quiet sounds? 1 2 3 4 5 6 7 X . . . . (96) attend to sights or sounds when outdoors (for example, wind chimes or water sprinklers)? 1 2 3 4 5 6 7 X . . . . (97) move quickly toward new objects? 1 2 3 4 5 6 7 X . . . . (98) show a strong desire for something s/he wanted? 1 2 3 4 5 6 7 X . . . . (99) startle to a loud or sudden noise? 1 2 3 4 5 6 7 X . . . . (100) look at children playing in the park or on the playground for 5 minutes or longer? 1 2 3 4 5 6 7 X . . . . (101) watch adults performing household activities (e.g., cooking, etc.) for more than 5 minutes? 1 2 3 4 5 6 7 X . . . . (102) squeal or shout when excited? 1 2 3 4 5 6 7 X . . . . (103) imitate the sounds you made? 1 2 3 4 5 6 7 X . . . . (104) seem excited when you or other adults acted in an excited manner around him/her? When being held, how often did the baby: 1 2 3 4 5 6 7 X . . . . (105) pull away or kick? 1 2 3 4 5 6 7 X . . . . (106) seem to enjoy him/herself? 1 2 3 4 5 6 7 X . . . . (107) mold to your body? 1 2 3 4 5 6 7 X . . . . (108) squirm? When placed on his/her back, how often did the baby: 1 2 3 4 5 6 7 X . . . . (109) fuss or protest? 1 2 3 4 5 6 7 X . . . . (110) smile or laugh? 1 2 3 4 5 6 7 X . . . . (111) wave arms and kick? 1 2 3 4 5 6 7 X . . . . (112) squirm and/or turn body? When the baby wanted something, how often did s/he: 1 2 3 4 5 6 7 X . . . . (113) become upset when s/he could not get what s/he wanted? 1 2 3 4 5 6 7 X . . . . (114) have tantrums (crying, screaming, face red, etc.) when s/he did not get what s/he wanted? When placed in an infant seat or car seat, how often did the baby: 1 2 3 4 5 6 7 X . . . . (115) wave arms and kick? 1 2 3 4 5 6 7 X . . . . (116) squirm and turn body? 1 2 3 4 5 6 7 X . . . . (117) lie or sit quietly? 1 2 3 4 5 6 7 X . . . . (118) show distress at first; then quiet down?

102

When frustrated with something, how often did your baby: 1 2 3 4 5 6 7 X . . . . (119) calm down within 5 minutes? When your baby was upset about something, how often did s/he: 1 2 3 4 5 6 7 X . . . . (120) stay upset for up to 10 minutes or longer? 1 2 3 4 5 6 7 X . . . . (121) stay upset for up to 20 minutes or longer? 1 2 3 4 5 6 7 X . . . . (122) soothe her/himself with other things (such as a stuffed animal, or blanket)? When rocked or hugged, in the last week, how often did your baby: 1 2 3 4 5 6 7 X . . . . (123) seem to enjoy her/himself? 1 2 3 4 5 6 7 X . . . . (124) seemed eager to get away? 1 2 3 4 5 6 7 X . . . . (125) make protesting noises? When reuniting after having been away during the last week how often did the baby: 1 2 3 4 5 6 7 X . . . . (126) seem to enjoy being held? 1 2 3 4 5 6 7 X . . . . (127) show interest in being close, but resisted being held? 1 2 3 4 5 6 7 X . . . . (128) show distress at being held? When being carried, in the last week, how often did your baby: 1 2 3 4 5 6 7 X . . . . (129) seem to enjoy him/herself? 1 2 3 4 5 6 7 X . . . . (130) push against you until put down? While sitting in your lap: 1 2 3 4 5 6 7 X . . . . (131) how often did your baby seem to enjoy her/himself? 1 2 3 4 5 6 7 X . . . . (132) how often would the baby not be content without moving around? How often did your baby notice: 1 2 3 4 5 6 7 X . . . . (133) low-pitched noises, air conditioner, heating system, or refrigerator running or starting up? 1 2 3 4 5 6 7 X . . . . (134) sirens from fire trucks or ambulances at a distance? 1 2 3 4 5 6 7 X . . . . (135) a change in room temperature? 1 2 3 4 5 6 7 X . . . . (136) a change in light when a cloud passed over the sun? 1 2 3 4 5 6 7 X . . . . (137) sound of an airplane passing overhead? 1 2 3 4 5 6 7 X . . . . (138) a bird or a squirrel up in a tree? 1 2 3 4 5 6 7 X . . . . (139) fabrics with scratchy texture (e.g., wool)? When tired, how often was your baby: 1 2 3 4 5 6 7 X . . . . (140) likely to cry? 1 2 3 4 5 6 7 X . . . . (141) show distress?

103

At the end of an exciting day, how often did your baby: 1 2 3 4 5 6 7 X . . . . (142) become tearful? 1 2 3 4 5 6 7 X . . . . (143) show distress? For no apparent reason, how often did your baby: 1 2 3 4 5 6 7 X . . . . (144) appear sad? 1 2 3 4 5 6 7 X . . . . (145) seem unresponsive? How often did your baby make talking sounds when: 1 2 3 4 5 6 7 X . . . . (146) riding in a car? 1 2 3 4 5 6 7 X . . . . (147) riding in a shopping cart? 1 2 3 4 5 6 7 X . . . . (148) you talked to her/him? Two Week Time Span When you returned from having been away and the baby was awake, how often did s/he: 1 2 3 4 5 6 7 X . . . . (149) smile or laugh? When introduced to an unfamiliar adult, how often did the baby: 1 2 3 4 5 6 7 X . . . . (150) cling to a parent? 1 2 3 4 5 6 7 X . . . . (151) refuse to go to the unfamiliar person? 1 2 3 4 5 6 7 X . . . . (152) hang back from the adult? 1 2 3 4 5 6 7 X . . . . (153) never "warm up" to the unfamiliar adult? When in the presence of several unfamiliar adults, how often did the baby: 1 2 3 4 5 6 7 X . . . . (154) cling to a parent? 1 2 3 4 5 6 7 X . . . . (155) cry? 1 2 3 4 5 6 7 X . . . . (156) continue to be upset for 10 minutes or longer? When visiting a new place, how often did the baby: 1 2 3 4 5 6 7 X . . . . (157) show distress for the first few minutes? 1 2 3 4 5 6 7 X . . . . (158) continue to be upset for 10 minutes or more? 1 2 3 4 5 6 7 X . . . . (159) get excited about exploring new surroundings? 1 2 3 4 5 6 7 X . . . . (160) move about actively when s/he is exploring new surroundings? When your baby was approached by an unfamiliar person when you and s/he were out (for example, shopping), how often did the baby: 1 2 3 4 5 6 7 X . . . . (161) show distress? 1 2 3 4 5 6 7 X . . . . (162) cry?

104

When an unfamiliar adult came to your home or apartment, how often did your baby: 1 2 3 4 5 6 7 X . . . . (163) allow her/himself to be picked up without protest? 1 2 3 4 5 6 7 X . . . . (164) cry when the visitor attempted to pick her/him up? When in a crowd of people, how often did the baby: 1 2 3 4 5 6 7 X . . . . (165) seem to enjoy him/herself? Did the baby seem sad when: 1 2 3 4 5 6 7 X . . . . (166) caregiver is gone for an unusually long period of time? 1 2 3 4 5 6 7 X . . . . (167) left alone/unattended in a crib or a playpen for an extended period of time? When you were busy with another activity, and your baby was not able to get your attention, how often did s/he: 1 2 3 4 5 6 7 X . . . . (168) become sad? 1 2 3 4 5 6 7 X . . . . (169) cry? When your baby saw another baby crying, how often did s/he: 1 2 3 4 5 6 7 X . . . . (170) become tearful? 1 2 3 4 5 6 7 X . . . . (171) show distress? When familiar relatives/friends came to visit, how often did your baby: 1 2 3 4 5 6 7 X . . . . (172) get excited? 1 2 3 4 5 6 7 X . . . . (173) seem indifferent?

Soothing Techniques Have you tried any of the following soothing techniques in the last two weeks? If so, how quickly did your baby soothe using each of these techniques? Circle (X) if you did not try the technique during the LAST TWO WEEKS. When rocking your baby, how often did s/he: 1 2 3 4 5 6 7 X . . . . (174) soothe immediately? 1 2 3 4 5 6 7 X . . . . (175) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (176) take more than 10 minutes to soothe? When singing or talking to your baby, how often did s/he: 1 2 3 4 5 6 7 X . . . . (177) soothe immediately? 1 2 3 4 5 6 7 X . . . . (178) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (179) take more than 10 minutes to soothe?

105

When walking with the baby, how often did s/he: 1 2 3 4 5 6 7 X . . . . (180) soothe immediately? 1 2 3 4 5 6 7 X . . . . (181) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (182) take more than 10 minutes to soothe? When giving him/her a toy, how often did the baby: 1 2 3 4 5 6 7 X . . . . (183) soothe immediately? 1 2 3 4 5 6 7 X . . . . (184) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (185) take more than 10 minutes to soothe? When showing the baby something to look at, how often did s/he: 1 2 3 4 5 6 7 X . . . . (186) soothe immediately? 1 2 3 4 5 6 7 X . . . . (187) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (188) take more than 10 minutes to soothe? When patting or gently rubbing some part of the baby's body, how often did s/he: 1 2 3 4 5 6 7 X . . . . (189) soothe immediately? 1 2 3 4 5 6 7 X . . . . (190) not soothe immediately, but in the first two minutes? 1 2 3 4 5 6 7 X . . . . (191) take more than 10 minutes to soothe?

106

References Adolphs, R. (2002). Recognizing emotion from facial expressions: Psychological and neurological mechanisms. Behavioral and Cognitive Neuroscience Reviews, 1(1), 21­62. http://doi.org/10.1177/1534582302001001003 Baron-Cohen, S. (1995). Mindblindness: An Essay on Autism and Theory of Mind. Cambridge, MA: MIT Press. Barrera, M. E., & Maurer, D. (1981). The Perception of Facial Expressions by the Three-MonthOld. Child Development, 52(1), 203­206. http://www.jstor.org/stable/1129231 Batty, M., & Taylor, M. J. (2003). Early processing of the six basic facial emotional expressions. Cognitive Brain Research, 17(3), 613­620. http://doi.org/10.1016/S0926-6410(03)001745 Bentin, S., Allison, T., Puce, A., Perez, E., & McCarthy, G. (1996). Electrophysiological Studies of Face Perception in Humans. Journal of Cognitive Neuroscience, 8(6), 551­565. http://doi.org/10.1162/jocn.1996.8.6.551 Boas, D. A., Elwell, C. E., Ferrari, M., & Taga, G. (2014). Twenty years of functional nearinfrared spectroscopy: Introduction for the special issue. NeuroImage, 85, 1­5. http://doi.org/10.1016/j.neuroimage.2013.11.033 Bombari, D., Schmid, P. C., Schmid Mast, M., Birri, S., Mast, F. W., & Lobmaier, J. S. (2013). Emotion recognition: the role of featural and configural face information. Quarterly Journal of Experimental Psychology, 66(12), 2426­2442. http://doi.org/10.1080/17470218.2013.789065

107

Boremanse, A., Norcia, A. M., & Rossion, B. (2013). An objective signature for visual binding of face parts in the human brain, Journal of Visualized Experiments, 13, 1­18. http://doi.org/10.1167/13.11.6.doi Bornstein, M. H., & Arterberry, M. E. (2003). Recognition, discrimination, and categorization of smiling by 5-month-old infants. Developmental Science, 6(5), 585­599. Bornstein, M. H., & Korda, N. O. (1984). Discrimination and matching within and between hues measured by reaction times: some implications for categorical perception and levels of information processing. Psychological Research, 46(3), 207­222. http://doi.org/10.1007/BF00308884 Bradley, M. M., Lang, P. J., & Cuthbert, B. N. (1993). Emotion, novelty, and the startle reflex: habituation in humans. Behavioral Neuroscience, 107(6), 970­980. http://doi.org/10.1037/0735-7044.107.6.970 Breiter, H. C., Etcoff, N. L., Whalen, P. J., Kennedy, W. A., Rauch, S. L., Buckner, R. L., ... Rosen, B. R. (1996). Response and habituation of the human amygdala during visual processing of facial expression. Neuron, 17(5), 875­887. http://doi.org/10.1016/S08966273(00)80219-6 Caharel, S., Courtay, N., Bernard, C., Lalonde, R., & Rebaï, M. (2005). Familiarity and emotional expression influence an early stage of face processing: an electrophysiological study. Brain and Cognition, 59(1), 96­100. http://doi.org/10.1016/j.bandc.2005.05.005 Calder, A. J., Keane, J., Manly, T., Sprengelmeyer, R., Scott, S., Nimmo-Smith, I., & Young, A. W. (2003). Facial expression recognition across the adult life span. Neuropsychologia, 41(2), 195­202. http://doi.org/10.1016/S0028-3932(02)00149-5

108

Calder, A. J., Young, A. W., Perrett, D. I., Etcoff, N. L., & Rowland, D. (1996). Categorical perception of morphed facial expressions. Visual Cognition, 3(2), 81­117. http://doi.org/10.1080/713756735 Campanella, S., Quinet, P., Bruyer, R., Crommelinck, M., & Guerit, J.M. (2002). Categorical perception of happiness and fear facial expressions: an ERP study. Journal of Cognitive Neuroscience, 14(2), 210­227. http://doi.org/10.1162/089892902317236858 Campos, J. J., Anderson, D. I., Barbu-Roth, M., Hubbard, E. M., Hertenstein, M. J., & Witherington, D. (2000). Travel Broadens the Mind. Infancy, 1(2), 149­219. http://doi.org/10.1207/S15327078IN0102 Camras, L. A., Oster, H., Campos, J. J., Miyake, K., & Bradshaw, D. (1992). Japanese and American Infants' Responses to Arm Restraint. Developmental Psychology, 28(4), 578­ 583. http://doi.org/10.1093/acprof:oso/9780195179644.003.0014 Canli, T., Sivers, H., Whitfield, S. L., Gotlib, I. H., & Gabrieli, J. D. E. (2002). Amygdala Response to Happy Faces as a Function of Extraversion. Science, 296(5576), 2191­2191. http://doi.org/10.1126/science.1068749 Caron, A.J, Caron, R. F., & Maclean, D. J. (1988). Infant Discrimination of Naturalistic Emotional Expressions: The Role of Face and Voice. Child Development, 59(3), 604­ 616. Caron, R. F., Caron, A. J., & Myers, R. S. (1985). Do Infants See Emotional Expressions in Static Faces? Child Development, 56(6), 1552­1560. Chaplin, T. M., & Aldao, A. (2013). Gender differences in emotion expression in children: A meta-analytic review. Psychological Bulletin, 139(4), 735­765. http://doi.org/10.1037/a0030737 109

Davis, M., & Whalen, P. J. (2001). The amygdala: vigilance and emotion. Molecular Psychiatry, 6(1), 13­34. http://doi.org/10.1038/sj.mp.4000812 de Haan, M., Belsky, J., Reid, V., Volein, A., & Johnson, M. H. (2004). Maternal personality and infants' neural and visual responsivity to facial expressions of emotion. Journal of Child Psychology and Psychiatry and Allied Disciplines, 45(7), 1209­1218. http://doi.org/10.1111/j.1469-7610.2004.00320.x de Haan, M., Johnson, M. H., & Halit, H. (2003). Development of face-sensitive event-related potentials during infancy: A review. International Journal of Psychophysiology, 51(1), 45­58. http://doi.org/10.1016/S0167-8760(03)00152-1 de Heering, A., & Rossion, B. (2015). Rapid categorization of natural face images in the infant right hemisphere. eLife, 4, e06564. http://doi.org/10.7554/eLife.06564 Dzhelyova, M., Jacques, C., & Rossion, B. (under review). At a single glance: fast periodic stimuliation uncovers the spatio-temporal dynamics of brief facial expression changes in the human brain. Cerebral Cortex. Eimer, M., & Holmes, A. (2002). An ERP study on the time course of emotional face processing. Neuroreport, 13(4), 427­431. Eimer, M., Holmes, A., & McGlone, F. P. (2003). The role of spatial attention in the processing of facial expression: an ERP study of rapid brain responses to six basic emotions. Cognitive, Affective & Behavioral Neuroscience, 3(2), 97­110. http://doi.org/10.3758/CABN.3.2.97 Ekman, P. (1992). An argument for basic emotions. Cognition & Emotion, 6(3/4), 169-200. http://doi.org/10.1080/02699939208411068

110

Ekman, P. (1999). Basic Emotions. In T. Dalgleish & M. Power (Eds.), Handbook of Cognition and Emotion (pp. 45­60). San Francisco, John Wiley and Sons Ltd. Ekman, P., & Friesen, W. V. (1976). Pictures of facial affect. [Dataset of photographs]. Palo Alto, CA: Consulting Psychologists Press. Ekman, P., & Friesen, W. V. (1978). Manual for the Facial action coding system. Palo Alto, California: Consulting Psychologists Press. Elfenbein, H. A., & Ambady, N. (2002). On the universality and cultural specificity of emotion recognition: A meta-analysis. Psychological Bulletin, 128(2), 203­235. http://doi.org/10.1037/0033-2909.128.2.203 Etcoff, N. L., & Magee, J. J. (1992). Categorical perception of facial expressions. Cognition, 44, 227­240. Fantz, R. (1958). Pattern Vision in Young Infants. The Psychological Record, 8, 43-47. Retrieved from: http://psycnet.apa.org/psycinfo/1959-07498-001 Farroni, T., Menon, E., Rigato, S., & Johnson, M. H. (2007). The perception of facial expressions in newborns. The European Journal of Developmental Psychology, 4(1), 2­ 13. http://doi.org/10.1080/17405620601046832 Farzin, F., Hou, C., & Norcia, A. M. (2012). Piecing it together: infants' neural responses to face and object structure. Journal of Vision, 12(13), 1-14. http://doi.org/10.1167/12.13.6 Field, T. M., Woodson, R., Greenberg, R., & Cohen, D. (1982). Discrimination and imitation of facial expressions by neonates. Science, 218(4568), 179­181. http://doi.org/10.1126/science.7123230

111

Flom, R., & Bahrick, L. E. (2007). The development of infant discrimination of affect in multimodal and unimodal stimulation: The role of intersensory redundancy. Developmental Psychology, 43(1), 238­252. http://doi.org/10.1037/0012-1649.43.1.238 Fox, E. (2002). Processing emotional facial expressions: the role of anxiety and awareness. Cognitive, Affective & Behavioral Neuroscience, 2(1), 52­63. http://doi.org/10.1080/02699930143000527 Fox, E., Lester, V., Russo, R., Bowles, R. J., Pichler, A., & Dutton, K. (2000). Facial Expressions of Emotion: Are Angry Faces Detected More Efficiently? Cognition & Emotion, 14(1), 61­92. http://doi.org/10.1080/026999300378996 Garstein, M., & Rothbart, M. K. (2003). Studying infant temperament via a revision of the Infant Behavior Questionnaire. Infant Behavior & Development, 26, 64­86. Geangu, E., Hauf, P., Bhardwaj, R., & Bentz, W. (2011). Infant pupil diameter changes in response to others' positive and negative emotions. PLoS ONE, 6(11). http://doi.org/10.1371/journal.pone.0027132 Goeleven, E., De Raedt, R., Leyman, L., & Verschuere, B. (2008). The Karolinska Directed Emotional Faces: A validation study. Cognition & Emotion, 22(6), 1094­1118. http://doi.org/10.1080/02699930701626582 Grossmann, T., Striano, T., & Friederici, A. D. (2007). Developmental changes in infants' processing of happy and angry facial expressions: A neurobehavioral study. Brain and Cognition, 64(1), 30­41. http://doi.org/10.1016/j.bandc.2006.10.002 Halberstadt, A. G., Cassidy, J., Stifter, C. A., Parke, R. D., & Fox, N. A. (1995). Selfexpressiveness within the family context: Psychometric support for a new measure. Psychological Assessment, 7(1), 93-103. 112

Hall, J. A. (1984). Nonverbal sex differences: Communication accuracy and expressive style. Baltimore, MD: John Hopkins University Press. Hamann, S., & Canli, T. (2004). Individual differences in emotion processing. Current Opinion in Neurobiology, 14(2), 233­238. http://doi.org/10.1016/j.conb.2004.03.010 Harnad, S. (1987) Psychophysical and cognitive aspects of categorical perception: A critical overview. Chapter 1 of: Harnad, S. (ed.) (1987) Categorical Perception: The Groundwork of Cognition. New York: Cambridge University Press. Hinojosa, J. A, Mercado, F., & Carretié, L. (2015). N170 sensitivity to facial expression: A metaanalysis. Neuroscience and Biobehavioral Reviews, 55, 498­509. http://doi.org/10.1016/j.neubiorev.2015.06.002 Holmes, A., Vuilleumier, P., & Eimer, M. (2003). The processing of emotional facial expression is gated by spatial attention: evidence from event-related brain potentials. Cognitive Brain Research, 16(2), 174­184. http://doi.org/10.1016/S0926-6410(02)00268-9 Hunnius, S., de Wit, T. C. J., Vrins, S., & von Hofsten, C. (2011). Facing threat: infants' and adults' visual scanning of faces with neutral, happy, sad, angry, and fearful emotional expressions. Cognition & Emotion, 25(2), 193­205. http://doi.org/10.1080/15298861003771189 Izard, C. E. (1994). Innate and Universal Facial Expressions: Evidence from Developmental and Cross-Cultural Research. Psychological bulletin, 115(2), 288-299. Izard, C. E., Fantauzzo, C. A., Castle, J. M., Haynes, O. M., Rayias, M. R., & Putnam, P. H. (1995). The ontogeny and significance of infants' facial expressions in the first 9 months of life. Developmental Psychology, 31(6), 997-1013. http://doi.org/10.1037/00121649.31.6.997 113

Jessen, S., Altvater-Mackensen, N., & Grossmann, T. (2016). Pupillary responses reveal infants' discrimination of facial emotions independent of conscious perception. Cognition, 150, 163­169. http://doi.org/10.1016/j.cognition.2016.02.010 Jessen, S., & Grossmann, T. (2015). Neural signatures of conscious and unconscious emotional face processing in human infants. Cortex, 64, 260­270. http://doi.org/10.1016/j.cortex.2014.11.007 Junghöfer, M., Sabatinelli, D., Bradley, M. M., Schupp, H. T., Elbert, T. R., & Lang, P. J. (2006). Fleeting images: rapid affect discrimination in the visual cortex. Neuroreport, 17(2), 225­229. http://doi.org/10.1097/01.wnr.0000198437.59883.bb Kaitz, M., Meschulach-Sarfaty, O., Auerbach, J., & Eidelman, A. (1988). A reexamination of newborns' ability to imitate facial expressions. Developmental Psychology, 24(1), 3­7. http://doi.org/10.1037/0012-1649.24.1.3 Kaneshige, T., & Haryu, E. (2015). Categorization and understanding of facial expressions in 4month-old infants. Japanese Psychological Research, 57(2), 135­142. http://doi.org/10.1111/jpr.12075 Kanwisher, N., Tong, F., & Nakayama, K. (1998). The effect of face inversion on the human fusiform face area. Cognition, 68(1), B1­B11. http://doi.org/10.1016/S00100277(98)00035-3 Kesler-West, M. L., Andersen, A. H., Smith, C. D., Avison, M. J., Davis, C. E., Kryscio, R. J., & Blonder, L. X. (2001). Neural substrates of facial emotion processing using fMRI. Brain Research Cognitive Brain Research, 11(2), 213­226. http://doi.org/S0926641000000732 Kiss, M., & Eimer, M. (2008). ERPs reveal subliminal processing of fearful faces. Psychophysiology, 45(2), 318­326. http://doi.org/10.1111/j.1469-8986.2007.00634.x 114

Klinnert, M. D. (1984). The regulation of infant behavior by maternal facial expression. Infant Behavior and Development, 7(4), 447­465. http://doi.org/10.1016/S01636383(84)80005-3 Kobiella, A., Grossmann, T., Reid, V. M., & Striano, T. (2008). The discrimination of angry and fearful facial expressions in 7-month-old infants: An event-related potential study. Cognition & Emotion, 22, 134­146. http://doi.org/10.1080/02699930701394256 Kolassa, I. T., & Miltner, W. H. (2006). Psychophysiological correlates of face processing in social phobia. Brain Research, 1118(1), 130­141. http://doi.org/10.1016/j.brainres.2006.08.019 Kotsoni, E., de Haan, M., & Johnson, M. H. (2001). Categorical perception of facial expressions by 7-month-old infants. Perception, 30, 1115­1125. http://doi.org/10.1068/p3155 Krombholz, A., Schaefer, F., & Boucsein, W. (2007). Modification of N170 by different emotional expression of schematic faces. Biological Psychology, 76(3), 156­62. http://doi.org/10.1016/j.biopsycho.2007.07.004 LaFrance, M., Hecht, M. A, & Paluck, E. L. (2003). The contingent smile: a meta-analysis of sex differences in smiling. Psychological Bulletin, 129(2), 305­334. http://doi.org/10.1037/0033-2909.129.2.305 Leppänen, J. M., & Hietanen, J. K. (2004). Positive facial expressions are recognized faster than negative facial expressions, but why? Psychological Research, 69(1-2), 22­29. http://doi.org/10.1007/s00426-003-0157-2 Leppänen, J. M., Milders, M., Bell, J. S., Terriere, E., & Hietanen, J. K. (2004). Depression biases the recognition of emotionally neutral faces. Psychiatry Research, 128(2), 123­ 133. http://doi.org/10.1016/j.psychres.2004.05.020 115

Leppänen, J. M., Moulson, M. C., Vogel-Farley, V. K., & Nelson, C. (2007). An ERP Study of Emotional Face Processing in the Adult and Infant Brain. Child Development, 78(1), 232 ­ 245. Leppänen, J. M., & Nelson, C. (2009). Tuning the developing brain to social signals of emotions. Nature Reviews Neuroscience, 10(1), 37­47. http://doi.org/10.1038/nrn2554 Leppänen, J. M., & Nelson, C. A. (2006). The development and neural bases of facial emotion recognition. Advances in Child Development and Behavior, 34, 207­246. http://doi.org/10.1016/S0065-2407(06)80008-X Leppänen, J. M., Richmond, J., Vogel-Farley, V. K., Moulson, M. C., & Nelson, C. (2009). Categorical representation of facial expressions in the infant brain. Infancy, 14(3), 346­ 362. http://doi.org/10.1080/15250000902839393 Liu-Shuang, J., Norcia, A. M., & Rossion, B. (2014). An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation. Neuropsychologia, 52(1), 57­72. http://doi.org/10.1016/j.neuropsychologia.2013.10.022 Lobue, V., & Deloache, J. S. (2010). Superior detection of threat-relevant stimuli in infancy. Developmental Science, 13(1), 221­228. http://doi.org/10.1111/j.14677687.2009.00872.x Ludemann, P. M. (1991). Generalized discrimination of positive facial expression by seven- and ten-month-old infants. Child Development, 62(1), 55­67. http://doi.org/10.2307/1130704 Ludemann, P. M., & Nelson, C. A. (1988). Categorical representation of facial expressions by 7month-old infants. Developmental Psychology, 24(4), 492­501. http://doi.org/10.1037/0012-1649.24.4.492 116

Lundqvist, D., Flykt, A., & Öhman, A. (1998). The Karolinska Directed Emotional Faces KDEF, [CD ROM]. Published instrument. Retrieved from: http://www.emotionlab.se/resources/kdef Madole, K. L., & Oakes, L. M. (1999). Making sense of infant categorization: Stable processes and changing representations. Developmental Review, 19(2), 263­296. Maurer, D., Le Grand, R., & Mondloch, C. J. (2002). The many faces of configural processing. Trends in Cognitive Sciences, 6(6), 255­260. http://doi.org/10.1016/S13646613(02)01903-4 McClure, E. B. (2000). A Meta-Analytic Review of Sex Differences in Facial Expression Processing and Their Development in Infants, Children, and Adolescents. Psychological Bulletin, 126(3), 424­453. http://doi.org/10.1037/0033-2909.126.3.424 Montague, D. P. F., & Walker-Andrews, A. S. (2001). Peekaboo: A new look at infants' perception of emotion expressions. Developmental Psychology, 37(6), 826­838. http://doi.org/10.1037/0012-1649.37.6.826 Morris, J. S., Frith, C. D., Perrett, D. I., Rowland, D., Young, A. W., Calder, A. J., & Dolan, R. J. (1996). A differential neural response in the human amygdala to fearful and happy facial expressions. Nature, 383, 812-815. http://doi.org/10.1038/383812a0 Moulson, M., Sugden, N. A., & Patel, T. (July, 2015). Smile! Everyday experiences with emotional faces in infancy. Paper presented at the Jean Piaget Society, Toronto, Ontario, Canada. Nakato, E., Otsuka, Y., Kanazawa, S., Yamaguchi, M. K., & Kakigi, R. (2011). Distinct differences in the pattern of hemodynamic response to happy and angry facial

117

expressions in infants - A near-infrared spectroscopic study. NeuroImage, 54(2), 1600­ 1606. http://doi.org/10.1016/j.neuroimage.2010.09.021 Nelson, C. (1987). The recognition of facial expressions in the first two years of life: mechanisms of development. Child Development, 58(4), 889­909. Nelson, C., & de Haan, M. (1996). Neural Correlates of Infants' Visual Responsiveness to Facial Expressions of Emotion. Developmental Psychobiology, 29, 577­595. Nelson, C., & Dolgin, K. G. (1985). The Generalized Discrimination of Facial Expressions by Seven-Month-Old Infants. Child Development, 56(1), 58­61. Öhman, A., Lundqvist, D., & Esteves, F. (2001). The face in the crowd revisited: a threat advantage with schematic stimuli. Journal of Personality and Social Psychology, 80(3), 381­96. http://doi.org/10.1037/0022-3514.80.3.381 Peltola, M. J., Leppänen, J. M., & Hietanen, J. K. (2011). Enhanced cardiac and attentional responding to fearful faces in 7-month-old infants. Psychophysiology, 48(9), 1291­1298. http://doi.org/10.1111/j.1469-8986.2011.01188.x Peltola, M. J., Leppänen, J. M., Mäki, S., & Hietanen, J. K. (2009). Emergence of enhanced attention to fearful faces between 5 and 7 months of age. Social Cognitive and Affective Neuroscience, 4(2), 134­142. http://doi.org/10.1093/scan/nsn046 Phillips, R. D., Wagner, S. H., Fells, C. A., & Lynch, M. (1990). Do infants recognize emotion in facial expressions?: Categorical and "Metaphorical" evidence. Infant Behavior and Development, 13(1), 71­84. http://doi.org/10.1016/0163-6383(90)90006-T Ravicz, M. M., Perdue, K. L., Westerlund, A., Vanderwert, R. E., & Nelson, C. (2015). Infants' neural responses to facial emotion in the prefrontal cortex are correlated with

118

temperament: a functional near-infrared spectroscopy study. Frontiers in Psychology, 6, 1-12. http://doi.org/10.3389/fpsyg.2015.00922 Rossion, B. (2014a). Understanding face perception by means of human electrophysiology. Trends in Cognitive Sciences, 18(6), 310­318. http://doi.org/10.1016/j.tics.2014.02.013 Rossion, B. (2014b). Understanding individual face discrimination by means of fast periodic visual stimulation. Experimental Brain Research, 232(6), 1599­1621. http://doi.org/10.1007/s00221-014-3934-9 Rossion, B., & Boremanse, A. (2011). Robust sensitivity to facial identity in the right human occipito-temporal cortex as revealed by steady-state visual-evoked potentials. Journal of Vision, 11(2), 1­21. http://doi.org/10.1167/11.2.16 Rossion, B., & Jacques, C. (2012). The N170: Understanding the time course of face perception in the human brain. In S.J. Luck & E. Kappenman (Ed.), The Oxford handbook of eventrelated potential components. (pp. 115­141). Oxford University Press. Rossion, B., Torfs, K., Jacques, C., & Liu-Shuang, J. (2015). Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain. Journal of Vision, 15(1). http://doi.org/10.1167/15.1.18 Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39(6), 1161­1178. http://doi.org/10.1037/h0077714 Russell, J. A., & Bullock, M. (1985). Multidimensional Scaling of Emotional Facial Expressions. Similarity From Preschoolers to Adults. Journal of Personality and Social Psychology, 48(5), 12901298. http://doi.org/10.1037/0022-3514.48.5.1290

119

Schupp, H. T., Öhman, A., Junghöfer, M., Weike, A. I., Stockburger, J., & Hamm, A. O. (2004). The Facilitated Processing of Threatening Faces: An ERP Analysis. Emotion, 4(2), 189­ 200. http://doi.org/10.1037/1528-3542.4.2.189 Schwartz, G. M., Izard, C. E., & Ansul, S. E. (1985). The 5-Month-Old's Ability to Discriminate Facial Expressions of Emotion. Infant Behavior and Development, 8(1981), 65­77. Seirafi, M., De Weerd, P., & De Gelder, B. (2013). Emotion categorization does not depend on explicit face categorization. Journal of Vision, 13, 1­9. http://doi.org/10.1167/13.2.12.doi Serrano, J. M., Iglesias, J., & Loeches, A. (1992). Visual discrimination and recognition of facial expressions of anger, fear, and surprise in 4- to 6-month-old infants. Developmental Psychobiology, 25(6), 411­425. http://doi.org/10.1002/dev.420250603 Serrano, J. M., Iglesias, J., & Loeches, A. (1995). Infants' responses to adult static facial expressions. Infant Behavior and Development, 18(4), 477­482. http://doi.org/10.1016/0163-6383(95)90036-5 Smith, M. L. (2012). Rapid processing of emotional expressions without conscious awareness. Cerebral Cortex, 22(8), 1748­1760. http://doi.org/10.1093/cercor/bhr250 Sugden, N. A., Festa, S., Vascotto, M., & Moulson, M. (May, 2016). Put on a happy face: Developmental changes in infants' exposure to facial expressions of emotion from 3 to 6 months. Paper presented at the International Conference on Infant Studies, New Orleans, Louisiana, USA. Sugden, N. A., Mohamed-Ali, M. I., & Moulson, M. C. (2014). I spy with my little eye: Typical, daily exposure to faces documented from a first-person infant perspective. Developmental Psychobiology, 56(2), 249­261. http://doi.org/10.1002/dev.21183

120

Thomas, K. M., Drevets, W. C., Whalen, P. J., Eccard, C. H., Dahl, R. E., Ryan, N. D., & Casey, B. J. (2001). Amygdala response to facial expressions in children and adults. Biological Psychiatry, 49(4), 309­316. http://doi.org/10.1016/S0006-3223(00)01066-0 Tottenham, N., Hare, T. A., & Casey, B. J. (2011). Behavioral assessment of emotion discrimination, emotion regulation, and cognitive control in childhood, adolescence, and adulthood. Frontiers in Psychology, 2, 1­9. http://doi.org/10.3389/fpsyg.2011.00039 Valentine, T. (1988). Upside-down faces: a review of the effect of inversion upon face recognition. British Journal of Psychology, 79(4), 471­491. http://doi.org/10.1111/j.2044-8295.1988.tb02747.x Vialatte, F. B., Maurice, M., Dauwels, J., & Cichocki, A. (2010). Steady-state visually evoked potentials: Focus on essential paradigms and future perspectives. Progress in Neurobiology, 90(4), 418­438. http://doi.org/10.1016/j.pneurobio.2009.11.005 Vuilleumier, P., & Pourtois, G. (2007). Distributed and interactive brain mechanisms during emotion face perception: Evidence from functional neuroimaging. Neuropsychologia, 45(1), 174­194. http://doi.org/10.1016/j.neuropsychologia.2006.06.003 Wells, P. H., Johnson, D. L., Ekman, P., Sorenson, E. R., & Friesen, W. V. (1969). Pan-Cultural Elements in Facial Displays of Emotion. Science, 164(3875), 86­88. Retrieved from http://science.sciencemag.org/content/164/3875/86.abstract Whalen, P. J., Rauch, S. L., Etcoff, N. L., McInerney, S. C., Lee, M. B., & Jenike, M. A. (1998). Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge. The Journal of Neuroscience, 18(1), 411­418. http://doi.org/9412517

121

Widen, S. C., & Russell, J. A. (2003). A closer look at preschoolers' freely produced labels for facial expressions. Developmental Psychology, 39(1), 114­128. http://doi.org/10.1037/0012-1649.39.1.114 Widen, S. C., & Russell, J. A. (2008). Children acquire emotion categories gradually. Cognitive Development, 23(2), 291­312. http://doi.org/10.1016/j.cogdev.2008.01.002 Young-Browne, G., Rosenfeld, H. M., & Horowitz, F. D. (1977). Infant Discrimination of Facial Expressions, Child Development, 48(2), 555­562. Zhan, M., Hortensius, R., & De Gelder, B. (2015). The body as a tool for anger awarenessdifferential effects of angry facial and bodily expressions on suppression from awareness. PLoS ONE, 10(10), 1­13. http://doi.org/10.1371/journal.pone.0139768

122

