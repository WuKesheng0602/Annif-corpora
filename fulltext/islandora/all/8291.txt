PRICING ENERGY CONTRACTS UNDER REGIME SWITCHING TIME-CHANGED LEVY PROCESSES by Konrad Gajewski Bachelor of Music, University of Toronto, 2016

A thesis presented to Ryerson University in partial fulfilment of the requirements for the degree of Master of Science in the program of Applied Mathematics

Toronto, Ontario, Canada, 2018 c Konrad Gajewski, 2018

Author's Declaration
I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of the other institutions or individuals for the purpose of scholarly research. I understand that my thesis may be made electronically available to the public.

ii

PRICING ENERGY CONTRACTS UNDER REGIME SWITCHING TIME-CHANGED LEVY PROCESSES Konrad Gajewski Master of Science, 2018 Applied Mathematics Ryerson University

Abstract
The failures of the popular Black-Scholes-Merton (BSM) model led to an interest in new, robust models which could more accurately model the behavior of historical prices. We consider one such model, the regime switching time-changed Levy process, which builds upon the BSM model by incorporating jumps through a random clock, as well as randomly varying parameters according to a continuous-time Markov chain. We develop the characteristic function as well as two methods for pricing European call options. Finally, we estimate the parameters of the model by incorporating historic energy data and option quotes using a variety of methods.

iii

Acknowledgements
I would like to thank my supervisors Dr. Pablo Olivares and Dr. Sebastian Ferrando, for their inspiration, guidance and encouragement in developing this thesis. My thanks also go to all my friends and colleagues, who through many conversations, inspired and supported me during the course of my studies.

iv

Dedication
To my family for their endless love, support, encouragement and sacrifice - I could never have done it without you.

v

Table of Contents
Declaration Abstract Acknowledgements Dedication List of Tables List of Figures List of Appendices Introduction 1 Background Material 2 Pricing 2.1 2.2 2.3 ii iii iv v viii x xii 1 5 24

Switching Time-Changed Levy Process . . . . . . . . . . . . . 25 Simulating a Single Trajectory . . . . . . . . . . . . . . . . . . 27 Pricing under Monte Carlo Simulation . . . . . . . . . . . . . 33 vi

2.4 2.5 2.6 2.7

Characteristic Function . . . . . . . . . . . . . . . . . . . . . . 43 Pricing under a Risk Neutral Measure . . . . . . . . . . . . . . 49 Fourier-Cosine Method . . . . . . . . . . . . . . . . . . . . . . 52 Pricing under Fourier-Cosine Method . . . . . . . . . . . . . . 56 59

3 Estimation 3.1 3.2 3.3 3.4

Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 Method of Moments . . . . . . . . . . . . . . . . . . . . . . . 67 Minimum Distance Estimation . . . . . . . . . . . . . . . . . . 71 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . 73 77 78 82 99

Conclusion Appendix A: Simulating Random Variables Appendix B: MATLAB Code Bibliography

vii

List of Tables
2.1 Comparison of European call payoffs under vectorizatrion vs. simulating one trajectory at a time, with parameters T = 1, 1 = 1, 2 = 0.5 . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.2 Comparison of various European call payoffs under different strike prices and maturities. We compare Black Scholes to the switching model under Gamma and Inverse Gaussian subordinators. We set r = 0.04,  1 =  2 , 1 = 2 ,  1 =  2 , µ1 = µ2 where µ1 is the Black Scholes risk neutral drift. . . . . . . . . 43 2.3 European call option payoff comparison between the Black Scholes formula and Monte Carlo simulation of the reduced switching Levy process at different parameters . . . . . . . . . 51 2.4 Comparison of European Call option Payoffs using Monte Carlo Simulation and Fourier-Cosine Pricing, as well as their computational times. . . . . . . . . . . . . . . . . . . . . . . . . . 57 3.1 3.2 Standard statistics of commodities . . . . . . . . . . . . . . . 62

Holding-rate parameters estimation for each commodity as well as the variance in each regime . . . . . . . . . . . . . . . 64

viii

3.3 3.4

Parameter Calibration using Root Mean Square Error . . . . . 67 Parameter Estimation using Method of Moments under Gamma Subordinator . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

3.5

Parameter Estimation using Method of Moments under Inverse Gaussian Subordinator . . . . . . . . . . . . . . . . . . . 71

3.6

Parameter Estimation using Minimum Distance Method under Gamma subordinator . . . . . . . . . . . . . . . . . . . . . . . 73

3.7

Parameter Estimation using Minimum Distance Method under Inverse Gaussian subordinator . . . . . . . . . . . . . . . . . . 73

3.8

Parameter Estimation using Maximum Likelihood Method under Gamma subordinator . . . . . . . . . . . . . . . . . . . . . 75

3.9

Parameter Estimation using Maximum Likelihood Method under Inverse Gaussian subordinator . . . . . . . . . . . . . . . . 76

3.10 Important moments of the Inverse Gaussian and Gamma distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

ix

List of Figures
2.1 Parameters: T = 1, µ1 = 0.01, µ2 = -0.1,  1 = 1,  2 = 5, 1 = 2 = 0.1,  1 = 0.1,  2 = 10, 1 = 0.2, 2 = 0.5. . . . . . . . . . 32 2.2 Same parameters as Figure 2.1 except for the holding-time rates 1 and 2 . 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . 32

European call payoff as a function of T and K under two different subordinators with risk neutral drift (see section 2.5). The other parameters are identical for each figure:  1 = 0.03,  2 = 0.7, 1 = 2 = 0.1,  1 = 1,  2 = 1.2, 1 = 0.4, 2 = 1. . . . . . . 38

2.4

Comparison of long term behaviour of the 95% confidence interval under different subordinators with identical parameters, including  1 = 0.1,  2 = 0.01. . . . . . . . . . . . . . . . . . . 39

2.5

Behaviour of the payoffs when the parameters of the subordinator are varied. . . . . . . . . . . . . . . . . . . . . . . . . . 40

2.6

Behaviour of the payoffs when the parameters of the subordinator are varied. . . . . . . . . . . . . . . . . . . . . . . . . . 41

2.7

Behaviour of the payoffs when varying different parameters. . 41

x

2.8

The Characteristic function Yt (u) with parameters  = 0.1,  = 0.1, µ = 0.01,  = 0.01, t = 1/250 where the subordinator L has increments with Inverse Gaussian distribution. . . . . . . . 46

2.9

The Characteristic function Zt (u) with parameters 1 = 0.1 = 2 ,  1 = 0.1,  2 = 0.01, µ1 = 0.01, µ2 = -0.01,  1 = 0.01,  2 = 0.05, t = 1/250 where conditionally on each regime, the subordinator L has increments with Inverse Gaussian distribution. 47

2.10 The characteristic function Yt (u) with parameters  = 0.1,  = 0.1, µ = 0.01,  = 0.01, t = 1/250 where the subordinator L has increments with Gamma distribution. . . . . . . . . . . . . 48 2.11 The characteristic function Zt (u) with identical parameters as 2.11, but now conditionally on each regime, the subordinator L has increments with Gamma distribution. Note the thicker tails. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 2.12 The difference between Fourier-Cosine Pricing and Monte Carlo, as a function of the strike price K and time to maturity K . . . 58 3.1 3.2 Price Process and Log-returns for two different commodities . 61 Empirical density functions vs. normal distrubtions . . . . . . 62

xi

List of Appendices
Simulating Gamma Random Variables . . . . . . . . . . . . . . . . . . 78 Simulating Inverse Gaussian Random Variables . . . . . . . . . . . . . 79 Simulating a Single Trajectory Regime Switching Time-Changed Levy Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 Monte Carlo Simulation for European Call Options by simulating N Processes Simultaneously . . . . . . . . . . . . . . . . . . . . . . . . 86 Fourier-Cosine Pricing . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 Minimum Distance Estimation Function . . . . . . . . . . . . . . . . . 92 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . 94 Simulating Gamma and Inverse Gaussian Random Variables . . . . . . 96

xii

Introduction
The famous Black-Scholes-Merton (BSM) model assumes that the market consists of a risky asset such as a stock, and a riskless asset such as a bond or bank account. It can be seen as the continuous limit of the binomial tree model. Furthermore, the model implies that the market is complete and assumes that the market admits no arbitrage opportunities. A market is complete if every payoff is attainable and every derivative asset can be perfectly hedged. A market admits arbitrage if it is possible to make money without taking on any risk. Under the BSM model, the rate of return on the riskless asset is assumed to be constant and hence is called the risk-free interest rate. The underlying risky assets are distributed under a Gaussian probability distribution with constant volatility. Hence, the price movements are continuous. This assumption is not supported by the empirical evidence. In particular for energy markets such as oil and electricity futures, more complex movements are observed, such as mean reversion, sudden oscillations and price jumps. The failures of the BSM model can be seen through the implied volatility skew. Given the pricing formula and a set of historical option prices, one can solve for the implied volatility, which would then cor1

rectly price each contract. The limits of the BMS model become apparent when volatilities are compared at different maturity times and strike prices - such volatilies vary substantially, implying that diffusion models are not robust enough to capture the movements of the market [5].

While there is overwhelming evidence from the historical data that the volatility should depend on time, there is no consensus as to how this volatility should be modelled [20]. Examples include local volatility models where the volatility term varies deterministically in time and stochastic volatility models where the volatility term is itself a stochastic process. Another approach is to consider time driven by a non-negative stochastic process called a subordinator, leading to the class of Levy time changed processes.

In option pricing theory, it is a consequence of the model that the market does not admit arbitrage opportunities. This means that the discounted asset prices are martingales under some probability measure Q, called the risk-neutral measure. By introducing stochastic volatility or jumps, this measure Q loses uniqueness and the existence of infinitely many risk-neutral measures is referred to as market incompleteness, meaning it is not possible to perfectly hedge every derivative asset.

Many financial time series including commodity futures seem to exhibit dramatic breaks in their behaviour, for example in the events of political changes or financial crises. Different intervals sharing similar characteristic 2

can be grouped together under a single regime. One class of models that describe such behaviour are regime switching Levy models. Under such a model, the process switches randomly between different Levy processes according to an unobservable Markov chain. We modify the regime switching model by introducing a random clock, which introduces random jumps in the model. The regime switching time-changed Levy process is a pure jump process which captures two key features of the market: the existence of regimes and price jumps.

We make a few more assumptions on the market and on the assets. For the market, we assume that it is possible to buy and sell any real number of the risky or riskless assets. This includes fractional amounts of a single share, and negative values which correspond to shorting. The transactions happen instantaneously and at zero cost. Finally, the principle of no arbitrage holds. For the assets, we assume that the risk-free interest rate is constant and doesn't change between regimes and that the risky asset pays no dividends.

The objectives of the thesis are to model regime switching Levy processes, price European call options, and estimate the parameters using option quotes and historical prices of comodities under a variety of methods. Although time-changed Levy processes and switching models have been studied separately, an integrated model has not been considered in connection to pricing and parameter estimation. Consequently, Monte Carlo simulation and Fourier Cosine pricing methods have not been implemented so far. 3

In Chapter 1 we introduce the relevant background terminology that will be used throughout the thesis. In Chapter 2 we define the regime switching time-changed Levy process, derive its characteristic function under Gamma and Inverse Gaussian subordinators and give two pricing methods: Monte Carlo and Fourier Cosine method. We compare prices under the regime switching model to those given by the Black-Scholes equation and show that the prices agree when the switching model is reduced to the BSM model. For Monte Carlo, we develop an algorithm for simulating trajectories of the regime switching Levy process, as well as for pricing European call options by simulating many regime switching processes simultaneously. In Chapter 3 we use calibration and various estimation methods to estimate values of the parameters using option quotes and historical prices of oil and electricity commodities. Relevant code and rejection methods for simulating random variables are given in the Appendix.

4

Chapter 1 Background Material
We give several definitions and theorems related to Levy processes and regime switching processes. We begin with an overview of stochastic processes and build up to include time-changed Levy processes, characteristic functions and continuous-time Markov chains. Definition 1.0.1. (Stochastic Process) [18][27].

A stochastic process is a collection of random variables X = {Xt }tI defined on a common filtered probability space (, F , {Ft }tI , P) and indexed by some totally ordered set I , usually denoting some interval of time. If I is countable, then X is called a discrete-time process, and if I is uncountable then X is a continuous-time process. We will usually take I = [0, T ] for some maturity time T  R+  {}. The probability space consists of: (1) The nonempty set  of all possible paths  . 5

(2) The  -algebra F , which denotes the set of all events. Formally F consists of the empty set  and the entire set  as well as a set of subsets of  such that F is closed under complements and countable unions. The pair consisting of a set with its  -algebra (, F ) is called a measurable space and the elements of F are called measurable sets. (3) The probability measure P : F  [0, 1]. (4) A filtration {Ft }tI , a non-decreasing sequence of  -algebras, such that F0 = {, } and Fs  Ft  F for s  t. (1.1)

A measurable space is complete if all subsets of a set of measure zero are also in the  -algebra. The filtered probability space (, F , {Ft }tI , P) is Pcomplete if Ft contains all subsets of sets of measure zero under P for each t  I , and {Ft }tI is right continuous i.e. Ft =
s>t

Fs .

A Borel set is a set in a topological space that can be formed by complements and countable unions of open sets defined by the topology. The collection of all Borel sets forms the Borel  -algebra - the smallest  -algebra containing all open sets. If we consider R with the standard topology, the Borel  algebra B (R) is the smallest  -algebra containing all the open intervals and the pair (R, B (R)) form a measurable space. A function between measurable spaces is called a measurable function if the preimage of a measurable set is 6

measureable. In particular, continuous functions between Borel  -algebras are measurable because the preimage of every open set under a continuous function is open. A random variable: Xt : (, F )  (R, B (R)) is an Ft -measurable function if for each set B  B (R), Xt-1 (B ) = {   : Xt ( )  B }  Ft . Intuitively, Ft represents the information available at time t, and the filtration {Ft }tI represents the information flow evolving over time. Mathematically, Ft = { (Xs ), s  [0, t]} where the  -algebra generated by the random variables Xs is defined as:
-1  (Xs ) = {Xs (B )|B  B (R)}.

A process X is cadlag if it is right continuous with left limits. X is adapted (or non-anticipating) to the filtration {Ft }tI if Xt is Ft -measurable for each t  I . The image of E  R of Xt is called the state space [15]. Finally, the conditional expectation of a random variable Xt given an event A  F , is defined as:

E[Xt |A] =

E[1A Xt ] , P(A)

where 1A is the indicator function on a set A.

7

Definition 1.0.2. (Stopping Time) [8][24].

A random time is a positive random variable  :   R+ which represents the time at which an event takes place, for example a change of parameters. Given an information flow {Ft }t0 , if we can determine at time t, given the information Ft whether an event has occured, the random time  is called a stopping time. Formally, a stopping time with respect to a filtration {Ft }t0 is a random variable  such that,    :  ( )  t  F t , t  0.

If 1 and 2 are stopping times, then 1  2  inf{1 , 2 } is also a stopping time. Given a stopping time  and an adapted process {Xt }t0 , we can define a new process Xt , the process X stopped at  as:    Xt , if t <  Xt =   X , if t   Given an adapted process {Xt }t0 , the hitting time A of an open set A  E is defined as the first time that X hits A: A = inf{t  0 : Xt  A} The set of hitting times is a subset of the set of stopping times. Definition 1.0.3. (Martingales) [23][24].

8

A cadlag process {Xt }t0 on a filtered probability space (, F , {Ft }tI , P) is called a martingale if the best prediction of the future value of a process is its current value:

E[Xt |Fs ] = Xs for t > s and E[|Xt |] < .
A local martingale is a process {Xt }t0 for which there exists a sequence of bounded stopping times {n }nN increasing to infinity for which {Xtn }t0 is a martingale for each n. A semimartingale {Yt }t0 is a process which can be decomposed as: Yt = Mt + At where M = {Mt }t0 is a local martingale and A = {At }t0 is a cadlag adapted process with locally bounded variation. A process {At }t0 is said to be of bounded variation if its total variation V (A) is finite. The total variation is given by:
nP -1

V (A) = sup
P P i=0

|Ati+1 - Ati | < .

where P = P = {t0 , ..., tnP } where P is a partition over [0, T ] . Definition 1.0.4. (Continuous-Time Markov Chain) [2][19][28].

A Markov process is a stochastic process with the additional Markov property; the future value is independent of the past given the present value. A 9

(R, B (R))-valued stochastic process {Xt }tI adapted to the filtration {Ft }tI has the Markov property if for each B  B (R):

P(Xt  B |Fs ) = P(Xs  B |Xs ) when s < t.
A Markov process with a finite countable state space is called a Markov chain. Additionally, if I = N, the Markov process is called a discrete-time Markov chain. The Markov chain is said to be in regime i at time t if Xt = i for each i in the state space. Each discrete-time Markov chain has an associated one-step transition matrix P = Pij , i, j  E where Pij is the probability of moving from regime i to regime j at time t = 0.

If I = [0, ) and the state space is finite countable, the Markov process is called a continuous-time Markov chain. In the continuous case, there is a transition matrix P (t) = Pij (t) for every t  0. As in the discrete case, the distribution of the future Xt , given the present Xt , does not depend upon the present time t, but only on the present state Xt = i  E : Pij (t) = P(Xt = j |X0 = i) In order to satisfy the Markov property, the amount of time spent between transitions must not depend on how much time has already elapsed. This is the property of memorylessness. A random variable M is memoryless if :

P(M > t + h|M > t) = P(M > h) t, h > 0
The only continuous probability distribution with this property is the exponential distribution. 10

The random times at which the continuous-time Markov chain X does transition between regimes are called switching times. The random time interval between switching times during which X does not transition is called a holding-time (or interarrival time), which is an exponentially distributed random variable. The holding time is equal to the duration of remaining in a single regime. Given a continuous-time Markov chain, we can define its embedded discrete Markov chain, which is the discrete-time process which keeps track of the jumps of the continuous-time Markov chain. The embedded Markov chain then has its own one-step transition matrix P = Pij . For each continuoustime Markov chain, there is an associated infinitesimal generator matrix Q, with the same dimensions as P defined element-wise: Q = qij  lim +
t0

Pij (t) - ij = Pij (0), t

(1.2)

where ij are the elements of the identity matrix, Pij (0) = i Pij for i = j is the transition rate from regime i to regime j when the process is in regime i, and Pii (0) = -
i= j

qij . The parameter i is called the holding-time rate.

The elements qij of the infinitesimal generator Q can now be used to define the infinitesimal transition probabilities:

P(Xt+h = j |Xt = i) = qij h + o(h), i = j P(Xt+h = i|Xt = i) = 1 + qii h + o(h) otherwise,
for some arbitrarily small time increment h > 0. The term o(h) denotes any function f (h) with the property that limh0 f (h)/h = 0.

11

Definition 1.0.5. (Brownian Motion) [10].

One of the most import examples of a stochastic process is Brownian motion ( also called the Weiner process), denoted B = {Bt }t0 . The process has independent and stationary increments, i.e. given 0  s < t the increment Bt - Bs is independent of Fs , and Bt+s - Bt  Bs respectively. The increments Bt - Bs are normally distributed random variables with mean 0 and variance t - s. The process is called Standard Brownian motion if B0 = 0. Given the set of partitions: P = P = {t0 , ..., tnP } where P is a a partition over [0, T ] , Brownian motion has infinite total variation:
nP -1

V (B ) = sup
P P i=0

|Bti+1 - Bti | = ,

and finite quadratic variation at each time t  (0, T ] :
nP -1

< B >t = lim

P 0 P P i=0

|Bti+1 - Bti |2 = t < . is defined as P = max{(ti - ti-1 ) :

Here the norm of a partition P

i = 1, .., nP }. Brownian motion is known to not be differentiable anywhere, however, it is almost surely everywhere continuous. Definition 1.0.6. (Poisson Process and Random Measures) [26].

A counting process is a non-decreasing pure jump stochastic process with a state space consisting of non-negative integers. The Poisson process is 12

the simplest example of a counting process. If {i }i1 is a sequence of identical, independent exponential random variables with parameter  and Tn =
n i=1 i ,

then the process {Nt }t0 defined by: Nt =
n1

1tTn

is called a Poisson process with intensity . The process at time t counts the number of random times {Tn , n  1} occurring in the interval [0, t]. For any path  , the sample path   Nt ( ) is piece-wise constant, increases by jumps of unit size, and the conditional expectation of future states depends only on the present state (this is the Markov property). For any t > 0, the random variable Nt follows a Poisson distribution with parameter t:

P(Nt = n) = e-t

(t)n n!

n  N,

and has expectation E[Nt ] = t. The increments Nt - Ns are Poisson distributed with parameter (t - s) for 0  s < t, and are independent of Fs . With each Poisson process we can define an associated Poisson random measure M , such that for a given realization  and A  R+ [8], M (, A) = i  1| Ti ( )  A For any set A, M (, A) follows the Poisson distribution with intensity  A where A is the Lebesgue measure of A. M is called a random measure because it depends on the random path  . For each path  , the Poisson process is related to its random measure by: Nt ( ) =
s[0,t]

M (, ds). 13

Given Poisson Process with intensity , we can define a "Compensated" ~t }t0 as: Poisson process {N ~t = Nt - t N The term t is called the compensator, and is the quantity subtracted from Nt in order to recover the martingale property. The Compensated Poisson Process is no longer integer valued and can be negative, and so the compensated random measure: ~ (, A) = M (, A) -  A M is now a signed measure, meaning it can take on negative values. Definition 1.0.7. (Compound Poisson Processes).

A process Y = {Yt }t0 is called a Compound Poisson process with parameter  is defined as:
Nt

Yt =
j =1

Dj

where {Nt }t0 is a Poisson process with intensity , and {Dj , j  1} is a set of identically distributed random variables, all independent of each other and of Nt for each t. We note that if Dj is the constant unit function the Compound Poisson process reverts to an ordinary Poisson process. For each Compound Poisson process, we can associate a random measure on R × [0, ) describing the jumps of Y . For any measurable set A × [t1 , t2 ]  R × [0, ), define: JY (A × [t1 , t2 ]) = #{t  [t1 , t2 ] such that Yt = 0, 14 Yt  A}, (1.3)

where Yt = Yt - limst- Ys . Intuitively, JY counts the number of jump times between t1 and t2 whose jump sizes are in A. Jumps of zero size are not allowed, and negative sizes values correspond to descending jumps. If

E[Dj ] =  <  for each j , then E[Yt ] = t. Just like in the ordinary
Poisson process, the Compound Poisson process has a compensated version ~t }t0 which is a martingale. It it defined as: {Y
Nt

~t = Y
j =1

Dj - t.

Definition 1.0.8. (Levy Processes) [22][23].

A cadlag, adapted, real-valued stochastic process X = {Xt }t0 is called a Levy Process if:

(1) X0 = 0 almost surely. (2) t1 , t2 , ..., tn such that 0  t1 < t2 < ... < tn < , the increments Xt2 - Xt1 , Xt3 - Xt2 , ..., Xtn - Xtn-1 are independent. (3) The increments are stationary, i.e  t  0, h > 0, Xt+h - Xt is equal in distribution to Xh , meaning the distribution of the increments depends only on the length of elapsed time h.

(4) X is stochastically continuous (continuous in probability):  > 0 and t  0,
h0

lim P({   : |Xt+h ( ) - Xt ( )| > }) = 0 15

Every Levy process has the Markov property. Important examples of Levy processes include Brownian motion and Compound Poisson processes. Definition 1.0.9. (Levy Triplet and Levy-Ito Decomposition) [8].

Every Levy Process X =

Xt

t0

is completely determined by three

parameters called the Levy Triplet (µ,  2 ,  ), where µ  R is the drift,  > 0 is the volatility and  is the Levy measure:  : B (R)  R+ . Given any Borel set B  B (R),  (B ) is the expected number of jumps whose size belongs to B , per unit time. Formally:  (B ) = E # t  [0, 1] : Xt = 0, Xt  B ,

where Xt is the jump size. If  (R) < , the process has a finite number of jumps on every compact interval. If  (R) = , the process jumps an infinite number of times on every compact interval and we say that the process has infinite activity. The measure  also satisfies the following condition:  ({0}) = 0 and
R\{0}

min{1, |x|2 } (dx) < ,

(1.4)

Every Levy Process X with an associated Levy Triplet (µ,  2 ,  ) can be written as a sum of the processes X 1 , X 2 , X 3 , X 4 such that X converges to

16

X 1 + X 2 + X 3 + lim 0 X 4 almost surely. The four processes are defined: Xt1 = µt Xt2 = Bt Xt3 =
|x|1 s[0,t]

xJX (ds × dx) x{JX (ds × dx) -  (dx)ds}.
|x|<1 s[0,t]

Xt4 =

This decomposition shows that X is a combination of a deterministic component, Brownian motion and a pure jump process. For the pure jump process, we differentiate between "large" jumps and "small" jumps. X 3 is a compound Poisson process whose jump sizes are greater or equal to 1:
|Xs |1

Xt3

=
0st

Xs ,

while X 4 is a Compensated Compound Poisson process with jump sizes between > 0 and 1:
|Xs |<1

Xt4

=
0st

Xs .

(1.5)

JX is the Poisson random measure defined by equation (1.3). Equation (1.4) requires that there are finite number of jumps of size greater or equal to 1, while allowing an infinite number of jumps smaller than 1 providing that the sum of the squared values of the jumps converges to a finite value. Since jumps can only be of nonzero size, this requires X 4 to be dependent on which cannot be taken to zero directly. The threshold separating the

"large" jumps from the "small" jumps is taken to be |X | = 1, however this is arbitrary, and any jump size greater than zero can be used. 17

From the Levy-Ito Decomposition, we can see that if  = 0, the process is almost surely continuous. If instead  = 0 and
|x|1

|x| (dx) < , then
|x | 1

almost all paths have finite total variation. If  2 = 0 or

|x| (dx) = 

(or both) then almost all paths have infinite total variation. Finally, if X has the triplet (0, 0,  ), then X is a pure jump process. There are many ways of constructing martingales from Levy processes. ~ t }t0 defined as Given any Levy process X with E[|X0 |] < , the process {X ~ t = Xt - E[Xt ], X is a martingale. X is also a martingale if and only if |x| (dx) <  and µ +
|x|1 |x|1

x (dx) = 0.

The exponential of a Levy process Y = {Yt }t0 defined as Yt = exp Xt is martingale if and only if: exp(x) (dx) < 
|x|1

and µ+ 2 + 2 (exp(x) - 1 - x1|x|1  (dx) = 0.
R

Finally the process {Mt }t0 defined by Mt = exp(Xt ) E[exp(Xt )]

is a martingale for   R providing that E[exp(Xt )] < . Definition 1.0.10. (Time-Changed Levy Processes) [22][4][6].

18

An almost surely increasing Levy process L = {Lt }t0 is called a Subordinator. Its Levy triplet (µ, 0,  ) must satisfy  ((-, 0)) = 0 as well as equation (1.4). It can be modelled as a nondecreasing semi-martingale: Lt = µt +
x>0 s[0,t]

xJL (dx × ds).

Financially, t is called the calendar time or operational time while the subordinator L is called the business time. If {Xt }t0 is any Levy process, we define the time-changed Levy process as {Yt }t0 : Yt  XLt . The process Xt
t0

is then called the base process. If X = Xt

t0

is

a Levy process with a Levy triplet (µX , X , X ) and L = {Lt }t0 is a subordinator with Levy triplet (b, 0, ), then the time-changed process Yt again a Levy process with Levy triplet (µY , Y , Y ) where:
 t0

is

µY = µX b +
0

(ds)
|x|1

xpX s (dx)

Y = bX


Y (B ) = bX (B ) +
0

pX s (B )(ds),

B  B (R),

(1.6)

where pX s (B ), is the probability of B under the probability measure of Xs . While equation 1.0.10 is useful for computing Levy triplets of new Levy processes, it is difficult to find closed form formulas in most cases. Every local martingale M = {Ms }s0 can be written as time-changed Brownian motion {B<M >s }s0 , where < M >s is the quadtratic variation of 19

M. If the process {Xt }t0 defined as: Xt = µt + Bt , is subordinated by a pure jump process {Lt }t0 , the resulting process {XLt }t0 defined as: XLt = µLt + BLt , is a pure jump process, and is distributed as: XLt |Lt  N (µLt ,  2 Lt )  µLt +  Lt N (0, 1), (1.7)

where N (0, 1) is a random variable with standard normal distribution. We will focus on two particular subordinators: Gamma processes and Inverse Gaussian processes. When the subordinator {Lt }t0 is a Gamma process, the increments Lt+t - Lt have Gamma distribution with parameters t and  . In particular, if  =  = 1/ for some  > 0, the the process (1.7) is called a variance gamma process with parameters µ, ,  . When the subordinator {Lt }t0 is an Inverse Gaussian process, the increments Lt+t - Lt have Inverse Gaussian distribution with parameters t  and  . In particular, if  = 1 and  =  a2 - b for a > 0, -a < b < a and  > 0; then the process Yt = b 2 Lt + BLt , is a Normal Inverse Gaussian process with parameters a, b, . While the variance gamma and Normal Inverse Gaussian processes have been thoroughly studied, we will focus on the more general Gamma and Inverse Gaussian processes. 20

Definition 1.0.11. (Characteristic function) [4][27]. Given a Levy Process X = Xt
t0

with Levy triplet (µ, ,  ), the charac-

teristic function is given by the Levy- Khinchine formula: Xt (u) = E[exp(iuXt )] = exp(-tX (u)) where the characteristic exponent (or Levy symbol) is defined as follows: 1 X (u) = -iµu +  2 u2 + 2 1 - eiu + iu 1||<1  (d )
R

(1.8)

where u  C. The characteristic exponent is completely determined by the Levy triplet of X. In particular,  (d ) is called the Levy density and is known in closed form in the case of Gamma and Inverse Gaussian processes. The Levy density of a Gamma process with parameters  and  is given by:  (d ) =  exp(- )1>0 d. 

The Levy density of an Inverse Gaussian process with parameters  and  is given by:  (d ) =  2 3 exp( 2 /2)1>0 d.

If Yt = XLt is a Time-Changed process, where X and L are independent Levy processes, we can use the tower rule to find a closed for solution to its characteristic function: Yt (u) = E[exp(iuXLt )] = E[E[exp(iuXLt )|Lt ]] (tower rule) = E[exp(-Lt X (u))] = LLt (X (u)), 21

where LLt is the Laplace Transform of the probability density function of the process L at time t, evaluated at the characteristic exponent of X . The characteristic exponent of L can then be recovered by: L (u) = t-1 log LLt (X (u)). (1.9)

The Laplace Transform of the density function of any random variable Mt , evaluated at s  C, is defined as LMt (s)  E[exp(-sMt )]. Definition 1.0.12. (Switching Levy Process) [5].

Assume the filtered probability space (, F , {Ft }tI , P). Consider a collection of independent Levy processes X j = {Xt }j t0 , for j = 1, ..., N , where each process X j has Levy triplet (µj , ( 2 )j ,  j ), as well as a continuous-time Markov chain s = {st }t0 with state space E = {1, 2, ..., N }, and infinitesimal generator matrix Q. We can then define a new process Z = {Zt }t0 where: Zt = Xtst . The process Z switches between the different Levy processes according to the continuous Markov chain s, which is governed by the transition rates in Q. The Switching Levy process is in general not Levy because the increments are no longer stationary. The process s is generally not observable, and must be inferred from empirical data.

In the next section, we introduce the regime switching time-changed 22

model where the Levy processes are now subordinated by pure jump processes.

23

Chapter 2 Pricing Methods
In this chapter we define our model - the regime switching time-changed Levy process and give two pricing methods to find the fair value of European call options, namely pricing by Monte Carlo simulation, and Fourier-Cosine pricing. We define the characteristic function for our model, which will be used when computing the price using the Fourier-Cosine method and also find the conditions under which our model, conditional on each regime, is a martingale.

24

2.1

The Regime Switching Time-Changed Levy Process

Let V = V (T, K ) be the price of a European call option under the risk neutral measure Q. The original Black-Scholes partial differential equation V 1 2 2  2V V +  S - rV = 0, + rS 2 t 2 S S is derived from the Feymann-Kac formula which relates partial differential equations with stochastic processes. This equation can in turn be solved to obtain the Black-Scholes formula. The Black Scholes PDE is derived from the stochastic process {St }t0 where: St = S0 exp(Xt ) where Xt = µt + Bt . (2.1)

The process {St }t0 represents the price movements while {Xt }t0 represents the logarithmic prices. St is the asset price at time t, µ is the constant mean return, also called the drift and  is the volatility. Under this model, price movements are assumed to follow geometric Brownian motion, and therefore follow continuous trajectories. Moreover, the parameters of the process (2.1) are assumed to remain constant with respect to time. We sacrifice both properties in the interest of introducing a more realistic model. This is done by introducing price discontinuities in the form of randomly changing regimes, as well as through time-changed Levy processes.

We fix a maturity time T <  and define a continuous-time Markov chain {st }0tT driving the changes between regimes. We assume our model will 25

jump between two states, and therefore fix the state space of the continuoustime Markov chain to be E = {1, 2}. The switching times are defined as the random variables 1 , 2 , ..  (0, T ] such that:

- tk

lim st = sk for k = 1, 2, ....

With each state j  E there is an associated parameter j > 0 which controls the jump rate between states. For ease of notation and because we are working with two states, we are denoting j to be the parameter which controls the jump time from state j to state (j mod (2) + 1), meaning 1 controls the jump rate from state 1 to state 2 and 2 controls the jump rate from state 2 back to state 1. The infinitesimal generator matrix of the continuous-time Markov chain is given by:   1 2 -1/ 1/ . Q= 1 2 1/ -1/ (2.2)

We also assume a constant risk-free interest rate which remains the same between the two states i.e. r1 = r2 = r  R+ . Consider a collection of independent geometric Brownian motion processes X j = {Xtj }0tT for j  E , where the j 'th process is given by Xtj = µj t +  j Bt , where µj  R and  j > 0, as well as a collection of independent subordinators
j Lj = {Lj t }0tT for j  E where each subordinator L is also independent

of each process X i , for i, j  E . We consider two different subordinators: 26

Gamma processes and Inverse Gaussian processes. Each is characterized by two parameters j ,  j > 0 which change between states. Each process Lj is a pure jump process and each process X j has almost surely continuous paths, therefore the time-changed Levy process, where each random variable Xtj is indexed by the random variable Lj t , is jump process. This can be seen by computing the new Levy measure from equation(1.0.10). We define the collection of time-changed Levy processes Y j = {Ytj }0tT where
j j j j Ytj = XL j = µ Lt +  BLj .
t t

(2.3)

The economic interpretation for evaluating Brownian motion at random times is that the relevant trading times are random [6]. We now define the regime switching time-changed Levy process Z = {Zt }0tT as:
st t s Zt  Ytst where Ytst = µst Ls t +  BLt t .

(2.4)

The regime switching time-changed Levy process Z is assumed to be the log-price process of the underlying asset and the stochastic process of the asset price itself {St }0tT is defined as: St = S0 exp(Zt ). (2.5)

2.2

Simulating a Single Trajectory

One popular method for pricing options is by randomly sampling payoffs at maturity and then computing the sample mean or confidence interval. The 27

class of computational algorithms that rely on random sampling are called Monte Carlo simulation. This method has the advantage of being able to price many different types of financial derivatives which rely on complicated models, (which usually don't have closed form solutions such as in the case of the Black Scholes equation) as well as when the derivative is both path dependent and independent. It is in general very difficult to derive partial differential equations from regime switching models, so Monte Carlo simulations are a popular method for pricing derivatives. Monte Carlo simulations also have the benefit of being able to handle multivariate models (for example if we wanted to simulate a regime process with a larger number of regimes, or with multiple stocks) with low additional computational cost. By contrast, numerical solutions to partial differential equations become computationally expensive in higher dimensional spaces. Monte Carlo simulation relies on the Law of Large numbers. The strong version states that, given a sample of independent and identically distributed ¯n = copies X1 , X2 , ..., Xn of a random variable X , the sample mean X converges to E[X ] almost surely [11]. Equivalently: ¯n = E[X ]) = 1. P( lim X
n 1 n n k=1

Xk

We begin by giving a method for simulating the values of the log-price process Z given by equation (2.4) on an partitioned time interval t  [0, T ] where T <  is the maturity time. The switching time-changed Levy process is more revealing if written in differential form:
st t s dZt  dYtst where dYtst = µst dLs t +  dBLt t .

(2.6)

28

Equation 2.6 is equivalent to (2.4) because conditional on each regime, the continuous-time Markov chain {st }0tT is constant. The simulation is done in two main steps: simulating the continuous-time Markov chain, and simulating the regime switching process. The continuoustime Markov chain spends exponentially distributed random time between transitions. To find the switching times 1 , 2 , ... we simulate exponentially distributed random variables 1 , 2 , ... and the k th switching time is given by:
k

k =
i=1

i

The parameters of the exponential random variables alternate between 1 and 2 , which causes the continuous-time Markov chain to remain in one regime longer than the other, on average. The number of switching times cannot be predicted and varies randomly with each simulation. The differential equation (2.6) can be approximated numerically through finite differences using the Euler Method, in a way similar to the method used in numerical solutions of ordinary differential equations. Conditional on each regime j  E , the increment of the process Z during the time interval t is given by:
j Ztj = µj Lj t +

Lj t N.
t

Here we used the fact that conditional on Lj t , the increment BLj is Brownian motion with variance Lj t [3]: Lj tN

BLj |Lj t 
t

29

where N is a normally distributed random variable with mean 0 and variance 1. The random variable Lj t is a Gamma (or an Inverse Gaussian) distributed random variable with parameters j t and  j . The value of Zt at time t is a summation of the increments:
t

Zt =
i=0

Zi .

Computations for simulating a single trajectory of the switching Levy process on a partitioned interval are summarized in the following algorithm, where the subordinator is a Gamma process. The algorithm is almost identical when the subordinator is an Inverse Gaussian process. 1. Initialize: the log-price process Z0 = 0 regime state j = 1 index k = 1 first stopping time k  exp(j ) 2. Recursively define the continuous-time Markov chain: (a) while k < T, switch regime j = j mod 2 + 1 k =k+1 simulate random variable e  exp(j ) define k = k-1 + e, (b) otherwise stop.

30

3. Define  = [1 , ..., k-1 , T ] as the set of stopping times. (Note k was removed because k > T ) 4. Initialize a partition of the domain into N subintervals of width t = T /N : t = [0, t1 , .., tn , ..., tN -1 , T ] where tn = nt

5. Define T to be the sorted union of  and t in ascending order. Let Tn denote the n th element of T 6. To simulate the trajectory of the log-returns: Reinitialize regime state j=1 7. for 1  n  size(T ) (for each element in T ) (a) initialize Z = 0 (b) simulate a gamma random variable L  Gamma(j (Tn -Tn-1 ),  j ) (c) simulate standard normal variable N  N (0, 1)  (d) Z = Z + µj L +  j LN i. if T is an element of the partitioned time interval (if Tn  t) Zn = Zn-1 + Z Z = 0 ii. if T is a switching time (if Tn   ) switch regimes j = j mod 2 + 1

31

Figure 2.1: Parameters: T = 1, µ1 = 0.01, µ2 = -0.1,  1 = 1,  2 = 5, 1 = 2 = 0.1,  1 = 0.1,  2 = 10, 1 = 0.2, 2 = 0.5.

(a) 1 = 2 = 0.25

(b) 1 = 10, 2 = 0.25

Figure 2.2: Same parameters as Figure 2.1 except for the holding-time rates 1 and 2 .

Figures 2.1 and 2.2 show a single realization of a switching time-changed 32

Levy process (top) as well as the underlying continuous-time Markov chain (bottom), with time domain [0, 1] partitioned into 1000 intervals. Figure (3.2b) is a time-changed Levy process, meaning no regime change has occured. If we have 1   the process will remain in regime 1 indefinitely and therefore it reduces to a time-changed process. The average length of a regime is directly proportional to the holding rate parameter.

2.3

Pricing European Options under Monte Carlo Simulation

For all option pricing methods, we assume that the present value of an option is equal to the discounted expectation of its payoff at maturity time, i.e. we will work under an equivalent martingale measure (EMM) Q. A measure Q is an EMM if: 1. Q and the real world probability measure P are equivalent i.e. they share the same null sets; 2. the discounted price process is a martingale under Q. There are several criteria to change the probability to a risk-netural setting. See for example [12] for a change based on an Esscher transform or [8] for a minimum entropy criterion. In this thesis we will not address this problem; we simply change the drift leaving the remaining parameters constant, as explained in section 2.5. At time t, the value of a European option is given

33

by [8]: V (t) = e-r(T -t) EQ [H (ST )|Ft ] for 0  t  T. Here H is the payoff function, which for European vanilla options is a function of the asset price at maturity given by: H (ST ) = max{ (ST - K ), 0} where K > 0 is the strike price, and  = +1 for call options or  = -1 for put options. If the payoff of the option is path independent and the underlying model of the log-return process is an ordinary Levy process, the payoff at time T can be simulated in one step because of the property of stationary increments. When the log-return process is a switching Levy process, the increments are independent only conditionally on each regime. We devise an algorithm which computes the payoff of a European Call option by simulating N independent realizations of the process Z simultaneously and then calculating the payoff according to: 1 C (T ) = exp(-rT ) N
N

{(S0 exp(Zn,1 ) - K ), 0}
n=1

where Zn,1 is the n th element of the N -by-1 array Z:,1 is at maturity T, and the expected value is approximated by the sample mean. The simulation of all N values at each step simultaneously allows for a much smaller computation time than simulating a single process at a time. The switching times are now contained in a (N -by-k ) matrix  where m,n is the n th switching time of the m th simulation. The number of columns 34

of  changes randomly with each simulation. The n th column of  (denoted as :,n ) is generated by simulating an (N -by-1) array of exponential random variables (all with the same parameter) and adding it component-wise to the (n - 1) th column of  : :,n = :n-1 + [e1 , ..., eN ]T where ei  exp(j ), 1  i  N,

and j  {1, 2}. The j parameter alternates between 1 and 2 after each step. The process of appending columns to  continues until each element of the final column is greater or equal to T. Then, for each pair of indices m, n such that m,n > T we redefine that element to equal T, because the stochastic process {Zt }0tT is defined only up to T. This is not the most efficient method as we have to simulate more random variables than is actually needed. This is because each of the processes require a different amount of steps to arrive at maturity. However, because we need to keep the number of steps the same for all paths, we add 0 instead of exponential random variables for all paths that finish early, and this amounts to replacing any elements greater than T by T . Because of this caveat, we also extend the domain of the parameters in the case of the subordinator to allow for the first parameter to equal 0. For example: if t = 0 and ,  > 0, then Lt  Gamma( · 0,  )  0. The same holds for the Inverse Gaussian process. Nevertheless, as seen in Table (2.1), simulating through vectorization is much more efficient than simulating one process at a time.

35

Table 2.1: Comparison of European call payoffs under vectorizatrion vs. simulating one trajectory at a time, with parameters T = 1, 1 = 1, 2 = 0.5

Number of Simulations

Computational time under vectorization (sec.)

Computational time under one by one simulation (sec.) 0.012886 8.238712 879.354925

N=10 N=104 N=106

0.001915 0.085080 9.617767

We note that computational time is very sensitive to the size of the matrix  . In particular, if 1 , 2 columns on average. T , the matrix  has around T /(1 + 2 )

Computing the switching Levy process is more straightforward; instead of simulating a value at each point in a partition, we simulate the increments in each regime. At each step, t holds the duration at which each process remains in a single regime. L is an array of random variables depending on the subordinator with each element having a different first parameter. Computations for the payoff of a European call option are summarized in the following algorithm, where the subordinator is a Gamma process. A similar algorithm holds when the subordinator is an Inverse Gaussian process. 1. Initialize: (a) regime state j = 1 (b) index k = 2 36

(c) first stopping time (N -by-1 matrix) given by :,k = [e1 , ..., eN ]T where en  exp(j ) for n = 1, ..., N 2. Recursively define the continuous-time Markov Chain: (a) while there exists at least one indice n such that n,k < T, · redefine k = k + 1 · switch regime j = j mod 2 + 1 · define the k th column (N -by-1) array given by :,k = :,k-1 + [e1 , ..., eN ]T where each en  exp(j ) for n = 1, ..., N (b) otherwise stop. (c) Define (N -by-1) array :,1 = [0, ..., 0]T and redefine the (N -by-1) array :,k = [T, ..., T ]T (d) Define the (N -by-k ) matrix  = [:,1 , ..., :,k-1 , :,k ] (e) For each pair of indices m, n such that m,n > T , redefine m,n = T 3. To simulate the switching process: (a) Reinitialize initial regime state j = 1 (b) Initialize (N -by-1 array) Z:,1 = [0, ..., 0]T 4. For 2  k  length(1,: ) (a) define t:,1 = :,k - :,k-1 (b) simulate (N -by-1) gamma random variable L:,1  Gamma(j t:,1 ,  j ) (c) simulate (N -by-1) array of standard normal variables N:,1

37

(d) Z:,1 = µj L:,1 +  j

L:,1 N:,1

(multiplication of column arrays is done component-wise) (e) Z:,1 = Z:,1 + Z:,1 5. Expected Call Option Payoff
1 C= exp(-rT ) N N n=1 ((S0

exp(Zn,1 ) - K )+ .

(a) Inverse Gaussian subordinator, µ1 = 0.3204, µ2 = 0.6450

(b) Gamma subordinator, µ1 = -0.2316, µ2 = 0.0541

Figure 2.3: European call payoff as a function of T and K under two different subordinators with risk neutral drift (see section 2.5). The other parameters are identical for each figure:  1 = 0.03,  2 = 0.7, 1 = 2 = 0.1,  1 = 1,  2 = 1.2, 1 = 0.4, 2 = 1.

Figures (2.3a) and (2.3b) demonstrate the behaviour of the payoff of a European call option under the regime switching time-changed Levy process model under Inverse Gaussian and Gamma subordinators. Both payoff models are monotone in T and K . Moreover as T increases, the expected payoff increases. For K >> S0 the probability that ST  K is very small, therefore 38

the payoff is close to 0. We can also estimate the price using confidence intervals. The confidence interval is useful because it provides a range of values that are likely to contain the population mean. At the  = 95% confidence level, the endpoints of the confidence interval are given by: s x ¯ ± z  , N where x ¯ is the sample mean, s is the standard deviation of the sample, N is the sample size and z0.95 = 1.96 is the z-score at the 95% confidence level.

(a) Gamma subordinator

(b) Inverse Gaussian subordinator

Figure 2.4: Comparison of long term behaviour of the 95% confidence interval under different subordinators with identical parameters, including  1 = 0.1,  2 = 0.01.

The confidence interval decreases as the number of simulations approaches infinity, however in the case of the Inverse Gaussian subordinator, the interval is larger at each simulation because Inverse Gaussian random variables have a larger variance than Gamma random variables (see section 3.10) when 39

 < 1. Figures (2.4a) and (2.4b) show the behaviour of the payoff as different parameters are varied. At N = 104 , the confidence interval when the subordinator is Inverse Gaussian is: [18.7353, 19.1168]. When the subordinator is a Gamma process, the confidence level is: [ 17.8768, 17.9136].

(a) Payoff as a function of the initial prices when K = 1.

(b) Payoff as a function of  1 and  2

Figure 2.5: Behaviour of the payoffs when the parameters of the subordinator are varied.

40

(a) Payoff as a function of parameters (b) Payoff as a function of parameters 1 , 2 1, 2

Figure 2.6: Behaviour of the payoffs when the parameters of the subordinator are varied.

(a) Payoff as a function of parameters (b) Payoff as a function of parameters 2 ,  2 1 , 2

Figure 2.7: Behaviour of the payoffs when varying different parameters.

In (almost) each case, parameters between states were held constant un41

less they were being varied: µ1 = µ2 = 0.01, 1 = 2 =  1 =  2 = 0.1,  1 =  2 = 0.01, 1 = 2 = 0.25, r = 0.04, T = 1, S0 = 20 and K = 1. Setting the parameters 1 = 2 implies the process spends an equal amount of time in each regime, on average. The only exception made is in Figure (2.7a), where 1 = 0.001 and 2 = 10 so that the process would spend a majority of the time in the second regime; this makes the process an approximation to a time-changed Levy process. When plotting Figure (2.7b), the domain had to be relatively small as the payoff varied greatly with changes in 1 , 2 .

The payoff of the European call option is monotone with respect to every parameter. In Figure (2.5b), the payoff increases as  1 and  2 increase. For changes in  1 ,  2 , the payoff approaches an asymptote because Gamma and Inverse Gaussian random variables both have mean /, which approaches infinity for   0+ . In Figure (2.7b), it is shown that the payoff decreases when 1  0+ , because the process then spends more time in regime 2 on average, and we let µ2 = -µ1 < 0. By comparing the payoff at different maturities Table (2.2), we demonstrate that the regime switching time-changed Levy process has more variability compared to the payoff under Black Scholes under identical parameters. This variability can be adjusted at will because the switching model has two additional types of parameters comparing to Black Scholes, namely the parameters characterizing the subordintor:  and . The difference in the payoff between subordinators can be seen by the fact that, even though the expected value is the same in both cases, namely /, the variance (and 42

Table 2.2: Comparison of various European call payoffs under different strike prices and maturities. We compare Black Scholes to the switching model under Gamma and Inverse Gaussian subordinators. We set r = 0.04,  1 =  2 , 1 = 2 ,  1 =  2 , µ1 = µ2 where µ1 is the Black Scholes risk neutral drift.

(T, K )

Gamma subordinator

Inverse Gaussian subordinator 18.2022 19.6440 16.0378 16.7613

Black Scholes model 19.0368 19.0768 18.0784 18.1537

(1, 1) (2, 1) (1, 2) (2, 2)

18.3364 18.5638 17.0453 17.5120

other moments) differs. In the case of a Gamma subordinator, the variance is equal to / 2 , and in the case of an Inverse Gaussian subordinator, the variance is equal to / 3 .

2.4

Characteristic function of the Regime Switching time-changed Levy Model process

For any regime switching Levy process M = {Mt }t0 which switches between n Levy processes X 1 , ..., X n according to the continuous-time Markov chain {st }t0 with generator matrix Q, the characteristic function is given by Choudarski [5]:

E(x,) exp(iuMt ) = exp(iux) · [1T · exp(t · (u)) · ],
43

(2.7)

where  is the initial regime distribution - an n × 1 array where the j 'th element is the probability that Z will begin in state j , i.e. s0 = j . The term x  log S0 is the initial log-price of the underlying asset, 1 = [1, 1, ..., 1]T is a n × 1 vector and (u) is an n × n matrix with elements:    q (i, i) + X i (u), if j = i [(u)]i,j =   q (j, i), otherwise. Here q (i, j ) is the i, j 'th element of the infinitesimal generator matrix Q and X i (u) is the characteristic exponent of the i'th base process X i . The exponential term refers to the exponential of a matrix. If  is a n × n matrix:


e =
k=1

k . k!

We attempted to simplify the exponential by using singular value decomposition however, this resulted in eigenvalues and eigenvectors that did not simplify the computations. Instead, the method used to numerically compute the exponential is the scaling and squaring algorithm [14]. The method is based on the following approximation: e = (e2
-s 

)2  rm (2-s )2 ,

s

s

where rm (x) is the [m/m] Pade approximant of ex and the nonnegative integers m and s are chosen in such a way as to achieve minimum error at minimal cost. A table of errors as a function of s and m is given in [1]. The [k/m] Pade approximant for the exponential function is given by rkm(x) = pkm (x)/qkm (x), 44

where
k

pkm (x) =
j =0

(k + m - j )!k ! xj , qkm (x) = (k + m)!(k - j )! j !

m

j =0

(k + m - j )!m! (-x)j . (k + m)!(m - j )! j !

We develop the characteristic function for the regime switching timechange Levy process where conditional on each regime, the Levy process is Brownian motion with a subordinator. Given the process Z defined in equation (2.4) along with a continuoustime Markov chain with the infinitesimal generator matrix Q defined as equation (2.2), the characteristic function is given by equation (2.7) where:    q (i, i) + t-1 log LLi (X i (u)), if j = i t [(u)]i,j =   q (j, i), otherwise. Here the term t-1 log LLi (X i (u)) is the characteristic exponent of the i'th t time-changed Levy process Li = {Li t }t0 . We will now find the characteristic function for switching time-changed Levy processes under different subordinators. The set of regimes is again E = {1, 2} and we assume that the process will always start out in state 1 with probability 1, and hence the initial probability distribution is given by  = [1, 0]T . Working conditionally on each regime j  E, the characteristic exponent of the j 'th base process {Xtj }t[0,T ] where Xtj = µj dt +  j Bt with j 'th Levy triplet (µj , ( j )2 , 0) is derived from equation (1.8): X j (u) = -iµj u + ( j )2 u2 /2. The associated Laplace transform LLj (s) of the density function of the rant

dom variable

Lj t

is derived from the characteristic function via a change of 45

variables when the characteristic function is known in closed form. When the subordinator Lj is an Inverse Gaussian process with shape parameter j and rate parameter  j , we have: LLj (s) = exp(j t 2s + ( j )2 -  j ).
t

Therefore the characteristic function of the j 'th time-changed process Y j =
j { XL } under Inverse Gaussian subordinator is: t t0

Y j (u)  E[exp(iuXLj )] = exp(j t 2(-iµj u + ( j )2 u2 /2) + ( j )2 -  j )
t t

(2.8)

(a) The real part of Yt (u)

(b) The imaginary part of Yt (u)

Figure 2.8: The Characteristic function Yt (u) with parameters  = 0.1,  = 0.1, µ = 0.01,  = 0.01, t = 1/250 where the subordinator L has increments with Inverse Gaussian distribution.

We now give the characteristic function of Z = {Zt }t0 where Zt = Ytst and the subordinator is an Inverse Gaussian process:

46

Corollary 0.1. The characteristic function of Zt under an Inverse Gaussian subordinator is given by: Zt (u) = E[exp(iuZt )] = exp(iux) · [1T · exp(t · (u)) · ], where     -1/j + j ( 2(-iµj u + ( j )2 u2 /2) + ( j )2 -  j ), if j = i otherwise. (2.9)

[(u)]i,j =

  1/j ,

(a) The real part of Zt (u)

(b) The imaginary part of Zt (u)

Figure 2.9: The Characteristic function Zt (u) with parameters 1 = 0.1 = 2 ,  1 = 0.1,  2 = 0.01, µ1 = 0.01, µ2 = -0.01,  1 = 0.01,  2 = 0.05, t = 1/250 where conditionally on each regime, the subordinator L has increments with Inverse Gaussian distribution.

When the subordinator Lj is a Gamma process with shape parameter j and rate parameter  j , we have: LLj (s) = 1 +
t

s j

-j t

47

j Then the characteristic function of the j 'th time-changed process Y j = XL

is: Y j (u)  E[exp(iuXLj )] = 1 +
t t

-iµj u + ( j )2 u2 /2 j

-j t

.

(2.10)

(a) The real part of Yt (u)

(b) The imaginary part of Yt (u)

Figure 2.10: The characteristic function Yt (u) with parameters  = 0.1,  = 0.1, µ = 0.01,  = 0.01, t = 1/250 where the subordinator L has increments with Gamma distribution.

We now give the characteristic function of Z = {Zt }t0 where Zt = XLt and the subordinator is an Inverse Gaussian process: Corollary 0.2. The characteristic function of {Zt }t0 under a Gamma subordinator is given by equation (2.9) where:    -1/j - j log(1 + (-iµj u + ( j u)2 /2)/ j ), if j = i [(u)]i,j =    1/j , otherwise.

48

(a) The real part of Zt (u)

(b) The imaginary part of Zt (u)

Figure 2.11: The characteristic function Zt (u) with identical parameters as 2.11, but now conditionally on each regime, the subordinator L has increments with Gamma distribution. Note the thicker tails.

2.5

Pricing under a Risk Neutral Measure

Consider a Market model consisting of one riskless asset B = {Bt }t0 where Bt = exp(rt) for some r  0, and exponential Levy processes S j = {Stj }t0 ~j = {S j }t0 are where Stj = S0 exp(Ytj ). The discounted price processes S t defined as ~j = exp(-rt)S j S t t where j = 1, 2.

It is convenient to work under a risk neutral measure, also called an equivalent martingale measure (EMM) Q when pricing contingent claims on a stock, as discounted prices are Q-martingales. If the risky asset is modelled under an underlying Levy process with jumps, then the market is incomplete, and there are infinitely many EMMs. This in turn implies that the parameters, 49

under which the underlying process is a martingale under Q, are not unique. From [23] we have that under an EMM Q, the discounted price process ~j is a martingale under Q if and only if the following equation is satisfied: S Y j (-i) = r, (2.11)

where Y j (u) is the characteristic exponent of the Levy process Y j . The process Y j is time-changed Brownian motion 2.3; when the subordinator is the Gamma process with parameters j ,  j , the characteristic function of the process is given by equation (2.10) and hence in each state j  E we solve:
j Gamma (-i) = t-1 log

iµj u - ( 1+ j
j

j )2 u2

- j t

2 u=-i

 r = -j log(1 +

-µ + ( ) /2 ) j r ( j )2 µj = - j (exp(- j ) - 1) + .  2

j 2

(2.12)

Holding all the other parameters constant, the drift verifies equation (2.11). The value of µj is such that the j 'th discounted price process when the subordinator is a Gamma process is a martingale. When the process Y j is a time-changed process subordinated by an Inverse Gaussian process with parameters j ,  j , the characteristic function is given by equation (2.8) and therefore for each state j  E we solve for µj :
j IG (-i) = t-1 log exp(-j t

2(-iµj u +

( j )2 u2 ) + (  j )2 -  j ) 2

u=-i

 r = -j

( j )2 ) + (  j )2 -  j 2 1 j r 2 ( j )2 j 2 j µ = [( - j ) + ( ) ] + . 2  2 2(µj - 50

(2.13)

Holding all the other parameters constant, the drift verifies equation (2.11). The value of µj is such that the j 'th discounted price process when the subordinator is an Inverse Gaussian process is a martingale.

We can reduce the regime switching Levy process (2.4) to the Black Scholes model (2.1) by defining the subordinator Lt = t and setting the parameters µ, , ,  equal across all regimes: µ1 = µ2 , 1 = 2, 1 = 2 ,  1 =  2.

Setting the drift such that the discounted Black Scholes price process is a martingale: 1 µ1 = r - ( 1 )2 , 2 we find that the expected European call payoff is close to the payoff given by the famous Black Scholes formula:
Table 2.3: European call option payoff comparison between the Black Scholes formula and Monte Carlo simulation of the reduced switching Levy process at different parameters

Parameters (T, K, r,  ) (1,1,0.04,0.5) (3,1,0.1,1) (2,30,0.5,0.001)

Reduced Switching Levy Model (# simulations N = 106 ) 19.0463 19.2955 8.963608

Black Scholes formula

19.0392 19.3139 8.96361

51

2.6

Fourier-Cosine Method

An alternative method of finding the fair price of options is via a FourierCosine Series expansion. The derivation of the following method is given by [9]. For many stochastic processes, the probability density function is not known, however, the characteristic function might be known explicitly, as in the case of Levy processes where the characteristic function is computed using the Levy-Khintchine formula (1.0.11). The risk-neutral valuation for European options is as follows: v (x, t0 ) = e-rt EQ [v (y, T )|x] = e-rt
R

v (y, T )f (y |x)dy,

(2.14)

where v (x, t0 ) is the payoff of the option as a function of the log-asset price x := ln(St0 /K ) and initial time t0 and v (y, T ) is the payoff at maturity time T and log-asset price y := ln(ST /K ). S0 and ST are the prices of the underlying asset at t0 and T respectively. t = T - t0 is the interval between the initial time and maturity time, K is the strike price and r is the risk-neutral interest rate. EQ is the expectation value operator with respect to some risk-neutral probability measure Q and f (y |x) is the probability density function of y given x. There are different ways of computing the integral in (2.14) such as using an Inverse Fast Fourier Transform method, or by approximating the expected value using Monte Carlo simulation. In this section, the solution to (2.14) is given by truncating the integral appropriately and replacing the unknown density function f with the characteristic function  which was derived for regime switching time-changed Levy processes under Gamma and Inverse Gaussian subordinators in the previous section. 52

The domain of integration can be truncated to a finite interval for the purposes of numerical integration, because in order for the integral in (2.14) to be finite, the integrand must converge to 0 at ±. Fourier-Cosine expansions have been shown to give an optimal approximation of functions with finite support, while the usual Fourier series expansion is optimal when the function is periodic[17]. By choosing an interval [a, b]  R appropriately, the density function f can be approximated by a function f which is equal to f on [a, b] and 0 otherwise. We denote approximations to f and later Ak ,  and v by f , Ak ,  and v respectively. Every density function f has an associated characteristic function  which is the Fourier Transform, and similarly for f :
b

 (u) =
R

eiux f (x)dx and  (u) =
a

eiux f (x)dx,

Given the function f (y |x) compactly supported on [a, b], its Fourier-cosine expansion is given by:


f (y |x) =
k=0

Ak (x) · cos k

y-a , b-a

(2.15)

where the first term of the summation is weighted by one-half. The coefficients of f are denoted by Ak and the coefficients of f1 are denoted by Ak . The terms Ak (x) are given by: Ak (x) = 2 b-a
b

f1 (y |x) · cos k
a

y-a dy. b-a

(2.16)

The coefficients Ak written above can be rewritten in terms of the characteristic function: A k ( x) = 2 k a Re  ; x · exp - ik b-a b-a b-a 53 .

If the interval of support [a, b] is chosen so that the integral on [a, b] approximates an infinite integral appropriately, we can then approximate 1 with  which is known, and Ak with Ak : Ak (x)  Ak (x) = k a 2 Re  ; x · exp - ik b-a b-a b-a .

The payoff can be approximated as:
b 

v (x, t0 )  v1 (x, t0 ) = e

-rt a

v (y, T )
k=0

Ak (x) cos k

y-a dy. b-a

(2.17)

If we define Vk to be the Fourier coefficients of v (y, T ): Vk := we can rewrite 2 b-a
b

v (y, T ) cos k
a

y-a dy b-a

(2.18)

1 v1 (x, t0 ) = (b - a)e-rt Ak (x)Vk . 2 k=0 Our pricing formula is now in terms of Ak and Vk , which are the Fourier coefficients of v (y, T ) and f (y |x) respectively. Putting everything in and truncating the infinite series to N terms, we get the generic payoff formula:
N -1



v (x, t0 )  e

-rt k=0

Re 

a k ; x e-ik b-a Vk . b-a

(2.19)

The payoff of an option v can be computed from of the characteristic function instead of the density function. The characteristic function is computed in section 2.4 for Inverse Gaussian and Gamma subordinators. Fourier-cosine series of entire functions converges exponentially, so N need not be very large for good approximations [9].

54

The terms Vk defined in equation (2.18) depend on the option payoff formula. For European call and put options the expression for Vk can be obtained analytically. The payoff for European Vanilla options as a function of the log-asset price is:     1 for a call option

v (y, T )   · K (ey - 1)

+

with  =

  -1 for a put option. For the call option: VkCall = 2 b-a
b

K (ey - 1) cos k
0

y-a dy, b-a

and the solution is found by using integration by parts: Vk = where: 2 K (k (0, b) - k (0, b)) b-a (2.20)

k (c, d) =

1 1+
k 2 b-a

cos k

d-a d c-a c e - cos k e b-a b-a

+ and k (c, d) 

d-a d k c-a c k sin k e - sin k e . b-a b-a b-a b-a

   

-a -a sin k d - sin k c b-a b-a

b-a k

k=0 k = 0.

  d - c

For the European put option, the Fourier coefficients are found to be: VkP ut = 2 K (-k (a, 0) + k (a, 0)) b-a 55 (2.21)

In pricing European call options, it was found that the payoff was not accurate and extremely sensitive to the values of b. It was also found that for large values of b, VkCall diverged to ±, while VkP ut converged quickly and varied little with changes in a. We therefore rely on the put-call parity which allows for the computation of the European call option using the put option: v Call (x, t0 ) = v P ut (x, t0 ) + S0 exp(-qT ) - K exp(-rT ), where q is the number of dividends. We always take q = 0. It was found that the payoff under the call-put parity varied very little for large a and b, so it was arbitrarily chosen for a = 0 and b = 10.

2.7

Pricing European Call Options under the Fourier-Cosine Method

We now give an algorithm for pricing European call options using FourierCosine Pricing (2.19) : 1. Initialize appropriate boundary points a, b such that the truncated integral will appropriately approximate the integral over R, constant risk-neutral interest rate r, initial stock price S0 , strike price K , with x := S0 /K time increment t = T , number of steps N 2. Initialize N × 1 array of payoffs v P ut and v Call 3. for k = 0 to N - 1 56

(a) Define the k th element of v P ut to be: v P ut (k ) = e-rT Real Zt (k/(b-a); x) exp(-ik (a/(b-a)) VkP ut ;

where Zt depends on the choice of the subordinator, equation (2.9) for Inverse Gaussian processes or equation (2.10) for Gamma processes respectively. VkP ut is defined in (2.21) (b) v Call (k ) = v P ut (k ) + S0 - Ke-rT
1 Call 4. vf inal = 2 v (1) + N -1 k=1

(put-call parity)

v (k )

(Summation)

We compare the European call payoff and running time under Monte Carlo simulation and Fourier-Cosine pricing in table 2.4 : It was found that
Table 2.4: Comparison of European Call option Payoffs using Monte Carlo Simulation and Fourier-Cosine Pricing, as well as their computational times.

(T, K ) (1, 1) (2, 1) (1, 2) (2, 2)

Monte Carlo 18.9554 19.9612 17.9942 19.0166

Running Time (sec.) 9.31 17.54 10.01 11.40

Fourier-Cosine 19.0401 20.3456 18.2164 18.5523

Running time (sec.) 0.0234 0.1433 0.1339 0.193

the difference between pricing European call options using Monte Carlo and Fourier-Cosine pricing remained constant for different strike prices. The error however grows linearly for increasing maturity times.

57

(a) Error as a function of K

(b) Error as a function of T

Figure 2.12: The difference between Fourier-Cosine Pricing and Monte Carlo, as a function of the strike price K and time to maturity K .

58

Chapter 3 Parameter Estimation and Calibration
In this chapter we estimate the value of the parameters using historical data. We use 4 different techniques: calibration using option data, method of moments, minimum distance estimation and maximum likelihood method. The data used was option and historical data of energy commodities - namely oil and electricity - because energy prices often exhibit sudden, drastic changes which make them suitable candidates for study under regime switching models.

We use two approaches of fitting the parameters of the underlying model to financial historical data: calibration and estimation. In calibration, the parameters are estimated by minimizing the error between option payoffs obtained numerically and option quotes. The option quotes are taken at a 59

variety of strike prices and times to maturity from Bloomberg. In parameter estimation, we use a variety of techniques based on historic asset prices: method of moments, minimum distance method and maximum likelihood estimation.

For the estimation methods we use daily historical NYMEX West Texas Intermediate (WTI) crude oil futures (11-16-2012 to 06-05-2018) and IESO Ontario Zone 24H electricity average spot prices (06-06-2008 to 06-05-2018). For calibration, the parameters are estimated using European call option quotes of WTI crude oil. For convenience we assume that there are 250 trading days in a year with each trading day corresponding to t = 1/250 of unit time. The codes for numerical computation of the methods are found in the appendix. Figure 3.1 plots the historic futures and average spot prices, as well as the log-returns. For historic electricity spot prices, the price sometimes moves below zero, implying a surplus of electricity produced during low demand. Because electricity produced by power suppliers must be consumed immediately, the supplier pays wholesale customers to buy the surplus energy [25]. All negative prices were modified to CAD $0.01 for estimation purposes. We also compare the empirical density function of the log-returns of each commodity to the normal distribution under the same mean and variance parameters as the historical log-return process. The density kernel method is summarized in 3.4. Table 3.1 summarizes common moments for each commodity, as well as 60

(a) WTI futures

(b) Average electricity spot price

(c) Log-returns of WTI futures

(d) Log-returns for Electricity spot price

Figure 3.1: Price Process and Log-returns for two different commodities
Source: Blooomberg Terminal, April 2018

for the log-return prices of each commodity. The kurtosis for both log-returns is much larger than that of the normal distribution, implying heavier tails and high risk assets. The variance of the log-returns of electricity spot prices is several orders of magnitude larger than the variance of oil futures and this difference can be seen qualitatively by comparing the difference of the range of the log-return processes.

61

(a) oil log-returns

(b) electricity log-returns

Figure 3.2: Empirical density functions vs. normal distrubtions Table 3.1: Standard statistics of commodities

Commodity WTI crude oil futures WTI crude oil log-return Electricity spot price Electricity spot price log-return

Mean 65.777 -1.8577e-4 28.011 -01.2039e-3

Variance 202.11 1.5707e-4 380.45 1.4581

Skewness 0.1750 -0.1314 3.2243 -0.1377

Kurtosis 1.4890 6.3383 27.368 23.947

Source: Bloomberg Terminal, April 2018

In our model 2.4, the regime switching time-changed Levy process can be in one of two states, therefore the parameters to be estimated are the holding-rate parameters j and the four parameters of each time-changed Levy process j = (µj ,  j , j ,  j ) for j = 1, 2. The locations of the regime changes are estimated by inspecting changes in the statistics of different regions of the historical log-return price processes. The random holding times are assumed to have exponential distribution. If X is an exponentially

62

distributed random variable with parameter , the expected value is given by E[X ] = . We assume that the duration of the j th observed historic regime is the most probable value i.e. it is equal to the expectation value j . For each commodity, the j th holding-rate parameter is given by: j = total number of days in regime j , number of occurrences of regime j (3.1)

where it is assumed that there are 250 trading days every year. By inspecting the log-return process data of oil futures (3.1c), we define the process to be in regime 1 between 11-16-2012 and 11-16-2014 as well as between 02-06-2017 and 06-05-2018; otherwise, we assume that the process is in regime 2. In the case of the log-returns of electricity spot prices; we define the process be in regime 2 whenever the absolute value of the log-returns exceeds 3 and in regime 1 otherwise. We also include the variance of the log-returns within each regime; the different orders of magnitude between regimes justifies the use of a switching model.

By having defined the location of the regime changes and therefore estimated the values of 1 , 2 , the historic log-returns are separated into two sets of data, one containing all the data points for each regime. Each of these two new processes is assumed to follow a time-changed Levy process and we are left with estimating j = (µj ,  j , j ,  j ) in each regime. 63

Commodity Oil Electricity

^1  0.900 0.2618

^2  3.80 0.0081

Variance (regime 1) 2.6669e-05 0.3624

Variance (regime 2) 2.1891e-04 37.3146

Table 3.2: Holding-rate parameters estimation for each commodity as well as the variance in each regime

The two states follow different parameters and are assumed to be uncorrelated. Define j  R4 to be the set of all feasible parameters for j . We assume that the two sets of parameters belong in different parameter spaces i.e. 1 = 2 . The two parameters of the subordinator and the diffusion coefficient are required to be positive, therefore we add the following natural constraints:  j , j ,  j > 0.

3.1

Calibration

In calibration we are given a set of option quotes with maturity times at a future date and therefore we need to estimate the future regimes using our estimated holding-rate parameters. We assume that the sequence and duration of future regimes follow the most probable path, therefore in the case of oil prices, the estimated holding-rate parameters imply that the continuoustime Markov chain remains in regime 1 for 0.9 · 250 = 225 trading days and then switches to regime 2 for 3.8 · 250 = 950 trading days. Table 3.2 gives the estimated holding-time rates for each commodity. To estimate the parameters j = (µj ,  j , j ,  j ) for j = 1, 2, within each

64

regime we minimize the root mean square error between the numeric option payoffs and European call option quotes. The numeric option payoff is simulated using Monte Carlo simulation (section 2.3), because the Fourier Cosine method exhibits significant error when the option is out of the money. We define the j 'th objective function to be the root mean square error between
j the numeric payoffs Callnumeric (; T, K ) - primarily a function of the paramj eters, and n option quotes Callhistorical (T, K ), taken over a range of strike

prices K and maturity times T which reside in regime j : J j ( ) = 1 n
j j (; T, K ) - Callhistorical (T, K ))2 , (Callnumeric T,K

j  {1, 2}. (3.2)

^j = (^ ^j ) to be the estimate for the true For each j , we define  µj ,  ^j ,  ^j ,  parameter j = (µj ,  j , j ,  j ): ^j = arg min J j (). 
j

(3.3)

We compute equation (3.3) numerically using the gradient method [30]. ^ using the following algorithm: The gradient descent algorithm approximates  1. Initialize the iterative step t = 0, the step size  > 0 and a starting point 0 . Define some stopping criteria. 2. Repeat until stopping criteria is reached: (a) Redefine t+1 = t -  J (t ) 65

(b) Iterate t = t + 1. Here J is the gradient of the cost function,  > 0 is the step size chosen accordingly. The fmincon function in MATLAB uses a variant of the gradient method. The particular variant we use is the Trust Region Reflective algorithm: instead of directly minimizing the cost function J (), the algorithm minimizes the first two terms of the Taylor series of J in a spherical neighborhood N called the trust region: 1 min q (s) where q (s) = sT 2 Js + sT J sN 2 and N = { Ds  r} (3.4) · is the

where 2 J is the Hessian matrix of J, D is a diagonal matrix,

2-norm and r is some positive scalar. The trial step s which minimizes (3.4) can be solved in a variety of ways such as a variant of Newton's method [16]. The modified algorithm is then given by: 1. Define D, r, initial  and s. 2. Repear until stopping criteria is reached: Find the trial step s such that s minimizes (3.4) (a) If J ( + s) < J (), define  =  + s (b) Else : Decrease r (This corresponds to shrinking the neighborhood N) and repeat the process We apply the above algorithm once for each regime. The final set of estimated parameters gives the best fit of the regime switching time-changed 66

Levy process to the option quotes - on average.

Table 3.3: Parameter Calibration using Root Mean Square Error

Commodity (subordinator) Oil log-return Regime 1 (Gamma) Oil log-return Regime 2 (Gamma) Oil log-return 1 (Inverse Gaussian) Oil log-return 2 (Inverse Gaussian)

µ ^ -0.03387 -0.01445 -0.04976 -0.04950

 ^ 0.0030 1.116184 0.130011 0.515891

 ^ 2.640710 2.56567e-5 0.24788 8.531e-4

^  1.007e-8 10.32441 92.6926 8.43091

The stopping criteria is taken to be step tolerance, taken to be equal to 1e10. The step tolerance is a lower bound on the size of the step (t - t-1 ). The solver stops if the stopping criteria is reached, or if the maximum number of iterations (fixed to 1000 steps) is exceeded. One payoff of the Monte Carlo Simulation is computed using 1e6 simulations of the switch process and different initial starting points were found to give similar estimation of the parameters. Table 3.3 gives the estimated calibration in the case when the subordinator is a Gamma process and an Inverse Gaussian process. As ^ 1 in the case of both subordinators. expected,  ^2 > 

3.2

Method of Moments

The method of moments is a method used to estimate the parameters (again, possibly vector valued)  from the historic log-return prices x = (x1 , ..., xn ) by solving a system of equations [31]. Suppose each element of the sample is a 67

realization of some unknown random variable X with unknown distribution f (x; ). Furthermore assume that the first d moments µ1 , . . . , µd can be written as functions of the parameters: µ1 = E[X ] = g1 () . . . . . .

µd = E[X d ] = gd () The idea is to find the j sample moments for each j = 1, . . . , d defined as: 1 µ ^j = n
n

xj i
i=1

(3.5)

and then to solve the following system of equations: 1 µ ^1 = n . . . µ ^d = 1 n
n n

^) xi = g1 (
i=1

. . . ^ xd i = gd ( )
i=1

^. The method of moments is based on the for the vector valued estimator  law of large numbers, namely that the sample moments converge to the moments of the distribution with probability 1 as the number of observations increase.

The moments can be computed from the derivatives of characteristic function, which are known for time-changed Levy processes:

E[X n ] = i-n

dn  (u) dun

(3.6)
u=0

68

This results in a system of nonlinear equations, which needs to be solved numerically. When the subordinator Lt has Inverse Gaussian distribution with ,  , the resulting increments of length t have characteristic function:

Lt (u) = exp(-t( 2(-iµu + (u)2 /2) +  2 ) -  )),

(3.7)

where again µ  R and , ,  > 0 and t = 1/250. Computing the first four moments results in a system of nonlinear equations: 0 = -µ ^1 + (µt)/ 0 = -µ ^2 + (t(µ2 +  2  2 + µ2 t))/ 3 0 = -µ ^3 + (µt(3µ2 + 3 2  2 + 3µ2 t + 2  2 µ2 t2 + 3 3  2 t))/ 5 0 = -µ ^4 + (t(15µ4 + 3 4  4 + 18 2 µ2  2 + 15µ4 t+ 62  2 µ4 t2 + 3  3 µ4 t3 + 3 5  4 t + 62  4 µ2  2 t2 + 18 3 µ2  2 t))/ 7 . The system of equations is solved separately for each regime, therefore for two commodities there are a total of 4 systems of equations. The solution must at minimum satisfy the natural constraints: , ,  > 0. We used the function fsolve, which solves a system of equations numerically using the trust region alogrithm, similarly to the function fmincon. The results are summarized in Table 3.5.

For convenience, we re-parametrize the Gamma subordinator. When the subordinator Lt is Gamma distributed with new parameters ,  where  = / > 0 , the resulting increments of length t have characteristic function: 69

Lt (u) = (1 +

iµu + (u)2 /2 - t ) , 

(3.8)

where again µ  R and , ,  > 0 and t = 1/250. Notice that  is the expected value of the Gamma random variable (see 3.10). Computing the first four moments results in a system of nonlinear equations: 0 = -µ ^1 + µt 0 = -µ ^2 +  2 t + (µ2 t(t + 1))/ 0 = -µ ^3 + (µt(t + 1)(3 2 + 2µ2 + µ2 t))/ 2 0 = -µ ^4 + (t(t + 1)(6µ4 + 3 2  4 + 12µ2  2 + 5µ4 t+  2 µ4 2 t2 + 6 2 µ2  2 t))/ 3 . Again we solve 4 systems of equations and the results are summarized in Table 3.4.
Table 3.4: Parameter Estimation using Method of Moments under Gamma Subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ 0.0874 0.0728 0.1277 -0.0328

 ^ 0.4979 7.5945 3.2767 13.3053

 ^ 2.3103 3.125e-4 1.0792 6.3567

^  2.5059 3.4075 1.1205 1.1376

The method had trouble finding a global minimum in the case where the empirical moments were calculated using electricity log-return prices. 70

Table 3.5: Parameter Estimation using Method of Moments under Inverse Gaussian Subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ 0.1624 -0.0354 0.1111 -3.7405

 ^ 0.7213 1.3402 3.1233 19.5346

 ^ 0.3238 0.0400 28.4386 0.0132

^  1.6971 1.9584 3.4862 0.3539

Changing the initial starting points resulted in varying results. Solving the system of equations using different algorithms and stopping criteria did not solve the problem. For the results in Table (3.4) and Table (3.5), we used as initial starting points the results from the minimum distance method (see Table (3.7) and Table (3.6)). In each case, the  in regime 2 is much larger than that of regime 1.

3.3

Minimum distance estimation

Another approach to estimate the parameters j = (µj ,  j , j ,  j ) of regime j is by minimizing the difference between the theoretical and empirical characteristic functions. While the Maximum Likelihood function can be unbounded, its Fourier Transform is necessarily bounded and therefore minimum distance estimation can work in cases where Maximum Likelihood method doesn't. Consider the sample x1 , ...xn of a random variable X with unknown cumulative distribution F (x; ) depending on the parameters .

71

The characteristic function of X is defined as:  (u; ) = E[exp(iuX )] =


exp(iux)dF (x; )
-

(3.9)

and the empirical characteristic function associated with the sample is defined as [32]: 1 n (u) = n
n 

exp(iuxk ) =
k=1 -

exp(iux)dFn (x),

(3.10)

where Fn (x) is the empirical cumulative distribution function: 1 Fn (x) = n
n

1xj x .
j =1

(3.11)

Define F to be the class of distribution functions. Define a non-negative distance function d : F × F  R such that:
 1/p

d[ (u; ), n (u)] =
-

| (u; ) - n (u)| w(x)dx

p

,

(3.12)

 where w : R  (0, ) is a weight function [7]. We take w(x) = (1/ 2 ) exp(-x2 /2) ^ is the minimum distance estimate of  if and p = 2. Then  ^), N (u)] = inf {d[ (u; ), N (u)]}. d[ (u; 
 

(3.13)

Again, we apply the algorithm to each regime separately. The empirical characteristic function is given by (3.10) and the analytic function (equation 3.9) is given by (3.8) when the subordinator is a Gamma process and (3.7) when the subordinator is an Inverse Gaussian process. The integral is computed numerically using a global adaptive quadrature algorithm, where the interval of integration is subdivided and the integration 72

takes place on each subdivided interval. Intervals are further subdivided if the algorithm determines that the integral was not computed to sufficient accuracy.
Table 3.6: Parameter Estimation using Minimum Distance Method under Gamma subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ -0.2018 0.19952 -0.494 -0.1205

 ^ 0.02975 2.1951 0.00198 2.24089

 ^ 0.421 0.00128 32.4815 0.0012

^  10.039 13.346 0.0019 16.2528

Table 3.7: Parameter Estimation using Minimum Distance Method under Inverse Gaussian subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ 0.01736 -0.4956 0.00813 5.7435

 ^ 0.11675 2.0078 02.0139 4.48714

 ^ 31.648 2.2260 67.456 76.004

^  8.0554 10.141 0.00154 6.871e-4

3.4

Maximum Likelihood Estimation

Given a random sample x = (x1 , ..., xn ) of a random variable X with an associated density function f (x; ) of the data x under the real world and 73

unknown parameters , maximum likelihood estimation (MLE) is a method used to estimate the vector valued parameter  of the model by maximizing the likelihood function [29] :
n

L(; x) =
k=1

f (xk ; );

  ,

(3.14)

with respect to . The value of  is constrained to   R4 , the space of all feasible values of the parameters. The maximum likelihood function L is primarily a function of the unknown parameters . The maximum likelihood estimator is given by: ^ = arg max L(; x). 
 

(3.15)

Finding the maximum (3.15) involves taking the derivative with respect to , or using numerical methods such as the gradient descent method.

To find the likelihood function L we require a sample and some density function. We derive the sample of daily log-returns by simulating n = 106 time-changed Levy process up to time T = 1/250. We approximate the density function by an empirical density function. We use the kernel smoothing technique to approximate an empirical probability density function given the historical data [13]. A kernel smoothing function K has the following properties: K (-x) = K (x), K (x)  0, and
R

K (x)dx = 1,

(3.16)

and has an associated scaled version Kh , given by: Kh (x) = x 1 , K h h 74 where h > 0. (3.17)

Assuming the log-return of the historical data x1 , x2 , ...xn are independent and identically distributed, the kernel density estimate of the probability distribution is given by: ^(x) = 1 f n 1 Kh (x - xj ) = nh j =1
n n

K
j =1

x - xj . h

(3.18)

There are many possible choices for the Kernel K including the uniform, triangle and parabolic kernels; we have decided to use the Gaussian kernel: 1 (x - xi )2 K (x - xi ) =  exp 2 2 (3.19)

The value of h, is chosen to equal Silverman's value h = 1.06n-1/5 where  is the standard deviation of the data [21]. In each of the four cases, the values of  were found to be higher in the
Table 3.8: Parameter Estimation using Maximum Likelihood Method under Gamma subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ 0.0023 -0.372 5.844e-3 -0.0148

 ^ 0.0431 0.52851 1.5002 7.543

 ^ 42.928 17.3008 93.271 90.5900

^  11.9960 88.556 2.1903 0.01770

second regime, hence justifying the use of a regime switching model. In nearly every method, the value of |µ| was found to be very small, which is expected as the long term deterministic contribution to the process is expected to be near zero. 75

Table 3.9: Parameter Estimation using Maximum Likelihood Method under Inverse Gaussian subordinator

Commodity Oil log-return Regime 1 Oil log-return Regime 2 Electricity log-return 1 Electricity log-return 2

µ ^ -0.4883 0.1201 -0.1781 -0.0191

 ^ 0.5058 2.9707 0.25873 4.9752

 ^ 0.64603 0.00014 0.91878 5.512e-5

^  63.709 9.993 20.0860 11.016

In choosing constraints, we set the lower bound of , ,  to be some small number = 10-6 . We set the upper bound of  to 5 as the diffusion

is expected to be smaller than 1 and for , , we set the upper bound to be 100, as the expected value of both Inverse Gaussian and Gamma random variables depends on the ratio / rather than any particular value for  and  . The drift µ is expected to be small, so in most cases, it was constrained to the set [-1, 1].

76

Conclusion
The goal of this thesis was to develop a new model by combining the regime switching Levy process, and the time-changed Levy process. We found the characteristic equation under Gamma and Inverse Gaussian subordinators, priced European call options under two pricing methods and estimated the parameters. Future work may include: using machine learning to rigorously estimate the location of the regimes based on characteristics of the historic data, as well as generate the most likely sequence of regimes into the future. The density function in the maximum likelihood method can be computed numerically in other ways, for example using the Inverse Fast Fourier Transform method. Finally, the number of regimes can be extended, multivariate processes can be explored, and pricing can be done on more exotic options.

77

Appendix A: Simulating Random Variables
Gamma Random Variables
We simulate Gamma Random random variables using Berman's Gamma Generator [27]. Given the shape parameter  > 0 and rate parameter  > 0, the probability density function of the Gamma distribution is given as: fgamma (x) =   -1 -x x e . () (3.20)

Sometimes the gamma distribution is parametrized with the shape parameter k > 0 and scale parameter  > 0, which are related by k =  and  = 1/ . If X  Gamma(,  ) then X/c  Gamma(, c), therefore all that is needed is to generate Gamma(, 1) random variables. If  = 1 the gamma distribution reduces to the exponential distribution with parameter  . We use Berman's Gamma Generator when 0 <   1: 1. 1 Simulate two independent random variables u1  U (0, 1) and u2  78

U (0, 1) 2. Set x1 = u1
1/

and x2 = u2

1/(1-)

3. If x1 + x2  1 move on to step 4, otherwise go back to step 1. 4. Simulate two independent random variables v1  U (0, 1) and v2  U (0, 1) 5. Return the number g = - log(v1 v2 ), which is a Gamma(, 1). If  > 1 we use Best's rejection method [8] to generate Gamma(, 1): 1. Define c =  - 1 and d = 3 - 3/4. 2. Simulate two independent, identically distributed standard uniform random variables u, v . 3. Define w = u(1 - u), y = 4. If x < 0, go to step 2. 5. Define z = 64 log w3 v 3 6. If log z > 2(c log (x/c) - y ), got to step 2. 7. Return x, which is a Gamma(, 1) distributed random variable d/w(u - 1/2) and x = c + y .

Inverse Gaussian Random Variables
We use the rejection method of Michael, Schucaney and Haas to generate Inverse Gaussian random variables [27]. Given the parameters  > 0 and 79

 > 0, the probability density function of the Inverse Gaussian distribution is given by: fIG (x) =  1 exp( )x-3/2 exp(- (2 x-1 +  2 x)), x > 0 2 2 (3.21)

A random variable with the above density function can be simulated in the following way: 1. Simulate a standard normal variable z and let v = z 2 . 2. Set x = / + v/(2 2 ) - 4v + v 2 /(2 2 ).

3. Generate a standard uniform random variable u. 4. If u  /( + x ), then x is an Inverse Gaussian random variable with parameters ,  , else, 2 /( 2 x) is the Inverse Gaussian random variable with parameters ,  . Inverse Gaussian random variables can also be parametrized by the mean and shape parameters µ > 0,  > 0 respectively: as µ = / and  = 2 .

The following table summarizes important moments for the Gamma and Inverse Gaussian distribution under the particular parametrization which is used throughout the thesis [27].

80

Common Moments Moments Mean Variance Skewness Kurtosis

Gamma distribution / / 2 2()-1/2 3 + 6()-1

Inverse Gaussian distribution / / 3 3( )-1/2 3 + 15( )-1

Table 3.10: Important moments of the Inverse Gaussian and Gamma distributions.

81

Appendix B: MATLAB Code
Simulating a Single Trajectory Regime Switching Time-Changed Levy Process
function Switching_Levy_Trajectory(T,N) %Plots the trajectory of regime switching process Z

%T=Maturity time (in years) %N=number of increments %EX: Switching_Levy_2(1,250) %Inverse Gaussian Subordinator

%drift coefficient mu1=0.01; mu2=-0.1; mu=[mu1,mu2];

%diffusion coefficient sigma1=1; sigma2=5; sigma=[sigma1,sigma2];

82

%parameter for exponential distribution i.e. holding times lambda1=0.5; lambda2=0.25; lambda=[lambda1, lambda2];

%s_0=1 Continuous-time Markov chain begins in regime 1 with probability 1 state=1;

%Set up a single switching time: Tau=exprnd(lambda(state));

%temp_state is only used to generate switching times temp_state=state;

%these arrays are only used to plot continuous-time Markov chain markov_t=[]; markov_st=[];

%initialize regime switching process Z=zeros(N,1);

%[0,dt,2dt,3dt,..., Ndt ] where dt=T/N time_vector=(0:N-1) * (T/N);

%generate stopping times while Tau(end)<T 83

Tau=[Tau, Tau(end)+exprnd(lambda(temp_state))]; temp_state=mod(temp_state,2)+1; end

for t=2:N

%DELTA=[t-1,stopping times which occur between t-1 and t, t] DELTA=[time_vector(t-1), Tau(Tau>=time_vector(t-1) &&... Tau<time_vector(t)),time_vector(t)];

dZ=0;

%dZ is the Z increment betweem t-1 and t

for i=2:length(DELTA) L=Lt(DELTA(i)-DELTA(i-1),state); dZ=dZ+mu(state)*L+sigma(state)*... sqrt(L)*randn; if i !=length(DELTA) state=mod(state,2)+1; end end %change states

84

%Z=sum dZ Z(t)=Z(t-1)+dZ;

end end % % % % % % % % % %

function L=Lt(dt,state)

%Subordinator increments

a1=0.1; b1=0.1; a2=0.1; b2=0.01; a=[a1,a2]; b=[b1,b2]; dist=makedist('InverseGaussian',a(state)*dt,b(state)); L=random(dist); end

85

Monte Carlo Simulation for European Call Options by simulating N Processes Simultaneously
function Expected_Price=MatrixLevy(T,K,N) %T=maturity, K=strike price, N=#of independent processes

global alpha, global beta, global string, global state

string='Gamma';

%All other

necessary variables are sored in file Variables.m

run Variables

%generate a column of the first stopping time ExpMatrix=zeros(N,1); state=1; while sum(ExpMatrix(:,end)<T)>0 %while there exist some final elements<T

%add columns until all elements are >=T ExpMatrix=[ExpMatrix,ExpMatrix(:,end)+exprnd(lambda(state),N,1)]; state=mod(state,2)+1; end

86

%Replace all the elements > T by T ExpMatrix(ExpMatrix>T)=T;

state=1; Z_end=zeros(N,1);

% Final value of Z_end is a N by 1 column vector of price values at Maturity for t=2:length(ExpMatrix(1,:))

L=Lt(ExpMatrix(:,t)-ExpMatrix(:,t-1),N); dZ=Mu(state)*L+Sigma(state)*sqrt(L).*randn(N,1); Z_end=Z_end+dZ; end Price=S0*exp(Z_end); EU_CALL=exp(-r*T)*max(Price-K*ones(N,1),0); Expected_Price=mean(EU_CALL); end

%

%

%

%

%

%

%

%

%

%

%creates distribution for subordinator function L=Lt(dt,N) %subordinator

global alpha,global beta,global state 87

%alpha,beta are stored in Variables.m %RVsimualtionMatrix generates Gamma and Inverse Gaussian random variables %given by the rejection method in the apprendix L=RVsimulationMatrix(alpha(state)*dt,beta(state),N);

end

Fourier-Cosine Pricing
function Price=COS_Pricing(T,K)

global dt, global string, global alpha, global beta, global Mu, global Sigma, global lambda

N=2^10;

string='Gamma'; run Variables %All other variables are stored in this file

x=log(S0/K); dt=(T-t0); a=-5; b=5; 88

v3=zeros(N,1); k=0:N-1; Psi=zeros(N,1); V_k=zeros(N,1);

for n=1:N %calls Characteristic, given below Psi(n)=Characteristic(k(n)*pi/(b-a),x); V_k(n)=V(a,b,k(n),K,'Put'); end

Re=real(Psi.*exp(-1i.*k*pi*a/(b-a))');

%Put-Call Parity v3=exp(-r*dt).*Re.*V_k+S0-K*exp(-r*T);

Price=v3(1)*1/2 + sum(v3(2:end));

end

%

%

%

%

%

%

%

%

%

%

function Char=Characteristic(u,x) %Characteristic function of Switching time-changed process 89

%u is the variable, x is initial price/strike price

global dt,global lambda,global Mu,global Sigma, global alpha, global beta,global string

%infinitesimal generator matrix Q=[-lambda(1),lambda(2);lambda(1),-lambda(2)];

if strcmp(string,'Gamma') PHI=[-alpha(1)*log(1+(-1i*Mu(1).*u+(Sigma(1).*u)^2/2)/beta(1)),0;... 0,-alpha(2)*log(1+(-1i*Mu(2).*u+(Sigma(2).*u)^2/2)/beta(2))];

elseif strcmp(string,'InverseGaussian') PHI=[-alpha(1)*(sqrt(2*(-1i*Mu(1).*u+(Sigma(1).*u)^2/2)+beta(1)^2)... -beta(1)),0;0,-alpha(2)*(sqrt(2*(-1i*Mu(2).*u+... (Sigma(2).*u)^2/2)+beta(2)^2)-beta(2))]; end

%matrix exponential expm Char=exp(1i*u*x)*([1,1]*expm(dt*(Q+PHI))*[1,0]'); end

90

%

%

%

%

%

%

%

%

%

%

function Chi=Chi(a,b,c,d,k) w=k*pi/(b-a); Chi=(1/(1+w^2)) * (cos(w*(d-a))*exp(d) - cos(w*(c-a))*exp(c) + ... w*sin(w*(d-a))*exp(d) - w*sin(w*(c-a))*exp(c)); end

%

%

%

%

%

%

%

%

%

%

function Psi=Psi(a,b,c,d,k) if k==0 Psi=d-c; else w=k*pi/(b-a); Psi=(w^-1) * (sin(w*(d-a))-sin(w*(c-a))); end end

%

%

%

%

%

%

%

%

%

%

function Vk=V(a,b,k,K,option) %Vk coefficients for Call and Put

91

if strcmp(option,'Call') Vk=(2/(b-a))*K*(Chi(a,b,0,b,k)-Psi(a,b,0,b,k)); elseif strcmp(option,'Put') Vk=(2/(b-a))*K*(-Chi(a,b,a,0,k)+Psi(a,b,a,0,k)); end end

Minimum Distance Estimation Function
function Distance=Distance_minimizing_Estimation_function(theta) %theta=[mu,sigma,alpha,beta] global data,global string

string='InverseGaussian'; p=2;

%data=log-returns of historical data Integrand_function=@(u)abs(Analytic_characteristic_function(u,... theta,string)-Empirical_characteristic_function(u,... data)).^p.*(1/sqrt(2*pi)).*exp(-u.^2/2);

%global adaptive quadrature with default tolearance Distance=integral(Integrand_function,-1e6,1e6)^(1/p); 92

end

%

%

%

%

%

%

%

%

%

%

function ECF=Empirical_characteristic_function(u,data) %u is an array of arguments ECF=0;

for j=1:length(u) ECF(j)=sum(exp(1i*u(j)*data)); end ECF=ECF/length(data);

end % % % % % % % % % %

function CHAR=Analytic_characteristic_function(u,theta,string) dt=1/250; if strcmp(string,'Gamma') Mu=theta(1); Sigma=theta(2); rho=theta(3); beta=theta(4);

93

CHAR=(1+(1i*u*Mu-(u*Sigma).^2/2)./-beta).^(-rho*dt/beta);

elseif strcmp(string,'InverseGaussian') Mu=theta(1); Sigma=theta(2); alpha=theta(3); beta=theta(4);

CHAR=exp(-alpha.*dt.*(sqrt(2*(1i.*u.*Mu-(u.*Sigma).^2/2)+beta^2)-beta)); end end

Maximum Likelihood Estimation
function MLE=MLE_function(x)

%data is log-returns of historical data global string, global data string='Gamma';

N=1e6; dt=1/250;

94

Mu=x(1); Sigma=x(2); alpha=x(3); beta=x(4);

%LevyMLE defined below dY=Mu*LevyMLE(dt*ones(N,1),N,x)+... Sigma*sqrt(LevyMLE(dt*ones(N,1),N,x)).*randn(N,1); [f_MLE,xi] = ksdensity(dY);

MLE=1; for k=1:length(data) %interpolation MLE=MLE*interp1(xi,f_MLE,data(k)); end MLE=-MLE; %Because we will want to MAXIMIZE using fmincon end

%

%

%

%

%

%

%

%

%

%

function L=LevyMLE(dt,N,x) Mu=x(1); Sigma=x(2); alpha=x(3); 95

beta=x(4);

L=RVsimulationMatrix(alpha*dt,beta,N); end

Simulating Gamma and Inverse Gaussian Random Variables
function arrayX=RVsimulationMatrix(alpha,beta,N) global subordinator Array=zeros(N,1);

for n=1:N

if strcmp(subordinator,'Gamma') %To simulate Gamma R.V

if alpha(n)==0 X=0; elseif alpha(n)<=1 && alpha(n)>0 %Berman Gamma Generator

x=rand()^(1/alpha(n));y=rand()^(1/(1-alpha(n)));

while x+y>1 x=rand()^(1/alpha(n));y=rand()^(1/(1-alpha(n)));

96

end X=-x*log(rand()*rand())/beta;

elseif alpha(n)>1 b=alpha(n)-1; c=3*alpha(n)-3/4; U=rand();V=rand(); W=U*(1-U); Y=sqrt(c/W)*(U-1/2); x=b+Y; Z=64*W^3 * V^3; i=0; while 1==1 i=i+1; U=rand();V=rand(); W=U*(1-U); Y=sqrt(c/W)*(U-1/2); x=b+Y; if x>0

Z=64*W^3 * V^3;

if log(Z)<=2*(b*log(x/b)-Y) X=x/beta;

97

break end end end end

elseif strcmp(subordinator,'InverseGaussian') %Simulate Inverse Gaussian if alpha(n)==0 X=0; else y=randn()^2; x=(alpha(n)/beta)+y/(2*beta^2)-sqrt(4*alpha(n)*beta*y+y^2)/(2*beta^2); if rand()<=alpha(n)/(alpha(n)+x*beta) X=x; else X=alpha(n)^2/(beta^2*x); end end end R.V

Array(n)=X; 98

end arrayX=Array; end

99

Bibliography
[1] Awad H. Al-Mohy and Nicholas J. Higham. A New Scaling and Squaring Algorithm for the Matrix Exponential. SIAM J. Appl., 31(3):970­989, 2009. [2] Gidi Amir. Chapter 6: Continuous Time Markov Chains. [3] Catherine Beauchemin. Computational Methods in Medical Physics. 2014. [4] Peter Carrr and Liuren Wu. Time-changed levy processes and option pricing. 2002. [5] Kyriakos Choudarski. Switching l´ evy models in continuous time: Finite distributions and option pricing. Center for Computational Finance and Economic Agents, 2005. [6] Kyriakos Choudarski. Multinomial method for option pricing under variance gamma. Center for Applied Mathematics and Economics - ISEG, 2018.

100

[7] Andreas N. Philippou Constantine A. Drossos. A note on minimuim distance estimates. [8] Rama Cont and Peter Tankov. Financial Modelling with Jump Processes. CRC Press, London, United Kingdom, 2003. [9] F.Fang and C.W. Oosterlee. A novel pricing method for european options based on fourier-cosine series expansions. Journal on Scientific Computing, 2008. [10] Roman N. Makarov Giuseppe Campolieti. Financial Mathematics: A Comprehensive Treatment. CRC Press, Boca Raton, United States, 2014. [11] David Williams Grimmett-Stirzaker. Probability with Martingales. Press Syndicate of the University of Cambridge, Cambridge, 1991. [12] Elias S.W. Shiu Hans U. Gerber. Option pricing by esscher transforms. Transactions of Society of Actuaries, Vol. 46, 1994. [13] Nathaniel E. Helwig. Density and Distribution Estimation. [14] Nicholas J. Higham. The scaling and squaring method for the matrix exponential revisited. SIAM J. Appl., 26(4):11791193, 2005. [15] Magdalena Hyksova. Stochastic Processses and their Classification. [16] Mor J.J. and D.C. Sorensen. Computing a trust region step. SIAM Journal on Scientific and Statistical Computing, 3:55357211791193, 1982. 101

[17] Boyd J.P. Chebyshev Fourier spectral methods. Springer Verlag, Berlin, 1989. [18] Oliver Knill. Probability Theory and Stochastic Processes with Applications. Overseas Press, Daryanganj, New Delhi, 2009. [19] Kevin Lin. Notes on Memoryless Random Variables. [20] Ronnie Sircar Matthew Lorig. Stochastic volatility: Modeling and

asymptotic approaches to option pricing and portfolio selection. pages 1­3, 2014. [21] A.I. McLeod and B. Quenneville. Mean Likelihood Estimators. Statistics and Computing 11, 57-65, 2001. [22] Antonis Papapantoleon. An introduction to levy processes with applications in finance. 2000. [23] Andrea Pascucci. PDE and Martingale Methods in Option Pricing. Bocconi University Press, 2011. [24] Philip Protter. Martingales and Local Martingales. [25] Stefan Trck Rafa Weron, Michael Bierbrauer. Modeling electricity prices: Jump diffusion and Regime Switching. Physica A, Hugo Steinhaus Center, Wroclaw University of Technology, 2004. [26] Sheldon M. Ross. Stochastic Processes. John Wiley and Sons, Inc., United States, 1996. 102

[27] Wim Schoutens. L´ evy Processes in Finance: Pricing Financial Derivatives. John Wiley Sons, Ltd., London, United Kingdom, 2003. [28] Karl Sigman. Continuous-Time Markov Chains. [29] Kyle Smith. Estimation methods of reference evapotranspiration at unsampled locations. New Mexico Institute of Mining and Technology. [30] M. Soleymani. Gradient descent - machine learning. [31] J. Watkins. Method of moments. [32] Jun Yu. Empirical characteristic function estimation and its applications.

103

