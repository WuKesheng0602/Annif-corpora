NOTE TO USERS

This reproduction is the best copy avaiiable.

UMI
R ep ro d u c ed with p erm issio n of th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

R ep ro d u ced with p erm issio n of th e copyright ow ner. Further reproduction prohibited w ithout p erm issio n .

IN D E X IN G OF A M E R IC A N FOOTBALL V ID EO U SIN G M PE G -7 D E SC R IP T O R S A N D M FCC FEA TU R ES
by

Syed G. Quadri B.Eng., Ryerson University, Toronto, 2002

A thesis presented to Ryerson University in partial fulfillment of the requirement for the degree of Master of Applied Science in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2004 ©Syed G. Quadri 2004

PRCFFHTYOF

L IBR A R Y

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited w ithout perm ission.

UMI Number: EC52933

INFORMATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

UMI
UMI Microform EC52933 Copyright 2008 by ProQuest LLC. All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code. ProQuest LLC 789 E. Eisenhower Parkway PC Box 1346 Ann Arbor, Ml 48106-1346

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm issio n .

B o rro w er's P a g e
Ryerson University requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date.

N am e

Signature

Address

D ate

Ill

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

ABSTR ACT
In d ex in g o f A m erica n F ootb all V id eo U s in g M P E G -7 d escrip tors and M F C C featu res
© S y e d G. Quadri 2004 M aster o f A pplied Science D ep artm en t o f E lectrical and Com puter E ngineering R yerson U niversity In this work, an application system is proposed to classify American Football Video shots. The application uses MPEG-7 motion and audio descriptors along with Mel Frequency Cepstrum Coefficient features to classify the video shots into 4 categories, namely: Pass plays. Run plays, Field G oal/Extra Point plays and Kickoff/Punt plays. Fisher's Linear Discriminant Analj^sis is used to classify the 4 events, using a leaveone-out classification technique in order to minimize the sample set bias. For a database of 200 video shots taken from four different games, an overall system per formance of 92.5% was recorded. In comparison to other American Football indexing systems, the proposed system performs 8% to 12% better. We have also proposed an algorithm that uses MPEG-7 motion activity descriptors and mean of the motion vector magnitudes, in a collaborative manner to detect the starting point of play events within video shots. The algorithm can detect starting points of the play with 83% accuracy.

IV

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

A ck n o w le d g e m e n t
I would like to thank my supervisor Dr. Ling Guan and co-supervisor Dr. Sridhar Krishnan for their encouragement, guidance and continuous support throughout my research work and writing of this manuscript. This work would have been impossible without their feedback, patience and kindness. I would also like to thank Canada Foundation for Innovation (CFI) and the Depart ment of Electrical and Computer Engineering for providing a very well equipped and technically supported Ryerson Multimedia Laboratory. My thanks are due to the School of Graduate Studies of Ryerson University for providing Graduate Student Scholarship and helping me in securing Ontario Graduate Scholarship (CCS). I would like to acknowledge my supervisors' funding resources National Sciences and Engineering Research Council of Canada (NSERG) and Canada Research Chair Pro gram, for financial support to this research work. My thanks are due to my manager in IBM Canada Ltd. for his unconditional support during my school years. I would like to thank my colleagues and members of the Ryerson Multimedia Laboratory for creating a friendly and congenial environment in the Lab. It was my pleasure to work with such a great team. In the end I could not have achieved this goal of mine without the support of my parents, family and wife.

R ep rod uced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission.

C ontents
Introduction 1.1 General need for I n d e x in g ........................................................................ 1.2 Focus of this w o rk......................................................................................... 1.3 Organization of th esis.................................................................................. M P E G -7 C oncepts and D escriptors 2.1 MPEG-7 O v e rv ie w ..................................................................................... 2.2 MPEG-7 D escrip to rs.................................................................................. 2.2.1 Visual Descriptors ........................................................................ 2.2.2 Audio D escriptors............................................................................ 2.3 Applications of M P E G -7 ........................................................................... P roposed Indexing System 3.1 In tro d u ctio n .................................................................................................. 3.2 Review of Sports Indexing S y s te m s ........................................................ 3.3 Motivation and Gontribution of the proposed s y s t e m ........................ 3.4 Proposed System Overview........................................................................ 3.4.1 Stage 1: Localization P h a s e .......................................................... 3.4.2 Stage 2: Feature Modeling P h a s e ................................................ 3.4.3 Stage 3: Classification P h a s e .......................................................... 3.5 Test Database of American Football Video Shots ............................... Sem antic Localization 4.1 In tro d u ctio n .................................................................................................. 4.2 Related W orks............................................................................................... 4.3 Proposed Algorithm .................................................................................. 4.4 Play start detection re su lts......................................................................... 4.5 C o n clu sio n s.................................................................................................. Indexing of A m erican football 5.1 In tro d u ctio n .................................................................................................. 5.2 Feature E x tr a c tio n ...................................................................................... 5.2.1 MPEG-7 Motion DescriptorsFeature M ap p in g ........................... 5.2.2 MPEG-7 Audio DescriptorsFeature M a p p in g ........................... 5.2.3 MFGC Feature M a p p in g ................................................................ 1 1 2 5 6 6 7 7 10 15 17 17 18 23 25 25 26 32 34 35 35 36 36 41 43 46 46 47 48 53 55

VI

R ep rod uced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

5.3

American Football RVS Event C lassification......................................... 5.3.1 Linear Discriminant Analysis ...................................................... 5.3.2 MPEG-7 Motion descriptor based classification ...................... 5.3.3 MPEG-7 Audio descriptor based classification......................... 5.3.4 MFCC feature based classification................................................ 5.3.5 Multi Modal feature based classification ................................... 5.4 C o n clu sio n s................................................................................................... C onclusions 6.1 Summary of Thesis contrib u tio n ................................................................ 6.1.1 Play event d e te c tio n ....................................................................... 6.1.2 Play events classification................................................................ 6.2 Future D irectio n s..........................................................................................

62 62 64 65 66 67 68 70 70 71 72 75 76

6

B ibliography

vu

R ep rod uced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

List of Figures
1.1 Knowledge Base of American Football .................................................... 3 7 10 11 25 26 28 29 30 31 32 33 37 39 40 41 42 43 44 47 48 49 50 51 52

2.1 Scope of MPEG-7 S ta n d a r d ....................................................................... 2.2 Direction of A c tiv ity .................................................................................... 2.3 Summary of Low level Audio D escrip to rs................................................. 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 4.1 4.2 4.3 4.4 4.5 4.6 4.7 5.1 5.2 5.3 5.4 5.5 5.6 5.7 Proposed System overview .......................................................................... Play localization phase overview ................................................................. Audio feature modeling p h a s e .................................................................... MFCC extraction process .......................................................................... MFCC feature red u n d an cy .......................................................................... Motion feature modeling phase ................................................................. Motion feature q u a n tiz a tio n ....................................................................... Classification phase overview....................................................................... Mean of Motion Vectors for different typesof p l a y s ............................... Mean and Standard deviation of Motion V ecto rs.................................... Flow chart of proposed a lg o rith m ............................................................. Deviation of estimated starting point from ground truth .................... Performance of proposed algorithm in time d o m a i n .............................. Deviation of estimated starting point from ground tru th .................... Performance of proposed algorithm in time d o m a i n ..............................

Different type of Motion A c ti v ity .............................................................. Key Frames of Pass P l a y ............................................................................. Key Frames of Run P l a y ............................................................................. Key Frames of Kickoff/Punt P l a y .............................................................. Key Frames of Field C oal/E xtra Point P l a y .......................................... Motion feature m a p ....................................................................................... (a) Original audio signal; (b) Audio Spectrum Envelope descriptor output 1/4 octave resolution; (c) Audio Spectrum Centroid descriptor output; (d) Audio Spectrum Flatness descriptor o u t p u t ...................... 5.8 The MEL S c a l e .............................................................................................. 5.9 MFCC feature extraction sub s y s t e m ........................................................ 5.10 Mel filter bank ............................................................................................. 5.11 MFCC feature red u n d an cy..........................................................................

54 57 57 60 61

vm

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

6.1 6.2

Multi-modal c la ssific a tio n .......................................................................... Scatter plot of classified data ....................................................................

73 74

IX

R ep ro d u ced with p erm ission o f th e copyright ow ner. Further reproduction prohibited without p erm ission .

List of Tables
3.1 Summary table of features used and semantic events retrieved in Bas ketball .............................................................................................................. 3.2 Summary table of features used and semantic events retrieved in Tennis 3.3 Summary table of features used and semantic events retrieved in F I r a c i n g .............................................................................................................. 3.4 Summary table of features used and semantic events retrieved in track and f ie ld ........................................................................................................... 3.5 Summary table of features used and semantic events retrieved in soccer 3.6 Summary table of features used and semantic events retrieved in baseball 3.7 Summary table of features used and semantic events retrieved in baseball 3.8 Summary table of features used and semantic events retrieved in football 3.9 Summary table of features used and semantic events retrieved in football 4.1 Comparison table of play detection performance using window size 3 vs 4 ................................................................................................................. 5.1 Classification Summary Table using MPEG-7 motion descriptor features 5.2 Confusion m atrix between categories using MPEG-7 motion descriptor features for classification............................................................................. 5.3 Classification Summary Table using MPEG-7 audio descriptor features 5.4 Confusion m atrix between categories using MPEG-7 audio descriptor features for classification............................................................................. 5.5 Classification Summary Table using MFCC fe a tu re s .............................. 5.6 Classification Summary Table using multi-modal f e a tu re s .................... 6.1 Performance Comparison of NFL Video Indexing S y s te m .................... 19 19 20 20 21 21 22 22 23 45 64 64 65 66 67 68 74

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

C hapter 1 Introduction
1.1 G e n e r a l n e e d for I n d e x in g
h e increase in digital information has created a vital need for development of new techniques th a t can provide efficient access to information. Recently

one of the areas for active research has been Indexing and Retrieval strategies in order to facilitate the access to the vast amount of information. Traditionally textual annotations are used for indexing of digital media, which is an extremely manual process and is prone to subjective bias. In contrast to textual annotations, there are indexing techniques based on content. These systems are known as content based indexing and retrieval systems (CBIR). Users can use query by example or query by sketch to retrieve the information from a multimedia database. The CBIR systems can form the query based on low level features and retrieve multimedia objects of similar features. However, we as users are more interested in semantics. Low level features are an integral part of any query system, but mostly we are not interested in retrieving information with similar shape, color or texture. We relate objects to its meaning, therefore a modern indexing and retrieval system m ust provide meaningful or semantically coherent search results. Appropriate semantic indexing of multimedia is a difficult task due to two main factors. First there is large amount of information present in all types of different modalities, such as audio, image and text. Secondly there are many different levels at which the information can contain semantic information. Additionally there are

T

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited w ithout p erm issio n .

issues with what to index and what not to index, as different users will search for different information and will have very different criteria definitions. Design and development of an effective video indexing and retrieval system based on the content of th e video poses some interesting challenges. Most people often think of video as a sequence of images, but in reality it is a medium with multiple media. Videos integrate the media presented by the images, graphics, tex t and audio. Therefore in order to index a video stream requires multiple strategies and a number of processing steps to deal with all the diverse media present. Thus video requires the content to be analyzed at a number of levels, namely lexical, syntactical and semantic. This could potentially mean th a t video first needs to be segmented into shots, the closed captioned text in the video stream has to be detected and recognized, the audio information needs to be extracted and interpreted, speech recognition may have to be used, visual information needs to be analyzed based on content and finally the shot needs to be annotated semantically [1]. In order to tackle the various issues of indexing a multimedia object, MPLC de fined a standard known as ''Multimedia Description Framework" or MPEG-7 with the prim ary goal of defining a scheme to describe a multimedia document by means of its content and the relationship between the different content sets within a mul timedia document. MPEG-7 only standardized th e description of content and not the algorithms utilized for feature extraction and application development. Therefore MPEG-7 can be used to develop new fast and efficient indexing and retrieval systems by utilizing the descriptors defined by the standard. These descriptors can be used to index a multimedia document at various levels, thus providing semantic retrieval tools.

1.2

F o c u s o f th is w ork

In this thesis a video indexing system is designed to automatically annotate National Football League (NFL) video shots. Every sport has many actions associated with it. But only a few fundamental events can describe the core of th e game. The whole

R ep ro d u ced with p erm issio n o f th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

game is bailt mi these fuudameutal I'veuts. Foi- example in basketliall we can identify jum p shots, free throws and turnovers as the three frmdauieiital semautie events (FSE). Likewise iu American foutiiall (l\'FL) we have lu'oposed three FSE; namely pass plays, running plays and speeial team plays. In the ADVENT pruje.et te.ehnical report [2], the authors generalize the eoneept of FSE liy calling it Recurrent Visual Semantics (RVS). They deliuc' RVS as the repetitive appearance of elements that are visually similar and have common level of meaning within a sjiecihc context. The domain in wliich RVS evmits most commonly occur are. news video and sports video. Tinsse RVS events or FSE point to the fact that for effective semantic indexing ol mult iiiu'dia content, we need to build a Ivnowledge Base of domain specific events which has the RVS events at the core. In the thesis we iJro]>ose a knowledge base for NFL video shots. Figure 1.1 details the propusetl NFL fxnowledge Base as a hierarchical graph.

Knowledge Base

Touchdown

Convert

Extra Point

Field Goal

D efense

Offense Running Play Passing Play Special Teams

V
Fundamental Semantic Events

y

~

F igure 1.1: Knowledgt' Base of American Football

R ep ro d u ced with p erm issio n of th e copyright ow n er. Further reproduction prohibited w ithout p erm issio n .

The graph shown in Figure 1.1, is designed in aueii aw ay that the uutornioat nudes carry highest level of semantic value. The deeper the node, the more specific semantic information it carries. For example the events of touchdown and field goals carry the most specific semantic value while the events like scoring or nun-scoring carry semantic value at a very high level. Much research has been done in identification of scoring vs non-scoring semantics [3]. In addition, some researchers have built systems to identify touc'hdowns, field goals and point after attem pts [4], The focus of this research lias been to identify the TVS events such as jiass. run or kick. In the research community a lot of sports indc'xing techniques have been cx]fiored. Mo.st of the indexing schemes rely on extracting low level visual and audio features which are passed to complex classifiers for identification. Since the finalization of the KIPEG-7 standard some of the research activity has focused on using MPEG7 descri])tors as features for indexing news and sports video. For example in [5]

MPEG-7 motion and audio features are utilized to summarize news video and sports video which included basc'ball, tennis and golf. Not much work has been done in the American Football domain. The work done in this domain mainly focuses on retrieval of scoring events from the sports video by extracting low level featnres such as audio, motion and closed caption text. In one of the works by Terry Caelli [G]. spoken commentary along with player movement is utilized to detect different types of formations. All the works conducted in this domain have relied significantly on rule based classification schemes. This work will focus on utilizing MPEG-7 descriptors in order l.o index NFL video shots using a simple linear classifier. One of the key issues with indexing of RVS events within a sports domain frame work, is the localization of the start and end points of tlie event within a large collection of videos. The localization of the eyc'Ut. helps in reducing the analysis win dow size and also by eliminating factors effecting the features which are not directly related to the action itself. In this work. I have proposed, kIPEG-7 motion descriptor ba.sed technic pie to find the starting point of plays from NFL video shots.

R ep ro d u ced with p erm issio n of th e copyright ow n er. Further reproduction prohibited without p erm issio n .

1-3

O r g a n iz a tio n o f th e s is

The remainder of this thesis consists of 5 chapters which are organized as follows: C h a p te r 2: M PEG-7 Concepts and Descriptors, contains an overview of the MPEG7 standard and a brief overview of audio and visual descriptors. It also contains a detailed explanation of the motion and audio descriptors th a t are utilized in this work. C h a p te r 3: Proposed Indexing System, contains the overview of the proposed in dexing system designed to index NFL video shots. Chapter 3 reviews other sports indexing techniques and provides the background for the proposed research activities.

C h a p te r 4: Play start Detection, details the proposed algorithm for localization of RVS action events within NFL video shots. It also presents the results of the algorithm and compares some other event localization efforts in the research community. C h a p te r 5: NFL video shot Indexing System, details the proposed indexing system using MPEG-7 motion, audio descriptors and Mel Frequency Cepstrum Coefffcients (MFCC). It summarizes the results of first using the features independently, and then by combining them together. C h a p te r 6: Conclusions and Future Work, summarizes the results and discusses the advantages of using motion and audio descriptors. Some consideration is provided on how to enhance the work in the future.

R eo ro d u ced with o er m issio n of th e coovrioh t ow ner. Further reoroduction orohlbited without a erm isslo n .

Chapter 2 M PE G -7 Concepts and D escriptors
2.1 M P E G -7 O v erv iew
PEG-7 is a standard developed by the Moving Picture Experts Group (MPEG) via the standardization organizations of ISO/IEC. The formal name of the

MPEG-7 standard is ''Multimedia Gontent Description Interface" and is organized in 8 parts [7]. Parts 1 through 5 define the core of MPEG-7 technology, while parts 6 to 8 provide supporting information to the standard. This standard differs from its predecessors, such th a t the previous standards were concerned with the representa tion of the content while the objective of MPEG-7 is to standardize the information about the content. Thus MPEG-7 defines Descriptors and Description Schemes to represent the information in the multimedia document. In MPEG-7 the descriptors represent all types of multimedia documents, such as Images, Graphics, 3D models, audio and video. It does not depend on how the multimedia document is created, coded or stored. Figure 2.1 shows the scope of the MPEG-7 standard.

M

The MPEG-7 standard consists of three main parts [8]. · D e sc rip tio n Tools; Descriptors and Description Schemes make up the De scription Tools. The Descriptors define the syntax and semantics of a feature, while the description scheme defines the relationship between descriptors and other description schemes.

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction p n

' u r j without p erm ission .

· D e sc rip tio n D efin itio n L an g u ag e (D D L ): DDL dofiiicK the syntax uf Deaciiption Tools in textual format. In MPEG-7 DDL is based on XML Seheina Language. A detailed introduction on DDL can be found in Chapter 4 uf [9] · S y ste m Tools: These tools are used for management, synehronmation, storage and transmission of descriptions. In MPEG-T system tools support descriptions in both textual and binary formats. Detailed introduction on any uf the aliove ])arts can 1»' found in [9]. Next section will provide more details on the descriptors that were utilized in the scope of this work.

2 .2
2 .2 .1

M P E G -7 D e sc r ip to r s
V isu a l D e sc rip to r s

The.se descriptors de.scribe the basic multimedia document contcmt based on visual information only. For example, in an image object, the content can be desci'ÜK'd based on the shape, texture and color media. Objei't motion and camera motion along with the above mentioned features can be used to describe video media. The main objective of the visual desmiptors is to assist user applications in iikmtification. categorization and filtering of images and videos. The. visual descriptors can be divided into general and domain specific dc'seriptors. General visual descriptors are made up uf shape, color, tc'xtnre and motion features. There is only one domain specific descriptor; the face descriptor. A brief summary

Description

P escription . consum ption

5cope of MPEG-7
Figure 2.1: Scope of MPEG-7 Standard

R ep rod u ced with p erm ission of th e coovrioht ow ner. Further reproduction prohibited without o erm issio n .

8

of tlie gc'iK'ral visual (losciiptuis is given below.

The details of only the motion

descriptors is included in the document as currently they are the only ones being utilized by the proposed system. · C o lo r D e scrip to rs: In image and video applications, color is the mo.st com monly used feature. It is easily extracted and is some what immune to rotation, translation and vicaving angle changes. There are seven (7) color descriptors defined by MPEG-7 as li.sted below; 1. Color S])a.c*e Descriptor 2. Color Quantization Descriptor 3. Dominant Color Descriptor 4. Scalable Color Dc'scriptor 5. Group of Frarnc's/Group of Pictures Dccscriptor C. Color Structure Descriptor 7. Color Layout De.scriptor · T e x tu re D escrip to rs: Texture defines the .spatial distribution of patterns in an image. There is no universal definition of texture, but patterns in a region of an image can create a.n appearance of texture. In MPEG-7 three (3) texture descriptors are defined, as listed below: 1. Homogenous Texture Dccscriptor 2. Texture Browsing Descriptor 3. Edge Histogram Descriptor · S h a p e D e sc rip to rs: Shape is an important feature in image and video re trieval and object, identification systems. Huma.ns tend to associate semantics with the shape of objects. In MPEG-7 three (3) shape de.scriptors are defined as listed below: 1. Region-based Shape Descriptor

R eo ro d u ced with o erm ission of th e coovrioht ow ner. Further reoroduction orohihited without o erm issio n

2. Contour-based Shape Descriptor 3. 3-D Spectrum Shape Descriptor · M o tio n D e scrip to rs: Many different types of motion occur in a video seg ment. There is motion associated with objects with in the pictures and motion due to camera movements. In MPEG-7 four (4) descriptors are defined, as listed below, which cover all types of motion: 1. Camera Motion Descriptor 2. Motion Trajectory Descriptor 3. Motion Activity Descriptor 4. Parametric Motion Descriptor

M o tio n A c tiv ity D e sc rip to rs The objective of the motion activity descriptor is to quantify the overall activity or pace of action in a video segment. We tend to perceive sports video segments as fast moving compared to news video segments. The activity descriptor is easily extracted from compressed domain, utilizing the encoded motion vectors. The descriptor utilizes the statistical properties of the motion vector magnitudes to measure intensity of motion activity. Following is the summary of attributes associated with the descriptor. · I n te n s ity of A ctiv ity : This attribute contains the global intensity of motion activity on a scale of 1 to 5. A high value indicates high activity. · D ire c tio n of A ctiv ity : This attributes expresses the dominant direction in the video segment. This attribute classifies the direction into eight equally spaced directions as shown in Figure 2.2. · S p a tia l d is trib u tio n of activ ity : This attribute specifies the number and size of high activity regions within a frame. The attribute gives an indication whether the activity is spread across many regions or one large region.

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission.

10

Figure 2.2: Direction of Activity · T em p o ral d is trib u tio n o f a c tiv ity : This attribute contains information re garding the duration of motion activity in a video segment. It specifies if the activity is confined to one part of the video or sustained throughout the segment. The intensity of motion activity attribute is most commonly associated with the motion activity descriptor. The other three descriptors, dominant direction, spatial distribution and temporal distribution are optional attributes. In this work we will utilize both the intensity of motion and dominant direction attributes of the motion activity descriptor.

2 .2 .2

A u d io D escrip to rs

In the standard, audio descriptors can be divided into two categories. One based on low level audio features designed for general use, while the other designed for application specific tasks. In the standard the generic audio tools are also known as Audio Description Framework. A detailed summary of the Audio description

framework is given in the next sub section. Following is a brief summary of the low level and high level audio descriptors and description schemes. · Low Level A u d io D escrip to rs: There are eighteen (18) low level spectral

R ep rod uced with p erm ission of th e copyrioht ow ner. Further reproduction prohibited without perm ission.

11

and temporal audio descriptors defined in the standard. These descriptors can be categorized into the following seven groups; 1. Basic Descriptors 2. Basic Spectral Descriptors 3. Basic Signal parameter Descriptors 4. Temporal timbrai Descriptors 5. Spectral timbrai Descriptors 6. Spectral basis Descriptors 7. Silence Segment Descriptor Figure 2.3 shows the summary of the groups along with the audio descriptors in the respective categories [8].

A'üaw;
Silence D Timbrai T entporal LogAttackTime D TemporalCentroid D Basic Spectral AitdioSpectnimEiiveiope D .AudioSpectrumCentroid D AudioSpjBctminSpreBd t) Timbrai Spectral HarmotiicSpectralCentioid D HamioiucSpectralDeviatioii D HannonicSpeciralSpread D HarmonicSpectral Variation D Sj>ectralCentrodd D

Spectral Basis AudioSpectram Basis D

Basic AudioWavefoitn D AudioPowcr D

Signal parameters AudioHamionicity D Audi oFundaniental Frequency D

F igu re 2.3: Summary of Low level Audio Descriptors

R ep rod uced with p erm ission of th e copyriqht ow ner. Further reproduction prohibited without perm ission.

12

· H ig h level A u d io D escrip tio n s: There are four description schemes defined in the standard for application specific tasks. The list of the descriptors is as follows: 1. General sound recognition and indexing tools 2. Spoken Content description tools 3. Musical instrument timbre description tool 4. Melody description tool

A u d io D e sc rip tio n F ram ew o rk Audio description framework consists of eighteen (18) low level descriptors. The

objective of the framework is to define a wide range of features th at can be readily used to build general audio based applications. A brief summary of the audio descriptors in the framework is given below. Details on how to compute these descriptors is not included here, but Section 5.2.2 provides details on computing the descriptors utilized in this work. 1. A u d io W aveform : This descriptor provides information about the envelope of the signal. It contains the minimum and maximum values of the signal within a specified window. 2. A u d io P ow er: This descriptor provides information about the power of the signal. It describes the instantaneous power of the samples in the specified window. 3. A u d io S p e c tru m E nvelope: This descriptor provides information about spectral resolution of the logarithmic bands. The resolution can be controlled between 1/16 of an octave to 8 octaves. The descriptor can be computed by the Fast Fourier Transform of the specified window. 4. A u d io S p e c tru m C en tro id : This descriptor contains information about the distribution of the power spectrum. The descriptor can be regarded as an

R ep rod uced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

13

approximation of the perceptual sharpness of the signal. It is calculated by adding all the energy within the frequency bins and dividing it by the total energy in the audio frame. 5. A u d io S p e c tru m S p read : This descriptor contains information about the shape of the power spectrum. It indicates if the spectral content is concentrated at the centroid or if it is distributed across a range of the spectrum. It also has the property of discriminating between noise like and tonal sounds. It is calculated by taking the second moment of the log-frequency power spectrum of the signal. 6. A u d io S p e c tru m F latn ess: This descriptor contains information about the tonal components in each band of the signal. It is calculated by taking the ratio of the geometric mean over the arithmetic mean of spectral power within a band. 7. A u d io F u n d a m e n ta l F req u en cy : This descriptor contains information re garding the musical pitch and periodic content of speech signals. Since calcu lation of fundamental frequency is not an exact science, the standard does not specify how the descriptor is calculated. 8. A u d io H a rm o n ic ity : This descriptor contains information about the har monic nature of signal spectrum. It contains two measures: the first, called the Harmonic Ratio, gives a measure of the proportion of harmonic components in the spectrum. The other measure called the Upper Limit of Harmonicity spec ifies the point in the spectrum beyond which no harmonic content is present. 9. Log A tta c k T im e: This descriptor defines the time it takes a signal within a window to start and reach a sustained level or a maximum. The time is represented in logarithmic scale. This descriptor can be used to differentiate between a suddenly rising sound and a smoothly increasing sound. 10. T e m p o ra l C en tro id : This descriptor calculates a tim e based centroid of the signal envelope for a particular segment. This descriptor provides time resolu

R ep rod u ced with p erm issio n of th e copyright ow ner. Further reproduction prohibited without p erm ission .

14

tion of the signal energy, in other words it provides information about the signal energy and where it is focused in time domain. 11. H a rm o n ic S p e c tra l C e n tro id : This descriptor is calculated by taking the amplitude weighted means of the harmonic peaks in a power spectrum. This descriptor is similar to other centroid descriptors, but most commonly used for musical tones. 12. H a rm o n ic S p e c tra l D ev iatio n : This descriptor is defined as the spectral deviation from a spectral envelope. 13. H a rm o n ic S p e c tra l S p read : This descriptor is calculated by taking the power weighted RMS deviation from the Harmonic Spectral Centroid. 14. H a rm o n ic S p e c tra l V a riatio n : This descriptor defines the spectral variation between adjacent frames. It provides the normalized correlation between the amplitudes of two subsequent frames. 15. S p e c tra l C e n tro id : This descriptor is calculated by taking the power weighted average of the frequency in a linear power spectrum. This descriptor is similar to Audio spectrum centroid, but most commonly used for musical instrument signals. 16. A u d io S p e c tru m B asis: This descriptor is calculated by taking a series of basis function derived from the Singular Value Decomposition (SVD) of a nor malized power spectrum. 17. A u d io S p e c tru m P ro je c tio n : This descriptor represents the projections of the basis function calculated with the Audio Spectrum Basis descriptor. 18. S ilence S eg m en t: This descriptor indicates if the audio segment has significant sound or not. It can also include an indicator for different level of silence based on a threshold.

R ep ro d u ced with p erm ission o f th e copyright ow ner. Further reproduction prohibited without p erm ission .

15

The low level descriptors defined in the Audio Description Framework are very useful for building high level audio description tools. For the purpose of this work 3 low level basic spectral descriptors were used, as we were only interested in getting the general sound characteristics from the video shots. The descriptors used are as follows; · Audio Spectrum Envelope · Audio Spectrum Centroid · Audio Spectrum Flatness

2 .3

A p p lic a tio n s o f M P E G -7

Due to the enormous growth in the production of digital content, there is now a critical need for sophisticated applications to manage the content. These applica tions must be able to provide extremely efficient methods of managing, searching and retrieving the digital content. MPEG-7 has taken the step in providing a standard format for describing the content in order for different types of applications to use the descriptions to produce f ^ t and effective retrieval of multimedia data. MPEG-7 provides a vast array of descriptors so th at digital archives, databases and libraries can be queried using not only text but also queries made of spoken words, images, melodies and video. Following are a few examples of th e applications th a t have been developed using MPEG-7. 1. A rtic le B ased N ew s B ro w ser: The application is developed to group related news articles. As detailed in [10] the application provides the users with four key frames which summarize the events. These key frames are anchor key frame, episode key frame, news icon and synthesized text. 2. M u sic B ro w ser: This application provides users to search a music database based on sound, music similarities as well as keyword search capabilities. This application is detailed in [11]

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited w ithout p erm issio n .

16

3. Q u e ry b y H u m m in g ; In this application the user hums a tune for retrieval from a database [12]. The application takes into consideration the general shape of melody from note to note, that is if the note is higher or lower than the previous note. 4. R e a l-tim e v id e o id e n tific a tio n : In this application the video clips are ana lyzed in real-time in order to identify broadcasted content. Details can be found in [13]

In this thesis we are developing an application utilizing MPEG-7 descriptors of motion and audio for sports domain. The application will provide user with an

interface from which they can choose a clip of interest and retrieve similar clips that have been annotated with high level semantics.

R ep ro d u ced with p erm issio n o f th e copyright ow n er. Further reproduction prohibited w ithout p erm issio n .

C hapter 3 P rop osed Indexing S ystem
3 .1 I n tr o d u c tio n

I

N th e entertainment and sports industry of today, video is extensively used. In sports it is being utilized to train athletes, scout for new talent, prepare strategy In the entertainm ent industry,

for th e opposing teams and also for self-analysis.

video on. demand is becoming very common. People are not looking for programming designed by some studio producer, but they are asking for programming depending on their mood and surrounding environment. Broadcasters are providing video for entertainm ent not only over the television but also over the Internet. Video is more a part of the sports and entertainment industry than most people ever imagined. As the technology advances and bandwidth constraints become less of an issue, the demand for video will only increase. The design of any video indexing and retrieval system relies on th e specific users of the system. Different users put different demands on the system based on their needs. For example in the sports video domain, currently two types of video logging approaches exist. First is the production logging, where the producer annotates live feeds or recorded footage to be used shortly, as an example the sports highlights pro gram. Second is the posterity logging, where librarians add detailed and standardized annotation to archived material [14]. This is used by statisticians and other people involved with the industry. Effective video indexing requires a multi-modal approach, the modes being visual.

17

R ep ro d u ced with p erm issio n of th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

18

auditory and textual. Efficient algorithms need to be devised so th a t either the most effective modality is selected or multiple modalities are used in collaborative fashion [15]. Not only do we need to fuse the modalities to effectively index a video based on context but we also need to include domain specific information to provide higherlevel semantic annotation. The domain knowledge can be used in both the indexing and querying aspects of the system [16].

3 .2

R e v ie w o f S p o r ts I n d e x in g S y s te m s

Popularity of sports and general interest of people in sports means th at in every part of the world sports video is being recorded and annotated for future use. Indexing and annotation are mostly done manually. Recently a lot of research has been conducted on autom ating the process of indexing and annotating the video streams. Nearly all the m ajor sports have been used to test the indexing and retrieval systems. But the vast m ajority of the systems designed rely on visual information to index the sports \ddeo. Only a few systems use both the audio and visual information. And some utilize visual, textual and audio information to segment particular events within a sports video. In this section we review some of the sports indexing systems. One of the major projects working in generating semantic sports video annotations is the ASSAVID project. As detailed in [17], this project focuses on developing a system th a t can categorize different types of sports and provide users with an interface to query events in a particular sport. A summary of recent sports indexing systems is given below. These systems utilize low level features to index semantics in a particular sport. As evident from the research activity, an indexing system has been developed for nearly every popular sport in the western world. B ask etb all In the paper by Zhou et al. [18], basketball game is classified using a rule-based ap proach. The rules were calculated using an inductive decision tree learning approach

R ep ro d u ced with p erm issio n of th e copvriaht ow ner. Further reproduction prohibited w ithout p erm issio n .

19

applied to low level image features such as motion, color and edge. T he system was used to classify the basketball video into 9 major events shown in Table 3.1;

F e a tu re s dominant direction motion magnitude Color Edge

E v en ts left/right Offense left/right Fast break left/right Dunk left/right Score Close up

Table 3.1: Summary table of features used and semantic events retrieved in Basketball

T en n is In the paper by Miyamori and lisaku [19], particular tennis events are classified using visual models of the court and the players. First the court and net lines are extracted using a court model and Hough transforms. Then player position is ex tracted and tracked. Then ball position is tracked using special prediction modes. Lastly player behavior is identified using player shape changes. The system was used to classify the 5 events shown in Table 3.2;

F e a tu re s court edges and net player position ball position player behavior

E v en ts forehand stroke backhand stroke forehand volley backhand volley service

Table 3.2: Summary table of features used and semantic events retrieved in Tennis

In the paper by Lu and Tan [20], color features are first used to segment the video and then camera motion is used to identify the volleyball or tennis serve events by different teams/players.

R eo r o d u ce d with n erm issinn nf th e cnnvrinht nwnmr

Fiirthar ranrndi intinn nrnhihitod withmit norm iccinn

20

Form ula 1 R acing In the paper by Petkovic et ai. [21], some of the events th a t occur repeatedly during a race are classified by using audio and visual features. Bayesian Network and Dynamic Bayesian Networks are used to classify events like fly out, passing and starts. Table 3.3 summarizes the features and events:

F e a tu re s Short term energy Pitch MFCC Pause rate color shape motion

E v en ts starts passing fly outs

Table 3.3: Summary table of features used and semantic events retrieved in FI racing

Track smd F ield In the paper by Wu et al. [22], track and field events are classified using a three layer inference scheme. Initially low level features like global motion, color and texture are extracted and used by the system to segment the clips into semantic units. Then semantic concepts are extracted using learning RBF neural networks and decision tree classifier. Finally, a rule based finite state machine is designed for event inference. Table 3.4 summarizes the features and events;

F e a tu re s Global Motion Color Texture

E v en ts High Jump Long Jump Javelin Weight throwing Dash

Table 3.4: Summary table of features used and semantic events retrieved in track and field

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

21

S occer In the paper by Assfalg et al. [23], two models are devised. The first model relies on motion features only while the second model also uses the location of players on the field. Hidden Markov model is used to classify the shots into 3 events. The features and events are summarized in Table 3.5.

F eatu re s Motion Player position

E v en ts Penalty Free kick Corner

Table 3.5: Summary table of features used and semantic events retrieved in soccer

B a se b a ll In the paper by Han, Chang and Gong [24], baseball highlights are classified into views, which are then used in a Hidden Markhov Model to detect 4 types of play events. The features extracted are primarily based on camera motion, color of grass or field, edge detection for player height and texture analysis of the field and shape analysis of the field. Table 3.6 summarizes the features and the classified views and events;

F e a tu re s Camera Motion Player height Shape Texture Color

E v en ts Home run Catch Hit Infield play

Table 3.6; Summary table of features used and semantic events retrieved in baseball In the paper by Han, Hua, Xu and Gong [25], an entropy based model is used to classify events in baseball. Closed caption text, audio features such as Mel cepstral coefficients and visual features such as color distribution, edge distribution, camera

R eo ro d u ced with o erm ission of th e coovriah t ow ner. Further reoroductlon orohiblted without o erm issio n .

22

motion and player tracking are used as features. Table 3.7 summarizes the features extracted and events classified by the authors.

F eatu re s Camera Motion Player tracking Edge Color MFCC Closed Caption Texture

E v en ts Home run Outfield hit Outfield out Infield hit Infield out Strike out Walk

Table 3.7: Summary table of features used and semantic events retrieved in baseball

Football In the paper by Miyauchi [26], audio, textual and visual information is used to classify American football video. Touchdowns and field goals are detected. The paper is an extension of previous work where only textual and visual information was utilized. In this paper the audio energy of a particular shot is also extracted and it is shown th a t the precision rate of the system is increased by adding this feature. Table 3.8 summarizes the features extracted and events classified by the authors.

F eatu re s Closed Caption text Short term audio signal energy Dominant Color

E v en ts Touch down Extra Point Field Goal

Table 3.8: Summary table of features used and semantic events retrieved in football

In the paper by Lazarescu [6], American football games are classified into events using the natural language commentary from the game, the geometrical information about the play and the domain knowledge. Only five formations are classified with results shown for four. Table 3.9 summarizes the features and events classified.

R eo ro d u ced with n erm issinn of th e cnnvrinht ow ner. Further renrorii lotion nrnhihiterl without o erm issio n

F eatu re s Natural Language Commentary Player tracking

E v en ts Pro formation I formation Single back formation Goal line formation Far formation

Table 3.9: Summary table of features used and semantic events retrieved in football As evident from the above review of previous works, the area of sports video indexing is a very active research area. Nearly every sport has been investigated and a wide variety of features have been utilized to classifj'- semantic events. The next section will provide details on the motivating factors for developing an indexing system in American football domain and also highlight the contribution made in this thesis.

3.3

M o tiv a tio n an d C o n tr ib u tio n o f th e p r o p o se d s y s te m

The concept of "0 n Demand" entertainro.ent and programming is fast becoming a reality w ith the popularity of digital TV channels. Now nearly every professional sports league and team in North America has a digital channel boasting of on demand programming and statistics. But the reality is th at it takes nearly three to four hours in post production work to prepare the highlights for a game. For example, on NFL Sunday Ticket you get Highlights-On-Demand on Monday morning for the games played on Sunday. In order to minimize the delay between the live broadcast to " On Demand" programming we need technological advancements th a t can analyze the contents of the broadcast and derive the semantics from the input. These semantics can be m ade available to the users for querying in order to create a true " On Demand" experience. The prim ary motivation is to develop an indexing system for American football th at could be easily implemented in a hardware device, thus providing " On Demand"

23

Ronrnrli irîArl with norm iccinn nf thn nnnurinht nwnnr

Piirthnr mnrnHnntinn nrnhihitoH \A/lthniit norm lecinn

24

indexing to the users. In order to accomplish this goal we had to utilize data that was readily available and also use features th at did not require complex computations but were able to discriminate between the different plays effectively. Keeping these objectives in mind, we decided on using motion vectors, which are already encoded in the MPEG bitstream. Thus only partial decoding of the bitstream is required to extract the motion vector information. From the discussion in Section 3.2 we can see that motion is a popular feature utilized in the sports indexing community. This is due to the fact th at the sports domain consists of Recurrent Visual Semantic (RVS) events. For RVS events the color and texture information usually stays the same but motion varies. Thus motion carries a wide variety of information that can be utilized to discriminate between the different RVS events. We also utilized audio information from the MPEG files. The audio track can be easily de-multiplexed from the MPEG bitstream without much computation, as most of the MPEG computational complexity is in the encoding of the image frames. The audio processing has long been established and is mature in the research community. The audio features provide complimentary support to the motion features, as using only audio information to discriminate between RVS events is very difficult if simple classification techniques are to be utilized. Also seen from the section on review of sports indexing techniques, most of the indexing schemes utilize some sort of domain knowledge to fine tune the system for a particular sport. In the paper by [6], the authors propose a domain knowledge system based on sets and subsets concepts in the game. In this thesis we are also proposing an American football knowledge base, but it differs from the one proposed, such that fundamental set proposed in this work consists of RVS events and not concepts of the game. In addition this thesis work focuses on utilizing existing standard MPEG-7 de scriptors as the basic features in order to index events in American football. Some of the researchers such as Divakaran [5], have proposed applications for generation of summary highlights in sports domain, but no one has yet to our knowledge used the MPEG-7 descriptors to index RVS events in the American football domain.

Ronrorli inorl \A/ith normic.cinn rvf tho nnm/rinht n\A/nor

Piirthor ronrnrli mtinn nrnhihi+oH \A/ithni it normictcion

25

3.4

P r o p o s e d S y s te m O v erv iew

The pro]3os(ul system eunsists of three stages. The first stage is responsible for lo calization of action within the video shots. The second stage extracts the MPEG-7 descriptors and the audio hlel Fi'Oqucmcy Cepstruin features. These fc'atures are then })assed to the third stage, the classification stage. The systcmi is detailed in Figure 3.1.

Audio De-Multiplexer

MFCC Feature Matrix Audio Descriptor Extractor

Video Stiots DB

MPEG-7 Motion Activity Extractor

Piay Start Detector

Linear Discriminant Classifier

Indexed Video Shots

Localization Phase

Motion Descriptors Extractor Feature Modeling P hase

C lassification P hase

F igure 3.1: Proposed System overview

3 .4 .1

S tage 1: L ocalization P h a se

The primary motivation of designing this stage was to reduce the tuialysis window size and secondly to remove the features that do not directly contribute ttj the action. The issue of localization of action in the sports domain is extremely important. In many sports like golf, American football, tennis, bowling and baseball, the ])layers come to a certain position before starting the play. Then the play is followed by a delay before the next action takes place. This gap between the plays contain information that is not directly related to the scunantics of the game.

R o n rrv H i

\A /ith

n o r m i c c i n n

n f

t h n

n n n \ / r i n h t

n v A /n o r

P u r t h o r

r o n m H i in tin n

n r n h ih ito r l

vA ylthniif

n o r m i c c i n n

2G

Figure 3.2

h I io w h

the details of the first phase. This phase can be cunsidered as

the pre-processing stage of the. indexing .system.

Video S h o ts DB

MPEG-1

J a v a MPEG1 Motion V ector E xtractor

M PEG -7 D escription E ngine

Play S tart D etector E ngine

Frame Number

F igu re 3.2: Play localization phase overview

First the motion vectors from the MPEG-1 video shots are passed through an MPEG-7 engine to extract the motion activity descri]itors. This stage calculates the mean and standard deviation of the intensity of motion activity for each frame. These two components of MPEG-7 motion activity descriptor are then passed to the dc'tection engine.

3 .4 .2

S ta g e 2: F eatu re M o d e lin g P h a se

The feature' c'xtraction and modeling phase is t.hc' heart of every indexing and retrieval systems. At this phase most of the important decisions are made regarding the

features that, can optimize 1he performance of the systc'in. In this stage the features are also normalized in order to minimize the bias. In the proposed system, three types of features are extracted from two groups of modalities. The first, group is based on the audio content of the video .shots. The second group is based on the visual content or more spei'ifically, the motion content, of the video shots.

R a r t m H i m o H \A /ith n o r m i c i c i n n r \f t h o r * n r \ \/r in h t n \A ;n o i'

P i i r t h o r r o n r r v r ii l o t i n n n r n h i h i t o H \A /it h n iit n o r m i c c i r v n

27

A udio Features Every sport lias a language associated with it. This specific language is used by most of the commentators to desmibc^ the action. In American football there are many different events th at take place, each play has its own specific words and its own rhythm, which are mostly spoken by tlie commentators to describe the play. In theory most of the similar types of plays will have similar sounding words s]ioken by the commentator. Thcaefore we want to extract the general sound characteristics from the audio information and use this to classify the video shuts into different categories. The first set of features were extracted by getting three (3) spectral MPEG-7 audio descriptors. The descriptors used were: 1. Audio spc:c.truni envelope 2. Audio spectrum centroid 3. Audio spectrum flatness

These descriptors were chosen since each one defines a specific property of the au dio signal. The first descrijitor. audio spectrum envelope, represcmts the log-frequency nature of the audio signal. The second descriptor, audio spectrum centroid, represents the sharpness of the audio signal and the last descriptor, amlio specdrum flatness, rep resents the tonal component in the audio signal. Therefore these three descriptors in combination provides details about the spectral characteristics of the audio signal. Figure 3.3 shows the details of feature extraction for the first set of features. First the de-multiplexed audio file is passed to the MPEG-7 engine to ext.rac;t the above mentioned audio descriptors, which are then normalized and (quantized into 10 bins. This provides us with 30 features related to the spe<.:tral audio desmiptors.

RonrnrliinoH

\A /ith

normiccinn nf tho nnnv/rinht nwnor

Fiirthor ronrnHiintinn nrnhlhitoH \A/lthniit normic.Qinn

28

Play S ta rt D etecto r E ngine

Frame Number

V ideo

Shots
DB

MPEG-1

Wav file Audio De-M ultlplexer

M PEG -7 D escription E ngine

F e a tu re M odeling

Feature Vector

F igu re 3.3: Audio feature inodeliug pliase

The Hecond K C 't of features eoiisist of Mel Freqiieney eepstnim Coefficients (MFCC). Due to the fact th at most of the video shots contain a lot of crowd noise, and we want to extract tint perceived rhythm and sound of the spoken content, we needed a feature th at can model the human hearing and also works well under noisy condi tions. MFCC has been used extensively in the speech recognition systems as it tries to emphasize the frequencies that are more perceptive to the human ear. Figure 3.4 shows the details of MFCC (extraction.

Ronmrii infid with nermiftcinn nf thn nnnv/rinht nwnnr

Fiirthnr rnnrnrlimtinn nrnhihitnH wlthniit narmLQÇiinn

29

Original Audio Signal
> V A'

'- #
`
V c

: ^ . iSsS
#

;V t »T>,

- ..-.y- ·

% C aK % > «w

Kr
'

^\

-

After ' ......... Blocking and Windowing and FFT operations

^L

t  'V i? ÿ C '^ :S ';< ` 'i'iî)' ^ 'l » ' ^i`|« ^ r < 4 -6 p .
After Mel Scale % Filter Bank 3 operation
{ -T ^ i. i ^ r * > -i < J

i
"«WAkapweagA, » fli» « j^..aiJw £ g .% > M & .s& SaS. k KI * * _'.

9

% -W 1 I 1

.L.'-rw.

'

After Log and IDOT operations

F ig u re 3.4: MFCC extraction process

First the audio file is pre-processed in order to remove the silent segments. Then 13 MFCC coefficient.s are extracted for each segment. Each of the segments have 50% overlap and thu.s there is lot of redundancy between adjacent MFCC values. This can be seen in the Figure 3.5. The blue colour represents low values and the red colours

R en rn d u csd with o erm issio n of th e coovriah t ow n er. Further reorodiiction nrohihited w ithout o er m issio n .

30

represent high values. In order to reduce the dimension of the matrix, the MFCC values are passed to a feature reduction stage.

30

40

(f of audio Frames
F ig u re 3.5: MFCC feature redundancy

The MFCC features are reduced to a 12 x 64 matrix. The first MFCC feature in every audio frame represents the average energy of the audio frame, therefore this feature is discarded. The other 12 coefficients are retained for each segment. 64 is the number of segments remaining after redundant feature reduction. M otion F eatures Motion plays an integra-l part in many sports indexing and retrieval systems. In this stage the MPEG-7 motion descriptor of motion activity is extracted with an optional descriptor of dominant direction of motion. Figure 3.6 shows the details of the feature extraction of motion.

R e o r o d u ce d with o er m issio n of th e coovriah t o w n e r. Further reoroduction orohibited w ithout o e r m issio n .

31

Play S tart D etector E ngine

Frame Number

V ideo S h o ts DB

MPEG-1

MPEG-1 MV E xtractor

M PEG -7 D escription E ngine

F e a tu re M odeling

Feature Vector

F igu re 3.6: Motion foanire iiiotk'ling iiiia.st'

First the MPEG-1 uiotiou veetors are jiasscd to an MPEG-7 engine wiiieli ex tracts the statistical itruperties of mean ami stanclnrtl clc'viation. It also caicnlates tlie dunlin ant direction of mot ion. The intensity of motion activity ilescriptor and tlie dominant direction descriptor are qua.ntized into a two dimensional matrix as sliown in Figure 3.7. The intensity of motion descriptor is cjuantizr'd into 12 liins while the dominant direction descriptor is quantized into 8 bins. This gave us a 12 x 8 matrix feature wliicli sinmitaneonsly represents tlie motion intensity and direction of motion in a video shot.

R ep ro d u ced with p erm issio n of th e copyright ow n er. Further reproduction prohibited w ithout p erm issio n .

32

tosgs M a|! ofW sui'itude wd D'wclion

3-

4

· S-

6

1

D iim W ant D irection iiinsi

F ig u re 3.7: Motion feature quantization

Another set of motion features is extracted by calculating the mean and sta.ndard deviation of the magnitude of motion vectors with in a specified window. First the highest peak of the magnitude is detected and then the adjacent motion vector magnitudes are included in the window. This provides us with 2 more features.

3.4.3

Stage 3: C lassification P h ase

The primary objective in de,signing this phase was to utilize classification schemes that were simple and efficient. A decision had to be made on what type of classification scheme can achieve the goals of this phase. First the decision was made to utilize a .supervised classification scheme, as un.supervised classification scheme use iterative algorithms which can be computationally expensive in a large data set. Secondly a decision was made to utilize linear classifiers ra.ther than non-linear classifiers. The reason for this was to evaluate the performance of the system by using a simple classification .scheme finst and then evaluate the need to utilize more complex classifiers such as Neural Networks or Radial Basis Functions.

R ep ro d u ced with p erm issio n of th e copyright ow ner. Further reproduction prohibited without p erm issio n .

33

Linear Discriuiiiiaut Analysis (LDA) generally refers to teelniiques that output a discriminant function th at take linear inputs. In a specific sense LDA also commonly refers to teclmique.s in which a transformation is done in ord(>r to nniximize. hetweenclass separability and minimixe with-in class variability. LDA works on the feature set with no prior a.ssumptions about the nature of the data set. It tries to compute a weight vector w. which winni multiplied by the input feature v('c:t.or x would generate discriminant functions g,[x). For C chusses problem we define C discriminant functions gi{x)...gc{x). The feature' vector x is assigned to a elas.s whose discriminant, fimct ion is the largest value of x. as given by the following equation. g.,{x) = ma.x^gj{x) . (3.1)

LDA has been a proven chi.ssifieation scheme. Thert'forc' in oirler to perform the classification in the proposed .system we utilised software jiackage SPSS. Figure 3.8 shows the details of the classification phase.

Audio F e a tu re

F e a tu re S electio n

C lassification E ngine

F e a tu re DB

Motion F e a tu re s

F igu re 3.8: Classification plta.sc overview

R ep ro d u ced with p erm ission of th e co p y rip ht ow n er. Further reproduction prohibited without p erm ission .

34

The first part of the classification phase performs feature selection using a scatter matrix of within class spread and between-class spread. The criteria maximized is given by Equation 3.2. T r{S ^ S a}, where (3.2)

is the within class spread and Sg is between-class spread and T r is the

trace function. After feature selection, th e features are classified using Fisher's criterion for LDA. This generates discriminant functions for each class and the selected features are classified in a class th a t has the highest discrimination value.

3 .5

T est D a ta b a s e o f A m e r ic a n F o o tb a ll V id e o S h o ts

The test database was created by recording some of the National Football League games broadcasted during the 2003-2004 season. The games were recorded from all the four major networks, namely: ABC, CBS, ESPN and FOX. The recorded video was manually cut into shots containing all the details of a game. The shots were indexed into three categories namely: pass plays, run plays and kicking plays. The database consists of 200 video shots with durations varying from 5 seconds to about 25 seconds. In the database there are 88 pass plays, 67 run plays and 45 kicking plays. A total of 8 difi' e rent teams were used to create the database from 4 different networks. This variety in the database ensured th a t the sample space of our work was diverse and included different types of production styles.

R eo ro d u ced with o erm ission of th e coovriah t ow ner. Further reoroduction orohibited w ithout o erm issio n .

Chapter 4 Sem antic Localization
4 .1 I n tr o d u c tio n
h e concept of segmentation or localization of objects within a multimedia doc ument has been an area of active research for quite some time. For example

speech recognition relies on good localization of phonemes in order to perform simi larity matching.. Object segmentation in images has been implemented in MPEG-4, which has the option of coding the entire video frame or arbitrary objects within a frame. Segmentation of objects from multimedia documents can be done in both the temporal domain or the spatial domain. Speech recognition is an example of temporal segmentation while segmentation of objects from images is an example of spatial segmentation. Likewise object tracking within a video is a combination of temporal and spatial segmentation. The philosophy behind segmentation is to extract meaningful information from the multimedia object. The segmented object must contain semantic information so that it can be easily mapped into human perception. For example, we localize words from spoken content, since humans can relate to words and not to the signal energy. We segment objects like sun, tree or house from the images and not color or texture. Building on the above mentioned philosophy, our objective was to localize play events from American football video shots. This localization will provide us with a semantic unit which can then be classified into a specific category. In this chapter, we

T

35

R eo ro d u ced with oerm issio n of th e coovriaht ow ner. Further reoroduction orohibited without o erm issio n .

will review some of the related works within a sports video context. Then, we propose a novel algorithm to segment plays, followed by the experimental results detailing the performance of this algorithm.

4 .2

R e la te d W ork s

Sport events have very well defined structures. They have a set of rules th a t must be followed in order for the game to be played properly. This definite structure provides a variety of clues which can be used to segment the sports video. Therefore most of the segmentation and localization algorithms within, the sports domain rely heavily on specific sport knowledge base. In the paper by N itta et al. [29], they proposed a scheme to localize semantic events by using closed caption (CC) text. The objective was to create semantic story events by evaluating CC text for keywords. The authors first segment the (CC) text based on pauses between dialogue or speaker changes. These segments are then input into a Bayesian network to evaluate the probability of the segment containing semantic information about American football. In the paper [30], Li and Sezan proposed a sports highlight generation scheme. The authors first segment video in two categories, namely: play and non-play events. The play events are detected by using low level features such as color, texture, motion and player shape and movement. Based on these features two models are developed to categorize the video segments. The first is a rule based inference model and the second is the probabilistic inference model based on HMM. The authors experimented this scheme with baseball, football and sumo wrestling.

4 .3

P r o p o s e d A lg o r ith m

Many sports such as golf, baseball, bowling and American football have a requirement th a t the team or players must be in a distinctive position before each play. In golf the player positions himself by the ball in order to hit it in a certain direction. In baseball the batter awaits for the pitcher to go through its motions and deliver the 36

R eo ro d u ced with oerm issio n of th e coovriaht ow ner. Further reoroduction nrohihited without n erm issinn

37

pitch. Likewise in American football the two teams first line up face to face before the. ball is snapped to begin the play. The common theme among all these sports 1 ,9the perceived motion activity, before and after the play starts. This distinction in the motion activity is utilized in the proposed algorithm in order to divide the video into non-play event segments and play event segments, as seen in Figure 4.1.

C om parison of motion vector m agnitude in different plays

60

S ·I I 'S
40 Run Play

Non Play Event Segment

50

100

150 200 250 P type frame num ber

350

400

F igure 4.1: Mean of Motion Vector.s for different tyires of play.s

The primary objective of the algorithm is to detect the key frame that can be used as the starting point of the play event in the shot. The end point of the play event is not extracted, as in most American football video shots containing play events, the shot usually terminates at the end of the play. Therefore, the algorithm is designed to detect the instance where motion activity in the video shot is sustained at a certain level. In order to extract the intensity of motion descriptor, MPEG-1 video motion vectors are used. Only the motion vectors from the P frames are analyzed in order to

R enrndiiced with n erm issinn nf th e nnnurinht nvuner

Further renrndi intinn nrnhihiterl withniit nerm issinn

38

speed up the processing time. In MPEG-7 the motion activity descriptor represents the standard deviation of motion vector magnitudes within a frame. This is given by the following equation.

O 'm v -- Y

-p j

)

where M A G m v is the magnitude of motion vector with coordinates {x, y), and is calculated by M A G m v = defined as: 4- y'^. Hmv is the mean of the motion vectors and is

=

(4 4

where N is the number of macro-blocks that have a motion vector coded in the MPEG-1 stream. The number N varies from frame to frame as not aU the macro blocks are coded with a motion vector. The two features (/Tm w and amv) are used

collaboratively in the algorithm to detect the start point of the play. Figure 4.2 shows the plot of the mean and standard deviation of the magnitudes of motion vectors in a video shot. An analysis w ith 20 video shots selected from each category was conducted to estimate the thresholds for the mean and standard deviation of motion vectors. From the analysis it was found th at at the starting point of the play the mean was consis tently within a range of 3 and 4.5, while the standard deviation of the motion vectors in the frame ranged from 1.2 till 4.2. This large variation between the standard devi ation was due to the fact th at in some video shots the play started as the camera was zooming in. Therefore based on the results from the above analysis, the threshold for mean was set at 4. In the MPEG-7 standard the motion activity descriptor goes from level 1 to level 2 when the standard deviation of the motion vector magnitude reaches 3.9. Therefore in our algorithm the standard deviation threshold corresponds to this change in level.

Ronm di ined with nfarmiRs.inn nf thn nnnv/rinht nwnnr

Fiirthnr mnrnHnntinn nrnhihitnri wlthniit nnrm issinn

39

I « y»<rj«y

I

: ................................ 'r ^ I» 

Vl" r.... j .......#   râ

y y -

Figure 4.2: Mean and Standard deviation of Motion Vectors Figure 4.3 shows the How chart of the proposed algorithm to estimate the frame which represents the starting point of the play event. The steps of the algorithm are explained below: · S tep l: Find a P frame with a mean value of 4 or higher · Step2: Determine the gradient of the mean values within a window (3 or 4 adjacent frames) · Step3: If gradients are all positive mark the frame as possible starting point,
else go back to Step 1.

» Step4: If the intensity of motion descriptor has a value of 2 or higher, return frame number as the starting point · StepS: If the intensity of motion descriptor has a value of 1, determine the

R o n rn rli

w i t h n f a r m i R s i n n n f th<=» n n n w r i n h t n w n o r

P i i r t h o r ro n r n H i in tln n n r n h i h i t o H w ith rv iit n o r m l c c i n n

40

gradient of the standard deviation values within a window (3 or 4 adjacent frames) S te p 6 : If the gradients are all positive return the frame number as the starting point, else go back to step 1.

No

Yes

Step 1 No

Yes Calculole stpntJprd deviation (a,,,,)

- *' i 6fjv < vkpfprne'" ' \

R eturn F ra m e N o As startin g point

W F ^adiacerjlfi^n^gS iW ill^ipv^iC idg.vtf'
gradient of véilues"S te p 1

No
R etu rn F ra m e N o As starting point

F i g u r e 4 .3 : Flow c h a r t o! p ro p o se d a lg o rith m

R o r v r n r l i in /^rt u/l+ln n / = s r m l c . c i n n n f t h o p n r v \ / r i n h t n v / u n o r

P n r t h o r r o n r n r t i m t i n n n m l n l K i + û H \A/i + hm it n o r m l c e i o n

41

4.4

P la y start d ete ctio n results

The above algorithm was tested on the American football video shot database which consists of 200 video shots talcen from 4 dilferent games and 4 different networks, as detailed in Section 3.5. In order to measure the perform ance of the algorithm , we had to establish the ground truth about the startin g point of the play event within each video shot. To e,stablish the ground truth a.n observer manually indexed the frame number within a video shot which best represented the sta rt point of the play event. Comparison of results was done by calculating the difference between the ground truth frame num ber and the frame number estim ated by the algorithm. Figure 4.4, shows the deviation of the estim ated frame numbers from the ground truth.

OompstCl.son G round iru^d v s Estlmaved Window size = 3

ideo Shots

F igure 4.4: Deviation oT eat.iinabed starUng point from ground truth

It was noticed th at we needed to develop some type of strategy to represent the results, since having a deviation chart only .showed quantifiable re.suIts. T he results

R o n r n H i i P o r l \A/ith n o r m i c c i n n n f t h e n n n v r i n h t n \A /n o r

P i i r t h o r r o n r n r l n n t i n n n r n h lh it n r l

\A /ith n i it

n o r m ic c in n

42
still needed to be evaluated in term s of what this deviation means in actual time. Thai, is, wo needed to evaluate if the algorithm is estim ating a startin g point too early or if it is estim ating the starting point after a certain am ount of delay. Since MPEG-1 video has a fra,me rate of 30 frames/sec, building a histogram whose bin size was 30 frames would give a general idea of how ap a rt the estim ated frame numbers were from the ground truth in time domain. Figure 4.5, shows the performance of the algorithm using a window size of 3. T he figure, details what percentage, of estim ated fram e numbers relate to early and delayed detection of the play event. From the figure we can see th at the algorithm is able to detect 83% of the startin g point within 1 second of the ground truth starting point.

Nutnjber of sh ots

60

-7

-6

-5

-4

-3

,2

- r ,0 ;'

1

2

3 "
-

'
\

'

N u m b e r o f S ü L onüp

*

\A *

Figure 4.5: Performance of proposed algorithm in time domain

RonrnHi

lA/ith normiccinn nf tho nnnv/rinht nvA/nor Piirthor rnnrnrli intinn nrnhihltorl VA/ithni if normic.cinn

43

An experim ent was also done by varying the window size to 4. Figure 4.6 shows the deviation between the estim ated fram e num ber and the ground truth. W ith the window size of 4, we are looking for the m otion activity to be sustained for a longer period as com pared to with window size of 3. Therefore, the algorithm will detect the startin g point after a little delay. Thus in Figure 4.7 we see a lot m ore video shots in the range of +1 second and + 8 second.

|CpmP3rison Ground truth ys Estimated Window size =; 4

F ig u re 4.6: Deviation of estimated starting point from ground truth

4.5

C on clu sion s

In this chapter an algorithm was proposed to localize the play event within a video shot. M PEG-7 intensity of motion descriptor along with tire mean of motion vector rna.gnitudes was used to detect the starting point of the play. Keeping with our

objective of fast and .simple, we devised the algorithm th a t worked by analyzing

R o n r n r l i m o r l lA /ith n /= » r m ie c .in n n f t h o n n n v / r i n h t n v A /n o r

P i i r t h o r r o n r n r l i i n t i n n n r n h i h i t o H V A /Ith n iit n o r m i c c l n n

44

lé s in lim e d om ain , w in d o w = 4

Number of sh o ts

60 -

F ig u re 4.7: Performance of proposed algorithm in time domain the m ean and standard deviation of the motion vectors w ithin a video shot. T he algorithm relied on dom ain knowledge, th at is in American football before the play sta rts the two team s face each other and stay in a particular' position for a period of time. Thus providing low intensity motion ju st before the s ta rt of the play. T h e algorithm detected the startin g points of the pi a,y with 83% a.ccuracy, th at is 166 of the 200 video shots in the database had the startin g points detected within ±1 seconds of the original starting point. T he accuracy of th e algorithm can be

increased to 86.5% by increasing the window size from 3 frames to 4 frames. B ut this change in window size has its own side effect. By increasing the window size we are looking for motion activity being sustained for a longer period of time. T h e trade off is th a t we get m ore shots th a t are detected after the play has started . In some of the cases the play sta rts w ithout much motion activity happening in the surroundings.

R a r t m H i

lA /Ith

n o r m l c c i n n

r \f t h o

r-m rw /rim h t rw A /n o r

P i i r t h o r r o r \m r lt

ir 'tlo n

r\rr\h lK lto W

\A /lth r\i it

r v o r m l o d o n

45

In these cases, the starting point estimated by the algorithm is delayed a few seconds compared to the actual play start. Table 4.1 shows a comparison of the performance of the algorithm utilizing the different window sizes.

Time index -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6

WinSize=3 1 2 3 3 4 3 12 17 115 34 6 0 0 0 0

WinSize=4 0 3 2 1 2 2 6 8 117 48 8 1 0 1 1

Difference(3 vs 4) -1 +1 -1 -2 -2 -1 -6 -9 +2 +14 +2 +1 0 +1 +1

Table 4.1: Comparison table of play detection performance using window size 3 vs 4

As seen from Table 4.1 columns 2 and 3, with a window size of 3 frames we have only 6 shots that are detected after the first second and all of them were detected within 2 seconds of the play start. But with a window size of 4, there are 11 shots that are detected after the first second, and one shot is detected 6 seconds after the play start. In the design of the system we used window size of 3, as most of the shots are detected within a couple of seconds of the play starting point. A 6 second delay is too much, as some critical feature information will be lost during this duration. The robustness of the algorithm can be estimated by the fact that four different broadcasting station were used to construct the database, thus incorporating diverse styles of production and camera movements. Therefore it can be concluded that this algorithm can also be utilized for other sports in which the players take a specific position before starting a play.

R ep ro d u ced with p erm issio n of th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

C hapter 5 Indexing o f A m erican football
5 .1 I n tr o d u c tio n

One of the biggest application areas for MPEG-7 is multimedia indexing and re trieval. The feature formulation and classification strategy forms the essence of all indexing and retrieval applications. The standard does not specify how the features are extracted or how the the standard descriptors are used in a classification scheme. MPEG-7 creates a standard description made up of low level features, such that apphcations can be developed without regard to how the features were extracted. The applications take the standard descriptors and combine them with other descriptors in order to create a feature space which can be used to index and query the related database. As evident from the review of works done in Section 3.2, indexing of sports video into semantic events requires a complicated strategy. It requires multi-modal features as well as domain knowledge in order to build an effective indexing system. Since the introduction of the MPEG-7 standard, there has been significant research effort put in developing applications based on MPEG-7 descriptors. In Section 2.3, we presented a variety of commercial that utihze the standard descriptors. But to this date there has been only a few applications that utilize MPEG-7 descriptors for sports video indexing and retrieval. The application we are proposing is a first in the American football domain, which utilizes MPEG-7 motion and audio descriptors along with MFCC features. 46

R ep ro d u ced with p erm issio n of th e copyright ow ner. Further reproduction prohibited w ithout p erm issio n .

Section 5.2 details the descriptors and features utilized to create the feature space of the proposed application system . Section 5.3 details the classification results ob tained by utilizing a LDA technique. Section 5.4 presents the conclusions and obser vations on the proposed system.

5.2

F eature E xtraction

From the review of sports indexing applications detailed in Section 3.2, we can see th a t audio and motion play an im portant role in providing discrim inant features for .sports dom ain video indexing and retrieval. In the case of American football, visual or motion features play a significantly dom inant role in discrim inating between different types of plays as shown in Figure 5.1. Therefore first we evaluate the efficacy of using motion descriptors for an American football video indexing system and then we evaluate the changes in system performance by adding audio descriptors and MFCC features.

C o m p a ris o n of m o tlo n v e c to r m a g n itu d e in different p la y s

60

t

P a ss Play

R un P la y

Feild

f 30
J 20

G o al

50

fop

150

200

2^

300

350

400

P ty p e fram e n u m b e r

F ig u re 5.1: Different type of Motion Activité

47

mfEASW W ia Â d ' ijgT.' T

R ep ro d u ced with p erm ission of th e copyright ow n er. Further reproduction prohibited without p erm ission .

48

5.2.1

M P E G -7 M otion D escriptors Feature M apping

The m otivation behind using the motion descriptors was due to the fact th at in American football the global motion between different types of plays provides a variety of clues. In order to understand fully the difference in motion between the plays, first we require a detailed explanation of general motion involved in the plays: · P a s s P la y s: During a pass play first the motion is lateral in order to track the movements of a quarterback who is going to throw the ball. Then it is followed by rapid zoom out and followed by a lateral movement to follow the throw. At the end of the play the motion is tracking the player to whom the ball was thrown. Therefore, the movements for a pass play involve first low intensity lateral movement followed by high intensity zoom out and lateral movement and then in the end low intensity lateral movement. Figure 5.2 shows some of the key frames from a. pass play.

F ig u re 5.2: Key Frames of Pass Play

· R u n P la y s: During a run play first the motion is lateral as the runner gets the ball. Then the cam era zooms in, to track the movements of the ball carrier. This zoom in provides the perception of high intensity motion. At the end

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm issio n .

49
the cam era tracks laterally the movements of the ball carrier. Therefore, the movements of a run play involve first low intensity lateral movement, followed by short high intensity lateral movement and in the cud low intensity lateral movement, Figure 5.3 shows some of the key frames from a run play.

Pi

Is I l-F PI

I nart '

j i r In*

\ n e i 11 m I

I f] " '

F ig u r e 5 .3 : Key Frames o f R u n Pla,y

K ic k in g P la y : In the kicking play category, there are two different types of kicics th at take place. Each one has a completely different type of motion

associated with it. The two types of kicking plays are detailed as follows: -- K ic k o flf/F u n t ( K / P ) ; In this category of kicking play, the kicker starts with kicking the ball high in the air. This motion causes the cam era to rapidly zoom out to capture the kicked ball. After the kick the camera zooms into the player who has the ball and tracks the movements of the ball carrier. Therefore this play has movements th a t involve first high

intensity motion of zooming out and zooming in with horizontal direction movement, followed by low intensity motion laterally. Figure 5.4 shows some of the key frames from a kickofl'/puut play.

R ep rod u ced with p erm ission of th e copvriaht ow ner. Further reproduction prohibited without p erm ission .

50

F ,sl

,

l-iki Ml

F ig u r e 5 .4 : K ey R a m e s of Kickofi'/Punfc P lay

- F ie ld g o a l / E x t r a p o in t ( F G / X P ) : In this category of kicking play, the ball is long snapped (short under hand throw) to a holder who sets the ball up to be kicked by a kicker. The m ajority of the movement is low inten.sity with m ost of it coming after the kick when the cam era is tracking the kicked b-all as it sails towards the goal post. Therefore the majority of motion in this category is vertical and low intensity. Figure 5.5 shows some of the key frames from a field goal/ extra point play. T he global motion of camera, the intensity of motion and the direction of motion provide valuable discrim inating information regarding different types of plays. In this work we build the motion based fea.lure set by utilizing intensity of motion descriptor and dom inant direction descriptor of MPEG-7. The m otivation behind using the two descriptor in combinatirm comes from analyzing the expla.nation of different pla.ys as formulated above. T he m agnitude of motion vectors was calculated by extracting the encoded motion vector given by coordinates ( x , y ) from the macro blocks within P fra,mes of the MP.EG-1 video stream . T he m agnitude is given by the following equation:
M A G m v = \!

(5.1)

R ep rod u ced with p erm ission of th e cop vh ah t ow ner. Further reproduction prohibited without perm ission.

5 1

N

k , r k - , _ h / , . , n , . n

.

. C .» -n o M r - 'C .

M il

F ig u r e 5 .5 : K ey Fram es of F ield G o a l/E x tra P oin t Play

According to M PEG-7 description the. standard deviation of the m agnitudes of motion vectors formulates the intensity of motion descriptor. T he descriptor takes on the value of 1 through 5, 1 meaning low intensity and 5 meaning high intensity. For the purpose of this work, the standard deviation of the magnitudes of motion vectors is quantized into 12 levels. Experim ents showed th at by using only 5 levels, the discrimination between plays was significa.ntly lower. Thus to provide better resolution of motion activity, the m agnitude of motion vectors were quantized into 12 levels. Similarly the direction of each of the motion vectors encoded in the macro blocks of P frames of the MP.EG-1 video stream was calculated. The direction of the motion vector is calculated by using the following equation:
Q mv -- a rc ta n (-)
X

(5 .2 )

According to M PEG-7 description the dom inant direction descriptor is calculated by quantizing the angles of the motion vectors into 8 levels as shown in Figure 2.2. For the purpose of this work, the same 8 quantization levels were used to define the dom inant direction descriptor.

R ep rod u ced with p erm ission of th e copvriaht ow ner. Further reproduction prohibited without perm ission.

52

T he fact that the intensity of motion descriptor and dom inant direction descriptor cannot provide sufficient discriminating features if utiiized independently, we decided to create a 2D feature map of intensity of motion descriptor and dom inant direction descriptor. This 2D map consisted of 12 levels for intensity and 8 levels for direction, as defined above. Figure 5.6 .shows the feature map developed based on the two motion activity descriptors for a video shot. In the feature m ap the blue colour corresponds to low values and the red colour corresponds to high values.

; I m g e M ap of

Dominant Direction bins

F ig u r e 5 .6 : M otion feature m ap

T he motivation behind this was to create a feature th at was modeled by taking both the intensity and direction of motion into consideration; thus discriminating between high intensity motion in upward direction versus high intensity motion in the lateral direction.

R eo ro d u ced with p erm ission of th e coovriaht ow ner. Further reoroduction prohibited without p erm ission.

53

The motion feature map provides a unique representation of only 96 dimension for both intensity of motion and direction of motion within a video shot. This compact representation can be used as input to a classifier to test the efficacy of motion descriptors in discriminating between American football plays.

5.2.2

M P E G -7 A u d io D escrip to rs F eatu re M a p p in g

The motivation behind using audio descriptors is th a t most sports have a certain vo cabulary associated with each event. Almost all the announcers will utilize some of the vocabulary to describe similar events. Therefore we wanted a compact representation of audio characteristics to describe the general tone and pitch of the announcer. The purpose was not to recognize all the spoken words, but only to analyze th e similarity in the spoken sound between similar events. As mentioned in Section 3.4.2.1, we used three MPEG-7 audio descriptors namely, Audio Spectrum Envelope, Audio Spectrum Centroid and Audio Spectrum Flatness. Figure 5.7 shows the input audio signal and the output of the three descriptors. Details on the extraction method of the audio descriptors is given below;

A u d io S p e c tru m E nv elo p e (A SE ): This descriptor represents the power spectrum of an audio signal. It is computed by calculating the Fourier transform of the audio signal which is windowed using a Hamming window with an overlap of 50% between adjacent audio frames or windows. The size of the Hamming window is taken to be 10ms, This descriptor is calculated using the following equations as given in [31]: Pt t 3{l, k) = ^n~os{n 4- lM )w{n) e x p { -j{ -- )nk) , (5.3)

where N is the size of the short time fourier transform S{1, fc), fc is the frequency bin index, I is the time audio frame index, w{n) is the analysis window function of length Iw and M is the hop size. The short time fourier transform S {l,k) needs to be normalized by a factor of N in order to preserve Pai'seval's Theorem

R eo ro d u ced with p erm ission of th e coovriaht ow ner. Further reoroduction prohibited without perm ission.

54

C« A
Audp Frames, d^rps each ! 'J

--------r ' .  *1------ 1
!

"T -------1 ------- 1 ------- 1 "

oI----- L_

1
,'!
____________ I____________ t _ J____________ 1

y

Ayhio'Framed, 1Qms ear^
rr fj v > V -V I )

,Audio Frames, 1Qrps each

F ig u r e 5 .7 : (a) Original audio signal; (Id) A udio Spectrum Envelope descriptor o u tp u t 1 /4 o cta v e resolution; (c) A udio S p ectru m Centroid descriptor output; (d) A udio Spectrum F latn ess descriptor ou tp u t

and since A S E represents only the power spectrum , therefore we can estim ate the A S E descriptor as follows: A S E (!, A :)
1
a N

(5.4)

where a is the window normalization factor. T he number of frequency bins can be varied based on the octave resolution required. One bin is reserved for power

R eo ro d u ced with o erm ission of th e coovriaht ow ner. Further reoroduction orohibited w ithout oerm issio n .

55

between 0 Hz and 62.5 Hz, while another one is reserved for power between 8 kHz and Nyquist rate. W ith 1/8 of octave resolution th e frequencies in the middle are divided into 8 bins, thus providing a spectrum envelope consistmg of 10 bins. Figure 5.7(b) shows Audio Spectrum Envelope description w ith 1/4 of octave resolution. A u d io S p e c tru m C e n tro id (A BC): This descriptor represents the center of gravity of the power spectrum. T hat is, it shows the dominant frequencies in the power spectrum. This is calculated by adding the energy in each frequency bin by the total energy in the frame as given by th e following equation;

Efro^fc-ASE(Z,fc) ' (5-5) Sg;^'ASE(Z,&) where k is the frequency bias index. The ABC for each frame was then normal
A SC (f) = ized between the values of 0 and 1, after which they were quantized into 10 bins in order to provide compact representation of the MPEG-7 descriptor. Figure 5.7(c) shows the ABC description. The figure shows th a t the audio signal has mainly low frequencies as the centroid is mainly below 1 kHz. · A u d io B p e c tru m F la tn e s s (A BF): This descriptor represents th e overall tonal component in the power spectrum of the audio signal. It is calculated by calculating the geometric mean of the audio frame and dividing it by the arithmetic mean of the audio frame as shown by the equation

where k is the frequency bins index and N is the size of the short tim e fourier transform window. Figure 5.7(d) shows the Audio Spectrum Flatness of an audio signal. The spectrum is then normahzed and quantized into 10 bins.

5 .2 .3

M F C C F eatu re M ap p in g

Mel Frequency Cepstrum Coefiicients (MFCC) have been widely used within the speech recognition community as a basic spectral feature set th a t provides robust classification of sound.

R eo ro d u ced with oerm issio n of th e coovriah t ow ner. Further reoroduction orohibited without o erm issio n .

56

Due to the fact th a t most of the video shots contam a lot of crowd noise, and we want to extract the perceived rhythm and sound of the spoken content, we needed a feature th a t can model the human hearing and also works well under noisy conditions. Mel Frequency cepstrum has been used extensively in the speech recognition systems as it tries to emphasize the frequencies th a t are more easily perceived by the human ear. The Mel scale first defined in [27] and revised in [28], was developed to model the pitch of a sound. Pitch is a non linear combination of both frequency and intensity. The Mel scale tried to put in perspective the relationship between pitch of the sound and its intensity. Therefore the pitch of 1000 Mels was half the intensity of the pitch at 2000 Mels. T hat is th e Mel scale measures pitch in an absolute scale. The following equation shows the relationship between frequency and pitch. 4491 7 ^ 1 + exp(7.1702 - 1.98241og(/)) " where / denotes frequency in Hertz and v denotes pitch in mels. Figure 5.8 shows the Mel scale from which the above equation was derived by curve fitting.

The MFCC features are extracted by first de-multiplexing the audio stream from the MPEG-1 video. This audio stream is then input into the a MFCC feature ex traction system, which first performs pre-processing on the raw audio data. After the pre-processing step the MFC Coefiicients are extracted and finally the coefficient m atrix is passed through a feature reduction or compaction step. This sub system is shown in Figure 5.9.

R eo ro d u ced with oerm issio n of th e coovriah t ow n er. Further reoroduction orohibited without o er m issio n .

57

B O d O

4000

2000

lilllil. JJiiU
F ré q u o rK ÿ in H z

F igure 5.8: The MEL Scale

Audio
File-

Feature matrix Ftrerprocessing:

Figure 5.9: MFCC feature extraction sub system M FC C Feature M apping - A udio preprocessing The objective of this stage is to remove any silent segments from the raw audio data. This is done in order to model the MFCC features for only the voiced segments of the audio signal. Following steps were taken to remove unvoiced segments from the audio data: · Window audio signal in 25ms frames with 25% overlap between adjacent frames

R sn m d u c e d with o erm issio n of th e coovriah t ow n er. Further reoroduction nrnhihited w ithout nerm ls.sion

58

· Compute the mean of each window · Remove frames where mean is less than a threshold

M FC C Feature M apping - C oefficient calcu lation In this step of MFCC feature modeling the Mel frequency coefhcients are extracted using the algorithm proposed in [32]. Following are the step followed to extract the coefficients for a N sample audio signal given by s = 5q, . . . , s#_i: · P re -E m p h a s is : This is a high pass filtering operation in order to compensate for the spectral tilt. In time domain this is performed by subtracting the original signal at a particular tim e instant from the signal in the previous time instant which is scaled by a constant. This constant is usually taken to be between 0.9 and 1. This step in tim e domain is given by the following equation: Si = Si -- a S i-i fo r 0.9 < a < 1 , (5.8)

where the constant a was taken to be 0.95. · B lock in g and W indow ing: In this step the input signal is divided into frames of equal length. Each frame is made to overlap the previous audio frame. The overlap portion can vary between the ranges of 20% to 50%. The selection of the frame length is dependent on the specific use, but in most speech recognition applications the frame size is 10-40ms long. The individual frames are then windowed using a windowing function in order to reduce the spectral artifacts and also to smoothen th e discontinuities in the signal edges. Usually a Hamming or Hanning type window function is used to perform this step. In this work we used an overlap window of 50% and Hamming window function given by the following equation: V ij^ y ijW j f o r j = 0 , . . . , W -- l, a n d i = 0 , .. . ,M -- I ] (5.9)

R onrnH i

\A /ith n o r m i c c i m n r \f t h o

r\V A /nor

P i ir + h a r

it n i a r m l c c î / i n

59

Here y isth e frame window, w is the Hamming window function as given by th'' equation below. M is the to tal number of frames in th e audio signal and W is th e size of th e window: Wj = 0.54 --0.46 c o y ys --( 1 ) J fo r j = 0 , . . . , W -- 1 ; (5.10)

· F re q u e n c y d o m a in tra n s fo rm a tio n ; In this step each frame is transformed in the frequency domain using th e Fourier Transform. The transform ation into th e frequency domain results in a signal w ith both real and complex parts. If only the power spectrum is to be utilized then th e magnitude square of the Fourier coefficients are used. Therefore the output at this stage is th e power spectrum coefficients based on th e length of th e transform for each frame. This is given by th e following equation: ^i = \ff't{yi)\^ fo r i = 0 ,...,M - l ', (5.11)

o M e l F ilte r B an k ; The Mel filter bank is designed to capture lower frequencies and emphasize the information in the speech signal a t these frequencies. Most of the im portant and useful information in a speech signal is present in the lower end of th e spectrum. The filter banks are constructed of triangular shaped filters and are made to overlap such th a t th e lower frequency of the filter corresponds to the center frequency of the previous filter and th e ffigh frequency of the filter corresponds to the center frequency of the next filter. The filters below 1 kHz are spaced linearly and th e filters above 1 kHz are spaced by increasing the distance 1.1 times after each filter. Usually the range of frequency covered by th e filter bank lies between 20 Hz till half th e sampling frequency of the signal. Figure 5.10 shows the arrangement of the Mel scale filter banks. W hen the power spectrum of the frequency coefficients is passed through the filter bank, th e output is the inner product of the filter w ith th e power spectrum coefficients. This provides the energy coefficients of each filter for every audio frame.

R ep ro d u ced with p erm issio n of th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

60

-s 0:8

isqo

aoao

zeoo

F n a q u e n ;^ in Hx--

F igu re 5.10: Mal filter bank

» L og o f e n e rg y coefficients: By passing the signal through the Mel scale filter bank the coefficients are modeled according to human auditory system of pitch perception. In a very crude fashion the intensity-loudness relationship of th e humane auditory system can be modeled by taking the log of the filter coefficients. This is done as shown in the equation below: 2 u -i (5.12)
3 k=0

where t = 0,..., M -- 1; and j = 0, ...,K -- 1. U is the number of audio frames in th e signal. A j is the normalized energy of each frame and is given by the following equation: u -i (5.13) t=0 · In v e rse fre q u e n c y d o m a in tra n s fo rm a tio n : This step is performed to re duce the dimension of the coefficients and also to de-correlate the coefficients. Since only the power spectrum was used as input to th e filter banks, the inverse frequency transformation only has real part. Because of the cosine transforma tion property most of the signal energy is compacted in the first few coefficients.

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited w ithout p erm ission .

61

T hus only a first few coefficients are selected for the feature vector of each frame. Usually in speech recognition system s 9 to 13 coefficients are used. In this work we took the first 13 coefficients, the first coefficient was discarded as it contains the energy information of each frame.

M FC C F eatu re M apping - Coefficient com paction
Since video shots for different plays varied in size, dimensions of the coefficient m atrix were not uniform. Longer video shots fiad more audio frames compared to shorter video .shots. Therefore to make the m atrix dimension uniform we had to apply a m atrix com paction strategy. Also there was redundancy between the coefficients of the adjacent frames, as seen in Figure 5.11. In the figure blue colour represents low values and the red colours represent high values.

30

40

# of audio Frames

F ig u re 5.11: MFCC feature redundancy

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

62

In order to make the m atrix uniform and reduce redundancy, we decided to fix the number of audio frames. Euclidean distance between adjacent audio frames was calculated and a threshold was calculated empirically th at would compact the coef ficient m atrix size into 64 segments. An experiment was conducted to find out the number of segments th a t would provide optimal results. In the experiment, segment sizes of 32,64,128 and 256 were used and the classification results showed th a t taking 64 segments provided best results. Thus this provided us with 12 x 64 feature matrix or a 768 dimension feature vector.

5.3
5.3.1

A m e r ic a n F o o tb a ll R V S E v e n t C la ssific a tio n
L inear D iscrim in a n t A n a ly sis

As stated in Section 3.4.3, to perform classification of the feature set we utilized LDA, specifically Fisher's LDA [33]. In general for a multi-class problem, Fisher's LDA tries to choose those vectors % of the feature matrix A th a t will maximize the equation:

SS'
subject to the orthogonality constraint within class spread and S b is between-class spread. That is to say, th a t the with-in class covariance m atrix in the transformed space is an identity matrix. The first- vector à i is the Fisher's linear discriminant and the second vector is orthogonal to a i and so on. In the equation S v k is the

The characteristics of Fisher's LDA'can be stated as follows [33]: · For O number of classes, a transformation is done to a space of 0-1 dimension. · The transformation is computed with no prior assumptions about the distribu tion of the d ata set. · Discrimination can be conducted by utihzing reduced dimension set of the fea ture set

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

63

· For complex non linear classifiers, LDA can be used as a post processing step. In order to calculate the transformation matrix which can be used to calculate the discriminant function of different classes, the following steps are used: « First the mean of the features for each class is calculated and for the class

is represented by Hi- Then the mean of the overall d ata set is calculated and is represented by m. · Then Ê the covariance matrix of each class is calculated using the following equation: t = {Xi - Hi){xi , (5.15)

· The between class scatter m atrix Sg is calculated using the following equation: Sg ; (5.16)

· Then th e with-in class scatter m atrix is calculated using: Sw = " Si ; i=i TM (5.17)

· The solution th a t maximized the with-in class scatter and the between class is found by calculating the eigenvectors as given by:

SgA = S w A A ,

(5.18)

where A is th e m atrix whose columns are transformed vectors ai and A is the diagonal m atrix of eigenvalues. · The eigenvectors corresponding to the highest eigenvalue are used for feature extraction. The discriminant functions are devised by multiplying the new feature vector with th e original vectors and finding th e constant th a t will maximize the separability between classes, as given by th e following equation. gi = a f x + ao, where oq is th e constant and is the discriminant function of the ith class. (5.19)

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

64

5.3.2

M P E G -7 M o tio n d escrip tor based cla ssifica tio n

In order to evaluate the efficacy of using MPEG-7 motion descriptors for indexing of American football plays, the 98 dimensional motion feature vector was used to classify 4 events from the football game. The classification was done on a database of 200 video shots taken from 4 different games, as detailed in Section 3.5. Table 5.1 shows th e classification results obtained by using MPEG-7 motion de scriptor feature model. This feature model consisted of a 96 dimensional feature map of motion magnitude and direction as well as 2 dimensional feature of mean and standard deviation of frames with the highest motion activity.

Play Category Pass Plays Run Plays F G /E x tra Point Plays Kickoff/Punt Plays

Classification accuracy 79.5% 92.5% 875% 65.5%

Table 5.1: Classification Summary Table using MPEG-7 motion descriptor features

The overall classification accuracy of the system is 82.5%. As seen from Table 5.1 we get best classification results for running plays and the worst results for kickoff/punt plays. The kickoff/punt play category is classified only 65.5% correctly with most of th e missed classifications falling in the pass play category. Table 5.2 shows the confusion matrix of the four category classification.

Actual category Pass Run F G /E xtra point Kickoff/Punt

Pass 70 5 0 10

Classified category Run FG /X P K /P 5 13 0 62 0 0 1 1 14 0 19 0

Table 5.2: Confusion matrix between categories using MPEG-7 motion descriptor features for classification

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

65

Most of the confusion is between passing play category and kickoff/punt category. This can be due to the fact th a t the kickoff/punt category has the ball catching and running actions as well as the kicking action.

5.3 .3

M P E G -7 A u d io d escrip tor b a sed c la ssifica tio n

The MPEG-7 audio descriptor feature vector was extracted to complement the motion descriptor features. Before we evaluate the fusion of motion and audio descriptors, we first evaluate the pro and cons of using audio descriptors only for event classification. Table 5.3 shows the classification results obtained by using MPEG-7 audio de scriptor feature model on a database of 200 video shots as detailed in Section 3.5. This feature model consisted of a 10 dimension feature vector of audio spectrum en velope descriptor, a 10 dimension feature vector of audio spectrum centroid and a 10 dimension feature vector of audio spectrum flatness.

Play Category Pass Plays Run Plays F G /E x tra Point Plays Kickoff/Punt Plays

Classification accuracy 65.9% 32.8% 0.0% 55.2%

Table 5.3: Classification Summary Table using MPEG-7 audio descriptor features

We can see th a t the classification rate is very low compared to the motion descrip tor classification. The overall classification accuracy of 48.0% was achieved using only MPEG-7 audio descriptor feature vector. This feature vector tried to classify play events into four categories based on general spectral characteristics of the audio sig nal. It can be seen from Table 5.3 th at the category of Field goal and Extra points had all th e shots miss classified. This could be attributed to fact th a t during these plays the commentators only comment if the attem pt to score was good or not. In some cases the commentators are talking about the previous plays, till the kick was made and by the tim e they mention the outcome of thé play, the video is cut into another shot. Table 5.4 shows the confusion m atrix of play classification.

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

66

Actual category Pass Run F G /E x tra point 1 Kickoff/Punt

Classified category Pass Run FG /X P K /P 58 22 8 0 40 22 0 5 10 6 0 0 13 0 16 0

Table 5.4: Confusion matrix between categories using MPEG-7 audio descriptor features for classification

It can be seen from the confusion matrix in Table 5,4 th a t the audio descriptor features do not contain much discriminating power to categorize the play events, as most of the plays are classified into the first category of pass plays. But the last category of kickoff/punt plays achieved much better results than any other category. Therefore this feature can potentially be combined with motion descriptor features to improve the classification accuracy of the kick/punt category.

5 .3 .4

M F C C fea tu re b ased classification

The MPEG-7 audio descriptor feature was purely a spectral feature, representing the various spectral characteristics of the audio sign^. Thus we require another audio feature th a t has been proven to be robust in the speech recognition and general sound recognition classification problems. MFCC have been established to provide a feature set that can be used for general sound recognition. Here we first evaluate the efficacy of using the MFCC features only before com bining them with the MPEG-7 motion and audio descriptor features. Table 5.5 shows the classification results of using 12 x 64 feature m atrix of MFCC features.

The overall classification accuracy is 57.5%. But using these features helps dis criminate between the pass plays and run plays much better than only using MPEG-7 audio descriptors. These features are not very good in classifying th e kickoff/punt cat egory. Therefore using these features in combination with MPEG-7 audio descriptors will help in achieving better classification accuracy.

R ep rod uced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

67

5 .3 .5

M u lti M o d a l fea tu re b ased c la ssifica tio n

Here we compare the efficacy of combining the MPEG-7 motion descriptor features with MPEG-7 audio descriptor features and also th e MFCC feature set. Table 5.6 summarizes the results of classification by combining the multi-modalities.

From Table 5.6 we can see the increase in classification accuracy by combining multi-modal features. In the case of combining th e MPEG-7 audio w ith MFCC

features we see an overall increase of 10%, while combining the audio features with motion descriptor features shows an increase of 5%. Combining all three features produce an overall classification result of 92.5% All the results th a t were presented in this work are based on using Fisher's LDA classification technique. The database contained 200 video shots and in order to minimize the bias of th e sample set we implemented leave-one-out classification. W ith this method one sample from the database sample set is removed and used as the test set. The classifier is trained with the rest of the samples. This process is repeated with each sample in th e database. This process ensures th a t classification scheme does not contain bias due to sample set size. [34]. Feature selection was also performed using the W ilk's Lambda criterion in order to optimize the feature space. The dimension of our feature space is large and some of the featiues may not enhance discrimination between classes. Therefore in the feature selection phase the features th a t provide redundancy and deteriorate the performance of the overall classification accuracy are taken out of the equation.

Play Category Pass Plays Run Plays F G /E x tra Point Plays Kickoff/Punt Plays

Classification accuracy 68.2% 6&7% 43.8% 2R7%

Table 5.5: Classification Summary Table using MFCC features

R ep rod uced with perm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

68

5 .4

C o n c lu sio n s

The primary design objective of the video indexing system was to utilize features and classification schemes th a t are fast, effective and simple to implement on hardware. The secondary objective was to evaluate the contribution of different multi-modal features. Also taken into consideration was the fact th a t domain knowledge plays an im portant part in the fine tuning of the system. As mentioned before, the knowledge base model th a t we have proposed contains 3 categories of RVS events, namely run plays, pass plays and kicking plays. The kicking plays category is further sub divided into two categories namely. Field Goal/ Extra point and Kickoff / Punt. The reason for dividing them is the totally different type of motion each play category exhibits. In this Chapter we have proposed and implemented an indexing system th a t uti lizes MPEG-7 motion descriptors features, MPEG-7 audio descriptor features and MFCC features to classify American football plays into 4 categories mentioned above. The system first extract the features and then uses LDA to classify them. In this Chapter we have shown the efiacacy of using MPEC-7 motion descriptor features, MPEC-7 audio descriptor features and MFCC feature sets. We have also shown the classification results when the feature sets are used in a combination. We have established th a t motion features best discriminate between the plays with an accuracy rate of 83%. But using only motion features cannot resolve the classification confusion between pass play category and kickoff/punt category. We also established th a t some MPEC-7 audio descriptor features provide good

Play Category Pass Run F C /X P K /P Overall

MPEC-7 audio MFCC . 70.5% 59.7% 75.0% 69.0% 67.0%

MPEC-7 motion audio 86.2% 91.0% 87.5% 82.8% 87.0%

MPEC-7 motion MFCC 8&2% 92.5% 87.5% 82.8% 87.5%

MPEC-7 motion audio -F MFCC 94.3% 89.6% 93.8% 93.1% 9&5%

Table 5.6: Classification Summary Table using multi-modal features

R ep ro d u ced with p erm ission of the copyright ow ner. Further reproduction prohibited without perm ission.

69

discrimination between Mckoff/punt category and th e other categories. Therefore we established th a t combining MPEG-7 audio features w ith MPEG-7 motion descriptors provides better classification accuracy. Table 5.6 shows th a t using MPEG-7 motion and audio in combination we can improve the classification accuracy from 82.5% to 87.0%. Most of the improvement is in the Idckoff/punt category where th e classifica tion accuracy jumped from 65.5% to 82.8%. We can conclude from our implementation of the system th a t using MPEG-7 motion and audio descriptors in combination with MFCC for classification of RVS events in American football can be very effective. We can also conclude th a t MPEG-7 descriptors can be readily used in sports indexing and retrieval applications.

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

C hapter 6 C onclusions

Chapter 1, a knowledge base for American football was proposed, using the concept of Recurrent Visual Semantic (RVS) at its root. Chapter 2 provided an overview of the MPEC-7 standard and highlighted the descriptors th a t were most relevant to this work. Then in Chapter 3, the proposed system was outlined and an explanation on the motivation behind using the proposed techniques is provided. In Chapter 4, we proposed and implemented an algorithm to detect the play events within the video shots. A comparison of using different parameters in the algorithm is also done in the chapter. In Chapter 5, we implemented the indexing and classification phase of the proposed system. The classification results of using motion descriptors, audio descriptors and MFCC feature sets is also provided in the chapter. The chapter concluded by analyzing the effects of combining features from multiple modalities. In this Chapter, we will summarize the results of the overall system and also provide some recommendation for future enhancement of the system.

I

N this thesis work we have proposed an American football video indexing system utihzing MPEG-7 motion and audio descriptors along with MFCC features. In

6 .1

S u m m a r y o f T h e sis c o n tr ib u tio n

In this work we proposed a system th a t consisted of two main components: First was the localization phase of the system, which dealt with finding the starting point of the play event in the video shots. Second was the indexing and classification phase which was responsible for the extraction of features and classification of the video 70

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm issio n .

71

shots into 4 categories. In the following sections the results of th e two main phases of the system are summarized.

6.1 .1

P la y e v e n t d e te c tio n

In Chapter 4 of this thesis work we proposed an algorithm to localize the play event within a video shot. This was done w ith th e motivation th a t in some sports there are a lot of non-play events th a t occur before the play event occurs for only a short period of time. For example in football the total play tim e is 1 hour, b u t it takes approximately 3 hours to play the entire game. Also in between each play the teams have 40 seconds to setup and start the play. Similarly in sports like golf, bowling, baseball and tennis, the play event is followed by non-play events. The algorithm we proposed took advantage of the fact th a t most of the play events are preceded with low motion intensity-segments. The proposed algorithm utilized the mean and standard deviation of the motion vectors from th e P frames of the MPEG-1 video stream. 83% of the time, the proposed algorithm was able to detect the starting point of the play events in the video shots w ithin one second before or after th e start of the play. Based on our implementation and results obtained we can make the following conclusions: · Localization of play events within a video shot is im portant as it removes nonessential d ata from our feature space and reduces the processing time. · Collaboration of mean and standard deviation of the magnitude of motion vec tors helped in enhancing the detection algorithm, as in some cases using only the mean may not be reliable. · There is a trade off th a t has to be considered when detecting th e starting point, accuracy versus delay in detection. As we saw by increasing the window size from 3 frames to 4 frames we got better accuracy b u t some of the plays were detected after six seconds from the actual starting point.

R ep ro d u ced with p erm ission of th e cop yright ow ner. Further reproduction prohibited w ithout p erm issio n .

72

· We have established th a t MPEG-7 motion descriptors can be utilized to build apphcations th a t can automatically summarize sports events. · T he algorithm th a t we proposed is only viable for sports th a t contain low inten sity motion ju st before th e actual play event takes place. For example, tennis, baseball etc. · The proposed algorithm can be plugged into a larger application system with out much modifications in order to generate highlights of a game or detect interesting plays. ·

6 .1 .2

P la y e v e n ts c la ssific a tio n

In Chapter 5 of this work we proposed a system to extract features based on MPEG7 audio and motion descriptors as well as MFCC. The motivation was to develop an application of indexing and retrieval for American football games using primarily MPEG-7 descriptors, since one of the main objectives of M PEG-7 standardization efforts was to create an interface environment th a t can facilitate th e application de velopment of indexing and retrieval based systems. MPEG-7 motion descriptors were primarily used to classify the events into the 4 RVS categories. MPEG-7 audio descriptors were used to complement and enhance the classification process. MFCC were used due to the fact th a t there were studies [35] th a t showed th a t MFCC performed b etter than MPEG-7 descriptors of Audio Spec trum Basis and Audio Spectrum Projections. These two MPEG-7 audio descriptors are very similar to features obtained through MFCC. We were able to classify th e events into 4 categories w ith an accuracy rate of 92.5% by using all th e three feature sets. Using only MPEG-7 descriptor based features we were able to get classification accuracy of 87.0%. All the classification results were obtained by using Fisher's LDA and implementing a leave-one-out classification criteria in order to minimize the bias of the database which contains 200 video shots from 4 different games taken from 4 different networks. Based on our implementation and results obtained we can make the following

R ep ro d u ced with p erm ission of th e cop vriaht ow n er. Further reproduction orohibited w ithout o e r m issio n .

73

conclusions: » M PEG -7 m otion descriptors are integral in classification of RVS events in Amer ican fo o tb all Using these simple features we were able to get 82.5% classification accuracy. · Com bining m ulti-m odal features in a reasonable fashion can enhance the clas sification. B ut always there are trade-offs th a t need to be considered. Some features may reduce classification of a. particular category but may enhance the overall perform ance of the system . Figure 6.1 shows the variations in classifica tion results from adding: audio features to the m otion features.

50.00%
40.00% 30.00%
2 0 .0 0 % 10 . 0 0 %

Pass ptays

Run piaya

Q MPEG-7 motion
tt.fi^E G -7 motion^audio m.MPEG-7motion * hM=CC 1S/MPËG-7 moSon*audio*MFCC

79.50% 05.20% 05.20% 94.30%

92.50%
91.10%

1

F ig u re 6.1: Multi-modal classification

A lthough there is no baseline to com pare our results with, since there is no stan d ard d atabase of American football. Som ewhat sim ilar works done in in dexing and retrieval of American football events [6] [26] have shown precision accuracy of 81% and 84% respectively. In this work the system classification

R ep rod u ced with p erm issio n of th e cop yright ow n er. Further reproduction prohibited w ithout p erm issio n .

74

accuracy is 82.5% by using MPEG-7 motion features only and increases up to 92.5% when ail the audio visual features are combined. Table 6.1 shows the comparison between the propo.sed work and some of the previous works. System Proposed System Miyauchi et. al. [26] T. Caelli et. al. [6] Nitta et. al. [29] Events Classified Pass,Run, PG/XP, K /P PG,TD 3 types of formations Scrimma.ge,FG/XP, K /P Performance 92.5% 83.7% 80.6% 84.3%

T able 6.1: Performance Comparison of NFL Video Indexing System

· There is still some work that needs to be done in order to reduce the miss classification between kickoff/punt plays and pass plays. This is evident from the scatter plot shown in Figure 6.2. In the plot we can see that there is a lot of overlap between the categories 1 and 4.

4

2

0
CA T
.2 Q Group Cenlroid

u_

I

C

·4

 6 ·4
·2 0

2

A

6

8

10

12

F u n c tio n 1

F ig u re 6.2; Scatter plot of classified data

R ep rod u ced with p erm issio n o f th e copyright ow ner. Further reproduction prohibited w ithout p erm issio n .

75

· In the system we have utilized a supervised classification scheme. The perfor mance of the system needs to be tested by using supervised as well as unsuper vised classification schemes.

6 .2

F u tu r e D ir e c tio n s

In th e thesis work we have proposed a system for indexing of American football video shots utilizing mainly MPEG-7 motion and audio descriptors. The following directions can be taken to further the work undertaken in this thesis: · In order to minimize the miss classifications between pass plays and kickoff/pimt plays, we can utilize th e MPEG-7 texture descriptors. This can help in discrim inating the kickoff/punt plays as in these plays the ball is kicked high which in tu rn makes the camera zoom out to capture the trajectory of th e ball. Thus in a video shot there is not only playing field but also audience in the stands. The frames with audience will have a higher texture compared to a frame w ith only playing field. · Currently we are only working with video shots and not a whole footage of the game. One of the steps th a t can be implemented is parsing of video footage into shots, thus providing a more complete system. · In the future a more complex classification scheme can be utilized which takes into consideration domain knowledge. This knowledge can be based on a state flow diagram of American football events. · In this work we only classified the events at the root of our proposed knowledge base. In the future we can examine how to classify the inner nodes of the knowledge base tree. · Further the work on retrieval by mapping the classification results to th e MPEG7 proposed visual descriptor objective measure of retrieval efficiency given by Average Normalized Modified Retrieval Rate (A N M R R ).

R ep ro d u ced with p erm ission o f th e copyright ow n er. Further reproduction prohibited without p erm ission .

Bibliography
[1] Jurgen Assfalg, Marco Bertini, Carlo Colombo and Alberto Del Bimbo, "Ex tracting semantic information from news and sport video" Proceedings of the International Symposium on Image and Signal Processing and Analysis, 2001, pp. 4-11, [2] A. Jaimes and S.-F. Chang, "Learning visual object filters and agents for on-line media" A D V E N T Project Technical Report, Columbia University, June 1999. [3] N. Babaguchi, Y. Kawai, and T. Kitahashi, "Event Based Indexing of Broad casted Sports Video by Intermodal Collaboration" IEEE Transactions on Mul timedia, vol. 4, pp. 68-75, 2002. [4] Y. Chang, W. Zeng, I. Kamel and R. Alonso, "Integrated Image and Speech Analysis for Content-Based Video Indexing" IEEE International conference on Multimedia computing and systems, pp. 306-313, 1996. [5] Z. Xioing, R. Radhakrishnan, and A. Divakaran, "Generation of Sports High lights Using Motion Activity in Combination with a Common Audio Feature Extraction Framework" IEEE Conference on Image Processing (ICIP), vol. 1, pp. 29-32, September 2003. [6] Terry Caelli, Mihai Lazarescu, Svetha Venkatesh and Geoff West, "On the au tom ated interpretation and indexing of American football," IE E E International conference on Multimedia computing and systems, vol. 1, pp. 802-806, 1999. [7] M PEG Requirements Group, "MPEG-7 Overview," Doc. ISO /M PEG N4317, Sydney MPEG Meeting July 2001. 76

R ep ro d u ced with p erm ission of the copyright ow ner. Further reproduction prohibited without p erm ission .

77

[8] http://www. chiariglione. org/mpeg/standards/mpeg- 7/mpeg- 7. htm. [9] B.S. M anjunath, Phillipe Salembier and Thomas Sikora, Introduction to MPEG7: Multimedia Content Description Interface, John Wiley and Sons, England
2002 .

[10] K. Yoon et al. "MPEG-7 based news browsing: description extraction, browsing and exchange," Multimedia Systems and Application I V , Proc. of SPIE, vol. 4518 Denver 2001. [11] W.W. Cohen and W. Fan, "Web-collaborative filtering: recommending music by crawling the web," Proc. Ninth International World Wide Web Conference, pp. 685-698, Amsterdam 2000. [12] A. Ghias, J. Logan, D. Chamberlin and B.C. Smith "Query by Humming: mu sical information retrieval in an audio database," Proc. of Third A C M Interna tional Conference on Multimedia, pp. 231-236, San Francisco 1995. [13] E. Kasutani and A. Yamada, "The MPEC-7 color layout descriptor. A compact image feature description for high speed image/video segment retrieval," Proc. of IE E E Intem ation Conference Image Processing ICIP2001, vol. 1, pp. 674-677, 2001. [14] Jurgen Assfalg, Marco Bertini, Carlo Colombo and Alberto Del Bimbo, "Seman tic annotation of sports video" IE E E Journal on Multimedia, vol. 9, Issue 2, pp. 52-60, A pril-June 2002. [15] Cees C.M. Snoek and Marcel Worring, "A Review on multimodal video index ing," Procedings o f IEEE International conference on Multimedia and Expo 2002, IC M E '2002, vol. 2, pp. 21-24, 2002. [16] Arun Hampapur, "Semantic Video Indexing: Approach and Issues," ACM SICMOD Record, vol. 28, no. 1, pp. 32-39, May 1999.

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

78

[17] J. Kittler, K. Messer, W .J. Christinas, B. Levienaise-Obadia and D. Koubaroulis, "Generation of semantic cues for sports video annotation," Proceedings of IEEE International conference on Image Processing, vol. 3, pp. 26-29, 2001. [18] Wensheng Zhou, Asha Vellaikal and C.C. Jay Kuo, "Rule based video classi fication system for basketball video indexing,"

http://w w w. acm. org/sigs/sigm m /M M 2000/ep/zhou/. [19] Hisashi Miyamori and Shun-ichi lisaku. "Video annotation for conteni based retrieval using human behaviour analysis and domain knowledge," IEEE 4th International conference on Automatic face and gesture recognition pp. 320-325
2000 .

[20] Hong Lu and Yap-Peng Tan, "Sports video analysis and structuring," IEEE fth Workshop on Multimedia signal processing, pp. 45-50. 2001. [21] Milan Petkovic, Vojlcan Mihajlovic, Willem Jonker, S. Djordjevic-Kajan, "MultiModal extraction of highlights from TV Formula 1 programs," US Patent no. 3069654 , 2002. [22] Chuan Wu, Yu-Fei Ma, Hong-Jiang Zhang and Yu-Zhuo Zhong, "Events recog nition by semantic inference for sports video," Procedings of IEEE International conference on Multimedia and Expo, vol. 1, pp. 805-808, 2002. [23] J. Assfalg, M. Bertini, A. Del Bimbo, W. Nunziati and P. Pala. "Detection and recognition of football highlights using HMM," IEEE 9th International confer ence on Electronics, Circuits and Systems, vol. 3, pp. 1059-1062, 2002. [24] Mei Han, Peng Chang and Yihong Gong, "Extract highhghts from baseball game video with hidden Markov models," IEEE International conference on Image processing, pp. 609-612, 2002. [25] Mei Han, Wei Hua, Wei Xu and Yihong Gong, "An Integrated baseball digest system using maximum entropy method," Proc. AC M conference on Multimedia, pp. 347-350, December 2002.

R ep ro d u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without p erm ission .

79

[26] Shingo Miyauchi, Akira Hirano, Noboru Babguchi and Tadahiro Kitahashi, "Col laborative multimedia analysis for detecting semantical events from broadcasted sports video," Proceedings of IEEE 16th International conference on Pattern recognition, vol. 2, pp. 1009-1012 2002. [27] 8.8. 8tevens, J. Volkmann and E.B. Newman, "A scale for the measurement of the psychological magnitude pitch," Journal o f Acoustical Society of America, vol. 8, pp. 185-190, Jan. 1937. [28] 8.8. 8tevens and J. Volianann, "The relation of pitch to frequency: A revised scale," American Journal of Psychology, vol., 53, pp. 329-353, July 1940. [29] N. N itta, N. Babaguchi and T. KLtaliashi, "Extracting actors, actions and events from sports video - A fundamental approach to story tracking," Proc. IEEE Intl. Conf. on Pattern Recognition, vol. 4, pp. 718-721, 2000. [30] Baoxin Li and M. Ibrahim Sezan, , 2001, CBAIVL 2001, pg(s): 132-138. R. 8uleesathira and L.F. Chaparro, "Event detection and summarization in sports video," IEEE Workshop on Content-Based access of image and video libraries, CBAVIL 2001 , pp. 132-138, 2001. [31] Hyoung-Gook Kim, Nicolas Moreau and Thomas Sikorau, "Audio Classification Based on MPEG-7 Spectral Basis Representations," IEEE Trans, on Circuits and Systems for Video Technology, vol. 14, no. 5, May 2004. [32] H. Gombrinck and E. Botha, "On the Mel-scaled Cepstrum,"

http://citeseer. nj. nec, com /524151.html, 1996. [33] Andrew R. Webb, Statistical Pattern Recognition, 2nd edition, Wiley Publishing USA, 2002. [34] Keinosulce Pukunaga, Introduction to Statistical Pattern Recognition, 2nd edi tion, Academic Press, 1990.

R ep rod u ced with p erm ission of th e copyright ow ner. Further reproduction prohibited without perm ission.

80

[35] Z. Xiong, R. Radhakrishnan, A. Divakaran, T.S. Huang, "Conipaiing MFCC and MPEG-7 Audio Features for Feature Extraction, Maximum Likelihood HMM and Entropie Prior HMM for Sports Audio Classification," IEEE International Conference on Multimedia and Expo (ICME), vol. 3, pp. 397-400, July 2003

R ep rod uced with p erm ission of the copyright ow ner. Further reproduction prohibited witnout p erm ission.

