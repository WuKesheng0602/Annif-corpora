SOURCE-CHANNEL CODEC FOR A WCDMA BASED MULTIMEDIA SYSTEM
by

Boris Backovic, B.Eng.
Toronto, September 2005

A Project Report presented to Ryerson University

in partial fulfillment o f the requirements for the degree Master o f Engineering in the Program o f Electrical Engineering

Toronto, Ontario, Canada, 2005 © Boris Backovic 2005
P R O P rtrT Y Q F R Y E R 3Q W U B R A FY

U M I Num ber: E C 5 3 0 0 5

All rights reserved INFO RM ATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

UMI
UMI Microform EC53005 Copyright 2008 by ProQuest LLC All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code.

ProQuest LLC 789 East Eisenhower Parkway P.O. Box 1346 Ann Arbor, Ml 48106-1346

The Journey of Success

W hen choosing the path to follow , I selected the road heading w est. It began in th e Forest o f C hildhood, and ceased at the C ity o f Success. M y bag w as packed full o f know ledge, but also som e fears and som e w eights. M y m ost precious cargo was a vision o f entering the C ity 's bright gates. I reached an im passable river, and feared that my dream had been lost. B u t I found a sharp rock, cut dow n a tree, and created a bridge, w hich I crossed. It started to rain, and I w as so cold, I shivered and started to doubt. B ut I m ade an um brella out o f som e leaves and kept all the cold w ater out. T he jo u rn e y to o k longer that I had planned; I had no food left in my dish. R ather than starve before reaching my dream , I taught m y self how to fish. I grew aw fully tired as I w alked on and on, and I thought o f the w eights in my pack. I tossed them aside, and I sped up again. Fear w as all that w as holding m e back. I could see the C ity o f Success, ju st beyond a sm all grove o f trees. A t last, I thought, 1 have reached my goal! The w hole w orld will envy me! I arrived at the city, but the gate w as locked. The m an at the d oor frow ned and hissed, " Y ou have w asted your tim e. I can 't let you in. Y our nam e is not on my list." I cried and I scream ed and I kicked and 1 shook; 1 felt that my life had ju st ceased. F o r the first tim e ever, I turned my head, and for once in m y life faced the east. I saw all the things 1 had done on my way, all the obstacles I 'd overcom e. I co u ld n 't en ter the City, but that d id n 't m ean I h a d n 't won. I had taught m y se lf how to ford rivers, and how to stay dry in the rain. I had learned how to keep my heart open, even if som etim es it lets in som e pain. I learned, facing backw ards, that life m eant m ore than ju st survival. M y success w as in m y jou rn ey , not in m y arrival.

Nancy Hammel

Abstract
T he project deals w ith the operation o f a Source-C hannel C odec for a W C D M A B ased M u ltim edia System . The system is m eant to transfer and receive both digitized speech and still im age signals. It uses a part o f the W C D M A technology to m ix up the tran sm itted signals throughout the im plem entation o f D irect Sequence Spread Spectrum and C hip S equencing m ethodologies. The W alsh code algorithm is used to ensure the o rth o g o n ality am ong different Chip Sequences. O n the tran sm itter side the system first offers the form atting stage w here both a speech and a still im age signal are digitized. The follow ing stage in the system exhibits a significan t degree o f data com pression applying appropriate com pression algorithm s: L em pel-Z iv-W elch for the speech signal and H uffm an C ode A lgorithm for the still im age. T hese com pression algorithm s are im plem ented in the Source E ncoder stage o f the system . T he system also provides basic EEC (Forw ard Error C orrection) capabilities, using both L inear B lock C ode and C onvolutional C ode algorithm s introduced in the C hannel E n co d er stage. The goal o f these EEC algorithm s is to detect and correct errors d u rin g the tran sm issio n o f d ata due to the channel im perfections. A t the W C D M A stage the tw o signals are added together form ing an aggregated signal that is being transm itted th ro u g h the channel. O n th e receiver side a digital d em odulator separates the aggregated signal to into two signals using the feature o f the orthogonality o f vectors. Then the Channel D ecoder stage follow s, w here both signals, w hich have gotten corrupted during the transm ission through the channel due to channel im perfections, are recovered. The im perfections in the channel are sim ulated by random noise that is added to the aggregated signal in the W CD M A stage o f the system . T he last stage in the system , the Source D ecoder stage, deals w ith the conversion o f the received signals from the digital to analog form and reconstruction o f th e signals in the sense that they can again be heard (speech) and seen (still image). E ach stage in the system is sim ulated using M A T L A B program m ing language. The rep o rt is form ed o f three m ajor parts; the theoretical part w here the theory behind each stage in the system is explained, the exam ple part w here applicable num erical exam ples are provided and analyzed for better understanding o f both the theory and the M atlab code, and th e result part w here the M atlab results for each stage are analyzed.

II

Table of Contents

Abstract Chapter 1: INTRODUCTION Chapter 2: FORMATTING
2.1 S peech F orm atting 2.1.1 Sam pling 2 .1 .2 Q uantizing 2.1.3 P ulse C ode M odulation (PC M ) 2 .1 .4 M atlab Im plem entation o f Form atting 2.2 Im age F orm atting

I 1 7
7 g 9 13 14 19

Chapter 3 : SOURCE CODEC
3.1 H uffm an C oding A lgorithm 3.1.1 H uffm an E ncoding 3 .1 .2 H uffm an D ecoding 3.1.3 M atlab Im plem entation o f H uffm an C oding A lgorithm 3.2 L em pel-Z iv-W elch A lgorithm 3.2.1 M atlab Im plem entation o f L em pel-Z iv-W elch A lgorithm .

23
23 25 28 30 32 37

Chapter 4 : CHANNEL CODEC
4.1 C hannel C odes 4.1.1 P arity C heck C odes 4.1.2 L inear B lock C odes 4.1.3 L inear B lock C ode E ncoding 4 .1 .4 L inear B lock C ode D ecoding 4.1.5 M atlab Im plem entation o f L inear B lock C odes 4.2 C onvolutional C odes 4.2.1 C onvolutional C ode E ncoding 4.2.1.1 Im pulse R esponse o f the C onvolutional E n co d er 4.2.1.2 Polynom ial R epresentation o f C onvolutional E ncoding

41
41 41 43 45 46 53 54 56 57 58

Ill
4 .2.1.3 State D iagram 4 .2 .1 .4 T rellis D iagram 4.2.2 C onvolutional C ode D ecoding 4.2.2.1 V iterbi D ecoding A lgorithm 4.2.3 M atlab Im plem entation o f the (2,1,4) C onvolutional C ode 59 60 61 62 66

C hapters: WCDMA
5.1 W C D M A T ech n o lo g y 5.1.1 D irec t Sequence Spread S pectrum 5.1.2 C ode D ivision M ultiple A ccess 5.1.2.1 W alsh O rthogonality 5.2 M atlab Im plem entation o f W C D M A

67
67 68 70 73 77

Chapter 6: RESULTS Conclusion References Appendix A: Matlab Simulation Files Appendix B: Matlab Example Files

80 92 94 96 135

IV

List of Figures

Figure 1 : S ource-C hannel C odec fo r a W C D M A B ased M ultim edia System . Figure 2: F requency Spectrum o f the Signal `sp eech .w av '. Figure 3: Signal `sp eech .w av ' in Tim e D om ain. Figure 4; G rey L evel Im age o f a D iagonal Black Line. Figure 5; PD F function o f the B lack D iagonal Line. Figure 6; A H uffm an Tree. Figure 7; Single P arity C heck C ode. Figure 8: R ectangular Parity C ode (I = 4, J = 6). Figure 9: D ecoding Table for L inear Block Codes. Figure 10; (2,1,4) C onvolutional Encoder. Figure 11 : T rellis D iagram R epresentation o f (2,1,4) C onvolutional E ncoder. Figure 12: The D ecoder Trellis D iagram (w ith H am m ing D istances). Figure 13: The D ecoder Trellis D iagram after t=5 Tim e U nits. Figure 14: D irect Sequence Spread Spectrum . Figure 15: R ight Shifted G enerator Polynom ial. Figure 16: Speech D ata. Figure 17: Still Im age D ata. Figure 18: F requency Spectrum o f Speech Signal. F igure 19: PD F o f Q uantization E rror. F igure 20: F requency o f U sage o f Q uantizer L evels. Figure 21 : PD F o f the Im age. Figure 22: B E R vs. SN R for (12,8) Linear B lock C ode. Figure 23: R eceived Speech D ata. F igure 24: R eceived Im age.

3 15 16 20 22 26 42 43 48 55 60 64 65 68 69 80 80 81 81 83 84 88 91 91

List of Tables

T able 1: G e n e rato r Polynom ials for C onvolutional C odes. T ab le 2: Im pulse R esponse o f the (2,1,4) E ncoder. T ab le 3: S tate D iagram o f the (2,1,4) C onvolutional E ncoder. T able 4: H u m m in g D istance U sed fo r D ecoding C onvolutional C odes.

56 57 59 61

Chapter 1: INTRODUCTION
E ven though now adays personal com puters are the dom inant Internet access client, m obile phones and handheld devices will very soon becom e the m ajor source o f In tern et connections. U nlike the first-generation (IG ) o f mobile com m unication system s designed m ostly to carry the voice application traffic, the third-generation ( 3 0 ) o f co m m u n icatio n system s prom ises unparalleled access in w ays that have never been possible before. Internet access, voice com m unication over Internet protocol, and tran sm issio n o f still and m oving im ages are ju st a few o f the com m unication techniques used in the "a lw ay s-o n " type o f access that 3 0 developers have been w orking on. These d ev elo p ers envision the picture o f an ordinary user receiving live m usic, conducting interactive w eb sessions, and having sim ultaneous voice and data access w ith m ultiple parties at the sam e tim e. U ndoubtedly, this kind o f access requires a special technology so th at com puters, handheld devices, o r any other appropriately geared com m unication device, m ay all be connected anytim e, anyw here. W ideband code division m ultiple access (W C D M A ) technology is one o f the main technologies for the im plem entation o f the th ird -g en eratio n o f com m unication system s that allow s very high-speed m ultim edia services to be perform ed. W C D M A will support high rate high quality data, m ultim edia, stream in g audio, stream ing video, and broadcast type services am ong users. In addition, W C D M A designers contem plate that broadcasting, m obile com m erce, gam es, interactive videos, and even virtual private netw orking will be possible in a near future, all from sm all p o rtab le devices. T hese new and exiting possibilities o f connecting people, businesses, and even industries all o v e r the w orld by using rapidly developing technologies and standards m ake im possible to im agine w hat m odern living w ould be like w ithout access to reliable, e co nom ical, and efficient m eans o f com m unication. C onsequently, an ordinary person m ay g e t easily confused and scared aw ay w ith concepts o f all these com m unication tech n o lo g ies and their im plem entations. H ow ever, the sole purpose o f com m unications has n o t changed m uch since the tim e o f G uglielm o M arconi w ho, in 1897, had d em o n strated ra d io 's ability to provide continuous contact with ships sailing the English channel. Even today, a com m unication is still a w ay o f conveying or transm itting inform ation from one place to another. The definition sounds sim ple and does not tell us m u ch a b o u t w hat is involved in inform ation transm ission as a m ean o f com m unication b etw een tw o o r m ore parties. The answ er w ould probably turn o ut to be m ore c o m p licated then it m ight first appear and will, in fact, form a basis for this project.

A sim ple answ er w ould be that the transm ission o f inform ation requires som e kind o f signals in o rd er to convey a m essage to the other party. The signal could be a voltage, but once th e voltage is established there is not m uch availability to convey inform ation unless w e change the value o f the voltage. The next step w ould be to attach the voltage (a battery) to a variable resistor creating m ore variations o f possible voltages that in addition allow s us to associate m ore parts o f inform ation w ith different voltage levels. If a signal varies w ith tim e (120V , 6GHz sinusoidal voltage) then variations o f the signal may be created by either changing the am plitude o r the phase o f the sinusoidal voltage. A t this point w e will m ake a rough aberration betw een tw o types o f signals that could be used in a process o f inform ation transm ission. If a signal is a continuous electrical signal th a t varies w ith tim e, it is considered to be an analog signal. On the other hand, if a signal is n on-continuous, it is said to be a digital signal. Furtherm ore, analog signals can take on any value from an infinite set o f values in a specific range w hile digital signals take on o n e o f tw o possible am plitude levels called nodes. Digital signals consist o f pulses or digits w ith d iscrete levels or values. The value o f the signal is specified as one o f two p o ssib ilities such as either I o r 0, high or low, true or false and so on. In th e process o f transm itting inform ation, all signals bearing inform ation are co ntam inated by noise. N oise is generated by m any natural and m an-m ade events that introduce errors during the transm ission and create serious problem s for the party rec e iv in g the inform ation to be able to properly recover transm itted m essages. W hen an an alo g signal is affected by noise it is m uch m ore difficult to regenerate that signal than it is to recover a signal that is o f the digital origin. W hen affected by noise a pulse (rep resen tatio n o f a digital signal) degrades as a function o f line length. B efore the pulse is degraded to an am biguous state, it could be am plified by a digital am plifier that reco v ers the p u lse 's original shape. In the case o f analog signals, once an analog signal is disto rted , the distortion can never be com pletely rem oved by the process o f am plification. B esides being m ore resentful to noise, there are m any other features that make a digital signal m ore suitable to convey inform ation com pared to an analog signal. Som e o f them are: reliability o f the system , flexibility o f the hardw are, and pricing o f the im plem entation. On the other hand, a digital transm ission would typically require a g re a te r system bandw idth to com m unicate the same inform ation in a digital form at as com pared to th o se in an analog form at. All this m akes a digital signal to be a signal o f ch oice for the com m unication w hen inform ation is to be transm itted in a reliable, robust, and relativ ely not expensive m anner.

FORMATTING: Channel A (Speech) Sampling ^ Quantizing ^ Pulse Code Modulation (PCM)

SOURCE ENCODING: % Lempel
Ziv

CRANNEL

W CDM4: ^ D uect Sequence Spied Spectimn (DSSS) Code Division
M ultiple

ENCODING: 4= Lmeai Block (LBC)

Welch Algoiitlun (LZ\\^

Access (CDMA.)

m W SNOTTER

Pseudo Random Sequence Geiieiatoi (PRSGl) WCDMADh ect Sequence Spied Spectimn (DSSS) H ' Code Division Midtiple Access (CDMA)

Z

EORE'IATTING: Channel B (Image) * PDF

SOURCE ENCODING: Huffman Coding Algoiitlun (HCA)

CRANNEL ENCODING: Convolutional

SOURCE DECODER: Chamiel A (Speech) 4= Lempel Ziv Welch Algorithm (LZW)

CHANNEL DECODER: Lmeai Block Code (LBC)

Pseudo Random Sequence Genei-atoi (PRSG2)

Dii ect Sequence Spied Spectimn (DSSS)

T

RECELER

4-

^ P s e u d o Random ( Sequence Generator V (PRSG l) CHANNEL DECODER; Viterbi Algoiitlun 'H D hect Sequence Spied Spectimn (DSSS)

SOURCE DECODER: Cliaimel B
( lin a g e )

iK Code Division Multiple Access (CDMA)

Huffman Codhig Algoridun (HCA)

Pseudo Random Sequence Generator] (PRSG2)

F ig u re 1: Source-C hannel C odec for a W C D M A B ased M ultim edia System .

T his pro ject sim ulates the basic elem ents o f a digital com m unication system show n in F igure 1. K eeping in analogy w ith the introduction given in the beginning o f this report, th is sy stem m ay be considered as a prim itive system o f the third generation (3G ) o f co m m u n icatio n system s. H ere are the reasons:

a)

It is a digital system ; m eaning that regardless o f the form at o f the input m essage, the system w ill conventionalize the m essage into a stream o f binary digits. An exception applies only in the case when the input m essage is already in the digital form at. This process o f transform ing any form o f input m essage into a stream o f bits (digital form ) is called form atting and will be discussed in details in C hapter 2;

b) It is a m ultim edia system ; m eaning that it processes m ore than one type o f m edia at the sam e tim e (speech and still image). The system is tw o channeled, w here channel A is carrying a speech data while channel B is dealing with a still im age data. In addition, the system is considered to be a sim plex tran sm issio n system . In m any cases it is desirable to m aintain tw o way com m unications, o r at least be able to send a m essage back to its origin for possible verification, com parison, or control. To sim ply im plem ent this type o f system , called full-duplex, another set o f blocks (stages) exactly the sam e ones as the ones show n in Figure 1 should be incorporated into the system but in a reverse order; That second set o f blocks would be responsible for the transm ission o f inform ation from the destination back to the source; c) It uses W C D M A technology in the process o f transm itting data from the source to the destination. At this point, it would be beneficial to say that W C D M A is m ore than Just a technology; it is a standard that establishes and defines various com m unication protocols and procedures used during a com m unication session. In this project m any o f these protocols and p rocedures will be ignored for the sake o f sim plicity. H ow ever, one that is considered as the main representative o f W CD M A standard, called M edium A ccess C ontrol (M A C ), will be fully presented and sim ulated. M edium access control defines how digital signals from different sources use the same allocated frequency spectrum to convey different inform ation to different recipients. M A C is discussed in C h ap ter 5; A fte r form atting, the next block in o u r digital com m unication system is the source e n c o d er stage. T he source encoding is the process o f rem oving redundant bits from the

sequence o f bits carrying inform ation. In other w ords, the source encoding com presses d a ta in a w ay th a t only necessary hits that m ake up the original inform ation are processed fu rth e r th ro u g h the system . In C hapter 3, tw o source encoding techniques are d em o n strated . O ne for the channel A that deals w ith the source encoding o f a speech sig n al fo rm atted into a stream o f hits in the previous stage (form atting stage). That source e n co d in g tech n iq u e is called the Lem pel-Z iv-W elch technique. The other technique called H u ffm an C oding A lgorithm will he used for the channel B to encode the still im age d a ta th at is, at this point in the process, already in the binary form . The chapter will also introduce the w ays how data encoded by the two encoding techniques can he d eco d ed on the receiver end. Since a source encoder and source decoder generally o p e ra te in pairs, this com bination o f a source encoder and decoder is called a source c odec (coder-decoder). The stage fo llow ing the source encoding stage is the channel encoding stage. The purpose o f the ch annel en co d er is to introduce, in a control m anner, some redundancy in the b in ary inform ation stream that can he used at the receiver to overcom e the effects o f n oise and interference encountered in the transm ission o f the bits through the channel. In o th er w ords, th e system will now add some redundant hits to the binary inform ation for the p u rp o se o f detecting and correcting som e o f errors occurred during the transm ission. T he red u n d a n t bits are form ed and organized in such a way that no response hack from the rec e iv e r is needed in order to detect and correct errors. This type o f error control is called forw ard erro r correction (FEC) and requires a one-w ay link only. In C h a p te r 4, tw o types o f channel encoding techniques are presented. One, for channel A, called th e L in ear B lock C ode technique and the other one, for the channel B, called the C o n v o lu tio n a l C ode technique. Along w ith both techniques their associated decoding p ro ced u res, such as V iterbi decoding algorithm for C onvolutional codes, will be discussed and im plem ented. Similarly as for a source encoder and decoder, a channel e n c o d er and d e c o d er operates in pairs as well, thus form ing w hat is called a channel codec. C h a p te r 5 entirely deals w ith the access control o f the m edium and, as it has earlier been m en tio n ed , presents the w ay o f m anaging this issue as defined by the W CD M A standard (tech n o lo g y ). A digital m odulation technique called D irect Sequence Spread Spectrum is p resen ted along w ith the C ode D ivision M ultiple A ccess technique. The role o f each one o f th e m in th e channelization process is discussed.

T he last chapter, C hapter 6, provides an overview o f the results obtained from the sim ulation in such a w ay that the input/output binary sequences to/from each block in the system s are exam ined and discussed. The chapter also gives a conclusion on the project as w ell as recom m endations on how to im prove the efficiency and efficacy o f the system presented and sim ulated in this project. F o r the end o f this introductory section, 1 w ould also like to m ention one im portant thing th a t w ould becom e m ore evident once a reader starts reading through the incom ing pages. N u m erical exam ples! T hroughout my entire education as an undergraduate as well as a g rad u a te student I had alw ays felt that any engineering theory or topic introduced in a class w ould have m ade m uch m ore sense to me if it was accom panied by a relevant exam ple. T h a t is w hy at the end o f each ch ap ter in this report an exam ple associated w ith th e algorithm o r technique presented in that chapter is w orked out. I believe that these ex am ples, besides clarifying the theory, will also help a reader to understand how the p resen ted algorithm o r technique is im plem ented in M atlab.

Chapter 2: FORMATTING
2.1 Speech Formatting
In general, signals in the com m unication theory could be considered either as an alo g o r digital ones. Signals in digital form are also told to be discrete signals. H o w ev er, b o th types o f signals bear inform ation that is conveyed from the source to the d estination th ro u g h different types o f m edium (s) (e.g. air, copper wire, etc.). Besides an alo g and digital types o f inform ation m entioned above, the data at the source, also called source inform ation, m ay be found in a textual form. Such kind o f source data is term ed as tex tu al inform ation. In th e digital com m unication system s the first and essential step in the process o f co n v ey in g inform ation from the source to the destination is to form at the source in fo rm atio n reg ard less o f the form that inform ation is in. That essentially m eans that the so u rc e inform ation has to be processed in a certain w ay that will make it suitable for fu rth e r digital processing. L e t's consider all three possible form s o f source inform ation th at have been m entioned earlier and the w ay they are form atted in order to be c o m p atib le w ith the next stages in a digital com m unication system . T he sm allest headache w ould give us an inform ation source, or data, that is already in the d igital form . It w ill sim ply ju st bypass the form atting stage and proceed to the next p ro cessin g step. T he problem begins with the fact that the m ost data in com m unications is e ith e r in tex tu a l o r in analog form at. Data in textual form at would usually be encoded w ith one o f several standards such as: ASCII (A m erican Standard C ode for Inform ation In terch an g e), E B C D IC (Extended Binary Coded D ecim al Interchange C ode), Baudot, and H ollerith . T he aforem entioned standards transform the textual data into a digital form at. N o w , w e com e to analog inform ation at the source and we are interested how it can be m ade su ita b le for further digital processing. Sim ply said, we would like to determ ine the n ecessary c o n d itio n s w hich will allow us to change analog inform ation to digital one w ith o u t loss o f inform ation. As a criterion o f how well the process o f converting the an alo g inform ation into the digital one can be carried out, w e use an im portant condition th at th e orig in al inform ation can be fully reconstructed by using reversible, digital to an alo g p ro cessin g steps.

In essence, analog inform ation is form atted using three separate steps: sam pling, quantization, and coding. 2.1.1 Sam pling

T he link betw een an analog signal and the corresponding digital signal is given by w h at is know n as the sam pling theorem . The sam pling theorem states the follow ing: A real valued band lim ited signal having no spectral com ponents above a frequency o f fm (H z), know n as the m axim um frequency o f the analog signal, is determ ined uniquely by its values at uniform intervals spaced no grater than Tg seconds apart, w here Tg is:

(0

T his statem ent is a sufficient condition that an analog signal can be reconstructed com p letely from a set o f uniform ly spaced discrete sam ples in tim e. The output o f the sam pling process is called the pulse am plitude m odulation (PA M ) because the successive o u tp u t intervals can be described as a sequence o f pulses w ith am plitudes derived from the sam ples o f the analog signal. I f equation stated in (1) is applied, all replicas o f the original spectral d ensity are ju st tangent to each other and an ideal low pass filter can be used to reconstruct (theoretically) the original analog signal from the sampled version o f th at signal. H ow ever, if the sam pling interval Tg becom es slightly larger than the right side o f the equation (1), then there will be an overlap o f spectral densities and the original signal w ill n ot be successfully reconstructed from its sampled version with the help o f an ideal low pass filter. In order to avoid the situation described in previous sentence it is essential and ab solutely necessary in the process o f sam pling o f an analog signal that:

' ' ' i t T he equation given in (2) is a m athem atical interpretation o f the sam pling theorem . The m ax im u m tim e interval Tg is called the N yquist interval. If w e w ant to see the equation (2) as th e relationship betw een the m axim um frequency o f the analog signal f ^ and the sam pling frequency fg, w here fg is reversely proportional to Tg then w e arrive to the fo llo w in g equation:

(2)

T he equation (3) given in term s o f the sam pling and m axim um frequencies is called the N y q u ist S am pling R ate. In practice, the full potential o f the sam pling theorem usually cannot be realized and the equations (2) and (3) serve as upper bounds on actual perform ance. O ne reality fact that we are faced w ith in dealing with the sam pling th eo rem is that w e cannot build an ideal low pass filter. We can only build a low pass filter w ith as fast an attenuation rate as possible. One thing that we can do to overcom e th e inability to have an ideal low pass filter is to increase the sam pling frequency to allow som e frequency space before the next frequency replica o f the sam pled analog signal appears.

A n o th e r reality fact being responsible for the sam pling theory not being used in its full potential is th e fact that a tim e limited signal is never strictly band lim ited. W hen such an an alo g signal is sam pled, there will alw ays be som e unavoidable overlap o f spectral co m ponents. F urtherm ore, in reconstructing the sam pled version o f the signal, frequency co m p o n en ts originally located above one h alf o f the sam pling frequency will appear below this point and will be passed by the low pass filter. This is known as aliasing and results in a d istortion o f the signal. The effects o f aliasing can be partially elim inated by a p p ly in g as good as possible low pass filtering before sam pling and by sam pling at rates g re a te r than the N y q u ist rate. A n interesting q u estion arises here; if we w ant to apply the sam pling theorem to bandpass signals, do w e still have to obey the rule given by the equation (2), stating that we have to sam ple b an d p ass signals at tw ice the highest frequency? The answ er would be no, since the m in im u m sam pling rate depends on the bandw idth o f a low pass signal rather then on its h ighest frequency. In the case o f low pass signals these tw o conditions coincide. H ow ever, w h en sam pling a bandpass signal we should use a m inim um sam pling rate in th e ran g e b e tw e en 2 and 4 tim es the bandw idth o f the signal. This m inim um rate req u irem en t fo r bandpass signals approaches the limit o f tw ice the bandw idth as the c en ter freq u en cy o f the signal increases. 2.1 .2 Q u an tization A fte r th e sam pling, there comes the second step in form atting o f an analog signal; quan tizatio n . Q uantization or quantizing is the task o f m apping sam ples o f an analog signal, obtained th ro u g h the process o f sam pling, to a finite set o f am plitudes. To m ake an a n alo g y w ith th e sam pling process, the process o f quantization is related to the yo rd in ate sim ilarly as the process o f sam pling is related to the x-axis. The sim plest q u a n tiz er perform s m apping o f each sam ple o f the sam pled analog signal to one o f the

10

p red eterm in ed qu an tizer levels. I f those predeterm ined quantizer levels are equally spaced then w e say that the quantizer is a linear quantizer. Sim ilarly, if the levels are not eq u ally spaced then w e say that the quantizer is a nonlinear quantizer. Since this project d eals w ith th e linear quantization only, the further discussion on the process o f the q u an tizatio n o f a sam pled analog signal will be strictly lim ited to a linear quantizer and its characteristics. H ow ever, it is im portant to m ake a com m ent that the nonlinear q u an tizatio n provides m uch better the signal to noise ratio (SN R ) than the linear q u an tizatio n does. This is particularly evident in speech com m unication w here very low speech volum es predom inate 70% o f the time. A linear q u an tizer is the universal form o f the quantizer in a sense that it m akes no assu m p tio n ab o u t the am plitude statistics and correlation properties o f the input analog signal. T he o nly tw o conditions that have to be know n in ord er to im plem ent a linear q u an tizer are: the dynam ic range o f the sam pled signal (D R ) and the num ber o f bits that each sam ple is represented. The dynam ic range is defined as: D R = [m ax_ sig - m in_ sig], (4)

w here 'm a x sig ' and 'm in sig ' are the m axim um and m inim um values o f the sam pled analog signal. T he second condition, the num ber o f bits that each sam ple in the sam pled v ersio n o f th e analog signal w ill be represented w ith, is directly related to the num ber o f levels o f the desired quantizer. The relation betw een the num ber o f bits for each sam ple rep resen tatio n and the num ber o f levels o f the quantizer is given by: L = 2'^. (5)

In eq u ation (5) L is the num ber o f the levels o f the quantizer and R is the num ber o f the bits th a t each sam ple is represented with. N ow , by taking a logarithm w ith the base 2 to each side o f the equation (5), we get another relation betw een the num ber o f bits for each sam ple representation and the num ber o f levels o f the quantizer: R = log2 L. A t th is p oint it is essential to reveal a restriction and an observation associated with the p ro cess o f quantization. The restriction is that the equations (5) and (6) are valid only if w e intend to apply the fix length representation o f each sam ple. In the case o f variable length rep resentation, the equations (5) and (6) are no longer valid. M ore overview and d iscu ssio n on fixed and variable length representations o f sam ples will be given later in (6)

11
C h a p te r 3, when w e deal w ith the H utfrnan code algorithm . The observation to be revealed is th at once quantized, the instantaneous values o f the analog signal can never be ex actly reconstructed again. O nce w e establish a desired num ber o f levels for the quantizer, w e can easily determ ine th e size o f each quantizer level, usually called the quantile interval. A rem ainder that a q u an tizer has all its quantile intervals o f the sam e size only if that quantizer is a uniform q u an tizer. T he size o f each quantile interval q is given by the follow ing equation: q _ (m ax s ig - m in sig)

A linear q u an tizer w orks in a very sim ply way. Inputs to the quantizer are sam ples o f an

a n a lo g signal obtain through the process o f sam pling, w hile the outputs from the q u a n tiz er are predeterm ined values o f the quantizer's levels that the sam ples are m apped to. T he d ifference betw een the input and output o f the quantizer is called the quantization error. M athem atically, the quantization error is represented as:

e (n )^ x (n )-x (n ),

(8)

w h ere x(n) represents the input vector containing the signal sam ples, x (n ) is the q u antized o u tp u t vector, w hile e(n) is the error vector. The quantization error vector is also referred to as the quantization noise. U nder the restriction that the input signal has a sm o o th probability density function (PDF) over the quantization interval, it can be assu m ed th at the quantization errors are uniform ly distributed over the quantization interval ranging from - q /2 to q/2. Each probability density function m ust be greater or eq u al to zero and m ust satisfy the follow ing condition:
q /2

| f ( x ) d x = l.
-q/2

(9)

In o rd e r to satisfy the equation (9), the probability density function o f the quantization e rro r p(e) m ust be equal to 1/q in the interval from -q/2 to q/2. O utside that given interval p (e) m u st be equal to zero. H ere is m athem atically interpretation o f w hat has ju st been said:

12

1 . 1 1 -- fo r -- < e < -- p(e) =

q
0

q

q

( 10)

otherw ise

A u sefu l figure o f m erit for a uniform quantizer is the error variance, w here the error v arian ce is d efin ed as:
q/2

=

|( e - m x ) ^
-q/2

p (e)-d e,

(11)

w h ere m ^ is th e e rro r m ean th at is equal to:
q/2 q/2 q/2

m. -

J

e p (e )d e =

J
-q/2

e -- de = -- a a

=

0.

( 12)

-q/2

-q/2

N o w , by placin g th e erro r m ean m ^ from the equation (12) into the equation (11) w e get th e erro r variance:
q/2 q/2 2

< 3 ^=

Je^ -p (e)d e =
-q/2 -q/2

_q_

(13)

T he e rro r variance is also com m only know n as the noise pow er. Sim ilarly, the signal varian ce (signal pow er) is defined as:

(js = j x ^ p ( x ) d x .

(14)

and can be substituted w ith the expression for the peak pow er o f an analog signal n o rm alized to 1 Q:

P s= V s Is= V s

^ = Vs2 = fVppl JhlS] I 2 J I 2 j Rs

(15)

13
T he ratio betw een the signal variance (signal pow er) given in the equation (15) and the n o ise p o w e r given in the equation (13) yields the quantization signal to noise ratio, (SNR)q:

L 2-q2 noise pow er =^ q^
12

=

(16)

T h e (SNR)q is usually given in the units o f decibels [dB] and is obtained by applying the fo llo w in g conversion form ula: (SNR)q[dB] = 10-logio(SNR)q. (17)

F rom the equation (16) w e conclude that the signal to noise ratio for a uniform quantizer solely d e p en d s on the num ber o f levels L. Since from the equations (5) and (6), the n u m b er o f levels L is directly proportional to the num ber o f bits used to represent each sam ple, thus w e can also say that the signal to noise ratio depends on the num ber o f bits used to represent each sam ple. The m ore bits are used to represent a sample, the better the signal to noise ratio. It is easily proven that for each additional bit used to represent a sam p le (e.g. increase from a 5-bit sam ple representation to a 6-bit sam ple representation), the im p ro v em en t in the (SNR)q is approxim ately about 6 dB. A quick approxim ation o f the (SNR)q for aquantizer is to m ultiply the num bers o f bits that each sam ple is rep resen ted w ith by 6 dB. H ow ever, the real quantization (SNR)q is much sm aller due to the im perfections in the quantizer itself (linear vs. nonlinear quantizer). As the num ber o f levels L approaches infinite, the signal approaches to its form before the quantization (P A M form at) and the (SNR)q approaches to infinity. In other w ords, with the infinite n u m b er o f quantization levels, ultim ately there is no quantization noise. 2 .1 .3 P ulse C od e M odulation (PCM ) T he next step in the process o f converting an analog signal into a digital one is to assig n a digital value for each quantile interval in such a w ay that each interval has a one to one correspondence w ith the set o f real integers. This is called the digitization. The p ro ce ss o f digitization reduces the original analog signal to a set o f digits, at the su ccessiv e sam ple tim es, m apped into L quantizer's levels. Each sam ple o f the original an a lo g signal is assigned a quantization level (quantile interval) closest to the value o f the

14

actual sam ple. The digits are expressed in a coded form . The m ost com m on code used for th is purp o se is a binary code w here each digit is represented as a com bination o f zeros and ones. Each binary 1 is further represented b) a pulse and each binary 0 is represented by th e absence o f a pulse. Thus, instead o f transm itting the individual sam ples, a com b in atio n o f zeros and ones, binary pulse code, is sent at each sam ple tim e carrying th e intended inform ation in digitized form. C om m unications system s m aking use o f this kind o f data representation during the transm ission are com m only called pulse code m o d u latio n system s; PC M system s.

W av efo rm carrying inform ation can be transm itted even more efficiently if we represent th em as sequences o f transitions betw een upper and lower voltage levels. W hen the w a v e fo rm is at the upper voltage level it represents a 1. Sim ilarly, w hen the w aveform is at th e low er voltage level it conveys a 0. There are m any PCM w aveform types classified in m any groups. H ere only a few will be m entioned: N on Return to Zero (N RZ), Return to Z ero (R Z ), Phase E ncoded, and M ultilevel binary. The m ost com m only used are N R Z PC M w aveform s. The reason w hy there are so m any different types o f PCM w aveform s lies in d ifferences in perform ances for different waveform coding schem es. Some schem es are better in perform ing erro r detections, som e are better in correcting data erro rs, som e schem es again are better in increasing the efficiency o f bandw idth utilization. C ertain types o f PCM w aveform s are m ore imm une than others to noise. All this con trib u tes that so m any o f PCM w aveform schem es are used, but the decision w hich one and w hen to be used greatly depends on required perform ances and characteristics o f the digital system used. 2 .1 .4 M a tla b Im p le m e n ta tio n o f F o rm a ttin g T he form atting process explained in preceding sections is sim ulated in M atlab w ith th ree M files: `S am pling.in' and `Q uantization.m ' files that describe the sam pling and q u an tizatio n processes and `P C M .m ' file that deals with the process o f digitization, w h ere each quantized sam ple is coded to a corresponding binary word. In the `S a m p lin g .m ' file an analog signal in the form o f a sound file ( `speech.w av') is loaded in th e M atlab environm ent. M atlab provides the com m and `w avread' w hich perform s sam p lin g au tom atically w hen the sound file is loaded into the environm ent. The standard sam pling rates for PC based audio hardw are are 8,000, 11,025. 22,050, and 44,100 sam ples per second. M ono signals are returned as one colum n m atrix, while stereo signals are returned as tw o colum n m atrices. The first colum n o f a stereo audio m atrix co rresp o n d s to the left input channel, w hile the second colum n corresponds to the right input channel. T he `w a v re ad ' com m and, after being executed, returns tw o output

15

variables, the sam pled data and the sam ple rate. The sam ple rate used for the speech file used in th is project w as 22,050H z. Let s see how the sam pling frequency is obtained. I f w e plot the frequency spectrum o f the speech signal as shown in Figure 2 we see that th e m ax im u m frequency o f the signal is 11,025 Hz. N ow , if w e recall the equation (3) stating th a t the N y quist Sam pling Rate m ust be equal o r greater than the m axim um freq u en cy o f the signal, it is clear why the sam ple rate that our signal is sam pled w ith is 2 2 ,0 5 0 Hz.

1800 1600 1400

S p e c tru m of th e S p e e c h S ignal

1200
1000
BOG

600 400

200

1 ^ 1 .; ..
6 8 F re q u e n c y [Hz]

10

12

14

X10

Figure 2: Frequency Spectrum o f the Signal `speech.w av'. T h e sam pled d a ta contains 1 10,033 samples. T hat num ber can be obtained through a sim ple c alcu latio n w here the num ber o f sam ples is equal to the product o f the duration o f th e sig n al (in seconds) and the num ber o f sam ples for 1 second. W e already know that the n u m b er o f sam ples for 1 second is equal to the sam pling frequency, w hich is 22,050, and w e can co n clu d e from the Figure 3, which show s the A SCII representation o f the speech signal, th a t th e duration o f the speech signal is 4.9902 seconds. A sim ple calculation g ives us th e num ber o f sam ples that the original speech signal is represented after the sam p lin g process; 4.9902 X 22,050 = 110,033 samples.

16

ASCII Version of The S peech Signal

2

2.5
tim e [sec.]

3

F igure 3; Signal `sp e ec h .w av ' in T im e D om ain. In th e `Q u n a tiz in g .m ' file, as per earlier discussion (the equations (4), (5), and (6)), w e firstly h av e to specify the d y nam ic range o f the sam pled signal as w ell as the num ber o f lev els th a t o u r q u an tizer w ill have. T he M atlab com m ands, `m a x ' and `m in ', allow us to o b tain th e d y n am ic range o f the sam pled signal (see the equation (4)). T he num ber o f levels L is ch o sen to be 256 since w e deal w ith a speech signal. Speech signals in general re q u ire 8 bits p er sam ple since it is the m inim um num ber o f bits that can allow us to hear o rig in a l sp e ec h e s in via their quantized versions. A pplying the equation (5), w e see how th e n u m b e r o f quan tizatio n levels is obtained. N o w w h e n w e k n o w the dynam ic range o f the sam pled signal and the num ber o f levels fo r th e q u a n tiz er, w e can calculate the quantization step q that is also the size o f the q u a n tile interval. O nce again, since w e deal w ith a uniform quantizer each one o f 256 q u a n tile in tervals w ill have the sam e size. To obtain the value o f q we sim ply ap p ly the e q u a tio n (7). U p o n its e x ecu tio n , the M atlab com m and `q u a n tiz ' produces as the output tw o variables fo r each sa m p le o f the sam pled signal; the quantization index `indexl and quantized o u tp u t v a lu e `q u a n ts l ':

[indexl,quant; si ] -- quant, iz (s i g ,part it i o n l ,codebookl ),

17
w h ere `p a r titio n ! ' is a real vecto r w hose elem ents are values o f 256 q u a n tiz er's levels a ssig n e d to each sam ple. T he elem ents o f the `partition! ' vecto r m ust be given in strictly a sc e n d in g o rd er. T he input variable `c o d eb o o k l ' is a vector codebook that prescribes a v a lu e fo r e a ch p artitio n in the quantization and its length exceeds the length o f the p a rtitio n ! v e c to r by one. A s m entioned earlier, th e o utput variables are; ` index 1' and `q u a n ts ! '. I f th e `p a rtitio n ! ' vecto r has the length o f n, then the `index! ' vector is a c o lu m n v e c to r w h o se k^^ e n try is given by:

0 index (k ) = m n if

if sig(k) < partition( 1) partition( m) < sig(k) < partition( m + 1). if partition( n) < sig (k)

T h e o u tp u t v ariab le `q u ants 1 ' is a row vector w hose length is the sam e as the length o f th e in p u t sam pled signal. T he row vector `quants 1' contains the quantization o f the sa m p le d sig n al b ased on the quantization levels and prescribed values. The `q u an ts! ' is re la te d to th e `c o d e b o o k l ' and `index! ' variables by th e follow ing equation: q u a n ts(k ) = codebook (in d e x (k ) +1). In th is e q u a tio n , k tak e s on integer values betw een 1 and the length o f the sam pled signal. T h e v a ria b le `in d e x l ' contains elem ents (decim al num bers) that represent the m e m b e rsh ip o f each sam ple to one o f 256 quantization levels. In other w ords, those d e c im a l n u m b ers rep resen t w hich level L each sam ple has been assigned to. Any num ber fro m 0 to 255 is a valid num ber that a sam ple can be assigned to. T he task o f `P C M .m ' M at lab file is to co n v ert the decim al index num ber, indexl (k), o f each sam ple to a related c o d e w o rd . S ince co d e w ords are supposed to be in the binary form , thus each code w ord is c re a te d as a co m b in atio n o f binary ones and zeros. T h e c o n v e rsio n from d ecim al to binary is perform ed by the `co n v ert2 b in ary .m ' M at lab file. T h e c o d e d o es the follow ing: It takes each elem en t o f the `index! ' v ecto r and keeps d iv id in g th e value by 2 until the result is 1, keeping the track o f rem ainders for each d iv isio n . W h en 1 is reached as a result o f dividing, the rem ainders are appended to the re su ltin g 1, fo rm in g the binary representation o f the decim al value. H ere is a quick ex a m p le: L e t's c o n v e rt th e n u m b er 109 to its binary representation:

18
1 0 9 : 2 = 5 4 ( 1 ), 54 27 : 2 = 2 7 ( 0 ), : 2 = 1 3 (1 ),
6

13 : 2 =
6

( 1 ), (0), ( 1 ). . H ere is also the

:2 = 3 :2
= 1

3

T h e re su ltin g b in ary rep re sen ta tio n o f the num ber 109 is: I 1 0 1 1 0

1

resu lt obtain ed by the M atlab file `d 2 b .m ' file, w here the `d 2 b .m ' file is the m odified v e rsio n o f th e `c o n v e rt2 b in a ry ' and can be used separately from the m ain `P C M .m ' file th a t p e rfo rm s the p ro cess o f d ig itizatio n ( `d 2 b .m ' is listed in the A ppendix B section o f th is rep o rt): an s .

=

1

1

0

1

1

0

1

T h e re is now one m ore th in g that has

to be

taken care of. In the equations (5) and

( 6 ) w e h av e given th e relation betw een the num ber o f levels o f the quantizer and the n u m b e r o f bits th at each co d e w ord should be com posed of. Since w e use a 256 level q u a n tiz er in this p ro ject, by a p p ly in g the equation ( 6 ) we obtain th at each code word should co n sist o f
8 8

bits. H ow ever, w e know that only the num bers from 128 to 255 need bit for their binary representations (e.g. the num ber 109 needs 7

bits fo r th eir b in ary rep resen tatio n . So, w hat happens w ith the num bers from 0 to 127
8

th a t do n o t need all re p re se n te d by

bits, see th e ex am p le above). W ell, to have each num ber in the range from 0 to 255
8

bits, w e have to add the so-called `m issing' bits. F or instance, the

n u m b e r I 0 9 's b in ary rep resen tatio n is a 7 bit com bination o f ones and zeros: 1 1 0 1 1 0 1. T o o b tain a 8 bit b inary rep resen tatio n w e have to concatenate one 0 at the beginning, so th a t th e resu ltin g 8 bit representation is: 0 1 1 0 1 1 0 1. The file `c o n v ert2 b in ary .m ' ta k e s c a re th a t ev ery n u m b er in th e [0,255] interval is represented w ith called a fixed length rep resen tatio n o f code w ords. A t th is p o in t w e also have to create a stream o f bits m ade up o f the binary representation (c o d e w o rd ) for each sym bol in th e inform ation that is to be transm itted. The code w ords a re p lace in th e stream in the FIFO m anner, m eaning that the code w ord for a sym bol that is to be sen t o u t first is placed at the beginning o f the stream , the second code w ord is a p p e n d e d o n to th e first o n e and so on until all sym bols from the inform ation are p ro ce sse d .
8

bits creating so-

19

2.2 Image Formatting
M a tla b sto re s a gray level im age in the form o f a tw o-dim ensional m atrix, w here each e le m en t o f the m atrix co rresp o n d s to a single pixel in the displayed image. For ex am p le, an im age c o m p o sed o f 200 row s and 300 co lu m n s o f d ifferently colored dots w ould be sto re d in M atlab as a 200 by 300 m atrix. Som e im ages, such as R G B (R edG re e n -B lu e ) im ages, req u ire a three-dim ensional m atrix to be stored in, w here the first plan e in th e th re e d im e n sio n s represents the red p ix els' intensities, the second plane rep re sen ts the g reen p ix e ls' intensities, and the third plane represents the blue p ix els' in tensities.

I f th ere is a need to convert a gray level image into a color level im age, a straight forw ard c o n v e rsio n is used. T hat is exactly w hat has been do n e in this project. A color im age is c o n v e rted into g ray level im age in order to save co m p u tatio n al tim e as w ell as m em ory space. T h a t m ea n s th a t instead o f processing three tw o-d im en sio n al m atrices for a color im age, w e w ill be p ro cessin g only one
2

-dim ensional m atrix for a gray level image.

M e m o ry w ise th a t m eans that instead o f using 3 bytes o f m em ory storage for each co lo re d pixel, w e w ill need only 1 b>te to store the n um ber representing one pixel. T he c o n v e rsio n w ill be carried by using the follow ing form ula: A = 0 .2 9 8 9 i(: ,:,l) + 0.5870 i(:,:,2) + 0.1140-i(:,:,3). (18)

In th e eq u a tio n (1 8 ), A is a 2-dim entional m atrix rep resen tin g a gray level im age, I(:,:,l) is th e `R ed In te n s ity ' p art o f the color image, 2) is th e `G reen In ten sity ' part o f the c o lo r im age, a n d ï(:,:,3 ) is the `Blue Intensity' part o f the c o lo r image. The coefficients th a t th e in te n sitie s are m ultiplied w ith in the equation ( 18): 0.2989, 0.5870, and 0.1140 are related to th e eye's sensitivity to a Red, G reen, and B lue c o lo r according to the N T SC (N atio n al T e le v isio n S ystem (s) C om m ittee) standard and are obtained th roughout an e x p e rim e n ta l w ay. A s m e n tio n e d e a rlie r each pixel in a gray level rep resen tatio n ( 2 -dim entional m atrix) w ill be re p re se n te d by
8

bits. T h is m eans that intensity o f each pixel will be in the range o f 0

to 2 5 5 , w h e re 0 rep re sen ts th e pure black color, w hile 255 represents the solely w hite c o lo r. T h is can be show n by a sim ple M atlab program that displays a 7 by 7 gray level im a g e re p re se n tin g a black diag o n al line as pictured in F igure 4:

20

2

3

4

5

6

7

F igure 4: G rey L evel Im age o f a D iagonal B lack Line. H e re is th e sim p le M atlab code that displays the im age show n in Figure 4:
» A = [ 2 5 5 2 5 5 2 5 5 2 5 5 2 5 5 2 5 5 0; 2 5 5 2 5 5 2 5 5 2 5 5 2 5 5 0 255, 255 255 255 255 0 255 255 255 255 255 0 255 255 255 255 255 0 255 255 255 255 255 0 255 255 255 255 255 0 2 5 5 2 5 5 2 5 5 2 5 5 2 5 5 2 5 5 ];

» » » »

im a g e (A ); c o lo r m a p (g ra y (2 5 6 )); i m w r i t e ( A , ' d i a g j i n e ' , 'J P G ') ; A = i m r e a d ( 'd i a g _ l i n e ', 'J P G ') ;

A t th e first d o uble-prom pt line a 2 -dim ensional m atrix A o f the size 7 x 7 (49 pix els) is c re a te d w here the all 255 num bers rep resen t pure w hite intensity o f the pixels, w h ile th e all 0 nu m b ers represent pure black intensity o f the pixels. N ext, the m atrix is d isp la y e d using M atlab `im ag e' com m and. The `co lo rm ap (g ray (2 5 6 ))' com m and sets the c o lo r m ap o f th e im age to black and w hite (gray level im age). The `im w rite' com m and sav es th e im age as `d ia g _ lin e .jp g ' file. I f w e now w ant to im port the file `d iag _ lin e .jp g ' into th e M atlab en v iro n m en t for any further p ro cessin g and m anipulation, w e w ould use

21

th e ^ m r e a d

co m m an d and the black diagonal line im age w ill be im ported back into

th e A m atrix . T he ab o v e d em o n strated procedure is exactly how the im age in th is project is fo rm a tted by using M a tla b environm ent.

A n atu ral q u e stio n th a t a curious person m ight ask at this point is: how do w e convert the A S C II rep re sen ta tio n o f the im age (a m atrix co n tain in g the num bers in the [0-255] range) to its e q u iv a len t b in ary interpretation? Recall that w e have earlier said th at in o rd er to p ro c e ss any in fo rm atio n th rough a digital com m unication system , w e have to have the in fo rm a tio n being rep resen ted (so is our image) by a com bination o f zero s and ones. T he a n sw e r is th at w e w ill produce a norm alized probability density function (PD F) o f o u r im age, c a lc u la te the probability o f occurrence o f each num ber in the [0-255] range and th e n use th e H uffm an coding algorithm to obtain the binary representation o f the im age. In th e p ro cess o f calcu latin g the probability o f o ccurrence for each num ber in the [0 -2 5 5 ] range, an y n u m b er in the range that does not ap p ear in the image m atrix is c o n sid e re d as irrelevant as if it carries no inform ation relevant to ou r image. All those ` irre le v a n t' n u m b ers w ill be rem oved from the list o f leaves that are supplied to the H u ffm a n co d e algorithm for data com pression. O nce the frequency o f occurrence o f each n u m b er in th e m atrix is calculated, it is divided w ith the total num ber o f elem ents in the m atrix fo rm in g th e p ro b ab ilities o f occurrence for each n um ber in the [0-255] range that ap p e ars in the m atrix. F o r e x a m p le, if w e co n sid er the m atrix A containing pixels o f the 7 x 7 black diagonal line im ag e from F igure 4, w e see that the num ber 255 o ccurs 42 tim es, w hile the num ber 0 o c c u rs o n ly 7 tim es. D iv id in g their frequencies o f o ccu rren ces w ith the total num ber o f ele m en ts in th e m atrix, 49, w e obtain the corresponding probabilities o f o ccurrence for th e n u m b e rs 0 and 255: P ro b ab ility o f occurrence for 0: 7/49 = 0.1428 = 14.28 (% ), P ro b a b ility o f occurrence for 255: 4 2 /4 9 - 0.8571 = 85.71 (% ). T h e c o rre sp o n d in g PD F function o f the black diag o n al line w ould look as show n in F ig u re 5. In th e p ro ject, th e M atlab file, `F o rm a ttin g .n r, perform s the form atting o f a c o lo r im age `w o rld .jp g '. T he file is w ritten in ex actly the sam e m anner as being e x p la in ed above: it p erfo rm s the RG B to gray co nversion (equation (18)), calculates PD F o f th e im age, and gives, fo r the next stage in the process, tw o vectors, one that contains all n u m b e rs p re se n t in the im age, and the other one that contains the respective p ro b a b ilitie s o f o c c u rre n ce for all num bers in the range [0-255] present in the im age.

11

N o rm a lize d P D F o f t h e B la c k D iagonal Line

0:9

...0.8 0.7
0.6
V= 0.85714

1

0.5 0.4 0.3
0:2
:ix=0 I Y= 0/I428E 1............ 1

-1

0.1
0

>

I

-50

0

50

100

150

200

250

300

Figure 5: PD F function o f the Black Diagonal Line.

23

Chapter 3: SOURCE CODEC
3.1 Huffman Coding Algorithm
B e fo re starting a discussion on the H uffm an coding algorithm , som e basic term s and d e fin itio n s n eeded for understanding the principles o f the H uffm an algorithm will be first p resen ted . W e w ill sta rt w ith the definition o f a finite discrete source, w here a finite d isc re te source is a d isc re te source that is em itting, at a constant rate, a finite num ber o f sy m b o ls fro m th e source alphabet called a stream o f sym bols. E ach sym bol in an infinite long (th e o re tic a lly ) stream em itted by the source is assigned its ow n probability o f o c c u rre n c e P. O n ce w e k n o w the probability o f a sym bol, w e can introduce the concept o f th e in fo rm atio n , w here the inform ation I is defined as a value inversely proportional to th e p ro b ab ility o f o cc u rre n ce o f the letter P:

loci.

(19)

P M o re precisely , the inform ation content o f a sym bol Xj is defined as a base 2 logarithm ta k e n fro m th e p ro b ab ility o f occurrence o f the sym bol x ; . M athem atically:

I(xi) = log2-- l - = -l o g2 P (X i) . P(Xi)
S in ce th e so u rc e alp h ab et contains a finite num ber o f sym bols N , w e w ould be m ore in te re ste d in fin d in g a m easure o f the inform ation co n ten t for all the sym bols in the so u rc e a lp h a b e t, rath e r than dealing w ith the inform ation co n ten t o f each sym bol in d ep e n d e n tly . Such a m easure o f the inform ation co n ten t for all the sym bols in the

(20)

so u rc e a lp h a b e t is called the entropy o f the source and is defined as the average am o u n t o f in fo rm a tio n em itte d by th e source: N H (X ) =
i=l P ( x j )· l ( x j ) = i =]

N
P(Xj

)· lo g 2

P (x i

).

(21)

T h e u n it fo r th e e n tro p y is a bit per event and should not be confused w ith a binary d igit c alled a bit. I f all sy m b o ls in the source alphabet are equally probable (have the sam e p ro b a b ilitie s o f o c c u rre n ce ), then the entropy o f such a source is given by the follow ing e q u a tio n :

24

'

H (X ) = log2 N ,

(22)

w h ere N is a fin ite n u m b er o f sym bols in th e source alphabet.

N o w , w e w ill c o n sid e r th e process o f encoding, o r representing each sym bol in the source a lp h a b e t by a seq u en ce o f binary digits; ones and zero s. Since there are N possible sy m b o ls in the so u rc e alphabet, the num ber o f b inary digits R per sym bol required to u n iq u ely e n c o d e each sym bol is given by;

R = lo g 2 N . T h e eq u a tio n (2 3 ) holds only w hen the num ber o f sym bols in the source alphabet is a

(23)

p o w e r o f 2. I f it is not the case, the equation (23) requires a m inor adjustm ent and is re w ritte n as; R = Llog 2 N j + l , (24)

w h e re th e sym bol, [ J , identifies the largest integer less than MogiN'. The efficiency o f th e e n c o d in g is defined as the ratio betw een the en tro p y H (X ) and the num ber o f binary d ig its R n eeded to rep resen t any o f N possible sym bols in the source alphabet. The binary re p re se n ta tio n s o f N eq u ally probable sym bols are called the hxed -len g th co d e w ords. O n th e o th e r side, w hen the source sym bols do not have the sam e probability o f o c c u rre n c e , as it is the case in the real w orld, a different encoding m ethod is required. An e x a m p le o f such an en coding schem e is the H uffm an co d in g algorithm . H uffm an coding a lg o rith m is a v ariab le-len g th encoding algorithm based on the probabilities o f the source sy m b o ls. T he a lg o rith m is considered as an optim um algorithm in the sense that the a v e ra g e n u m b er o f bits required to represent the source sym bols is m inim um . In other w o rd s, th e o b jectiv e o f the algorithm is to provide a system atic procedure for c o n stru c tin g uniq u ely d eco d ab le variable-length co d e w ords that are efficient in the sense th a t th e av erag e n u m b er o f bits per source, defined as the follow ing quantity;
N

n = ^ P (X j)n i, i=l

(25)

is m in im iz ed . It is ab so lu te ly necessary that these variable-length code w ords arc u n iq u ely and in sta n ta n e o u sly decodable. They also have to satisfy the prefix condition th a t req u ires th a t fo r a given co d e w ord C o f length k, no other code w ord o f length m, w h ere m > k, c o n ta in s the code w ord C as its prefix.

T h e m e a su re o f e ffic a c y o f th e variable-length en coding is the ratio betw een the source e n tro p y H (X ) and th e average length o f the code w ord n , and is represented by the G reek le tte r r\:

r| = i l 2 ^ (o /o )^

0

< T |< 1 .

(26)

T h e e q u a tio n (26) te lls us th at the entropy o f the source can not be greater than the a v e ra g e length o f th e co d e w ord. Based on this, w e can present another equation th at g iv e s th e relatio n b etw een the source entropy and the average length o f the code w ord:

H (X )< n < H (X ) + l. 3.1.1 H u f f m a n E n c o d in g L e t's n o w ta k e a look how the coding algorithm w orks. The H uffm an coding

(27)

a lg o rith m th a t g e n e ra te s variable-length code w ords is considered to be a tree form ing p ro c e ss. T h e p ro ce ss starts by listing all letters in the source alphabet, along w ith th eir p ro b a b ilitie s in d e sce n d in g o rd er o f occurrence. These entries correspond to the leaves (en d s) o f th e tree. E ach le a f is assigned a leaf w eight equal to the probability o f o c c u rre n c e o f th a t leaf. The tw o leaves w ith low est probabilities o f occurrence are m erg ed to g e th e r fo rm in g a branch w hose w eight will be equal to the com posite p ro b a b ilitie s o f the m erged leaves. A fter each m erging, the new branch and the rem aining b ra n c h e s (o r leaves) a re reordered to assure that the reduced tree preserves the d e sc e n d in g o rd e r o f the probability o f occurrences for all current braches and rem aining leaves. H e re w e d istin g u ish e d tw o different w ays o f reordering the rem aining branches: the to p and th e b o tto m a p p ro ach . To dem onstrate these tw o approaches, let's assum e that a n e w ly fo rm in g b ran ch has the com posite probability equals to 0.25, and that during the p ro c e ss o f re o rd e rin g th e branches in the tree we find that another tw o branches have the s a m e p ro b a b ilitie s o f occu rren ce o f 0.25 as the new ly form ed branch does. T he top a p p ro a c h p u ts th e new ly form ed branch on the top o f the branches w ith the sam e

26
p ro b ab ility . O n th e o th er side, the bottom approach places the new ly branch at the bottom o f th e 0.25 p robability group. T he process o f m erging the tw o branches w ith the low est p ro b ab ilitie s and then reo rd erin g all branches in th e deseending o rd er according to their p ro b ab ilitie s o f o ccu rren ce continues until last tw o branches are m erged to g eth er p ro d u c in g a root o f the tree w hose probability o f occurrence is equal to
1

.

O n ce w e end up w ith th e co m plete tree (obtaining a root w ith the probability o f o c c u rre n c e eq u als to
1

), all the upper branches o f the tree are labeled w ith a
6

0

and all the

lo w er b ran ch es are labeled w ith a 1. A H uffm an tree in Figure p ro c e d u re ju s t explained:

dem onstrates the

P (S1)=0.4

0.6 0
ROOT

I
tre e sh o w n in F ig u re
6

P(S2)=0.3 P(S3)=0.2 P (S4)=0.1 Figure : A H uffrnan Tree.

6

T he c o d e w ord fo r each sym bol in the source alphabet (leaf) is a sequence o f zero s and o n e s o b tain e d by tra v e lin g the tree in th e direction starting from the root and ending at th e le a f w h o m the c o d e w ord is being form ed for. H ere are the code w ords fo r the H uffm an : S, =1

Sz = 0 0 53 = 0 1 0
54 = 0 1 1

T h e b e st w ay to understand th e H uffm an coding alg o rith m is to go th roughout an entire e x a m p le th a t w ill em p h asize all steps o f the algorithm : th e m erging, the top approach reo rd e rin g , th e entropy, the average length o f co d e w ords, and the encoding efficiency.

EXAMPLE (Huffman Coding Algorithm):
A d i s c r e t e m e m o r y l e s s s o u r c e ( D M S ) h a s a n a l p h a b e t o f e i g h t s y m b o l s w i th t h e f o l l o w i n g p r o b a b i l i t i e s : 0 .2 5 , 0 .2 , 0 .1 5 , 0 .1 2 , 0 .1 . 0 .0 8 , 0 .0 5 , a n d 0 . 0 5

27

a ) D e te r m in e b in a r y c o d e w o r d s f o r th e s o u r c e o u tp u t; b ) D e t e r m i n e t h e e n t r o p y o f th e s o u r c e ; c ) F i n d t h e a v e r a g e c o d e le n g th ; d ) D e te r m in e th e e ffic ie n c y o f th e e n c o d in g ;

S O L U T IO N :

P(S1)=0.25 P(S2)=0.2 P(S3)=0.15 P(S4)=0.12 P(S5)=0.1 P(S6)=0.08 P(S7)=0.05 P(S8)=0.05

0.25

0.25

0.25

0.33 0.25 \0 .2 2 n 0.2

0.42 0.33 0 0.25

0.58 0

1.0
0.2
0.15 0.18 0.15
.
.

0.22
0.2 0.18 0 0.15

0.42

0.12 0.1 0.1 0
0.08

0.12 0

0.1

Sj = 01

82 S3
8 4

=

11
1 0 0

= 0 0 1
=

8 5

=

0

0

0

0

86 = 0 0 0 1 8j = 1 0 1 0
88=1011

h)

H ( X ) = ^ P ( x i ) - I ( x j ) = 0 .2 5 - 2 + 0 .2 - 2 .3 2 + 0 .1 5 - 2 .7 4 + 0 .1 2 - 3 .0 6 + 0 .1 -3 .3 2 +

8

i=l
+ 0 .0 8 - 3 .6 4 + 0 .0 5 ' 4 .3 2 + 0 .0 5 - 4 .3 2 = 2 .7 9 7 9

I

28

c)
8

J

0-

.

' ^ ( x i ) = 2 - 0 . 2 5 + 2 - 0 . 2 + 3 - 0 . 1 5 + 3 - 0 .1 2 + 4 - 0 . 1 + 4 - 0 ,0 8 + 4 - 0 .0 5 + 4 - 0 . 0 5

i=l
= 2 .8 3 d)

7? =

--
n

= ^

7

2.0 3

^

= 0 .9 8 8 6 = 9 8 .8 6 ( % )

3 .1 .2

H u f f m a n D e c o d in g

E arlier, it w as m entioned that it w as absolutely necessary for variable-length code w o rd s to be u n iq u ely and instantaneously decodable. W e w ill use an exam ple to d e m o n stra te w h a t it m eans. B elow are show n three code assignm ents for representing the sy m b o ls
8 2

,8

2

, 8 3 , and S 4 : vm bols S]
8 2

8

Code I
1 0 0 0 1 1

C ode 2
0 0 0 0 1 1 1

C ode 3
0 1 0 1 1 1 0 1 1

S3
8 4

1 1 1

L e t's n o w d eterm in e w hich codes are uniquely and instantaneously decodable and w hich a re n o t.
8

u p p o se w e are presented w ith the follow ing sequence to be decoded:
0 10 0 0 1 1 1 10 0 1

...

U sin g C o d e 1 it is clear that the first tw o sym bols correspond t o 8 3 are not u n iq u ely deco d ab le. T hey can be decoded either as
8 3

8

,. The next tw o bits . O bviously this co d e , 7, to

or

8 3

is n o t un iq u e and th ere fo re is not useful for the purpose o f decoding. The second code, C o d e 2, w ill d eco d e th e first four bits as S 2 S ] S |. The problem begins w ith the bits 5, and
8 6

. W o u ld th e bits 5 and

6 6

correspond to , and 7) as
6 8 3 8

8 2

, o r w e have to wait for the bit 7 and
8

d e c o d e th e seq u en ce (bits 5,

, or w e have to wait even m ore for the bit ) as
8 4

d e c o d e th e new sequence (bits 5, se q u e n c e as

, 7, and

? C ode 2 is not instantaneously

d e c o d a b le and c a n not be used for decoding. N ow , C ode 3 will decode the given
8 , 8 2 8 2 8 2 8 4 8 2

and so on. It is uniquely and instantaneously decodable. N o te

29
th a t no c o d e w ord in th is co d e is a prefix o f any o th e r code w ord. T his is exactly w h at h elp s us d e c o d e a stream o f bits at the receiver side. T h e re are several m ethods to decode sequences previously encoded by th e H uffm an co d in g alg o rith m . F o r all o f these m ethods one th ing is in com m on; the d eco d er m ust h av e so m e kind o f kno w led g e ab o u t the encoding procedure in o rd er to w ork properly. O ne w a y is to send th e list o f all sym bols and their counts to the receiv er so the receiver can re c re a te th e H uffm an tree on its end. Then w hen a sequence to be decoded arrives, o n e bit in th e seq u en ce is read at a tim e and the H uffm an tree is traveled from the root d o w n u n til a le a f is reached. Let s apply this m ethod using the tree from Figure h a s a rriv ed :
6

. We

a ssu m e th a t th e tre e is reco n stru cted at the d e c o d er's end and that the follow ing sequence

0 1 0 0 0 0 1 1 1 0 1 0 ...

T h e first bit, 0, is read and the tree will be traveled in the upw ard direction. Since the n o d e w e have arrived at is not a lea f node, w e go on and take the next bit, 1. W e now tra v e l th e tree in th e dow nw ard direction. O nce again, we check if the reached node is a le a f n o d e , ft is n o t, so w e take the next bit, 0, and travel upw ards. T his tim e we have reach ed a le a f node, the leaf S 3 . W e decode the first three bits as the sym bol S 3 and go b a c k to th e root o f the tree to start decoding the next sym bol. Sim ilarly, w e take the next 2 bits (o n e at a tim e) and reach the leaf S 2 . The process continues until w e run out o f bits in th e a rriv in g sequence. A n o th e r m eth o d is to store the code w ords for each sym bol in different structures (tables) a c c o rd in g to th e length o f the code w ords. In other w ords, all code w ords that are 5 bits lo n g w ill be sto red in the table 5, for exam ple. T hose w ith the length o f 7 will be stored in th e ta b le 7 and so on. W e also create a so-called general table w hich contains the code w o rd s w ith the length o f 1, 2, 3, and 4 bits (since there are not too m any o f them ). All th e se ta b le s are then sent to the decoder. N ow , when a stream o f bits arrives at the d e c o d e r, e a ch bit in the stream is read at a tim e and the cum ulative subsequence (could be fo rm ed fro m
1

o r m ore bits) is search for in the appropriate table. If a co rresponding

sy m b o l is found, w e have the decoded sym bol. If not the next bit is appended to the su b se q u e n c e . T his m ethod is used in this project. T h e m o st in te restin g d eco d in g m ethod (but also the m ost tim e consum ing m ethod) is soc alled th e a d a p tiv e H uffm an tree decoding. In this m ethod w e do not have to have the k n o w le d g e a b o u t the probabilities o f occurrence o f the sym bols. All we have to know is

30
h o w m a n y sy m b o ls w e could have in the source alphabet. That inform ation is shared w ith th e d e c o d er. T h a t is all th a t has to be transm itted to the decoder. Thus, instead o f sending th e list o f all sy m b o ls and their counts or the tables w ith the code w ords, all w e send is a sim p le n u m b e r th a t rep resen ts how m any sym bols the source has in the alphabet. The n ex t s te p is th a t both th e tran sm itter and the receiver assign all the sy m b o ls' counts to T h at re su lts in hav in g th e sam e probabilities o f occurrence for all the sym bols. F o r e x am p le, if w e w an t to com press a hum an speech we will need an -bit quantizer
1

.

8

g e n e ra tin g 256 p o ssib le output levels. So, the num ber o f sym bols in the source alphabet is 2 5 6 . T h is is sent to the receiver and both the transm itter and receiver set the counts for each o f 2 5 6 sym bols to 1 th at generates the sam e probability o f occurrence for each sy m b o l (1 /2 5 6 = 0 .003906). N ow , the code w ords are generated using the H uffm an a lg o rith m on b oth ends. F or exam ple, w hen a sam ple that is quantized to the level 56 is to be tra n sm itte d , th e tra n sm itte r firstly, sends the code word that corresponds to the sym bol 56 to th e receiv er, secondly, increm ents by one the count o f the sym bol 56 w hich a u to m a tic a lly increases the probability o f occurrence o f the sym bol 56, and thirdly, re g e n e ra te th e new co d e w o rd s using H uffm an algorithm for all the sym bols according to th eir new p ro b a b ilitie s o f occurrence. A t this point the transm itter is ready to process a new q u a n tiz ed sam ple. T h e rec e iv e r, on the o th er end, firstly, decodes the received code w ord (to 56) by c o m p a rin g it w ith th e existing code w ords at the receiver, secondly, increm ents by one th e c o u n t o f th e sym bol 56, and thirdly, re-generates the new code w ords using H uffm an a lg o rith m fo r all th e sym bols. A fter this the receiver is ready to accept the next code w o rd . W e see th a t d u ring the pro ced u re o f encoding and decoding the code w ords are constantly ch a n g in g an d th a t is w hy th is m ethod is called the adaptive o r dynam ic H uffm an coding. T h e o rd in a ry H uffm an discussed in the m ost o f this ch ap ter is in som e books called the static H u ffm an coding. 3 .1 .3 M a tla b Im p lem en tation o f H uffm an C od ing A lgorithm T h re e M atlab files im plem ent the H uffm an C oding A lgorithm . The M atlab file " Im a g e H u ffm a n .m " is th e m ain program th at contains calls for the routines called at v a rio u s tim e s d u rin g the execution o f the program . It perform s the m erging o f tw o leaves w ith lo w est p ro b ab ilities o f occurrences form ing a new branch w hose w eight is equal to

31
th e c o m p o site probabilities o f the leaves. It also calculates the entropy, the average length c o d e , and e fficie n cy o f the encoding.

T h e file " M ak eS tru ctH u ff.m ' is the M atlab function that creates a cell `H u f f w ith the fo llo w in g stru c tu re elem ents; probability that keeps the values o f probability for each so u rc e sym bol, `c o d e w ' w hich keeps the binary ones and zeros obtained at various stages o f th e H u ffm a n tre e process. A t the end o f the process the structure `c o d e w ' contains the c o d e w o rd s for all th e source sym bols, 'sy m b o ls' containing the rearranged original p o sitio n o f e a ch lette r assigned to each source letter at the beginning o f the H uffm an tree p ro ce d u re ;

T h e file " S o rtin g H u ff.m " is the M atlab function that keeps the o rd er in the H uffm an tre e . It m ak es su re th at at the beginning o f the execution o f the algorithm , the p ro b a b ilitie s o f the source sym bols are arranged in a descending order. A lso, this routine is c alled ev e ry tim e w hen a com pound branch is form ed in the tree (from the th a t h a v e th e low est probabilities o f occurrence); T h e last file " D e c o d er_ lm ag e_ H u ffm an .m " is th e M atlab file that decodes a sequence o f c o d e w o rd s using the tables containing all possible code w ords. T hese tables are created a c c o rd in g to th e lengths o f the code w ords. They m ust be sent to the receiver before the p ro c e ss o f d ec o d in g m ay begin. H e re is how th e co d e w orks: A t the beginning, the " M akeS tructH uff.m " routine is called to c re a te a cell (stru ctu re) in o rd er to allocate the necessary m em ory space for holding the p ro b a b ilitie s o f all th e source sym bols, their original positions in the source alphabet, and a
2 2

branches

-d im e n tio n al m atrix that keeps the binary ones and zeros assigned to each source

sy m b o l w h ile th e code travels throughout the H uffm an tree. O nce the cell is created, the m ain p ro g ra m calls the " Sorting_H uff.m " routine that check the o rd er o f the probabilities all th e e le m e n ts in th e `H u ff.probability' structure. I f necessary, it rearranges the existing o rd e r o f th e p ro b ab ilities to obey the descending o rd er requirem ent. N ow , the main p ro g ram ta k e s from the `H uff.probability' the tw o sym bols w ith the low est probabilities and c re a te s th e ir com pound probability. T he process repeats itself until only one co m p o u n d p ro b ab ility is left (equal to 1). The decoding procedure is straight forw ard and is e x p la in e d into details in the previous section o f this chapter. A t th e end o f th e H uffm an code algorithm discussion, let's take a look at the M atlab print o u t to th e H u ffm a n algorithm program . The sam e input sequence o f the source sym bols

32
used in the ea rlie r exam ple w ill be used as the input array: [0.2500 0 .1 0 0 0 0 .0 8 0 0 0.0500 0.0500] 0.1500

0 . 2 0 0 0

0

1 2 0 0

»

H u f f C o d e A Ig o r ith m

S I =01 82= 11 S3= 001 S4= 100 85= 0000

86=0001
87= 1010

88=1011
A v e r a g e C o d e L e n g t h ( u s i n g m y c o d e ) : 2 .8 3 E n tr o p y o f th e S o u r c e H (x )= 2 .7 9 7 9 E f f i c i e n c y = 9 8 .8 6 5 % H u ff = p r o b a b i l i t y : [ 0 . 2 5 0 0 0 .2 0 0 0 0 .1 5 0 0 0 .1 2 0 0 0 . 1 0 0 0 0 .0 8 0 0 0 .0 5 0 0 0 . 0 5 0 0 ] c o d e w : [ 8x 8 d o u b l e ] s y m b o l s : [ 8x 8 d o u b l e ] so r te d H u f f = p r o b a b ility : s y m b o ls :

1.0000 [ 8x 1 d o u b l e ]

It can be easily observed that the M atlab code has produced the sam e ou tp u t o f c o d e w o rd s as the exam ple dem onstrated earlier. The entropy o f the source, the average c o d e length , as w ell as the efficiency m atch those obtained in the exam ple.

3.2 Lempel-Ziv-Welch Algorithm
U n lik e the static H uffm an coding w here we m ust know the probabilities o f o c c u rre n c e for all source sym bols, the L em pel-Z iv-W elch com pression algorithm does n o t req u ire any p rio r know ledge o f occurrence o f source sym bols. This is sim ilar to w hat g o e s on in th e real w orld, w here the statistics o f sources are very often unknow n. D o e s th is m ean th a t w e can not design a H uffm an code fo r a source for w hich w e do not k n o w th e statistics o f o ccurrence o f the source sym bols? T heoretically the an sw er is a yes

33
b ut p rac tic a lly th e a n sw er w ould be a no (adaptive H uffm an coding is extrem ely tim e c o n su m in g ). F u rth e rm o re , theoretically it can be done by estim ating the probability o f so u rc e sy m b o ls th ro u g h observing a long inform ation sequence em itted by th e source and th en e m p iric a lly o b tain in g the probabilities o f all the source sym bols. H ow ever, this w o u ld be v ery im practical since the process o f em pirically obtaining th o se probabilities w o u ld b e c o m p u ta tio n a lly very com plex. N ot to even m ention the com plexity o f H u ffm a n c o d in g alg o rith m being applied onto sources w ith m em ories. T hus, the a p p lic atio n o f the H uffm an code algorithm for m any real sources is generally considered im p ra ctic a l and a different m ethod o f source encoding is needed. This is w here the L e m p e l-Z iv -W e lc h alg o rith m com es into the picture. T he L e m p e l-Z iv -W e lc h algorithm for the source encoding is designed to be com pletely in d e p e n d e n t o f th e probabilities o f occurrence o f source sym bols. Due to this fact the a lg o rith m is placed in the class o f algorithm s called the universal source coding a lg o rith m s. T he alg o rith m w o rk s in a follow ing m anner; the output sequence from the so u rc e is parsed into v ariable-length blocks called subsequences. A new subsequence is c re a te d ev e ry tim e w hen a sequence o f bits com es out o f th e sources and has not been in tro d u c ed in any p rev io u sly defined subsequence. The subsequences are then listed in a L e m p e l-Z iv -W e lc h tab le in the order o f appearance. B esides having a colum n in w hich all su b se q u e n c e s are listed, th e subsequence colum n, the L em pel-Z iv-W elch table c o n ta in s th re e o th er colum ns: the m em ory location colum n, the m em ory representation c o lu m n , and th e co d e w ord colum n. T he m em o ry location co lu m n contains entries that are binary representation o f the d e c im al n u m b ers listed in ascending order starting from
1

and representing the

a p p e a ra n c e o f th e d ifferen t subsequences. As we have earlier said the subsequence c o lu m n c o n ta in s th e subsequences w here the later subsequences take low er positions in th e su b se q u e n c e colum n. T he fields (positions) in the m em ory location colum n and su b se q u e n c e colum n are m utually related in a sense that every new subsequence in tro d u ced has th e m em ory location num ber associated with it being greater by
1

than the

m em o ry lo catio n n u m b er o f the last know n subsequence. T he very first obtained su b se q u e n c e o ccu p ies the third position in the subsequence colum n (having the m em ory lo ca tio n o f 3) since th e positions (m em ory locations) I and 2 are reserved for su b se q u e n c e s 0 and I th at are usually assum ed even before the source starts em itting bits. T h e last bit o f each su b seq u en ce entered in the subsequence colum n is called the in n o v a tio n bit and is im portant for creating the code w ords that will be placed in the code w o rd co lu m n . B efo re considering how the code w ord colum n is filled, let s first consider h o w w e o b tain th e en tries for the m em ory representation. In order to do that, w e will

34
assu m e th a t th e source has ju st started em itting a stream o f binary ones and zeros. The a ssu m p tio n g oes on assum ing that the very first tw o bits em itted are a I and 0, creating a new su b seq u e n c e (recall that a assig n ed the m em ory location is th e bit
0

and

1

are considered as the first tw o subsequences

o c c u p y in g the first tw o positions in the subsequence table). O ur new subsequence 10 is
0 1 1

(binary representation for the decim al num ber 3 ) and

resp e c tiv e ly the position 3 in the subsequence colum n. The last bit o f the subsequence 10
0

and is co n sid ered , as w e have previously said, to be the innovation bit.

T o o b tain th e m em ory representation for the subsequence 10, we divide the subsequence in tw o groups. T he first group contains n - 1 bits (w here n is the total num ber o f bits in the su b seq u e n c e ), w hile th e second group contains only one bit, the innovation bit. In our case th e first g ro u p form s bit
1

(since n =

2

), w hile the innovation bit,

0

, form s the second

g ro u p . A t this p o in t w e will also m ention that the first tw o subsequences 0 and 1 do not u su a lly have th eir respective m em ory representations in the m em ory representation co lum n. N o w , w e tak e the first group, the bit 1 and look for its exact replica am ong p re v io u sly in tro d u ced subsequences. This m eans that w e w ould look for subsequence 1 a m o n g th e m - 1 previous entries in the subsequence colum n, w here m is the position in th e m e m o ry rep resen tatio n table that the current subsequence holds (in our case m = 3). So, w e w ill look for th e subsequence 1 in the positions 1 and 2 in the subsequence co lu m n . W e find it at the position 2 and that is the first num ber o f the m em ory rep re sen ta tio n o f the subsequence 10. We repeat the sam e procedure for the innovation bit and find it at th e position 1. T hat represents the second num ber o f the m em ory re p re se n ta tio n fo r th e subsequence 10. N ow , those tw o num bers are concatenated fo rm in g th e n u m b er su b se q u e n c e
1 0 21

and this is the num ber being the m em ory representation o f the

.

T h e last colum n in the L em pel-Z iv-W elch table is the code w ord colum n. A code w ord re p re se n ta tio n for a sequence is obtained from its m em ory representation along w ith the v alu e o f its innovation bit. To dem onstrate this, w e will still stick with o u r subsequence 10 w h o se innovation bit is 0 and its m em ory representation is 21. T he code w ord is o b tain e d on the follow ing w ay; the m em ory representation entry
21

is divided into tw o

g ro u p s. T he first g ro u p is form ed from k-1 num bers, w here k is the total num ber o f n u m b e rs in the m em ory representation entry. In our case k = 2 and the first group c o n ta in s the n u m b er 2. T he second group w ould contain the last num ber o f the m em ory re p re se n ta tio n e n try w hich w ould be I . N ow , the co d e w ord for the subsequence 10 w ould co n sist from the binary representation o f the num ber(s) in the first group, in this case
2

, and th e innovation bit that w ould be appended to it. It is here im portant to

m en tio n th a t w e have to know in advance how m any bits the binary representation o f the

35

first g ro u p w ould o ccu p y . W e w ill assum e that in o u r exam ple to represent the binary re p re se n ta tio n o f th e first group, w e w ould use 3 bits. So, th e num ber 2 w ould be c o n v e rted to 010. A p p en d in g the innovation bit to il w e w ould obtain the code w ord 0100 fo r th e seq u en ce 10. W e see that our code word is consisted from 4 bits. E v e n th o u g h w e have ju s t presented how the m em ory location, subsequence, m em ory rep re sen ta tio n , and c o d e w ord entries are obtained for one subsequence, the
10

su b se q u e n c e , a co m p lete exam ple o f L em pel-Z iv-W elch algorithm follow s as it w ould be b e n e ficia l to a rea d e r to fully understand the process o f obtaining subsequences from a stre a m o f ones and z e ro s em itted by the source. It w ould nicely round up the discussion o n th e L e m p el-Z iv -W elc h algorithm and provide us w ith the firm foundation for the M a tla b im p lem e n ta tio n o f the algorithm .

EXAMPLE (Lempel-Ziv-Welch Algorithm):
O b t a i n c o d e w o r d s f o r t h e f o l l o w i n g s t r e a m o f b i n a i y n u m b e r s e m i t t e d b y th e s o u r c e : 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 . E a c h c o d e w o r d s h o u l d b e g i v e n in a 5 - b i t f o r m a t .

S O L U T IO N :

S tep l: S u b s e q u e n c e s S t o r e d (S S ):

0, 1
D a ta to b e P a r s e d (D P ):

1 0 1 0 1 1 0 1 0 0 1001110101000011001
S te p _ 2 : S u b s e q u e n c e s S to r e d (S S ):

0, 1, 10
D a ta to b e P a r s e d (D P ):

101101001001110101000011001
S te p _ 3 : S u b s e q u e n c e s S to r e d (S S ):

0, 1, 10 , 101
D a ta to b e P a r s e d (D P ):

101001001110101000011001
S te p _ 4 : S u b s e q u e n c e s S to r e d (S S ):

0, 1, 10 , 101 , 1010
D a ta to b e P a r s e d (D P ):

01001110101000011001

36
S te p _ 5 : S u b s e q u e n c e s S t o r e d ( S S ): 0, 1, 1 0 , I C I , l O I O , 0 1 D a t a to b e P a r s e d ( D P ) :

001110101000011001
Step ô : S u b s e q u e n c e s S t o r e d ( S S ):

0. 1, 10 , 101 , 1010 , 01 , 00
D a t a to b e P a r s e d ( D P ) :

1110101000011001
Step ? : S u b s e q u e n c e s S t o r e d ( S S ):

0 , 1, 10 , 101 , 1010 , 01 , 00 , 11
D a t a to b e P a r s e d ( D P ) :

10101000011001
S te p

8:
S u b s e q u e n c e s S to r e d (S S ):

0, 1, 10 , 101 , 1010 , 01 , 00 , 11 , 10101
D a t a to b e P a r s e d (D P ):

000011001
S te p _ 9 : S u b s e q u e n c e s S t o r e d ( S S ):

0, 1, 10 , 101 , 1010 , 01 , 00 , 11 , 10101 , 000
D a ta to b e P a r s e d (D P ):

011001
S te p 10: S u b s e q u e n c e s S to r e d (S S ): 0, 1, 1 0 , 1 0 1 , 1 0 1 0 , 01, 0 0 , 1 1 . 1 0 1 0 1 , 0 0 0 , O i l D a t a to b e P a r s e d ( D P ) :

001
S te p _ ll: S u b s e q u e n c e s S to r e d (S S ): 0, 1, 1 0 , 1 0 1 , 1 0 1 0 , 01, 00, 11, 1 0 1 0 1 , 0 0 0 , O i l , 0 0 1 D a ta to b e P a r s e d (D P ):

M EM ORY L O C A T IO N SU BSEQ U EN C ES

M EM ORY R E P R E S E N T A T IO N
-

CODE W ORD

1 2
3 4 5

6
7

0 1 10 101 1010 01 00

21
32 41

12 11

00000 00001 00100 00111 01000 00011 00010

37

8
9

n

22
52 71 62 72

10 11 12

10101 000
O il

00101 01011
O H IO

001

01101 01111

It should be n o ticed th a t the L em pel-Z iv-W elch table show n above has encoded th e o rig in al 29 bit sequence into 12 code w ords o f 5 bits each, resulting in 60 coded bits. O n e m ig h t ask w h a t is th e point in sending 60 bits through the channel instead o f sending o n ly 29 b its? Is it not the purpose o f the source coding to rem ove any redundant bit from th e seq u en ce th a t is to be transm itted? Should not the source encoding provide som e kind o f c o m p re ssio n , and in this exam ple w e have ju st seen that the algorithm provided no d a ta c o m p re ssio n at all? A s a m atter o f fact in the latest exam ple the algorithm has in tro d u c ed an ex p an sio n o f the data. The answ er to all these questions is very sim ple; the in effic ie n c y o f th e L em pel-Z iv-W elch algorithm is due to the fact that the sequence o f b its w e have used in o u r exam ple is very short. As the sequence is containing m ore bits to be tra n s m itte d , th e L em pel-Z iv-W elch encoding procedure becom es m ore efficient and resu lts in a co m p ressed sequence that is m uch shorter than the original sequence, p ro v id in g a sig n ific a n t data com pression. T h e L e m p el-Z iv -W elc h algorithm is w idely used in the com pression o f the com puter files (e.g. Z IP a p p lic atio n ). M any so-called `co m p ress' and `u n co m p ress' utilities under the U N IX and M ic ro so ft o p erating system s are nothing m ore than the im plem entations o f v a rio u s v e rsio n s o f this algorithm . D e c o d in g o f c o d e w o rd s th at have been encoded by using L em pel-Z iv-W elch algorithm is stra ig h t fo rw ard . T he tran sm itter m ust provide the table to the receiver and then each b it in th e stre a m is read at a tim e and the cum ulative subsequence (could be form ed from
1

o r m ore bits) is search for in the table. If a corresponding sym bol is found, we have the

d ec o d ed sym bol. I f n o t the next bit is appended to the subsequence. 3.2 .1 M a tla b Im p lem en tation o f L em p el-Z iv-W elch A lgorithm S ix M atlab files im plem ent the L em pel-Z iv-W elch algorithm . The M atlab file " L Z W _ E n c o d e .m " is the m ain program that contains calls for the routines called at v a rio u s tim e s d u rin g th e execution o f the program . The file S ubsequence.m M a tla b fu n ctio n th a t is doing the parsing o f the " D ata-to-be-P arsed tab le: th e m em o ry location colum n and the subsequence colum n; is the sequences into

v a ria b le -le n g th su b seq u en ces. It creates the first tw o colum ns in the L em pel-Z iv-W elch

n'r

R Y E B SG Nü6W imiTY U B R A H Y

38

The file C heck_up.m fro m th e

is the Matlab function that checks if a chosen subsequence parsed sequence has already been created in the L em pel-Z iv-

D ata -to -b e -P a rse d

W elch table. I f it has, th e subsequence is rejected from being entered into the table. S u b se q u e n ce .m file w ill chose a new subsequence that will be form ed from the rejected s u b seq u e n c e plus the n ex t allow able bit from the "`D ata-to-be-P arsed" sequence. The new s u b seq u e n c e w ill be checked against the existing subsequences by the "C heck up.m " ro u tin e;

T h e file "T a b le .m " is th e M atlab function that com pletes the rest o f the L em pel-Z ivW elch tab le c reatin g th e m em ory representation colum n and the code w ord colum n. The fu n c tio n "C o n v e rt2 b in a ry .m ' converts the m em ory representation entry (m inus the in n o v atio n p a rt) into its binary representation. T h e M a tla b function "T a ilin g .m " checks if the tail o f the original sequence contains the bits th a t are left o u t and can not form a new subsequence (those bits are already part o f th e p rev io u sly form ed subsequence). The routine also creates an array o f com pressed data th a t is to be tra n sm itte d . T he decoding process is the part o f digital to analog conversion p e rfo rm e d in th e "D to A .m " M atlab file. In g e n e ra l the co d e w o rk s in the follow ing m anner; the main program calls the " S u b se q u e n t.m " routine th at is responsible for parsing the " D ata-to-be-P arsed" sequences into v a ria b le -le n g th subsequences. The first tw o colum ns in the L em pel-Z iv-W elch table are c re a te d as w ell. T he m ain program then calls the "T able.m " routine that com pletes the rest o f th e L em p el-Z iv -W elch table. A t this point the Lem pel-Z iv-W elch table is c o m p le te c o n ta in in g all four colum ns: the m em ory location colum n, the subsequence c o lu m n , th e m em o ry representation colum n, and the code word colum n. At the end, the "T a ilin g .m " ro u tin e creates an array o f com pressed data containing the binary re p re se n ta tio n o f the subsequences obtained through the process o f parsing. A t th e end o f this section, let's tak e a look at the output o f the M atlab code that im p le m e n ts the L em p el-Z iv -W elch algorithm . T he sam e input sequence used in the e x a m p le a b o v e w ill be used as the input array in the code.

> > L e m p e lZ iv W e lc h

39
/ 1]
[ [ [ [ 1x

2]

0] 1] 2
d o u b le ]

0
[] [ 2] [3] [4] [ 1] [ 1] [ 2] [5 ] [7] [ 6] [7]

[ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ] [ 1 x 5 d o u b le ]

[ 3] [ 4 ] [ 5] [ [ [

[ 1 x 3 d o u b le ] [ 1 x 4 d o u b le ] [ 1x

6]
7] 8]

2 [ 1x 2 [ 1x 2

d o u b le ] d o u b le ] d o u b le ]

[ 9] [ 10] [ 11] [ 12] » c { : , 2]

[ 1 x 5 d o u b le ] [ 1 x 3 d o u b le ] [ 1 x 3 d o u b le ] [ 1 x 3 d o u b le ]

ans =

0
I

1 1 1 0 0 1 1 0 0 0
» ans c { : ,A 4} ]

0 0 0 1 0 1 0 0 1 0

1 1

0

1 0 1 1

0

1

0 0 0 0 0 0 0 0 0 0 0 0

0 0 0 0 1 0 0 0 1 1 1 1

0 0 1 1 0 0 0 1 0 1 1 1

0 0 0 1 0 1 1 0 1 1 0 1

0 1 0 1 0 1 0 1 1 0 1 1

It can be seen that the c are identical to their respective columns obtained in the earlier exam ple . The third colum n in the c e ll c contains only the first number found in the memory representation

40

co lu m n in th e ex am p le; th e innovation bit part is om itted. T he reason for that is that the in n o v atio n bit is n o t needed to be in the third colum n in ord er to create the fourth colum n. It is faste r to co n v ert th e th ird colum n to its binary representation w ith o u t considering the in n o v atio n bit and then ju s t add the innovation bit w hen form ing the code w ord. The sam e M a tla b file as being used for the m ain sim ulation is used. The only change is that fo r th e ex a m p le th e input sequence is 29 bit stream given in the exam ple.

41

Chapter 4: CHANNEL CODEC
4.1 Channel Codes
4 .1 .1 P a r it y C h e c k C o d es

T h e d isc u ssio n on channel codes w ill begin w ith the introduction o f parity check co d es. O n ly th e b in ary code w ords o f inform ation w ill be considered m eaning that any c o d e w o rd c o n sists o f only tw o digits as th e elem ents o f the binary alphabet: a T h o se c o d e w o rd s are called binary code w ords. P arity check codes involve the arith m e tic o p e ra tio n s o f both addition and m ultiplication perform ed on binary code w o rd s. Since th e binary alphabet is used to represent any binary code w ord, the o p e ra tio n s o f ad d itio n and m ultiplication perform ed on binary code w ords are also called th e b in ary a d d itio n and binary m ultiplication. T hese operations are perform ed according to th e c o n v e n tio n s o f the algebraic set S that contains the alp h ab et's sym bols as its elem en ts. T h e set satisfies the follow ing properties associated with the binary addition: 1. 2. 3. 4. 5. if a, b are binary code w ords and a ,b e S, then a + b e S ; if a, b, and c e S, then a + (b + c) = (a + b) + c; if a, b e S, th en a + b = b + a; th e set S contains a zero elem ent that satisfies the condition: a + 0 = a ; ev e ry elem en t in the set S, except zero, has its ow n negative elem ent and their a d d itio n yields the zero elem ent. H ence, if a  S, its negative elem en t is --a and a + ( - a ) =
6 0 1

and a

0

.

;

. th e operation, designated w ith th e sym bol © is defined as:
0 0 0

=

0

0 0 1 = 1

100

=
=

1

101

0

and th e fo llo w in g o nes for the binary m ultiplication: 1. 2. 3. 4. i f a ,b are binary code w ords and a ,b e S, th en a · b e S; i f a, b, and c e S, then a (b · c) = (a · b) c; i f a ,b e S, th e n a · b = b - a; i f a, b, a n d c e S, then (a + b) · c = a · c + b · c;

42
5.
6

ev e ry elem en t in the set has its ow n `id en tity ' elem ent ` 1 w here a l = a ;

. ev e ry elem en t in the set, except zero, has its ` inverse', w here a a~* = 1 ; 7. th e operation is defined as: x x x =

0 0 1 1

0

0

1 = 0 0

=

0 1

x1 =

P a rity c h e c k c o d e s are types o f codes that use the sum o f all bits present in inform ation, o r in a b lo ck o f inform ation, to detect errors inflicted in the inform ation as a result o f ch an n el im p erfectio n s. W e differentiate tw o types o f parity check codes: the single parity c h e ck c o d e and th e rectan g u lar code. In th e sin g le p arity check code all bits in a codew ord are sum m ed up and a resulting bit, th e p a rity bit, is concatenated at the beginning o f the code w ord, as show n in Figure 7. T he p a rity bit could be either a 1, sym bolizing th a t the sum m ation o f all the bits in the c o d e w ord y ield s an odd result, o r a 0 , sym bolizing that the sum m ation o f all the bits in th e c o d e w o rd y ield s an even result. If the added parity bit is designed to yield an even result, th e m ethod is called the even parity m ethod. Sim ilarly, if it yields an odd result, it is called the o d d parity m ethod. C ode w ord: 0 N ew co d e w ord: 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1; 1;

Parity bit: 0 + 1 + 0 4 - 1 + 0 + 1 + 0 + 1 = 0 ;

Figure 7: Single P arity C heck Code. In th e rec tan g u la r code, a rectangle o f code w o rd s is form ed containing I row s and J c o lu m n s. N o w , fo r each o f 1 row s, a horizontal parity bit is appended at the beginning o f e a ch row . A lso, fo r each o f J colum ns, including the very first colum n representing the h o riz o n tal p a rity bits, a vertical parity bit is appended at the beginning o f each colum n, c re a tin g a new block o f the (1 + 1) x (J +1) dim ensions. L e t's n o w ta k e a look at the erro r detection capabilities o f these tw o codes. T he single p a rity ch e ck c o d e can only d etect errors if the odd num ber o f bits in a code w ord is w ro n g ly receiv ed . I f th e num ber o f corrupted bits in a code w ord is even, the code will n o t be ab le to d etect it since the parity bit will rem ain the sam e. Even though the single

43
p a rity co d e can d etect the odd num ber o f corrupted bits, it cannot correct them since the c o d e h a s no k n o w led g e w here in the code w ord the eorrupted bit(s) is(are) located. O n th e o th e r hand, unlike the single parity code the rectangular code can correct a single e rro r a n y w h e re in th e block (rectangular) o f code w ords since every bit in the I x J block is u n iq u e ly lo cated at the intersection o f the horizontal and vertical parity bits.

Hoiizoutal Paiitv Bits

0 0 0 i 0 0 1 P4 0 1 0 0 1 1 1 I > £ 0 1 0
m

0

1 i

0

I

1 0 1 0 1 0 0 0 0 0 0 1

Figure

8

: R ectangular Parity C ode (1 = 4, J =

6

).

F o r e x a m p le, if in th e block show n in Figure c o lu m n w e re received as a
1

8

, the bit in the second row and third
1

, then the horizontal parity bit for the second ro w w ould be

in ste a d o f 0, w hile the vertical parity bit for the third row w ould change from 0 to I. This w a y th e location o f th e corrupted bit is easily identified according to the locations o f h o riz o n ta l and v ertical parity bits. R ectangular codes are defined by tw o param eters: k and n, w h ere k = I · J and n = (I + 1) · (J +1 ) . It is a com m on practice that rectangular c o d e s are depicted w ith the (n, k) notation. For the case in Figure sh o w n rec tan g u la r code is o f type (35, 24). 4 .1 .2 L i n e a r B lo c k C odes L in e ar block codes are, as m entioned earlier, characterized by the (n, k) notation. T h e p u rp o se o f the code is to encode a block o f k-bit code w ords into a bigger block o f c o d e w o rd s w here each code w ord contains n bits. In o ther w ords, the encoding p ro c e d u re assigns to each o f the
2 8

, w e say that the

^ possible k-bit code w ords a unique n-bit code w ord n-bit code w ords. The only
2

c h o se n from the set o f 2"^ possible code words. A block code represents a one to one a s sig n m e n t w h ere 2 ^ k-bit code w ords are m apped to 2 p ro b le m here is how to select a subset o f
2

^ n-bit co d e w ords from the set o f

p o s s ib le n-b it co d e w ords. To answ er this question w e first introduce tw o term s, a vector sp a ce a n d v e c to r subspace. A vector space Vp is the set o f all n-bit code w ords defined

44
o v e r th e binary field o f tw o elements: 0 and 1. A subset called a subspace if the follow ing two conditions are met:
1 2

o f the vector space V,, is

. the subset

contains the all zero vector,

(28a) (28b)

. i f c o d e w o rd s C i , C j  V n then Ck e V n , where Ck = C j ® C j .

T h e second condition m eans that the sum o f any tw o code w ords that belong to the vector space V]^ m ust also belong to the vector space . This condition is fundam ental for the e x iste n ce o f linear block codes. Once we have defined the vector space and vector subspace, let's now establish the goal that a chosen block code has to achieve. The se lected block code should have as large as possible vector space (containing as m a n y c o d e w o rd s as possible in it). Then, w e should select code words that will form the su b sp ac e S^, in th e w ay that the selected code w ords are as far apart from each other in the su b sp ac e as possible. N ow , we have to define the distance from two code words in a su b sp ac e in o rd e r to select those that are as far apart as possible. The H um m ing distance is the m ea su re o f distance between tw o code words. The H um m ing distance is defined to be the n u m b e r o f elem ents in which two code words differ. W hy is it so important that tw o w o rd s are as far apart as possible? L e t 's a ssu m e that on the receiver side a received code word has 2 corrupted bits. If the corru p ted received code word is the same as another code word from the subspace, there w o u ld be no w a y that the corrupted bits in the received word are detected as erroneous and po ssib ly corrected. The decoder would simply consider the received code word as a valid c o d e w ord from the designated subspace set and no correction will be performed. E v e ry th in g looks O.K. but the problem is that the decoded symbol is not the one that had been tra n sm itte d . This is exactly w hy it is desirable that code words in a subspace have as m a n y d iffe re n t bits am ong them selves as possible in order to prevent the above situation. H o w e v e r, cho o sin g code words from a subspace that are as far apart as possible is not an e asy j o b by any m eans. A lot o f researches have been done on the issue o f finding code words from a subspace that are as far apart as possible but so far no acceptable algorithm has been developed that w o u ld h e lp us select the best set o f code words for a subspace. In this project we will follow th e recom m endation from the B. Sclar's book " Digital Com m unications , where it is s u g g e ste d that for a particular subspace to be considered as valid one, it is sufficient that th e c o d e w o rd s from the subspace satisfy the conditions given by the equations (28a) and (2 8 b ). In o ther w ords, the subspace contains the all zero element and the sum o f any tw o c o d e w o rd s in the subspace is another code word that also belongs to the subspace.

45
L e t s n o w consider a block code with n = 7 and k = 4 , a (7,4) block code. The code has th e v e c to r space Vy containing th at need to have 2
2

= 1 2 8 distinct code words, and the vector subspace S 4

= 1 6 code words. We can choose any 16 code w ords out o f 128

possible c o d e w o rd s from the vector space as long as they satisfy the conditions (28a) and (28b). H ere are, for instance, four code w ords that satisfy both (28a) and (28b) conditions and c a n be a p art o f any S 4 subspace:

0000000
1 0 1 0 1 0

1

0 10 10 10
1 1 1 1 1 1 1

1 0 1 0 1 0 1
+ 0 1 0 1 0 1 0 +

0 1 0 1 0 1 0
1 1 1 1 1 1 1 +

1111111
1 0 1 0 1 0 1 +

1111111
0 1 0 1 0 1 0

1 1 1 1 1 1 1

101 0101

0 1 0 1 0 1 0

1010101

W e see that th e selected four code words contain the all zero code word and that the sum o f an y tw o code w ords, am ong those four code words, produces a code word that is one o f th o s e four code w ords. Addition between the all zero code word with the remaining th re e c o d e w o rd s is om itted as obvious. One o f the ways to encode a m essage with a (7,4) b lo c k c o d e is to use a look up implementation and for each m essage look up in the table an asso c ia te c o d e word. But would that work if we, for example, had k = 92 , as it is the case in the real w orld. Certainly not, since we would need a table with 4.95 · 1 0 ^^ entrances. T im e wise, it w ould be totally unacceptable, or to be m ore realistic, it would be im possible. 4 .1 .3 L in e a r B lo c k C od e E ncoding Luckily, it is possible to obtain a set o f n-bit code w ords that will generate all 2*^ v e c to rs o f the subspace. In other words, any o f 2 ^ possible n-bit code words can be ge n e ra te d by using the following matrix equation: U = m · G = m j · Gi + my · G 2 H r m j^ -G k (29)

w h e re m = [m, m y · · ·

] is a vector that contains all m essage sym bols that are to be

tra nsm itted, and G is a `generator m atrix' with the dim ensions (k x n ) .

46
G i' G =
Gz

gn 8 2 1

8 1 2 8 2 2

"· ·"

8ln
8 2

n

(30)

Gk

_8kl

Sk2

···

8kn.

In (3 0 ) th e v e c to rs G i, G i ,

G k ^re linearly independent vectors that can generate all

c o d e w o rd s. I f th e generated code w ords contain the original m essages that are parts o f the c o d e w o rd s, then w e say that the code is `system atic' and the generator matrix is o f the fo llo w in g form:

G =

Ik

(31)

w h e re th e m a trix P is a parity matrix o f the ( k x (n - k)) dimensions, while Ik is an identity m atrix, the dim e n sio n s (k x k ) , with ones on the main diagonal and zeros e lse w h e re . T h is sim plifies the process o f encoding significantly since now we only have to find a m a trix P in o rd e r to obtain a generator matrix G . Once the generator m atrix G is k n o w n w e c a n easily e n c o d e any message by using the equation (29). T o s u m up th e process o f e ncoding o f a message w ith a (n, k) block code: we first create a s u b s p a c e o f 2 ^ n-bit code w ords. A m ong those code w ords we select k o f them in o rd e r to g e n e ra te a g e n e ra to r m atrix G , in such a w ay that our generator matrix is created fro m a p a rity m atrix P and an identity matrix I k . N ow , once we obtain the generator m a trix w e e n c o d e any m essage symbol by simply applying the equation (29). The process is illustrated in a sim plified flow chart shown below: (32)

4 .1 .4 L in e a r B lo c k C od e D ecoding W h e n it c o m e s to decoding o f a received code word, one should think that the p ro c e s s o f d e c o d in g w ould ju s t be an opposite process from the process o f encoding. By lo o k in g at the linear block c o d e encoding flow chart in (32), it would be natural to think th at by m u ltip ly in g the received code word with the inverse generator matrix, we would be a ble to o b tain the transm itted message. The idea sounds O.K. but unfortunately it does n o t w o r k since an inverse generator matrix G does not exist in the sense that it would help us re c o v e r the e n c oded m essage symbol. To overcom e this problem we will keep the

47
initial d e c o d in g idea presented e ^ l i e r but instead o f an inverse generator matrix w e will in tro d u c e a p arity ch e ck m atrix H that indirectly enables us to decode the received code w o rd , w h e re th e parity check m atrix H with its dim ensions ((n --k) x n) is defined as: H = I n -k PT

(33)

W e k n o w that a received c o d e w ord, encoded at the transiver side, is an n-bit vector with d i m e n s io n s
(1

x n ) and that the matrix multiplication betw een the received code word and

th e p a rity c h e c k m atrix H , with dimensions ((n - k ) x n ) , is m athem atically impossible. So, h o w do w e th en d e c o d e the received code w ord? I f w e ta k e a c areful look at the matrix H we can notice th at if w e transpose the m atrix w e ge t the d im e n s io n s th at allow us to perform the m atrix multiplication between the tra n s p o s e d parity ch e ck m atrix and the received code w ord. The transposed parity check m a trix is d e fin e s as:

In-k (34)

C a n w e c o n s id e r th e result o f the matrix multiplication betw een the transposed parity c h e c k m atrix and th e received code word to be the recovered original m essage sym bol? N o , at least n o t yet. W ith the m atrix multiplication b e tw e en the transposed parity check m a trix and th e received code w ord we obtain a (I x n) vector S called a syndrome: S= rH
T

,

(35)

w h e re th e v e c to r r is a received code word. The purpose o f the syndrom e vector is to d e te r m in e if the received code word has been corrupted during the transmission. Here is h o w th e s y n d ro m e vector S achieves it. We will start with the assum ption that the re c e iv e d c o d e w o rd has been corrupted during the transm ission and that it is defined as. r = U + e. (36)

w h e r e th e v e c to r U is one o f 2^ n-bit code words from the subspace, while the vector e is th e e rr o r v e c to r introduced by the channel. From the equation (35) we obtain the s y n d ro m e S o f th e received code word r . I f the sy ndrom e S is an all zero vector, then

48
th e rec e iv e d c o d e w o r d has not been corrupted by the channel. Conversely, if the s y n d ro m e S is not an all z e ro vector, then the received code word has been corrupted by th e c h a n n e l an d the d e c o d e r has to m ake an effort to correct the corrupted code word. B e fo re w e get to th at point, let s first rearrange the equation (35) by replacing the vector r w ith th e v alue from th e equation (36):

S = (U + e)H since T h e e q u a lity G · H U

=U H

+e- H^=eH^, =0.

(37a) (37b)

= m G ·

-- 0 used in (37b) can easily be proven from the equations (31) and

(34). T h e e q u a tio n s (37a) and (37b) tell us that the syndrom e vector S has a one to one c o rr e s p o n d e n c e b e tw e en the error vector e and the corrupted code word r . This is a very p o w e rfu l o b s e rv a tio n th at enables us not only to detect the error but also to correct it. L e t 's c o n s id e r all 2 " possible n-bit code words that can be received at the receiver. R e c a ll th a t th e n u m b er o f valid n-bit code w ords belonging to the subspace is 2^ , and th a t th e rest
2

" -

2

^ n-bit code words represent all possible com binations o f the

c o rru p te d valid c o d e w o rd s having the w num ber o f corrupted bits, where w = l, 2 , . . . , n . I f w e put w h a t has ju st been said into a two dimensional array, we will end up with a tw o d im e n s io n a l a rray as sh o w n in Figure 9.

Ui 62 63

U2 U2 + 6 2 U2 + 63

UI U; + 62 Ui +63

U2" U2" + 6 2 U2" + 6 3

Gj

U2 + 6 j

U i+ e j

U2" + 6 j

62"-"

U2 +62""

U i + 62""

U 2 ` + 6 2 '"

F ig u re 9; D ecoding Table for Linear Block Codes. T h e d e c o d in g table for linear block codes m ust satisfy the following conditions. . th e first r o w c ontains ^ valid code w ords;

1

2

49
2. the first colum n, starting from the second elem ent, contains correctable error patterns. T h e first elem ent in the first colum n contains the first valid code w o rd from the subspace which is an all zero code word; 3. the rest o f the array is filled in such a w ay that each elem ent in the array is obtained as the sum o f the first element o f that colum n (valid code w ord) and th e first elem ent in that row (error pattern);

A t th is point w e have all necessary information in place to detect and correct errors d u rin g a transm ission, except one; what error patterns are going to be considered d e te c ta b le and co rre c ta b le ? To determine that, we have to recall the definition o f the H u m m in g distance w hich states that the H um m ing distance is the num ber o f elem ents in w h ic h tw o code w o rd s differ. A nother interesting term here is the H um m ing w eight o f a c o d e w ord th at defines the num ber o f non-zero elem ents in a code word (e.g. for the code w o rd : 1 0 0 1 1 0 1, the H um m ing weight o f the code word is: 4). Both the H um m ing d ista n c e (directly) and the H um m ing weight (indirectly) are tied to the c o d e 's detection and co rre c tio n capabilities through the following equations:

e - d `m mi i nn - l e t-

(38a) (38b)

2

w h e re d^^j^ is the m in im u m H um m ing distance o f the code, e is defined as the error d e te c tin g capability o f the code, while t is the error correcting capability o f the code. At th is p o in t w e m ay notice that in order to obtain the error detecting and correcting cap ab ilitie s w e have to calculate the Huffman distance betw een any two code words in th e su b sp a c e in o rd e r to find the m inimum H um m ing distance o f the code. It is easy to rea liz e th a t it w ould take a long time, especially for the codes with the large k (e.g. for a (127, 92) code w here k = 92 , the subspace contains 5 x 1 0 ^ ^ code words). However, as w e h a v e discussed earlier, w e know that the sum o f any two code words in the subspace y ield s a n o th e r code w ord from the subspace. Thus, the H u m m in g distance between any tw o c o d e w o rd s in the subspace is in fact the H um m ing w eight o f the resulting code w o rd . So, instead o f calculating the H um m ing distance for any pair o f code words in the s u b sp a c e , w e sim ply ju s t calculate the H um m ing weight o f the code word obtained by th e ir addition, w h ic h theoretically could be any code w ord in the subspace. It is now easy to c o n c lu d e that the sm allest H um m ing w eight o f the code is in fact the minimum H u m m in g distance o f the code.

50
T o sum up th e d e c o d in g procedure for the systematic block codes: upon the arrival o f a c o d e w o rd , w e first calculate the syndrom e o f the received w ord using the equation (35), then w e locate th e error pattern in the table that gives a one to one correspondence b e tw e e n s y n d ro m e s and e rro r patterns. Once the error pattern is identified the received c o d e w o rd is b in ary added with the identified error pattern and the original m essage sy m b o l is o b tained. W e will dem onstrate the encoding and decoding procedures w ith an e x a m p le th a t will provide a com prehensive overview o f these two procedures. In the e x a m p le w e will use a (5,3) code to encode and decode 8 m essage symbols.
E X A M P L E ( L in e a r B lo c k C o d e s):

a) b)

F i n d a l l t h e c o d e w o r d s p r o d u c e d b y a ( 5 ,3 ) e n c o d e r ; F i n d a c o m b i n a t i o n o f e r r o r p a t t e r n s t h a t a ( 5 ,3 ) d e c o d e r u s e s to d e t e c t a n d c o r r e c t r e c e i v e d c o d e w o r d s th a t a r e e r r a n t;

c)

D e c o d e t h e f o l l o w i n g i n p u t s e q u e n c e to th e d e c o d e r : 1 0 1 0 0 1

1 1 0 1 1 0 0 0 0;

S O L U T IO N :

a)

n = 5, k = 3. T h e v e c t o r s p a c e s u b s p a c e S j c o n ta in s 2 ^ s y m b o ls i s 2 ^ =

c o n ta in s 2 ^ = 3 2 p o s s ib le 5 - b it c o d e w o r d s. T h e c o d e w o rd s. T h e n u m b e r o f a ll p o s s ib le 3 -b it m e s s a g e

=8

8.

T h e m e ss a g e s y m b o ls a re : 000, 001, 010, O il, 100, 101, 110, a n d s u b s y s te m

1 1 1 . O n e p o s s i b l e c o m b i n a t i o n o f 5 - b i t c o d e w o r d s t h a t m a k e u p th e c o u ld he:

00000 10 101 01010 11111 11001 01100 10011 00110
T h e s h o w n s u b s p a c e s a t i s f i e s th e c o n d i t i o n s ( 2 8 a ) a n d ( 2 8 b ) ; it c o n t a i n s a n a l l z e r o v e c t o r , a n d t h e s u m o f a n y tw o c o d e w o r d s y i e l d s a c o d e w o r d f r o m th e .s u b s p a c e . H o w e v e r , w e n e e d t o c h o o s e o n l y th r e e 5 - b i t c o d e w o r d s f r o m th e s u b s p a c e t o o b m i n a ll

8 p o s s ib le

c o d e w o r d s . I n o t h e r w o r d s w e n e e d t o b u i l d th e g e n e r a t o r m a t r i x G o f

t h e d i m e n s i o n s ( k x n ) t h a t s a t i s f i e s th e c o n d i t i o n f r o m t h e e q u a t i o n ( 3 1 ) :

51

1 1 0
G =

0 1 0
0 1

1 0
1 0

'0
w here: P -

f

f and

0

O'

0 1

1 1

13

=

0 0

1 0 0 1

N o w , b y u s i n g t h e e q u a t i o n (3 ) w e c a n g e t a l l

8 code

w o rd s. T h e y a r e lis te d b e lo w :

M ESSAG E:

C O D E W O RD S:

H U M M IN G W E IG H T :

000 001 010
O il

100 101 110 111
b) dfjjifj = 2 , s o w e c a n d e t e c t a l l

00 0 0 0 11001 01010 10011 01100 10 101 00110 11111
1- b i t

0
3

2
. 3

2
3

2
5

e r r o r s in th e r e c e i v e d c o d e w o r d s , a n d

c o r r e c t 2 '^ -- 2 ^ - 1 = 3 e r r o r p a t t e r n s . W e w i l l c h o o s e t h a t o u r e r r o r p a t t e r n s a r e :

0
e -

0
0

1 0
0 0 0 0

0
0 1

1 0 0

c)

F i r s t , w e r e a r r a n g e t h e i n p u t s e q u e n c e in to 5 - b i t r e c e i v e d c o d e w o r d s :

r / = [] r

0

1 0

o]

2

= \l

1 1
0 0

0 i\
0 O]

r 3 = [l

-- T N e x t, w e f i n d tr a n s p o s e d p a r ity c h e c k m a tr ix H w i t h t h e ( (n -- k ) x n ) d i m e n s i o n s u s i n g th e e q u a t io n (3 4 ):

52

1 0 0 0 1

0
I I I

1 '0

w here:

o' l2 =

I' I

0

1

and

P =

0 1

1

N o w , w e c a lc u la te th e s y n d r o m e m a tr ix f r o m th e e q u a tio n (3 7 a ):

-- --
S = e -H

J' =

0
1
1

1
0
1

T h e n , w e f o r m a o n e t o o n e c o r r e s p o n d e n c e b e t w e e n t h e e r r o r p a t t e r n s e a n d th e synd ro m e S :

'0

0

1 0 o'

'0

f

1 0 0 0 0 o 0 0 0 0 1

1 0 1 1

T h e f o l l o w i n g t h i n g i s t o c a l c u l a t e th e i n d i v i d u a l s y n d r o m e s f o r t h e e a c h r e c e i v e d c o d e w o r d a s p e r e q u a tio n (3 5 ):

Si= {l

l]

S2=[0

l]

S3=[l

O]

U s in g th e o b ta in e d in d iv id u a l s y n d ro m e s w e f i n d th e ir c o r r e s p o n d in g e r r o r p a tte r s fr o m e : ~ei =

[0

0 0 0 0 1 0 0 0 0

l] O] d\

62=[0

e j = [/

T o o b ta in th e c o r r e c t c o d e w o r d s w e p e r fo r m th e f o l l o w i n g b in a r y a d d itio n :

Ü -r-ve

53
T h a t y ie ld s :

Ul=[l

0 1 0

1 0 0 0 0 0
0]

f/2 = [7 f7 i= [o

F r o m h e r e i t is e a s y t o d e t e r m i n e w h i c h m e s s a g e s y m b o l s h a v e b e e n tr a n s m i t t e d . B y u s i n g t h e t a b l e f r o m t h e p a r t a ) w e c a n e a s i l y f i n d th e c o r r e s p o n d i n g m e s s a g e s y m b o ls :

mi={l

0

l\

n i2

=[ 0

0

l]

n i 3 =[ 0

0

O]

4 .1 .5

M a tla b Im p lem en tation o f L in ear B lock C odes

T h e fo llo w in g M atlab files simulate the linear block code implementation: T h e M a tla b file "G e n e rate M atrices.m " that creates the (12, 8) linear block code that is u sed for th e project sim ulation. The file also creates the following matrices needed for the e n c o d in g and d ec o d in g procedures: Ik, P ,G , H ^ , e , and S . The " L B C _E ncode.m " and " L B C D e c o d e .m " M atlab files that perform the encoding and decoding procedures im p le m e n te d in an exact fashion as presented in the discussion and exam ple earlier. T o c o n c lu d e the discussion on linear block codes, let's take a look at the printout o f the M a tla b c o d e th at w o rk s out the exam ple above (the code for " LB C E xam ple" file is p ro v id e d in th e A p p e n d ix B o f this report): > > L B C _ E x a m p le 1

G =

0

1

1

0

0

0 1

1 0 1 0 000 001 010

1 0 0 1
U {1}= 0 0 0 0 0 U {2}= 1 1 0 0 1 U {3}= 0 1 0 1 0 U {4}= 1 0 0 1 1 U {5}= 0 1 1 0 0 U { 6} = 1 0 1 0 1

1 1

m e s s a g e { 1} = m e s s a g e { 2} = m essa g e {3}=

m essa g e {4}= O il m essa g e {5}= 100 m e s s a g e { 6} =

101

54
m essa g e{7 }= 110 m e s s a g e { 8} = U {7}= 0 0 1 1 0 U { 8} =

111

11111

r e c e iv e d c o r r u p te d se q u e n c e =

101001110110000
Ht =

e =

0

0

1

0

0

1 0 0 0 0 1

0 1
I

S=

0

1

1 0

0 0

0 0

0 0 0 1

1
1

0
1

1 1 1

d e c o d e d _ m e s s a g e _ s y m b o ls =

101 001 000

W e see th a t in both cases, the example and the M atlab code, w e get the sam e results o b ta in in g th e sam e d e c oded m essage symbols: 101 001 000 as expected.

4.2 Convolutional Codes
C on v o lu tio n a l codes are generated by passing a specified num ber o f bits, o r a sin g le bit itself, th ro u g h a linear finite state register. T he linear finite state register is also c o m m o n ly called a convolutional encoder. Convolutional codes are usually specified by th re e para m ete rs: n, k, and m, w here k is a num ber o f bits in a c odew ord coming in the e n c o d e r (input sequence), n is a num ber o f bits in a codew ord going out o f the encoder ( o u tp u t c o d e w o rd ), and m is a num ber o f stages o f m em ory registers. From the definition o f th e index m w e note an important characteristic o f Convolutional codes; convolutional e n c o d e rs contain m em ory. T he encoder in Figure 9 is classified as a (2,1,4) convolutional e n c o d e r du e to th e follow ing reasons: the output o f the encoder contains a 2-bit c o d e w o r d , th e input to the encoder contains a I-bit input sequence that is shifted through 4 sta g e s o f th e m em o ry register (encoder). It is a s s u m e d th at each bit in the input sequence (in the case in Figure 9 only one bit m a k e s th e input sequence) is equally likely to be either 1 or 0 and is independent from an y o t h e r bit in the sequence. The mutual bit independency is exactly a main reason w hy w e a re a ble to perform the channel coding operation before the data is transmitted out. F o r e x a m p le , if du rin g a transm ission one or m ore bits o f information are incorrectly re c e iv e d ; fro m th e rec e iv e r's prospective w hat can be do n e to correct those corrupted bits?

55

Input
Sequence

Switch Box

Output Codewoi-d (nl +u2)

Figure 10: (2,1,4) Convolutional Encoder. F irst o f all, th ere w ould be no way we could determine that the transmission has caused a c o rru p tio n o f a bit, o r stream o f bits, unless we have a previous knowledge o f the data th at had b e e n sent out. Secondly, the concepts mentioned in the previous sentence pretty m u c h c o n tra d ic t each other; if we know in advance what is to be transmitted, then w hy do w e need to tra n s m it it at all? This is where convolutional codes come into the picture by a d d in g a prev io u sly d e term ined num ber o f redundant bits that carry the information about th e o r d e r and structure o f bits that the data is formed of. Convolutional codes help control th e o r d e r and structure o f the data bits at the receiver side w here some bits might be received in error. In general, it is the function o f the encoder to introduce redundant bits in th e tra n sm itte d inform ation sequence which then can be used at the receiver side to o v e rc o m e the effects o f noise and interference e ncountered during the transmission o f the signal th ro u g h the channel. The am ount o f redundancy introduced during the encoding p r o c e d u r e is m easured by the ratio n over k. O n c e th e co n v o lu tio n a l c o d e indexes (n, k, and in) are determined (the responsibility o f th e de sig n e r), the convolutional encoder is easy to implement. First, m boxes representing th e m m e m o r y register stages are drawn, then n m odulo-2 adders are placed, and finally th e m e m o r y register stages are connected to the adders using the generator polynomial vector. T h e g e n e ra to r p olynom ial vector g determines a w ay in which the stages o f the m e m o r y register are going to be connected to the m odulo-2 adders. The generator p o ly n o m ia ls , as th e elem ents o f the generator polynomial vector (also called connection v ecto rs), give the convolutional code its unique error protection quality. 1 here arc many c h o ic e s for a selection o f generator polynomials for an m order code. However, they all do not result in o u tp u t codew ord sequences that have good error protection properties. T h o s e g e n e ra to r polynom ials that do provide good error protection properties are usually o b tain e d by c o m p u te r sim ulations. The generator polynom ials used for the simulation o f a co n v o lu tio n a l c o d e in this project are taken f r o m Peterson and Weldon s book C o rre c tin g C o d e s " and th ey are presented in Table 1; Error

56

N m n b e i of Registei-s 3 4 5

F i r s t P o ly n o m ia l 111 1101 11010

Second P o ly n o m ia ls

10 1 1110
11101

6 7 8 ^ 10

110101 1101011 11011101 110111001 1101110010

111011 1101010 11100110 111001101 1110011001

Table 1; G enerator Polynomials for Convolutional Codes. F r o m T a b le 1 w e conclude that w e must have the same num ber o f generator polynomials as w e have ou tp u t bits from the encoder. Also, each generator polynomial would contain m e le m e n ts th at can be represented by a 1 or a 0, where 1 represents a connection and 0 th e a b sen c e o f the connection between an adder and a m em ory stage. In the case o f the (2 ,1 ,4 ) e n c o d e r show n in Figure 10, we have two output bits (n = 2) and four stages o f a re g is te r (m = 4). Thus, w e would need tw o generator polynomials, each o f w hom would h a v e 4 elem ents. 4 .2 .1 C o n v o lu tio n a l C ode E n coding Several m eth o d s are used to represent the process o f convolutional encoding. The m o s t p o p u la r ones are: impulse response o f the encoder, polynomial representation, state r e p re s e n ta tio n and the state diagram, and the trellis diagram. Each o f these m ethods will be b rie fly disc u sse d here in this chapter while the impulse response and the polynomial r e p re s e n ta tio n m ethods will be implemented in the Matlab simulation. L e t 's now go back to our (2,1,4) convolutional e ncoder from Figure 10. At each input tim e , o n e bit fro m the input sequence is shifted into the leftmost stage (box 1) o f the e n c o d e r and the bits in the other stages o f the encoder (boxes 2, 3, and 4) are shifted one p o sitio n to th e right. The switch box at the output samples the outputs from the adders, in te rla c es them , and form s an output codeword (ni &
no).

The sampling process keeps on

r e p e a tin g until no m ore bits are available at the input o f the encoder. The connections b e tw e e n th e m o d u lo -2 adders and the m em ory stages o f the (2,1,4) encoder are given by th e fo llo w in g tw o generator polynomials: g l = [1 I 0 I] and g2 = [I 1 I 0],

57
4 .2 .1 .1 Im p u lse R esp o n se o f the C on volutional E n cod er A fte r ha v in g described the structure o f the convolutional encoder in Figure 10, w e w ill n o w ta k e a look at the functionality o f the encoder and how it responses to a single input bit 1 th at m oves th roughout the register's stages from the left to the right. This is called th e im pulse response o f the encoder and creates the so-called a walking one effect th a t sim u late s the im pulse excitation o f the encoder. At the certain time t only one stage o f th e reg iste r (encoder) contains a 1 while the rest o f the stages are populated w ith zeros. T h e c o rre s p o n d in g o u tp u ts at U] and ni are obtained and saved in tw o separate vectors. At the t i m e t -t-1, the 1 m oves into the next right stage o f the register while the previously o c c u p ie d stage is populated with a 0. The output bits U| and ni are again obtained and ad d e d to th e existing values in the vectors. The process is repeated until the 1 is walked th ro u g h all th e stages o f the register: T im e 1 2 3 4 Register Contents 1000 0 100 0 0 10 000 1 O utput bits: ny 1 1 1 1 0 1 0 1 n?

T able 2: Impulse Response o f the (2,1,4) Encoder. F r o m T a b le 2 w e see th at for each time t corresponding outputs from n, and n; are sa m p le d and stored creating vectors: n, and no. The impulse response o f the encoder is fo rm e d by a lte rn a tely tak in g bits from vectors ni and n? at tim es t = 1 , 2 , 3 , and 4: Im pulse R esponse o f the Encoder = 1 1 I 1 0 1 I 0. N o w w h e n w e k n o w the impulse response o f the encoder, we can easily find the response o f th e e n c o d e r to any type o f input. All we have to do is to apply the principle o f the su p e rp o sitio n , o r linear addition o f the time shifted impulses. For instance if we w ant to ob tain th e c o d e w o rd for the input sequence [ 1 0 1 1], all we have to do is that for each 1 in th e input sequence w e assum e that the encoder produces the impulse response at the o u tput, and for each 0 in the input sequence the encoder produces a zero response at the ou tput. A t ea ch tim e t we have tw o outputs o f the encoder, n; and n;, and at the time t + 1 th e o u tp u t resp o n se o f the encoder will be shifted tw o bits to the right compared to the o u tp u t re sp o n se at tim e t:

58 [t] ^ ^ ^
^

[Input] ^ ® ^ ^ T h e resulting codew ord: 4 .2 .1 .2

[Output] 11110110 00000000 1 1 1 1 0 110 1 1 1 1 0 110 111 1 1 0 1 0 1 0 1 1 10 (im pulse response) (zero response) (im pulse response) (im pulse response) (input response)

P oly n o m ia l R ep resen tation o f C on volutional E n cod ing

A n o th e r m ethod that a convolutional encoder can be represented with is the p o ly n o m ia l representation. In the polynomial representation each generator polynomial is r e p r e s e n te d by a real polynomial. Since in the binary world addition and subtraction give th e sa m e results, the sign o f each term in the polynomial representation is o f no interest to us. A s a co n v e n tio n , w e will assume that each polynomial factor contains a positive sign. T h e tim e w ill be represented by the degree o f each polynomial factor. An input bit that ge ts into th e e n c o d er at time t = 1 will be multiplied by the polynomial factor on the t - 1 d e g re e , w hich m eans if a 1 is a first input bit entering the encoder it would be assigned th e X° p o ly n o m ia l factor. I f w e now go back to the encoder in Figure 10, then the g e n e ra to r po ly n o m ials in the polynomial representation will be declared as: g l ( X ) = l - X ° + l - x ' + 0 - X ^ + 1 -X ^ =1 + X + X ^ g 2 ( X ) = l - X ^ + l - X ^ + 1 X ^ + 0 X^ =1 + X + X^; I f w e w a n t to e n c o d e the input sequence m = [1 0 1 1] that we have used in the previous se ction, th e n th at input sequence in the polynomial representation would look like: m (X ) = l - X ° + 0 - X ' + 1 - X ^ + 1 - X ^ =1 + X ^ + X ^ T h e o u tp u ts o f the adders (n, and n?) are found by m ultiplying both generator p o ly n o m ia ls , g i(X ) and g 2 (X) individually with the input polynomial, m(X): n i ( X ) = m ( X ) - g i ( X ) = l -X ® + 1-X' +1 · X ^ + 3 - X ^ + 1· X'^ + 1 -X^ +1 · X^ = 1+X +X ^+3X ^ +X" ^+X ^+X ^;

In th e result for nj from above, we can notice that the polynomial factor X

has the

p re fix 3 as a result o f the multiplication. How ever, we know that in a binary system w e c a n h a v e o n ly tw o num bers a 0 and a 1. The n u m b er 3 in binary, is in fact, 1 + 1 + 1, and is e q u a l to 1 a p p lying the m ethod o f binary addition. N o w we have.

59

n](X) = l + X + X ^ + X ^ + X " * + X ^ + X ^ ;
w h ic h is basically equal to n, = [l th e se co n d o u tp u t ni:
1 1 1 1 1

1]. Similarly, we get the following result for

n 2 ( X ) = m ( X ) g 2 ( X ) = I X ® +1-X ^ + 2 - X ^ + 2 - X ^ +2-X^^ + 1 -X ^ + 0 - X ^ = l + X + 2 - X^ + 2 - X ^ + 2-X"^ + X^ = 1+ X + X ^ Since 2 = 1 + 1 = 0 in the binary system, the polynomial factors X ^ X \ and X" will dim in ish . T his is equal to the vector where ni = [1 1 0 0 0 1 0]. The final codew ord is o b tain e d by alternately taking one bit from the vectors m and n^: C odew ord; 1 1 1 1 1 0 1 0 1 0 1 1 1 0 . T h e fo llo w in g tw o m ethods o f the convolutional encoder representation; the state d ia g ra m and th e trellis diagram will not be discussed in details. Rather, they will be in tro d u c e d in su c h a m anner and quantity that a reader can follow the rest o f the report and be a ble to understand the decoding techniques based on the state and trellis re p re s e n ta tio n s o f a convolutional code. 4 .2 .1 .3 S ta te D iagram

T h e state diagram represents the possible contents (states) o f the rightmost m -1 re g is te r sta g e s (m is the num ber o f the m em ory register stages), and the transitions b e tw e e n states as a result o f the arrival o f a new input bit to the leftmost register stage. T a b le 3 p ro v id e s the state diagram for the convolutional encoder shown in Figure 10.
CiirTEiif S tate Next Slate 1 0 ----------100 000 100 000 101 001 101 001 110 010 110 010 111 o il 111 o il OutDUt 1 0 n l n2 n l n2 1 1 0 0 ^ 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0

o tn 001 010 O il 100 101 110 111

T a b le 3: State Diagram o f the (2,1,4) Convolutional Encoder.

60

Since th e e n c o d e r in Figure 10 has 4 stages o f the m em ory register, its last 3 stages will cre a te th e states o f the encoder. The last 3 stages can generate 2^ = 8 possible states. T he next state th a t the c urrent state moves to every tim e a new bit enters the encoder depends on th e fact if the new bit is either a 0 or a 1. To sum up. each current state can transit to one o f tw o possible next states depending w eather the first stage o f the m em ory register c o n ta in s a 0 o r a 1. F rom Table 3 we see that, for example, if the current state is 100, it m e a n s that the three rightm ost stages o f the memoiy' register are occupied with 1, 0, and 0 respectively. N o w , th e outputs ni and m o f the encoder depend if the very first stage o f the m e m o r y register is occupied by a 0 or 1. If the first stage contains a 0, then the output m = 1 and n? = 1. Similarly, if the first stage contains a 1, then the output n, = 0 and n j = 0. T h e next state depends on the occupancy o f the first stage as well. In this case, if the first stage has a 0 in it, the next state is 010 (after shifting all bits to the right by one), and i f th e first stage contains a 1, the next state is 101. 4 .2 .1 .4 T rellis D iagram T h e trellis diag ra m is characterized with row states as well as with colum n states. The trellis d ia g r a m requires 2 '^ " ' nodes to represent 2^^" ' possible encoder states, where m is, o n c e again, th e n u m b er o f stages o f the m em ory register that determines the state o f th e e n c o d e r. 2 t= 3 t= 4 t= 5 t= 6 t= 7 t= 8

000  100
010 110


(00)

(00)

(00)

(00) _ (00)

(00)

I

I



/

^

1011

00 1

I

o il I

111,
F ig u re 11: Trellis D iagram Representation o f (2,1,4) Convolutional Encoder.

61
F ig u re 11 sh o w s the resulting trellis diagram for the encoder in Figure 10 w hen the input s e q u e n c e is 1011. In th e trellis diagram the transition from one state to another state upon th e arrival o f a 1 is represented by a dashed line. The transition cased by a 0 at the input o f the e n c o d e r is pictured by a solid line. From Figure 11 we see that if the current state is 0 0 0 and a bit 1 co m e s in, the encoder reaches the state 100. The output values for m and t \2 are 1 and 1 respectively. 4 .2 .2 C o n v o l u t io n a l C o d e D ecoding

I f w e a ssu m e that 3 bits are encoded using (2,1,4) convolutional encoder in Figure 10, th en th e o u tp u t c o d e sequence from the encoder will contain 6 bits, since each input bit is represented by 2 bits at the output o f the encoder. These 6 bits m ay o r may not have errors. W e k n o w from th e encoding procedure that these bits are uniquely m apped. That m e a n s th a t a 3 bit sequence will have a unique 6 bit output. But due to errors, we can rec e iv e a n y o f all possible com binations o f 6 bits. The perm utation o f 3 input bits results in eig h t possible input sequences. Each o f these input sequences has a unique m apping to a six bit c o d e sequence by the encoder. These form a set o f perm issible sequences and the d e c o d e r 's ta s k is to determ ine which code word has been sent. Table 4 sum m arizes what ha s j u s t been explained:

Input

Valid Code Sequence

Receii'ed Sequence

Humming Distance

000 001 010 o il 100 101 110 111

000000 000011 001111 001100 111110 111101 110001 110010

111100 111100 111100 111100 111100 111100 111100 111100

4 6 4 2 1 1
3 3

T a b le 4: H u m m in g Distance Used for Decoding Convolutional Codes. L e t 's say w e have received the following sequence: 111100. As it can be seen from Table 4 th e received seq u en c e does not match to any o f the 8 valid code sequences. H ow do w e d e c o d e th e received se q u ence? There are two approaches: we can do a correlation b e tw e e n e a c h valid c o d e sequence and the received sequence and pick the sequence with th e best correlation. T his approach is known as the soft decision decoding. The second a p p ro a c h deals w ith the term the Hamming Distance introduced earlier w hen we

62
d isc u sse d d e c o d in g o f linear block codes. As we have defined it, the H am m ing distance b e tw e e n tw o c o d e w o rd s is defined to be the num ber o f elements in which those tw o code w o rd s differ. In o u r case we compute the H um m ing distance between the received word and all p o ssible valid code w ords and choose the code word with the smallest Ham m ing distance. T his is basically what is behind the so-called hard decision decoding. As the n u m b e r o f bits to be transm itted increases, the num ber o f calculations required to be done d u rin g th e process o f decoding (H um m ing distance) significantly increases such that it is no longer practical to do decoding this way. This brings a need to find a m ore efficient m eth o d th a t d o es not exam ine all options and has a w ay o f resolving am biguity such as in cases w h e n w e have tw o o r m ore code words have the same minimal H am m ing distance (in T a b le 4, both 111 110 and 11 1101 valid code sequences have the Ham m ing distance o f 1) T hen, w hich one to choose? To conclude; a m ethod o f decoding is needed in which fo r a m e ssa g e containing in bits we do not have to check each o f 2"^ possible code w ords to d e te rm in e w h ic h valid code word has been transmitted. This is where Viterbi decoding a lg o rith m c o m e s along. 4 .2 .2 .1 V iterb i D ecod in g A lgorithm V iterbi de c o d in g is the best known im plem entation o f the m axim um likelihood d e c o d in g . T he m ax im u m likelihood concept is a c o m m o n sense way to make decisions w h e n th e statistical know ledge o f the probabilities is available. In other words, the d e c o d e r c h o o s e s a valid code word c, as the transmitted sequence, if the likelihood P(r|c) is g re a te r th an the likelihood o f all the other valid code words. The Viterbi algorithm had been d e sig n e d w ith the following two assum ptions kept in mind: 1. 2. the probability o f occurrence o f a single error is very small; the probability o f occurrence o f two errors in a row is even much smaller than the probability o f occurrence o f a single error; W h a t th ese tw o assum ptions mean is that in com m unication systems errors occur in fre q u e n tly and th at they are random ly distributed. The Viterbi decoder examines an entire received sequence o f a given length and then the decoder computes a metric for e a c h p a th and m ak e s a decision based on this metric. The m ost com m on metric used is th e H a m m in g distance metric. All paths are followed until tw o paths converge on one n o d e . T h e n the path with the lower metric is kept and the one with higher metric is d isc a rd ed . T h e paths selected are called the survivors. The early rejection o f the unlikely p a th s grea tly red u c e s the decoding complexity. Even though the Viterbi decoding a lg o rith m solves th e problem s o f unnecessary checking all o f the 2 possible code

63
w o rd s, as w e ll as the a m biguity when two code words have the same H am m ing distance, it can n o t be c o n sid e r as the ideal. It can be only considered as the best decoding alg o rith m a m o n g the existing ones. In some aspects o f decoding it does ov e rp o w e r other d e c o d in g alg o rith m s but it self has severe constrains imposed on by hardware c om plexity. T h e V iterbi d e c o d in g algorithm uses the trellis diagram to perform the decoding p ro ce d u re . H ow ever, the d e c o d e r s trellis diagram is a bit different than the e n c o d er's trellis diag ra m . F or the d e c o d e r s trellis it is much more convenient to label each trellis p a th at tim e t (path b e tw een any two states in the trellis) with the Ham m ing distance b e tw e en s e q u en c e received at the time t and the corresponding code word. It is important to realize th at th e e n c o d e r 's trellis is known a priory to both the encoder and the decoder. T o fully und e rsta n d the operation o f the Viterbi algorithm, let's assum e that the following stre a m o f bits have been received at the receiver side and is to be decoded by the Viterbi algorithm : received sequence; 1 1 1 1 0 1 1 1 0 1 0 1 1 1 . Since th e d e c o d e r has the know ledge about the encoder, it knows that the encoder is a (2,1,4) c o n v o lu tio n a l encoder. That gives a clue to the decoder that the received sequence o f bits has to be partitioned into subsequences where each subsequence will contain 2 bits (since ea ch input bit at the transmitter side is encoded into a code word o f 2 bits). So, the rea rra n g e d received sequence would now look like this: 11 11 01 11 01 01 11. The d e c o d e r trellis d ia g ra m along with the respectful metric values is shown in Figure 12. L o o k in g at Figure 12, w e see that all output pairs (bits ni ni) have been replaced by the m etric values (H a m m in g distance values). For instance, the metric value for the path that rep re sen ts the transition from the state 000 to 100 is equal 0. That means that the output o f th e e n c o d e r and the received sequence at time t = 1 are the same (H am m ing distance is eq u a l to ze ro if all bits on a received word and the corresponding code word are the sam e). Sim ilarly, for th e transition from 000 to 000 (when 0 arrives at the input o f the e n c o d er) th e m etric is equal to 2. The meaning is that the received sequence and the code se q u e n c e at that particular state differ in 2 bits. G oing back to the state diagram show n in T a b le 3 w e can see th at for the state 000 the code sequence (output bits), when a 0 arrives, is 00. F r o m the Figure 12, we see that the received sequence at time t -- 1 is 11. T hus, w o r d s 00 and 11 differ in both their bits, thus the corresponding metric is therefore e q u a l to 2.

64
received sequence:

t= 000
100 010 110 101

11 t=6 t=

7

t=8

001 o il
111

F ig u re 12: The D ecoder Trellis Diagram (with H am m ing Distances). A t tim e t = 5 there are path crossing in every possible state (column nodes). These is w h e r e the V iterbi algorithm will get rid o f some paths and reduce the com plexity o f the d e c o d in g w h ic h , as seen from Figure 12, already has become quite com plex even though o n ly 5 tim e units have elapsed. L e t 's start with the state 000 (first node in the t = 5 column). Tw o paths merge in this no d e . T h e first path is 000, 000, 000, 000, and 000 with the metric equals to 7 (2+2+1+2). T h e se c o n d path is 000, 100, 010, 001, and 000 having the metric value o f 1 (0+0+0+1 ). T he a lg o rith m will keep the second path since its cum ulative metric is smaller than the c u m u la tiv e metric o f the first path. The second node (state 100) has the following tw o pa th s th at get m erged in the node. The first one is: 000, 000, 000, 000, 001 with the m etric e q u a ls to 5 (2+2+1+0), and the second one: 000, 100, 010, 001, 100 with the m etric value o f 1 (0+0+ 0+ 1). The algorithm will select the second path and discard the first on e due to the fact that the first path's metric is higher than the second p a th 's. Sim ilarly, the algorithm will choose only 8 survivors and the trellis tree will have the sa m e n u m b e r o f branches as it did at the t - 1 unit o f time. The resulting trellis diagram is s h o w n in Figure 13. There w e see that the path that has been running through the 000 states (first row ) has com pletely disappeared and that for the time t = 1 only one state

65
tra nsition rem ains; from 000 to 100 with metric 0. I f w e now go back to the slate diagram in T a b le 3, w e c a n conclude that transition from 000 to 100 happens w hen a 1 is at the input o f the encoder (1 is encoded). Therefore, the decoder decodes the first code word (se q u e n c e ) to be a 1. received sequence: 11 t= 2
"

H t=3

qi

01 t=4 t=5 t=6

01 t= 7

11 t= 8

000

100 010
110
101

001 O il

111
F ig u re 13: The Decoder Trellis D iagram after t=5 Time Units. It is in te restin g to notice that it has taken 5 time units before the decoder could pull the d e c isio n th a t a 1 has been received at the time t = 1. These 5 time units represent a d e c o d in g d e la y w h ic h can be as much as five times the constraint length (if m = 4, the d e la y is 2 0 tim e units). This decoding process will continue until the last transmitted code w o rd is d e c o d e d . In general, w hen a binary convolutional code with the constraint length K is d e c o d e d by V iterbi algorithm, there are 2*^ * states as well as 2*^ *surviving paths at each stage. F urtherm ore, a convolutional code with k bits being shifted in the encoder at a tim e will g e n e ra te a trellis with 2 2
k-(K-l)

states. At each stage o f the trellis, there are

p a th s th a t g e t m erged at each node. Only one path survives and this is the m ost

p r o b a b le ( m in im u m distance) path. Thus, the num ber o f computations in decoding p e rf o r m e d at ea ch stage increases exponentially with k and K. This is a main reason w hy th e use o f V iterbi algorithm in decoding convolutional codes is troubled. A nother d is a d v a n ta g e o f th e Viterbi decoding is that the decoding delay is usually too long for m o st practical applications, which often is not acceptable. Also, the m em ory required to

66

store th e surviving sequences w culd be o f a hum ongous size and can be very expensive. T he solution to this problem w ould be to modify the Viterbi algorithm in a w ay that results in a fixed d ecoding delay without significantly affecting the optim al perform ance o f th e algorithm . Viterbi decoding is quite important since it also applies to decoding o f block co d es. This form o f trellis decoding is also used for Trellis-Coded M odulation (T C M ).

4.2.3 M a t l a b I m p l e m e n ta t io n o f t h e (2,1,4) C o n v o lu tio n a l C o d e T w o M atlab files sim ulate the processes o f convolutional encoding and decoding. T he M a tla b file "C o n v o lE n c o d e r'" performs the encoding procedure implem enting a (2,1,4) convolutional encoder. The file does all three types o f encoding which have been d iscussed in sections above; Impulse Response, Polynomial, and Trellis. The file "V ite rb i_ D e c o d e r.m " perform s the decoding algorithm as being discusses in previous section. T h e de c o d in g delay has been set to 20 tim e units. A t th e end o f this section, let's show the print out as the result o f running an exam ple c o d e im p lem e n tin g the (2,1,4) convolutional code in M atlab (the file "C o n v o lE x a m p le .m " is listed in A ppendix B o f the report): > > C o n v o lE x a m p le

D a ta to b e e n c o d e d :

1011
C o n n e c tio n R e p r e s e n ta tio n C oded: 1 1 1 1 1 0 1 0 1 0 1 1 1 0

P o ly n o m ia l R e p r e s e n ta tio n C o dew ord: 1 1 1 1 1 0 1 0 1 0 1 1 10

T r e llis R e p r e s e n ta tio n E n c o d e d S tr e a m :

11111010101110

D eco d ed Sequence: 1 0 1 1

D i f f e r e n c e b e t w e e n D e c o d e d a n d E n c o d e d S e q u e n c e s ( U s i n g V ite r b i D e c o d i n g A lg o r ith m ): 0

67

Chapter 5: WCDMA
5.1 W CDM A Technology
T h e first generation o f mobile comm unication systems is represented by the a n a lo g sy ste m s designed to carry the voice application traffic. Those systems are mostly based o n the a n a lo g frequency modulation technique and all systems em ploy the F re q u e n c y D ivision M ultiple Access (FDM A ) channelization technique to share the m e d iu m for the transm ission o f information. In FD M A , the transmission m edium is divided a m o n g all stations w hich share the same allocated frequency bandwidth called the spectrum . T h e spectrum is divided into N channels where N represents the num ber o f stations sharing the m edium . Each o f N stations transmits its information continuously on an a s sig n e d c h a nnel and during the period o f the transmission no two stations share the sam e channel. B a n d p a ss filters are used to contain the transmitted energies within a ssigned channels. For the reasons mentioned above, FD M A is suitable for stream traffics and finds its use in c onnection oriented systems. On the other hand, FD M A is extrem ely inefficient for bursty traffics since during the periods o f no traffic the channel will sit idle, e sse n tia lly a w a sted resource. T h e second g e n e ra tio n o f mobile communication systems relies exclusively on digital m o d u la tio n tec h n iq u es and two channelization techniques: Time Division M ultiple A c c e s s ( T D M A ) and C o d e Division Multiple A ccess (CDM A ). In T D M A , each station tra n s m its d u rin g its assigned time slot and uses the entire spectrum during its tra n s m is s io n . In the basic form o f TDM A, each station is assigned the same size time slot, 1/N for N stations, so each station could approxim ately have the same transm ission bit rate. H o w e v er, the stations' bit rate can be made variable by allocating several slots to th e sam e c h a nnel o r by changing the duration o f time slots. Nevertheless, in T D M A the bit rate allocated to a station is static and this m edium access approach, like FD M A , is not v e ry d e sirable for bursty traffic. B o th F D M A and T D M A can be used to connect to a base station or a controller in two w ays; u sing fre q u e n c y division duplexing (FDD) and time division duplexing (TD D ). F D D p ro v id e s tw o distinct bands o f frequencies for each station (two channels), the fo rw ard ban d (forw ard channels) that provides traffic from the base station to other stations, and the reverse band (reverse channels) that provides traffic from the stations to the base station. T h e frequency separation between each forward and reverse channel is c o n s ta n t th ro u g h o u t th e system regardless o f the particular channel being used (usually

68

45 M H z ). T D D uses tim e instead o f frequency to provide a forward and a reverse link. In T D D , each station has both a forward and a reverse time slot to facilitate bidirectional c o m m u n ic a tio n . I f the tim e separation between the forward and reverse time slot is small, th en the c o m m u n ic a tio n betw een the base station and other stations seem s to be sim u lta n e o u s. The F D M A channelization technique is used with FDD, while the T D M A c h a n n eliz a tio n technique can be used with either FD D or TD D. C D M A provides another ty p e o f chan nelization technique. In T D M A and FD M A the transmission from different stations is clearly separated either in time or in frequency. The C D M A technique c o m b in e s the T D M A and F D M A techniques and the transmissions from different stations o c c u p y th e entire frequency (like TD M A ) at the same time (like FDM A ). 5.1.1 D irect S eq u en ce Spread Spectrum

T ra n sm issio n s from different stations are separated with the use o f the Direct S e q u e n c e Spread S pectrum (DSSS) method where a baseband data, before being tra n sm itte d , is m odulated into a signal that occupies a much larger bandwidth than the o riginal data. T he technique is conceptually quite simple; the data is m odulated via the X O R o p eratio n w ith a m uch higher data rate spreading code. Figure 14 show s an e x a m p le o f the D S S S method:

1

0
|_______ |

1

1

0
|_______ |

1

^
\

0

0

Data inpul A

|

|________________

1~5 0 ^ 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 0

/

)

0 1 1 0 0 1 1 0 0 1 10 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0
_ n _ n r i n r n n i
u i t l

01100110011010111010001110110110
Received signal C
Locally generated PN b it stream identical to B above
D a ta o u tp u t A=C®B

j

T

j

^

"i n

^

i n

_

r

T

n

r

L

1 0 0 1 0 1 1 0 1 0 01 0 10 01 0 1 0 w o 0 1 0 w o w o
r L n n ^ j i ^ ^ i r ^ L T i r x

1

0

1

1

0

^

0

0

I

I

I

Figure 14: Direct Sequence Spread Spectrum.

69
T h e s p re a d in g c o d e is generated by a pseudo-random num ber generator. The spreading c o d e only a p p e a rs to be random ly generated but is in fact known to both the transiver and receiver. It can be v e ry long, even infinitely long and must satisfy the following im p o rta n t properties, it has to be uniformly distributed, independent, and to have the c o rre la tio n and cross-correlation property. The spread signal can not be detected by an e a v e s d r o p p e r unless th ey can generate the exact same spreading code and can s y n c h ro n iz e its generation in time. The energy from the spread wideband signal within a n a rr o w band is v ery low, and even if detected will appear to be a random noise. A s m ore D S S S signals o c c u p y a wideband channel the noise baseline will eventually increase until c o m m u n ic a tio n s is no longer possible. In g e n e ra l, the result o f the DSSS process produces a very robust signal for transmission. W e w ill go back to the C D M A technique once we introduce the third generation o f m o b ile c o m m u n ic a tio n s system s which use a new technology called W ideband C D M A (W C D M A ). T h e p rec e d in g discussion about DSSS requires that both the transmitter and receiver select very long ra n d o m spreading codes ahead o f time before they even begin to c o m m u n ic a te . This is very impractical and another method that will provide an a u to m a te d m ea n s o f generating pseudo random spreading codes is needed. A simple g e n e ra to r p o ly n o m ia l (a shift register) used for the cyclic block codes can be modified to p ro d u c e th e s e spreading codes. Figure 15 shows a right shifted generator polynomial:

11-1

Figure 15: Right Shifted G enerator Polynomial. I f the initial state o f the registers R ^ . R ^ - i , appropriate feedback

c o n n e c tio n s are selected, then the contents o f the registers will cycle through all possible 2 " --1 n o n z e ro states before repeating. The resulting sequences are o f the m axim um len g th p o ssib le and can be considered to approxim ate random binary sequences in the sense th a t shifted version o f a sequence are approxim ately uncorrelated. The length o f the s e q u e n c e s d e p e n d s on the num ber o f the registers in the shifter and it is left to a designer to dec id e h o w m any e lem ents the sequences will have. L e t's now take a look at the

70
p ro ce ss o f g en eratin g approxim ately random sequences by a right shifter that has 4 reg iste rs and w h o se initial state is: 1 0 0 0. The same shifter configuration show n in F ig u re 15 is used h av in g 4 registers, from Rg to R g . By having the sam e hardw are (right sh ifter) installed at both ends, the transm itter and receiver can generate the sam e se q u en c e s and synchronize them by im plem enting a constant delay. T his is crucial to re c o v e r the receiv ed signals and extract them for the right channels: Tim e 0 1 2 3 4 5 6 7 8 9
1 0

R^ 1 0 0 1 1 0 1 0 1 1
1

Ri 0 1 0 0 1
1

R. 0 0 1 0 0
1 0

Rn 0 0 0
1 0

4- Initial State

0 1 0 1
1

1 1 0 1 0
1 0 1

0 1 1

11
1 2

1 0 0 0 1 0

1
1

1
1

1

13 14 15

0 0

1 0 0

1 1 0

W e c a n see th a t 15 different sequences are generated before the sequence 1 0 0 0 repeats itself. 5 .1 .2 C o d e D iv isio n M u ltip le Access

T h e th ird generation o f m obile system s is designed to support w ideband services such as high speed Internet access, video and high quality im age transm ission along w ith th e v o ice service. T he W C D M A technology is developed in o rd er to create a global stan d ard for real tim e m ultim edia services. W ith the support o f International T e le c o m m u n ic a tio n U nion (IT U ) a specific spectrum w as allocated (2G H z) used for sh a rin g in fo rm atio n am ong the third generation system s. T he W C D M A system s use the C D M A m edium access technique but a bit different than w e have introduced in the

71
e a rlie r p a ra g ra p h s. It goes a step further in the C D M A technology by using a so-called c h ip se q u e n c e s instead o f pseudo-random generated spreading code sequences. L e t s su p p o se th at a data is generated at R bits/seconds and that each bit in the data is tra n sfo rm e d into G bits by m ultiplying each bit (represented by either a +1 representing a 1 o r a -1 re p re se n tin g a 0) by G chip elem ents. All c h ip 's elem ents are also represented b y e ith e r a +1 o r a -1. T he ehip sequence is produced by a special code and appears to be ra n d o m e x c e p t th at it repeats after a very long period. The spreading factor G is chosen in su ch a w a y th a t th e tran sm itted signal occupies the entire speetrum . The chip sequences th a t are used fo r different channels m ust be selected so that any pair o f chip sequences w ill h av e low cro ss-eo rrelatio n . O therw ise, the reeeiver w ould not be able to separate the tra n s m is s io n s c o m in g from stations using these chip sequences. One w ay to ensure that th e c ro ss-e o rre la tio n betw een any pair o f chip sequences will have low cross-correlation is to u se th e o rth o g o n a lity property. I f w e d e fin e tw o chip sequences, A and B, both w ith the sam e num ber o f elem en ts n as: A = ( a ] ,a 2 ,a 3 ,...,a p ), B = ( b i,b 2 ,b 3 ,...,b p ) . T h e se tw o chip seq u en ces are said to be orthogonal if their inner product, also called the d o t p ro d u c t, is zero: and

A B --^ ^ a ; - bj = a j - b j + a 2 'b 2 -i
i= l

t&n '^n

(^9)

S ince e a ch c h ip sequence consists ft"om +1 and -1 elem ents, w e can define that:
n

A · A = ^ ^ a ; ^ = a[^ + a 2 ^  ! ---------------= " , 1=1 g . B = ^ ^ b j ^ = bj^ + b 2 ^ H
i= i

(40a)

1 bj

= n.

(40b)

I f w e n o w a ssu m e th at w e w ere sending a binary 0 in the channel that uses the chip se q u e n c e A , and a b in ary I in the channel that uses the chip sequence B. T he binary 0 sp read o u t by th e chip sequence A will be:

72

0 = -A = (-a,,-a 2 ,.,, - a J ,
w h ile th e b in ary 1 spread o u t by the chip sequence B will be:

1

= B = ( b i ,b 2 ,...,b ,, ) .

T he a g g re g a te signal th at w ill appear in the spectrum as the result o f adding the spread v e rsio n s o f b in ary 0 and 1 is:

-5^11 ) "

( " ^1

+ 1>],-- 32 + b 2 , . . .,-3^1 + b ^ ) = -- A + B.

(41)

W h en a re c e iv e r tries to recover the data sent through the channel w ith the chip sequence A , it w ill m u ltip ly th e received signal T w ith the chip sequence A and then integrate (add) a ll th e e le m e n ts o f the resulting sequence:
n

^ ^ t j 3; = t] -3] + t 2 32 H
i=l

l-tn `^n = T A = ( - A + B )-A

= -A

A + B A - - n + 0 = -n .

S im ilarly , th e d a ta in the channel w ith the chip sequence B w ill be decoded as:
n

^
i= ]

tj bj = t] b] + 12 b 2 4 =-- A ·B + B `B = 0 + n = n.

= T B = (-- A + B) B

S u m m a riz in g th e results o f the m ultiplying the received signal with the appropriate chip se q u en c e s, th e receiv er w ill determ ine that a binary B (n ). L e t's now ta k e a look how w e can choose chip sequences to use them within the sam e sp e c tru m but for different channels. As we know from the discussion above, those chip se q u e n c e s m ust be orthogonal am ong them selves. That m eans that the dot product o f any c h ip se q u en c e w ith itse lf w ill yield a positive num ber n, w here n is the num ber o f e le m e n ts in th e chip sequence. A lso, the dot product o f any chip sequence w ith its c o m p le m e n t w o u ld yields a negative num ber n. A nd finally the dot product o f any se q u e n c e w ith any o ther sequence o r their com plem ents will yield a zero num ber.
0

has been sent through the channel

w ith the chip seq u en ce A (-n) and a binary 1 through the channel with the chip sequence

73
5 .1 .2 .1 W alsh O rthogonality

O ne o f th e m ethods to obtain orthogonal chip sequences o f length n, where n=
2

is to use the W alsh m atrix. The W alsh m atrix is build from binary coefficients

and is d efin e d rec u rsiv e ly as:

Wi=[+ll Wn W2n = W,, W,, Wn

(42a)

(42b)

w h ere W n is th e com p lem en t o f the elem ent W^ ..In som e literature the W alsh m atrix W% is d e fin e d as having its only elem ent equal to - I. R egardless o f the type o f the W | W alsh m atrix being used, higher order W elsh m atrices (n = 2, 4, 8 , ...) provide a set o f o rth o g o n a l sequences. It is straight forw ard to show the construction o f the W alsh m a trix fo r n = 2, 4,
8

o r any higher order W alsh m atrix. The W 2 is form ed by having

m atric e s W j and W 1 as its elem ents: + 1 +1
Wo =

+ 1 -1

T h en , th e W 4 m atrix is form ed from W 2 according to (42b): +1 +1 W4 = +1 +1 +1 --1 +1 -1 +1 +1 +1 -1 -1 --1 -1 +1

A n d fin a lly , th e W g m atrix is form ed from the W 4 m atrix and its com plem ent W 4 ;

74
+1 4-1 4-1 4-1 4-1

+ l + l 4-1 4-1 4-1 + \ -1 + l -1 + l -1 + 1 -1 4-1 - 1 - 1 4-1 + \ - 1 - 1
-1 4-1 -1 4-1 -1 -1 4-1 4-1 -1 -1 4-1 4-1 -1 -1 4-1 4-1 -1 -1 -1 -1 -1 -1 -1 -1 4-1 -1 4-1 4-1 -1

Wg =

4-1 4-1 4-1 4-1

+ ] -1
-1 +1 4-1 +1

G en era lly , so m e o f the n orthogonal sequences in the

do not alternate quickly

b e tw e e n +1 and -1 and as the result th ese sequences w ill not produce a transmitted signal that is spread o v er the w h o le available spectrum. D ue to this fact, the W alsh m atrices are prim arily used to provide channelization w hile in practice the m atrices are com bin ed w ith oth er ad dition al spreading such as D S S S. A ll th e tim e in our d iscu ssion about C D M A tech nology, w e assum ed that at the receiver end ea ch re c e iv e d signal has the sam e pow er level. This is very important assum ption and in ord er to w ork properly the C D M A technique requires all signals at a given receiver to h a v e a p p ro x im a tely the sam e pow er. If this is not the case, then transm issions from n earby sta tio n s cou ld o verw h elm transm issions from distant stations. This problem is ca lled the near-far problem . T he goal is that the receiver receives the sam e pow er level from all station s regardless o f distance from the receiver (w ithin a cell). I f the p ow er level from o n e station is higher than needed, the quality w ill be ex c essiv e thus taking a d isp ro p o rtio n ate share o f the resources and generating unnecessary interference with sig n a ls fro m other stations. On the other hand, if the pow er level from a station is too lo w , it w ill result in poor quality o f the received signal. In order to keep the received p o w e r at a su itable level, the W C D M A standard defines a fast pow er control that updates p o w e r le v e ls from each station 1500 tim es every second.

L e t's n o w tak e an exam p le that w ill sum m arize everything that has been said regarding th e C D M A tech n ology:

EXAMPLE (CDMA Channelization):
A t w o c h a n n e l s y s t e m u s e s th e f o l l o w i n g tw o o r t h o g o n a l c h i p s e q u e n c e s ( ta k e n a s t h e 3'^`^ a n d r o w f r o m th e W e ls h m a t r i x W g.'

C _ A = ( + 1 , + 1 , - L - I , + L + L - L -V>

75

- i . -1 , -1 , -1 . + 1 , + 1 ).

T r a n s m i t b its . 1 0 0 o n t h e c h a n n e l A a n d b its : 0 1 0 o n t h e c h a n n e l B . R e c o v e r th e s ig n a l o n th e c h a n n e l A .

S O L U T IO N :

T o t r a n s m i t a 0 t h r o u g h th e c h a n n e l A , th e s y s t e m w i l l a p p l y th e c o m p l e m e n t o f th e c h i p s e q u e n c e C _ A a n d to t r a n s m i t a 1, it w i ll t r a n s m i t th e c h i p s e q u e n c e C _ A its e lf.

S i m i l a r l y , t o t r a n s m i t a 0 t h r o u g h th e c h a n n e l B , th e s y s te m w i l l s e n d o u t th e c o m p l e m e n t o f th e c h i p s e q u e n c e C _ B a n d to tr a n s m i t a 1, it w i l l s e n d o u t th e c h i p se q u en c e C _B.

F o r t h e c h a n n e l A , b i t s 1 0 0 a r e s p r e a d b e fo r e b e i n g t r a n s m i t t e d a s s h o w n b e lo w :

0

0

Chaimel A
11 00 11 40 0 :0 0 411 00 11 00 11 00 11

O n t h e c h a n n e l B , b i t s 0 1 0 a r e s p r e a d o v e r th e s p e c t r u m in th e f o l l o w i n g m a n n e r :

0

1

0 ; -------------------------- 1 11 00 00 00 11 : 1 1 1

Cliaimel B
11 00 00 11 00 1 1 11

S o , t h e d a t a b e i n g s p r e a d a t th e c h a n n e l A is: A =

1, + 1, - 1, - 1, + 1 ,+ 1 ,- 1 ,- 1 ,- 1 ,- 1 ,+ 1 ,+ 1 , - 1 , - 1 , + l .

+ l r L - l , + l , + l . - l - L ' ^ l . + l).

w h i l e t h e s p r e a d d a t a a t c h a n n e l B is:

76

T h e a g g r e g a t e s i g n a l (A + B ) m a d e u p b y a d d i n g th e s i g n a l s o n th e c h a n n e l s A a n d B l o o k lik e th is :

SigiialA
+

11 11 00 00 00 00

11 11 00 00

11 11

Signal B

A + B = (+ 2 , + 2,-2, -2 , 0 ,0 .0 ,0 , -2,-2, +2, + 2 , 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,- 2 ,- 2 , +2, + 2 ). A t t h e r e c e i v e r s id e , t o r e c o v e r th e s i g n a l m e a n t f o r th e c h a n n e l A th e a g g r e g a t e s i g n a l i s m u l t i p l i e d w i t h t h e c h i p s e q u e n c e A a t th e c o r r e la t o r :

CIlip Sequence A

11 00

11 00

11 00

11 00

11 00

11 00

C _ A = ( + I , + I , - l , - l , + l , + l , - l , - l , + 1 .+ 1

, +

1 ,+ 1 ,- 1 ,- I , + l , + l , - l , - l , + l , + l , - l , - l ) ;

Outjiut of CoiTelator

11 11

11 11

00 00 00 00

00 00 00 00

C o r r O u t = ( + 2 ,+ 2 ,+ 2 , + 2 ,0 ,0 ,0 ,0 , -2,-2, -2, -2, 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,- 2 ,- 2 , -2 , -2 ), N o w , a t t h e i n t e g r a t o r e a c h s e q u e n c e o f 8 b its c o m i n g o u t o f t h e c o r r e l a t o r w i l l b e a d d e d d u e t o t h e f a c t t h a t e a c h b i t is r e p r e s e n t e d ( s p r e a d ) b y 8 b its ( c h ip s e q u e n c e ) .

77

11 11 11 O u tp u t of ü i t e g i 'a t o r 11 00 00 00 00 00 00 00 00

I n t O u t = (8, -8 , -8 ): I t i s n o w e a s y t o d e t e r m i n e t h a t th e r e c o v e r e d s i g n a l o n c h a n n e l A is:

Recovered Signal oil Cliaimel A

0

0

5.2 Matlab Implementation of WCDMA
S ix M atlab files im plem ent the process o f W C D M A . The main file "A g g r e g _ o n e .m " that contains calls for the routines called at various tim es during the e x e c u tio n o f the program . It first calls the routine that create the W elsh matrix, W g . N ext, it p erfo rm s the D S S S m odulation. Then, the chip seq u en ces for the channels are extracted fro m th e W alsh m atrix (row s 3 and 7). An aggregate signal is created next by perform ing the A + B m atrix operation. The noise is created and added to the aggregate signal that is at th is p oin t ready to be transmitted through the air. A t the receiver end the received sig n a l is m u ltip lied w ith both the chip sequence A and chip sequence B. The all elem en ts o f th e resu ltin g seq u en ces are added up and a received bit is recovered (extracted). The M atlab fu n ction " W elsh _M atrix.m '' that creates a W elsh matrix with the size 8 x 8 . The M atlab fu n ction "Com p le ment, m" that creates the com p lem en ts o f the chip seq u en ces and B . T he M atlab function "D S S S .m " that perform s the Direct Sequence Spread S p ectru m o n both channels. The Matlab function "C D M A _H ncode.m ' in w hich each bit o f d ata is represented (spread) by the chip seq u en ce. A 1 is replaced by the chip seq u en ce w h ile a 0 is replaced by the com plem ent o f the chip seq u en ce. And the last one, "C D M A D e c o d e .m " M atlab function in w hich the received signal is m ultiplied w ith the A

78
chip seq u en c e s and the resulting sequences are added up. The decision w hether a I or a 0 is receiv ed is m ade on th e follow ing way: if the value obtained from the integrator is g re a te r o r eq u al to 4, a bit I is recovered. Sim ilarly, if the value is less or equal to -4, a bit 0 is rec o v e red . T he values obtained betw een -4 and 4 give us the m eaning that that part o f th e receiv ed signal is n o t m eant for that particular channel; A t th e end o f th e d iscu ssio n about W CD M A let's take a look how the M atlab code w orks w ith the sam e input co n ditions as in the exam ple above; bits on the channel A; 1 0 0, bits and th e c h a n n el B: 0 1 0. N o te that the M atlab code im plem ents the D SSS m odulation w h ile th e ex am p le ab o v e did not. >> a g g reg _ o m
O u tp u t f r o m C o r r e la to r ( c h a n n e l A ): 0 0 - 1 0 - 2 - 2 -2 -2 O u tp u t f r o m I n te g r a t e d o u tp u t: -9

O u tp u t f r o m C o r r e la to r (c h a n n e l A ): 2 2 2 2 0 0 0 0 O u tp u t f r o m I n te g r a t e d o u tp u t: 8

O u tp u t f r o m C o r r e la to r (c h a n n e l A ): - 2 - 1 - 2 - 2 0 0 0 0 O u tp u t f r o m I n te g r a t e d o u tp u t: - 7 O u tp u t f r o m C o r r e la to r (c h a n n e l B ) : 0 0 -1 0 2 2 2 2 O u tp u t f r o m I n te g r a t e d o u tp u t: 7 O u tp u t f r o m C o r r e la to r (c h a n n e l B ): 2 2 2 2 0 0 0 0 O u tp u t f r o m I r ite g r a te d o u tp u t: 8 O u tp u tf r o m C o r r e la to r (c h a n n e l B ): - 2 - 1 - 2 - 2 0 0 0 0 O u tp u t f r o m I n te g r a t e d o u tp u t: -7 Sent on channel A : 10 0

R eco vered on channel A : 1 0 0 Sent on channel B: 0 1 0

R ecovered on channel B: 0 1 0 C o m p a r e d a ta s e n t & re c o v e r e d on ch a n n e l A : 0 C o m p a r e d a ta s e n t & r e c o v e r e d o n ch a n n e l B : 0

W e see th a t at the receiver the data for both channels A and B is recovered p ro p erly . O n e m ig h t notice th at in the exam ple above th e values out o f the integrator are

79

alw ay s either 8 o r -8, w hile from the M atlab printout we can see that the values out o f the in te g ra to r are not alw ays eith er 8 or -8. This is due to the fact that the M atlab sim ulation m ix es th e a g g reg ate signal w ith random noise before the corrupted signal is decoded at th e re c e iv e r end. In th e exam ple above w e assum ed that the transm ission m edium w as n o ise le ss. T h e sam e M atlab file "aggreg one.m " used for the m ain sim ulation was used fo r th e exam ple.

80

Chapter 6: RESULTS
In o rd e r to sim ulate th e operation o f the source-channel codec, tw o signals are tra n sm itte d th ro u g h th e system in Figure 1. On the channel A, a 4.99 second long speech file, sp e ec h .w av , saying: "T he D iscrete Fourier Transform o f a real valued signal is co n ju g a te sy m m etric" , is applied. On the channel B, the image file, `w orld.jpg', rep re sen tin g a g ray level still im age w ith the resolution 102 x 132 is applied. Both signals are show n bellow : speech.wav

woild jpq

2 3 time (sec.] F igure 16: Speech D ata.

4Ü

resolution [pixelf

G Ü

B IJ

Figure 17: Still Im age Data.

A t th e F o rm a ttin g stage, speech signal on the Channel A is first sampled at the sam pling fre q u e n c y o f 22050 H z w ith the help o f `w avread' M atlab com m and. The sam pled v e rsio n o f th e speech signal contains 1 10,033 sam ple points. In order to explain how th e se n u m b ers a re o btained, w e first plot the frequency spectrum o f the speech signal as sh o w n in F igure 18. F rom F igure 18 we see that the m axim um frequency oi the signal is 1 1,025 H z. A p p ly in g th e N y quist Sam pling Rate criteria w hich states that the sam pling fre q u e n c y o f an an alo g signal m ust be equal or greater than tw ice the m axim um fre q u e n c y o f th e signal, it is clear w hy the sam ple rate o f the speech signal is 22,050 Hz. S ince th e n u m b er o f sam pled points in one second is equal to the sam pling frequency, we c o n c lu d e th at the sam pled version o f the speech signal w ould have 22,050 sampled points fo r o n e seco n d . W h en th at num ber is m ultiplied w ith 4.99, which is the total duration o f th e sp e e c h signal, w e g et 110,033 sam pled points for the entire speech signal. In further d isc u ssio n th e sam pled version o f the speech signal w ill sim ple be referred to as sam pled signal.

81

1800
1600 1100 1200 1000 800 600 100 200 0 1 1 6 B Frequency [H z] 10 12 X10 . i l l

Figure 18: Frequency Spectrum o f Speech Signal. N e x t, th e sam pled signal is quantized to 256 levels since a speech signal requires 8 bits to re p re se n t each sam ple. The m axim um value o f the sam pled signal is 0.9921, w hile its m in im u m valu e is -0.7268. Consequently, the quantile interval q is 0.0067. Since the 8 bit q u a n tiz e r used in th is project is a linear quantizer, every level in the quantizer (e.g. 256 levels) w ill be o f the sam e size, the size o f the quantile interval q. A t this point w e obtain th e q u a n tiz ed signal as a result o f the quantization.

P D F o f 8 bit q u a n tiz a tio n error!

15000

,, 10000

-4

-3
e rror [e]

-2 X 10'

N o rm a liz e d P D F o f 8 -b it Q u a n tiz e r

m

0.15

error [ej

X 10

Figure 19: PDF o f Q uantization Error.

82
T o o b se rv e the q u a lity o f the quantization w hich is m easured through the quantization signal to noise ratio (S N R )q, w e first have to obtain the error caused by the quantization. T he e rro r is calculated as the difference betw een the sampled signal and the quantized signal. T h e p ro b ab ility density functions (PD F) o f the quantization error as well as its n o rm a liz e d v ersio n are show n in Figure 19. The quantization signal to noise ratio is the ratio betw een the signal p o w er and the noise pow er w here, in this case, the noise pow er is th e q u a n tiz atio n error. T he quantization error pow er can be found in tw o ways. The first o n e is to in teg rate th e p ro d u ct o f the squared difference o f the quantization erro r and the its m ean, w ith th e probability density function o f the quantization error (e.g. see the e q u a tio n (11) in C h a p te r 2). The second way, w hich is used in this project, is to calculate the q u a n tiz atio n erro r po w er as the mean o f the squared error vector:
n

error

pow er =

w h e re e is th e q u an tizatio n erro r vector and n is the size o f the quantization error vector. T o g et th e value in [dB]s: erro r pow er [dB] = 10 · log| q (e rro r_ pow er).

T h e sig n al p o w e r is obtained exactly the same w ay but instead o f the quantization error v e c to r, th e sig n al vector, containing the values o f the sam ples signal, is used. U sing M a tla b , th e quan tizatio n signal to noise ratio (SN R )q is then obtained to be: SN R = 30.9077 [dB], I f w e recall fro m C h ap ter 2; a quick approxim ation o f the (SN R )q for a quantizer could be m ad e by m u ltip ly in g the num ber o f bits that each sam ple is represented with by 6 dB. A p p ly in g th a t, th e theoretical value o f the (SN R )q in this project w ould be 48 dB. T he reason w hy w e obtain m uch low er the (SNR)q is due to the fact that we have used a lin ear q u an tizer. In general, linear quantizers are w asteful for speech signals since m any o f th e q u a n tiz atio n levels are rarely used. To support this claim a histogram given in F ig u re 20 show s the frequency o f usage for each o f 256 levels for our 8-bit uniform q u an tizer. It can be seen th at the m ajority o f the quantization levels used in the process falls in th e fo llo w in g range [96, 125] o f quantization levels. We also see that the q u a n tiz atio n level 111 contains 9,164 sampled points w hich m akes alm ost 10 percents o f

83
all sam p led points in the sam pled signal. A non-uniform quantizer w ould provide a better q u a n tiz atio n and the quantization error w ould be m ade proportional to the signal size. T he overall (S N R )q w ould im prove by reducing the error for the predom inant parts o f the signal, a t th e expense o f an increase in error for the rarely occurring parts o f the signal.
Usage of Quantizer Levels

10000
9000 8000 7000
eooo

X = 111 Y =9164

5000 I 4000 3000

2000 1000
-50

200

250

300

Figure 20: Frequency o f Usage o f Q uantizer Levels. T he last p a rt o f form atting the speech signal is the Pulse C ode M odulation, w here the q u a n tiz a tio n level for each elem ent o f the sam pled signal veetor is eonverted to its binary re p re se n ta tio n . H ere a sim ple decim al to binary conversion is used w here the quantization level (d e c im a l num ber) for each signal elem ent is eonverted to 8 bit binary num ber. The o b ta in e d b in ary values are appended into a binary sequence that represents the digitized sp e e c h d ata. T he data on the C hannel A consists o f 880,264 binary digits (8 x 1 1 0 ,0 3 3 ). O n th e C h an n el B, the form atting stage for the im age is m uch sim pler com pared to that o f th e sp eech signal. W e only have to calculate the PDF o f the image. The im age is read into th e M a tla b 's environm ent in the ASCII form at. The PD F o f the image, show n in Figure 2 1 , w ill g iv e us the frequency o f occurrence for each o f 256 num bers that the intensities o f th e im a g e 's pixels are represented with. In the process o f calculating the probability o f o c c u rre n c e fo r each num ber in the [0, 255] range, any num ber in the [0, 255] range that d o e s not a p p e a r in the image m atrix is considered as irrelevant since it carries no in fo rm a tio n rele v an t to our image. Onee the frequency o f occurrence o f each num ber in th e im age m atrix is calculated, it is divided by the total num ber o f elem ents in the im age m atrix , fo rm in g the probability o f occurrence for each num ber in the [0, 255] range that

84
ap p e ars in th e m atrix. T he x-axis lists num bers in the [0, 255] range show ing all 256 p o ssib le in ten sities o f the im age. The y-ordinate lists the probabilities o f occurrence for th e intensity o f each p ix el th at is present in the image.

0.03

N o r m a l i z e d P D F of t h e I m a g e

0.025

0.02
0.015 -

0.01
0.005 -

50

100

150

200

250

300

pixel i n te n sity

Figure 21; PD F o f the Image. T h e nex t stage in th e system for both Channels is the source encoder stage. C hannel A u se s th e L em p el-Z iv -W elch algorithm to perform the source encoding; to com press the d ig itiz e d sp eech signal as m uch as possible. The principles o f the Lem pel-Z iv-W elch a lg o rith m are explained in the C hapter 3 and here only the L em pel-Z iv-W elch table is p rese n ted as the result o f the M atlab sim ulation. Since the table has 45705 entries, it w ould be im possible to list all o f them here in this report. However, the first 50 entries w ill be listed, th u s a reader can get a feeling how the algorithm has perform ed the o p e ra tio n . D uring th e im plem entation o f this algorithm , Fve taken a liberty to change the w a y th a t th e co d e w ord colum n is im plem ented. Instead o f having a fixed-length im p lem e n ta tio n o f the code w ord (12 bits per code w ord), I 've used a variable-length im p lem en tatio n . In the real w orld this change w ould require a bit more com plex decoder. T h e so ftw a re im plem entation o f the decoder only requires know ledge o f how m any bits are used to e n co d e every single code word. This is solved by adding another colum n in th e L em p el-Z iv -W elch table called code w ord length that provides the inform ation to the d e c o d e r ab o u t how m any bits are used to encode each code word. Let s take a look at the first 50 en tries o f the L em pel-Z iv-W elch table obtained through the M atlab sim ulation.

85
M EM ORY L O C A T IO N I 2 3 4 5 6 7 8 9 10 II 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 M EM ORY R EPRESEN T 0 1 1 2 2 3 5 7 3 9 4 11 6 12 14 11 8 13 10 19 18 8 17 15 21 16 15 23 20 24 1 17 30 20 33 32 9 31 31 12 35 40 34 CODE W ORD
.

SU BSEO U EN CES 0 1 01 10 11 010 110 1101 011 0110 101 1011 0101 10110 101101 1010 11011 01011 01101 011011 010110 11010 110110 1011011 0101101 10101 1011010 1101101 0110110 10110111 00 n o m 101101111 0110111 1011011110 1101111 0111 000 001 10111 10110111101 101110 01101110

W ORD LEN G TH

11 100 101 110 1010 1111 111 10010 1001 10111 1101 11000 11101 10110 10001 11011 10101 100111 100100 10000 100010 11111 101011 100001 11110 101111 101000 110001 10 100011 111101 101001 1000010 1000001 10011 111110 111111 11001 1000111 1010000 1000100

2 3 3 3 4 4 3 5 4 5 4 5 5 5 5 5 5 6 6 5 6 5 6 6 5 6 6 6 2 6 6 6 7 7 5 6 6 5 7 7 7

86
44 45 46 47 48 49 50 011011100 1101110 0110111001 1011100 11011100 110111001 101111 43 32 44 42 45 48 40 1010110 1000000 1011001 1010100 1011010 1100001 1010001

O n ce th e tab le is com pleted, the code w ords from the fourth colum n are appended to each o th e r c re a tin g a binary sequence; a new com pressed binary representation o f the speech d ata. T h e new binary sequence contains 684,407 binary digits, w hich m akes the c o m p re ssio n ratio o f 22% for the speech signal. W hat is interesting for the L em pel-Z ivW e lc h a lg o rith m is th at w hen the input sequence is longer, the com pression ratio is better (h ig h e r). O n the ch an n el B, the im age data is source encoded with H uffm an C oding A lgorithm . As it has been m entioned in C hapter 3, H uffm an C oding A lgorithm provides a system atic p ro c e d u re for constructing uniquely decodabie variable-length code w ords that are e fficie n t in the sense that the average num ber o f bits per source is m inim ized. Since w e h av e 2 5 6 p o ssib le intensities fo r each pixel, it m eans that w e have 256 (from 0 to 255) p o ssib le sy m b o ls in the source alphabet. Here is the result o f the M atlab sim ulation w hich a ssig n s a v ariable-length code w ord to each o f these 256 sym bols as per discussion in C h a p te r 3. T he alphabet sym bols and their respective binary representations are listed in th e left-to -rig h t up-to-dow n m anner. 80=1001111011 814=1001011001 819=10001110100 823=10001110110 827=10100101 831=11110001 835=1001011100 839=10100111 843=10001101 847=10111001 851=11110010 855=10111011 859=100000110 863=10001000 83=10001110001 815= 1001011010 820= 100111110 824=1011001 828= 110110010 832=110110011 8 3 6 = 1 0 0 0 1 111000 840= 1010110 844=100000101 848= 10101000 852=11110011 856= 10111100 860= 10000110 864= 1111101 87= 10001110010 817=10001110011 521=101100011 525=100111111 529= 10001010 533=10001011 537= 10100110 541= 100000100 545=10000101 549= 10111010 553= 10000000 557= 1101110 561=10000111 565=10111101 513= 1001011000 518=1001011011 522=10001110101 826=100000010 830=10001110111 834= 10111000 838=100000011 842= 10001100 546=1111111 550=1101101 854=1011111 858=10101001 862= 11110100 866= 111001000

87

8 6 7 = 1 00 0 1 0 0 1 871=10101010 875=111001100 88 0= 1 0 1 0 0 0 0 0 1 884=11110110 888=10001111010 89 4= 10 0 0 1 1 1 1 1 0 1 89 8= 11 10 10 0 0 1 8 1 05 =1 11 01 00 11 8110=1001100100 8 1 14 =1 11 01 01 0 1 8119=1001100110 81 23 = 1 0 0 1 0 0 0 0 0 1 1 8131=1001101000 8135=111011000 8 1 40 =1 00 10 00 10 01 8144=10010001100 8149=10010001110 8155=1001101110 8 1 6 0 = 11 10 11 01 1 8166=10010010010 8173=10010010100 8 1 7 7 = 1 00 10 01 01 11 8 1 8 3= 10 01 11 00 11 8 1 90 =1 00 10 01 11 01 8 1 9 7 = 1 00 10 01 11 11 8 2 02 =1 11 01 11 01 8 2 0 9 = 11 10 11 11 1 8 2 1 5 = 1 00 10 10 01 01 8 2 2 2 = 1 00 10 10 01 11 82 27 =1 00 11 10 11 1 8236=10010101010 8240=10010101110 8 24 4= 10 10 11 1 8248=111000 8253=1001111010

868= 111001001 872=1001011101 876=1001011110 881=10001111001 885=111001110 889=10001111011 895=10001111110 8100=1001100010 8107=10010000000 8111=1001100101 8115=100001000 8120=11111000 8124=101000100 8132=10010000101 8136=10010001000 8141=1001101010 8145=111011010 8150=1001101100 8157=101000110 8161=10010001111 8169=10010010011 8174=1001110010 8180=10010011000 8185=1001110100 8191=1001110101 8198=10010100000 8204=111011110 8210=10010100011 8217=11111001 8223=101100010 8228=111100000 8237=10010101011 5241=111100001 8245=10101011 8249=10110000 8254=1100

869=111001010 873=11110101 877=11011000 882=1001100000 886=111001 111 890=1 11 010000 89 6 =1 00 00 01 11 8101=10001111111 8108=10010000001 8112=111010100 SI 16=101000011 8121=111010110 8126=10010000100 8133=10010000110 8137=1001101001 8142=10010001010 8147=1001101011 8151=1001101101 8158=1001101111 8164=10010010000 8170=111011100 8175=10010010101 8181=10010011001 8186=10010011011 8194=101000111 8199=10010100001 8205=10010100010 8211=100001001 8218=10010100110 8224=10010101000 8233=1001111000 8238=10010101100 8242=10010101111 8246=1101111 825 1=] 101 0 8255=0

870=101000000 874=111001011 879=1001011111 883=111001101 887=101000010 892=10001111100 897=1001100001 8103=111010010 8109=1001100011 8113=10010000010 8117=11110111 8122=1001100111 8130=111010111 8134=10010000111 8138=111011001 8143=10010001011 8148=10010001101 8154=101000101 8159=1001110000 8165=10010010001 8171=1001110001 8176=10010010110 8182=10010011010 8187=10010011100 8196=10010011110 8201=101001000 8208=101001001 8213=10010100100 8219=1001110110 8226=10010101001 8234=1001111001 8239=10010101101 8243=10001110000 8247=1111110 8252=101101

88
A verage C ode Length: 3.482 E ntropy o f the S ource H (x)= 3.42933 Efficiency = 98.4872% A t th is p o in t in th e system , the image data is represented by 8,705 binary digits. The e ffic ie n c y as th e ratio betw een the entropy and the average code length is 98.48% , w hich is c o n sid e red to be an alm ost m axim um com pression. T he fo llo w in g stage in the source-channel codec system is the channel encoding. As d iscu ssed in C h a p te r 4, the purpose o f the channel encoder is to introduce, in a control m an n er, som e red u n d an cy in the data binary sequence that can be used at the receiver to o v e rc o m e th e e ffe c ts o f noise and interference encountered in the transm ission o f the signal th ro u g h the channel. In other w ords, redundancy in the data binary sequence aids th e re c e iv e r in d eco d in g the desired data sequence. The Channel A uses the Linear Block C o d e alg o rith m to add redundant bits to the data binary sequence obtained from the so u rc e e n c o d er stage. The (12, 8) type o f L inear B lock Code is used, m eaning that every 8 b its from th e d a ta binary sequence will be transform ed into a 12 bit sequence, thus ad d in g 4 red u n d an t bits. To show the ability o f the code to detect and correct errors, as p e r th e d isc u ssio n in C h a p te r 4, a B it E rror Rate (B ER ) versus Signal to N oise Ratio (S N R ) p lo t is obtained and show n in Figure 22.
vs.

BER

SNR

m 10'

SNR[dB]

F igure 22: B E R vs. SN R for (12,8) Linear B lock Code.

89

T he B it E rro r R ate is defined as the num ber o f errors not being corrected by the decoder d iv id ed by th e total num ber o f bits in the input sequence to the L inear B lock C ode system . T h e S ignal to N oise R atio is, as before, defined as the ratio betw een signal pow er and th e n o ise pow er. F our points are plotted on the graph obtained for four dilTerent v a lu e s o f th e noise. F rom the plot we see that the first uncorrected errors occur while the S N R is still relativ ely high (~33 dB). This w ould lead us that the code is not sufficiently c a p a b le o f p erfo rm in g e rro r corrections since the B E R vs. SN R curve from the reference [2, pp 267] starts having first undetected errors around 10 dB. T he reaso n fo r the early appearance o f uncorrected errors lies in the fact that the Linear B lo c k C o d e ( 1 2 x 8 ) used in the sim ulation can detect and correct only 2 n-k
- 1 = 15

e rro r pattern s (see C h ap ter 4). That m eans that only 12 single bit error patterns plus three 2 b it e rro r patterns can be detected (in the project a single bit erro r patterns are detectable and c o rre c ta b le only). To im prove the perform ance o f the code, m ore error patterns m ust be d e te c te d and corrected w hich requires the longer tim e execution o f the code (it takes 5 h o u rs fo r M atlab to generate ( 1 2 x 8 ) co d e 's B E R vs. SN R). H ow ever, in the real w orld o f c o m m u n ic a tio n s errors happen approxim ately 1 in 10^ transm itted bits and the p o s sib ility o f h av in g 2 errors in a row are very rare. H ere are the m ain m atrices obtain in th e M atlab sim u latio n for the (12, 8) Linear B lock Code:
P = [ 1

1 1 1 1 0 0 0

0 1 0

1

0

G = [ 1

0

1

0 1 0 1 0 1 1

0 1 1 0 1 1 0 ];

1 1 1 1 1 1 0 0 0

1
0

0
1

0 0
1

1
0

0
1

0 0
0

0 1 1 1 0 1 1

0 0

0 0

0 0 0 0 0 0 0 0 0

0

0 0 0
1

0

0 0 1
1

0 0 0 0
0

0 0
0 0 0

0 0 0 0
0

1 1

0 0 0 0 0 0 0
I

0

0

0

0

0 0 0

0 0

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 ];

Ht = [ 1

0 0 0

0 1 0 0 0 1 0

0 0
1

0 1 0
1

0 0 0 1 0 0
1 1

0
0 0 0
1 0 1
1

0
1 1

0 ];

90
[ 0 G G G G 1 o G 1 G G G G G G G G G G G G G G G G G G G G
g o

1
G G G G G G G G G G G

G G
g g

G G
g g

G G
g g

G G
g g

G G
g g g g

G G
g g

G G
g g

G G

1 G G G G
o

G 1 G G G
g

G G G G G G G 1 G G G 1 G G G 1
g g g i

G G G G G
g

G G G G G
g

G G G G G
g

Se = [ G 1 G G G

G G G G G G G G G

G G G

G G G

G G G

G G G

G G G G G G

]

G G

G 1 G

1 ];

G G 1 G G G 1 G 1 1 1 1 1

G G G 1 G

G G G G

1
G G

1 G

1
G
1

1
1
G 1 1

G

1

0 ];

T h e C h an n el B applies a different algorithm for erro r detection o f the (2,1,4) C o n v o lu tio n a l C ode. It is called the V iterbi algorithm . For this algorithm th e m easure o f e ffic ie n c y o f th e co d e cannot be established through the BER vs. SN R since the V iterbi a lg o rith m g u aran ties th a t any input sequence will be correctly decoded as long as the d e c o d in g d e la y is sufficien tly long. The decoding delay used in this project is 2G. The p rin c ip le s o f th e C o n v o lu tio n al C ode algorithm as well as those o f the Viterbi algorithm are fu lly p rese n ted in C h ap ter 4. T he last stage on the tra n sm itte r side is the W C D M A stage that consists from D irect S e q u e n c e S pread S pectrum m odulation and CD M A channelization (M edium A ccess C o n tro l). F irst, each signal is X O R -ed w ith different pseudo-random generated sequences and th a t each bit o f these new obtained signals is represented by the chip sequences. Tw o chip seq u en c e s o b tained from the W elsh m atrix are used. For Channel A: C _A =[+1 and fo r th e C han n el B: c_B = [+ i +1 -1 -1 -1 -1 +1 + 1 ]; +1

+1

-1

- 1 ];

an d th e n b oth signals are added. O ne aggregate signal is transm itted through the air m ed iu m . T o sim ulate the noise and interference during the transm ission th rough the airy c h a n n e l a ran d o m sequence o f Is and Gs is produced and added to the transm itted signal. T he n o ise is generated by using the follow ing M atlab com m and:

noise = r a n d s r c (1,len_trans,[0,1;p_0,p _ l ] );

91
w h ere p_0 is the probability o f occurrence o f 0 w hile p i is the probability o f occurrence o f 1. T h e v ariab le l e n t r a n s is the length o f the transm itting aggregated signal. In the sim u latio n , p_0 and p i are set to 98% and 2% respectively. The 2% occurrence o f ones in th e noise g ives us approxim ately 60,000 errors that are to be corrected. A t tb e rec e iv e r side, the received corrupted signal is first m ultiplied w ith both chip se q u en c e s, C A and C B, in order to extract the signal m eant for the C hannel A as well as fo r th e C hannel B. The error correction is perform ed as w ell as per discussion given in C h a p te r 5. I f not all errors are detected and corrected during this stage, the rem aining o n es w ill be co rrected during the channel decoding stage (L inear Block C ode decoder for th e C h a n n e l A and C onvolutional C ode decoder for the C hannel B). A fter separating the rec e iv e d signal to tw o C hannels A and C hannel B signal, each o f these signals is X O R -ed w ith th e p seu d o generated noise used at the transm itter end (both pseudo generated se q u en c e s are know n in advance to the receiver). At this point, we have tw o received sig n a ls th a t represent the received speech and still im age data. The decoding principles and ca p ab ilitie s o f L inear B lock C ode and C onvolutional C ode algorithm are presented and d iscu ssed in C hapter 4 and will not be discussed here again. A reader is encouraged to read C h a p te r 4, in o rd er to understand the decoding principles o f the both channel d e c o d e rs. A t the source decoding stage, the speech and image data are reconstructed u sin g L em p el-Z iv -W elch and H uffm an Code tables to their analog equivalents and the b o th sig n als are displayed. O nce again, the coding tables are know n to the decoder in a d v a n c e for both techniques. The exception is the adaptive H uffm an tree decoding, w here th e e n c o d e r and d eco d er only share the know ledge about the num ber o f sym bols in the a lp h a b e t.
R e c e iv e d S p e e c h S ig n a l Received Im age

------~r -i------ 1

20
ti m e s e c .

40

60

100

120

F ig u re 23; R eceived Speech Data.

Figure 24: Received Image.

92
A s th e v ery last step on C hannel A, the received digital speech signal is converted back to its a n a lo g rep resen tatio n and is played in M atlab using the `w avplay' com m and. The receiv ed speech signal plotted in the tim e dom ain is shown in Figure 23. The received im age is show n in Figure 24.

Conclusion
T he pro ject presents a source-channel codec for a W CD M A based m ultim edia system . T he sy stem c o n sists from tw o Channels, A and B, w here the Channel A is reserved for tran sm ittin g /re ce iv in g a speech signal w hile the Channel B transm its/receives a still im age data. O n the tran sm itter end, the Channel A has the follow ing stages: form atting (sa m p lin g , q u antizing, and PC M ), source encoding (Lem pel-Ziv-W elch Encoder), ch annel e n c o d in g (L inear B lock C ode Encoder), and W CD M A m edium access control (D S S S plus C hip S equencing) stage. On the other side, the Channel B has these stages at the tra n sm itte r: form atting (A SC II), source encoding (H uffm an Code Encoder), channel e n c o d in g (C o n v olutional C ode Encoder), and W CDM A m edium access control (D SSS plus C h ip S equencing) stage. A t th e re c e iv e r end, firstly, the received aggregate signal is divided into two signals; the sig n al m ean t for the C hannel A and the signal m eant for the Channel B. The signal m eant for th e C hannel A (speech signal) goes through the follow ing stages; channel decoder (L in e a r B lo ck C o d e D ecoder), source decoder (L em pel-Ziv-W elch D ecoder), and digital to an alo g conversion. The signal that is selected for the C hannel B goes through; channel d e c o d e r (C onvolutional C ode D ecoder) and source decoder (H uffm an C ode D ecoder). T h ro u g h o u t th e project 1 have faced with quite a few challenges in im plem enting the c o d ec. T he b iggest one w as the tim e o f the execution. M atlab as a language o f choice for th is p ro je c t has not had the needed speed capabilities. Som e im plem entations o f the alg o rith m s such as L em pel-Z iv-W elch encoding and C onvolutional C ode decoding have ta k e n w a y to o m uch tim e, approxim ately 2 days for each sim ulation! An interested stu d e n t w ho w ould have patience and w illingness to update this project should choose a p ro g ra m m in g language that is based on the concurrent type o f code execution instead o f u sin g a language based on sequential type o f code execution. The sequential languages su ch as M atlab execute a code in a line by line fashion, w hile the concurrent languages such as V H D L execute several lines o f code at the sam e tim e (the analogy w ould be as if a c o m p u te r had several processors that are capable o f executing the lines at the sam e tim e). T h a t's w hy a concurrent language would be m ore appropriate for this type o f p ro je c t. T he ideal solution w ould be to im plem ent this project in hardw are.

93

A n o th e r recom m endation to im prove this project is not to use L em pel-Z iv-W elch a lg o rith m for a speech signal. B esides the fact that the execution o f its sim ulation is too slow , a n o th e r problem is th at a Lem pel-Z iv-W elch code w ord can contain 12 bits only. I f a b in ary d a ta sequence is too long, as they usually are, there is a need to create m ore than o n e tab le w hich takes additional space in the m em ory. The better solution w ould be to use a d ap tiv e H uffm an C ode algorithm to com press a speech signal. T h e e rro r detecting and correcting capabilities are another thing that could be improved. A L in e ar B lo ck C ode that w ould detect and correct as m any bit error patterns as possible is n eeded (in the real w orld (127, 92) linear block code is used). As stated earlier in this c o n c lu d in g section, a program m ing language that can execute the code faster m ust be used (e.g. C + + ) to im prove the speed o f detecting and correcting processes. T o sum up, the project show s all necessary stages that are needed for transm itting and rec e iv in g m ultim edia signals th rough a digital system . C om plex m ultim edia system s m ay h av e m ore stages than the system show n in this project does, but the stages presented and im p lem en ted in th is project m ake a foundation for all digital system s and absolutely can n o t be excluded as a part o f their im plem entations.

94

References:
1] F. G. Strem ler, I n t r o d u c t i o n to C o m m u n i c a t i o n S y s te m s ., Add id son - W esley Series in E lectrical E ngineering, 1997. 2] B. Sklar, D i g i t a l C o m m u n i c a t i o n s : F u n d a m e n t a l s a n d A p p l i c a t i o n s . , PTR P rentice H all, 1998. 3] J. G. P roakis, D i g i t a l C o m m u n ic a tio n s ., M cG raw Hill, 2001. 4] T. S. R appaport, W ir e l e s s C o m m u n ic a tio n s : P r i n c i p l e s a n d P r a c t i c e ., Prentice H all, 2002. 5] A . L. G arcia, I. W idjaja, C o m m u n ic a tio n N e t w o r k s : F u n d a m e n t a l C o n c e p t s a n d K e y A r c h i t e c t u r e s . , M cG raw Hill, 2003. 6] M . A b ram ovici, M. Bauer, D i g i t a l S y s t e m s T e s ti n g a n d T e s ta b le D e s i g n ., W iley In te r Science, 1990 7] W . W . W u, E l e m e n t s o f D i g i t a l S a te llit e C o m m u n ic a tio n ., C om puter Science Press, 1995. 8] R. J. S luyter, "D igitalization o f Speech" in P h i l i p s T e c . R e v ., vol. 41. pp. 201-221. 9] N . S. Jayant, D i g i t a l C o d i n g o f W a v e fo r m s ., Prentice Hall, 2000. 10] T. J. L ynch, "D ata C om pression Techniques and A pplications", in L i f e t i m e L e a r n i n g P u b l i c a t i o n s , Belm ont, Calif., 1985. 11] J. M . T ribolet, " Speech Coding", in I E E E T r a n s a c t i o n s o n C o m m u n ic a tio n s , vol. 27, no. 4, A pril 1979, pp. 710-737. 12] W . W . Peterson, E r r o r C o r r e c tin g C o d e s ., M IT Press, 1972. 13] G. C. C lark, E r r o r C o r r e c t i n g C o d i n g f o r D i g i t a l C o m m u n ic a tio n s ., Plenum P ress, 1981. 14] R. G. G allager, I n f o r m a t i o n T h e o r y a n d R e l i a b l e C o m m u n ic a tio n ., John W iley & Sons, 1968. 15] T he S tu d en t Edition o f M A TLA B, T h e U s e r 's G u id e ., The M ath W orks Inc., 1995 16] A . J. Viterbi, C D M A : P r i n c i p l e s o f S p r e a d S p e c t r u m C o m m u n ic a tio n ., A d dison - W esley R eadings.

95

[17] "C o d in g T ech n iq u es for Digital C om m unications," class notes for EE8110, D ep artm en t o f E lectrical and C om puter E ngineering, R yerson U niversity, W in te r 2005.

96

Appendix A: Matlab Simulation Files

97
%%%%%%%%%%%%%%%%%%%%%%

% M-File : "Main.m" ^ % AUTHOR: Boris Backovic % % page: l/l % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% clear all; close all; % FORMATTING STAGE % %Channel A Sampling; Quantizing; PCM; %Channel B Formatting; % SOURCE ENCODING % %Channel A LZW_Encode; %Channel B Image_Huf fm a n ; % CHANNEL ENCODING % %Channel A Generate_Matrices ; LBC_Encode; %Channel B ConvolEncoder; % WCDMA Medium Access Control % aggreg_one; % CHANNEL DECODING % %Channel A LBC_Decode ; %Channel B Viterbi_Decoder; % SOURCE DECODING % %Channel A DtoA; %Channel B Decoder_Image_Huf fman;

98
;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % M-File : "Sampling.m" ^ % AUTHOR: Boris Backovic % % page: 1/1 %
e o o o o o o o o o o o o o o o o o o o o ^ "b "o ^ "6 "

clear all; close all; display('IN_Sampling.m' ) [sig, f s] =wavread{ 'f ilel.waV ); wavplay(sig,fs); t = (1 :length(sig))/fs ; plot(t,sig); grid; title('ASCII Version of The Speech Signal'); x label('time [sec.]'); yl a b e l ('voltage [V] ' ); drawnow; F_sig=fft(sig,num_fft_pnt) ; f igure; plot (abs (F_sig ( 1 :num_f ft_pnt/2 )) ) ; grid; title('Spectrum of the Speech Signal'); x l a b e l ('Frequency [Hz] ') ; y l a b e l ('|sig|'); drawnow; figure ; semi logy (abs (F_s ig ( 1 : num_f ft_pnt / 2 ))) ; grid; title('Log-magnitude Spectrum of the Speech Signal'); xlabel('Frequency [Hz] ') ; y l a b e l ('log_l_0|sig|'); drawnow; s a v e ('Sampling.mat' , 'sig', 'fs ' ); display('OUT_Sampling.m ');

99
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^^^^^^^^^^^^^^^

% M-File: "Quantizing.m" % AUTHOR: Boris Backovic % page: 1/2

^ ÿ %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; close all; display('IN_Quantizing.m' ) loa d ('Sampling.mat' , 'sig' , 'fs ') levels = 2 5 6 ; sigmax = max(sig); sigmin = min(sig); q = (sigmax-sigmin)/levels ; parti = sigmax-q; partitionl= [sigmin:q:partl] ; codebookl = [sigmin:q:sigmax] ; [indexl,quant si] =quantiz (sig,partitionl, codebookl) ; sigl = sig '; errorl = sigl - quantsi; f igure; subplot(211); hist(errorl); grid; title('PDF of 8 bit quantization error!'); x l a b e l ('error [e]'); ylabel (' [pdf (e) ] ') ; [yl, xl] = hist(errorl) ; yl = y l ./length(sig); subplot(212); bar(xl,yl,1); grid; title{'Normalized PDF of 8-bit Quantizer'); x label('error [e]'); ylabel (' [pdf(e)/length(sig) ] ') ; drawnow; wavplay(quantsi,fs); sig_power = io*loglO(mean(sigl. 2)); noise power = 10*log1 0 (mean(errorl. 2) ) ; SNR_q = sig_power - noise_power; fprintf ( '\nSNR = %g [dB] \n\n ',SNR_q) ; figure ; x 3 = (0 :1 :2 5 6 ) ; [yy, XX] = hist (indexl, x 3 ); bar(xx, yy, 1); grid;

100
;%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

M-File: "Quantizing.m" AUTHOR: Boris Backovic page: 2/2

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

title('Usage of Quantizer Levels'); x l a b e l ('qunatization levels'); y l a b e l ('frequency of occurrence'); save { 'Quantizing .mat ', 'indexl ', 'quants 1 ') display ('OUT_Quant izing. m ')

101
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % %

M-File: "PCM.m" AUTHOR: Boris Backovic page: 1/1 Containing M-Functions : 1. convert2binary . t n

% % % % %

' o ' O O O ' o ' O O O O O O O O O O O O O O O O O O O O ^ ' o ' O '

clear all; close a ll; display('IN_PCM.m' ) load ( 'Quantizing .mat ', 'indexl ', 'quant si ') clear cnt ; k=l ; f o r (cnt=l:1 :size(indexl)) c {cnt,1} - cnt; c{cnt,2} = quantsl(cnt); c{cnt, 3 } = indexl(cnt); c = convert2binary(c,cnt); end tran = c a t (2 , c{:, 4 }); c_PCM = C; save ( 'P CM.mat ', 'tran ') ; display('O U T P C M . m !) ;

102

i% % % % % % % % % % % % %% % % % % % % % % % % %% % % % % % % %

% M-Function: "convert2 binary.m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [c] = convert2binary(c, cnt) clear remndr; clear fixed_code_length; clear added_bits; clear to_add; remndr = 0 ; tmp_dec = c {cnt, 3 } ; if(tmp_dec == 0) c{cnt, 4 } = zeros(1 ,8 ); else if(tmp_dec == 2 5 6 ) tmp_dec = tmp_dec - 1 ; end while (tmp_dec ~= 1) remndr = [mod(tmp_dec, 2) remndr); tmp_dec = tmp_dec/2; tmp_dec = fix(tmp_dec); end remndr = remndr(1 : (length(remndr)-1) ) ; var_code_length = [1 remndr]; if (length(var_code_length) < 8) to_add = 8 - length(var_code_length); added_bits = zeros(1, to_add); fixed_code_length = [added_bits var_code_length]; else fixed_code_length = var_code_length; end c{cnt, 4 } = f ixed_code_length; end

103
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "Formatting.m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; close all; display('IN_Formatting.m') ; I = imread('a r r o w j p g '); A = 0 .2 9 8 9 *I(: , : ,1 )+0 .5 8 7 0 *I(: , :,2 )+0 .1 1 4 0 *I(:, :,3 ) ; f igure; colormap(gray ( 2 5 6 )); image (A) ; t i t l e ('Transmitted Image'); drawnow; [m,n] = size (A) ; clear i; for (i=l:l:m) if (i == 1) B = A (1 , :) ; else B = IB A (i , :)] ; end end [j ,k] = size(B); X = [ 0 :1 :2 5 5 ] ; [s,n]=hist(B,x); if (sum(s)~=(j *k)) display('ERROR: mismatched number of pixels\n') ; end ss=s./k; figure ; bar(n, ss, 1) ; ti t l e ('Normalized PDF of the Image'); grid; drawnow; z=l ; clear i ; for (i=l:1 :length(ss)) if (ss (i)~=0) p (z) = ss (i) ; index(z ) = i - 1; z = z + 1; end end . save ('Image_Format ting .mat index ) , display('OUT_Forma 1 1 ing.m ');

104
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "LZW_Encode.m" % AUTHOR: Boris Backovic % page: l/l % Containing M-Functions : % 1, subsequence.m % 2 . Table.m % 3 . tailing.m %%%%%%%%%%%%%%%%%%%%%%%%%%«-5-5-5-°-9clear all; display('IN_LZW_Encode.m') l o a d ('P CM.m a t ','tran'); sig = tran; c{l,l} = [1] c{l,2} = [0] c{2,l} = [2] c{2,2} = [1]

% % % % % % %

c = subsequence(c , sig); c = Table(c); Itransmition, decode_c, last_bits] = tailing(sig, c) ; save ('LempelZivWelch.mat ', 'c ', ' decode_c ', 'transmition' , 'last_bits I1 . display('OUT_LZW_Encode.m ') ;

105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % %

M-Function: "subsequence.m" AUTHOR: Boris Backovic page: l/l Containing M-Functions : 1. check_up.m
o © `S ' o ' o ' o ' b ' b ' b ' b ' o ' o ' b ' B ' Ç ' b ' © ' ^

% % % % %

'o 'o'O 'O 'o o ' o ' b ' o ' 6 o o ' o ' o ' b

function [c] = subsequence (c, sig) pnt = 1; k = 1; curr = 3 ; set_over = 'N' ; while (pnt <= length(sig)) if (pnt+k <= length(sig)) temp = sig(pnt:pnt+k) ; else temp = sig (pnt :length(sig) ) ; end check_res = check_up(c, temp); if (check_res == 'Y ' ) while (check_res ~= ' N' ) i f ((pnt+k+l) <= length(sig)) k = k + 1; temp = sig(pnt: pnt+k); check_res = check_up(c, temp); else check_res = ' N' ; set_over = ' Y' ; end end end if(set_over ~= 'Y') c{curr,l} = curr; c {curr,2} = temp; curr = curr + 1; pnt = pnt+k+1; clear temp; k = 1; else break; end end

106

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function : "check_up. m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [check_res] = check_up (c, temp) clear check_res; clear cnt; f o r (cnt=1:1 :length( c )) i f (length(c{cnt,2}) == length(temp)) i f (c{cnt,2} == temp) check_res = 'Y'; %fprintf('found at k=%d \n',cnt); break; end end check_res = ' N ' ; end

107
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % %

M-Function: "Table.m" AUTHOR: Boris Backovic page: 1/1 Containing M-Functions : 1. convert2binaryl.in

% % % % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function[c]= Table(c) c{l, 4 } = [ 0] ; c{2, 4 } = [1] ; clear cnt; f o r {cnt = length(c) :-1: 3 ) %fprintf('%d\n',cnt) ; tempi = c{cnt,2}; len = length(tempi) ; temp2 = tempi(1 :len-1) ; for(cntl=cnt-l:-1; 1) i f (length(c {cntl,2} ) == length(temp2 )) if(c{cntl,2} == temp2) c{cnt, 3 } = cntl; c = convert2binaryl(c, cnt); inovat ionbit = tempi(length(c{cnt,2})); c{cnt, 4 } = [c{cnt,4 } inovation_bit]; c{cnt, 5 } = length(c {cnt,4 }); break; end end end end

108

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "converting2 binaryl .m" % AUTHOR; Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [c] = convert2binaryl(c, cnt) clear remndr; clear fixed_code_length; clear added_bits; clear to_add; remndr = 0 ; tmp_dec = c {cnt, 3 }; if(tmp_dec == 0) c {cnt,4 } = 0 ; else while (tmp_dec ~= 1) remndr = [mod(tmp_dec,2) remndr]; tmp_dec : tmp_dec/2; tmp_dec = fix(tmp_dec); end remndr = remndr(1:(length(remndr)-1)); var_code_length = [1 remndr]; c{cnt, 4 } = var_code_length; end

109
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^%%^%%%%

% M-Function : "tailing.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [transmition, decode_c,last_bits] = tailing (sig, c) variable_rate = 0; for(k= 3 :1 :length(c)) variable_rate = variable_rate + length (c{k, 2}) ; end dodati = length (sig) - variable__rate; last_bits = sig(variable_rate+l:length(sig)); tail_flag = 'N'; clear c nt ; f o r (cnt=l:1 :length(c) ) if (length(c{cnt,2}) == length(last_bits)) if(c{cnt,2} == last_bits) tail_transmit = c{cnt,4 ); len_tail = length(tail_transmit); tail_flag = 'Y ' ; end end end transmition = c a t (2 , c { ( 3 :end) ,4 }) ; decode_c = c a t ( 2 , c{ ( 3 :end),5 } ); if(tail_flag == 'Y') transmition = [transmition tail_transmit] ; decode_c (length (decode_c)+1) = length (tail_transmit) ; end

110
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % % % % %

M-File: "Image_Huffman.m" AUTHOR: Boris Backovic page: 1 / 3 Containing M-Functions: 1- MakeStructHuff .m 2 . sorting_Huff-m

% % % % % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; close a l l ; display ( ' IN_Image_Huffman.m' ) ; l o a d ('Image_Formatting.mat' )
H = 0;

clear i; for {i=l:l:length{p)) X = log(p(i)); X = x/ 0 -6 9 3 1 5 ;
y = - p (i ) * x ;

H = H + y; end symbols = [1:1:length(p)]; m = length (p) ; code_dim = length(p); Huff = MakeStructHuff (symbols,p) ; sorted_Huff = sorting_Huff (Huff, m, code_dim) ; ushile (m > 1) last_s = sorted_Huf f .symbols (:,m) ; for (u=l:l :length (last_s) ) if(last_s(u,1)~= 0) for(g=l:1:codedim) if (Huff.codew(g,last_S(u,1 ))-= 0 ) else Huff.codew(g,last_s(u,1 ))= 1 ; break,end end end
e n d

eec_last_s = sorted_Huff .symbols (:, (m-1 )) ; for {u=l :1: length (sec last_s )) if(sec_last_s(u,1)-= 0) for( g=l:1:codedim) if (Huff.codew(g,sec_last_s(u,l) )-= 0 ) else Huff.codew(g,sec_last_s(u,X))= -1 ; break; end end end end
c l e a r t e n ç ) _ l a s t ;

Ill
%%^<%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^%%%%^%

% M-File: "Image_Huffman.m" % % AUTHOR: Boris Backovic % % page: 2 / 3 % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% clear temp_sec_last; last_prob = sorted_Huf f .probability (m) , · second_last_prob = sorted Huff. probability (m-1) ; new_prob = last_prob + second_last_prob ; new_Huff.probability(m- 1 ) = new_prob; for (s = l-.l-.TO-2) new__Huff .probability (s) = sorted_Huff .probability (s) ; end new_Huff-probability = new_Huff .probability (1 :m-l) ; for ( t = l ;1 ;code_dim) if (sorted_Huff.symbols(t,m)~= 0 ) temp_last(t)=sorted_Huff.symbols(t,m); end if(sorted_Huff.symbols(t,m-l)~= 0 ) temp_sec_last(t)=sorted_Huff.symbols(t,(m-1 )); end end new_temp= [temp_sec_last temp_last]; sorted_Huff .syTObols (1 :length (new_temp) , (m-l))= new_temp; new_Huff .symbols = sorted_Huff .symbols (1 :code_dim, 1 :m-l) ; m = m -1 ; sorted_Huff = sorting_Huff (new_Huff, m, code_dim); sorted_Huff.symbols ; end clear cnt; for(cntl=code_dim;-1:1) for(cnt2=l:1 :code_dim) source(code_dim-cnt1 + 1 ,cnt2 )=Huff.codew(cnt1 ,cnt2 ); end end for(cnt=l:1 :code_dim) S(cnt).source=source(;,cnt); S (cn t ) .source=nonzeros(S(cnt).source); for (h=l :1 :length (S (cnt) .source) ) if(S(cnt).source(h)==-1) S(cnt) .source(h)=0 ; end end S (cnt) .source = (S (cnt ).source) '; end average_length = 0 ; for (cntl=l:1:code_dim) Ik = length(S(cntl)-source); pk = p (cntl) ; 1 V\ average_length = (average_length + (Ik p ' end

112
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "Image_Huffman.m" % AUTHOR: Boris Backovic % page: 3 / 3

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear i ; clear k; for (k=l :1 :length(B) ) f o r (i=l:1: length(index) ) i f (index(i) == B(k)) if(k == 1) bit_stream = S(i).source; length_of_each_letter = length(S(i) .source); else bit_stream = [bit_stream S(i).source]; length_of_each_letter = [length_of_each_letter length(S(i).source)]; end end end end save ('Image_Huff man. mat ', 'bit_stream' , 'length_of_each_letter ', ' S ' ,'index','A ' ); di splay('OUT_Image_Huf fman.m ') ;

113

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "MakeStructHuff.m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [Huff] = MakeStructHuf f (symbols,p) m = length(p); H u f f .probability = p; for (j = 1 :1 ;m) Hu f f .num_bits(j) = 0 ; for(k=l:1 : r a ) H u f f .codew(k,j) = 0 ; H u f f .symbols(k,j)= 0 ; end end for (m=l:l:m) Huff.symbols (l,m)= symbols(m); end

114
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "sorting_Huff.m" % % AUTHOR: Boris Backovic % % page: 1/2 % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% function [new_Huff] = sorting_Huff (Huff, m, code_dim) X = m; t = m; if(m == code_dim) w h i l e (x >= 1) Temp_probab = Huf f .probability (m) , · Temp_Symbol = Huff.symbols(:,m ) ; for (w=l:l:TO-l) if(Temp_probab >= Huff.probability(w)) for(y=m-l:-1:w) Huff.probability(y+1 ) = Huff.probability(y); Huf f .symbols {: ,y+l) = Huf f .symbols (:, y) , * end Huff.probability(w) = Temp_probab; Huff .symbols ( ,w) = Temp_Symbol ; break; end end if(w == m-1) m = m - 1; end X = X - 1; end else T e m p ^ r o b a b = Huf f .probability (m) ; Temp_Symbol = H uff.symbols( : ,m) ; for (r=m-l:-l:l) if(Temp_probab < Huff.probability(r)) for(q=m-l;-1;r+1) H uff.probability(q+l) = Huff.probability(q); Huf f .symbols(;,q+ 1 ) = Huff.symbols(:,q ) ; end Huff-probability(r+1 ) = Temp_probab; Huff .symbols (: ,r+ 1 ) = Temp_SyTnbol ; break; end if (r == 1) f o r ( y = m - l ;-1:1) H u f f - p r o b a b i l i t y (y+1 ) = H u f f -probability(y); H u f f .s y m b o l s (:,y + 1 ) = Huff -s y m b o l s (:,y ) ; end H u f f - p r o b a b i l i t y (1 ) = Temp_probab; H u f f - s y m b o l s (:,1 ) = Temp_Symbol; br e a k

115

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

M-Function: "sorting_Huff.m" AUTHOR: Boris Backovic page; 2/2

% % %

end end end new Huff = Huff;

116

;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "Generate_Matrices.m" % AUTHOR: Boris Backovic % page: 1/2 % Containing M-Functions: % 1. convert2dec.m

% % % % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display {'IN_Generate_Matrices .m ') ; n = 12 ; k = 8; flag = 0 ; while(flag == 0) Ik = eye(k); P = zeros(k, n-k) ; w_P = n - k; P{ 1 ,:) = randint(1 ,w_P); while ((sum(P(1,:))==0) | | (sum(P(1,;))==1) ) P (1 , : ) = randint(l,w_P); end clear cnt; for (cnt=2:l:k) temp = randint(1 ,w_P) ; while ((sum(temp)==0) | | (sum(temp)==1)) temp = randint(1,w_P); end for (cntl=cnt-l:-1:1) P_flag = 0 ; while (P_flag == 0) if (temp == P(cntl^:)) P_flag = 0 ; temp = randint(1 ,w_P); while ((sum(temp)==0) | | (sum(temp)==1)) temp = randint(1 ,w_P); end else P_flag = 1 ; P (cnt,:)=temp; end end end end G = [P Ik] ; Ink = eye(n-k); Ht = cat (1 , Ink, P) ; all zero = zeros(1, n) ;

117
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File : "Generate_Matrices.m" % AUTHOR: Boris Backovic % page: 2/2

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

e = eye(n) ; e = c a b (1, all_zero, e); Se = mod(e*Ht, 2 ); s2_Se = size(Se); row_Se = s z_Se(1 ,1 ) ; for(i=l:1 :row_Se) Se_dec(i)=convert 2 dec(Se(i, :)) ; end Se_dec clear cnt; clear cntl; flag_ok : 1; for(cnt = 6 :1 :length(Se_dec) ) f o r (cntl=cnt-1 :-1:1) if(Se_dec(cnt) == Se_dec(cntl)) flag = 0; break; else flag = 1; end end if(flag == 0 ) break; end end end save ('Generate_Matrices.mat', 'n' , ' k
'H t ' , ' e ' , ' S e ');
' ,

'r o w S e ', '

I k ' , 'I n k ' , ' P ' , 'G' ,

display ( ' OUT_Generate_Matrices .m' ) ;

118

;% % % % % % % % % % % % % %%%%%%%%%%%%%%%%%%%%%

M-Function: "convert 2 dec.m" AUTHOR: Boris Backovic page: 1/1

% % %

;% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

function[d]=convert2dec (b) d = 0; k = length(b); f o r (cnt=l:1 :k) d = (b (cnt) * ( 2 ^ ^ (k-cnt) ))+ d; end

119
%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "LBC_EnGode.m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display('IN_LBC_Encode.m' ) l o a d {'Generate_Matri ce s .mat ', ' n ', 'k ','G ' ); load {'LempelZivWelch.mat ', 'transmit ion ') ; m = transmition; cc = 0 ; w h i l e (rem(length(m),8) ~= 0) m = [m 0] ; cc = cc + 1; end pnt = 1 ; indx = 1; w h i l e (pnt < length(m)) message{indx,l} = m (pnt:pnt+k-l) ; U{indx,l} = m o d (message{indx,l}*G,2 ) ; pnt = pnt + k ; indx = indx + 1 ; end transmit = c a t ( 2 , U{;,l}); s a v e ('BLC_Encode.m a t ' , 'transmit' , 'U', 'message' , 'm' , 'cc'); display('OUT_LBC_Encode.m ')

120

;% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

% % % % %

M-File: "ConvolEncoder.m" AUTHOR: Boris Backovic page; 1 / 3 Containing M-Functions : 1. d2o.m

% % % % %

%%%%%%%%%%%:

cSrSrSr% SrS'2'2'9'S'S'2-& %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display('IN_ConvolEncoder.m ') ; l o a d ('Image_Huffman.mat', 'bit_stream') ; num_output_per_bit = 2 ; generator = {[1 1 1; 1 0 1 ],
[1 [1 [1 [1 [1 [1 [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 ; 1 1 1 0 ], 1 0 ; 1 1 1 0 1] 1 0 1; 1 1 1 0 1 0 1; 1 1 0 1 1 1 1 0 ; 1 1 1 1 1 1 0 0 1; 1 1 1 1 0 0 1 0 ;
,

1 0 0 1 1

1] 1] 0 1 1

, ,

1 1] , 0 0 1 1 0 1 0 0 11

1] , 0 0

1

]};

m = 2; M = m + 2 ; M look_up = generator{m, gen_vl = look_up(1, :) ; gen_v2 = look_up(2, : );

;

clear i ; for (i=l:l:M) impuls_res = zeros(M); imp_in = impuls_res(1,:); imp_in(i) = 1; output1 = gen_vl .* imp_in; output2 = gen_v2 .* imp_in; outl(i) = s u m ( output1); out2(i) = sum(output2); if (i==l) seq_outl = out1(i ); seq_out2 = out2(i); else seq_outl = [seq_outl outl(i)]; S0q out2 = [seg out2 out2(i)]; end end clear k; for (k=l:l:M)

121
:%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File; "ConvolEncoder.m" % AUTHOR: Boris Backovic % page: 2 / 3

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

if (k==l) imp_codeword = [seq_outl(k) seq_ouf2 (k)]; else imp_codeword = [imp_codeword seq_outl(k) seq_out2 ( k ) ]; end end imp_codewords_len = length (imp_codeword) ; data = bit_stream; d_size = length(data) ; coded = zeros (1, (imp_codewords_len + (num output per bit * d_size - num_output_per_bit) ) ) ; coded_length = length (coded) ; clear t; for (t = l :1 :d_size) parse = d ata(t ) ; if (parse == 1) suma = imp_codeword; else suma = zeros(imp_codewords_len); suma = suma(1,:); end clear i; for (i=(2*t-l) :1 ;(imp_codewords_len + (2*t-l)-l)) i f ((coded(i) + suma ( i - (2*t-2))) == 2) coded(i) = 0; else coded(i) = coded(i) + suma(i- (2*t-2)); end end end code_check = coded; polyl = conv(data, gen_vl) ; poly2 = conv(data, gen_v2) ; polyl len = length (data) + length (gen._vl) - 1; poly2~len = length (data) + length (gen_v2) - 1; clear k; for (k=l:1 :polyl_len) if(rem(polyl(k),2) ~= 0) polyl(k) = 1; else polyl(k) = 0; end end

122
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % !^% !^^^^^^^^^^^^^^^^^^^^^^^ % M-File: "ConvolEncoder.m" ^ % AUTHOR: Boris Backovic ^ % page: 3 / 3 ^ %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^^^^^^%^^^^^^

clear k; for (k=l:1 :poly2_len) if(rem(poly2(k),2) ~= 0) poly2(k) = 1; else poly2(k) = 0; end end clear k; for (k=l:1:polyl_len) if (k == 1) codeword = [polyl(k) poly2(k)]; else codeword = [codeword polyl(k) poly2(k)]; end end d_gen_vl o_gen_vl d_gen_v2 o_gen_v2 = = = = convert2dec(gen_vl) ; d2o(d_gen_vl) ; convert2dec(gen_v2); d2o(d_gen_v2);

flush = zeros(1 ,M- 1 ) ; %flushing M - 1 zeros for M = 4 memory registers message = [data flush]; trel = poly 2 trellis(M, [o_gen_vl o_gen_v2 ] ); encoded_stream = convene (message, trel) ; if(sum(xor(coded,encoded_stream) )~= 0) display('WARNNING: Convolutional Encoding Performed with Errors ! 1 ! ! ' \ n ' ); end s a v e ('Convoi Encoder.m a t ','coded','code_check ' , 'codeword','trel ' , 'encoded_stream','d ata'); display('OUT_ConvolEncoder. m ') ;

123

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "d2 o.in" % AUTHOR: Boris Backovic % page: l/l
'b'o'o'b'o'S'o'S'b'ô o ô
o o o o o o o

% % %

o'b'o'o'S'o'b'S'b'o'o'o'b % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

function [sol] = convert2binary (d) if(d == 0) b = 0; else clear ostatak; ostatak = 0; tmp_dec = d; while (tmp_dec >= 8) ostatak = [mod{tmp_dec,8) ostatak]; tmp_dec = tmp_dec/8; tmp_dec = fix{tmp_dec) ; end ostatak = ostatak(1:(length(ostatak)-1)); b = [tmp_dec ostatak] ; end sol = 0 ; len = length(b); for (cnt = l ;1 :len) sol = (b (cnt)*(10^(len-cnt)))+ sol; end ·

124
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % ^% ^% % % ^^% ^^^^^^^^^^^^^^^^^^^^^

% % % % % % % % %

M-File: "aggreg_one.t n " AUTHOR: Boris Backovic page: 1/2 Containing M-Functions : 1. Welsh_Matrix.m 2. complement .m 3 . DSSS.m 4 . CDMA_encode.m 5 . CDMA_decode.m

^ ^ ^ % % % % % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display('IN_aggreg_one.m') ; l o a d ('BLC_Encode.mat' , 'transmit' ); load {'Convol_Encoder. mat' ,' coded');
1 8 = Welsh_Matrix(8); A = transmit ; B = coded; A 1 = A; B 1 = B;

pseudo_A = A = DSSS(A, pseudo_B = B = DSSS(B,

randsrc (1 , 2 0 , [0 , 1 ;0 .5 ,0 .5 ] ) ; pseudo_A); randsrc(1 , 2 0 , [0 ,1 ;0 .5 ,0 .5 ] ) ; pseudo_B);

C_A = 1 8 (3 ,:); comp_C_A = complement (C_A) ; seqA = CDMA_encode (A, C_A, comp_C_A) ; C_B = 1 8 (7 , :) ; comp_C_B = complement(C_B) ; seqB = CDMA_encode (B, C_B, comp_C_B) ; chip_len = length(C_A); if(length(seqA)>length(seqB) ) add_bit = length(seqA) - length(seqB); seqB = [seqB zeros(1 , add_bit)]; el se if (length (seqB) >length (seqA) ) add_bit = length(seqB) - length(seqA); seqA = [seqA zeros(1 , a d d b i t ) ]; end send_out = seqA + seqB; noise = randsrc(1,length(send_out) , [0,1; 0 .9 8 ,0 .0 2 ]),

125

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File; "aggreg_one.m" % AUTHOR: Boris Backovic % page: 2/2

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

received = send_out + noise; [received_A, tmpA] [received_B, tmpB] = CDMA_decode (' A ' , received, chip_len, C _A ) ; = CDMA_decode{'B ' , received, chip_len, C _ B ) ;

received_A = D S S S {received_A, pseudo_A) ; received_B = DSSS (received_B, pseudo_B) , save {' WCDMA. mat ', 'received_A' , 'received_B ') ; display('OUT_aggreg_one.m' ) ;

126

%%%%%%%%%%%%%%%·&%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "Welsh_Matrix.m" % % AUTHOR: Boris Backovic % % page: l/l % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% function [A] = Welsh_Matrix (n)

result = n; flag = 0; k = 0; while(result ~= 1) if(mod(result, 2) ~= 0) flag = 1; break; end result = result / 2 ; k = k + 1; end i f (flag == 0) if (n ~= 2) A = [Welsh_Matrix(2 ^ (k-1 )) Welsh_Matrix(2 " ^(k-1 ) ) ; Welsh_Matrix( 2 ^ (k-1 ) ) -Welsh_Matrix(2 " "(k-l) )] ; else A = [1 1;
1 - 1] ;

end else display('INVALID ENTRY!!!!'); fprint f ( '%d is not = 2 m, where in = 1,2,3,4,... \n ,n) , end

127

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "complement.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [B] = complement (A) f o r (cnt=l:1 :length(A) ) if(A(cnt) == 1 ) if(cnt == 1) B = -1 ; else B = [B -1 ] ; end else if(cnt == 1) B = 1; else B = [B 1 ] ; end end end

128

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "DSSS.m" % AUTHOR: Boris Backovic % page: 1/1

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [DS_A] = DSSS

(A, pseudo_A)

DS_A = [0 ] ; cntl = 1; for(cnt=l:1 :length(A)) C = xor(A(cnt) ,pseudo_A(cntl) ) ; DS_A = [DS_A C ] ; cnt1 = cnt1 + 1; if(cntl > length(pseudo_A) ) cntl = 1; end end DS A = DS A (2 :end) ;

129

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function : "CDMA_encode.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [seqA] = CDMA_encode (A, C_A, comp_C_A) seqA = [0 ] ; for (i= l :1 :length(A)) if(A(i) == 1 ) seqA = [seqA C_A] ; else seqA = [seqA comp_C_A] ; end end seqA = s e q A (2 :end);

130
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-Function: "CDMA_decode.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

function [received_A, tmpA] = CDMA_decode (A, send, chip_len, C_A) pnt = 1 ; tmpA = [ 0] ; received_A = [ 0 ] ; while (pnt < length(send)) tempA = send (pnt :pnt+chip_len-l) .* C_A; tempA = sum(tempA); tmpA = [tmpA tempA] ; if(tempA >= 4 ) received_A = [received_A 1 ]; elseif(tempA <= -4 ) received_A = [received_A 0 ] ; end pnt = pnt + chip_len; end received_A = reçeived_A ( 2 :end) ; tmpA = tm p A ( 2 :end) ;

131

% M-File: "LBC_Decode.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display ( 'IN_LBC_Decode . t n ') load ('BLC_Encode.mat ', 'U' , 'message ', 'm' ) ; load ('Generate_Matrices .mat ', 'n' , 'row_Se ', 'Ht ', 'S e ', 'e ') ; l o a d ('WCDMA.mat' , 'received_A') ; corrupted = received_A; sz_e = s i z e (e); row_e = sz_e(l,l); for(cnt=l:1 :row_e) el{cnt,l} = e(cnt,:); end for(cnt=l:1 :row_Se) SI{cnt, l}=Se(cnt,:); end pnt = 1 ; w h i l e (pnt < length(corrupted) ) received = corrupted(pnt:pnt+n-l); Sr = m o d (received*Ht,2 ) ; for (cnt=l : 1 :row_Se) if(Sr == SI{cnt}) decoded = m o d (received+el{cnt,;},2); for(cntl=l:1:length(U)) i f (decoded == u{cntl,l}) sent_mess = message{cnt1,l}; end end end end if (pnt == 1) dec_mess = sentîmess ; else dec mess = [dec mess sent mess]; end pnt = pnt + n; end s a v e ('BLC_Decode' , 'dec_mess ') ; display ( 'OUT_LBC_Decode.m ')

132
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%:

% M-File: "Viterbi_Decoder.m" % AUTHOR: Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display('IN_Viterbi_Decoder.m'); l o a d ('Convol_Encoder. m a t '); l o a d ('WCDMA .m a t ' , 'rece ived_B' ); coded = c a t (2 , received_B, zeros ( 1 ,3 4 )); decoded_bits = vitdec(coded, trel, 20, 'cont' , 'hard'); decoded_bits = decoded_bits (21 :length (decoded_bits) ) ; if(sum(xor(data,decoded_bits))~= 0) display('WARNNING: Convolutional Encoding Performed with E rrors!!!!'); fprintf('\n'); end save ('Viterbi_Decoder.m a t ' , 'decoded bits'); display ('OUT_Viterbi_Decoder.m ') ;

133
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File: "DtoA.m" % AUTHOR; Boris Backovic % page: l/l

% % %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

clear all; display ( 'IN_Digital_to_Analog.m' ) l o a d ('Sampling.mat' , 'fs ') load Quantizing.mat ; load PCM.mat ; load LempelZivWelch.mat; LZW_received = c a t (2 , c{ ( 3 :end),2 )) ; LZW_received = [LZW_received last_bits] ; pnt = 1; D2 A = 0 ; clear cntl; clear c n t ; while (pnt < length (LZW_received) ) temp = LZW_received(pnt :pnt+7 ) ; tempi = convert2dec(temp) ; if(tempi == 2 5 5 ) tempi = tempi + 1; end for (cnt1=1:1 :length(indexl) ) if (tempi == indexl(cntl)) D 2 A = [D2 A quantsl(cntl)]; break; end end pnt = pnt + 8 ; end D 2 A = D 2 A( 2 :end); wavplay(D 2 A ,fs) t = ( 1 :length(D 2 A ) )/fs ; p lot(t,D 2 A) grid t i t l e ('Received Speech Signal' ) x l a b e l ('time [sec.]') y l a b e l ('voltage [V]' ) drawnow display ( ' OUT_Digital_to_Analog.m' )

134
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^^^^^^^^^%^%%%%%^^

% M-File; "Decoder_lTTiage_Hufftnan.m" % I AUTHOR: Boris Backovic % % page: 1/2 % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% close all; clear all; display ( 'IN_Decoder_Image_Huffman.m' ) ; load ('Image_Huff man. mat ', 'length_of_each_letter ', 'S', 'index', 'A') load ( ' Viterbi__Decoder.mat ', 'decoded_bits ') ; clear i ; for {i = l :1 :length (length_of_each_letter) ) if(i==l) offset = length_of_each_letter(i)-1 ; bit_number = decoded_bits(1:1+offset) ; next_start = length_of_each_letter(i)+1 ; clear j ; f o r (j=l:1 :length(S)) if(length(bit_number)==length(S(j ).source)) if(bit_number == S(j).source) decoded number = index(j); end end end else offset = length_of_each_letter(i)-1; bit_number = decoded_bits(next_start:next_start+offset) ; next_start = next_start+offset+1 ; clear j ; f o r (j =1:1 :length(S )) i f (length( bit_number)==length(S (j ).source) ) if(bit_number == S(j).source) decoded_number = [decoded number index(j)1 ; end end end end end matrix_size = size(A); row_number = matrix_size (1,1); column number = matrix_size (1,2);

135
r%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % M-File: "Decoder_Image_Huffman.m" % % AUTHOR: Boris Backovic % % page: 2/2 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

R = zeros (row_nuniber, column_number) ; clear m; for(m=l:1 :row_number) R(m, : ) = decoded_number((m1) *column_number+l :m*column_nuttiber) ; end f igure colormap(gray(2 5 6 ) ) ; image(R) t i t l e ('Received Image'); RESULT = sum (sum (xor (R, A) ) ) display ( 'OUT_Decoder_Image_Huf fman.m'} ;

136

Appendix B: Matlab Example Files

137
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^^^^^^^^^^^^^^^^^^

% M-File: % AUTHOR:

"HuffmanCodeAlgorithm.m" Boris Backovic

% %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

SS = [0 . 2 5 z=l; clear i ;

0.2

0.15

0.12

0.1

0.08

0.05

0 .0 5 ];

for (i= l ;1 :length(ss) ) if (ss(i)~=0) p (z) = SS (i) ; index (z ) = i
z = z + 1;

end end H = 0; clear i ; for (i=l:1 :length(p) )
X X

= log(p(i)); = -x/ 0 .6 9 3 1 5 ;

y = p (i) *x; H = H + y; end symbols = [1:1:length(p)]; m = length(p); code_dim = length(p) ; Huff = MakeStructHuff (symbols,p ) ; sorted Huff = sorting_Huff (Huff, m, code dim); sorted_Huff.probability; sorted_Huff.symbols ; while(m > 1) last_s = sorted_Huff.symbols(:,m ) ; for (u=l:l:length(last_s)) if(last_s(u,1)~= 0) for(g=l:1:code_dim) if (Huff.codew(g,last_s(u,l))~= 0 ) else

138
Huff.codew(g,last_s(u,l))= 1; break; end end end end sec_last_s = sorted_Huff.symbols( : , (m-1)); for (u=l:1 :length(sec_last_s) ) if(sec_last_s(u,1)~= o) for(g=l:1:code_dim) if (Huff.codew(g,sec_last_s(u,1))~= 0) else Huff.codew(g,sec_last_s(u,l))= -1; break; end end end end clear temp_last clear temp_sec_last last_prob = sorted_Huff .probability (m) ; second_last_prob = sorted_Huff .probability (m-1 ) ; new prob = last_prob + second_last__prob; new_Huff .probability (m-1 ) = new__prob; for (s = l:l:m-2) new^Huff.probability(s ) = sorted_Huff.probability(s); end new_Huff .probability = new_Huf f.probability ( 1 :m-l) ; for (t=l:1 :code_dim) if(sorted_Huff.symbols(t,m)-= 0 ) temp_last(t)=sorted_Huff.symbols(t,m); end if(sorted_Huff.symbols(t,m- 1 )~= 0 ) temp_sec_last (t) =sorted__Huff .symbols (t, (m-1 )) ; end end new_temp= [temp_sec_last terap__last] ;

sorted H u f f .symbols(1: length(new_temp) , (m-1 ))= new_tenp;

139
new_Huff.symbols - s o r t e d _ H u f f . s y m b o l s ( l : c o d e _ d i m , ; m = m -1 ; sorted_Huff = sorting_Huff {new_Huff, m, c o d e d i m ) ; sorted_Huff.symbols ; end clear cnt; f o r (cnt1=code_dim:-1:1 ) for(cnt2=l:1:code_dim) source(code_dim-cntl+1 ,cnt 2 )=Huff.codew(cntl,cnt 2 ); end end for(cnt = l :1 ;code_dim) S(cnt) .source=source(:,cnt) ; S(cnt) .source=nonzeros(S(cnt).source) ; for (h=l :1 :length(S (cnt) .source) ) if(S(cnt).source(h)==-1) S(cnt) .source(h)=0 ; end end S(cnt).source = (S(cnt).source)'; fprintf('\nS%d=%d%d%d%d%d%d%d%d%d%d%d%d%d%d%d \ n ', index(cnt),S(cnt).source) end f p r intf('\n' ) ; average_length = 0 ; for (cnt1 = 1 :1:code_dim) Ik = length(S(cntl).source) ; pk = p(cntl); average_length = (average_length + (lk*pk)); end fprintf ('\nAverage Code Length (using my code) : %g\n',average_length) ; f p r intf('Entropy of the Source H(x)= % 3 .5 g\n' , H) ; fprintf ('Efficiency = %g%%\n\n', H/average_length* 1 0 0 ) ;

140
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^^^%^^^^^^^^^^^^^^^^^^^^
% M - File: % AUTHOR: "LBC_Example.m" Boris Backovic, ^ %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%^%%^%^^^^^^^^^^^^^^
n = 5; k = 3; Ik = e y e ( k ) ; P = G = [0 1;0 1;1 1]; [P Ik] ;

In k = eye{n-k); Ht = c a t (1, e = Ink, P);

[0 0 1 0 0;1 0 0 0 0;0 0 0 0 1 ];

s z _ e = size (e) ; r o w _ e = s z _ e (1,1); f o r ( c n t = l ;1 :row_e) e l { c n t ,l} = e ( c n t , :); end S = m o d ( e * Ht,2); sz_S = s i z e ( S ) ; r o w _ S = sz_S(l,l); c o l u m n = sz_S(l,2) ; f o r (cnt = l :1 :row_S) Si{cnt, end m = pnt
[0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1

l } = S ( c n t ,;);

11 ;

= 1;

i n d x = 1; while(pnt < l e n g t h (m)) l} = m ( p n t : p n t + k - l ) ; l}*G,2);

m e s s a g e {indx, U{indx, pn t

1} = mod(message{indx,

= p n t + k;

i n d x = indx + 1; end corrupted = pnt = 1; < length(corrupted) ) r e c e i v e d = c o r r u p t e d ( p n t :pnt+n-l) ;
[1 0 1 0 0 1 1 1 0 1 1 0 0 0

0];

while(pnt

141

S r = mod(received*Ht,2) ; fo r ( c n t = l ;1;row_S) if(Sr == S l { c n t }) d e c o d e d = m o d (rece ived+el{c n t ,:},2); f o r ( c n t 1 = 1 :1 :l e n g t h ( U ) ) if(decoded == U{cntl,l}) sent_mess = m e s s a g e {c n t 1,1}; end e nd end end if ( p n t == 1) dec_mess = s e n t m e s s ; else dec_mess = end pnt end f o r ( c n t = l :1 :length(message) ) f p r i n t f ('\ nmessage{%d}= % d ' ,cnt) ; f p r i n t f ('% d ',m e s s a g e { c n t ,1} ) ; end f p r i n t f ('\ n ') ; f o r (cnt = l :1; length(U) ) f p r i n t f ('\nU(%d}= %d',cnt); f p r i n t f ('% d ' , U { c n t ,1}); end f p r i n t f ('\n\nreceived co r r u p t e d sequence = ') ; f p r i n t f ( '%d ' ,corrupted) ; f p r i n t f ('\n' ) ; = pnt + n; [dec_mess s e n t _ m e s s ] ;

142
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% M-File : "ConvolExample.m" % AUTHOR: Boris Backovic,

% %

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

num_output_per_bit = 2 ; generator = {[1 1 1; 1 0 [1 1 0 1 0 ; 1], 0] , 11 [1 1 0 1; 1 1 1

1 0 1] , [1 1 0 1 0 1; 1 1 1 0 1 1] , [1 1 0 1 0 1; 1 1 0 1 0 1] , [1 1 0 1 1 1 0 ; 1 1 1 0 0 1 1], [1 1 0 1 1 1 0 0 1; 1 1 1 0 0 1 [1 1 0 1 1 1 0 0 1 0; 1 1 1 0 0 19. , '10 ')

m = m e n u ('Enter the Number of Memory Registers : ', '3 ', 4 ', 5 ', '6', '7 ','8'
M = m + 2; l o o k _ u p = g e n e r a t o r {m, :} ; gen_vl gen_v2 = look_up(l, : ) ; = l o o k _ u p (2,:);

clear i ; for (1=1 :1 :M) impuls_res = zeros(M); i m p _ i n = i m p u l s _ r e s {1,:); imp_in(i) = 1; =

f p r i n t f {'\ n v ( % d )

[%d %d %d %d]

',i ,i m p _ i n ) ;

o u t p u t 1 = gen_vl o u t p u t2 outl(i) out2(i) = gen_v2

.* imp_in; .* imp_in;

= s u m ( o u t p u t 1) ; = sum(output2) ;

% f p r i n t f ('\nFirst A d d e r = % d %d %d % d ',outp u t 1); % f p r i n t f ('\nFirst Sum = %d \ n ',o u t 1 ( 1 ) ) « % f p r i n t f ( '\ n S e c o n d A d d e r = %d %d %d %d ',out p u t 2 ) ; % f p r i n t f ('\nSecond Sum = %d \ n ',o u t 2 ( i )); if (i==l) seq o u t 1 - o u t l ( i ) ; seq o u t 2 = o u t 2 (i); else

143
s eq_outl = seq_out2 = end end fprintf \ nFirst Out_Sequence = %d %d %d %d \ n ' ,seq_outl) ; [seq^outl outl(i)]; [seq_out2 out2(i)];

f p r i n t f < 'Second Out_Sequence = %d %d %d %d \ n ' ,s e q _ o u t 2 ) ; c l e a r k; for (k=l:l:M) if (k==l) imp_codeword = [seq_outl(k) seq_out2(k) ] ;

% f p r i n t f <'\nlmpuls Codewordlll = %d %d %d %d %d %d %d %d \ n ',imp_codeword) ; else imp_codeword = end end fprintf \ n l m p u l se C o deword = %d %d %d %d %d %d %d %d [imp_codeword seq_outl(k) seq_out2(k) ] ;

\ n ',i m p_codeword) ; i m p _ c o d e w o r d s _ l e n = length {imp_codeword) ; d a t a = i n p u t ('Input a Stream of Bits to be D e c o d e d 1 0 0 0 1] ) :\n' ) ; d_size = length(data) ; (e.g. [10 10

c o d e d = zeros (imp__codewords_len + (num output p e r bit * d_size n u m _ o u t p u t _ p e r _ b i t )) ; c o d e d = c o d e d (1,;),· c o d e d _ l e n g t h = l e n g t h (coded) ; clear t ; for (t = l :1:d_size) p a r s e = d a t a (t ) ; if (parse == 1) suma = imp_codeword; else suma = z e r o s (i m p _ c o d e w o r d s _ l e n ) ; suma = s u m a (1,:); end c l e a r i; for (i= (2 *t-l) :1 : (imp_codewords_len + i f ( (coded(i) ( 2 * t -l)-D)

+ s u m a ( i - ( 2 * t - 2 ) )) == 2)

144
c o d e d (i) = o ; else coded(i) end end end f p r i n t f ('\ n ' ); d i s p l a y ('C o n n e c t i o n Representation') ; f p r i n t f ( 'Coded: f p r i n t f ('%d polyl poly2 ' ); = coded(i) + s u m a ( i - ( 2 * t - 2 ) );

',c o d e d ) ; gen_vl); gen_v2); + length (gen_vl) + length (gen_v2) - 1; - 1;

= conv(data, = conv(data,

p o l y l _ l e n = l e n g t h (data) p o l y 2 _ l e n = l e n g t h (data) c l e a r k; for ( k = l : 1 :polyl_len) i f ( r e m ( p o l y l ( k ) ,2) polyl(k) else polyl(k) end end c l e a r k; for ( k = l :1 :poly2_len) i f ( r e m ( p o l y 2 ( k ) ,2) p oly2(k) e lse = 1; = 0; = 1;

~= 0)

~= 0)

poly2(k) = 0;
end end c l e a r k; for ( k = l : 1 :polyl_len) if (k == 1) codeword = else codeword = end end [codeword polyl(k) poly2(k) ]; [polyl(k) poly2
(k)

];

145

f p r i n t f ('\ n \ n '); d i s p l a y ( 'Po lynomial Representation' ) f p r i n t f ('Codeword: f p r i n t f ('%d flush = message trel ') ;

',c o d e w o r d ) ;

[0 0 0]; = [data f l u s h ] ; [15 16]);

= p o l y 2 t r e l l i s (4,

e n c o d e d _ s t r e a m = c o n v e n e ( m e s s a g e , trel) ; i f ( s u m ( x o r ( c o d e d , e n c o d e d _ s t r e a m ) ) ~= 0) d i s p l a y ('W A R N N I N G : Convolutional E n c o d i n g Performed with E r r o r s 11 ! ! '\ n ' ) ; end decoded_bits = vitdec(coded, trel, 1,'cont', 'hard');

d e c o d e d b i t s = d e c o d e d _ b i t s (2 :l e n g t h ( d e c o d e d _ b i t s ) ) ; d e c o d e d bits = [decoded_bits 0 ] ;

d e c o d e d b i t s = d e c o d e d _ b i t s (1: l e n g t h ( d e c o d e d _ b i t s ) -3) ; f p r i n t f ('\ n \ n '); f p r i n t f ('D e c o d e d b i t s : '); f p r i n t f ('%d ',d e c o d e d _ b i t s );

f p r i n t f ('\ n '); if (sum (xor (data, d e c o d e d _ b i t s ) ) ~= 0) d i s p l a y ( 'WARNNING: E r r o r s 1 ! 1 ! '\ n ') ; end Convolutional E n c o d i n g Performed with

