b\lB\T

o

CONTENT BASED AUDIO W ATERMARKING AND RETRIEVAL USING TIM E-FREQUENCY ANALYSIS
by

Shahrzad Esmaili M.Eng., Ryerson University, Toronto, 2002

A thesis presented to Ryerson University in partial fulfillment of the requirement for the degree of Master of Applied Science in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2004 ©Shahrzad Esmaili 2004

UMI N u m b e r: E C 5 3 0 1 3

All rig h ts r e s e r v e d INFORMATION TO U SE R S

T he quality of this reproduction is d e p e n d e n t upon th e quality of th e copy subm itted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, su b sta n d a rd m argins, an d im proper alignm ent can ad v ersely affect reproduction. In th e unlikely e v e n t that th e a u th o r did not se n d a com plete m anuscript and th e re a re m issing p a g e s, th e s e will be noted. Also, if unauthorized copyright m aterial had to be rem oved, a note will indicate the deletion.

UMI
UMI Microform E C 53013 Copyright 2008 by P ro Q u e st LLC All rights reserv ed . This microform edition is protected ag ain st unauthorized copying u n d er Title 17, United S ta te s C ode.

P ro Q u e st LLC 789 E a st E isenhow er Parkw ay P.O . Box 1346 Ann Arbor, Ml 48106-1346

A u th o r 's D ecla ra tio n
I hereby declare th a t I am the sole author of this thesis. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. A uthor's signâturec_. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. A uthor's .. -------- --------------------------

u

B orro w er's P age
Ryerson University requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date.

N am e

S ign atu re

A d dress

D a te

Ill

A b stra ct
C o n te n t B a se d A u d io W a term a rk in g an d R e tr ie v a l u sin g T im e-F req u en cy C h a ra cteristics
© S h a h rz a d E sm aili 2003 M aster o f A p p lied Science D ep a rtm en t o f E lectrical and C om p u ter E n gin eering R yerson U n iv ersity This research focuses on the application of joint time-frequency (TF) analysis for waterm arking and classifying different audio signals. Time frequency analysis which originated in the 1930s has often been used to model the non-stationary behaviour of speech and audio signals. By taking into consideration the human auditory system which has many non-linear effects and its masking properties, we can extract efhcient features from the T F domain to waterm ark or classify signals. This novel audio watermarking scheme is based on spread spectrum techniques and uses content-based analysis to detect the instantaneous mean frequency (IMF) of the input signal. The waterm ark is embedded in this perceptually significant region such th a t it will resist attacks. Audio watermarking offers a solution to d ata piracy and helps to protect the rights of the artists and copyright holders. Using the IMF, we aim to keep the waterm ark imperceptible while maximizing its robustness. In this case, 25 bits are embedded and recovered within a 5 s sample of an audio signal. This scheme has shown to be robust against various signal processing attacks including filtering, MP3 compression, additive noise and resampling with a bit error rate in the range of 0-13%. In addition, content-based classification is performed using T F analysis to classify sounds into 6 music groups consisting of rock, classical, folk, jazz and pop. The fea tures th a t are extracted include entropy, centroid, centroid ratio, bandwidth, silence ratio, energy ratio, frequency location of minimum and maximum energy. Using a database of 143 signals, a set of 10 time-frequency features are extracted and an ac curacy of classification of around 93.0% using regular linear discriminant analysis or 92.3% using leave one out m ethod is achieved.

IV

A ck n o w led g em en t
I would like to express my sincere gratitude to my primary supervisor, Dr. Sridhar Krishnan not only for providing me with excellent feedback and support but also for sharing his knowledge in this field with me. I would also like to thank my co-supervisor Dr. Kaam ran Raahemifar for his advice and support. Also, I am grateful for the financial help th at I received through my supervisors, the Electrical Engineering departm ent at Ryerson University and the Government of C anada (Ontario G raduate Scholarship (OGS)). In addition, I acknowledge Micronet and N atural Sciences and Engineering Research Council (NSERC) of C anada for their financial support. Also, a special thanks to the organizing committee members at Canadian Conference on Electrical and Computer Engineering (CCECE 2003), for awarding us the best paper award and publishing our work in the IEEE Canadian Journal. Their recognition has been an encouragement for this thesis.

D e d ic a tio n
To m y fam ily fo r their love, support and encouragement.

VI

C o n ten ts
I n t r o d u c t io n 1.1 O rganization of the T h e s i s ............................................................................. T im e F re q u e n c y A n a ly sis a n d S h o r t- tim e F o u rie r T ra n s f o r m 2.1 I n tro d u c tio n ....................................................................................................... 2.2 Short-Tim e Fourier T ra n s fo rm ...................................................................... 2.3 W igner-Ville D istribution ........................................................................... 2.4 Applications of T F analysis and S T F T ....................................................... 2.5 C hapter Sum m ary .......................................................................................... C o n te n t B a s e d A u d io W a te r m a r k in g U s in g T im e -F re q u e n c y A n a ly  sis 3.1 I n tro d u c tio n ....................................................................................................... 3.2 A p p lic a tio n s ....................................................................................................... 3.3 R elated W o r k ................................................................................................... 3.4 M o tiv a tio n .......................................................................................................... 3.5 Background and M e th o d o lo g y ...................................................................... 3.5.1 Introduction to Spread Spectrum S y s t e m s .................................... 3.5.2 Spread Spectrum C haracteristics .................................................. 3.5.3 Spread Spectrum T echniques............................................................ 3.5.4 Instantaneous m ean frequency e stim a tio n ...................................... 3.5.5 W aterm arking a lg o r ith m .................................................................... 3.6 Sim ulation R e s u l t s ......................................................................................... 3.7 C o n c lu s io n s ....................................................................................................... 1 6 8 8 10 13 15 18

20 20 22 23 25 26 26 27 28 37 41 50 53

C o n te n t B a s e d A u d io C la s s ific a tio n a n d R e trie v a l U s in g T im e -F re q u e n c y A n a ly s is 57 4.1 I n tro d u c tio n ....................................................................................................... 57 4.2 R elated W o r k ..................................................................................................... 58 4.3 Audio Feature E xtraction .......................................................................... 61 4.3.1 E n t r o p y .................................................................................................. 61 4.3.2 Energy r a t i o .......................................................................................... 64 4.3.3 B rig h tn e ss............................................................................................... 65 4.3.4 B a n d w id th ............................................................................................... 66 4.3.5 Silence r a t i o .......................................................................................... 67 Vll

4.3.6 Summary of F eatu res...................................................................... 4.4 Audio Classification.................................................................................... 4.4.1 Linear Discriminant Analysis ...................................................... 4.4.2 Classification R e su lts...................................................................... 4.5 Chapter Summary .................................................................................... 5 Conclusions 5.1 Summary of r e s u lts .................................................................................... 5.1.1 Spread spectrum watermarkingand instantaneous mean fre quency 5.1.2 Content based audio classification................................................ 5.2 Future w o r k .................................................................................................

67 68 69 70 74 77 77 77 79 81 82 87

Bibliography A List of Publications

vm

L ist o f F igu res
1.1 1.2 2.1 2.2 2.3 2.4 2.5 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 Block diagram of proposed audio w aterm arking and classification schemes Applications of M PEG-7 [ 1 ] .......................................................................... Tim e Frequency Tilings ................................................................................. Tim e-dom ain Plot, Spectrogram and Spectrum of Linear Chirp Signal W indowing Effects-Quadratic Chirp S i g n a l ............................................... W igner-Ville distribution of two tones (tones are clearly visible at 90 and 360 Hz and cross-term activity can be seen around 240 Hz . . . . Spectrogram of sound waveform "safety" (spoken by a m a l e ) .............. 4 5 11 12 13 16 18

3.9 3.10 3.11 3.12 3.13 3.14 3.15 3.16 4.1 4.2 4.3 4.4 4.5

Overall block diagram of w aterm ark embedding and decoding . . . . 23 Block diagram of spread spectrum e n c o d i n g ............................................ 24 Spreading process in a direct-sequence s y s t e m ........................................ 29 Model of a direct sequence spread spectrum transm itter and receiver 30 Spreading of a d a ta s i g n a l ............................................................................. 32 A utocorrelation plots for PN sequences of length 32, 441 .................... 33 Spreading in discrete s y s te m .......................................................................... 35 C alculating IE and IMF of a ROCK music signal "acorg.wav" : a) Spectrogram of music signal as well as IM F of the signal b) IM F of music signal extracted using S T F T analysis c) IF of music signal calculated using derivative of phase m ethod . . . 39 Tim e-dom ain plot, spectrogram and IM F of linear chirp and music . . 40 42 W aterm ark embedding and recovery using IMF ..................................... Absolute Threshold of H e a rin g ...................................................................... 44 Overview of waterm arking procedure for P O P voiced segment ( "viorg.wav" ) 49 W aterm arking procedure for classical piano music segment ( "piorg.wav" ) 54 Before and after waterm arking for a classical music s e g m e n t .............. 55 A dditive white Gaussian noise attack (BER vs. S N R ) ........................... 56 B E R vs. message s iz e ....................................................................................... 56 Block diagram of proposed scheme ............................................................ E ntropy of different s o u n d s............................................................................. Com parison of entropy values a) Results for different genres b) D istri bution for classical and rock............................................................................. D istribution of frequency location w ith minimum e n e r g y .................... M ean of centroid ratio to previous tim e w in d o w ..................................... 60 62 63 65 66

IX

4.6 Standardized Canonical Discriminant Function Coefficients................ 4.7 All-groups scatter plot with the first two canonical discriminant functions 4.8 Territorial Map- Symbols used in territorial map: Symbol, Group, La bel; 1 1 ROCK; 2 2 Classical; 3 3 Country; 4 4 Folk; 5 5 Jazz; 6 6 Pop;*-Indicates a group centroid.............................................................. 4.9 Comparison with m usclefish.....................................................................

71 72

75 76

L ist o f T ables
3.1 Perform ance of algorithm after various a t t a c k s ................................. 52

4.1 Classification results. Method: Original - Linear discrim inant analy sis, Cross - validated - Linear discrim inant analysis w ith leave-one-out m ethod (RO-Rock, CL-Classical, FO-FoLk, Ja-Jazz, PO -Pop, CA% Classification accuracy r a t e ) ................................................................ 73

XI

Chapter 1 Introduction

non-stationary behaviour. One of the popular ways for describing the notion of TF is to understand musical notation. Each musical note corresponds to a specific instant in time (localization in time) and frequency localization or pitch. One of the most commonly known methods of spectral analysis developed by Fourier known as the Fourier transform is quite useful in analyzing periodic and stationary signals. However this transform does not allow for the concept of frequency evolving over time therefore rendering instantaneous frequency meaningless. Since many practical signals have frequency information which changes over time, joint TF analysis is required. The resolution of such transforms is limited by their time duration and bandwidth product in the uncertainty principle. This notion was first examined in quantum mechanics with position and momentum used instead of time and frequency. This work was first noted by Heisenberg (1925), later by Weyl (1927) and Gabor (1947) in his application to signal theory. This principle stated that the energy spread of a function and its corresponding Fourier transform cannot both be simultaneously small [2]. Furthermore, the concept of instantaneous frequency (IF) as first explored by Gabor and Ville involved the use of the Hilbert transform to compose an analytical signal from which the IF could be derived. This approach was faced with a limitation for multi-component signals requiring a two dimensional distribution such as the

J

OINT time-frequency (TF) analysis of signals such as radar, sonar, communica tions and biomedical signals is necessary to understand and analyze their true

sliding Fourier transform to analyze them . W ith the advancem ent in m ultimedia systems and audio coders, the concept of music analysis rem ains the same. To analyze a music signal such as in an audio coder, it is necessary to understand and mimic the characteristics and lim itations of the Hum an A uditory Systems (HAS). Here, several characteristics are im portant. For instance, the hum an ear is able to perceive the frequencies th a t create a sound localized in tim e. Therefore, the model used needs to use joint T F analysis to process the music signal. In this thesis, we examine two different areas of content-based audio w aterm arking and retrieval using T F param eters. Figure 1.1 shows the block diagram of the pro posed schemes. Since most of the previous work in this area examine audio in either the tim e or frequency domain, it is assumed th a t the signals are either wide sense sta tionary (WSS) or th a t they have constant frequency components w ithin the discrete Fourier transform window. In reality however, audio signals are non-stationary and m ulti-com ponent signals which consist of a series of sinusoids w ith harm onically re lated frequencies. In this case, we consider the short tim e Fourier transform (STFT) of the audio signal to extract param eters th a t will be used to w aterm ark or classify the signal. M any advanced T F distributions based on Cohen's class of T F representations such as the W igner-Ville distribution and Choi-Williams distribution have been pro posed over the years [3]. However, by taking advantage of the masking properties of the HAS, we are able to use a simple technique namely, the ST FT to analyze audio signals. T he m asking property of the HAS implies th a t certain sounds will not be heard by the hum an ear depending on the sounds th a t occurred in nearby frequencies or close in time. Also, note th a t Wavelet analysis is not used in this work as it is tran slatio n variant implying th a t the features would be destructed where S T F T is in variant. Also, S T F T provides good frequency resolution in all bands, where Wavelet does a poor job at higher frequency bands. In addition, as a consequence of the m asking property, we find th a t we do not need to examine audio signals at every in sta n t in tim e, thereby, introducing th e concept of instantaneous m ean frequency and

bandwidth which are extracted for each time duration. The two topics are further explored in Chapters 3 and 4. Using TF analysis, we first examine audio watermarking applications. The need for watermarking audio files has increased in the last couple of years due to a faster rate of downloading and uploading data on the Internet. This has created increased number of file transfer applications with large number of users. In turn, recording companies have lost billions of dollars due to Internet piracy of music files (approx imately 4.5 billion dollars loss as reported by US Congress in 2001 [4]). In fact, the loss due to music piracy has been much greater than that of movies (approximately 3 billion dollars US [5]). This is due to the fact that, using MPEG-1 layer 3 (MP3) technology, compact audio files can be obtained from professional CDs which essen tially duplicate the crisp sound quality of the original segment. Transfer of movies over the Internet has generally not been as popular due to the decrease in quality of the compressed files and the large size of files increasing the amount of time required to transmit them. One of the earliest methods of hiding data was used by the ancient romans who would write on paper using "invisible inks" made from substances such as lemon juice and milk. The paper would be left to dry and the ink would disappear. Once the paper was heated, the message would re-appear. Such techniques were even used up to World War II. These days, watermarking is used in audio, image, video and text files. In watermarking audio signals, an imperceptible and statistically undetectable digital signature consisting of a sequence of bits is embedded within the music seg ment. These bits could be used to prove ownership of intellectual property, track pirated copies of multimedia and prevent illegal copying among other applications. These embedded bits are referred to as a watermark. In audio watermarking schemes, the watermark has to be statistically undetectable to prevent its removal by unautho rized parties. This can be obtained using spread spectrum watermarking which has been used since the 1930s for military applications due to its robustness to jamming attacks and since it spreads the message across all frequency bands that it renders it

i

TF^K im

Patterti An^ysis

#
A u d io . Signal
TimeFrequency Analysis (STFT)

'

Six Genres (Rock, Classical, -» Country, Folk, Jazz, Pop)

Classification & R eirievai

Watermarking 1. Joint Temporal a n d Frequency

MasRlrp
2. instan tan eo u s M ean Frequency extraction

W atajjiark Insertion

Distpbution

»

Estimated Message Sequence

PN
Sequence

Figure 1.1: Block diagram of proposed audio watermarking and classification schemes statistically undetectable to outsiders. The w aterm ark should also be robust against intentional removal and jam m ing attem pts. However, the attacks to which a w ater m ark may be exposed to are lim ited since the pirate would not want to damage the original audio file. The second concept exam ined in this thesis is th a t of content-based audio classifi cation and retrieval. The need for this has risen from the requirem ent to m anage and index large m ultim edia databases available on the Internet and even on PC s which are currently being indexed based on the file name or a u th o r's name alone. Several prob lems exist w ith this technique. First, this results in extra work to m anually classify such files and im proper nam ing or indexing could result in inefficient and incorrect searches. Second, this technique does not allow for retrieval of files of a specific type or one th a t sounds sim ilar to an existing musical piece. As a solution, th e M PEG -7 standard started in 1996 in order to improve searches

over the web and bring them to the level of text-based searches [1]. MPEG-7 is also referred to as "Multimedia Content Description Interface" and uses a standardized set of descriptors. However these descriptors can vary according to the context and application. Some descriptors for audio may include; melody descriptions, sound timbres, mood and tempo. These descriptors may be encoded within the data or may exist somewhere else as long as a link exists between the file and the descriptors. Also, they may be generated either manually or automatically.

Manual / autom atic

Decoding (for Storage)

Encoding (for transm ission)

U ser & com putational sy stem s

F igure 1.2: Applications of MPEG-7 [1]

This standard also shows the applications of MPEG-7 . Consider an input of mul timedia content from which we extract features from this either manually or automat ically thereby resulting in an audio visual description [1]. Prom these descriptors one can create a database from which a user can look for specific criteria. If we consider a pull scenario, client applications will submit queries to the description repository and will receive a set of descriptions matching the query for browsing (just for inspecting the description, for manipulating it, for retrieving the described content, etc.). Unlike other MPEG standards that describe compression coding methods, such as MPEG-1, -2 and -4, MPEG-7 represents information about the signal (such as features describing the audio signal). The MPEG-21 standard also concentrates on a

standardization framework rather than the coding approaches [1]. In an efficient content-based retrieval system, audio signal is analyzed, domi nant perceptual features such as brightness and loudness are selected, extracted and the music is classified according to these features. The stronger the features, the higher degree of separation between the different types of music and therefore an im provement in audio classification accuracy. The aim is to make music search engines content-based and as effective as text-based search engines. This is a complex task since text-based Web search engines simply count the num ber of words in common, while audio retrieval techniques need to take a perceptual approach. A lthough several audio classification techniques exist in literature, they do not take advantage of the fact th a t audio signals are non-stationary in nature and the best m ethod to extract features from them is to use a T F technique. Instead, existing audio classification techniques concentrate on frequency or time domain feature extraction which is not as efficient. Also, while many such classification techniques provide discrim ination between speech and audio signals, the proposed technique in this thesis has the more difficult task of distinguishing between different music genres which may have similar spectral features. The possibilities in this area are endless. In fact, there is current research on allowing users to whistle a specific tune while using pitch extraction al gorithm s to convert these results to their note-like representation to query a music database.

1.1

O rg a n iza tio n o f th e T h esis

T he thesis consists of 5 chapters which are organized as follows; C h a p te r 2: T im e F re q u e n c y A n a ly sis a n d S h o r t- tim e F o u rie r T ransform C hapter 2 offers a background on T F analysis techniques including W igner-Ville Dis trib u tio n and Short-Tim e Fourier transform . We also examine the need for joint T F analysis and how ST FT, in particular, is applicable to the scope of this thesis. C h a p te r 3: C o n ten t B a se d A u d io W aterm ark in g U sin g T im e-F req u en cy A n a ly sis

In Chapter 3 we introduce some new TF parameters such as instantaneous mean frequency and discuss their application for TF domain watermarking. We review existing audio watermarking procedures and discuss their benefits and limitations. We also look at the characteristics of spread spectrum systems which are important in digital communications to understand spread spectrum based watermarking. The results of our watermarking algorithm against various attacks are also presented. Chapter 4; Content-Based Audio Classification and Retrieval using Tim eFrequency Analysis In Chapter 4, we discuss content-based retrieval of audio signals. We start by review ing existing audio classification techniques and discuss the previous work in this area using techniques such as Mel-Frequency Cepstral Coefficients. We discuss important features for audio analysis and classification and introduce our TF based classification technique. Chapter 5: Conclusions and Future Work Chapter 5 discusses the conclusions and direction for future work. A summary of our work is also presented here. At the end of this thesis, a list of publications that have risen out of this work are shown.

C h a p ter 2 T im e F req u en cy A n a ly sis and S h o r t-tim e Fourier T ransform
2.1 In tr o d u c tio n

on the other hand can provide information about the signal's energy or phase as a function of frequency. It decomposes a signal into a set of basis functions which consist of complex exponentials. These exponentials of varying frequencies add up to compose th e original signal. The Fourier transform has been widely used to analyze the spectral distrib u tio n of signals. For example in speech signals, the frequency spectrum can be used to differentiate between male and female voices. W hen sounding a letter in the alphabet, the location of the spectral peaks (formants) represent the resonances of the vocal cavity. The difference in these locations between the male and the female speaker is used to identify the speaker. The discrete Fourier transform of a signal g{n) where the windowed signal is f { n ) = g{n)w{n) can be expressed as:

I

N signal analysis, tim e-dom ain representation is often used to measure changes in the signal's am plitude or energy as a function of time. The Fourier transform

= E
n=0

(2-1)

where A : = 0 ,1 ,..., N -- I, and the length of the window sequence w{n) is less th an or equal to the D F T length L. This analysis assumes th a t regardless of the length of the window, th e signal would rem ain tim e-invariant and provides no inform ation about th e local frequency distributions of the signal. The window used to tru n cate the signal

length can have a significant effect on the frequency response. Since multiplication in the time domain implies convolution in the frequency domain, the window can distort the signal's spectrum. Rectangular windows which have an amplitude of one up to the desired cutoff have oscillatory side lobes with large amplitudes in the frequency domain. Although a good frequency concentration can be obtained using this window, the signal spectrum will leak to adjacent frequencies due to the window's side lobe behaviour. Hamming windows on the other hand which do not have as sharp cutoffs in the time-domain will reduce spectral leakage in the frequency domain although they will have a lower frequency concentration which is referred to as spectral spreading. Some applications of where joint TF analysis are required include telecommu nications, speech and music analysis, radar analysis (i.e. laser radar on vehicles), underwater acoustics and bioacoustics including identification of whale or dolphin songs, geophysics, and structural analysis. For signals with time-varying amplitudes, frequencies and phases, a non-stationary signal model with TF representation is re quired to describe them. Radar signals for instance, are transient in the time domain and require TF analysis to capture their rapid changes. They use a transmitter which sends electromagnetic waves to an object and then its antenna receives the scattered waves from the target. The manner which the waves reflect off the object are cap tured in a radar image. Due to the nature of radar signals, to use the standard Fourier transform would require the assumption that the Doppler frequency does not change over time. The TF extraction, however, allows for de-noising and extraction of weak radar signal in noise [6]. The human ear is a capable instrument, able to perceive the various frequencies that create a sound, distinguish their volume, and even recognize various instruments at any given time. Ultimately, we would like to imitate the capability of the ear and provide simultaneous information about time and frequency of the music. TF analysis was examined as early as 1930 by Wigner, Weyl and von Neumann and later in 1946 by Gabor in his work on the theory of communications [2]. This analysis can be considered as a set of transforms that map a one-dimensional time domain signal into a two-dimensional representation of energy vs. time and frequency.

10

Some of th e common T F representations used include Short Tim e Fourier Transform (ST FT ), G abor Transform , Wavelet Transform, W igner-Ville distribution and Cohen Class transform s.

2.2

S h o r t-T im e F ourier T ran sform

S T F T is widely used as it offers ease of im plem entation and low com putational com plexity com pared to other distributions. Its T F m apping can be displayed on either a 3-D or a 2-D plot where the energy is represented by the light intensity of the colours. It uses a sliding window and computes the Fourier transform of the signal in th a t region, thereby providing an estim ate of the "local frequency" at a given time. By moving this window over the entire signal, and com puting the Fourier transform of the windowed signal, an estim ate of the signal's spectral change over tim e is estab lished. This process of computing the ST FT for a signal x(n) can be m athem atically shown as: L-l STFT(n,f) --
m=0 L -l

x(n + x(jn )w {m --

, (2.2)

-- ^
m=0

where w{n) is the window function and L is the window length [7]. This equation is essentially a comparison of the similarity between the signal and the elem entary function w{ m -- As opposed to the S T F T which is complex, th e spectro

gram is a real-valued function which shows the energy density in the T F plane. For a signal x{n), the spectrogram w ith respect to w{n) is defined as:

S P E C i n J ) = \STFT{nJ)\^.

(2.3)

In fact, S P E C { n , f ) A n A f represents the energy in the tim e interval [n, n -t- An] in th e frequency band [/, / -t- A /]. From the T F uncertainty principle we are given th a t A n · A / > \ [2]. This reveals the drawback for ST FT as it implies th a t fre quency resolution can be improved by decreasing the spectral w idth A / at the risk of

11

An

High Frequer icy Bas is Fun :tions

Mid F requer cy Bas s Func tiens

Low -reque ic y Ba sis Fun ctions

Af

Figure 2.1: Time Frequency Tilings increasing the temporal width An (poor time resolution). Figure 2.1 shows the TF tilings used in STFT calculation. The shape of the window w[n) is also important as a window with a sharp cutoff will introduce artificial discontinuities. Hanning or other smooth windows are mainly used in audio analysis techniques as they reduce spectral leakage. Rectangular win dows on the other hand have high oscillations which can be perceivable in audio signals. The linear chirp signal can be used to show the benefits of the STFT relative to the Fourier transform (Figure 2.2). We can also write this expression as x{n) = cos(^f(n)) where ^f(n) represents the phase of the chirp signal. Here, the IF is the derivative of the phase which we express as;

fiW ~

2tt an

-- /o + Pn,

(2.4)

where /? = (/i -- fo)/ni. The values /o and fi are the frequencies at time 0 and rii. In Figure 2.2, the spectrogram shows that this chirp signal is 2 seconds long, starts at DC and crosses 150 Hz at 1 sec. The magnitude spectrum plot showing the DFT exhibits some dominating frequencies, but the information about the temporal behavior of the signal is lost. The fact is, neither the time-domain plot nor the DFT plot clearly show how the frequency content of the chirp evolves over time. If we

12

com pute the Fourier transform of a linearly decreasing chirp signal, we will find th a t the power spectrum of such a signal is identical to th a t of the linearly increasing chirp signal shown in Figure 2.2. As m entioned earlier, the Fourier transform decomposes signals into sum of fixedfrequency basis functions where n ranges from --oo to + 0 0 . These basis ele

m ents are evenly spread out over all time which does not give any IF information. Ideally, we would like the T F plot to give us information about the IF of a signal, th a t is, the frequency at every tim e instant. However, this contradicts with the uncertainty principle stated earlier.
1 0.5
C h irp S ig n al

I

-

0,5
0.4

0.2

0.6

08 T im e!
Spectroq

Ilin lL k tllllila tllii
1.2 1.4 1.6 1.8 2

c

300

m 200

O^Time
M agnitude spectrum

1.4

1.6

.i 40

20

0

50

1 0 0

150

200

250
F re q u e n c y (Hz)

300

350

400

450

500

F ig u re 2.2: Time-domain Plot, Spectrogram and Spectrum of Linear Chirp Signal We can also examine the quadratic chirp where x{n) -- cos('F(n)) w ith IF sweep f^{n) = fo + Pv?. In Figure 2.3, we have a concave chirp where /5 = ( /i -- fo)/n{. This chirp signal starts at /o--100 Hz and crosses /i= 2 0 0 Hz a t Isec. The two

figures show the effect of different window sizes. The top figure splits the chirp signal into overlapping segments and for each segment computes the 128-point D FT. The b o tto m figure which uses a 256 point D FT has a higher frequency resolution (increased

13

Hanning Window of Length NFFT=128

2 200

Hanning Window of Length NFn'=256

Time

F igu re 2.3: Windowing Effects-Quadratic Chirp Signal

number of frequency windows) and lower time resolution (decreased number of time windows compared to the top figure).

2.3

W igner-V ille D istribution

The Wigner Ville distribution (WVD) is a commonly used tool in non-stationary signal analysis. For a discrete signal s(n), the WVD is defined as:

WVDs{n,f)=

^
m=~L

z{n -- m)z*{n + m)e

-j2 n fm

(2.5)

where z*{n) is the complex conjugate of the analytical signal z{n), and N = 2L + 1 is its length. The analytical signal is derived from the real signal s{n) as [8]:

z{n) = s{n) +jH{s{n)).

(2.6)

Here, H{.) represents the Hilbert transform. The WVD is the Fourier transform of the auto-correlation of the signal s{n). Notice that the WVD does not contain the

14

window function used in S T F T calculations, as the window here is th e signal itself. This type of distribution, has a much b etter tim e and frequency resolution th an the S T F T spectrogram used earlier. The main disadvantage of W VD however, is the large cross term s which cause m ajor problems when analyzing m ulticom ponent signals. Note th a t if a signal is composed of a single m odulated tone, it is considered mono com ponent while a signal which is composed of the sum of several m odulated tones is m ulticom ponent. Consider the W VD of a m ulticom ponent signal s{n) = Si(n)-|-S 2 (n):

(2.7)

where the W VD of the cross term s can be seen as: ^
m = --L

(2 .8 )

As can be seen from the above equations, the WVD of a signal containing two other signals, is not simply the linear sum of the W VD of the two signals, instead it contains two cross term s. These cross-terms have a strong oscillation as can be seen in Figure 2.4. This figure shows th a t the WVD consisting of a sum of two sinusoids a t different frequencies produces a cross term in the middle of the two frequencies. Several quadratic tim e frequency distributions have been proposed in order to minimize the cross term interference but preserve the high resolution T FD perfor m ance. One such example is the G abor spectrogram which has a high T F resolution w ith m inim al cross terms. A nother example is the Smoothed Pseudo WVD w ith

decreased the cross term interference but also decreased T F resolution. Cohen's class of bilinear T FD s are based on the WVD and include the Choi-W illiams distribution and th e G abor Spectrogram. The Choi-Williams distribution provides a sm oothed version of th e W VD, good tim e and frequency resolution w ith less cross term s th an the original W VD bu t is com putationally quite expensive [3]. Similarly, the G abor Spectrogram improves the cross term s while increasing complexity. A lthough the S T F T provides th e worst T F resolution, it possesses no cross term s and is very fast to compute. As we will show later in C hapter 3, the IF of a signal

15

can be easily and quickly derived. Also, since this thesis deals mainly with audio signal analysis, the temporal masking properties of such signals do not require us to compute and extract features such as the IF for every instant in time. Instead the window size is chosen similar to existing audio coders, so that these features are extracted for every time window. Also, Boashash, found that although a high TF resolution is obtained using WVD or the Choi-Williams distribution, they stated that "result suggests that only a mea sure of spread derived from a positive distribution (such as the STFT) will be a useful physical quantity" [3]. Another point is that the spread of frequency around the IF is not always positive and, therefore, not useful. These factors for TFDs derived from WVD, such as the inherent cross-terms for multi-component signals, the requirement to calculate the analytical signal first for single component signal and the additional computational complexity of computing the IF at each moment in time have solidified our desire to analyze audio signals for watermarking and classification using STFT analysis. Our study of IF and STFT distributions and their applications in audio watermarking and retrieval will continue in Chapters 3 and 4.

2.4

A pplications of TF analysis and STFT

One of the well-known applications of TF analysis is for speech analysis and com pression [7]. Here, there is a need to decrease the data rate and in order to increase the bandwidth efficiency for the purpose of transmission or storage. In representing speech by a small number of bits, the perceptual quality of the speech needs to be maintained. The two methods of representing speech samples include direct quanti zation and parametric quantization. Pulse Coded Modulation (PCM) is an example of direct quantization where the speech samples are directly represented. PCM is simple to implement as it maps the sampled data to fixed quantizer levels although it does not achieve low enough bit rates. In fact a bit rate of 64 kbits/sec is achieved using PCM while speech has a sampling rate of 8 kHz. Parametric quantizers are more efficient as they represent and quantize the speech model or spectral param-

16

4 50

200

400

600

800

1000

1200

1400

1600

1800

T im e S a m p le s

F ig u r e 2 .4 : W ig n er-V ille d istrib u tio n of two tones (tones are clearly visible a t 90 an d 360 Hz a n d cro ss-term a c tiv ity can be seen aro u n d 240 Hz

eters rath e r th an the speech directly. More advanced coding techniques have d a ta rates in the range of 2.4 k b its/s - 16 kbits/s. To achieve these rates, speech anal ysis and synthesis is required. This process refers to the encoding and decoding of a set of param eters th a t represent the speech. In the synthesis part, the speech i s . decoded and is m apped through a set of transform ations to the original speech. As m entioned earlier, the m ost successful speech coders or voice coders are those th a t use th e perceptual speech models. In fact, as early as the 1940s, there has been much work on speech coders (vocoders) in order to decrease the bandw idth of speech. This work started w ith a variety of PC M techniques which eventually obtained a rate of 32 kb its/s. Linear prediction models were also used in the spectral analysis of speech. In 1971, A tal and H anauer developed an analysis-synthesis m ethod based on linear prediction [9]. There has also

17

been much work in the application of STFT for the analysis and synthesis of speech. The design and simulation of such a system was explored in 1973 by Schafer and Rabiner [10]. In addition, STFT analysis and spectrograms can be quite useful in distinguishing between the different types of speech from a spectral point of view. It was examined in [11] that speech signals can be referred to as non-stationary or even quasi-stationary over the duration of 5-20 ms. They can be broken down into voiced, unvoiced, and mixed speech segments. Voiced speech consists of sounds such as vowels ( "a" , "i" ,...) which are quasi-periodic in the time-domain and harmonically related in the frequency domain. Unvoiced speech segments such as consonants ( "sh" ) have no periodicity and are random and broadband in the frequency domain. Unvoiced fricative sounds are created by forming a constriction in the vocal tract and forcing air through it. Here, the air does not flow freely from the mouth but it is not completely stopped as in unvoiced fricatives. This turbulence creates a noise-like excitation. In such sounds, the energy is concentrated high in the frequency band and the appearance is noise like. Examples of fricatives include ("/ f / " , "/ v / ", "/ t h / " , "/ s / "). In unvoiced plosive sounds, there is a silent period and then a sudden explosion of sound which is shown as a strong energy in many frequency bands. These sounds are created by completely closing the vocal tract while pressure builds up and then abruptly releasing the sound. Examples of such sounds include ( "/ p / ", "/ d / ")As can be seen from the spectrogram in Figure 2.5, the unvoiced segments are noise-like or random in nature while the voiced segments are more organized and their energy is usually higher than that of the unvoiced segments. Also, the spectrogram can identify formants in voiced speech. These formants are horizontal bands where the spectral peaks. They are the frequencies where the mouth gives resonance to sounds. In the spectrogram they are represented by the bands of spectral peaks. Different sounds have different locations of formants and the location of these formants is important in speech synthesis and perception [12]. It was then concluded in [10] that using STFT offers several benefits including increased flexibility in altering speech parameters, lower bit rate and no need for pitch

18

Sound waveform /safety/
0 .2

3D plot of STFT witfi N = 1024 , winlen = 256

0 .1 C D 0 Q.

F <
-0 .1

- 0 .2

4000
2000

0.2

0.4

0.6

0.8

1.2

1.4

F requency (Hz)

Time (s)

Time(s)

Spectropram

C ontour plot of STFT with N = 1024

4000 3500 3000

X

2500

I 2500 E 2000

2 1500

2 1500

%
0.6 0.8 0.2 0.4 0.6 0.8
1

Tim e (s)

Time (s)

F ig u re 2.5: Spectrogram of sound waveform "safety" (spoken by a male) tracking before the analysis-synthesis procedure.

2.5

C h a p ter S u m m a ry

In this chapter we have explored the lim itations of classical Fourier analysis and the need for T F modelling. Since m any sonar, seismic, biomedical, speech and au dio signal are non-stationary in nature, their analysis requires the use of a joint T F model such as STFT. We also examined some common applications of S T F T in cluding speech analysis. Up till now, T F analysis has not commonly been used for state-of-the-art m ultim edia and wide-band signal processing techniques such as audio retrieval and waterm arking. In this thesis we examine how to apply S T F T based T F analysis for wide-band signals. Also, we examine the application of the spectrogram

19

which was previously used primarily to give a visual representation of the signal, to extract numbers and features that will be applied to the watermarking and retrieval techniques.

C h a p ter 3 C o n ten t B ased A u d io W aterm ark in g U sin g T im e-F req u en cy A n a ly sis
3.1 In tr o d u c tio n
V E R the past several years, the ease of copying and distributing copyrighted m ultim edia such as video, audio, image and software over the Internet has in

creased significantly. W ith the emergence of peer-to-peer (P2P) file-sharing systems, this problem has only become more critical. These systems allow each PC to act as a file server for the network, sharing illegal multimedia data. As a result, there is a strong need to protect the rights of the authors. In order to keep up w ith the new technologies and to increase sales, record companies now offer purchase of music over the Internet through online subscriptions. However, computer-savvy individuals who can o btain the files for free have not converted to this m ethod. Although the amount of m oney lost as a result of online piracy has not be estim ated, in the recent lawsuit against N apster (a file-sharing service), it was found th a t an average of 12-30 million files were downloaded a day [4]. Furthermore, a recent report subm itted by the In tern atio n al Federation of the Phonographic Industry (IFPI) showed th a t worldwide music sales have decreased by 7% over the last year; and although it is difficult to estim ate th e exact loss due to online piracy, this practice is considered one of the m ain contributing factors. These factors have been a strong motivation for recent research in m ultim edia
20

O

21

watermarking. Watermarking provides a solution to data piracy by allowing a series of bits which identify the author's name or logo to be embedded within the original image, audio or video signal. Although there are many similarities between audio and image watermarking, audio watermarking presents more complications as the human auditory system (HAS) is much more sensitive to changes than the human visual system (HVS). In fact, the ratio of the highest to lowest audible frequency is approximately 1,000 (range of 20 Hz-20 kHz) where the ratio of the highest to lowest frequency light waves we can see is a factor of 2. Also, in the HAS, small changes in audio files can be perceived as low as one part in ten million [13]. Regardless of the large dynamic range, the HAS has a small differential range that allows loud sounds to down quiet ones. These factors are taken into consideration when developing our scheme. In the case of audio watermarking, there are three main requirements. The first is that the watermark needs to be inaudible and must not affect the sound quality of the original music segment [13]. Second, the watermark bits need to be embedded or hidden in such a way that their pattern is not easily detectable and open to manipulation. Finally, the watermark needs to be robust, such that it will withstand intentional signal processing attacks including lossy compression algorithms (such as MP3), low-pass filtering, cropping and additive noise. Other important factors in evaluating a watermarking scheme may include its security, complexity and the number of bits that can be embedded with a small bit error rate (HER). However, in any watermarking scheme the trade-off always exists between the robustness of the watermarking algorithm to signal processing attacks and the trans parency of the watermark. It is known that as the energy of the watermark is in creased, the probability of full recovery of the watermark is also increased. However, by increasing the watermark energy, we increase the noise in the signal and, thus, make the watermark audible. Although digital watermarking is an application for data hiding, there exist some differences. Digital watermarking consists of embedding a handful of bits which iden tify copyright information, whereas data hiding embeds a large number of bits such

22

as an image w ithin a host signal. Also, unlike d a ta hiding, the w aterm ark usually gives an indication of ownership within a host signal. W aterm arking's main purpose is to ensure th a t the hidden message rem ains hidden and recoverable; it does not aim to prevent access to the original file [13]. In addition, there is much confusion between waterm arking and cryptography. C ryptography does not aim to hide a message such th a t it will not be easily noticeable, it only encrypts it to hide the original message.

3 .2

A p p lic a tio n s

In recent years, several waterm arking applications have been proposed [14]. One of the m ost popular applications is copyright protection. This consists of two types of w aterm arks including proof of ownership and enforcement of usage rights. Proof of ownership w aterm ark aim to help determine the rightful owner or copyright holder in a court of law or in a lawsuit. These types of waterm arks not only require strong robustness, b u t they m ust also be able to resolve the deadlock dispute. Here, the problem arises of determ ining the first real w aterm ark and one of the solutions pro posed is th e idea of "tim estam ps" where the owner sends a request to a "third party tim e stam ping service" . Enforcement of usage waterm arks still have m any flaws as the exact m ethod of im plem entation has not been explored. The idea behind it is th a t these w aterm arks provide instructions to applications which in turn, would not allow duplication or copying the files if it is in violation of the usage policy. A nother common application is fingerprinting where w aterm arks are embedded to identify the recipient of each single distributed copy. This can be used to track pirated copies to th e original recipient who m ade illegal copies and pirated them . These types of w aterm arks will require a very strong robustness to intentional attacks by pirates. Fragile w aterm arks on the other hand are used for authentication purposes. They are able to w ithstand innocent signal processing operations such as change in volume equalization or MP3 compression bu t are destroyed once exposed to malicious attacks to dam age them . Intuitively, such waterm arks do not need to be as robust as they are broken once certain attacks are performed.

23

3.3

R elated Work

Several techniques currently exist for hiding data within audio files. In general, these algorithms are defined for either the time or the frequency domain. The overall block diagram used in audio and image watermarking is depicted in Figure 3.1.
Audio Signal (sj M essage
(m)

Encoding
f(s,m,k)

Watermarked Audio (x)

Distribution
f(x,n)

M essage Estimate Decoding

Password (k)

m
Password (k)

Attacks
(n)

F igure 3.1: Overall block diagram of w aterm ark embedding and decoding

In one of the pioneer works on audio watermarking, Bender et al [13] presented several methods, which include phase coding, low-bit coding, echo hiding and spread spectrum coding. Phase coding breaks an audio signal into segments, applying a discrete Fourier transform (DFT) and replacing the phase of the first segment with a phase that represents the watermark. All consecutive segment phases are changed relative to the first segment. This technique could change the perceptual quality of the audio signal by introducing perceptual clicks in cases where the modification of the phase is not small enough. Low-bit coding embeds the watermark data by changing the least significant bit of the audio signal. This allows for a large number of message bits to be encoded within the audio file. Bender et al [13] suggest that up to 1 kb per second (kbps) per 1 kiloHertz (kHz) can be encoded. This means that for a noiseless channel a bit rate of 44 kbps can be achieved if the sampling frequency is 44 kHz. However this large data rate presents two problems. The first is that it does not meet the imperceptibility requirement for data hiding [15]. In fact the noise would be audible in music signals without much background noise. One method to compensate for this audibility is to decrease the amplitude of the hidden data. However, even with this a second and more important problem exists with this method, it is not robust to signal processing manipulations. As mentioned by

24

Gordy in [15], d a ta hiding or waterm arking techniques need to be able to w ithstand a ttem p ts at removal. In this technique it was found th a t the embedded d a ta could not be recovered if attacks such as channel noise or resampling were performed. One m ethod to improve the robustness of this technique is to introduce error-correcting codes. However such techniques tend to decrease the payload of the message. The th ird technique, echo hiding embeds the waterm ark as an echo of the original signal w ith different delays representing a one and a zero. The problem w ith this m ethod is th a t the am plitude of the echo m ust be decreased in order to make it inaudible, but doing so sacrifices its robustness. A nother w aterm arking technique utilizes the M PEG audio psychoacoustic model 1 in order to shape the w aterm ark and is examined by Swanson et al in [16] and Boney et al in [17]. Their proposed algorithm uses the frequency and tem poral masking properties of the HAS. Here, the frequency masking property is used to generate a m asking filter th a t is applied to the waterm ark. Then, the tem poral m asking property of th e HAS w aterm ark is exploited by weighting the w aterm ark by the envelope of the audio signal. This is one of the most popular waterm arking techniques and the block diagram for this procedure as is shown in Figure 3.2. These m ethods, although efficient, have a high com putational complexity.
Audio Signai
s(n)

» T im e d o m a in a n a ly s is
a(n)

1
F re q u e n c y a n a ly s is
b(n)

Watermarked Audio

w(n)

Message
m -seq u en ce

Watermark Data

Figure 3.2: Block diagram of spread spectrum encoding

25

In a similar technique, Bassia and Pitas [18] examine time-domain watermarking by using a constant to control the energy of the watermark and make it inaudible. Here, noise shaping is done by using a low-pass filter. In [17], Boney et al also used a "scale factor" to decrease the energy of the watermark in the frequency domain. The spread spectrum watermarking technique used in [13] makes the watermark transpar ent simply by decreasing the amplitude of the watermark to a fixed rate of 0.5% the amplitude of the original audio signal. Finally, Erkucuk [19] presents an audio watermarking algorithm which embeds a chirp signal as the watermark using spread spectrum techniques. Once passed

through the channel, the extracted bits are postprocessed using TFD and Hough Radon Transforms. This transform can detect patterns, thereby allowing an increased number of watermark bits to be hidden. For instance, the watermark message can correctly be extracted up to 20% BER and its presence can be detected up to 30% BER. However, the technique offers a disadvantage in that it is difficult to generate a chirp signal in reality as it has a constantly changing IF. But the proposed approach can be used in other watermarking algorithms such as the one proposed in this thesis, as an error correction technique [19].

3.4

M otivation

In this chapter, we examine a spread spectrum watermarking scheme that inserts a watermark into the audio file using time and frequency characteristics simultaneously [20]. This approach reduces the computational complexity compared to techniques examining time and frequency separately while maintaining transparency. Our mo tivation for this work is to address two important features of security and imper ceptibility and this can be achieved using spread spectrum and instantaneous mean frequency (IMF) respectively. In fact, the estimated IMF of the signal is examined as an optimal point of insertion of the watermark in order to maximize its energy while achieving imperceptibility. This Chapter is organized as follows: Section 3.5 offers a review of spread spectrum systems, introduces the fundamentals of IMF and analyzes the proposed watermark-

26

ing scheme. Sim ulation results are presented in Section 3.6, and conclusions are given in Section 3.6.

3.5

B a ck g ro u n d and M e th o d o lo g y

In this section we examine the background, theory and m ethodology involved in achieving the content-based waterm arking scheme for audio files. We begin the section w ith an overview of spread spectrum theory and its application to waterm arking. At the end of th e section, we discuss the benefits of IM F and our w aterm arking algorithm.

3 .5 .1

I n tr o d u c tio n to S p read S p e c tr u m S y ste m s

The development of spread spectrum communication started as early as 1940s and was initially used for m ilitary communications during World War II. It was attractive for such applications due to its anti-jam m ing capability, low probability of intercept by intruders, and secure communications. Up until the 1970s, much of the information regarding spread spectrum techniques was classified and used by the military. Since th a t tim e, m any civilian applications were developed including code division multiple access (CDMA) used in cellular telephones, wireless local area networks (WLANS), and Global Positioning Systems (GPS) which is the largest spread spectrum system used today [21]. The advances in microelectronics technology and signal processing techniques have m ade it much more cost-effective for spread spectrum techniques to be applied for commercial purposes. A lthough bandw idth and energy efficiency are im portant concepts in digital com m unications, in some cases it is necessary to sacrifice this bandw idth in order to take advantage of the benefits of spread spectrum systems such as their resistance to inter ference, and m ultip ath interference rejection. Spread spectrum technology essentially spreads the tran sm itted spectrum much wider than the original signal bandw idth in order to provide the mentioned advantages. In fact, all spread spectrum systems satisfy two main criteria [22]: · The bandw idth of the transm itted d a ta sequence, i.e., the hidden message, is

27

much larger than the minimum bandwidth required for transmission. · The data sequence is spread by a pseudonoise (PN) code, which is independent of the original data sequence. This same code must then be used at the re ceiver to despread the received signal and recover the original hidden message sequence. Note that synchronization between the transmitted sequence and the received sequences is necessary to ensure proper recovery. Moreover, the robustness that spread spectrum provides including its transparency to outside jammers make it ideal for watermarking applications. Where several au dio watermarking algorithms concentrate on imperceptibility, spread spectrum based watermarking provides an added measure of security that is quite desirable. This concept will be further explored in this Chapter.

3.5.2

Spread Spectrum Characteristics

In summary, several characteristics of spread spectrum systems make them attractive for a variety of applications particularly audio watermarking. Such characteristics include: · Jammer robustness: Since the carrier signal or code is random, it is difficult for the jammer to predict. Also, their wide-band characteristics make them more difficult to jam than narrowband signals. · Low probability of intercept: Their noise-like characteristics and their uniform spectral spread make the embedded signal appear as noise. Therefore, they are difficult to detect surveillance receivers. · Low spectral energy: By modulating with a spreading sequence, the information bearing signal is spread over a large bandwidth making it seem like noise. Since the signal is spread over a large frequency band, the power spectral density is also decreased by this amount [23]. · Cryptographic capabilities: Spread spectrum data once modulated with the car rier signal will appear as random to outsiders since the carrier code is unknown

28

to them . This feature provides a privacy th a t makes it difficult for an intruder to decode the message also making it attractive tool for w aterm arking systems including image and audio. As discussed in this chapter, for audio waterm arking techniques th a t use spread spectrum , the original music signal is considered as a jam m ing signal trying to degrade the transm ission and recovery of the waterm ark signal. Since the power of the spread waterm ark is much less th an the audio signal to which it is added, this could present a problem in the recovery of the waterm arked signal. In fact, an em bedding strength is required to decrease the am plitude of the w aterm ark relative to the audio. However, there is a tradeoff between the embedding factor (the sound quality) and the full recovery of message bits.

3 .5 .3

S p rea d S p e c tr u m T ech n iq u es

The m ain spread spectrum techniques used today include direct sequence, frequency hopping, tim e hopping, chirp and hybrid methods. These techniques were reviewed and exam ined by Peterson [22], Haykin [24], and a brief overview is presented here. D ir e c t S eq u en ce Direct Sequence Spread Spectrum is the most prom inent spread spectrum technique. Here, th e d a ta signal is m ultiplied by a pseudorandom (PN) sequence which is a series of bits valued at +1 and -1. In this section, we first consider the tim e-dom ain rep resentation of direct sequence system. The discrete direct sequence spread spectrum technique will be exam ined at the end of this Section. Let the inform ation bearing d a ta sequence be denoted as and puk as the pseudo

noise sequence. Conversely, their nonreturn-to-zero tim e-dom ain representations can then be expressed as m{t) and pn(t), where each waveform can take on the values of ± 1 . By m ultiplying the signal (a narrow band signal m (t)) by a w ideband random signal pn{i) we will produce a spectrum th a t is nearly the same as the wideband PN signal. This is intuitive from the Fourier transform theory where m ultiplication in the tim e dom ain of two signals is equivalent to the convolution of the spectra of the

29

two signals in the frequency domain. Through this modulation procedure, the PN sequence spreads signal to give it a noise like appearance. The amount that the signal is spread is determined by the ratio of the bit rate of the spreading sequence divided by the data rate of the information signal. This ratio is also referred to as the processing gain: Bss _ Th B " Te'

(3.1)

where B is the message signal bandwidth and Bss is the corresponding spread spec trum signal bandwidth in Hz. The duration of one chip of the spreading signal is Tc, which is much smaller than the duration of the signalling interval Tb- Thus, the bandwidth of the spread signal is the product of the bandwidth of the unspread signal and the processing gain.

0.9

0.8

0.7

0.6

0.5 0.4

0.3

0.2
0.1

1/Tc

-1 /T b
Frequency index

1/Tb

1/Tc

Figure 3.3: Spreading process in a direct-sequence system In Figure 3.3, the narrowband signal and the spread-spectrum signal both use the same amount of transmit power and carry the same information. However, the amplitude of power density for the spread-spectrum signal is much lower than that of the narrowband signal. As a result, it is more difficult to detect the presence of the

30

m.

Integrator (over 0-T)

Decision device
-1 if v < 0

pn(t)

T hreshold=0

T ransm itter

C hannel

R eceiver

Figure 3.4: Model of a direct sequence spread spectrum transmitter and receiver spread spectrum signal. The power density is defined as the am ount of power over a certain frequency. In the case of Figure 3.3, the narrow band signal's power density is 2 tim es higher th an th a t of the spread spectrum signal, assuming the spread ratio is 2. Now, given a d a ta signal m{t) and its corresponding spreading sequence pn(t), the transm ission model for a baseband spread spectrum system can be represented as:

w(t) = m{t)pn{t).

(3.2)

T he received signal r(t), which consists of the tran sm itted signal w{i) plus the additive interference signal i{t) can then be expressed as;

(3.3) In order to recover the transm itted signal, a dem odulator consisting of a multiplier follower by an integrator and a decision device is applied to the received signal r(t). Figure 3.4, shows the tran sm itter and receiver models of a baseband spread spectrum system . Since th e tran sm itter and receiver share information about the spreading signal, a copy of the locally generated PN sequence is applied to the multiplier. In this step, we assume th a t there is complete synchronization between the receiver and the tran sm itte r such th a t the PN sequence is the same in b oth the receiver and the tran sm itter. After the multiplier stage, the output can be seen as:

31

z{t) = pn{t)r{t), = pn^{t)m{t)+pn{t)i{t), = m{t)+pn{t)i{t), (3.4)

since pn{t) = ±1 and pn?{t) = 1. If the system has been exposed to an interference jammer in the same band, its impact will be severely reduced as it will spread out during the de-spreading process. In Equation 3.4, we are spreading the interference signal i{t) by multiplying it by the locally generated PN sequence. This causes the power spectral density of the jamming signal to decrease by a factor of N (the pro cessing gain as explained in Equation 3.1) creating a wideband signal. This is an example of how direct-sequence spread spectrum radio combats the interference jam mer [21]. At the same time, the power spectral density of the data signal m{t) has increased due to despreading and the narrowband signal has been re-created. The original signal can be recovered easily using a low-pass filter with a bandwidth that is just large enough to recover the message signal m{t). This will significantly decrease the effect of the interference signal i{t). It is important to note that despreading does not provide any advantage against broadband noise since it can not be spread any further. Therefore, the reduction in the Power Spectral Density (PSD) only occurs if the interference bandwidth is in the same order as the bandwidth of the baseband signal. In Figure 3.4, low-pass filtering is actually done by the integrator that evaluates the area under the signal produced at the multiplier output. The integration is carried out for the bit interval 0 < t < T, providing the sample value v. The decision device is then used by the receiver to determine whether the original data symbol was 1 (in the duration 0 < t < T) if u > 0 or -1 if n < 0. If the sample value v is equal to zero, then the decision device makes a random guess as to wether the original bit was a 1 or --1.

32

a) d a ta signal m(t)

b) spreading s e q u e n c e pn(l)

o

c) w(t)

spread data 0

F ig u re 3.5: Spreading of a data signal P seu d o -P ta n d o m N o ise Seq u en ces Spread spectrum systems use "white noise" to spread data. This m eans the PN

sequence needs to be a signal with a flat power spectral density. This signal ideally has an autocorrelation th a t is a delta at zero lag {3(n) = 1 at n = 0). Figure 3.6 shows the autocorrelation of a 32-point and a 441-point PN sequences. It also shows the effect of reducing its am plitude on the autocorrelation value. This is often required in w aterm arking procedures to decrease the energy of the w aterm ark relative to the audio signal. As Figure 3.6 shows, the longer the length of the PN sequence, the higher th e value of the autocorrelation at zero lag. As we will examine later in this C hapter, th e perform ance of a spread spectrum technique is proportional to the length of the PN sequence. T h at is, the longer the PN sequence, the b e tter the recovery of the hidden message. T his is because the sequence th a t is encoded (the hidden

33

A utocorrelation function of 3 2 point PN s e q u e n c e

10

V
lo
-5 '-

-40

-30
1  1

-20
I

-1 0
1

C o rre la to r: Lag
1 1

10
1

20
1

30
J

40

200 150
= 100

1"
----------------

-50 -500 8 6

t

i

l

l

:

1

:

1

-400
1

-300
1

-200
1 I

C o r r e la to n L a g
I I

^00
I

300
1

400
1

500

|:

----------t i l l 1 1 1 1

-500

-400

-300

-200

-100

C orrelation Lag

100

200

300

400

500

F igure 3.6: Autocorrelation plots for PN sequences of length 32, 441

message) using the PN sequence will be amplified by the value of the autocorrelation at zero lag, but everything else, i.e. the background music will be spread over the whole spectrum. Several criteria exist for choosing a spreading sequence. First, a spreading se quence typically consists of a series of ±1. Second, it possesses the autocorrelation properties discussed above. Ideally, the spreading code should be designed so that the chip amplitudes are statistically independent of one another. This is why our method of randomly generating the PN sequence using Matlab's randn() function is highly useful. Note that there are many different types of spread spectrum codes that could be used. The most well known are called PN codes, as discussed earlier. A variant of PN

34

codes is Gold codes which, are used in GPS systems [24]. There axe also Kasami codes and W alsh codes which are used in 1895 technology [24]. In radar communication, a subset of PN codes called Barker codes are used which are short codes with a length of up to 13. Barker codes are aperiodic sequences th at meet the criteria of pseudo-random ness of leng th = l,2 ,3 ,4 ,5 ,7 ,ll, and 13. Due to these short lengths, such sequences are usually too short for useful spreading of the signals. In general, only periodic sequences are used in direct sequence spread spectrum systems. D isc r e te D ir e c t S eq u en ce Spread S p ectru m T he process examined at the beginning of this Section, can be conversely examined for a discrete-tim e communication system as expressed in [25]. Let us consider a simple com m unication system in two forms. In one form, we transm it the d a ta as it was and in the other one, we transm it the signal after spreading with a PN sequence. First consider, the non-spread digital communication system model. Here, the transm itted d a ta sequence is rrik, where E {±1}, and assume th a t it is equiprobable th a t the

bit is +1 or -1. Now, the received sequence which has gone through the channel and exposed to additive noise can be seen as: rk = E r t ik + j k . (3.5)

Here, S' is a positive value, the energy of the pulse representing each bit. Also, jk is the interference, an additive white Gaussian noise sequence with zero m ean such th a t its auto-covariance is: E[jnJn+l] = (3.6)

In order to determ ine the transm itted bit, the optim al receiver which is a simple level detector is used where if > 0 then assume th a t a -1-1 bit was sent, otherwise

a -1 bit was sent. Note th a t in Equation 3.5, r* is a Gaussian random variable with m ean Errik and variance the bit energy [22] : In this case, the probability of bit error is a function of

Pb = Q

(3.7)

35

1 1 --Ô -- e-- 0-- 9-- 9-- 0-- 9-- Q 0-- 9

<p

Q- -0

Q

G)

Q

G)

9

Q- 9

PN Sequence ^

«-- e---------0-- 9 -

è--

F igu re 3.7: Spreading in discrete system

Now consider the spread spectrum discrete system. Here, we are transmitting a series of N identical bits ruk, so we can examine just one bit m for ease of calculation. Similar to the time domain case, this bit is spread by a chip sequence which is expressed as:
(3.8)

This spreading sequence has two important characteristics. First, the ideal spreading sequence has a mean of approximately zero as shown in Equation 3.9: E\pn,i = jÿ ^
n=0

1

N~1

pn., « 0.

(3.9)

Also, as mentioned earlier its discrete-time periodic autocorrelation can be given by:
G [p rin ) = ^ E n = 0 p^n ,

I 1,

2=0

[0 ,

0 < |2| < A ,

(3.10)

where pUn+N = P^n since the spreading sequence is periodic with N. Note, that the above two conditions represent ideal conditions and can be approached realistically using the techniques mentioned in [22]. For example in a maximal length sequence (m-sequence), the number of bits at 4-1 differs from the number of bits at -1 by exactly one meaning that the mean of the sequence is not exactly zero.

36

Now, sim ilar to the tim e-dom ain case, the transm itted signal can be w ritten as; Wn = Ecmprin, n = 0, .. ., A /" - 1 (3.11)

where the energy Ec -- E / N . The received sequence for the k-th transm itted bit with additive white Gaussian noise is seen as ~ E qT tlpriji -f- jji. A gain (3.12)

this received sequence r,, goes through a correlation receiver which de

spreads the received sequence by correlating it with the locally generated spreading sequence and estim ates the transm itted bit using a decision device w ith a threshold of zero. T his process is shown as:
N -l
V

= n=0

pU n + jn )p n n ,

N -l

=

N E c m -In=0

jnpnn,

(3.13)

and the o u tp u t of the decision device

' ^ 4 - 1 In E quation 3.13, the decision variable v has mean or expected value shown below by th e symbol fj, and is defined as follows:
N -l

P'

--

\NEcTn\

-f- [ ^ ]

jnijP^nj\i

n=0

= =

N E c m -I- 0, Em. (3.15)

We can also show th a t it has variance:
JV -l

VAR{v)

= = =

VAR{NEcm) + V A R {Y,jn{pnn)).
n=0

0+ ^ ^ . < 7 ". (3.16)

37

Similarly, it can be shown that in the case of the non-spread digital communication system model, the decision variable is also Gaussian with mean E m and variance cr^. This means that in both cases, the AWGN channel contributes the same effect and that the probability of error is the same in both cases. In effect, spreading shows an improvement in narrowband interference which can be spread out further in the despreading process of the receiver. In communication systems, such effect is produced from multipath or multiuser interference [23]. In this Section, we have explored spread spectrum communication and the effect of spreading sequences. We have also examined direct sequence spread spectrum systems and its advantages and disadvantages. In Section 3.5.5, we will continue our discussion of spread spectrum for watermarking applications . There, we will take advantage of the benefits of spread spectrum explored here such as security, robustness to jamming attacks and transparency to attackers as solutions for watermarking audio signals. In the following analysis, the process of generating a watermark that will be em bedded in an audio signal is expressed in spread spectrum terminology. The original audio signal is equivalent to the "noise" mentioned in Section 3.5.3. The watermark sequence is transformed in a watermark audio signal and then the audio signal (noise) is added to it. Similar to before, this procedure of adding noise to a signal is called "jamming". In a communication system, the jammer aims to degrade the perfor mance of transmission by exploiting knowledge of the communication system. In the watermarking algorithm, the music signal is considered the jammer and it has much more power than the transmitted watermark bit stream, thereby reducing the probability of error free transmission.

3.5.4

Instantaneous m ean frequency estim ation

Before addressing our watermarking technique, we will first review the IMF and its application to watermarking in this Section. In order to understand the need for IMF, we must first realize the limitations of Fourier transform. As discussed earlier, the standard Fourier transform only provides information about a signal in the frequency

38

domain. In th e case of nonstationary signals where the signal's spectral peaks are varying over time, T F analysis is required. In such a case, the S T F T can be used to interpret the signal in both time and frequency domains by calculating the Fourier transform of the signal in each time segment. The IM F of a signal obtained from its ST FT can show its local frequency at a particular tim e. Note th a t a chirp signal is an example of a nonstationary signal w ith tim e-varying frequency (Figure 3.9). One of the known definitions of IF is defined as the derivative of the phase with respect to tim e [26]. Consider the case where a real signal may be expressed as

g{t) = then th e instantaneous frequency fi is evaluated as:
f =
27T

,

(3.17)

dt ' (3.18) = arctan{v{t)/u{t)).

-

U

The signal can also be expressed as g{t) = u{t)+jv{t) where

Then, the solution for ^(t) will not be unique as the choice of v(t) is arbitrary [27]. Also calculating the IF as the derivative of the phase, could result in a negative IF. This concept can be quantified using the following example. Given a real signal, the following steps are required to derive its IF as per Equation 3.18: 1. Take a m ulti-com ponent real signal s{t). 2. G enerate th e complex signal g(t) from the real one s(t). T he analytic signal g(t) can be calculated using:
g(t)^s{t)+}H(s{t)), (3.19)

where H{.) denotes th e Hilbert transform. 3. Determ ine th e phase (p(t) from Equation 3.19.

39

4. Compute the IF as the derivative of the phase of the analytic signal such that
fi = d(l){t)/dt.

S pectrogram of acorg.w av and its IMF
X

10

15000

S'

gioooo
u- 5000

ML

F igu re 3.8: Calculating IF and IM F of a ROCK music signal "acorg.wav" : a) Spectrogram of music signal as well as IMF of the signal b) IMF of music signal extracted using STFT analysis c) IF of music signal calculated using derivative of phase method

The IF of a music signal computed using the derivative of the phase is shown in Figure 3.8c. As can be seen, this interpretation is not meaningful for multi-component signals where the IF has resulted in negative IF. Also, in audio watermarking algo rithms, it is not necessary to compute the IF which is defined for every moment in time. Instead, we compute the IMF for each time window. As will be explained further in Section 3.5.5, the temporal masking properties of sounds will allow us to use STFT and to compute the IMF in every window.

40

Based on G abor's work on IF, Ville devised the WVD, which showed the distri bution of a signal over time and frequency. The IMF of a signal was then calculated as the first m om ent of the WVD with respect to frequency. Therefore, the IMF of a signal could be expressed as [28];

(3.20) T his IM F is com puted over each tim e window of the ST FT, and T F D [ n , f ) refers to the energy of the signal at a given time and frequency. Note th a t in Equation 3.20, refers to the maximum frequency of the signal, n is the tim e index and / is the frequency index. From this we can derive an estim ate of the IMF of a non-stationary signal assum ing th a t the IMF is constant throughout the window. A non-stationary chirp signal w ith linear IF deviation can be expressed as s{t) = cos{2nat)t. The top row in Figure 3.9 shows the spectrogram and the IM F of a linear chirp signal. As expected, th e IM F of a chirp signal increases with time. The bottom row in Figure 3.9 shows the IM F of a music signal.
Chirp Signal S p e c tro g ra m of Chirp Signal IMF of chirp

fi"

`

'iiiymniT

Time O riginal M usic Signai

5

10

15

Time

?^6000
C D4000

^Time^
F i g u r e 3 .9 : T im e-d o m ain plo t, sp ectro g ram and IM F o f linear ch irp an d m usic

41

3.5.5

W aterm arking algorithm

Many watermarking schemes proposed in literature use ideas from spread spectrum communications. They embed a watermark by adding a PN sequence with low am plitude to the image or audio file. This specific watermark is then detected using a correlation receiver. Consider an audio signal s(n) of length N samples, divided into B = N / M blocks of M samples each. Twenty five message bits are embedded in each block. A block division was chosen because it allows a variable number of bits to be embedded by adjusting the block size. The host signal can be expressed as the concatenation of B non-overlapping blocks, S i,. . . , Sk with concatenation denoted by C. s(n) = Cg.1 Sk (3.21)

The hidden message representing the author's name, logo or copyright informa tion, is defined as a sequence randomly generated and consisting of D bits. However for simplicity of notation, we assume that we are embedding a single random message bit into each block of the audio signal. Now, the message can be represented as a bipolar sequence m*. e {±1}. To spread the signal, every element of the message sequence (or the one bit) is multiplied with its corresponding PN sequence. In Figure 3.10, we demonstrate the watermark generation procedure using a PN sequence and BPSK modulation. In our watermarking technique we begin by multi plying the original message signal by a narrowband PN sequence. The PN sequence piii discussed briefly in Section 3.5.3 must be generated in such a way that it has an autocorrelation of S{n) = 1, for n=0. The longer the PN sequence, the higher the value of the autocorrelation at zero lag. It follows the characteristics discussed earlier such as zero mean and its elements are random numbers chosen from the continuous uniform distribution on the interval from -1 to -fl. The benefit of this technique is that it has a chaotic nature, thereby, improving the cryptographic security of the system. Also, the sequence generation mechanism cannot be reverse engineered and knowledge of part of the sequence would not give clues about the remaining bits. Next, this sequence is low-pass filtered according to the frequency characteristics of

42
Encoding

-**4-----AWGN

Channel

Recovery

Audio Signal Segm ents

s,

Watermarked Music

y,
Message
Embedding
Strengtti Spectrally S tiaped W aterm ark I

/ STFT

> PN sequence BPSK

Analysis &
IMF

pn',

Modulation

extraction

IM F f
V i\ IMF Carrier Frequency

f,

......

Watermark Generation

J

PN s e q u e n c e

M essa g e bit

p n ',

m,

F ig u r e 3 .1 0 ; W atermark embedding and recovery using IMF

the music signal. In a similar technique, Bassia et al [18], shaped the m odulated w aterm ark using a low-pass Hamming filter. They described the shaping process as necessary to reduce the audibility of the watermark before embedding. In this case, a narrow band PN sequence will be generated first which will also result in a narrow band m odulated w aterm ark. This filtered sequence can be expressed as:

p n ] = h p n ;.

(3.22)

43

where h is the filter impulse response with filter order L and can be written as:
ho hi

0
ho hi

0 0
ho

h-2
; h i-i

0 0 0

0 0 0

0 0 0 0

0 0 0 0

0 0 0 0

0 0 0 0

0 0 0

. <.

0 0 0 0 0 0

>. *

ho

0

0

h=

0 : : 1 0 0

0

h i-i

hi

ho

0

0

0 0 0

0 0 0 0 0
ho

(3.23)

0 0

h i-i

hi h i-i
» « «

ho hi

0

where the FIR lowpass filter has cutoff frequency of 1.5 kHz (chosen empirically). In order for the watermark to survive typical transformations of audio signals, including MP3 coding, it is important that the watermark should be limited to the perceptu ally relevant portions of the spectra. Therefore, a spread-spectrum signal with an uncharacteristically narrow bandwidth is used. More information about perceptual coding and MP3 coding can be found in [29]. In the next stage, we use a variation of BPSK modulation where the IMF of the signal is the time-varying carrier frequency. Since it was shown in [16] that an effective watermark needs to be perceptually shaped and placed in a region that will limit the chances of removal, we believe that the IMF will embed the watermark in such a region. The modulated watermarked signal can now be defined by: Wi = mi pn[ài |cos(27rfi)|, where
 |cos(27r/i(ll))| 0 0 0 |cos(27r/i(22))| 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... ... ... 0 0 0 0 0

(3.24)

|cos(27rfi)| =

0 0 0 0

(3.25)

............
0 ...
...

0
1C O S (27r/i(MM))|

_

44 In E quation 3.24, rrii refers to the w aterm ark or hidden message bit before spread ing, is the low-pass filtered spreading code or the PN sequence and f; refers to

the tim e-varying carrier frequency which represents the IM F of the audio signal. The power of the carrier signal is determ ined by a; and is adjusted according to the frequency-m asking properties of the signal. Figure 3.10 presents the block diagram used for w aterm ark generation and recovery. The embedding strength a; is selected based on the sim ultaneous frequency-masking properties of the HAS as given in [29]. A u d io M a sk in g Here we will give a brief overview of audio masking and how it relates to audio w aterm arking. There are three main concepts to consider in masking, first is the

threshold of hearing, second is tem poral masking and third is frequency masking.
A bsolute Threshold of Hearing (Hz scale)

250

200

150

CO

j

100

50

-- 50

0.5
F requency (Hz)

2.5 X 10*

Figure 3.11: Absolute Threshold of Hearing T he absolute threshold of hearing or the threshold in quiet (TIQ) describes the energy th a t is required for a pure tone at a given frequency to be heard by the listener. T he im plication is th a t we are unable to hear sounds which are too weak and different

45

frequency of tones may require different energy to be heard. Figure 3.11, shows the absolute threshold of hearing curve [29]. From this, several things can be observed, first the ear is most sensitive to frequencies between 1 and 5 kHz, where we are able to hear signals even below 0 dB. Second, two tones of equal power and different frequencies need not be equally loud to be heard. Finally, the sensitivity decreases at low and high frequencies. We should also note that this threshold of hearing curve rises and the range of hearing decreases as the age of test subjects increases. While someone at the age of 20, will be able to hear between 20 Hz to 20 kHz, a middle age person's range of hearing is on average closer to 50 Hz - 8 kHz. This threshold of hearing is mathematically estimated in dB as [29]; = 3.64(//1000)-°-^ - 6.5e-°^(//1000 - 3.3)^ + 10-^(//1000)'^, (3.26)

where / is the frequency variable. The TIQ is commonly used in audio coders to determine the feasible amount of compression as the sound which falls below the TIQ curve is not perceived and can be removed. The second concept, is temporal masking where sounds are hidden due to maskers which have occurred earlier in time or even after maskers which are about to appear. Essentially, if we hear a loud sound and then it stops, it takes a little while until we can hear a soft tone nearby in frequency. This phenomenon can be explained by the physiology of the human ear. Temporal masking is in effect a defence mechanism used by the ear to protect its delicate structures from loud sounds. When we are exposed to a loud sound, the human ear automatically responds by contracting slightly causing the perceived volume of the proceeding sounds to decrease. This reflex which is similar to the blinking of the eye is performed to protect the structure of the ear from potentially harmful sonic power. Which in turn causes us to not be able to hear sounds that occur just before or just after a loud sound. The effect of masking after a strong sound is called post-masking and usually lasts about 50-200 ms. The technique of pre-masking where a sound is masked by some thing which appears after it, is relatively short and usually lasts 5-20 ms. Consider an experiment where we turn on a 1 kHz tone at 60 dB, followed by a 1.1 kHz tone

46

at 40 dB. It can be seen th a t the second tone (test tone at 1.1 kHz) will not be heard as it is masked. Now if the masking tone at 1 kHz is stopped first then the test tone is stopped after a short delay. One will notice th at the shortest tim e delay when the test tone can be heard is 5 ms. Note th a t m any audio coders use this information to change their window size for spectral analysis. Typically, the audio is transform ed into blocks and every 256 samples (5.8 ms) at 44.1 kHz, the Fourier transform of the signal is taken. Similarly, in audio w aterm arking literature, specifically those using frequency and tem poral m asking properties separately compute the FF T of a signal every 512 samples (ap proxim ately 11.6 ms) [17, 29, 16]. Finally, the third concept is simultaneous masking which occurs when one sound (maskee) becomes inaudible due to the presence of another sound (masker) with higher intensity when both sounds are heard at the same time. This can be examined by the simple experim ent where a 1 kHz masking tone is played at 60 dB, plus a test tone a t 1.1 kHz at 40 dB. One will notice th a t the test tone cannot be heard and is m asked by the 60 dB tone. T he HAS detects perceived sounds in sub-bands called critical bands and can be m odeled as a set of bandpass filters with varying bandwidths. These 26 critical bands in frequency are linearly related to lengths of the basiliar membrane. Each band corresponds w ith an equal section of the cochlea around 1.3 mm. The widths of the critical bands differ within the frequency range. A simple explanation is th at for each critical band, the hum an ear has approximately the same sensitivity. The critical band table can be found in [29]. The critical bands are uniformly wide from 0 to 500 Hz (100 Hz wide) but after that, a nonlinear exponential relationship is followed where each critical band is around 20% larger than the band below 100 Hz. A bark scale is defined where each bark corresponds to the w idth of one critical band [29]. In order to understand the sim ultaneous masking phenomenon, we will examine two different scenarios of simultaneous masking. First, in the case where a narrow band noise m asks a sim ultaneously occurring tone within the same critical band, the signal-

47

to-mask ratio is about 5 dB. Second, in the case of tone-masking noise, the noise needs to be about 24 dB below the masker excitation level. Meaning that it is generally easier for a broadband noise to mask a tonal sound, than for the tonal sound to mask a broadband noise. Note that in both cases, the noise and tonal sounds need to occur within the same critical band for simultaneously masking to occur. In our case, the tone- or noise-like characteristic is determined for each window of the spectrogram and not for each component in the frequency domain as in [16]. We found the entropy of the signal useful in determining whether the window can best be classified as tone-like or noise-like. The entropy can be expressed as /) ) logg /=o where = (3.28, /)), (3.27)

and Pf(TFD{n, /)) is the probability of energy for each frequency in a given window of the spectrogram. Since the maximum entropy can be written as; -- iog2Fm(3.29)

We assume that if the entropy calculated is greater than half the maximum entropy, the window can be considered noise-like; otherwise it is tone-like. Based on these values, âi controls the energy of the embedded watermark W i relative to the energy of the audio signal. For each time window, once the tone or noise-like behaviour of the music determined, the energy of the music signal is determined in that window. The watermark energy is then scaled by the coefficients in â; such that the watermark energy will be either 24 dB or 5 dB below that of the music signal. The embedding strength matrix can be defined as:

48



%(11) 0 0

0 Ui(22) 0 0 0 0 0

0 0 0 0 0 0

0 0 0

... .. . .. .
...

0 0 0 0 0 0 tti(MM) (3.30)

ai =

0 0 0 0

...............

0

...

. . .

_

where each row represents the scaling coefficient for a tim e window. Once the w aterm ark is perceptually shaped and m odulated at the IM F according to E quation 3.24, it is added to the music signal in the tim e domain as: Yi = Si + Wi, where Si represents the original audio signal. In order to recover the waterm ark and thus the hidden message, the user needs to know the PN sequence and the IMF of the original signal. Figure 3.10 illustrates the message recovery operation. The decoding stage consists of the following: · Step 1: T he w aterm arked music signal and the locally generated tim e-varying carrier signal are applied to a product dem odulator. Assuming th a t this is a noise-free channel such th a t ji is not present then: r; = = = = Since: cos^{a) = oes{a)cos{a) = -[1 4- cos(2a)], then r, = (si|cos(27rfi)|) 4 --- mi p n -a i(l 4 - |cos(47rfi)|), (3.34) (3.33) yi|cos(27rfi)|, ( s î Wi)|cos(27rfi)|, (si 4- m ;pnja; |cos(27rfi)|)|cos(27rfi)|, (si|cos(27rfi)|) + (mi pn|â||cos^(27rfi)|). (3.32) (3.31)

49

where the component with double the frequency of the IMF is then removed using a LPF, and the audio signal modulated by the IMF is considered as noise iX i resulting in a received signal:

r; =

(si · lcos(27rfi)|) + mipnja;, (3.35)

= mi pn|â; + n;.

Step 2: Spectrum despreading occurs by correlating the result obtained in Step 1 with the filtered PN sequence. The resulting message is

m =

(3.36)

where r, and p n ' f denote vectors of length IxM and M xl. Finally, a threshold detector is used to recover the original message bits as shown in Figure 3.10.
X10 Original Music PN Sequence M essage

0)

0.6

u- 0.4

1000 X 10
X

2000 3000 Time

4000

-10
V
p s

10

S h a p ed W atermark

W atermarked Music 2 "20

: music

m -60
CL

\

wa term ark

0.5

1 1.5 Frequency

x IO

Figure 3.12: Overview of watermarking procedure for POP voiced segment ("viorg.wav")

50

3.6

S im u la tio n R esu lts

The w aterm arking algorithm described in this thesis was applied to several different music files ranging between classical, pop, rock and country music. These files were sam pled a t a rate of 44.1 kHz, and 25 bits were embedded into a 5 sec sample of the audio signal. Figure 3.12 gives an overview of the waterm ark procedure for a voiced pop segment while Figure 3.13 shows the waterm ark procedure completely for a classical very quiet piano segment. As can be seen from these plots, the waterm ark envelope follows the shape of the music signal. As a result, the strength of the w aterm ark increases as the am plitude of the audio signal increases. Figure 3.14 shows the before and after time domain and T F plots of the original and w aterm arked segments. As can be seen from these plots, the transparency of the w aterm ark is apparent and is very im portant for ensuring secure waterm ark transfer.

The perform ance criteria presented by Gordy and Bruton in [15] were used to evaluate our w aterm arking algorithm. First, the bit error rate (BER) was calculated w ithout any signal m anipulations. Note th at the BER is given by:

Depending on the music signal, the waterm ark was either fully recovered or had an average B E R of around 0.04. In term s of imperceptibility of the w aterm ark,

it was found th a t the w aterm ark was undetected by the listener regardless of the music signal. Fig. 3.14 shows the power spectral density (PSD) of the audio signal com pared to th a t of the perceptually shaped watermark. It can be seen th a t the inaudible w aterm ark PSD is below th a t of the music. The effect of increasing the message payload was examined for five different audio files (Figure 3.16). These files can be classified as follows: acorg.wav is a rock-like signal and a sample of an ACDC music signal, a spatially rich sample. Deorg.wav is a voiced sam ple of a rock-hke Def Leppard song, hporg.wav is a classical harpsichord

51

segment and viorg.wav is a voiced pop segment of the song "Visit" with instrumental accompaniment. Finally, "piorg.wav" is a segment of a solo piano music. Since the sound of a piano is well known, any distortion introduced by the algorithm should be readily perceptible. It can be observed from Figure 3.16 that as the message payload decreases, the length of the PN sequence increases, thereby improving the robustness of the message and decreasing its BER. However, it should be noted that although the BER is shown as zero for a message payload less than 25 bits, this value might vary since our PN sequence is generated randomly. Also, since our embedding algorithm is content-based, the message payload for some files may be better than that for others, depending on their frequency and energy components. Several robnstness tests were then performed on the five different audio files to examine the reliability of our algorithm against signal manipnlations. The snmmary of these resnlts can be seen in Table 3.1. For simplicity, we considered the case where the message signal was fully recovered without exposure to attacks. First, the watermarked music signal was low-pass filtered at 5 kHz. It was found that the message bits were still fnlly recovered with a BER of 0. However, a lowpass filter at 4 kHz or a high-pass filter at 100 Hz resulted in an average BER of 0.05-0.06. Next, we considered additive white Ganssian noise with zero mean and variance a. Figure 3.15 shows the effect of adding noise for five different wave files. The signal-to-noise ratio (SNR) was varied between -40 dB and -|-40 dB. In this case, the BER remained 0 for SNR greater than 0 dB, and increased to 0.08 for an SNR of -10 dB. However, since the noise is generated randomly the BER could vary slightly depending on the iteration. Our scheme was also tested for robustness to lossy compression techniques including MP3 compression. Here, it was found that the average BER increased to 0.08 (2 out of 25 bits were not recovered). Other attacks included resampling the mnsic down to 22.05 kHz and back to 44.05 kHz, amplitude changes, equalization and noise removal. Up till now, there is not a fixed database of music signals where all audio wa termarking algorithms are tested against. Also, there is no set of attacks which all

52

audio w aterm arking algorithm s are exposed to. In an attem p t to standardize this, Petitcolas et al [30] realized th a t many claims of robustness have been made in sev eral papers w ithout following the same criteria. They have published a work where 4 popular audio w aterm arking algorithms, three of which were subm itted by companies have been exposed to several attacks. The algorithms are referred to as A, B, C and D. For each algorithm , 6 audio segments were watermarked and it was noted whether the w aterm ark was completely destroyed or somewhat changed by the attacks. Al though the B ER from these attacks were not shown, Table 3.1, shows which of the four algorithm s were affected by each of the attacks. Their algorithm s were not tested against M P3 compression.

A tta c k s 1. 2. 3. 4. 5. 6. 7. 8. N one H P F (1 0 0 H z) L P F (4 kH z) R e sa m p lin g factor (0.5) A m p litu d e ch an ge (4 -/- lO dB) P a ra m e tr ic eq u alizer (bass b o o st) N o is e r e d u c tio n (hiss rem oval) M P 3 co m p ressio n

A verage B E R 0.00 0.05 0.06 0.04 0.08 0.13 0.02 0.08

A ffected A lg o rith m s in StirM ark N /A A, D A , C, D C, D N /A A , B , C, D C ,D N /A

T a b le 3.1: P erform ance of algorithm after various a tta c k s

As can be seen from the above tests, our technique offers several improvements over existing algorithms. First, since our algorithm is content-based, it is adap

tive to various music files where algorithms with fixed atténuation of the waterm ark am plitude will not maximize the im perceptibility/robustiicss criteria. Second, this algorithm is less com putationally complex than several other algorithm s, particularly frequency-dom ain waterm arking techniques. In fact, using MATLAB encoding and decoding of the w aterm ark takes around 24 sec (on a Pentium 4 CPU, 1.40 GHz with 256 MB of RAM) for a 5 sec audio sample. In addition, our technique offers more th an simple detection of the waterm ark; we are able to recover the w aterm ark with

53

minimal error. Finally, using the IMF of the signal we maximize the robustness to various signal processing attacks.

3.7

Conclusions

In this Chapter, we proposed a novel spread spectrum watermarking technique that utilizes IMF estimation of the original audio signal and the simultaneous masking property to determine optimal points of insertion of the watermark. It was found that the watermark was imperceptible within the host signal, statistically undetectable and robust to common signal processing attacks including filtering, additive noise and MP3 compression (BER 0-13%). Furthermore, the algorithm is secure as knowledge of both the PN sequence and the time-varying carrier frequency are required to recover the hidden message.

54

Original Music Signai piorg.wav

X 10

Original m usic- Spectrogram

1

2

3

1200 1000
é 800
I 600 400

Instantaneous Mean Frequency

200
1 2 3 4

PN Se

Soectro

Filtered PNSequence- Spectrogram

5= 0 .4

0

1000

2000

3000

4000

1000

2000

3000

4000

Message Sequence- Spectrogram
0 .0 4

Watermark Before Modulation

0.02
0
-0 02
- 0 .0 4 10 X 10 15 20 25 1 2 3

Modulated W aterm ark- Spectrogram

Modulated Watermark

Q.

I

F ig u r e 3 .1 3 : W atermarking procedure for classical piano music segment ( "piorg.wav" )

55

Oriainal Classical Music "piorg.wav"

X 10

S pectrogram ot Original m usic

W aterm arked Music

X 10

W a t o i m a i k o d Music

P SD of Music Signal an d W aterm ark

q -50

-60 -70 -80

0

0.5

1 Frequency

1.5

2 X IO

Figure 3.14: Before and after watermarking for a classical music segment

56

0.5

0 .4 -

0 .3

0.1

-40

-3 0

-2 0

-1 0

40 SNR (dB)

F ig u re 3.15: Additive white Gaussian noise attack (BER vs. SNR)

0.25

-- -e-* -4 -M -

acorg.wav deorg.wav viorg.wav hporg.wav piorg.wav

0.15

0.1

0.05

20

40 Hidden M essage Size

too

120

140

F ig u re 3.16: BER vs. message size

Chapter 4 Content Based Audio Classification and Retrieval Using Time-Frequency Analysis
4.1 Introduction
ITH the abundance of personal computers, advances in high speed modems operating at 100 Mbps and GUI based P2P file-sharing systems that make it

simple for individuals without much computer knowledge to download their favorite music, there has been an increase of digitized music available on the Internet and on personal computers. As such, there is also a rising need to manage and efficiently search the large number of multimedia databases available online which is difficult using text searches alone. Current multimedia databases are indexed based on song title or artist name which requires manual entry and improper indexing could result in incorrect searches. A more effective content based retrieval system, analyzes audio signals, selects and extracts dominant perceptual features and classifies the music based on these features. Stronger features provide a higher degree of separation

W

between classes and thereby a higher classification accuracy. The aim is to make music search engines as effective as text-based ones and this is examined further in this chapter.

57

58

4 .2

R e la te d W ork

In recent years, there has been many works on audio classification with various per ceptual features and several classification algorithms. In one of the pioneer works done on audio classification and later commercialized into the "Muscle Fish" project, Wold et al [31] extracted an N dimensional vector consisting of several acoustical features such as loudness, pitch, brightness, bandw idth and harm onicity from each sound. A Euclidean (M ahalanobis) distance is then calculated between the input sound feature vector and the existing models in the database. Using the nearest neighbor (NN) rule, the signal is grouped into the class with the minimal Euclidean distance. In 1997, using a different approach, Foote [32] uses a 13 dimensional feature vector consisting of 12 Mel frequency cepstral coefficients (MFCCs) and an energy term . A tree-based vector quantizer is built and used to divide the feature space into non overlapping regions. A tem plate or a histogram showing the relative frequencies of samples in each region is constructed for different audio sources. The histogram of the audio signal to be classified is then compared to the existing tem plates using the NN rule and Euclidean or Cosine distances. The main benefit of this approach is th a t the M FCC feature set is uncorrelated and the features do not need to be adjusted depending on the audio file characteristics. In a sim ilar work to th a t of [31], Liu et al [33] extracted 13 different audio features to separate audio clips into different scene classes such as advertisement, basketball, football, news and weather. Features consist of volume distribution, pitch contour, bandw idth, frequency centroid and energy. A neural network classifier with a oneclass-in-one network structure is used and an overall classification rate of 88% is achieved. Artificial neural networks (ANN) are designed to im itate the human nervous system and its ability for adaptive learning. They are effective in detecting complex nonlinear relationships while requiring little formal training. However, their process is com putationally expensive due to the training process and more im portantly, the relation between the input and output variables is defined in a black box model th a t has no analytical basis. In term s of audio classification this means th a t it is difficult

59

to deduce which acoustical features are significant in classifying each type of sound [31]. Also using ANNs, Wan et al [34] employ a combination of a probabilistic neural network (PNN) and a NN classifier. A set of 87 perceptual features are extracted from the time domain, frequency domain, and coefficient domains. A sequential feature selection (SFS) technique method is then used to decrease the feature set and the PNN to classify the sounds into 3 general classes. Finally the NN rule is applied to determine the subclass of the input signal. In a different technique, Lu and Hankinson [35] used a rule-based heuristic clas sification method to classify an audio signal into speech, music and noise. For each feature, a threshold is set to determine the segment type and the feature set in cludes silence ratio, centroid, harmonicity and pitch. Since the feature threshold must change for different audio inputs, this type of classifier is tedious and not ideal. A classification rate of 75% for speech, and 89% for music is reported. Lu et al [36] proposed support vector machines (SVMs) as an alternative to current classification methods. Using a kernel-based SVM increases the classification rate by separating nonlinear cases. Here, a nonlinear kernel function maps the data to a high dimensional feature space where the data is linearly separable. The authors use a combination of a rule-based classifier and a kernel based SVM to distinguish between 5 different audio classes including silence, music, background sound, pure speech and non-pure speech. Their feature set include similar features to those reported in [31] and [32], such as MFCCs, zero-crossing rate (ZCR), short time energy (STE), sub band powers, brightness, and bandwidth with some new features such as spectral flux (SF), band periodicity (BP), and noise-frame-ratio (NFR). An average classification accuracy of around 90% is achieved. Finally, in one of the few TF approaches to content based audio retrieval, Umapathy [37], uses a technique based on matching pursuit with Gabor functions. They decompose a signal into TF functions based on Gabor functions. They find that the octave parameter used in decomposition, can provide discriminatory information about the audio signals and can be used for pattern classification. The distribution of the 14 octave parameter values are calculated over 3 different frequency bands

60

resulting in a to ta l of 42 values for each audio signal to be used as the feature set for classification. In their technique, an overall classification accuracy of 98.6% is achieved using the regular LDA m ethod and 95.8% using the leave-one-out m ethod. The database of signals is the same as th a t used in this thesis. However, although high accuracy rates are achieved, this technique is quite com putationally complex. First, the num ber of features extracted is large and second, extracting the features using G abor decomposition takes up a lot of processing time. We find th a t in the m ajority of the previous work in this area, audio is examined in either the tim e or frequency domain where it is assumed th at the signals are wide sense stationary. In reality, audio signals are non-stationary and m ulti-com ponent signals consisting of series of sinusoids with harmonically related frequencies. Our algorithm considers the short-tim e Fourier transform (STFT) of an audio signal to ex tract param eters th a t will be used along with linear discriminant analysis (LDA) to classify signals. Figure 4.1 shows the block diagram of our proposed audio classifi cation and retrieval scheme. Our retrieval technique is less com putationally intensive th an those th a t use ANN, SVM, or Hidden Markov Models (HMM). Also, the effi ciency of features can be examined which is not directly feasible in ANNs. Note th a t while HMM can be used to examine spectral change over time, past works have shown th a t HMM needs to be coupled with external features such as Cepstral or perceptual features to be efficient [38]. Finally, our m ethod also offers the added improvement th a t it is not specific to certain audio files and can be applied without adjusting the algorithm such as in rule-based models.
1. Rock
2. C lassical Music Segm ent T im e-F requency A nalysis (Short-tim e F ourier T ransform ) G enre Classification (Linear Discriminant Analysis)

3. Country 4. Folk
5. J a z z

F eatu re Extraction

F eature Reliability Testing

6. Pop

F ig u r e 4 .1 : Block diagram of proposed scheme

O ur work on content-based audio classification is presented as follows. Section 4.3 presents the application of T F analysis to feature selection and analysis for audio

61

classification. In Section 4.4 we present our classification results for the system and our conclusions are provided in Section 4.5.

4.3

Audio Feature Extraction

The set of features extracted are critical as they need to be strong enough to clearly separate the classes of signals. This procedure requires perceptual features that model the human auditory system. Discriminating music from speech is less complex than between different classes of music. The latter may only require a small number of features such as zero crossing rate or energy envelope and since the spectral charac teristics are not very similar, high accuracy rates are achieved. Here, we examine the similarities of 143 audio signals and classify them under six different genres. Each audio signal is 5 seconds long, mono-channel, sampled at 44.1 kHz with 16 bits/sample quantization. The length of the audio samples was chosen to be 5 seconds in relevance with the human neurological behavior which was examined by Perrot et al in [39]. They found that human beings require at least 3 second long excerpts to identify different musical genres with a 70% accuracy rate while the accuracy decreases to 53% for a 250 ms excerpt. We start by transforming our audio signal into a spectrogram with a window size of 1024 samples which corresponds to about 23 ms at 44.1 kHz. This window size is similar to that used in [36] and [40]. A Hanning window with 50% overlap is used and the DFT is calculated in each window. The audio features extracted from the two-dimensional time-frequency distribution (TFD) are explained below.

4.3.1

Entropy

The entropy of a signal is a measure of its spectral distribution and portrays the noise-like or tone-like behavior of the signal. The entropy of a signal in time frame n can be calculated as: H(n) = t , P A T F D ( n , f ) ) h g 2 Pf(TFD {n,f)),
/= 0

(4.1)

62

4000

W h ite N o is e

S p e c tr o g ra m

4 0 0 Hz sin e w a v e - S p e c tr o g r a m

V o w el " a " - S p e c tr o g ra m

3 2000

3 2000

0.1

0.2
T im e

0.1

0.2
T im e

0.1

0.2
T im e

E n tro p y o f W h ite N o ise

E n tro p y of 4 0 0 H z s in e w a v e

E n tro p y of V ow el "a'

S 4

S 4

0

10
T im e w in d o w

20

10
T im e w in d o w

20

0

10
T im e w in d o w

20

F ig u re 4.2: Entropy of different sounds where T ,T J,''T F D (n,S) Here, T F D { n , / ) represents the energy of the signal at time frame n and frequency index / (it is equivalent to S P E C ( n , f ) defined in Section 2.1). Also, Fm refers to the m axim um frequency. Consider the case where there are L number of frequency bins. Then the maximum entropy in tim e window n is log2 L which occurs if the frequency bins are equiprobable. First, we exam ined the entropies of 3 different types of signals. These signals were analyzed using 128 frequency bins (Figure 4.2), implying th a t the maximum entropy is 7 bits. T he first signal consisted of a single sine wave, at a sampling frequency of 1 kHz. In this case, the m ean entropy was 1.24 bits and the standard deviation at

63

5.636 X 10"® . Next we considered the vowel "a" (a signal component with harmonic structure) and its entropy was calculated to be 2.84 bits with a standard deviation of 0.1. Finally, we considered white Gaussian noise and its mean entropy was 6.38 bits with a standard deviation of 0.06. As we expected, the sine wave had the lowest entropy and a standard deviation of almost zero while white noise had the largest entropy (approaching maximum) with a larger standard deviation.

CLASSICAL

COUrfTRY

FOLK

JAZZ

PO P

(a)

llll II I
C lw stcal

lll.lll ll
(b) Figure 4.3: Comparison of entropy values a) Results for different genres b) Distribution for classical and rock. From our database of music signals, we found that entropy was a dominant feature in classifying particularly rock or folk music. As shown in Figure 4.3a, rock signals possessed the highest entropy followed closely by folk music while classical, country,

64

jazz and. pop had low entropies. Figure 4.3b shows the distribution of entropy for rock music com pared to classical. As can be seen, the entropy ranges for the two types of signals are quite different. In order to determine the strength of entropy from a different perspective, a receiver operating curve (ROC) was plotted. The ROC curve is a two dimensional measure of classification performance. The area under this curve m easures discrim ination, or the ability of a feature to correctly classify signals. An area of
1.0

represents a perfect test; where an area of

0.5

or less shows the feature

is not useful in discrim ination of th a t class. Rock, folk, jazz, classical, country and pop music had ROC areas of 0.933, 0.808, 0.644, 0.337, 0.294, and 0.145 respectively. These results show th a t although entropy is a strong feature, further features are required to improve classification.

4 .3 .2

E n e r g y ra tio

The ra te of change in the spectral energy over time was measured as the mean of the to tal energy in a frequency sub-band to the previous time window. This energy ratio can be expressed as:

T lr J ,Z Z T F D ( n - lJ )
T his was exam ined in three different sub-bands [0, 5 kHz], [5, 10 kHz], [10 kHz, Fin]. However, it was found empirically th at the energy ratio in mid and high fre quency bands did not improve the classification. This is probably because most energy activity in audio signals is in the low frequency band. Therefore, only the mean of energy in the low-band was used in our feature set. The frequency location with the lowest energy component was also computed. A lthough an estim ate of the mean can be calculated from the frequency domain, it was included in our feature set as it improved the classification rate by 5%. In fact, using the m ean and standard deviation of the location of minimum energy provided
100

%

classification rates for classifying country, folk and jazz music but low classification rates for the other three genres. W hen examining the histogram of the location of m inim um energy for our database of signals (Figure 4.4), the frequency spread was smaller for country (21.4-21.5 kHz), folk (21.45-21.85 kHz), jazz (21.36-21.51 kHz)

65

Rock
0.5

Classical

Country

Pop

17

18

19

F re q u e n c y in kHz

Figure 4.4: Distribution of frequency location with minimum energy and a wider range for pop (18.1-21.5kHz), classical 15.5-21.5kHz) and rock (20-21.6 kHz).

4.3.3

Brightness

The brightness of a signal also referred to as its frequency centroid, shows the weighted midpoint of the energy distribution in a given frame. It is defined by:

/>{») =

EfSo / TFD{nJ)
EfioTFD{nJ)

(4.4)

The brightness feature could also be seen as the instantaneous mean frequency

66

p aram eter, a typical non-stationary feature of a signal. The frequency centroid of the audio signal in the low frequency range (0-5 kHz) is also examined as most of the frequency content of audio signals is concentrated in low frequency. In addition, th e m ean of centroid ratio to previous window is a useful feature as it measures the spectral change over time. As shown in Figure 4.5 rock, folk, pop and country music signals had the largest change in centroid frequency over time while classical and jazz signals had the lowest change. This is expected as classical and jazz music generally have less activity over tim e compared to the other 4 genres.

i
s

2

Ô
0.5

Rock

C lassical

Country Type

Folk

Jazz

Pop

F ig u r e 4 .5 : Mean of centroid ratio to previous tim e window

4 .3 .4

B a n d w id th

B andw idth is the m agnitude-weighted average of the difference between the signal s spectral com ponents and centroid. It can be defined as:

E;so(/-/i(«))rmn./)
TFD( n, f )

(4.5)

Effectively, it shows the spectral shape and the spread of energy relative to the

67

centroid, therefore it is also a non-stationary feature. without noise has zero bandwidth.

For instance, a sine wave

4.3.5

Silence ratio

Silence ratio is the number of silent time window frames with total energy less than 0.01. This threshold is set empirically. Note that this feature could also be extracted from the time domain. Bandwidth, brightness and silence ratio have been proven to be effective in previ ous audio classification papers including [31, 33] although an STFT approach showing the rate of change to previous windows has not been used.

4.3.6

Sum m ary o f Features

Using the above analysis, the 10 features extracted for each sample included mean and standard deviation of centroid frequency (IMF) , mean centroid (low-frequency range), mean of centroid ratio to previous window, mean bandwidth, silence ratio, mean and standard deviation of the frequency location with the lowest energy, mean and standard deviation of entropy. These features are summarized below. Note that the timeindex for each window is denoted by n and there are N time windows in total. Also, e()denotes the energy in a specific time windowwhile expected or mean value operator. 1. Mean Entropy : E[H{n)\n = u i .. .Un ] 2. Standard Deviation of Entropy : STD[H{n)\n = n i...n N ] 3. Mean IM F / Centroid Frequency: E[fi{n)\n = n i...n N ] (4.8) (4.7) (4.6) E\\ denotes the

68 4. Standard Deviation o f IM F / Centroid Frequency: S T D [fi{n )\n = n i...n N ] 5. M ean IM F Ratio to Previous Window: ^ I f i M / f i i n i ) , fi{n /)/fi{n 2 ), fi{ n /) /fi{ n z ) ...] . M ean IM F (low hand 0-5KHz): ^[fi(_lowband){p}\'^ ~ 7. M ean Bandwidth: E[Bi{n) |n = Til. . . un]
8

(4.9)

(4.10)

6

. . . Hjif]

(4.11)

(4.12)

. Silence Ratio:
t in

E
n = 7 ii

(4.13)

where:

^(")= { o: 2 ^ " ' "
9. M ean o f Frequency with Lowest Energy: E[f{emin{n))\n = n i ...n N ] 10. Standard deviation of Frequency with Lowest Energy: S T D [ f( e m in { n ) ) \n ^ n i...n N ]

(4.14)

(4.15)

(4.16)

4 .4

A u d io C la ssifica tio n

Once th e features are extracted for the 143 audio signals, linear discrim inant analysis (LDA) cases. is th en apphed using SPSS software [41], to predict group classification of

69

4.4.1

Linear Discrim inant A nalysis

There are several techniques that can be used for data classification. One of the most popular techniques includes linear discriminant analysis which has been used in speech recognition and face recognition techniques. LDA offers the benefit that it can handle different within-class frequencies. In fact, the discriminant function finds the coefficients
61

...

6 10

that will maximize the ratio of between-class variance to the

within-class variance. The objective here is to obtain the highest possible ratio so that adequate class separability is obtained. Note that another common data classification technique is cluster analysis where the software is not told which groups or classes the data set belongs to and its objective is to find the best way in which the cases may be clustered into groups. In discriminant analysis however, the groups are predetermined and the objective is to find the linear combination of independent variables that will best discriminate among the groups. In the proposed classification algorithm, use of LDA is preferred as it is supervised and will therefore be more likely to group classes into the correct genres compared to an unsupervised cluster analysis. In fact, LDA tries to find a linear combination of those extracted features that best separate the group of cases. To represent this linear combination, a discrimination function is formed using the extracted features as discrimination variables and can be expressed as: L = biXi 4- b2X2 + ...... + feio^^io + c, (4.17)

where b \... b\o are the coefficients, c is a constant and are both derived using the Fisher's linear discriminant analysis [41]. Also, X i... xio are the set of extracted TF features and L is a function which classes the cases into different groups. In the case where more than two groups exist, this technique finds the first function that separates the groups as much as possible and then finds further functions that improve the separation and are uncorrelated to previous ones. The number of functions is the smallest of the number of predictor variables or features or the number of groups available minus one.

70

SPSS com putes the within-class scatter m atrix and the between-class scatter m a trix for all th e samples of classes. The within-class scatter m atrix is the expected covariance of each of the classes and is expressed as [42]:

c
~
Ù

(4.18)

where pj is the apriori probabilities of the classes, Xj is the sample of class j , p,j is the m ean of the class j and C is the number of classes. The between class m atrix on the other hand is:

c
Sh = j where p. is the m ean of all classes. Now the algorithm will maximize the ratio of the between class to w ithin class scatter. We can also describe the above process simply th at linear planes are used to divide each d a ta set into different groups. The covariances and probabilities are used to confine the area in the space where each class of signal occur. Once this area is defined, statistical distances are calculated between the centroid of each of the classes and linear planes are introduced to segregate the classes. - //)(//; --//)^, (4.19)

4 .4 .2

C la ssific a tio n R e su lts

The audio files are categorized into six groups (rock, classical, country, folk, jazz and pop). SPSS shows the standardized canonical discriminant function coefficients which indicate the relative im portance of the independent variables (the in predicting the dependent or the music type(Figure 4.6). Using F isher's coefficients and prior probabilities of each group, a scatterplot (Figure 4 . 7 ) is created showing the discriminant scores of the cases on two discrim inant functions. This plot shows the separation between different cases. T he territorial m ap (Figure 4.8) shows a plot of the boundaries used for classifying cases into groups based on discriminant functions. Note th at where we see 63 the top of the m ap is the border where in the discriminant space, group separated from group
3 6 10

features)

at

(PO P) is

(country) music.

71
s ta n d a rd iz e d C anonical D iscrim inant Function C oefficients Function 3 -.716 .631 1.205 -.913 1.018 -.575 1.667 -1.718 -1.257 .690

1 m ean entropy m ean minimum freq m ean Inst freq m ean If low m ean If ratio low energy ratio bandwidth std entropy std if std_min freq .032 .997 -4.004 4.320 .707 .862 1.388 1.023 -.273 .219

2 -1.649 2.427 -2.262 2.107 -.655 .166 .905 .375 .447 1.506

4 -1.228 .509 2.945 .185 .166 -.133 -.768 -.410 1.104 .439

5 -.291 1.760 2.256 -2.703 -.683 .850 2.100 -.503 -3.245 1.140

Figure 4.6: Standardized Canonical Discriminant Function Coefficients The classification table or the confusion matrix depicted in Table 4.1 shows the performance of LDA. In this table when the prediction accuracy is 100%, all the cases will lie on the diagonal. In fact, the hit ratio is defined as the number of cases that are on diagonal or the percentage of correct classifications. Using the original LDA, 93.0% of all original grouped cases are correctly classified with folk music having the lowest rate. A more accurate estimate is obtained through the cross-validated method where a portion of cases belong to the learning sample and the other cases belong to the test sample. In fact, in [43], it was stated that "the use of nonparametric error estimates may lead to biased results if the kernel covariances are estimated from the same data as are used to form the error estimate." It was also shown that the leave-one-out method (also referred to as the Jack-knife algorithm), provides a least-biased estimate. In the leave-one-out type estimate, one sample case is excluded from the feature vector and the classifier is then trained with all the remaining samples. The excluded data now belonging to the test data is used to determine the classification accuracy. The data is re-entered into the learning sample and a different sample case is excluded and used to test the classification accuracy. This process is repeated until all the samples of the vector have been used as test samples. The number of correctly classified cases is then used to calculate the classification accuracy rate. Since each signal is excluded from the training set in turn, the independence between the test set and the training set is maintained. Using the leave-one-out method, 92.3% of songs were correctly

72

C a n o n ic a l D iscrim inant Functions

pop
|a22



Folk

file type
Group Centroid»

Classic^
-2
-

O

pop jazz Folk count

CM

Classical
u.

ROCK

Function 1

F ig u re 4.7: All-groups scatter plot with the first two canonical discriminant functions classified, revealing the discrimination strength of our feature set. Finally, we compare our database of sounds to one of the popular m ethods men tioned in Section 4.2, the Muscle Fish Project which started in 1996 by Blum et al and is a commercially licensed software th a t allows you to search for audio files th at sound like a given file. Similarity is based on perceptual features such as loudness, pitch, brightness and bandwidth. Figure 4.9 shows the audio classification results from Musclefish when asked to classify other files th a t sounded like rock. We provided the rock signal training set: acl.wav, ac 2 .wav, acsl.wav, acs 2 .wav, del.wav, de 2 .wav... and we were looking for the following files to be classified under rock ac3.wav, ac4.wav, acs3.wav, acs4.wav, de3.wav, de4.wav ... (testing sequence). As the figure shows, all 143 records were given sorted in closest Euclidean distance but several files were misclassified as ROCK

73

M ethod 1. Original

Type RO CL CO FO JA PO Overall RO CL CO FO JA PO Overall

RO 14
0 0 2 0 0

CL
0

CO
0 0

FO
2 0 0

JA
0 0 0 1

PO
0 1 1 1 0

30
0 0 0

15
1 0 0 0 0

27
1 0 2 0 0

15
0 0 0 0 1

0
0

32
0

CA% 87.5 96.8 93.8 84.4 93.8 100 93.0 87.5 96.8 93.8 81.3 93.8 100 92.3

2. CrossValidated

14
0 0 2 0 0

30
0 0 0

1
1 2 0

15 1
0 0

26
1 0

15
0

0

32

Table 4.1: Classification results. Method: Original - Linear discriminant analysis, Cross validated - Linear discriminant analysis with leave-one-out method (RO-Rock, CL-Classical, FO-Folk, Ja-Jazz, PO-Pop, CA% - Classification accuracy rate) while they were not. These files are highlighted in Figure 4.9. There were 16 rock files in total,
8

were used for training and
20

8

for testing. As seen in Figure 4.9, the
3

16 rock files were returned in the top classified under rock among them.

matches, however

non-rock files were also

In the case of classical music, the files used in the training set inclnded bchrisl, bchris2, bchris5,bchris6, chrisl, chris2, chrisS, chris6 and the testing set included: bchris3, bchris4, bchris7,bchris8, chris3, chris4, chris7, chris8 . There were 31 classical music files in total, 16 were used for training and 15 for testing. The 31 files were recovered in the top 61 matches. In the top 61 closest matches, 30 other non-classical files were also listed. We tested the Musclefish demo for all 5 classes of songs and a 15-60% misclassification rate existed.

74

4.5

C h a p te r S u m m ary

In this C h ap ter, we examined a technique where features used to classify music signals are derived directly from the T F domain. Using six different genres for classification, we have shown th a t high accuracy rates can be obtained using features th at reflect the non-stationarity properties of audio signals and are able to depict its spectral, energy and entropy change over time. In fact, using the original LDA, 93.0% of all original grouped cases were correctly classified while 92.3% of songs were correctly classified using the leave-one-out technique. In addition to the success rate, the algorithm has a low com putational complexity compared to techniques using HMM, ANN, or SVM, and offers versatility as it can be apphed to any audio signal w ithout alteration.

75

T e r r i t o r i a l Map ( A s s u m in g a l l f u n c t i o n s b u t t h e f i r s t C a n o n ic al D is c rim in a n t F u n c tio n 2 -6 .0 -4 .0 -2 .0 .0 2 .0
6.0

tw o a r e z e r o )

4 .0

6.0

63 63 63 63 63 63 333 33555 3355554 33555444 33 5 4 444 33 3 444 33 3 444 33 3 444 3 34 44 33344 63 63 2226666 2222666 6444 64 64 64 64 224 24 * 4 44 11 4 4111 4* 411 4 4111 44411 4 411 1 44411 4 4111 2222664 44111 44 44411 2226666 22 2 2 6 6 6 2226666 63 63 63 63 63 63 63
666
*

4.0

2.0 222666 222666

6333444*

.0

-

2.0

24 24 24 24 24 24

-4 .0

24411 211 21 21 21 21

-

6.0

21
-

-6

0

-4 .0

-2 .0 .0 2 .0 C a n o n ic al D is c rim in a n t F u n c tio n

4.0 1

6.0

Figure 4.8: Territorial Map- Symbols used in territorial map: Symbol, Group, Label; 1 1 ROCK; 2 2 Classical; 3 3 Country; 4 4 Folk; 5 5 Jazz; 6 6 Pop;*-Indicates a group centroid

76





1

I







I

1


N am e . Distance D.ZS-i »ci ,wav D.zns jsc2 D.3IB ac l.v iw 0.338 a w Z .w â â 0.360 fa ts 4.wav 0,394 a t i a ts i V K 3V 0,401 O es2% w 0.417 d«3w4» 0.440 iJ«s3,Vi-av 0,449 ttesl.w av 0.436 6e4.wav 0,462 ^acsS.wa-t 0,434 0«2.wav 0,584 sco15.wav 0 5 8 6 stolT.waw 0 6 9 4 sccrW.wav 0 5 9 7 Oesl ,ww 0 6 1 8 d«l,wa< 0631 bg fee 13.wav 0 6 8 4 bs cot1.wav 0 6 8 5 b g reelfS .w t 0.637 bgreet$.w ày ô r a ï ' bgreekr.w sf O ural CÏI 4,938 4,989 4,939 4,889 4,339 4,939 4,989 4,989 4,889 4.389 4.989 4,989 4.889 4.838 5 5 5 4,989 4,089 5 5 5 5 6

1
Segm ent no no no no no no no no no rw no no no

1

1

1 1

0 .3 9 8

n o

no no no no no no no no no

n o

0 .7b g sM S r l'.w a w 0712 BODtiiiwair

5 ---- 0.

6n o

Louij Lou dR ate Pitch Rale Brlphl I BngMRale! -24839 0 923 112523! 005 1.339 527 0 001 1,1 04 ! -21,798 1,315 237 0 004 137,63 ! 0,073 -20.746 0.341] 1 6 5 4 0 6 !" ......... 0062 ! 1:255432 0,002 21 136 0 797] 141051 !.............0:053 1,405 11 1 0 055 0-914 ; -20,702 143459! 0659 f ......1:264 794 0.063 -21,29 1.218 89 0 0671 0,361 i Î24fé66 i C ,, I h 0 846 : 1,454 054 -21,046 1 6 057 0.739 ; 116,939  0 015 0,046 i 1,240,156 0Ô3 -19,897 i 0192 f .....137:754" ....... L343 732 -20.105 ! 0,359 1 " Ï 4T 3T7 0:055 1 0 05 3 -20,947 137,204 0 042 : 1,355 729 0,09 ...0 053 0.9 ! -21.4 r 0 051 1,345996 0 059, 143.063 * 0 0561 -21 096 0.375 1 0 061 ' ........1,35 3 942 Î3 1 5 2 1,221 721 ......'6 067! 0 055 -21,045 "129 M 9 0-95,r r 156,741 ! 1:224 251 0 032! D.45! 0,045 -19,562 1,50 3 9061 0.4131 364.587 0 0351! -21,673 1,471 739! 0.37 0 0 29!I -21,637 374,439 0,364 ! 1,496.618 0*027 362,1 4j -21,888 0,033, 1,573 497 0 012 0 034 -27,447 i " . . . o . o M : __ 202-724! *1,573,104* 0 04l' 0 013 227.959 0,705 -27,412 1,153,605 0,804 005* 7:! -14,547 12*2645 1,508478 0029 1«2253 0,015 I 0,612 -21 ,GM : *1,1*61*943 0 051 152142 0,92 -i5,3S« * 0 *013 0 044 * 1,106154 0,744 121.539 -14,25 0049 1*47,097 0,058 * "f,139326 0,723! -13,776 132904 1.148.259 *0,1»; 0J829j -l& W 1,617.474 1 474.677 0J854 19.694

_ _P ith

R a te
44100 44100 44100 44100 44100 44100 44100 44100 44100 44100 44100 44100 44100 44100

I

I

003 4 6 4 1 0 0 1 00 2 8 4 4 1 0 0 1 6 4 4 1 0 0 1 6
44100 44100 16 16

m m 16 16 16 16 16 16 16 16 16 16 16 16 16 16

0 Q 6 t

0 8 3 3 1

4 4 1 0 0,16 1 6 44100 1 6 44100 4 1 0 0 , ; .....0 .0 5 6 4 0 .0 2 9 « 4 1 0 0

4 1 0 0 16 9.m 4 6 4 4 1 0 0 1

F ig u re 4 .9 : C om parison w ith musclefish

Chapter 5 Conclusions

for our work were explained in Chapter 1. TF theory and STFT analysis were intro duced in Chapter 2. Chapter 3 discussed a novel spread spectrum audio watermark ing technique based on instantaneous mean frequency and using spectrum technology. Audio classification using TF approach and new TF based perceptual features were discussed in Chapter 4. In this Chapter we will present a summary of our results and future work. Publications generated from our work are listed in Appendix A.

I

N this thesis, we examined a technique where features used to classify and water mark signals are derived from the joint TF domain. Introduction and motivation

5.1

Summary of results
1

The summary of our results can be divided into two sections; watermarking and 2. Results of audio classification.

. Results of audio

5.1.1

Spread spectrum waterm arking and instantaneous m ean frequency
5

Our novel audio watermarking algorithm was tested using

different types of audio

signals including classical, pop, rock, and country music. Using TF analysis, the watermark (consisting of 25 bits within a 5 second sample of an audio signal) was embedded using the IMF of the audio signal and perceptual shaping. The watermark

77

78

was exposed to common signal processing attacks including MP3 compression, addi tive noise, filtering operations, equalization, noise reduction, resampling and additive white noise. This resulted in a bit error rate in the range of 0-13%. Based on the research in this area and our experimental results, we were able to make the following deductions: · The perform ance of a spread spectrum watermarking technique is proportional to the length of the PN sequence. The longer the length of the PN sequence, the b e tte r the recovery of the hidden message. This is because, in the recovery stage, there will exist a higher degree of correlation between the message spread w ith the PN sequence and the locally generated PN sequence. And even if the message is exposed to channel distortions or AWGN, there is higher chance of recovery. However, as the length of the PN sequence is increased, the number of message bits m ust be decreased therefore the message payload is not as high. · T here exists a tradeoff between the imperceptibility and the robustness of the em bedded w aterm ark. In order to decrease the BER and improve the w ater m ark 's recovery, the energy of the waterm ark must be relatively high compared to th e music segment. However, by increasing the embedding strength of the w aterm ark, we are making it more perceptible within the audio segment. An ideal balance needs to be reached by using perceptual shaping th a t will maxi mize th e energy of the embedded bits and improve recovery. · By exam ining different TFDs, we found th at Cohen's class of transform s based on the W VD gave high resolution in the T F domain, however such TFD s pre sented several drawbacks. First, the TFD of multi-component signals suffered from cross term interference. Due to the tem poral masking properties of the hum an ear, it is sufficient to compute the IF in each time window using ST FT analysis (referred to as IMF). Such ST FT analysis is also more ideal since it has a low com putation tim e and can be practically implemented compared to th e W VD.

79

· In examining the IF to be extracted we found that by using the direct definition of IF as the derivative of the phase, several problems exist. For one thing, it is necessary to compute an analytical signal first using Hilbert transform which will generate an IF at each instant in time. Depending on the type of signal, the IF can yield negative values using this definition which will render it meaningless. Second, as mentioned earlier, there is no need to compute the IF at every instant in time (especially in the case of audio signals), leading us to the definition of IMF for each window. · Using the proposed IMF, the watermark can be modulated to a perceptually undetectable and statistically imperceptible region of the audio signal. Along with the benefits offered with spread spectrum technology, a highly secure wa termark can be obtained as the security of the system is not dependent upon the knowledge of the algorithm. · Other benefits of this type of watermarking can be summarized as follows. First, since the algorithm is content based, it is adaptive to various audio files where algorithms with fixed attenuation are unable to maximize the robust ness / imperceptibility criteria. Second, the algorithm is less computationally complex than frequency domain watermarking techniques. Finally using the IMF of the signal we maximize the robustness to various attacks by embedding in perceptually significant regions. · The disadvantage of this system is that we have assumed perfect synchronization between the receiver and transmitter. This means that an intentional synchro nization attack such as cropping would destroy the watermark. To improve this performance, several solutions such as redundancy coding or synchronization bits have been proposed in literature [44].

5.1.2

C ontent based audio classification

Our novel audio classification algorithm was tested on 143 different music segments consisting of 5 different types of audio signals including classical, pop, rock, folk and

80

country music.

In classifying music into six different genres, we have shown th at

high accuracy rates can be obtained using features th at reflect the non-stationarity properties of audio signals and are able to depict its spectral, energy and entropy change over tim e. In fact, using the original LDA, 93.0% of all original grouped

cases were correctly classified while 92.3% of songs were correctly classified using the leave-one-out technique. These results were also compared to a popular commercially licensed software (Musclefish) where a classification error of up to 50% was achieved. Based on the research in this area and our experimental results, we were able to make the following deductions: · The entropy of a signal is an efficient and simple technique for computing the tonality and noise of a music segment. This novel T F derived feature along with its stan d ard deviation, performs well for discriminating between different genres of music. However, further T F features are required to improve classification accuracy. · O ther T F derived features such as overall IMF, IMF from low subbands IMF ratio, instantaneous bandw idth can efficiently characterize music signals. The IM F ratio allows us to monitor the spectral change of an audio signal while the instantaneous bandw idth can show the spread around the IMF of the signal. · Techniques th a t extract feature from the time or frequency domain alone can achieve high classification rate for discrimination between music and speech segments since their spectral characteristics are quite different. However, such techniques will not perform efficiently for discrimination between different gen res of music th a t can have similar sounds. · For p a tte rn classification, it is advantageous to use linear discrim inant analysis th a t has commonly been used in speech recognition algorithms. Such technique has low com putational complexity and offers versatility as it can be applied to any audio signal w ithout altering the algorithm. Also, neural network classifiers which are popular are not always ideal as they have a black box effect where the efficiency of the extracted features can not be examined.

81

5.2

Future work

· In terms of audio classification, the algorithm should be tested on a larger database of signals including other genres of music such as rap, hip hop and international music among others. The TF approach and the features extracted could also be extended to other forms of multimedia data and its efficiency examined. Further work should also include examining other classification methods such as minimum classification error (MCE) instead of LDA to improve classification accuracy. · In audio watermarking, a larger database of signals should also be tested for imperceptibility of embedded watermark and recovery rate. In order to improve the BER for larger message sizes and small PN sequence lengths, error correcting schemes should be implemented. Finally, the effect of further attacks such as cropping and dual watermarking should be examined. · For both audio classification and audio watermarking techniques, optimization of window sizes used in the STFT could be examined to improve feature accuracy.

B ib lio g ra p h y
[1] IS O /IE C JT C 1/SC 29/W G 11 N4031, "Overview of the MPEG-7 standard," Mar 2001 . [2] S. Qian and D. Chen, Joint Time-Frequency Analysis: Method and Application^ Prentice Hall, New York, NY, 1996. [3] G. Jones and B. Boashash, "Instantaneous frequency, instantaneous bandw idth and the analysis of multicomponent signals," International Conference on Acous tics, Speech, and Signal Processing, vol. 5, pp. 2467-2470, April 1990. [4] J. G ibeaut, "Facing the music," A B A Journal, vol. [5] "http://zdnet.com /2100-1104_2-5130503.htm l," . [6 ] V. Chen and H. Ling, Time-frequency Transforms fo r Radar Imaging and Signal Analysis, A rtech House, San Diego, CA, 2002. [7] A.V. Oppenheim and R.W . Schafer, Discrete-Time Signal Processing, Prentice Hall, New Jersey, NY, 1999. [8 ] P.J. Kootsookos, B.C. Lovell, and B. Boashash, "A unified approach to the stft, tfds, and instantaneous frequency," IE EE Transactions on Signal Processing, vol. 40, no.
8 86

, pp. 36-41, October 2000.

, pp. 1971-1982, Aug 1992.

[9] S. A tal and S.L. Hanauer, "Speech analysis and synthesis by linear prediction of the speech wave," J. Acoust. Soc. Amer., vol. 50, pp. 637-655, 1971.

82

83

[10] R. Schafer and L. Rabiner, "Design and simulation of a speech analysis-synthesis system based on short-time fourier analysis," IEEE Transaction, vol. AU-21, no. 3, pp. 165-174, 1973. [11] L. Rabiner and B.H. Juang, Joint Time-Frequency Analysis: Method and Appli cation, Prentice Hall, Englewood Cliffs, NJ, 1993. [12] J.L. Flanagan and R.M. Golden, "Phase vocoder," Bell System Technical Jour nal, pp. 1493-1509, 1996. [13] W. Bender, D. Gruhl, N. Morimoto, and A. Lu, "Techniques for data hiding," IBM Systems Journal, vol. 35, no. 3-4, pp. 313-336, 1996. [14] S. Graver et ah, "What can we reasonably expect from watermarks?," in Pro ceedings of the IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics, 2001, pp. 223-226. [15] J.D. Gordy and L.T. Bruton, "Performance evaluation of digital audio water marking algorithms," in Proc. JSrd Midwest Symp. Circuits and Systems, August 2000, pp. 456-459. [16] M.D. Swanson, B. Zhu, A.H. Tewfik, and L. Boney, "Robust audio watermarking using perceptual masking," Proc. IEEE Signal Processing, vol. 1998. [17] L. Boney, A. Tewfik, and K. Hamdy, "Digital watermarks for audio signals," in IEEE International Conference on Multimedia Computing and System, June 1996, pp. 473-480. [18] P. Bassia, I. Pitas, and N. Nikolaidis, "Robust audio watermarking in the time domain," IEEE Trans. Multimedia, vol. 3, no. 2, pp. 232-241, June 2001. [19] S. Erkucuk, "Time-frequency analysis of spread spectrum based communication and audio watermarking systems," in MSc thesis, Ryerson University, July 2003.
66

, pp. 337-355,

84 [20] S. Esmaili, S. Krishnan, and K. Raahemifar, "A novel spread spectrum audio w aterm arking scheme based on time-frequency characteristics," in Proceedings o f the IE E E Canadian Conference on Electrical and Computer Engineering [CDROM], M ontreal, Quebec, May 2003. [21] B.P. Lathi, Modern Digital and Analog Communication Systems, Oxford Uni versity Press, New York, NY, 1998. [22 ] R. Peterson, R. Ziemer, and D. Borth, Introduction to Spread Spectrum Com munication, Prentice Hall, New Jersey, NY, 1995. [23] R.L. Pickholtz, "Spread spectrum for mobile communications," IE E E Transac tions on Vehicular Technology, vol. 40, no. 2, pp. 313-321, May 1991. [24] S. Hay kin. Communication Systems, John Wiley & Sons Inc., New York, NY,
2001.

[25] P.G. Flikkema, "Spread spectrum techniques for wireless communication," IE E E Signal Processing Magazine, vol. 14, no. 3, pp. 26-36, May 1997. [26] B. Boashash, "Estim ating and interpreting the instantaneous frequency of a Proceedings of the IEEE, vol. 80, no. 4, pp.

signal, p a rt i: Fundam entals," 520-538, 1992.

[27] P. Loughlin and B. Tracer, "Instantaneous frequency and the conditional mean frequency of a signal," Signal Processing, vol. 60, no. 2, pp. 153-162, 1997. [28] S. K rishnan, "Instantaneous mean frequency estim ation using adaptive time-

frequency distributions," in Proc. IE E E Canadian Conference on Electrical and Computer Engineering, Toronto, Ontario, May 2001, pp. 141-- 146. [29] T. P ainter and A. Spanias, "Perceptual coding of digital audio," Proceedings o f the IEEE, vol.
88

, no. 4, pp. 451-513, Apr 2000.

85

[30] A.P. Petitcolas et al., "Stirmaxk benchmark: Audio watermarking attacks," in International Conference on Information Technology: Coding and Computing (ITCC '01), Las Vegas, April 2001, pp. 49-55. [31] E. Wold, T. Blum, D. Keislar, and J. Wheaton, "Content-based classification, search, and retrieval of audio," IEEE Multimedia, pp. 27-36, 1996. [32] J. Foote, "Content-based retrieval of music and audio," in Multimedia Storage and Archiving Systems II, Proc. of SFIE, 1997, pp. 138-147. [33] Z. Liu, J. Huang, Y. Wang, and T. Chuan, "Audio feature extraction and analysis for scene classification," in IEEE Workshop on Multimedia Signal Processing, June 1997, pp. 343-348. [34] M. Liu and C. Wan, "A study on content-based classification and retrieval of audio database," in Proc. IDEAS-Ol, July 2001, pp. 339-345. [35] G. Lu and T. Hankinson, "A technique towards automatic audio classification and retrieval," in Fourth International Conference on Signal Processing, Beijing, China, October 1998, pp. 1142-1145. [36] L. Lu, H. Zhang, and S. Li, "Content-based audio classification and segmentation by using support vector machines," ACM Multimedia Systems Journal 8, vol. 8 , no. 6 , pp. 482-492, March 2003. [37] K. Umapathy, "Time-frequency modelling of wideband audio and speech sig nals," in MPhil thesis, Ryerson University, October 2002. [38] T. Zhang and C. Kuo, "Hierarchical classification of audio data for archiving and retrieving," in Proc. ICASSP, March 1999, pp. 3001-3004. [39] D. Perrot and R.O. Gjedigen, "Scanning the dial: An exploration of factors in the identification of musical style," Proceedings of the 1999 Society for Music Perception and Cognition, p.
88

, 1999.

86 [40] G. T zanetakis and P. Cook, "Music genre classification of audio signais," IE E E Transactions on Speech and Audio Processing, vol. 10, no. 5, pp. 293-302, July 2002 . [41] SPSS Inc., "SPSS advanced statistics user's guide," in User manual, SP SS Inc., Chicago, IL, 1990. [42] A. M artinez and A. Kak, "Pca versus Ida," IEEE Transactions on Pattern

Analysis and Machine Intelligence, vol. 23, no. 2, pp. 228-233, February 2001. [43] K. Fukunaga, Introduction to Statistical Pattern Recognition, Academic Press, Inc., San Diego, CA, 1990. [44] D. Kirovski and H. Malvar, "Spread-spectrum audio watermarking: Require m ents, applications,and lim itations," E E E Fourth Workshop Multimedia Signal Processing, pp. 219-224, October 2001.

Appendix A List of Publications
This section lists our contributions to research and development including work that has been published or submitted. Refereed Journals · S. Esmaili, S. Krishnan and K. Raahemifar, "Audio watermarking using timefrequency characteristics," Canadian Journal of Electrical and Computer Engi neering, vol. 28, no. 2, pp. 57-61, 2003. · S. Esmaili, S. Krishnan and K. Raahemifar, "Audio classification and retrieval via STFT parameters," submitted to Eurasip Journal on Applied Signal Pro cessing, Nov. 2003. Special issue on Anthropomorphic Processing of Audio and Speech. Refereed Conferences · S. Esmaili, S. Krishnan and K. Raahemifar, "A novel spread spectrum audio watermarking scheme based on time-frequency characteristics," Proc. IEEE Canadian Conference Electrical and Computer Engineering (CCECE) , vol. 3, pp. 1963-1966, May 2003. Awarded the Best Paper prize among 730 submis sions. · S. Esmaili, S. Krishnan and K. Raahemifar, "Content based audio classification and retrieval using joint time-frequency characteristics," accepted in Intema-

87

88 tional Conference on Acoustics, Speech, and Signal Processing (IC ASSP), May 2004. W ork sh op s · S. Esmaili, A. Ramalingam and S. Krishnan, "W atermarking and Retrieval of M ultim edia D ata," to appear in the proceedings of Micronet Annual Workshop, April. 2004. Awarded the Best Paper prize in Systems Group. H o n o u ra b le M en tio n · O btained honourable mention in IE E E Canadian Review, vol. "Summer 2003" , no. 44, p p .27-28, 2003.

