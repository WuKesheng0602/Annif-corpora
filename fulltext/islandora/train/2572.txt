STOCHASTIC OPTIMAL CONTROL WITH APPLICATION IN VISUAL SERVOING

by Aidin Foroughi
B.Sc., University of Tehran, 2010

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Applied Science in the Program of Mechanical and Industrial Engineering

Toronto, Ontario, Canada, 2013 c Aidin Foroughi 2013

Author's Declaration

I hereby declare that I am the sole author of this thesis or dissertation.

I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

i

Abstract

STOCHASTIC OPTIMAL CONTROL WITH APPLICATION IN VISUAL SERVOING
Master of Applied Science, 2013 Aidin Foroughi Department of Mechanical and Industrial Engineering Ryerson University In this thesis, a new inference-based solution to stochastic optimal control (SOC) for general nonlinear systems is developed. This novel method applies to standard SOC problem, as well as robust and risk-seeking variations. The presented approach unifies many existing works, and makes possible, inference-based approximations to be applied to robust, risk-seeking, and standard SOC problems. Thus, an approximate method based on extended Kalman filtering is developed and tested on the inverted pendulum problem, and compared with existing methods. As an application, the developed algorithm was adapted to a practically important problem in visual control in robotics known as image-based visual servoing (IBVS). The proposed control methodology for visual servoing was implemented for real-time experiments, and was compared with the standard IBVS methodology. The experimental results show that the proposed method can improve the myopic behaviors of standard IBVS methodology.

ii

Dedication

To my parents, Farzad and Nahid, and to my gaurdian angel, Parna.

iii

Contents

1 Introduction 1.1 1.2 1.3 1.4 Background and Origins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Challenges and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4.1 1.4.2 1.5 1.6 An Efficient Approximate SOC Solution . . . . . . . . . . . . . . . . . Visual Servoing via SOC . . . . . . . . . . . . . . . . . . . . . . . . . .

1 1 2 3 6 6 7 8 9 10

Notation and Other Conventions . . . . . . . . . . . . . . . . . . . . . . . . . Structure of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 The Classics: Dynamic Programming 2.1 2.2 2.3 2.4

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Stochastic Optimal Control with Full Observability . . . . . . . . . . . . . . . 11 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Bellman's Principle of Optimality and Dynamic Programming . . . . . . . . . 13 2.4.1 2.4.2 2.4.3 2.4.4 Bellman's Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Complexity of Dynamic Programming . . . . . . . . . . . . . . . . . . 16 Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.5

Related Works and Our Approach . . . . . . . . . . . . . . . . . . . . . . . . 17 19

3 Inference for Optimal Control 3.1 3.2 3.3 3.1.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Roadmap and Contributions . . . . . . . . . . . . . . . . . . . . . . . 19 Stochastic Policies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Robust, Optimistic and Standard SOC Problems . . . . . . . . . . . . . . . . 23 3.3.1 3.3.2 Addressing Robustness: Pessimistic Control . . . . . . . . . . . . . . . 24 Optimism: Risk-seeking Control iv . . . . . . . . . . . . . . . . . . . . . 25

3.3.3 3.4 3.4.1 3.4.2 3.4.3 3.4.4 3.4.5 3.5 3.6

Standard SOC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Unconstrained Optimal Distribution Problems . . . . . . . . . . . . . 27 Constrained Optimal Distribution Problems . . . . . . . . . . . . . . . 29 The Relationship Between KL-Constrained and KL-Penalized Problems 30 The KL-Constrained Maximization Problem . . . . . . . . . . . . . . . 32 Relationship to Inference . . . . . . . . . . . . . . . . . . . . . . . . . 33

Optimal Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

A Proximal Point Method for SOC . . . . . . . . . . . . . . . . . . . . . . . . 35 The Solution to the Proximal Point Problem . . . . . . . . . . . . . . . . . . 37 3.6.1 3.6.2 3.6.3 3.6.4 Backward, Synchronous Updates . . . . . . . . . . . . . . . . . . . . . 38 Alternating Convex Optimization . . . . . . . . . . . . . . . . . . . . . 40 Viewing Updates as Messages . . . . . . . . . . . . . . . . . . . . . . . 40 The Special Case of Posterior Policy Iteration . . . . . . . . . . . . . . 41 Choice of k and k . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 Application to the Cart-Pole Problem . . . . . . . . . . . . . . . . . . 47 3.7.2.1 3.7.2.2 3.7.3 Convergence Rate and Convergence Success . . . . . . . . . . 49 Robust and Risk-Seeking Control . . . . . . . . . . . . . . . 49

3.7

An Extended Kalman Filter Approximation . . . . . . . . . . . . . . . . . . . 43 3.7.1 3.7.2

Advantages of Our Solution . . . . . . . . . . . . . . . . . . . . . . . . 52 Linearly Solvable SOC . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 Dynamic Policy Programming . . . . . . . . . . . . . . . . . . . . . . 54 Robust Control Approach of Sargent-Hansen . . . . . . . . . . . . . . 54 Rationality Bounded Games . . . . . . . . . . . . . . . . . . . . . . . . 54 Risk-Sensitive Control and Stochastic System with Relative Entropy Uncertainty Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 Other Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . 56

3.8

Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.8.1 3.8.2 3.8.3 3.8.4 3.8.5 3.8.6

3.9

Filtering Problem and Integration with SOC . . . . . . . . . . . . . . . . . . 56 58

4 Application to Visual Servoing 4.1 4.1.1 4.1.2 4.1.3 4.1.4 4.2 4.3 4.4 4.5 4.6

Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Kinematic Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Visual Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 Dynamics of Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Classical vs. Advanced Control Methods and Problem Definition . . . 64

State Space Model for Image Features . . . . . . . . . . . . . . . . . . . . . . 67 Partial Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 Cost Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 Scheduling for Real-time Implementation . . . . . . . . . . . . . . . . . . . . 70 The Choice of k and k Parameters . . . . . . . . . . . . . . . . . . . . . . . 72 v

4.7 4.8 4.9

The Description of the Complete Algorithm . . . . . . . . . . . . . . . . . . . 72 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Experimental Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . 80 4.9.1 4.9.2 Scenario I: Large Rotations and the Retreat Problem . . . . . . . . . . 80 Scenarios II and III: Shorter Work-Space Trajectories . . . . . . . . . 83

4.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 5 Conclusion 5.1 5.2 5.3 88

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 Limitations of the Proposed Methodology . . . . . . . . . . . . . . . . . . . . 89 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 5.3.1 5.3.2 5.3.3 5.3.4 5.3.5 Other Types of Approximations . . . . . . . . . . . . . . . . . . . . . . 90 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 90 Formal Proof of Global Convergence . . . . . . . . . . . . . . . . . . . 91 A Distributed Implementation . . . . . . . . . . . . . . . . . . . . . . 91 Application to Advanced Robotics Problem . . . . . . . . . . . . . . . 92 93

A Entropy

A.1 Kullback­Leibler Divergence ­ or Relative Entropy . . . . . . . . . . . . . . . 93 B Multivariate Gaussian Functions and Distributions C Deriving Messages D Kalman Filtering 94 96 102

D.1 The standard Kalman filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 D.2 The Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . 103

vi

List of Tables

3.1 4.1 4.2 4.3

Parameters of the cart-pole problem. . . . . . . . . . . . . . . . . . . . . . . . 48 Values of camera parameters resulted from the calibration phase. . . . . . . . 75 Table of DH parameters for the A255 robot. . . . . . . . . . . . . . . . . . . . 76 The comparison of translational, orientation error, and the work-space trajectory length for the three scenarios. . . . . . . . . . . . . . . . . . . . . . . . . 84

vii

List of Figures

2.1 2.2 3.1

A general template depicting different types of optimal control problems and the relationship among the basic concepts and terms in our problems. . . . . 11 A diagram depicting SOC problems with full observability. . . . . . . . . . . 17

Comparison of sub-optimal stochastic and deterministic policies against the optimal deterministic policy. µ ~k D and µ ~k are sub-optimal deterministic and stochastic policies, respectively, and µkD is the optimal policies. . . . . . . . 22

3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9

An illustration of a q -distribution for a one dimensional state and control system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 The relationship between  and  , which states that Prob. 6 with 1 is equivalent to Prob. 5 with 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 The relationship between  and  , including negative values for  . . . . . . 33 Inference interpretation of Eq. 3.28. A KL-penalized or a KL-constrained problem can be interpreted as standard posterior calculation. . . . . . . . . . 35 Iterations of proximal policy solutions for time slice k . In each update of the policy at time k , a better policy on a KL-ball of the one before is found. . . . 37 Out message passing solution for SOC. The temperature of messages passed from one environment to the other has to be modulated. . . . . . . . . . . . . 42 Bayesian network representation of Posterior Policy Iteration . . . . . . . . . 43 The effect of choice of k andk , on the convergence rate and the behavior of resulting policy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

3.10 The cart-pole problem. The objective of the control is to balance the pole, as well as positioning the cart on the origin. . . . . . . . . . . . . . . . . . . . . 48 3.11 Convergence rate comparison of iLQG vs. our method for  = 10-8 , and different choices of  . In most cases, our algorithm convergence within 400 iterations, whereas it takes iLQG about 650 iterations to converge. . . . . . . 50 viii

3.12 Comparison of the average of mean total cost and total cost variance for 1000 trials of the cart-pole problem with a) 1% parameter perturbation b) 3% parameter perturbation c) 5% parameter perturbation. In all cases, the dashed (red) line corresponds to the benchmark results for iLQG. seeking policy. . . . . . . 51 3.13 Comparison of the gain matrix for a robust policy the standard and a risk. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.14 A controller with varying robustness behavior. The controller starts with a pessimistic assumption about the available model of the plant and gradually shifts towards an optimistic behavior. . . . . . . . . . . . . . . . . . . . . . . 53 3.15 PPI combined with filtering . More than one observation are used in order to get more accurate state estimation. . . . . . . . . . . . . . . . . . . . . . . . . 56 4.1 4.2 4.3 4.4 A typical cycle in visual servoing demonstrating the diversity of the problem. 58 Two configurations of camera in visual servoing for industrial manipulators. . 60 Pinhole model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Image-based visual servoing. The 2D features in the initial image are matched and aligned with the 2D features in the target image, and as a result, the robot is moved to the desired pose. . . . . . . . . . . . . . . . . . . . . . . . . 65 4.5 4.6 The separation principle and certainty equivalence heuristic. . . . . . . . . . . 69 The control topology, as a result of the assumed certainty equivalence heuristic. An EKF is used as the state estimator, and the state estimation is fed to a fully observable SOC controller. 4.7 4.8 4.9 the modified online algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . 70 . . . . . . . . . . . . . . . . . . . . . . . . . . 71 The scheduling of computations in the original offline algorithm for SOC vs. A block diagram depicting the proposed algorithm for visual servoing. . . . . 74 Applying the inverse of distortion mapping. The first and second row illustrate the images obtained from the camera, before and after distortion removal, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 4.10 Experimental setup. The LED markers are tracked simultaneously by the camera and the optical tracker. . . . . . . . . . . . . . . . . . . . . . . . . . . 77 4.11 Scenarios. Each row displays the robot pose and the corresponding image seen through the camera. a) The initial pose of the robot for all scenarios. b) The target pose for scenario I. c) The target pose for scenario II. d) The target pose for scenario III. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 4.12 The motion of the robot for scenario I. It is observed that the retreat is significant for IBVS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.13 The image space trajectories of features for scenario I. . . . . . . . . . . . . . 82 4.14 The visual feature errors for scenario I. . . . . . . . . . . . . . . . . . . . . . . 82 4.15 The joint velocity of the robots for scenario I. . . . . . . . . . . . . . . . . . . 83 4.16 The image space trajectories of features for scenario I. . . . . . . . . . . . . . 84 ix

4.17 The visual feature errors for scenario II. . . . . . . . . . . . . . . . . . . . . . 85 4.18 The joint velocity of the robots for scenario II. . . . . . . . . . . . . . . . . . 85 4.19 The image space trajectories of features for scenario III. . . . . . . . . . . . . 86 4.20 The visual feature errors for scenario III. . . . . . . . . . . . . . . . . . . . . . 86 4.21 The joint velocity of the robots for scenario III. . . . . . . . . . . . . . . . . . 87

x

Nomenclature

Abbreviations ADP BP CoV DP iLQG KL LQ MDP MIMO MP OC OD PP PPI PV RDP RL ROC Approximate dynamics programming Belief propagation. Calculus of variations Dynamic Programming Iterative linear-quadratic-Gaussian. Kullback­Leibler Linear quadratic Markov decision process Multiple-Input Multiple-Output Pontryagin's maximum principle Optimal control Optimal distribution Proximal point. Posterior policy iteration. Probabilistic variational. Robust dynamic programming. Reinforcement learning Robust optimal control xi

SDDP SOC VS KF LQ LQR RBG

Stochastic differential dynamic programming. Stochastic optimal control Visual servoing Kalman filter Linear quadratic Linear quadratic regulator Rationality bounded game.

Mathematical Symbols Ls A c (x ¯, u ¯) . c , CV
CV

Interaction matrix. Ambiguity set. The set of possible dynamics of the plant. Cost function, which is a function of state and control trajectories. Cost incurred under policy  . The vertical coordinate of the center of the image, in image plane The horizontal coordinate of the center of the image, in image plane

f (xk+1 | xk , uk , k ) The time-dependent probability of landing on state xk+1 from state xk when the control signal is uk .

fc

Focal length of the camera

fk (xk+1 | xk , uk ) The probability of landing on state xk+1 from state xk when the control signal is uk .

aF

b

The homogenous transformation of frame b w.r.t. frame a. The value function, which is the expected cost under the optimal policy. The interaction matrix The dimension of the control space. The dimension of the state space. The number of features xii

Ji (xi ) Ls m n nf

aP

The coordinates of a point w.r.t. coordinate frame a The solution to a constrained optimal distribution problem, on a KL-ball with center p0 and radius .

p(,p0 )

p

(,p0 )

The Boltzmann disribution with degeneracy, p0 and temperature  . The state-control trajectory distribution for dynamics f , policy µ and initial distribution,p0 .

qµ,f,p0

R
aR b

The space of real numbers. The rotation matrix of frame b w.r.t. frame a Feature vector, which is composed of the 2D coordinates of a set of 2D features

s

U

The control space, a subset the space of real-valued vectors with dimension m.

u ¯

The control trajectory from the current time to the end of the control horizon, i.e., u0 , ..., uN .

u

Control signal. A real valued vector which specifies the control signal applied to the plant.

U V

The horizontal coordinate of a 2D point on the image plane The vertical coordinate of a 2D point on the image plane The state space, a subset the space of real-valued vectors with dimension n.

X

x ¯

The state trajectory from the current time to the end of the control horizon, i.e., x0 , x1 , ..., xN

x X Y Z Z ( )

State vector. A real valued vector which specifies the state of plant. The x coordinate of a 3D point The y coordinate of a 3D point The z coordinate of a 3D point The partition function.

Roman and Greek Symbols  (·) The Dirac delta function. xiii

 fk

The radius of the KL-ball, defining the ambiguity set for the dynamics fk .

 fk µ µ µ
o

The radius of the ambiguity set of the dynamics. The risk-seeking optimal policy. The robust optimal policy. The robust optimal policy. A stochastic policy, which is equal to Pr (Uk = uk | Xk = xk ). Velocity screw, composed of stacking rotational and translational velocities

r

r

µk (uk | xk ) 

  D S  k

The rotational velociy vector Control policy. The space of all deterministic policies. The space of all stochastic policies. The optimal policy. A control policy at time step k . It's a function from state space to control space.

Miscellaneous · KL (p (x) Expected value operator. q (x)) Kullback­Leibler divergence of q (x) from p(x).

xiv

Chapter

1

Introduction
1.1 Background and Origins

Stochastic optimal control (SOC) is a branch of modern control theory which deals with optimal control of systems with stochastic uncertainty. It is a powerful framework which allows one to consider various sources of uncertainty including measurement noise, stochastic process noise and parameter uncertainty, and can be viewed as the stochastic extension of optimal control (OC). Problems in OC date back to as early as 17th century [1] with the outset of developments in calculus of variations (CoV)1 . By the 20th century, CoV had matured enough, through the works of Euler, Lagrange, Jacobi and Hamilton, to permit the development of a complete variational framework to OC. However, development of such a framework did not occur until the end of the second world war. The post-war years constitute an important era in modern control, as very influential theories were introduced to deal with the emerging control and military applications. The classical frequency-domain and root-locus methods were gradually substituted by the more powerful state space (time-domain) models which simplified the representation of nonlinear, multiple-input multiple-output (MIMO) problems. Moreover, OC allowed direct implementation of performance criteria, such as fuel consumption and trajectory time, as cost functions to be minimized by the control. Starting in 1950s, an important development on the Soviet side was a complete variational framework for OC introduced by Pontryagin, through what is known as the Pontryagin's maximum principle (MP), articulated in his seminal work [2]. Almost in parallel to Pontryagin, a second school emerged in the U.S. through the seminal works of Bellman [3, 4], who developed the theory of dynamic programming (DP) and the Bellman principle of optimality. Further developments of the same impact (if not more) were those by Kalman [5­7] who
Calculus of variations is the mathematical field which deals with optimizating functionals. Functionals are mappings from functions to real values. Therefore, a variational problem (VP) is an optimization problem which picks a function as the optimal solution. For instance, finding the minimum energy trajectory of a robot can be expressed as a VP.
1

1

CHAPTER 1. INTRODUCTION derived optimal control for linear-quadratic (LQ) problems2 and identified the duality of filtering and control for linear-quadratic (LQ). His works resulted in what is today known as LQ regulators (LQR) and Kalman filtering (KF) which are of great theoretical and practical significance. Although both MP and DP are valid frameworks for deterministic OC problems, the former doesn't easily extend to stochastic settings while the latter does ­ and elegantly so. Therefore, since in this thesis our interest is only with optimal control of systems with random uncertainty, our focus will be on DP methodology. The works of Bellman and Kalman mark the beginning of a large and ever-increasing body of works on (stochastic) OC which were contributed by researchers from a wide range of disciplines. Indeed, an optimal controller can be viewed as an intelligent decision maker which, given a cost function, a model of the system, and models of noise and uncertainty in the system, takes appropriate actions to minimize the cost. In this sense, (S)OC is a question of optimizing a sequence of decisions. Therefore, the research on (S)OC transcends control engineering and engages researchers in any field where decisions are important. Naturally, as a result of this fact, the literature on OC and SOC is rich in fields as diverse as artificial intelligence, operations research, management science, economics and finance. The applications are just as disparate and range from robotics to inventory management [8] to addressing U.S. financial debt crisis [9]. Although these problems have a common core, there are differences among them too; control theory and economics tend to work with continuous state/control spaces, whereas operational research and artificial intelligence deal with discrete, finite state/control spaces. The term, optimal control, is usually used when dealing with problems of the former type, while the latter problems are usually discussed under the title of Markov decision processes (MDPs). Nevertheless, in both discrete and continuous spaces, when the problems become larger, one faces serious challenges.

1.2

Challenges and Motivation

One of the most challenging issues in (S)OC is computational tractability (or intractability) of problems with high and/or continuous dimensions, which are typical of problems in robotics and, generally, in control engineering. In addition to problems in control engineering, large decision making problems can occur in a number of applications in energy, aerospace and resource allocation [10]. For these high-dimensional problems, approximate solutions to (S)OC play an indispensable role, as these problems couldn't be solved otherwise. Many researchers go as far as claiming that DP, while an important development, is useless without practical approximate solutions [10]. Indeed, from the time Bellman introduced the theory of DP up until today, developing efficient numerical solutions and tractable algorithms has always been a hot topic. The continuous growth of the literature on (S)OC can be partly attributed to the ever-increasing
2

i.e. problems with linear dynamics and quadratic cost functions.

2

1.3. LITERATURE

emergence of new applications for it, and partly associated with challenges of applying DP. The past few years, in particular, have seen increased interest in research on efficient and approximate methods, due to new breakthroughs in our perception of the relationships between SOC and inference [11­15]. Inference is a broad term, and different problems, such as finding posterior distributions, marginal distributions, or maximum a posteriori estimation and the filtering problem, all fall under the umbrella of inference problems. Inference also suffers from computational complexities which afflict SOC. Inference, as a field, has a very rich literature and many efficient approaches have been developed to deal with computational complexities of inference problems. The new-found relations between inference and SOC has opened new doors for application of approximate inference methods to SOC problems [16­24]. More recently, the body of empirical evidence showcasing the success of inference-based solutions for real-world control and robotics applications has grown considerably [25­30]. Motivated by the importance of practical solutions to SOC, and following the success of recent inference-based solutions to SOC, this thesis is an attempt to develop a new approximate and efficient, inference-based solution to SOC.

1.3

Literature

Bellman identified and acknowledged the computational complexity of DP, referred to as the curse of dimensionality. Although toy problems with small dimensions may be easily solved using the current power of digital computers, for practical problems with large or continuous states, the exact solution based on DP is highly infeasible. Some problems, such as the ones in energy, have dimensions as large as tens or even hundreds of thousands [10]. Therefore, for practical and problems of interest, DP is "hard" to solve and the necessity for approximate methods is the rule rather than the exception. The literature on approximate solutions to (S)OC and, more generally, to sequential decision making, is vast and convoluted. However, for our purposes, the literature can be broken down to four lines of work. The first line of work was that of Bellman and his community of operations researchers, who developed the theory of MDPs in their works [31­34]. Conventional MDP deals with sequential decision making problems when the spaces of states and actions are discrete and finite. An excellent review of the MDP theory can be in [35]. The second line of work was that of computer scientists who approached the problem of sequential decision making from an artificial intelligence perspective. Initial works were aimed at having computer programs learn to make optimal decisions, for instance, in games. This line of work is commonly referred to as reinforcement learning (RL). Although, RL initially started separately from other lines of work in approximate methods for decision making, later it turned out to have deep connections with some adaptive approximate DP 3

CHAPTER 1. INTRODUCTION

algorithms. Important references on RL include the excellent survey in [36] and the landmark textbook by Sutton and Barto [37]. Again, RL usually considers discrete and finite states and actions. The third line of work is done by control theorists, dealing with continuous state/control space problems. Important works by this community include results by Werbos [38, 39], Bertsekas [40] and Tsitsiklis [41]. These works also established the connections among the literature on MDPs, RL and their own approximate control theoretic solutions. Important texts on the topic, from a control theoretic perspective, include the celebrated volume by Bertsekas and Tsitsiklis [42] and the subsequent textbooks by Bersekas [43]. In this thesis, in all of the above three lines of work are collectively referred to as approximate dynamic programming (ADP). Some other good textbook on ADP include [10, 44] and the second volume Bersekas two volume series [45], which he updates periodically and has made available for free online access. The fourth and final line of works, which started only a few years ago, came mainly from computer scientists involved in the field of approximate inference methods. An important work is [12] which established a general duality between estimation and SOC. Subsequent works showed how SOC can be cast as an inference problem and solved using efficient inference algorithms [14, 16­22, 24]. Since inference is known to have deep relations with other fields such as statistical physics, thermodynamics, information theory, stochastic optimization and game theory [46­48], a consequence of relating SOC to inference is the implication that SOC is related to all these fields. A recent workshop, entitled "The Statistical Physics of Inference and Control Theory" [49], was aimed at discussing these links and connections. Such connections are not attractive merely from a theoretical point of view, but also from a practical one, as they enable us to use efficient algorithms developed separately in statistical physics and in inference to be applied to SOC. This thesis is mainly motivated by the latter of these four lines of work for the following reasons: · Computational efficiency: Several recent inference-based solutions show substantial [17, 20, 28] report inference-based solution which is faster than older methods such as differential DP or policy gradient methods. Also, the work in [16] describes an algorithms which outperforms the most successful RL algorithms, in terms of computation time. Some other works in this area, sometimes referred to as linearly solvable OC, show how a subset of nonlinear SOC problems can be solved as linear problems, and therefore, solved much more efficiently than other ADP methods [13, 50, 51]. · Extensibility to other problems: In robotics and other control problems, aside from the improvements over previous works, in terms of computational efficiency. For instance,

control problem, one often faces several other problems such as state estimation, highlevel reasoning and learning. These problems are often interwoven and interrelated, and can greatly benefit from interaction. Applying dissimilar and inconsistent frameworks 4

1.3. LITERATURE

to each of these problems can make such interaction very hard or impossible. Instead, if one could integrate all this problems into a larger problem and treat them within a single framework, the resulting synergy could be substantial. Indeed, since high-level reasoning, learning and state estimation can be readily approached using inference, solving OC within an inference-based framework may bring us closer to an integration of all the above problems. Some examples of integrating high-level reasoning and optimal control of low-level robotic motions through an inference-based framework can be found in [30]. · Agreement with motor control theories: Motor control theories try to explain the

mechanism governing the unparalleled motor abilities of biological systems. OC lays the foundations for some of the most notable theories in motor control. However, a recent paradigm shift in motor theories from OC to inference-based frameworks has received positive critiques from researchers in the field [52­55]. Accordingly, inference seems to be a better paradigm for explaining motor abilities of biological systems.

Also, important problems in robotics, such as motion planning and control may be approached by mimicking and emulating biological mechanisms. Therefore, approximate inference-based solutions to SOC may be useful in realizing successful robotics applications. Despite the above advantages, because of their recency, approximate inference-based solutions leave certain areas open for future work, including the following: · Limited applicability: A large number of results in inference-based control can only be applied to a sub-class of SOC problems. For instance, in [13, 50, 51], only SOC problems with a special form of cost function3 are addressed. Moreover, these works make certain assumptions regarding controllability of the system4 . These requirements are hardly ever met by practical problems or even cases as simple as LQ problems [16]. · Analytical intractability: There exist few works in the literature which tried to overcome al. [16] developed a general inference-based framework which applies to the largest class of SOC problems to-date. However, this method still lacks a tractable analytical form, because it doesn't lend itself to a simple closed-form solution even for LQ problems. A tacit evidence to support this deficiency is that the work presented in [16] lacks an implementation of their method for a benchmark control problem. · Robustness issues: It is well-known that OC, in general, may be subject to robustness issues, because it fully trusts the available model of the system. Therefore, although
i.e., SOC problems with a relative entropy term in the cost function. More specifically, they assume fully controllable dynamics, which is the assumption that the stochastic dynamics of the system may be brought into any desired form by control.
4 3

the limited applicability of the above inference-based solutions. For instance, Rawlik et

5

CHAPTER 1. INTRODUCTION

control maybe optimal for the available model, the optimality may be highly sensitive to the model of the system; a small misspecification of the model can result in highly sub-optimal solutions. There exists works that try to address robustness in OC, but they all result in computational complexity higher than the standard DP, and their additional dimensions exacerbate the curse of dimensionality. Currently, there is no inference-based robust optimal control methodology. In short, most results in inference-based SOC, suffer from either limited applicability or analytical tractability. Motivated by the these issues, our work builds upon [16] and the related literature [12, 17, 22, 56], and offers a solution which enjoys applicability to general SOC problems and analytical tractability, along with several other advantages to be discussed. Our work is also inspired by some other works in the area such as, [57­59], which relate OC to thermodynamics and rationality bounded game (RBG) theory [47], and the robust control theory of Hansen-Sargent [60]. As a final note, it should be stressed that the four lines of work discussed above are by no means disjoint and there are big overlaps and parallels among them. As a result, our approach has fundamental connections to many other approximate solutions, therefore, the intent has been to faithfully reflect these connections. However, success in this matter is naturally limited as a result of the large amount of works in this area.

1.4

Contributions

In this thesis, a novel approximate solution to SOC is proposed and its applicability to problems in robotics is verified by applying it to visually guided robotics. Therefore, this thesis offers theoretical significance as well as practical value.

1.4.1

An Efficient Approximate SOC Solution

The significance of our solution comes from the fact that not only it addresses the aforementioned open problems in inference-based solutions to SOC, but it goes beyond that and provides additional generality and capabilities. Specifically, our solution offers the following: · Generality: Unlike most other works in the area which have limited applicability, our solution can be applied to a large class of SOC problems. In fact, our solution constitutes the most general methodology in inference-based solutions, because among all inference-based methods, it can be applied to the largest class of SOC problems with continuous state/control spaces, arbitrary dynamics, and cost functions. Because of its generality, it can reproduce several methods as special cases [16, 56], including Rawlik's [16] recent work, which on its own is currently the most general inference-based solution in the literature. 6

1.4. CONTRIBUTIONS

· Analytical tractability: Unlike [16], our solution has a good analytical form which such as LQ problems. This nice analytical form allows for LQ approximations to nonlinear-non quadratic.

lends itself to closed-form solutions for a large class of problems in exponential families,

· Robustness: While other inference-based solutions do not address robustness, our solution can produce robust optimal control (ROC) as a special case. Our ROC solution model. · Flexibility: Our solution comes with an additional set of useful design parameters can handle unstructured uncertainties resulting in misspecification of the stochastic

which enables the designer to fine-tune the behavior of the control to vary from robust, to optimal, to risk-seeking control. Moreover, these behaviors can be varied throughout the time, allowing the designer to define, for instance, a controller which is initially robust and conservative and gradually shifts towards optimality or risk-seeking control. To the best of our knowledge, such added flexibility is novel in control theory.

In addition to deriving a general solution with the above advantages, a closed-form solution is derived for the practical case of linearized dynamics and quadratized cost functions. This results in a new iterative solution similar to differential DP methods such as iLQG [61], except with much added flexibility, for instance with additional support for robustness and asynchronous updates. Finally, important connections to several related areas, such as RBG theory, robust control, inference, statistical physics and thermodynamics are established. These connections open up new doors for future research and improvements.

1.4.2

Visual Servoing via SOC

Visual servoing (VS) is control of robotic systems using visual feedback, i.e., through cameras and other visual sensors. VS is a well-studied problem which has been around at least as early as late 70s [62]. Since then, VS has become a standard technique in robot control with many industrial applications. Our team in robotics, mechatronics and manufacturing automation lab. (RMAL) at Ryerson has a successful record in developing new theories and applications in VS [63­66] Our equipments and facilities enable us to apply and evaluate new control methodologies to problems in robotics including VS. SOC is especially well-suited for representing and solving VS, among other robotics problems. The reason is that VS is characterized by complicated MIMO nonlinear dynamics along with numerous sources of uncertainty, and SOC can naturally handle such nonlinear models in addition to explicitly accounting for different forms of uncertainty. Conventional approaches to VS make extensive use of linearized models of the dynamics of the system along with other simplifying assumptions regarding uncertainties. In particular, stochastic uncertainties of the problem, such as noise in visual sensors, imperfect observability 7

CHAPTER 1. INTRODUCTION

of depth of the scene and other unmodeled random processes are usually ignored in control level. Moreover, as for the choice of control methodology, classical approaches to VS use very simple proportional type (P-type) controllers, which suffer from myopic behaviors. OC approaches do not suffer from such myopic behaviors, because they look further than a single time-step. Although there are a few instances of successful application of more advanced control methods to VS, including OC-based methods, SOC is the most powerful methodology to have yet to be applied to VS. As another contribution of this thesis, VS is formulated as an SOC problem and our novel approximate method is utilized to derive a powerful solution for VS. In addition to the above, a simple one-step optimal controller, specifically for VS, with very low computational complexity. However, since it explicitly considers the uncertainty in the knowledge of the state of the system in control level, it will be shows, through both theory and practice, that it results in decreased noise sensitivity of VS, decreased sensitivity to imperfect knowledge of the depth and also increased accuracy of control. VS is subject to many sources of uncertainty and our work addresses some of them. The uncertainties one faces in VS includes: noise in the images received from the cameras, changes in the illumination of the environment, noisy and imperfect control of the robot, imperfect knowledge of the calibration parameters of the camera and kinematics parameters of the robot, and imperfect knowledge of the depth of the scene. Also, there are always unmodeled effects, such as the the vibration of camera, the backlash of robotic joints and hysteresis, among other things. Although all of the above can be potentially included in an SOC formulation, in this thesis only image noise, randomness in the control of the robot and uncertainty in the knowledge of the depth of the scene are considered. It will also be shows that our robust optimal control can provide robustness against unknown and unstructured errors the combined model of the camera and robot.

1.5

Notation and Other Conventions

A large part of this thesis is, to some extend, mathematically inclined and it helps to provide a brief discussion on the mathematical notation and other conventions used herein. (x, u, f (·)) represent vectors, matrices, a vector-valued functions. Italic symbols (x, u, f (·)) are used to represent scalar variables and functions. Bold symbols For representing probability and conditional distributions, a common notation found in

many probability theory textbooks is used in this thesis. Specifically, p (x), represents the probability of an underlying random variable, X , having the value, x. Or in mathematical terms, p (x) = Pr (X = x) . Similarly, p (y ) = Pr (Y = y ). Although the above may seem natural and commonplace, there is a peculiarity to it. To see this fact, it should be noted that normally a function is defined by its symbol and the number of its arguments. That is, 8

1.6. STRUCTURE OF THE THESIS

if f (x) = x2 , then f (y ) equals y 2 , which is why sometimes a function is represented without explicitly defining it's arguments, such as, f (·) = (·)2 . The peculiarity of the p notation in probability theory comes from the fact that the

function, p (·), changes according to its argument, e.g., p (y ), is not equal to p (x)|x=y , but rather, it is a different function, defined as p (y ) = Pr (Y = y ) . In other works, with this is defined, not only by the number of arguments, but also by the variables passed to it as arguments. With this clarification, despite the fact the p notation enjoys less mathematical rigor, since it greatly enhances readability and because of its popularity, it is adopted in this thesis. commonly used loose notation in probability theory, the function p is a special function which

1.6

Structure of the Thesis
putational complexity of DP, known as the curse of dimensionality.

· Chap. 2 reviews the theory of DP and important concepts therein and explains com· Chap. 3 Motivated by these computational challenges, details our new approximate solution to SOC and describes its relations and improvements to existing works. · Chap. 4 explains the application of our SOC solution to VS and derives special controllers, discusses improvements over basic VS methods and discusses experimental results. · Chap. 5 concludes the thesis with a discussion on the results and potential directions for future research.

9

Chapter

2

The Classics: Dynamic Programming
2.1 Introduction
"The 1950s were not good years for mathematical research ...Hence, I felt I had to do something to shield the Air Force from the fact that I was really doing mathematics ... What title, what name, could I choose? I was interested in planning, in decision making, in thinking. ... I decided therefore to use the word, `programming.' I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying--I thought, let's kill two birds with one stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense ... and it's impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It's impossible. It was something not even a Congressman could object to. So I used it as an umbrella for my activities." From the autobiography of Richard E. Bellman: Eye of the Hurricane.

This chapter is an attempt to present a brief overview of the most important concepts and ideas in SOC and the Bellman theory of DP, hopefully, without resorting to a duplication of classical theorems and results in the field. Our emphasis is to review the basic concepts and definitions in DP and to show the many sources of difficulty in obtaining the optimal solution. Since contributions to OC come from researchers from various fields, the terminologies and notations used in the literature are different. For instance, one can encounter control signal, action and decision in different contexts, while in fact, they all refer to the same concept. For consistency, the terminology and the language of control theory is used throughout this chapter, with a few exceptions when some terms are borrowed from other contexts. The chapter starts with a problem definition of SOC with full state observability and continuous state/control spaces. The Bellman principle is discussed and DP solution is derived. The computational challenge of applying DP, also known as the curse of dimensionality, is discussed in some depth. This chapter is concluded with a high-level discussion on the ideas behind some important approximate solutions to SOC, and how they compare to our approach. 10

2.2. FULLY OBSERVABLE SOC

2.2

Stochastic Optimal Control with Full Observability

Most textbooks on the subject of DP start their discussion with deterministic formulations, then extend it to stochastic settings. However, since our interests lie solely with stochastic problems, we delve right into the subject matter. For a review of deterministic OC see [43]. What follows is the basic definitions used throughout this chapter and the rest of the thesis. These definitions are consistent with the definition used in the literature concerning MDPs with continuous state and control spaces.

2.3

Definitions

Let us start by reviewing the basic definitions and terminology in OC. Plant: The system to be controlled. It is assumed that a (nominal) mathematical model of the plant is available. This mathematical model describes the evolution of the state of the plant. State vector: The state of the plant is described by a finite dimension real-valued vector, x. State vector encodes all the information about the system which is of importance to the control engineer. The space of all possible states of the system, or the state space, is denoted by X. Therefore, x  X  Rn , where n is the the dimension of the state space. Control signal: The plant is controlled using a control signal, u, which is a finite dimension real-valued vector. The space of all possible control signals is referred to as the control space, denoted by U. Again we have,u  U  Rm , where m, is the dimension of the the control space. Dynamics: The dynamics of the plant mathematically describes the evolution of the state of the plant given the control signal. In this thesis, the focus is mainly on state space
Plant
Observations

Controller
Control signal

Policy

{ }
Knowledge of Dynamics and Cost Function

Figure 2.1 ­ A general template depicting different types of optimal control problems and the relationship among the basic concepts and terms in our problems.

11

CHAPTER 2. THE CLASSICS

models in discrete-time and continuous state and control spaces. The reason is that these models appear frequently in most control problems and also because, ultimately, they are more amenable to a digital implementation. One of the ways to describe stochastic dynamics of a plant is by using transition function s. A transition function is the (conditional) probability that the plant transitions to state xk+1 from xk as a result of applying the control signal uk . In mathematical terms, dynamics is defined by the following time-dependent transition function, f (xk+1 | xk , uk , k ) = fk (xk+1 | xk , uk ) Pr (Xk+1 = xk+1 | Xk = xk , Uk = uk ) ,

(2.1)

where k denotes the discrete time index. It should be noted that the definition of dynamics through transition functions implicitly encodes all sources of randomness in the processes governing the dynamics of the plant.

Observations: It is assumed that some property of the plant is constantly observed. In general, these observations may be the exact state of the plant, exact measurements of a function of the state, or some noisy variant of these two, depending on the type of the problem at hand. However, in this chapter and Chap. 3, it is assumed that these observations constitute exact measurements of the state of the system. That is, at time step k , the complete information about the state, i.e.,xk , is available. This assumption is referred to as the full observability assumption.

Cost function: The desired behavior of the plant is described through the cost function. Cost functions may arise naturally in some problems, while they may also be designed by the control engineer, such that they encode the desired behavior of the plant. For instance, if the desired behavior of control is to minimize the energy consumption of the plant, one can define the cost function to be the energy consumption of the plant, or an approximation of it. In general, the designer can define a cost function with higher values around undesirable states and lower values around desired regions of the state space. Hence, minimizing the cost function is meant to drive the plant to desired regions of the state space, and thus, result in the desired behavior of the plant. The cost function, in mathematical terms, is a real-valued function of the current and future values of state and control signal. That is, the cost function is of the form, c(x0 , x1 , ..., xN , u0 , u1 , ..., uN ), (2.2)

c describes the control cost from time step 0 to time step N , and N is referred to as the control horizon. When N is infinite, one has an infinite horizon OC problem. For simplicity of notation, the state and control trajectories are summarized into x ¯ and u ¯ 12

2.4. BELLMAN PRINCIPLE AND DP

. Therefore, the control cost can be defined as, c (x ¯, u ¯) . (2.3)

An important class of cost functions are those which decompose into separate terms for each time step, i.e.,
N

c (x ¯, u ¯) =
k=0

ck (xk , uk ) .

(2.4)

Most monographs on the subject [43, 67] simply start with decomposable cost functions, while it should be noted that for general cost functions, DP does not apply. The reason is that in general case, the problem cannot be broken into substructures due to coupling between decisions. Indeed, the decomposition of the cost function gives the problem a Markov property which admits a separation of the cost to an instantaneous cost and a future cost. Without this separation, the DP solution is not possible to derive. This is why for the rest of this thesis, only decomposable cost function are considered. Control policy: The controller needs to decide which control signal to choose in order to minimize the cost, based on all information available to it. Mathematically, policy, , is a time-dependent mapping from the state space of the plant to the control space, : X × t  U. In other words, policy1 defines control signal as a function of time and the state of the plant, uk =  (xk , k ) k (xk ) , (2.5)

where,k , which is the policy at time step, k , is referred to as a decision. In this sense, for discrete-time problems, policy consists of a sequence of decisions. It should be immediately clear that control policy is, indeed, very closely related to the concept of control law in feedback control theory. The above definitions and the relations among them are depicted in Fig. 2.1 to visually describe a general SOC problem. Having the above definitions, the next step is to review Bellman principle of optimality and DP.

2.4

Bellman's Principle of Optimality and Dynamic Programming

The optimal controller is one that implements the optimal policy. Therefore, optimality should be defined more accurately. It was previously stated that the objective of OC is to minimize the cost function. However, because of the stochasticity of the dynamics of the
What is presented in this chapter is more accurately called a deterministic policy, because it specifies a single control signal for each observed state. In the next chapter, the concept of stochastic policy will be presented, where for each observed state, a random distribution of control signals is given by the policy.
1

13

CHAPTER 2. THE CLASSICS

system, one cannot predict the incurred cost of a policy. In other words, the cost incurred under policy,  ,
N

c =
k=0

ck (xk , k (xk )) ,

(2.6)

is a random variable. Therefore, one cannot directly minimize the incurred cost. In SOC, one chooses to minimize the expected value of the incurred cost2 . Therefore, the optimal policy is defined as the policy, under which, the expected cost is minimized. In mathematical terms,
N

 = arg min 
k=0

ck (xk , k (xk )) ,

(2.7)

where, · , is the expected value operator, and  is the optimal policy. Because of the decomposability of cost function and linearity of expected value operator, the above problem can be broken into smaller sub-problems. Bellman's principle of optimality, introduced in his seminal works [3, 4], describes the relationship between these sub-problems and the original problem.

2.4.1

Bellman's Principle

Theorem 1. "An optimal policy has the property that, whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.". [3] Proof. See [43]. In mathematical terms, the principle of optimality states that if  = (0 , 1 , ..., N ) constitutes the optimal policy for a problem starting at time 0 to N , the truncated policy i , i+1 , ..., N constitutes the optimal policy for the truncated problem starting at time i, until N . As a consequence of Bellman's principle of optimality, one can break the original problem of finding  into smaller truncated problems, knowing that the optimal policy for these truncated problems make up the optimal policy for the original problem. In fact, DP is nothing but the application of this principle. However, before going about describing DP completely, let us define the concept of value function (or optimal cost-to-go ). Value function: Value function is defined as the expected cost incurred under the optimal policy, i.e.,
The expected value of the cost is not the only plausible objective in the stochastic setting. Other possibilities include the risk-sensitive objective [68], which tries to minimize a linear combination of the expected cost and the variance of the cost. This objective is useful in problems where the variance of the cost are important as well, i.e. for cost sensitive problems. With the same logic, higher cumulants of the cost variable may enter the new definition of the cost function [69]. Ultimately, one can imagine a cost density shaping framework, with the objective of completely shaping the distribution of the cost [70].
2

14

2.4. BELLMAN PRINCIPLE AND DP

N

Ji (xi ) =
k=i

ck (xk , k (xk )) .

(2.8)

Value function is important because one can derive the optimal policy from value function by, i (xi ) = arg min ci (xi , ui ) + Ji+1 (xi+1 )
ui

,

(2.9)

where the expected value is given by, ^ Ji+1 (xi+1 ) =
xi+1

fi (xi+1 | xi , ui ) Ji+1 (xi+1 ) dxi+1 .

(2.10)

Therefore, finding the value function leads one to the optimal policy. In fact, DP finds the value function as an intermediate step, instead of directly seeking the optimal policy. The reason for this intermediate step is that the minimization in Eq. 2.7 is over decisions, which are functions. This means that the minimization problem in Eq. 2.7 is a variational problem. However, the minimizations described by Eq. 2.9 are over control signals, which are vectors. Therefore finding the value function as an intermediate step turns the original variational problem into a regular optimization problem which, in general, can be solved much easier. We will shortly see that DP finds the value function for truncated problems and applies Bellman's principle to find the optimal policy for the original problem.

2.4.2

Dynamic Programming

Bellman's principle suggests that one can start off with smaller sub-problems and reuse the solutions to make up the solution to the original problem. The smallest sub-problem for a finite horizon problem is the one with only the last time step. It's easy to see that the value function at the last time step is, JN (xN ) = min {cN (xN , uN ) + 0} .
uN

(2.11)

After finding the value function for the smallest sub-problem, one can move back one-step and find the value function at time N - 1, i.e., JN -1 (xN -1 ) = min {cN -1 (xN -1 , uN -1 ) + JN (xN ) } ,
uN -1

(2.12)

where, ^ JN (xN ) =
xN

fi (xN | xN -1 , uN -1 ) JN (xN ) dxN .

(2.13)

At this point, it's clear that one can continue solving for the value function at time step N - 2 and so on. Therefore, the following backward recursion solves the value function for 15

CHAPTER 2. THE CLASSICS

all steps, starting at the time horizon and sweeping backwards.

Ji (xi ) = min ci (xi , ui ) + Ji+1 (xk+1 )
ui

.

(2.14)

This backward recursion is nothing but the DP solution which was to be derived. The optimal policy is then given by, i (xi ) = arg min ci (xi , ui ) + Ji+1 (xi+1 )
ui

.

(2.15)

2.4.3

Complexity of Dynamic Programming

The DP algorithm discussed in the previous requires a minimization of cost function for each time step, over the space of control inputs, U, for xi  X. This minimization can result in several difficulties as follows: · Curse of modeling: The dynamics of the plant may have a highly nonlinear and complex form, which suggest that the DP may not be carried out in closed form and numerical minimization may be needed. The trade-off between the accuracy of modeling and the increased computational effort is sometimes referred to as the curse of modeling. · Curse of dimensionality: A numerical solution of each minimization in DP is prone dimensions of the state and the control signal. To further explain the curse of dimensionality, one can observe that in each iteration of the Bellman recursion, an expected value operation and a minimization operation needs to take place. In general case, these nested expectation and minimizations are largely considered to be intractable [71]. The expectation is an integration on the state space, X, and the minimization is over the control space,U. Both of these spaces grow exponentially w.r.t. their dimensions. It is important to note that for special cases, for instance, unconstrained linear quadratic (LQ) problems, the minimization and expectation can be performed analytically, and therefore the curse of dimensionality doesn't exist [6].

to curse of dimensionality, because it has an exponential complexity with respect the

2.4.4

Robustness

As mentioned in Chap. 1, SOC may suffer from robustness issues, because by assumption it fully trusts the nominal model of the dynamics of the plant. The optimal policy minimizes the expected cost for the nominal dynamics, but it does not say anything about the true dynamics of plant, which might be slightly different from the available model. Therefore, perturbations in parameters of the dynamics may render the optimal policy severely suboptimal or even infeasible [72]. As a result, all exact and approximate solutions to standard 16

2.5. RELATED WORKS AND OUR APPROACH

Plant Controller

xk

Optimal Policy

uk

 (x ) k k

{

 ,   , ...,   )   = ( 0 1 N

{

{ {
Knowledge of Dynamics and Cost Function

Figure 2.2 ­ A diagram depicting SOC problems with full observability.

SOC suffer from the same robustness problem, because their underlying assumptions are the same. In the next section, a high-level overview of some of the remedies for computational complexity and robustness problems are discussed.

2.5

Related Works and Our Approach

Previously, the curse of dimensionality was discussed and it was shown how it can limit the use of DP for many practical problems. To tackle the curse of dimensionality, a rather obvious idea is that instead of taking the expectation and minimization over the full spaces of state and control, one can reduce these operations to a small relevant region or subset of them [45]. Methods based on function approximations comprise a large part of the works in ADP. In these methods, either the value function or the policy are approximated by different function approximation methods such as neural-networks or linear functions, resulting in approximate value iteration [42] or approximate policy iteration [73] methods. However, almost all of these works are best applicable to discrete and finite state/control spaces, and are not easily applicable to continuous spaces. Among approaches which are applicable to continuous space SOC problems, one can name stochastic differential dynamic programming (SDDP) methods [74], such as the very successful iterative linear-quadratic-Gaussian (iLQG) approach [61]. iLQG is based on the idea of solving the DP for a small region of the state and control spaces, by assuming an initial state-control trajectory and solving the DP around this initial guess. Furthermore, iLQG uses linearized dynamics and quadratic approximations to the cost function around the initial trajectory, and therefore, it can solve DP analytically. Therefore, it achieves second order convergence rate to local optimal solutions. Because of its empirical success 17

CHAPTER 2. THE CLASSICS

for problems with large and continuous state and control spaces, iLQG has also been used as ground truth and for comparison in several works and publications [16, 28] Moreover, because of its computational efficiency and fast convergence, it's a useful benchmark for comparing the computational load of new algorithms. To address robustness, several works have developed what is known as robust dynamic programming (RDP) [75, 76], which is a robust counterpart of the DP algorithm seen above. However, RDP is even more computationally demanding than the standard DP. This fact limits its applicability to problems which can be quickly solved with standard DP. However, for our applications, DP is too hard, and consequently, RDP is also not applicable. To address computational efficiency and robust control, our work formulates a more general version of SOC (which includes robust control) as an inference problem. Casting SOC as an inference problem does not immediately solve the curse of dimensionality but it opens up possibilities of applying efficient approximate inference problems to SOC. This idea has already been explored for standard SOC with much empirical success for robotics applications[16, 28]. However, better analytical solutions are possible and, more importantly, robust control yet remains to be addressed. After formulating the problem as an inference problem, approximations used in extended Kalman smoothing are borrowed from the inference field and applied to SOC. Because extended Kalman smoothing also assumes linearized dynamics and quadratic log-likelihood functions, our solution turns out to be very similar to an improved version of iLQG. In fact, iLQG can be assumed as a special case of our approach. Furthermore, extended Kalman smoothing approximation is just one of the many approximations used in inference. Namely, the sort of approximations used in unscented Kalman filters or particle filters could potentially be used to solve the SOC, formulated as an inference problem.

18

Chapter

3

Inference for Optimal Control
3.1 Introduction

In Chaps. 1 and 2 several challenges in SOC were pointed out. Namely, computational complexity and the curse of dimensionality were discussed in the context of standard DP and the robustness issues associated with the standard SOC were discussed in some depth. In this chapter, the goal is to derive a solution to SOC which enjoys flexibility, generality, computational efficiency and robustness. It will be shown that ultimately a probabilistic variational (PV) formulation of SOC problems lends itself to an iterative and closed-form solution which enjoys all the above characteristics. Variational calculus serves as the hub connecting many approximate and exact methods for solving various problems in different contexts, such as statistical physics, game theory, information theory and inference. Therefore, adopting a variational approach enables us to better appreciate the connections of SOC to problems within different contexts and the existing approximate solutions therein. History has shown that revealing the connections between two fields is often mutually beneficial and can lead to major breakthroughs. For this reason, in this chapter the relationships between our approach to SOC and other fields will be made clear. Specifically, it will be shown that our method is related to some approximate inference methods such as belief propagation (BP) [77], and has close relationships with rationality bounded games (RBGs) of Wolpert [47], robust control framework of Hansen and Sargent [60], and inference-based solutions to control theory [16, 23, 27, 28]. These relationships may prove useful for future developments.

3.1.1

Roadmap and Contributions

In Sec. 3.2, the concept of stochastic policy is introduced by extending the concept of deterministic policy seen in Chap. 2, similar to [16]. The reason for introducing stochastic policies is that the problem of finding stochastic optimal policies may be solved in a continuous and unconstrained domain, and it will be shown that under certain assumptions, stochastic 19

CHAPTER 3. INFERENCE FOR CONTROL

policies may be found in closed-form. This wouldn't be possible, had we constrained ourselves to deterministic policies. In order to address robustness and also to increase generality and flexibility of our solution, in Sec. 3.3 three problem formulations including standard, robust and risk-seeking SOC are defined, in terms of variational problems seeking stochastic optimal policies. These problems have been considered by other works in the literature and addressed separately. In this chapter, although a common formulation with previous works is used, a novel solution to the problem is derived. Specifically, It will be shown that a single solution based on the proximal point approach [78, 79] for all three problems may be derived, and that the designer may smoothly switch the behavior of the controller from standard to robust, to risk-seeking SOC. Before deriving the proximal point solution, however, it is necessary to review some preliminary variational problems and see how they can be approached. This is done Sec. 3.4. These basic problems can be found throughout the literature as basic lemmas [16, 51, 58], but they are gathered here for completeness and for a quick review. Secs. 3.5 and 3.6 constitute most of the contribution of this chapter, where the basic lemmas introduced earlier are applied to the three variations of SOC problems, and a general closed-form iterative solution is derived. Next, some related works in the field are reviewed in Sec. 3.8 and their relationship to our work is studied in some detail. It will be shown that many of these works can be considered as special cases or variations of our approach. In Sec. 3.7 our general solution is approximated by making assumption similar to those made in the context of extended Kalman filtering, to give a practical solution for nonlinear problems. Finally, in Sec. 3.9 it is shown how inference for optimal control and inference for state estimation can be combined to yield an integrated estimation-control framework. The core contribution of this chapter is: · A proximal point formulation of several variations of SOC and the subsequent unified closed-form solution. Other contributions include: · Deriving a unified Gaussian message-passing solution for stochastic optimal/robust/riskseeking control. · Presenting an integrated estimation-control method. · Unifying numerous works in the area of inference-based control. Aside from the novelties above, a unified overview of several basic but important variational problems along with their solutions and relations is presented in this chapter. This overview is valuable because although these problems and their solutions constitute the cornerstone of many works in the field, a useful and self-sufficient overview of them doesn't exist. 20

3.2. STOCHASTIC POLICIES

3.2

Stochastic Policies

The policies encountered in Chap. 2 (and also in conventional literature concerning MDPs and SOC) can be thought of as special cases of a more general class of policies. Previously, policy was defined using functions which mapped each observed state to a control signal, i.e., uk =  k ( x k ) . (3.1)

Given an observed state, xk , the (deterministic) policy function, k (xk ), specifies a single control signal to be applied to the plant. However, here, the definition of policy is extended such that the policy can propose a distribution over the control signals, for each observed xk . Such a policy is known as a stochastic (or randomized) policy. In mathematical terms, a stochastic policy is defined by a (time-dependent) conditional probability function, µ (uk | xk , k ) µk (uk | xk ) Pr (Uk = uk | Xk = xk ) . (3.2)

With this definition, deterministic policies become the special case when these distribution collapse to delta Dirac functions, i.e., when they become concentrated on a single control signal. More specifically, a deterministic policy, µD , is the special case when,

µD k (uk | xk ) =  (uk - k (xk )) ,

(3.3)

where  (·) is the Dirac delta function. The set of all possible deterministic policies is denoted by D , and the set of all stochastic policies by S . Clearly, D  S . It is well-understood is superior to the optimal deterministic policy [80, 81]. Therefore, if the goal is to find the exact optimal policy, one could limit the search for the optimal policy to D . This is why it is common in MDP to narrow down the search to the class of deterministic policies [43]. However, when the ultimate goal is to find good sub-optimal policies, there is no reason to rule out stochastic policies, because a sub-optimal stochastic policy may still do better than a sub-optimal deterministic policy. Fig. 3.1 compares a sub-optimal stochastic policy and a sub-optimal deterministic policy against the optimal determinist policy and illustrates how a stochastic policy may be closer to the optimal policy than a deterministic one. Furthermore, since D  S , increasing the scope of problem from D to S causes no loss of generality. In fact, if a policy is found to be superior to all other policies in S , it is also superior to all policies in D , and therefore, it is bound to be either the deterministic optimal policy or an equally good stochastic policy. This fact has been recognized in [16], while stochastic policies have also been used in the literature for other reasons [81]. The last remaining question is that, given a good stochastic policy, how one can execute 21 that in a decision making problem with a deterministic cost function, no stochastic policy

CHAPTER 3. INFERENCE FOR CONTROL

p( u k )

µ ~ k (u k | x k )

µD k

=  (uk -

 k (xk ))

µ ~k

D

uk =  (uk -  ~k (xk ))

Figure 3.1 ­ Comparison of sub-optimal stochastic and deterministic policies against the optimal deterministic policy. µ ~k D and µ ~k are sub-optimal deterministic and stochastic policies, D respectively, and µk is the optimal policies.

it. Instead of proposing a single control signal to be applied to the plant, a stochastic policy proposes a distribution over control signals, and a distribution can't be directly applied to the plant. Therefore, an extra sampling step is needed before one can execute a stochastic policy. As for the sampling, one can either generate a random sample from the distribution µk (uk | xk ) or deterministically choose this sample to be, for instance, the mean, the mode, or the median of µk (uk | xk ). With this discussion, from this point on, the goal is to find an efficient solution for

sub-optimal stochastic policies. It will be shown that under certain conditions, sub-optimal stochastic policies can be derived analytically and in closed-form. But before that, it is useful to take a look at the expected value of the cost under a stochastic policy. It's easy to see that the expected cost is given by, ^ ¯) = c (x ¯, u
¯ x ¯ ,u N

p0 (x0 )
k=0

¯ ) dx ¯, µk (uk | xk ) fk (xk+1 | xk , uk ) c (x ¯, u ¯du

(3.4)

where, p0 (x0 ) is the distribution of the initial state, i.e., p0 (x0 ) = Pr (X0 = x0 ) . (3.5)

One may notice that the expected cost is, in fact, a weighted integral of the cost function, where the weighted by the probability distribution of state and control trajectories. This probability distribution is generated by applying the policy, µ, to the dynamics, f , starting 22

3.3. ROBUST, OPTIMISTIC & STD. SOC

x q -distribution

u

time

Figure 3.2 ­ An illustration of a q -distribution for a one dimensional state and control system.

from initial state distribution, p0 (x0 ). We refer to this distribution as q -distribution, formally defined by,
N

¯ ) = p 0 ( x0 ) qµ,f,p0 (x ¯, u
k=0

µk (uk | xk ) fk (xk+1 | xk , uk ) .

(3.6)

Therefore, the expected cost, in terms of q -distribution is given by, ^ ¯) = c (x ¯, u
¯ x ¯ ,u

¯ ) c (x ¯ ) dx ¯. qµ,f,p0 (x ¯, u ¯, u ¯du

(3.7)

A q -distribution can be thought of as a probabilistic tube, comprised of all possible state-control trajectories generated by applying a certain policy to a given plant dynamics, at a given initial state distribution. Fig. 3.2 illustrates a distribution over state and control trajectories for a single dimension problem. Having defined stochastic policies, since our goal is to address more general problems than the standard SOC, the next step is formulation of these problems.

3.3

Robust, Optimistic and Standard SOC Problems

As promised in Chaps. 1 and 2, the robustness issues of standard SOC are also to be addressed in our approach. To add even more flexibility and generality, our approach also handles a related problem, known as the optimistic (or risk-seeking) control [82, 83] to be defined shortly. Therefore, in this section, three variations of SOC problems are defined: Robust SOC, standard SOC and optimistic SOC. Although these problems are formulated separately in this section, later on in Sec. 3.6, it will be shown that a single solution can address all three. 23

CHAPTER 3. INFERENCE FOR CONTROL

3.3.1

Addressing Robustness: Pessimistic Control

As stated in Chaps. 1 and 2, one of the natural shortcomings of SOC is that, by assumption, it fully trusts the available model of the plant. Therefore, the optimal policy is specifically optimal for this nominal model. However, this optimality may be very sensitive to the model; if the true model of the system is slightly different from the available one, standard SOC may result in severe sub-optimality [72, 76, 84, 85]. There are several works in the literature which try to address the robustness issue. As mentioned in the previous chapter, a straightforward treatment of robustness results in RDP, which is similar to DP, except with additional dimensions and computational complexity compared to standard DP [75, 76]. Therefore, RDP is only applicable when the standard DP is tractable. Since for problems of our interest standard DP is intractable, existing works in RDP become useless. In this chapter, robustness is addressed similar to the Hansen and Sargent's approach, described in their seminal text in economics[60] . This approach tries to robustify the control against misspecification of the stochastic model, by not fully trusting a given nominal model. Instead, this approach assumes a set of possible stochastic models for the plant, called the ambiguity set, and finds the optimal policy for the worst-case model within this set. It should be noted that, although the high-level definition and formulation of robustness is the same as Hansen's, our solution applies to general sequential problems with nonlinear and non-quadratic costs, while Hansen and Sargent provide the solution for special problems in economics or for a restricted class of problems with certain assumptions about the dynamics and uncertainties To formally define the ambiguity set, suppose a nominal model of the plant is given by, f (xk+1 | xk , uk , k ) , (3.8)

which we refer to as the nominal dynamics of the plant. The ambiguity set is a set, A, which contains the nominal dynamics f  A , along with other possible dynamics. It is assumed that the (unknown) true dynamics is also contained within this set. The robust policy, µ r , is the policy which minimizes the expected cost for the worst-case dynamics within the ambiguity set. That is, µ
r

¯) . = arg min max c (x ¯, u
µs f A

Here, our focus is on ambiguity sets of a special form. Specifically, Kullback­Leibler (KL) bounds are used to define a bounded set of dynamics1 . KL-divergence (or relative entropy) is formally defined in Appx. A.1. However, for our purposes in this chapter, KL-divergence can be simply considered as a measure of distance between two probability distributions. For the two distributions, p (x) and q (x), KL (p (x)
1

q (x)) is a non-negative real number, with

For other possible structures of ambiguity set see [85].

24

3.3. ROBUST, OPTIMISTIC & STD. SOC

zero occurring only when p (x) = q (x). Also, roughly speaking, the more different p (x) looks from q (x), the higher their KL-divergence. If the nominal dynamics is f , one can define a special form of ambiguity set given by,

A = f (xk+1 | xk , uk , k ) | KL f (xk+1 | xk , uk , k )

f (xk+1 | xk , uk , k )  fk .

(3.9)

where fk is a positive real constant. This set contains all dynamics which are not farther than fk from the nominal dynamics. This set obviously includes the nominal dynamics, f , because KL (f f ) = 0 < fk . Moreover, it's assumed that fk is chosen big enough so that the unknown true dynamics of the plant also falls within this set. The set defined by Eq. 3.9 is sometimes referred to as a KL-ball, with center, f , and radius, fk . For KL-ball ambiguity sets, the SOC problem, robustified against misspecification of the dynamics becomes:

µ

r

¯) , = arg min max c (x ¯, u
µs f

(3.10) (3.11)

subject to

KL f

f   fk .

The problem above is reminiscent of other minimax problems in robust control, with the exception that it uses a stochastic definition for dynamics, instead of a deterministic one. The solution to the problem above is robust to misspecification of the dynamics of the plant. Similarly, SOC can also be robustified against the misspecification of the initial distribution of the state, p0 (x0 ). In fact, in standard SOC, one assumes that the given initial state distribution is to be trusted completely. Therefore, using similar KL-ball ambiguity set assumptions on the initial state distribution, one has the robust SOC problem: Problem 1. Find the robust policy, µ r , which minimizes the expected cost for the worstcase dynamics and initial state distribution within the ambiguity set. µ
r

¯) , = arg min max c (x ¯, u
µs f ,p0

(3.12) (3.13) (3.14)

subject to and

KL f KL p0

f   fk , p0  p0 .

The solution to the problem above, along with two other upcoming problems, will be developed in Sec. 3.6.

3.3.2

Optimism: Risk-seeking Control

The pessimistic problem defined previously makes a worst-case assumption about the true dynamics and the initial distribution of the system, which are assumed to be contained 25

CHAPTER 3. INFERENCE FOR CONTROL

within KL-balls centered on the nominal dynamics and the given initial distribution. If instead of a worst-case assumption, one makes a best-case assumption, the optimistic control (or risk-seeking control) problem is defined. In other words, if the max operation in Prob. 1 is replaced with a min operation, one has the optimistic control problem as follows: Problem 2. Find the optimistic policy, µ o , which minimizes the expected cost for the best-case dynamics within the ambiguity set. That is, µ
r

¯) , = arg min min c (x ¯, u
µs f ,p0

(3.15) (3.16) (3.17)

subject to and

KL f KL p0

f   fk , p  p 0 .

The optimism in these problems are justified by an "optimism in face of uncertainty" heuristic, which has been shown to be a useful heuristic for some problems [82, 83]. Specifically, for partially observable problems, a risk-seeking behavior compared to a greedy and optimal behavior may result in more informative observations. Moreover, it will be shown that riskseeking control constitutes one extreme of our approach while the other extreme is robust control. The neutral case in the middle is the standard SOC. Therefore, although for our purposes, the standard and robust problems are more interesting, for the sake of unification the risk-seeking case is also considered in this chapter. Moreover, it will be shown that it can be easily handled with the same solution that applies to robust and standard SOC.

3.3.3

Standard SOC

If the ambiguity set is reduced to the nominal dynamics and initial state distribution, either of the two problems discussed previously in this section reduce to the standard SOC problem. In other words, for ambiguity sets defined as KL-balls, when the radii, fk and p0 approach zero, the KL-balls shrink down and in the limit only contain the nominal dynamics and initial state distribution. Therefore, the optimistic problem can be reduced to standard SOC as follows: Problem 3. Find the optimal policy, µ , which minimizes the expected cost for the nominal dynamic. That is, µ subject to and  fk p 0 = ¯) , arg min ext c (x ¯, u
µs f ,p0

(3.18) (3.19) (3.20) (3.21) (3.22)

KL f KL p0  0,  0. 26

f   fk , p  p 0 ,

3.4. OPTIMAL DISTRIBUTIONS

This means that if one has the solution to either the optimistic or pessimistic optimal problems, the standard SOC solution can be recovered at the limit of fk , p0  0. The three problems defined in this section are all variational problems, because they

are optimization problems which seek optimal (conditional) distribution functions. In the robust problem, one has maximization and minimization sub-problems while the optimistic case is composed only of minimization sub-problems. All of this sub-problems are essentially problems of finding a distribution under which the expected value of a cost function is minimized (or maximized). Therefore, to be able to solve these three variations of SOC, one needs to know how to approach problems of optimal distributions.

3.4

Optimal Distributions

Previously, three related SOC problems were formulated, without discussing their solutions. The reason is that all of these variational problems are that of finding constrained or unconstrained optimal distributions (OD). Therefore, before approaching Probs. 1­3, one needs to know how to approach basic constrained and unconstrained OD problems. A discussion on OD problems constitutes the topic of this section.

3.4.1

Unconstrained Optimal Distribution Problems

Let us define a simple unconstrained OD problem. Problem 4. Find the distribution Pr (X = x) = p(x) such that the expected value of the cost function, c (x) under p(x) is minimized.

p (x) = arg min c (x)
p(x)

p(x)

(3.23) (3.24)

^ c (x) p (x) dx.
x

= arg min
p(x)

These problems and their solutions are frequent topics in stochastic optimization methodologies, such as probability collectives [86, 87]. A Proximal Point Method and KL-Penalized Problems The proximal point (PP) method was first developed by Rockafellar in [79] with some complete reviews available in [78, 88, 89]. However, here the discussion is limited to relevant high-level concepts. The basic idea in PP algorithms is that instead of solving the minimization in one step, it might be easier to gradually revise and refine an initial solution, by finding an improved solution in the "proximity" of the one before. Thus, the general form of proximal algorithms is, 27

CHAPTER 3. INFERENCE FOR CONTROL

f (i+1) = arg min cost (f ) + i D f
f

f (i)

,

(3.25)

where f is the argument of optimization and D (·

like measure of the proximity of the solutions. The proximity functional penalizes the

·), the proximity functional, is a distance-

deviation of the solution from the previous solution, and therefore, keeps the solution in the proximity of the one before. For the choice of proximity function, originally, quadratic distances have been used [79]. However, very general types of distance-like functions have since been introduced, including entropy-like distance measures [78, 89], which are particularly useful in variational inference problems. What follows is the PP problem, which uses a relative entropy (or KL-divergence) term as the proximity function. Problem 5. Iteratively solve,

p(i+1) = arg min
p

c (x)

p(x)

+ i KL p

p(i)

(3.26)

Each iteration of the above is a variational problem penalized by KL-divergence (or relative entropy) penalty function. We refer to this problem as a KL-penalized variational problem. The reason that the above PP problem is preferred to the original problem is that the original problem is linear in p (x) while the KL-penalized problem is strictly convex 2 , and therefore, finding the solution to the PP problem is easier. In fact, the solution to an iteration of the PP problem above can be found in closed-form and is given by the following lemma. Lemma 1. The solution to the KL-penalized problem,

p

(,p0 )

= min
p

c (x)

p(x)

+  KL (p

p0 ) ,

(3.27)

is the Boltzmann distribution (with degeneracy, p0 ), p with the partition function, ^ Z ( ) =
x (,p0 )

=

c(x) 1 - p 0 ( x) e  , Z ( )

(3.28)

p0 (x) e

-

c(x) 

dx.

(3.29)

Proof. See [90]
2

It is strictly convex in

p(x)/p(i) (x).

28

3.4. OPTIMAL DISTRIBUTIONS

Lemma 1 is very useful because KL-penalized problems appear frequently in our treatment, as well as in most works dealing with approximate inference-based solutions of SOC. In Sec. 3.8, a review on related works which use this lemma will be given. Prob. 5 is an unconstrained problem with a relative entropy penalty term in the cost function. Because of our assumption on the form of the ambiguity set in Sec. 3.3, constrained OD problems with KL constraints also appear in our work.

3.4.2

Constrained Optimal Distribution Problems

So far, it was shown that (unconstrained) OD problems can be solved iteratively using a PP method, and that each iteration of the PP method is a KL-penalized problem, which in turn, admits a closed-from solution given by Lemma 1. Here, the solution to constrained OD problems is reviewed. In particular, the focus is on constrained OD problems, where the constraints are KL-bounds. We refer to the problems as KL-constrained problems. KLconstrained problems show up in our robust and optimistic SOC problems defined in Sec. 3.3, because the ambiguity sets were defined by KL-bounds. It will be shown that the solution to KL-constrained problems turn out to be closely related to the solution to KL-penalized problems seen earlier. The KL-constrained problem is formally defined as follows: Problem 6. Find the solution to the KL-constrained problem below.

p(,p0 ) (x) = arg min c (x)
p(x)

p(x) ,

(3.30) (3.31)

subject to

KL (p (x)

p0 (x))  .

Prob. 6 is essentially a variational problem on a KL-ball. The KL constraint can be enforced using a Lagrange multiplier method. For an in-depth introduction to the Lagrange multiplier method see [91]. The Lagrangian reads, L (p (x) ,  ) = c (x) +  (KL (p (x) p0 (x)) - ) , (3.32)

p(x)

with   0 as the Lagrange multiplier. Therefore, problem 6 is equivalent to: min min L (p (x) ,  ) .
p(x) 

(3.33)

The Lagrangian dual is formed by changing the order of minimization. Thus, the dual problem is, min min L (p (x) ,  ) .
 p(x)

(3.34)

Since Prob. 6 is strictly convex3 , strong duality holds and the solutions to the primal
3

It is convex in

p(x)/p (x). 0

29

CHAPTER 3. INFERENCE FOR CONTROL

and dual problems are equivalent . Therefore, one can continue with the dual problem. The inner minimization in the dual is,

L p

(,p0 )

,

=

min
p(x)

c ( x)

p(x)

+  KL (p (x)

p0 (x)) -  ,

(3.35) (3.36)

for   0

where the last term in the r.h.s is constant w.r.t. p (x) and, therefore, can be dropped from the inner minimization problem. As for the Lagrangian multiplier, , the case where  = 0 happens only when the solution is inside the KL-ball and  > 0 when it's on the boundary, KL (p (x) p0 (x)) = . Here, the case when  = 0 is dismissed. The reason is that unless p0 (x) is the optimal Dirac distribution itself, the solution to the problem above always lies on the boundary. Therefore, for all non-trivial problems, it is always the case that  > 0 . By taking  > 0 , it becomes evident that the inner minimization problem is a KL-penalized problem and, therefore, Lemma 1 applies. Hence, p
(,p0 )

=

c(x) 1 - p 0 ( x) e  . Z ( )

(3.37)

Plugging the solution above back into the dual problem in Eq. 3.34 yields,  () = arg min [- log Z ( ) - ] .
>0

(3.38)

For our purposes, the solution of Eq. 3.38 is not as important as the general relationship between  and  (), and specially the relationship at the limits of zero and infinity. This relationship is interesting, because it states that a KL-constrained problem with radius  is equivalent to a KL-penalized problem with a penalty coefficient of  () and vice versa. Ultimately, this relationship establishes a two-way relationship between KL-penalized and KL-constrained problems. This relationship has also been studied in the context of economics, in [92, 93].

3.4.3

The Relationship Between KL-Constrained and KL-Penalized Problems

To investigate the general properties of the relationship between KL-constrained and KLpenalized problems, one can observe from Eq. 3.38 that  / is always negative (since  > 0). Therefore, if the solution,  , exists, it is decreasing in . This suggests an interesting (twoway) relationship for Probs. 5 and 6: The solution to problem 5 with  () is a solution to problem 6 with , and vice versa. This means that adding a relative entropy term to the objective function has the same effect as restricting the problem to a KL-ball. Thus, an (unconstrained) variational problem with a relative entropy term in the objective is equivalent to a variational problem without the relative entropy term, but restricted to a 30

3.4. OPTIMAL DISTRIBUTIONS



1

1



Figure 3.3 ­ The relationship between  and  , which states that Prob. 6 with 1 is equivalent to Prob. 5 with 1 .

KL-ball with an appropriate radius. In mathematical terms, p(,p0 ) (x) = p
( (),p0 )

,

(3.39)

where  and  are related by Eq. 3.38 4 . The radius of the KL-ball and the scaling coefficient of the relative entropy (or the temperature 5 ) are related by Eq. 3.38, and because of the monotonicity of the mapping, it is the case that the lower the radius ,, the higher the corresponding temperature,  , and vice versa. In particular, for   0 one has   +, and for   + one has   0. To give the reader an image, the conceptual relationship between  and  appears in Fig. 3.3, keeping in mind that the exact relationship is a complicated functional of c (x) and p0 (x). These results were anticipated because the solution of Prob. 6 with the KL-ball constraint should approach the optimal unconstrained solution when the radius of the KL-ball is increased infinitely. Also, when the radius of the KL-ball approaches 0, the set of distributions satisfying the constrained shrinks to {p0 (x)}, which means that the solution is bound to approach p0 (x). For the KL-penalty term in Prob. 5, when the penalty coefficient,  , approaches zero, the unconstrained optimal solution is recovered and when penalty coefficient increases, the KL-penalty dominates and holds the solution closer to p0 (x). So far, the unconstrained and constrained minimization problems were studied. In our treatments, we also face constrained maximization problems and so, a review on these problems will be given shortly.
One could arrive at the same statement much quicker by simply pointing out that a KL-penalized problem is the Lagrangian relaxation of a KL-constrained problem. However, our discussion explicitly relates KL bound,  , and the corresponding KL-penalty coefficient,  . 5 The coefficient scaling relative entropy or entropy terms are historically referred to as temperature, because they first appeared in statistical physical problem.
4

31

CHAPTER 3. INFERENCE FOR CONTROL

3.4.4

The KL-Constrained Maximization Problem

The robust SOC defined in Sec. 3.3 is a minimax problem, where the max part requires finding a maximizing distribution on a KL-ball ambiguity set. Therefore, it is useful to review the problem of maximizing the expected cost on a KL-ball. It is easy to see that in order to find the maximizing solution one can simply negate the cost function and, once again, perform a minimization. That is, arg max c (x)
p(x)

p(x)

= arg min -c (x) .
p(x)

(3.40)

Repeating the same steps as before, with the Lagrange multiplier,  0, one has, p (x)
( ,p0 )

= ^

c(x) 1 + p 0 ( x) e  , Z ( )

(3.41)

Z  and,

=
x

p0 (x) e

+

c(x) 

dx,

(3.42)

 = arg min - log Z  -   .
 0

(3.43)

Comparing the maximization problem with the minimization counterpart, one can easily realize that switching the sign of the Lagrange multiplier in one of the problems results in the other; the reader can simply replace  = - in the above to arrive at the same expressions as c(x) ´ - in Eq. 3.38. It should be noted that, while the partition function, Z ( ) = x p0 (x) e  dx, for the minimization problem was always well-defined (assuming c (x)  0, c (x) = cte. and  > 0), the partition function in Eq. 3.42 for the maximization problem may not always be finite. Therefore, for the Eq. 3.42, the range of  where the integral is finite may be bounded. It's helpful to briefly find the admissible range of  a simple example: Example 1. Let's consider a scalar problem with a quadratic cost c (x) = ax2 and a nominal Gaussian distribution p0 (x) = N 0,  2 . For this problem, the integrand in Eq. 3.42 is proportional to, e for a 2
2 ax2 - x2  

, which suggests that the partition function is well-defined

<  (or, equivalently,  < -a 2 ).

This section is concluded with a remark summarizing the results for constrained OD problems: Remark 1. The constrained maximization and minimization problems may be combined into a single extremization problem, where the solution given by Lemma 1 is a maximizing solution when  > 0 and it's a minimizing solution when - <  < limit . In other words, the solution to,

32

3.4. OPTIMAL DISTRIBUTIONS



Possible bound so that the partition function be well-defined.



Maximization of the Expected Cost

Minimization of the Expected Cost

Figure 3.4 ­ The relationship between  and  , including negative values for  .

arg ext p(x) c (x) subject to KL (p (x)

p(x) ,

(3.44) (3.45)

p0 (x))  ,

given by Eq. 3.28 is equivalent to the solution of the KL-penalized problem, arg min when  is positive, and to, arg max c ( x) +  () KL (p p0 ) , (3.47) c (x) +  () KL (p p0 ) , (3.46)

p(x)

p(x)

when  is negative. Fig. 3.4 depicts the relationship between KL-constrained minimization/maximization problems and KL-penalized problems.

3.4.5

Relationship to Inference

In this section some important optimal distribution problems were reviewed. However, their relation to inference was not discussed. Indeed, inference problems such as, finding the posterior distribution given a prior distribution and some evidence, can be formulated as variational problems. For instance, the posterior can be found as the optimal distribution which minimizes a free energy functional. Such variational formulations of Bayesian inference are known as variational Bayes methods [94, 95]. Space doesn't permit a complete presentation of variational inference, nor is it really necessary. Instead, it's easier to simply point out that the solution to the KL-penalized and KL-constrained problems above, given by Boltzmann distribution in Eq. 3.28, can be interpreted as Bayesian posterior calculation. 33

CHAPTER 3. INFERENCE FOR CONTROL

Specifically, it's easy to see that: Remark 2. In Eq. 3.28, if p0 (x) is a prior distribution and - -1 c (x) is the log-likelihood of the evidence given x, then p posterior =


(x) is the posterior distribution.

The above can be simply verified by remembering that the Bayes law states that,
prior×likelihood/normalization constant.

This observation ultimately enables us to

solve OD problems using any approximate method for posterior calculation. Indeed, the solution to the KL-penalized problems is equal to the solution of the following Bayesian posterior problem: Problem 7. A prior distribution is given by p0 (x). R is binary random variable and it is observed as R = 1. Also, it is known that, Pr (R = 1 | X = x) = e
-
c(x) 

.

(3.48)

given the information that R = 1, the posterior distribution is simply given by,
c(x) 1 - p 0 ( x) e  . Z ( )

(3.49)

Because of the equivalence of the above inference problem and the KL-penalized problems seen above, whenever a cost function c (x) is to be minimized under the unknown distribution, one can define an auxiliary random variable and assume it is observed to be in a state with a log-likelihood proportional to - -1 c (x), and solve the above posterior problem. This relationship between cost (or utility) and log-likelihood is well-known and studied in detail in [57, 58]. Similar to [16, 28] an auxiliary binary random variable, R, is defined such that, Pr (R = 1 | X = x) = e
-
c(x) 

.

(3.50)

The equation above is, indeed, a Boltzmann distribution for a system with energy function, E (x) = c (x), at temperature  . With these definitions, solving the KL-penalized minimization problem (Prob. 5) with the penalty coefficient  , (or KL-ball constrained Prob. 6 with the radius  ( )) is equivalent to a standard posterior Bayesian having observed R = 1, when Pr (R = 1 | X = x) = e
-
c(x) 

. This relation is depicted in Fig. 3.5. It is clear from
 =

the figure that the expected cost under the posterior is lower than the expected cost under the prior. The KL-penalty coefficient,  , controls how strength of the evidence; when   0 the evidence becomes sharp at the minimum of the cost function and, as a result, posterior will also be strongly shifted towards the region where the cost function is minimum. Thus, the solution to the KL-penalized and KL-constrained problems above can be thought of as special forms of Bayesian posterior inference, where the log-likelihood function is scaled by a parameter / (or the temperature is scaled by  ). It will be shown that the fact that the solutions to KL problems are equal to simple posterior calculation allows us to apply approximate inference methods to solve SOC. 34

3.5. A PROXIMAL POINT METHOD FOR SOC

c ( x)

Cost function

Evidence
Pr (R = 1 | X = x) = e p( x )

-

c( x ) 

x

- 1 Z ( ) p 0 ( x ) e

Posterior

c(x) 

prior

p0 ( x )

x

Figure 3.5 ­ Inference interpretation of Eq. 3.28. A KL-penalized or a KL-constrained problem can be interpreted as standard posterior calculation.

3.5

A Proximal Point Method for SOC

In this section, as a first step, the three variations of SOC defined in Sec. 3.3 are combined into a unified problem. This problem is then transformed into a KL-penalized problem and solved using a PP method. To begin, let us present the unified SOC problem: Problem 8. Find the optimal policy,

µ subject to and

¯) , = arg min ext c (x ¯, u
µs f

KL f KL p0

f   fk , p0  p0 , (3.51)

where the reader can easily verify that, depending on whether the extremum is a minimum or maximum, the optimistic and pessimistic variants are resulted respectively, and when fk  0, the standard SOC is recovered. The problem above is a KL-constrained problem. According to the results n Sec. 3.4.3 which were summarized by Remark 1, the KL-constrained problems above can be converted to the following equivalent KL-penalized problem: Problem 9. Find the optimal policy, 35

CHAPTER 3. INFERENCE FOR CONTROL

µ

¯) ¯, u = arg min ext { c (x
N µs f

+
k=0

k KL f 0 KL p0 p0

f , (3.52)

+

where each pair of (k , fk ) and (0 , p0 ) are related according to the duality results described in Sec. 3.4.3 and depicted in Fig. 3.4. The problem above is now an unconstrained OD problem. In Sec. 3.4 it was shown that such unconstrained problems can be solved using a PP method. Therefore, by adding a proximity function to the above problem, the PP problem to be solved iteratively can be formulated as follow: Problem 10. Iteratively solve for policies, µ(i+1) = arg min ext
N µs

f, p0

¯) + c (x ¯, u f + 0 KL p0 . p0 (3.53)

+
k=0

k KL f

+  (i) D µ

µ(i)

The proximity function of our choice is a weighted sum of (conditional) KL divergences, where each term penalizes the divergence of µk
N (i+1)

from µk , i.e., µk
(i) (i)

( i)

i D µ

µ

( i)

=
k=0 N

i k KL µk k KL µk
k=0

=

µk

.

(3.54)

In the above,  (i) k was renamed to k . Therefore, the PP iteration consists in solving, µ(i+1) = arg min ext
N µs f, p0

¯) + c (x ¯, u f + 0 KL p0 µk
(i)

+
k=0 N

k KL f k KL µk
k=0

p0 (3.55)

+

.

The solution to the PP problem above is given in the next section. 36

3.6. THE SOLUTION

 (x ))  ( u k - k k (4)

µk µk µk µk µk
(0) (1) (2) (3)

Figure 3.6 ­ Iterations of proximal policy solutions for time slice k . In each update of the policy at time k , a better policy on a KL-ball of the one before is found.

3.6

The Solution to the Proximal Point Problem

There exists still at least two ways that Prob. 10 can be solved, which are discussed in this section. First, it will be shown that Prob. 10 can be broken into sub-problems which are individually of the same form as the basic problem Sec. 3.4 and, therefore, admit to a closed-form solution given by Lemma 1. The result is a DP algorithm which updates the policy in a backward sweep. We refer to this backward recursion as synchronous updates of the policy. This backward recursion is similar to the original DP algorithm in Chap. 2, expect the exact minimizations are replaced by approximate soft-mins. Secondly, it will be shown that although the problem is jointly non-convex in, p0 , fk , µk | k = 0 : N , it is separately convex in each of the variables. That is, if one fixes all variables except one, the resulting problem is convex in this remaining variable. Therefore, a solution based on alternating convex optimization is possible. This roughly means that one can optimize each variable separately keeping the others fixed and alternate the fixed variables until convergence. (See [96] for more details on alternating convex optimization). This approach allows one to have asynchronous updates, i.e., as opposed to the backward sweep which requires a sequence of updates backward in time, one can update the policy at any time slice, µk , and in any order. In both approaches, each update of the policy, results in an improved policy which is closer to the deterministic optimal policy. Fig. 3.6 depicts how the updates of the policy at time slice k improve towards the optimal policy. 37

CHAPTER 3. INFERENCE FOR CONTROL

3.6.1

Backward, Synchronous Updates

Let us expand the problem in each PP iteration in Prob. 10 in the following way, where the dependencies of all functions on xk and uk are suppressed for brevity. ^
{µk |k=0:N } {p ,f } 0 k

N

min

ext
N

¯ ,u ¯ x

p0
k =0

µk fk p0 ¯ dx ¯du p0 log µk µk
( i)

¯ ) dx ¯ c (x ¯, u ¯du

^

+ 0
¯ ,u ¯ x N

p0 ^
k =0

µk fk
N

log

+
k=0 N

k
¯ ,u ¯ x

p0
k =0 N

µk fk µk fk
k =0

¯ dx ¯du

^ k
¯ ,u ¯ x

+
k=0

p0

log

fk ¯ . dx ¯du fk

(3.56)

In the above, the log terms come from expanding the KL-divergence terms according to their definition in Appx. A.1. The terms in the last time step are then separated by moving the extremization and minimizations inside: ^ min ext p0
N

{µk |k=0:N -1} {p0 ,fk |k=0:N -1} x0:N -1 ,u0:N -1 k=0 k =0 k ^ ^ fN -1 µN + min µN cN + N log ( + ext fN -1 N -1 log duN dxN dx0:N -1 du0:N -1 . i) f µN fN -1 xN N -1 uN µ
N

µk fk

N -1

ck + k log

µ fk + k log (k i) fk µ

(3.57) The two inner-most problems in the equation above are unconstrained minimization problems penalized by a relative entropy term, which are exactly of the same form as Prob. 5. Specifically, the inner most problem is, ^ min
µN uN

µN
µN

cN + N log + N KL µN

µN µN
( i)

duN ,
( i)

(3.58) (3.59)

= min
µN

cN

µN

.

Therefore, the closed-form solutions given by Lemma 1 can be used. Thus, solving for µN using Lemma 1, µN (uN | xN ) e
(i) (i) -
cN (xN ,uN ) N

(i+1) µN (uN

| xN ) =

ZµN (N , xN )

,

(3.60)

38

3.6. THE SOLUTION

and by plugging 3.60 into 3.59, one has the partition function given by, ^
(i+1) Zµ (N , xN ) N

=
uN

µN (uN | xN ) e

( i)

-

cN (xN ,uN ) N

duN .

(3.61)

Therefore, the second innermost optimization problem is becomes, ^
fN -1

ext

xN

fN -1 N -1 log
fN -1

fN -1 (i+1) + Zµ (N , xN ) dxN , N fN -1 fN -1 .

(3.62) (3.63)

=

fN -1

ext

(i+1) Zµ N

+ N -1 KL fN -1

One can see that the above has the same form as the problem in Remark 1. Therefore, results of Remark 1 and Lemma 1 may be applied, which results in,

fN -1 (xN | xN , uN -1 ) =

(i+1)

fN -1 (xN | xN -1 , uN -1 ) e
(i)

N log Zµ (N ,xN ) N N -1

ZfN -1 (N -1 , xk-1 , uN -1 )

.

Again, plugging the above back into 3.63 results in, ^ (N -1 , xk-1 , uN -1 ) =
xN

(i) ZfN -1

fN -1 (xN | xN -1 , uN -1 ) e

N log Zµ (N ,xN ) N N -1

dxN .

(3.64)

One can continue factorizing the minimization and extremization, and solve using Lemma 1. This gives the general update equations as,
(i)
k+1 log Zµ  k+1 ( k+1 ) k

(i+1) fk

=

fk e
( i)

Zfk (k )
(i) - µk e
ck -k log Zf (k ) k k

.

(3.65)

µk and,
(i)

(i+1)

=

Zµk (k )

( i)

.

(3.66)

^ fk e
xk+1

Zfk (k ) =

k+1 log Zµ  k+1 ( k+1 ) k

dxk+1 .

(3.67)

^
(i) Zµ (k ) k

=
uk

(i) - µk e

ck -k log Zf k

k

(k )

duk .

(3.68)

39

CHAPTER 3. INFERENCE FOR CONTROL

These update equations constitute the core of our contribution.

3.6.2

Alternating Convex Optimization

Since Prob. 1 is separately convex in each of the variables {p0 , fk , µk | k = 0 : N }, one can except one, and perform the minimization w.r.t. this remaining variable. Then, the solution

apply the alternating convex optimization method [96]. The idea is to keep all variables fixed, for this variable is used as its fixed value, and the problem is solved w.r.t. another variable. The variable is then "alternated" until convergence is resulted. Moreover, it is known that as long as a solution to the minimization exists, each iteration of the alternating minimization decreases the objective6 . Assuming that the optimal objective is bounded from below, and in each step the objective is not increased, the convergence follows ­ albeit possibly to a local minimum [96]. To apply alternating convex method to the problem at hand, let us fix all variables except µk and minimize with respect to this variable. It is easy to see that update Eqs. 3.66 and 3.68 are the solution. Similarly, if one fixes all variables, except fk , the solution is given by Eqs. 3.65 and 3.67. Since in the alternating optimization method one can optimize the variables in any order, it is easy to see that this approach simply amounts to update Eqs. 3.65­3.68 applied to any time slice, instead of strictly updating them backwards in time, hence the name, asynchronous.

3.6.3

Viewing Updates as Messages

Referring back to update Eqs. 3.65­3.68, one can begin to interpret each update as a special type of message in a message passing algorithm. Viewing our algorithm as a message passing algorithm is advantageous becomes one can easily appreciate the distributed nature of our solution, as well as its relationships to some related works such as [16, 23]. For an overview on message passing algorithms for inference on graphical models see [77]. To see each update as a message passed between two nodes, let us take a look at the update Eq. 3.65. This update equation can be interpreted as a message passed from node µk+1 to fk , given by mfk µk+1 = ek+1 log Zµk+1 (k+1 ) . This message resembles a standard
( i)

message in an inference problem, except for an extra term in the exponent, i.e., k+1 . By comparing with a Boltzmann distribution, one can see that the the role of the term k+1 can be interpreted as scaling down the temperature of a Boltzmann distribution with energy log Zµk+1 by k+1 . Similarly, one can interpret Eq. 3.66, as a combination of two messages; one passed from dynamics to policy, defined as mµk fk = ek log Zfk (k ) and one passed from
(i) (i)

the cost node to policy, given by mµk rk = e-ck . All these messages are closely similar to

standard messages in inference, except for frequent temperature modulation of messages. Therefore, it is useful to define a temperature operator,T , which scales the temperature by
6

Or more strictly, each iteration, does increase the objective, i.e., either decreases it or keeps it the same.

40

3.6. THE SOLUTION

-1 , scales the temperature by -1 . Therefore, . Similarly, the inverse operator T -1 [Z (·)] = Z (·) . T [Z (·)] = Z (·)  , T
1

(3.69)

With the definition of this operator the messages become:  ^ 
(i)

-1 mfk µk+1 = T k+1

(i)

µk Tk+1 mµk+1 fk+1 Tk+1 mµk+1 rk+1 Tk+1  ^ -1  =T
k (i) mfk µk+1 (i)

(i)

(i)

( i)

uk+1

mµk fk

fk Tk mfk µk+1 Tk 
1 (i) mµk fk

xk+1

i) -1  m( µk rk = Tk

rk =0

p (rk )



dxk+1 



duk+1 



(3.70)

(3.71)

(3.72)

The update equations in message passing form become, fk Tk mfk µk+1 Tk mµk fk
(i) ( i) ( i) (i)

(i+1) fk

=

(3.73)

(i+1) µk

=

µk Tk mµk fk Tk mµk rk Tk mfk-1 µk
(i)

(i)

(3.74)

One can see that the scale of the temperature of the messages entering a node is changed to that of the node, and when exiting a node, it is normalized. Accordingly, one can view each of µk , fk , and rk to be in an environment with their own temperature unit. When messages go from one environment to the other, their temperatures has to be scaled. Fig. 3.7 is a depiction of these messages and explains how the temperature should be modulated from one node to the other. Also, one can easily verify that if all the temperatures are taken to be the same, the messages reduce to those of a message passing solution of the posterior of the dynamics and the policies, given the observations, Pr (Rk = 1 | Xk = xk , Uk = uk ) = e special case is discussed in the following section.
-
ck (xk ,uk ) 

. This

3.6.4

The Special Case of Posterior Policy Iteration

Let's assume, k = k =  > 0. In this case, the PP iterations in Prob. 10 reduce to, 41

CHAPTER 3. INFERENCE FOR CONTROL

T =

rk
T  k mµ k  r k
(i )

T =  k -1
mfk-1 µk
(i )

mµk rk
T = k
(i ) T  k mf k - 1  µ k

(i )

f k -1

Tk mµk rk
(i ) µk
(i )

(i )

(i ) Tk-1 mfk-1 µk

mµk fk

(i )

Tk mµk fk

(i )

T  k m µk  f k

fk

T = k

Figure 3.7 ­ Out message passing solution for SOC. The temperature of messages passed from one environment to the other has to be modulated.

^
p0 ,µk ,fk

N N

p0 µk fk log
(i) p0 N k=0

min 
¯ du ¯ dx

p0
k=0 (i) -

k=0

µk fk
c0:N 

¯ du ¯ dx

(3.75)

(i) (i) - µk fk e

= min  KL q
q

q e

c0:N 

.

(3.76)

The minimization above can be solved as a standard posterior inference, which due to the factorized structure of q can be solved efficiently using message passing algorithms [77]. The terms, e
-
¯) c(¯ x.u 

=e

-

c0 

e

-

c1 

observations. Therefore, one can define auxiliary binary random variables, R0 · · · RN , ('R' as in reward)7 and define their probability of being observed to be 1 , as Pr (Rk = 1 | Xk = xk , Uk = uk ) = e
-
ck (xk ,uk ) 

···e

cN 

can be considered as likelihoods of some auxiliary

.

(3.77)

Therefore, one can represent this problem using a graphical model. A Bayesian network representation is given in Fig. 3.8. This solution is known as posterior policy iteration (PPI) as described in [16]. However, here it was derived as a special case where all multipliers are chosen equally.
7

It should be noted that these auxiliary variables can be defined arbitrarily, as long as they are observed
c (x ,u ) - k k k

 to some event with likelihood proportional to e . Here we choose R, as in reward, because the likelihood of R decreases as the cost increases and, therefore, such naming seems very appropriate.

42

3.7. LOCAL SOLUTIONS

r0

r1

rN - 1

rN

x0 µn 0

f0

x1 µn 1

x N -1 µn N -1

f N -1

xN µn N

u0

u1

u N -1

uN

Figure 3.8 ­ Bayesian network representation of Posterior Policy Iteration

The message passing solution to the posterior inference on the Bayesian net in Fig. 3.8 bears some similarities to our backward updates. The difference lies in the fact that with our backward updates, the conditional distributions are inferred directly, whereas with standard message passing, one has to infer the posterior marginals as a first step, and then derive the posterior conditionals by marginalization. Because of this difference, the messages passed on the Bayesian chain consist of a forward pass and a backward pass, while in our case, there was no forward flow of information and we only had the backward updates of Sec. 3.6.1. Indeed, to infer the posterior conditionals one does not need any forward message as they will be integrated out. Finally, PPI is known to be risk-seeking [16]. The risk-seeking property of PPI can easily be explained by noting that positive and finite choice of the multipliers ,k , results in an optimistic and risk-seeking property of the policy. This risk-seeking effect can also be seen from the perspective of risk sensitive control and by realizing that the posterior policy minimizes e
c -

be shown that for large values of | |,  log e
c -

q

, which is exactly the risk sensitive objective as defined in [68] . It can

q

= c

q

1 - covq (c) + higher order terms. 
c -

(3.78)

This means that for positive values of  , minimizing e

cost covariance. This interpretation is also mentioned in [16]. However, our own interpretation of  in connection with the radii of KL-ball constraints for the policies and dynamics is more general and can explain more general cases where k and k are chosen differently.

q

, gives policies with increased

3.7

An Extended Kalman Filter Approximation

In Sec. 3.4.5, it was shown that each KL-penalized problem can be interpreted and solved as a standard posterior problem. Therefore, any approximate inference method can be applied to solve each of the updates in Eqs. 3.65 ­3.68. Some of the approximations that can be 43

CHAPTER 3. INFERENCE FOR CONTROL

used include, approximations based on extended Kalman filtering (EKF) [97], unscented Kalman filtering, [98], particle methods [99], expectation propagation [100] and MCMC methods [19]. Here, results based on EKF-type approximations are considered, because of the simplicity of their implementation, and also, because despite it's simplicity, EKF is quite effective in dealing with nonlinear filtering problems. Naturally, just like the filtering problem, using more accurate approximate methods, results in increased accuracy and globalist of the solution, at the expense of increased computational load. Therefore, applying more advanced inference methods to approximate Eqs. 3.65 ­3.68 is a worthy direction for future research. In the following, an algorithm for computing update Eqs. 3.65­3.68 based on EKF-type approximations is described. The notation used for denoting Gaussian distributions, along with other related definitions are detailed in Appx. B. Also, fore a review on Kalman filtering and EKF see [97] and also Appx. D. With EKF-type approximations, one assumes that the stochasticity in the dynamics is Gaussian (or that it can be well approximate by one). That means that the nonlinear dynamics has the following form, fk (xk+1 | xk , uk ) = N (xk+1 | fk (xk , uk ) , w ) , (3.79)

where w is the covariance matrix for the additive Gaussian process noise. In order to apply EKF, the first step is to linearize the above nonlinear dynamics around an initial ¯ (0) . That is, state-control trajectory8 x ¯(0) , u fk (xk+1 | xk , uk ) where, N (xk+1 | Ak xk + Bk uk + ak , w ) . (3.80)

Ak = Bk =

 fk (xk , uk )  xk  fk (xk , uk )  uk

xk =xk ,uk =uk

(0)

(0)

, ,

(3.81) (3.82) (3.83)

xk =xk ,uk =uk

(0)

(0)

^k) . ^ k ) - (Ak x ^ k + Bk u ak = fk (^ xk , u

The (possibly non-quadratic) cost function is quadratized around the same state-control ¯ (0) , and approximated by the following quadratic form, trajectory, x ¯(0) , u

ck (xk , uk )
8

1 T 1 T   k + uT  xk Qk xk + uT Rk uk + xT k Cxu,k uk - xk x ku k + const. 2 2 k

(3.84)

The initialization doesn't have to be very informative. Our algorithm works for non-informative and trivial initializations. However, if a high-level path planning algorithm is available for the problem, one may use it for initialization.

44

3.7. LOCAL SOLUTIONS

Since the approximation of the cost function is quadratic, the auxiliary reward variables will have a Gaussian form9 :

Pr (Rk = 1 | Xk = xk , Uk = uk )  N

xk uk

|  -1

k x u k

,  -1

Qk CT xu,k

Cxu,k Rk

. (3.85)

Also, it is assumed that the initial guess10 for the policy has the following form, µk = N uk | Kk xk + uk where Kk is an initial control gain, uk
(0) ol(0) (0) (0) ol(0)

, µ(0) ,
k

(3.86)

is an initial open-loop control signal and µ(0)
k

is the covariance matrix for the initial policy. This policy covariance matrix describes the uncertainty in the control policy; the larger the covariance matrix11 , the more the uncertainty in the policy. The update Eqs. 3.66­3.67 can now be performed using standard posterior calculation for each update, which will be the same as extended Kalman smoothing along the path ¯ (0) . The complete derivation of Gaussian update equations is given in Appx. C and it x ¯(0) , u is shown that with an initial Gaussian policy, all updated policies remain Gaussian as well. The Gaussian updates described in Appx. C can be applied to the problem, starting from the last time slice and going backward in time (synchronous) or updating each time slice in any desired order (asynchronous). Specifically, the update Eqs. C.25­C.30 are used for computing messages passed from dynamics to policies, and Eqs. C.48­C.55 for updating policies. After finding the updated values for Kk , uk
(1) ol(1)

¯ (0) , , µ(1) , the initial trajectory, x ¯(0) , u
k

is revised by applying the updated policy to the system at initial state p0 (x0 ). This is done by iteratively drawing a random control signal from the distribution,

uk

(1)

 - - - µk

rand.

(1)

uk | x k

(1)

,

(3.87)

and applying it to the dynamics. Then a sample is drawn from the resulting distribution,

xk+1  - - - fk xk+1 | xk , uk
9

(1)

rand.

(1)

(1)

.

(3.88)

The likelihood of the auxiliary variable is formed by exponentiating the cost function. The exponential of a quadratic form is a multivariate Gaussian 10 Again, the initial guess can be non-informative or informative. An initial zero gain and zero open-loop control worked just fine in all cases during our evaluations. 11 Larger, as in the positive definite matrix sense. Indeed, the covariance matrix is positive definite, and positive definite matrices can be ordered. For instance, if the algorithm works, it is expected that µ(i) µ(0) .
k k

45

CHAPTER 3. INFERENCE FOR CONTROL

Algorithm 3.1 EKF-type approximation of general PP update Eqs. 3.66­3.67 Input: Dynamics: fk (xk+1 | xk , uk ) Initial State Distribution: p0 (x0 ) KL-Penalty Coefficients: k , k | k  {0 · · · N } (0) ol(0) Initial guess for policies. Default: Kk = 0m×n , uk = 0m×1 , µ(0) = Im×m .
k

1: 2:

i=0 while !(Maximum no. of iterations reached) or !(Converged) do (i) 3: x0 - p0 (x0 ). # Forward stochastic simulation. 4: for all k  {0 · · · N }. do
5: 6: 7: 8: 9:
rand. (i) (i) (i) xk+1  - - - fk xk+1 | xk , uk . (i) (i) ( i) x ¯(i) = x0 , · · · , xN , u ¯ (i) = u0 , · · ·

uk  - - - µk

(i) rand.

(i)

uk | xk

(i)

.

, uN .

( i)

10: 11: 12: 13:

{Ak , Bk , ak | k  {0 · · · N }} Linearize dynamics around x ¯(i) , u ¯ (i) . k, u ¯(i) , u ¯ (i) .  k | k  {0 · · · N }} Quadratize cost around x {Qk , Rk , Cxu,k , x Choose a desirable update schedule from any permutation of {0 · · · N }. Default:{N, N - 1, · · · , 0} for all k  schedule do (i+1) ol(i+1) Kk , uk , µ(i+1) | k  {0 · · · N } Update policies using Eqs. C.25­C.30 and C.48­C.55 i++
k

Output: The policy after i iterations: Kk

(i+1)

, uk

ol(i+1)

, µ(i+1)
k

Performing the above from k = 0 to k = N is referred to as a forward stochastic ¯ (1) . The simulation, and it results in a first revision of the state-control trajectory x ¯(1) , u dynamics and the cost are then linearized and quadratized around this new trajectory ¯ (1) , and the update equations are applied to update the policy. This iteration is x ¯(1) , u detailed in Algorithm 3.1, and continues until convergence is reached.

3.7.1

Choice of k and k

The choice of the set of parameters, k , and k defines the behavior of the resulting policy, as well as convergence rate of the algorithm. The proximal penalty coefficients,k , penalizes the new policy for getting too far from the old one. Therefore, increasing k , slows down the convergence for the decision, µk . For k , from the discussion in Sec. it is known that for positive values of k , the optimal distribution is a minimizing solution, whereas for negative values of k , the optimal distribution is a maximizing solution. Therefore, for positive values of k , one has Prob. , which is the risk-seeking problem, whereas for negative values of k , one has Prob. ,which is the robust problem. Finally, for k   one recovers the the standard SOC. Also, from the discussion in Sec. it's known that the inverse of|k | has a direct 46
(i)

3.7. LOCAL SOLUTIONS

k

Slower Convergence

Standard SOC

Breaking point -1 for k

Robust

Risk-Seeking

-1 k

Figure 3.9 ­ The effect of choice of k andk , on the convergence rate and the behavior of resulting policy.

relationship with the radius of the ambiguity set. This means that choosing a large value for
-1 k increases the the robust, or risk-seeking behavior. Moreover, for some negative values

of k , the partition function in Eq. may not be well-defined. This corresponds to the case where the ambiguity set is so large, that there exists a worst-case dynamics under which, the maximum of the expected cost is infinity. That is, the robust problem is too pessimistic. The effect of these parameters on the convergence rate and behavior of the policy is summarize in Fig. 3.9.

3.7.2

Application to the Cart-Pole Problem

In order to compare the presented algorithm, the nonlinear cart-pole problem is chosen. The cart-pole problem is often used as a test-bench for evaluating and comparing the performance of nonlinear control algorithms. Fig. 3.10. The objective of control is to balance the pole at  = 0, while positioning the cart at the origin xcart = 0. The state vector is defined as,  xcp = xcart x  cart   The dynamics of the cart pole is governed by,     g sin((mc +mp ))-(F +mc l2 sin ) cos          4  d   l(mc +mp )-mp l cos2   = 3 ,     dt  x x  cart  cart    2 ¨  F -mp l( cos - sin ) x  cart  
mc +mp T

.

(3.89)



(3.90)

47

CHAPTER 3. INFERENCE FOR CONTROL

 mp

l mc

xcart
Figure 3.10 ­ The cart-pole problem. The objective of the control is to balance the pole, as well as positioning the cart on the origin.

Parameter Pole mass Cart mass Pole Length

Symbol mp mc l

Nominal value 0.5 Kg 1 Kg 1m

Table 3.1 ­ Parameters of the cart-pole problem.

The parameters of the above model are defined in Table 3.1, and the control input is the force applied to the cart, F . The dynamics of cart-pole above is simulated using a fourthorder Runga-Kutta method, and to create stochasticity, a zero mean Gaussian noise with small covariance (1e - 5 × diag ([0, 5, 1, 1, 1])) is added to the predictions of the deterministic model above. The cost function is defined as the quadratic form below, c(xcp ) = xT cp diag

0.1 0.01, 1 0.01

xcp + 0.01F 2 .

(3.91)

The above cost function is minimized when the pole is balanced and the cart is positioned correctly, while giving the balancing task more priority by assigning it larger weight. To compare our method with standard SOC, the iLQG method [61] was also implemented for the problem. When iLQG converges correctly it finds a locally valid optimal policy to the problem. iLQG is chosen because it's been shown to have very fast convergence rate and also because it's been used for comparing SOC algorithms [16, 27, 28]. Specifically, the goal is to compare the convergence rate and convergence success rate and to showcase our robust SOC methodology compare to the standard SOC policy. 48

3.7. LOCAL SOLUTIONS

3.7.2.1

Convergence Rate and Convergence Success

First, the convergence rates for the two algorithms are compared. iLQG has second order convergence rate, and so, it's already quite fast. However, our algorithm also enjoys second order convergence rate, in addition to some advantages which further increase its convergence rate in practice. Specifically, our algorithm uses a stochastic forward pass, as opposed to the deterministic procedure in iLQG. The randomness in the forward pass allows it to escape local minima and steep regions, while iLQG spends many iterations near steep regions. This is similar to stochastic optimization methods such as simulated annealing, where the added stochasticity allows the algorithm to escape local minima. Moreover, iLQG needs complicated regularization and/or a line search method to achieve successfully converge. For our comparison, an advanced implementation of iLQG which utilizes a backtracking line search in the inner loops was used. This extra step, adds to the number of forward passes, but ensures a greedy decrease of expected cost in each iteration. Our method, however, enjoys a built-in regularization through the the set of k parameters, which control the proximity penalty. That is, they control how far it is allowed for the updated policy to go from the current policy. For our simulations, it was found that k = 1E - 8 was good enough to ensure successful convergence for all cases. This builtin regularization was also found to be fairly insensitive to the choice of, k , and changes However, for larger choice of k , the regularization starts to be conservative and, so, the convergence rate begins to slow down. However, for some problems with sharp cost functions or other irregularities, increasing k trades convergence rate for convergence success. Fig. 3.11 shows the convergence rate of our approach for a fixed value of k = 1E - 8 and different choices of k , in comparison to iLQG. It is observed that the convergence rate speed is improved. This fast speed was also achieved by other inference based methods such as the work reported in [28], although our work enjoys more control over convergence through independent choice of k for each time step, k . However, the true power of our approach comes from the fact that it can reproduce robust and risk-seeking control, without any increase in the computational load. 3.7.2.2 Robust and Risk-Seeking Control within 2 order of magnitude did not effect convergence rate or convergence success noticeably.

As seen earlier, our algorithm is able to produce robust and risk-seeking through the choice of k . It was shown that for a negative choice of k robust control is resulted, while for positive choices, one reaches risk-seeking policies. In this section, the behavior of the optimal policy produced by our algorithm, for several choices of k is evaluated, compared to iLQG. It should be noted that for k   our algorithm results in standard SOC, and therefore, it should result in the same average cost and variance as iLQG. To compare the policies with different behaviors, several policies have been derived using our algorithm. Then, the policies were applied to a 1000 simulations of cart-pole system 49

CHAPTER 3. INFERENCE FOR CONTROL

65

iLQG
60 55

 -1 = -10  -1 = -50  -1 = -100  -1 = -200  -1 = -400  -1 = 0  -1 = 10  -1 = 100

Mean total cost

50 45 40 35 30 25

0

100

200

300

400

500

600

700

800

900

1000

Number of Iterations

Figure 3.11 ­ Convergence rate comparison of iLQG vs. our method for  = 10-8 , and different choices of  . In most cases, our algorithm convergence within 400 iterations, whereas it takes iLQG about 650 iterations to converge.

with 1%, 3% and 5% perturbation in the parameters of the system. The the cost resulted from iLQG and policies with different robustness (or risk-seeking) behavior were recorded for 1000 trials, and the average and the variance of the total cost was compared. The results are presented in Fig. 3.12. It's observed that when k is chosen as negative values, the cost variance is considerably lower than the cost variance resulted from applying iLQG. Also, for positive k the risk-seeking control results in increased cost variance. These results match our expectations based on the theory developed in this chapter, and show that with the same, or lower computational burden, it is able to derive robust control laws which achieve the same mean expected cost in low-uncertainty cases, while achieving much lower cost variance for cases when the uncertainty in the parameters is high. Fig. 3.12 show that the cost variance for our robust control policy can be as much as 5 times lower than the standard stochastic optimal policy calculated using iLQG. It is also shown that for the risk-seeking case, the cost variance is actually increased. The risk-seeking case was already derived through an inference based method in [16], while the results presented here show that all the three cases of risk-seeking, standard, and robust control can be reproduced by sweeping a parameter. It's also useful to compare the time varying control gain matrix, resulted from robust control and risk-seeking control vs. the standard SOC. Fig. 3.13. A first inspection suggests that in all cases, the robust gain is lower while the risk-seeking gain is larger (in absolute value) than the standard SOC gain. However, the control gain is not simply different by a scale factor, as any attempt to reproduce one control gain by simply scaling the other failed. 50

3.7. LOCAL SOLUTIONS

Average of total cost variance for 1000 trials

Average of mean total cost for 1000 trials

0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 -400 -200-100 -50 -10

-1

35 30 25 20 15 10 5 0 -400 -200-100 -50 -10

-1

a)

0

10 100

0

10 100

Average of mean total cost for 1000 trials

Average of total cost variance for 1000 trials

35 30 25 20 15 10 5 0 -400 -200-100 -50 -10
- 1

3.5 3 2.5 2 1.5 1 0.5 0 -400 -200-100 -50 -10

-1

b)

0

10 100

0

10 100

Average of total cost variance for 1000 trials

Average of mean total cost for 1000 trials

35 30 25 20 15 10 5 0 -400 -200-100 -50 -10 0
- 1

35 30 25 20 15 10 5 0 -400 -200-100 -50 -10
- 1

c)

10 100

0

10 100

Figure 3.12 ­ Comparison of the average of mean total cost and total cost variance for 1000 trials of the cart-pole problem with a) 1% parameter perturbation b) 3% parameter perturbation c) 5% parameter perturbation. In all cases, the dashed (red) line corresponds to the benchmark results for iLQG.

51

CHAPTER 3. INFERENCE FOR CONTROL

1

4

The first element of the gain i.e., K 1, 2 [m/N · Sec.]

The first element of the gain i.e., K 1, 1 [m/N]

0

2 0 Robust Gain Standard SOC Gain Risk-seeking Gain

-1 -2 -3 100

Robust Gain Standard SOC Gain Risk-Seeking Gain

-2 -4 -6 40

0

1

2

3

4

5

6

Time [Sec.]
80 Robust Gain 60 40 20 0 -20 -40 0 1 2 3 4 5 6 Standard SOC Gain Risk-Seeking Gain

0

1

2

3

4

5

6

Time [Sec.]
Robust Gain Standard SOC Gain 20 10 0 Risk-Seeking Gain

The first element of the gain i.e., K 1, 3 [rad/N]

The first element of the gain i.e., K 1, 4 [rad/N·Sec.]

30

-10 -20 0 1 2 3 4 5 6

Time [Sec.]

Time [Sec.]

Figure 3.13 ­ Comparison of the gain matrix for a robust policy the standard and a risk-seeking policy.

3.7.3

Advantages of Our Solution

· Computational Efficiency: Compared to standard DP, the EKF approximation of our approach doesn't suffer from curse of dimensionality, by reducing the computation to a relevant region of the state and control space. This is the same technique as SDDP and iLQG, however, it was shown that our method outperforms iLQG because of superiority in dealing with local minima. · Analytical tractability: The update Eqs. 3.65­3.68 lend themselves to analytical solutions for large classes of problems. Specifically, analytical solutions for LQ cases were derived in this section. · Generality: Our solution can be applied to problems with any form of nonlinear dynamics, non-Gaussian noises and non-quadratic cost function. Therefore, it applies to the most general class of SOC problems. · Flexibility: Our solution subsumes robust, optimistic and standard SOC, because each of these three variations can be derived as a special case of the solution to the PP problem above. For k , 0 > 0 one has optimistic control, for k , 0 < 0, robust control and for k , 0  +, one recovers the standard SOC. The Lagrange multipliers,  k , control the optimistic/pessimistic behavior of the policy locally in time. That is, the policy can be pessimistic at time k = 4, by a choice of  4 < 0, while optimistic at time 52

3.8. RELATED WORKS

k

k=9

k

Figure 3.14 ­ A controller with varying robustness behavior. The controller starts with a pessimistic assumption about the available model of the plant and gradually shifts towards an optimistic behavior.

k = 5, by choosing k > 0. This allows the designer to define new types of controllers with a mixed pessimistic/optimistic behavior. This added flexibility has never been possible before. Fig. 3.14 depicts an example controller with a certain choice of k that results in an initially pessimistic (or robust) behavior and slowly shifts towards optimistic (or risk-seeking) behavior, as time passes. For instance, if the model of the system, fk , is being learned or updated through a adaptive methodology, the designer can adjust the robustness based on how much trust he/she wants to put on the model.

3.8

Related Works

Our treatment of SOC can be related to a large set of works from different fields. However, for brevity only the most significant connections are discussed.

3.8.1

Linearly Solvable SOC

It was shown that problems with a weighted relative entropy term in the objective have a closed-form solution given by Lemma 1. A wide range of works in the literature take advantage of this fact. A large class of these works, referred to as linearly solvable SOC ( or sometimes, KL Control ) assume SOC problems with cost functions of the following special form: ^ cKL k (xk , uk ) = ck (x) +
uk

µk (uk | xk ) log

µk (uk | xk ) duk , passive µk (uk | xk )

(3.92)

for which closed-form solutions of the form in 3.28 can be derived. Also, it is shown that these problems can be linearized by an logarithmic transformation [22, 50], hence the title. Path integral control [101] is a special case of these problem. Also, it's been shown that these problems can be considered as a special case of Linearly Solvable Markov Games (LMGs) 53

CHAPTER 3. INFERENCE FOR CONTROL

[102]. All of these works assume the above form for cost function, and therefore, do not apply to general SOC problems, whereas the presented approach does.

3.8.2

Dynamic Policy Programming

A recent approach to SOC, namely the dynamic policy programming [56], and a later work [16] artificially inject a KL term into the cost function, in order to produce a sequence of problems with solutions converging to the optimal policy. This is, in fact, nothing but a PP method, which is used in these works without explicitly calling it so. The class of problems solvable by these approaches is more general than KL control problems, and includes very general nonlinear SOCs. However, our approach is more general than these works and reduces to these works as a special case.

3.8.3

Robust Control Approach of Sargent-Hansen

In Sec. 3.6 it was shown that for finite and negative values of the parameters,k and 0 , the PP problem solves the robust control of [60, 92]. Also, each iteration of the PP algorithm, solves a robust control problem penalized by a proximity function on the policy. Therefore, our approach provides a novel solution to the robust control as a special case.

3.8.4

Rationality Bounded Games

In a seemingly disparate set of works, the concept of rationality bounded games are introduced, in which, a rationality cost (or information processing cost), expressed in terms of a (relative) entropy term is added to the cost function of agents in a two-player game. Using these ideas Ortega [57, 58] showed that a generalization to the Bellman equation can be derived. Also, they show how a two-player rationality bounded games can be reduced to SOC and robust control in the limits of rationality of the two agents. In our work, each iteration of the PP problem can be interpreted as a rationality bounded game of the same form as in [47, 57]. In other word, rationality bounded games constitute the inner loop of our iterative algorithm. Thus, our approach can be considered as a sequence of rationality bounded games. The Ortega discusses how a penalty term of the form  KL (p (x) p0 (x)) can quantify the amount of (computational ) work an agent has to do to
p(x) .

The more computational resources available to the agent, the less the impact of computational cost on the result. Therefore, one can say that the weight of this penalty term is lower for more resourceful (or more rational) agents. A perfectly rational agent has unlimited computational resources and approach p (x) as much as he/she wants, i.e. an agent with   0. Therefore,  -1 , can be interpreted as the rationality parameter. One can interpret a

transform an initial distribution, p0 (x), to a solution which maximizes a utility, -c (x)

single iteration of our proximal point algorithm with Lagrange multipliers k , k and 0 , 54

3.8. RELATED WORKS
-1 -1 -1 as a two player bounded rationality game with rationality parameters k , k and 0 .

To see this interpretation more clearly, let's first identify the two players (or agents). In game theory, agents are epistemic metaphors which represent a (utility or cost) optimization problem. In the SOC problem, the controller tries to come up with a policy that minimizes the expected total cost. So the controller is clearly one of the agents. The plant, however, looks like a passive entity which merely follows a certain dynamics. Therefore, the plant in SOC is not a rational player. Fortunately, in rationality bounded game theory we can consider such an agent as an extreme case of a rationality bounded agent, i.e. , an agent
-1 with zero rationality, or, k  0.

Each iteration of the optimistic and pessimistic proximal point solutions discussed in Sec.

3.5 can also be interpreted as a two player game with plant as the cooperative or adversarial second player, respectively. The adversarial case, which is when the plant tries to maximize the cost and the controller tries to minimize it, is very similar to a game-theoretic minimax problem where the perfectly rational players are replaced with rationality bounded ones. If the second player is cooperative, which is when, she/he, too, tries to minimize the total average cost, we have the optimistic. It should be noted that while Ortega [57, 58] considers the case of fully rational adversarial
-1 plant, i.e., k  -, as robust control, here we content that this extreme case is not

interesting at all, because it assumes an arbitrary and worst case dynamics for the plant and

solves the optimal policy for this dynamics. This worst case dynamics becomes increasingly
-1 irrelevant to the nominal dynamics of the plant as k  -, and as a result, the policy

also becomes increasingly over-conservative for the nominal dynamics and other plausible

dynamics in its proximity. Also, it was shown that robust control approach of Sargent-Hansen calls for finite and reasonable radii for the reasonably chosen ambiguity set and it cannot be may not even be well defined.
-1 chosen to be arbitrarily large. Moreover, in Sec. 3.4.4 it was shown that the case k  -

3.8.5

Risk-Sensitive Control and Stochastic System with Relative Entropy Uncertainty Bounds

Stochastic system with relative entropy bounds on their uncertainty, i.e., the pessimistic case defined in Prob. 1, have been studied in [103], and their relationship to risk-sensitive control has been known. Specifically, [103] explains how a robust control problem with relative entropy bounds can be solved as a risk-sensitive control problem. The work in [103] also provides the solution for the LQG case. While the proposed method here uses the same problem formulation as in [103], the solution given here is applicable to problems with nonlinear dynamics, while the LQG results in [103] are only applicable to linear problems with quadratic costs. In fact, the proposed solution in this chapter is based on an iterative application of posterior calculation, and is inherently different from the method proposed in [103]. 55

CHAPTER 3. INFERENCE FOR CONTROL

3.8.6

Other Related Works

There are several other inference-based works which have connections to our work. In some works, [17, 24], Expectation-Maximization (EM) is used to derive an iterative solution to SOC. Indeed, EM, alternates between finding the expected value of the log-likelihood for a set of parameters (the E-step), and maximizing this expected log-likelihood with respect to some parameters (the M-step). Therefore, EM can be considered as an instance of alternating optimization approach [96]. Our approach also finds the policy which maximizes the energy (or expected log-likelihood) of the total graph. However, our approach does this differently, with synchronous or asynchronous updates that improve the policy at each update. Another recent work [23] studies message-passing solutions to general influence diagrams based on a proximal point algorithm, with a penalty function very similar to ours, except our penalty function is more general and allows for choosing the penalty coefficients to be chosen independent of each other.

3.9

Filtering Problem and Integration with SOC

It was shown that that the solutions to optimal control problem can be derived by a special type of inference. Especially, in Sec. 3.15 it was shown that a risk-sensitive policy can be derived by using standard posterior inference. This special case has been previously derived in [16] and is known as PPI. Since both filtering and SOC can be solve using inference, in this section it will be briefly shown how PPI and the filtering problem can be combined into a single inference problem and solved simultaneously and efficiently using message passing algorithms.

r - Ne

y -1

r0

r1

r Nc

x -N e

x -1
n µ- 1

x0 µn 0

f0

x1 µn 1

f N -1

x Nc µn N

u - Ne

u -1

u0

u1

u Nc

Figure 3.15 ­ PPI combined with filtering . More than one observation are used in order to get more accurate state estimation.

To integrate filtering and control, an integrated filtering/control graphical model can be created and solved for posterior marginals and subsequently, for policies. To solve the graph a variety of solutions to inference on graphical models can be used, including efficient message passing algorithms [77, 94]. Fig. 3.15 shows the combined filtering and optimal 56

3.9. INTEGRATION

control graphical model. For a problem with quadratic costs and Gaussian noises, when a linearized model of the plant is used, the solution to the above is a standard extended Kalman smoothing, solve using a forward and backward sweep [104].

57

Chapter

4

Application to Visual Servoing
Visual servoing deals with the problem of controlling robotic systems through visual feedback. Research on visual servoing has been around for at least three decades, since the term was coined in 1979 [62]. Since then, numerous useful surveys and reviews have been written on the subject such as [105­109]. Visual servoing is a multifaceted field and relates to many disciplines such as computer vision, control theory and robotics. Visual information, i.e., images from cameras or other visual sensors, has to be processed to extract information relevant to control. This information is then fed to the controller which has to be designed in order to stabilize the complicated and nonlinear dynamics of the robot, in addition to controlling the robot according to a desired behavior. Fig. 4.1 illustrates a typical cycle and the various sub-problems faced in visual servoing. In this chapter, our focus is on the control problem and, therefore, vision problems and low-level control issues of the robot are assumed to be readily taken care off, using standard approaches. These are realistic assumptions that are assumed by most works in visual servoing [107]. The goal, in this chapter, is to apply our novel SOC solution to visual servoing and to implement it in real-time. Optimal control has several advantages over classical approaches
Controller Feature extraction Estimation

Images Camera

Robot

Figure 4.1 ­ A typical cycle in visual servoing demonstrating the diversity of the problem.

58

4.1. PRELIMINARIES

in visual servoing, and so, in recent years several works have tried to apply such advanced control approaches to visual servoing [110­113]. These works clearly show the improvements resulted from application of optimal control methods to visual servoing. In fact, it is natural to expect advanced control methods to show improvements over classical and much simpler controllers. The real challenge, however, is to implement these methods in real-time. This is not a trivial task, because such advanced control methodologies suffer from heavy computational loads, which don't simply allow a practical implementation for real-world applications. Consequently, most of the previous results on the use of advanced optimal-control-based methods to visual servoing were only verified through simulations [110, 111, 113]. Since one important aspect of the proposed SOC solution is its computational efficiency, it's a great candidate for real-time implementatio of optimal contro for visual servoing. In this chapter, the proposed SOC solution is tailored and modified for application to visual servoing, and experimental results of its successful real-time implementation are presented and compared with a standard visual servoing method to show its advantages.

4.1

Preliminaries

There are many variations and flavors of visual servoing problems. First of all, the robotic system to be controlled can be a mobile robot, as well as an industrial robotic manipulator. In the case of industrial manipulators, which is the focus of this chapter, many configuration of camera (or cameras) w.r.t. the manipulator are possible. For a single camera, it can be mounted on top of the end-effector of the robot (i.e., eye-in-hand configuration) or placed statically in the work-cell of the robot to observe the robot and the environment together (i.e., eye-to-hand configuration) [107]. Of course, a combination of the two, as well as using stereo vision has also been studied. Fig. 4.2a depicts the eye-in-hand configuration and Fig. 4.2b illustrates the eye-to-hand configuration [105, 109]. Although due to its generality, given appropriate state-space modeling, the proposed control methodology can be applied to all of these problems, the goal of this chapter is to demonstrate its usefulness by applying it to a practically important case. Specifically, an eye-in-hand configuration with a single camera mounted on top of an industrial robot is assumed. The control objective of the visual servoing problem considered here is to drive the robot into a target pose (i.e. position and orientation) with respect to a target object. In order to represent these with more mathematical rigor, a brief review on kinematic notations follows.

4.1.1

Kinematic Notations

The kinematic notations used herein are similar to the tutorial on visual servoing in [109]. Accordingly, the position of a point in an arbitrary coordinate frame is denoted by a P , where the leading superscript, a, defines the coordinate frame. The superscripts, c, o, b, and 59

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Camera

Object Object Field of View Field of View

Camera

(a) The eye-in-hand configuration.

(b) The eye-to-hand configuration.

Figure 4.2 ­ Two configurations of camera in visual servoing for industrial manipulators.

e are used to denote camera frame, object frame, base frame and the end-effector frame, respectively. The rotation matrix and the translation vector of frame b w.r.t frame a are denoted by a Rb and a Tb , respectively. Also, a Fb is used to denote the pose of frame b w.r.t. a, or equivalently, a frame transformation from b to a. Therefore,a Fb can be thought of as a standard homogenous transformation used very commonly in robotics [114]. The leading

superscript specifying the frame may be dropped whenever it refers to the base (or the world) frame. Furthermore, the rotational, and translational velocity vectors are defined as  = [x , y , z ]T and V = [vx , vy , vz ]T , respectively. These two can also be stacked to give the commonly known velocity screw  = [VT , T ]T . In order to transform a velocity screw from a coordinate frame to another, one can use,

b

=

bV b

=

bR aV a

- b Ra a  × b Ta
bR a a

.

(4.1)

Using 4.1 one can change the coordinates of the velocity screw of the camera from base frame to camera frame and vice versa. This relationship is helpful for conversion between frames since it is desirable to describe everything in the camera frame. Given the notations above, the objective of the control can now be described more rigorously. The goal of the control is to bring the pose of the camera w.r.t the object, o Fc , into a the desired pose,
oF c,

or equivalently, to bring the corresponding rotation matrix and translation matrix into

desired ones, o Tc and o Rc . In order to achieve this goal using visual feedback, the raw images referred to as visual features. 60

from the camera are processed to extract the most relevant visual information, which are

4.1. PRELIMINARIES

4.1.2

Visual Features

For control purposes, one does not need to use all the information contained in raw images from camera. Typically, it is enough to base the control on a set of prominent visual features. For instance, corners, sharp edges, distinctive blubs or shapes such as circles and rectangles can be easily detected and tracked in a sequence of images, and used for matching and alignment, and ultimately, for controlling the pose of the robot. There exist more advanced visual features extraction mechanisms that can be used for visual servoing, such as scaleinvariant features [115] used for visual servoing in [116]. However, these methods are more complex onto the image processing side, while the focus of this chapter is on the control methodology. Therefore, for our purposes, it suffices to consider two dimensional point features in the image, which can be detected from simple corner detection or by finding centers of high contrast blubs. Naturally, any image processing module which outputs a set of 2D points, (such as scale-invariant feature extraction procedures) can simply replace our basic blub detection module without modifying the controller. Each point feature in the image can be described by two parameters, namely, U and
V

which are the horizontal and vertical pixel coordinate of the point, respectively. Each

The relationship between points in the environments and their projection on the image a fish-eye lens has a different mapping than a wide-angle lens. Even for typical lens-camera configurations the projection may not be a a simple perfect planar projection and there

of these 2D image points is a projection of an actual 3D point, P , on the camera sensor. depends on the type of the camera and the types of lenses among other things. For instance,

is always some image distortion. The mapping between actual 3D points and their 2D projection on the image is described by the camera model. The simplest camera model is the pinhole camera model, which is used in this chapter as well as nearly all works in visual servoing. In a pinhole model, the mapping is described by a perspective projection. Fig. 4.3 illustrates the projection of a 3D object onto the 2D image plane of the camera. The relationships governing this projection are given by Eqs. 4.2­4.4.

c

P =
U V

X Y X
Z/fc

Z

T

(4.2) (4.3) (4.4)

= =

+ CU + CU

Y
Z/fc

The parameters, fc , CV and CU are the focal length and the vertical and horizontal center pixel coordinates, respectively. These parameters can be derived from a standard camera calibration procedure, such as the one described in [117]. It's also useful to define the following quantities, 61

CHAPTER 4. APPLICATION TO VISUAL SERVOING

y

x

ag Im
Center of Projection

e lan P e

z

Focal Length

Figure 4.3 ­ Pinhole model.

X Y

= =

d =

(U - CU ) , fc (V - CV ) , fc 1 , Z

(4.5) (4.6) (4.7)

where X and Y are the coordinates of 2D points on image plane, in meters (instead of U and V which were in pixels) and d is the inverse depth. For reasons which will become clear shorty, it's preferred to work with the inverse of depths instead of depths themselves. Finally, the coordinates of several image points can be stacked together to give the feature vector, (inverse) depths, d. s = [X1 , Y1 , X2 , Y2 , · · · ]T , and several inverse depths are stacked to make up the vector of

4.1.3

Dynamics of Features

Since it's assumed here that the target object is rigid, s is a function of b Fc and b Fo , which are the camera and object coordinate frames, respectively. Mathematically, s = s(Fc , Fo ).

(4.8)

is a function of the movements of the object in the base frame. Also, the image features are 62

In an eye-in-hand configuration, for instance, Fc is a function of the robot joints and Fo

4.1. PRELIMINARIES

not directly a function of the base frame, as they only depend on the pose of the camera relative to the object; if the camera and the object were rotated and translated identically, w.r.t. the base frame, the image and therefore the image features wouldn't change. Therefore, Eq. 4.8 can be written more accurately as, s = s (c Fo ) . (4.9)

Furthermore, it is possible to establish a linear relationship between the time variation of s and the time variation of c Fo , represented in velocity screw form, as explained in [118, 119]. s  = Ls [c  c (q  ) - c  o (t)] . (4.10)

The first term in the parenthesis is the velocity screw of the camera represented in the camera coordinate frame and the second term is the contribution of the motion of the object. In this chapter, it is assumed that the target object is static (or quasi-static). This means that the there is no (or little) contribution from the motion of the object. Therefore, s  = Ls c  c . (4.11)

Ls is referred to as the interaction matrix or the image Jacobian. The interaction matrix can be derived by differentiating the projection Eqs. 4.2­4.4. Therefore, for a single 2D point feature, the interaction matrix is given by, Li = -di 0 0
Xi

-di Yi · di 1 +

·d

Xi · Yi

Yi2

- 1 + Xi2 -Xi · Yi

Yi

-Xi

.

(4.12)

For more features, the interaction matrix is given by stacking the above matrix for each 2D feature. That is,  L1      

  L2 Ls (s, d) =   . .  .

(4.13)

Lnf

(2nf ×6)

where nf is the number of features. The interaction matrix is a function of the feature vector, s, and the vector of feature depths, d. Given the above, the rate of change of the features can be related to the velocity of the camera by, s  (2nf ×1) = Ls (2nf ×6) 63
cV c c

.

(4.14)

c

(6×1)

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Since the camera is rigidly attached to the end-effector, the velocity of the camera can  . Indeed, one can relate c  c and q  through be related to the time variation of robot joints, q the robot Jacobian as,

c

c =

cV c

c

= c Jr (q) q .

(4.15)

c

It's of practical importance to pay attention to the coordinate frames in which these quantities are defined. In Eq. 4.15, c Jr (q) is the robot Jacobian referenced in the camera frame. In practice, either the Jacobian of the robot in the base frame has to be transformed to the camera frame or the velocity screw of the camera has to be transformed into the base frame using Eq. 4.1. Therefore, represented in base frame, one has,  c = J r ( q) q . (4.16)

where Jr (q) and  c are represented w.r.t the base frame1 . Therefore, the relation between time variation of the feature vector and the robot joint velocities is given by, s  = Ls (s, d) Jr (q) q . (4.17)

Indeed, Eq. 4.17 is the starting point in many works, such as the tutorial in [107].

4.1.4

Classical vs. Advanced Control Methods and Problem Definition

An important classical method in visual servoing is known as image-based visual servoing (IBVS), in which, in order to drive the robot into the desired pose, some visual features are chosen in an initial image and a target image, and the goal of the control is set to align these two sets of features. It is possible to show that for sets of four or more features, if the error between all 2D features in the initial image and the target image are reduced to zero, the resulting pose is the exact pose from which the target image was observed [118]. Therefore, IBVS tries to reach the desired pose, indirectly, through minimizing the error between a set of observed 2D features and a set of target 2D features. All methods that try to reduce feature errors in image space, in order to reach a target pose, are referred to as image-based methods. Fig. 4.4 illustrates the idea behind the IBVS. Another important classical method is known as position-based visual servoing (PBVS), in which, the pose of the robot is directly regulated . Naturally, position-based approaches require an estimation method for determining the current pose of the robot with respect to the target. For performing pose-estimation, position-based methods often need more information about the environment and the object, compared to the image-based methods [109]. For instance, a lot of PBVS approaches require a 3D model of the target object for
1

The superscript, b, has been dropped by convention, as previously stated.

64

4.1. PRELIMINARIES

Field of view

Target image

Initial image

Figure 4.4 ­ Image-based visual servoing. The 2D features in the initial image are matched and aligned with the 2D features in the target image, and as a result, the robot is moved to the desired pose.

accurate pose estimation. Even so, accurate pose estimation is often quite demanding and modeling errors and measurement noise can result in pose estimates which are far from accurate. As a result, IBVS methods are commonly believed to be more robust to modeling errors and other forms of uncertainty [105, 109]. However, while PBVS can be a straightforward problem in control level, IBVS is more challenging, because of the highly coupled and nonlinear dynamics of the problem, in addition to uncertainties in the depth information. In this chapter, an image-based methodology is to be perused, because of the smaller amount of information needed for implementation of IBVS, compared to PBVS. Specifically, it is not desirable to assume that a 3D model of the target object is always available. Therefore, in this chapter, minimal assumptions are made about the requirements of the problem. The standard IBVS method tries to regulate the error between the desired values of the features and their current values, through a simple proportional type controller of the form, q  = - (Ls Jr ) (s - s ) , (4.18)

where, s , is the desired values of the features,  is the pseudo-inverse operator, and  is the gain for the proportional controller. In fact, when  is equal to 1, the above control law is cost2 ,
When  is not equal to one, the cost function has a slightly different form:c (s) = (s - ((1 - )s + s ))T (s - ((1 - )s + s )), which simply means instead of trying to regulate all of the feature errors to zero in one step, a portion of it is regulated.
2

equivalent to a one-step optimal controller, which tries to minimize the following quadratic

65

CHAPTER 4. APPLICATION TO VISUAL SERVOING

c (s) = (s - s )T (s - s ) ,

(4.19)

subject to the dynamics of the feature described by Eq. 4.17. Therefore, the classical IBVS can be though of as an optimal controller with a one-step control horizon. Because of the fact that the classical IBVS only foresees a single time step into the future, it suffers from a myopic behavior, which in turn, results in several well-documented problems [113, 120]. These problems include infinite retreat of the camera in face of large rotations, getting trapped in local minima, and singularity of interaction matrix. Also, the work-space path traveled by classical methods is not optimal, because there is no procedure in place for a choosing a shortest path. In fact, the coupling between translation and rotation causes the work-space trajectories to curve considerably for scenarios with large rotations. Optimal control methods, on the other hand, look further than a single step, and therefore, are able to devise a longer plan which results in a more forward-looking behavior. As a result, large rotations and local minima can be handled with ease, and the resulting work-space paths traveled by robot using optimal control methods is shortened [111]. Indeed, there already exist several works which apply a powerful optimal control-based methodology, known as model predictive control, to visual servoing [110, 111, 113]. These works are fairly successful in demonstrating the strengths of optimal control for visual servoing. However, in most of the existing works, the optimization problem encountered when applying optimal control to visual servoing, is solved using general purpose nonlinear programming tools. In fact, solving a high-dimensional nonlinear program in real-time, for a reasonably large control horizon, is far from practical, even with the current computational power of computers. Therefore, all existing verification of these methods is done through computer simulations. Thus, despite the fact that optimal control is a very desirable approach for visual servoing, without the possibility of an actual real-time implementation, these methods will stay on paper. Fortunately, in Chap. 3, a computationally efficient algorithm for solving SOC was developed, which is a good candidate for realizing optimal control for visual servoing. In the following sections, Algorithm 3.1 will be tailored specifically for IBVS and successfully applied in practice. However, before ending this discussion, it should be acknowledged that some older works exist which apply LQR theory to IBVS, in real-time [121]. However it's been shown that LQR control for IBVS is very sensitive to modeling errors and, especially, various delays in the system. The work reported in [122] demonstrates how delays as low as a single camera frame can render LQR useless, or even unstable. In fact, in preliminary studies for this thesis, LQR was implemented in both simulations and real-time experiments. It was observed that despite very fascinating performance of LQR in simulations, in practice the system only demonstrated unstable behavior. Therefore, LQR, while interesting and potentially useful for very low frame rates, proves too sensitive for an actual use. This might be one of the reasons why LQR, despite its simplicity and theoretically good performance, hasn't become 66

4.2. STATE-SPACE MODEL

a standard method in visual servoing. In order to be able to apply the proposed SOC solution to IBVS, the first step is to derive a discrete-time state-space model of the system.

4.2

State Space Model for Image Features

In order to derive a state space model, the first step is to define the control signal and the state vector. For the control signal, choosing robot joint velocities is a good option, provided that a low-level joint velocity controller is readily available. In our case, a PI controller for controlling the joint velocities is already implemented and tested. Therefore, the joint velocities are taken as the control signal, u q . (4.20)

The state vector should contain the feature vector, the vector of feature (inverse) depths, and the robot joint vector. Therefore,   s   d . q

x

(4.21)

Using Eq. 4.17, the time variation of the state of the system is given by,      s Ls (s, Z) Jr (q)     , d =  Ld (s, d) Jr (q)  q =u  q I
=x  =J(x)

(4.22)

where the derivations of Ls and Jr were already discussed, and I is the identity matrix. The remaining Ld (s, d) term defines the time evolution of the inverse depths w.r.t. camera velocity screw c  c , given by,   0 0 d2 1 Y1 · d1 -X1 · d1 0    0 0 d2 Ld =  2 Y2 · d2 -X2 · d2 0 .  . . . . . . . . . . . . . . . . . .
nf by 6

(4.23)

Therefore, x  = J (x) u. A Euler discretization of the above, with time step 67 t results in, (4.24)

CHAPTER 4. APPLICATION TO VISUAL SERVOING

xk+1 = Ak xk + Bk uk , where,

(4.25)

Ak = I + Bk

J (x) u t, x = J (x) t.

(4.26) (4.27)

In the above, all required terms except J(x)u/ x can be readily calculated from analytical formulas. To further improve the computational efficiency of the proposed solution for visual servoing, the analytical description for
J(x)u x

was derived by symbolically differentiating

J (x) u w.r.t. x using the Matlab Symbolic Toolbox. For an example case of 4 point features, and a 6 DOF robot, this matrix is an 18 by 18 matrix (the dimension of x is 4 × 2 = 8 for each 2D point, plus 4 depth values and 6 joint values). Thus, the derivation of the state space model is complete.

4.3

Partial Observability

In Chap. 2, it was explicitly assumed that the full information of the state of the system is observed in each time-step. However, for the case of IBVS, the state of the system defined by, x sT dT qT , is not fully observed, because the depth information is not available at all. Also, the readings of the coordinates of point features in the images are subject to image noise, as well as other forms of uncertainty caused by illumination changes and temporary occlusion of the features, among other things. The joint values of the robot are available through motor encoder readings which are, at least, subject to quantization error. Therefore, the problem of IBVS studied here is not fully observable. The full solution to SOC for partial observable problems involves the concept of dual control [123­125], which is out of the scope of this thesis. It can be pointed out, briefly, that dual control states that the problem of estimating the state of the system through noisy observations, and the problem of optimal control of the plant are interrelated. This means that the control policy should try to improve the state estimation, at the same time as it's trying to minimize the control cost (hence the name, dual). Fortunately, for the important LQ case, there exists an influential result known as the separation principle [123, 126]. The separation principle states that for LQ problems, the problem of state estimation and optimal control are not interrelated, and hence, can be performed in separate phases. In fact, dual control is rarely implemented in practice, due to the fact that the exact solution suffers from curse of dimensionality, in infinite dimensions [127­129]. However, in practice, even when the separation does not hold exactly, it is often assumed as a useful heuristic. 68
T

4.3. PARTIAL OBSERVABILITY

Separation Principle

Plant

Noisy Observations

Plant

Noisy Observations

=
Certainty Equivalence
Dual Dual Control Controller Signal



Controller (Fully observable)

State Estimate

Filtering

Figure 4.5 ­ The separation principle and certainty equivalence heuristic.

Therefore, for partially observable problems, a common approach is to break down the problem into an estimation (or filtering phase) and a control phase. The estimator outputs the most-likely state estimation, given all previous measurements. The controller, in turn, simply treats this estimation as the true value for control, and solves the control by implicitly making a full observability assumption. This assumption is sometimes known as the certainty equivalence heuristic [123, 128]. Fig. 4.5 summarizes the topology of the controller and filter when certainty equivalence is assumed. The certainty equivalence heuristic described above is also adopted in this work. Specifically, an EKF is implemented to observe the noisy image features and motor encoder readings, and to estimate the unknown (inverse) depths of the features. The observation equations can be described by, ok = where, H= I 0 I
(nq ×nq )

sk qk

+ wk = Hx + wk ,

(4.28)

(2nf ×2nf ) (nf ×nf )

.

(4.29)

Given the linear approximation of the dynamics of the system described by Eq. 3.80, and using the standard EKF equations derived in Appx. D , the state, including (inverse) depth estimates can be derived.  ^ sk ^  ^ k = d x k . ^k q 

(4.30)

This state estimation is updated iteratively, and fed to the controller. The reason that inverse depths were used instead of depths themselves is precisely because inverse-depth estimation with EKF is known to be more accurate than depth estimation [130­132]. The 69

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Robot

Camera

ok

uk

(Fully Observable) SOC Controller Based on Algorithm. 3.1

^k x

Extended Kalman Filter

Figure 4.6 ­ The control topology, as a result of the assumed certainty equivalence heuristic. An EKF is used as the state estimator, and the state estimation is fed to a fully observable SOC controller.

reason can be simply summarized by the fact that the inverse of depth may be better approximated by a Gaussian distribution than the depth itself. With the adoption of such estimation technique, the topology of the controller and estimator for our visual servoing application can be depicted as in Fig. 4.6.

4.4

Cost Function

The IBVS problem was brought into standard state space model as in Eq. 3.80. Therefore, given a cost function, Algorithm 3.1 can be applied to IBVS, with minimal modification. In fact, a simple quadratic cost function may be defined as,  

 (2×nf )by(2×nf ) c (xk , uk ) = (xk - xk )   0 
T T

Qk

0 0 (nf +nq )by(nf +nq )

   (xk - x ) + uT Rk uk , k k  

where, xk = skT 0T , is the desired state vector, which contains the desired feature vector, and Qk and Rk are positive semi-definite weighting matrices that can be defined by the control designer in order to achieve the desired behavior. The first term in the cost function above penalizes the deviation of the state from the desired state, xk , whereas the second term penalizes large control signals.

4.5

Scheduling for Real-time Implementation

Algorithm 3.1 presented in Chap. 3 for general SOC problems was essentially an offline algorithm. That is, in an initial phase, with hundreds of iterations, a locally optimal policy was constructed and then, in a second phase, the policy was evaluated and executed on the plant. For a partially observable case, however, an offline solution is not possible, because the estimation of the state of the system is constantly being updated, and these updates constantly change the initial point of the solution. 70

4.5. SCHEDULING FOR REAL-TIME IMPLEMENTATION

Computational Load

Policy Iteration Policy Iteration

Policy Iteration Policy Iteration

···

Evaluation Evaluation Evaluation Evaluation Policy Policy Policy Policy

···
time

Offline Policy Construction

Online Policy Execution

Policy Iteration

Policy Iteration

Policy Iteration

Policy Iteration

Policy Iteration

Policy Evaluation

Policy Evaluation

Policy Evaluation

Policy Evaluation

Policy Evaluation

··· ···
time

Online Policy Iteration Execution

Figure 4.7 ­ The scheduling of computations in the original offline algorithm for SOC vs. the modified online algorithm.

One solution is to solve many problems offline, each with a possible initial condition for the problem. However, the number of such offline problems grows exponentially w.r.t. the state dimension. Therefore, for a large state space problem such as visual servoing, an offline solution is not possible. Moreover, in a single frame, even with a relatively small control horizon, the number of required iterations for complete convergence to a locally optimal policy is too large. For online implementation of the proposed algorithm, one option is to spread out the policy updates in between, policy evaluations, in the execution phase. This idea comes from model predictive control methodology and is known as the receding horizon technique. In fact, as a future research direction, the authors of iLQG method [61] propose that a receding horizon technique together with an EKF be developed for application to partially observable problems. This is exactly what is pursued in the rest of this chapter, for application to visual servoing. With receding horizon technique, at each execution step, an optimal control problem is solved from scratch, and the first control signal of the resulting control sequence is applied to the plant. In the next execution step, the optimal control is solved from the top. Fig. 4.7 depicts the modified policy update and execution schedules for realizing an online version of Algorithm 3.1. As a comparison of the above method with MPC, it should be noted that in conventional 71

CHAPTER 4. APPLICATION TO VISUAL SERVOING

MPC, the optimal control problem solved in each iteration is an open-loop, deterministic problem. However, in the proposed method here, each backward policy update solves a locally optimal policy problem. Therefore, the proposed method here essentially constitutes a closed-loop MPC method, such as the one described in [133]. The close-loop MPC methods are known to be superior to the conventional open-loop MPCs, in terms of robustness [133]. Therefore, in principle, the proposed algorithm here is stronger than the existing optimal-control-based methods for visual servoing, which all based on conventional MPC [110, 111, 134]. Unfortunately this fact cannot be easily verified in practice, because existing works using conventional MPC for visual servoing are simply too computationally prohibiting to be implemented for comparison.

4.6

The Choice of k and k Parameters

Before Algorithm 3.1 can be applied to visual servoing, the choice of the parameters k and
-1 k should be clarified. For the application to visual servoing, these parameters are all chosen

to be zero. The reason is that, for the particular computational resources that is available, adding robustness to solution requires additional iterations to reach convergence. It will be explained that the maximum number that the policy is updated is equal to the horizon plus one. Therefore, In order to reach convergence for robust case, one needs to increase the control horizon. Increasing the horizon, however, also increases the computational load. For the time being, since real-time applicability is the goal, the choices of zero for these parameters is justified. However, if more computational power was available, one could also experiment with different risk-seeking or robust variations of the proposed algorithm.
-1 With this specific choice for the set of parameters k , k , from Chap. 3 it is known that,

Algorithm 3.1 results in a local solution to standard SOC, and the policy exactly matches the one given by iLQG. However, since iLQG has not ever been used for visual servoing either, the proposed algorithm for visual servoing, described in the following section, remains novel.

4.7

The Description of the Complete Algorithm

Each iteration of the proposed algorithm for visual servoing consists of 5 steps. The first step is to receive an image from the camera and search it for visual features (distinctive blubs, in our case). The centroids of the blubs are then extracted to serve as the set of 2D point features. Also, the robot joint encoders are read to find the current values of the robot joints. The set of features and the joint values are stacked into the observation vector, ok . The second step is to update the state estimation, by passing this new observation vector, ok , together with the previous joint velocity commands, uk-1 , to the EKF. The EKF finds ^ k , and passes it to the optimal the most likely approximation of the state of the system, x 72

4.7. THE DESCRIPTION OF THE COMPLETE ALGORITHM

^ k as the true value for xk . controller. The optimal controller treats x The optimal controller is based on Algorithm 3.1. Therefore, the third step is a forward simulation of the dynamics. Thus, after receiving the state, xk , the controller applies the most updated policy, µk , µk+1 , · · · , µk+N , to the linearized dynamics of the system, to
( i) (i) (i) (i) (i) ( i) ( i)

compute a predicted trajectory. In this step, the controller essentially computes what would happen if the current policies were to be applied to the system. The result of this step is a predicted control-state trajectory, {xk , uk , xk+1 , · · · xN } and the linearized dynamics around this trajectory. The fourth step is to update the policies. This is done in a backward sweep starting from the end of the horizon, µk+N , and applying C.25­C.30 and C.48­C.55 , to get the updated policy µk+N . This is done for all decisions, one by one, and backward in time. The result is the updated and improved policy, µk
(i+1) -1 k , k chosen to be zero, this backward sweep is very similar to a Riccati recursion, similar (i+1) (i)

, µk+1 , · · · , µk+N . It should be noted that with

(i+1)

(i+1)

to the one in a backward pass of iLQG [61]. The fifth and final step is to pop out the first decision, µk , and execute it by picking a (i+1) ^ k ), and applying it to the system. Algorithm 4.1 summarizes sample from uk  µk (uk | x the offline version in Algorithm 3.1, will be clear by revisiting the last step. In the offline algorithm, all µ0 through µN , were updated hundreds of times offline, and then this updated policies, µ0
(500) (i) ( i) (i+1)

the above in a pseudo-code. The difference between the online solution proposed here and

· · · µN

(500)

, was applied to the system. However, in the online version, at
(1) (2)

time step zero, after only one update, the policy ,µ0 , is applied to the system. At the next time step, the applied policies is µ1 , and so on. Since the control horizon is N, each decision can be updated at most (N + 1) times, before it's time to execute it. Therefore, in the online version, the policy applied to the system is µ0 , µ1 , · · · , µN
(1) (2) (N +1)

means, in order for the policies to be near optimal, the control horizon has to be chosen as large as possible. In the real-time implementation discussed in the next section, a control horizon of 25 steps was achieved. Finally, a block diagram illustrating the components of the proposed algorithm is given in Fig.4.8.

, µN +1 , µN +2 , · · · . This

(N +1)

(N +1)

73

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Algorithm 4.1 The pseudo-code of the proposed algorithm for visual servoing. 1: while !Terminated do 2: ok Extract the feature points and read joint encoders ^ k  EKF (ok , x ^ k-1 , uk-1 , k-1 ) 3: x # Update State Estimation. ^k 4: xk  x # Certainty Equivalence Heuristic. 5: for all k  {k · · · k + N }. # Forward Simulation. Compute a predicted trajectory. do (i) rand. (i) (i) ol(i) 6: uk  - - - µk (uk | xk ) = N uk | Kk xk + uk , µ(i) .
7: 8: 9:

10: 11:

and C.48­C.55 given, Ak (xk ) xk , Bk (xk ) , Qk , Rk , xk (i+1) (uk | xk ) uk  µ k k++ # Time Passes.

xk+1  Ak (xk ) xk + Bk (xk ) uk . for all k  {k + N · · · N } # Backward Pass. do (i+1) ol(i+1) Kk , uk , µ(i+1) | k  {0 · · · N } Update policies using Eqs. C.25­C.30
k

(i)

(i)

( i)

k

Target features

uk
Camera Feature Extraction

Delay

u k -1
EKF

sk

ok

SOC Controller

uk

Robot Controller

qk

Figure 4.8 ­ A block diagram depicting the proposed algorithm for visual servoing.

74

4.8. EXPERIMENTAL SETUP

4.8

Experimental Setup

For our experiments, a 5 DOF robotic manipulator (CRS-A255 [135]) is used. In order to verify the applicability of the control methodology in real-time, the CRS-A255 robot was controlled through Simulink, using the Matlab Real-time Workshop Toolbox and via a QuaRC Windows Target PCI board [136]. An industrial digital CMOS camera (FireFly MV [137]) with a resolution of 640 × 480 and frame rate of 60 Hz is attached to the end-effector of the robot. The first phase of preparing the setup is finding the parameters of the pinhole model, through camera calibration. The is done using the Matlab Camera Calibration Toolbox, which uses the method reported in [117]. The result of the camera calibration phase is a set of values for parameters of the pinhole model, listed in Table. 4.1. Of course, there is always some image distortion, depending on the type of the lens, which does not allow us to use a simple pinhole model, without modeling the distortion. Therefore, a distortion mapping model is also given by the calibration procedure. Using the inverse of this distortion model, it is possible to correct the effect of distortion. After distortion is removed, the pinhole model assumption can be safely assumed. Fig. 4.9 shows some images obtained from the camera, before and after applying the inverse distortion mapping. The second phase is deriving the robot Jacobian. This step is a basic procedure in robotics, for which, one needs to form the table of Denavit­Hartenberg (DH) parameters (Table. 4.2). This table is based on the nominal values available in the manual of the robot [135], and may be slightly inaccurate. However, for our experiments, these nominal values were used. Having this information, it is possible to find the robot Jacobian using basic results on differential kinematics, which can be found in all robotics textbooks [114]. To establish ground truth for poses, a high accuracy optical tracking system (NDI Optotrack Certus[138]) is employed. The optical tracker can track the 3D position of LED markers with an accuracy of 0.1 mm. Four markers are placed on the target object such that they are visible by both camera and the optical tracking device. A coordinate frame is defined with respect to these LED markers, and the pose of target is tracked accurately using the tracking device. The same makers are also detected as blubs in images obtained from the camera, and used in our visual servoing algorithm as 2D visual features. Therefore, the accuracy of depth estimation can be verified using data from optical tracker. Also, three LED markers are placed on the end-effector and a coordinate frame is defined with respect Parameter
CU CU

fc

Description The x coordinate of the center of the image plane. The y coordinate of the center of the image plane. Focal length of the camera.

Value [px] 323.9 235.5 613.3

Table 4.1 ­ Values of camera parameters resulted from the calibration phase.

75

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Figure 4.9 ­ Applying the inverse of distortion mapping. The first and second row illustrate the images obtained from the camera, before and after distortion removal, respectively.

Joint 1 2 3 4 5

Type r r r r r

 0 0 0 0 0

d 0.254 0 0 0 0.12

r 0 0.254 0.254 0 0


/2

0 0 0 0

 0 0 0 0 0

Table 4.2 ­ Table of DH parameters for the A255 robot.

76

4.8. EXPERIMENTAL SETUP

Optical tracker

Optical makers

Camera Robot
ot CRS Rob

Object

Figure 4.10 ­ Experimental setup. The LED markers are tracked simultaneously by the camera and the optical tracker.

to these markers. Fig. 4.10 demonstrates the experimental setup. Each experiment follows the steps below: 1. The robot is moved to a desired pose with respect to the object. 2. The relative pose of the object with respect to the end-effector is recorded using the optical tracker. 3. A single image is recorded which constitutes our target image. 4. The LED markers are located in the image using simple blub detection, and their position in the image frame is saved as target feature vector. 5. The robot is moved to an initial pose with respect to the object. 6. The LED markers are detected in the current image and saved as initial feature vector. 7. The control sequence is started, while the optical tracker is recording the data. The controller tries to drive the robot towards the target pose. 8. After a final pose is reached, the control sequence and the optical tracker are stopped. In existing works which apply optimal control (mostly, MPC methodology) to visual servoing, it is claimed that compared with standard IBVS, shorter workspace trajectories can be 77

CHAPTER 4. APPLICATION TO VISUAL SERVOING

achieved. It's also known that myopic behaviors such as the retreat of the camera in face of large rotations, and getting trapped in local minima can be avoided. For our experiments appropriate scenarios are devised to evaluate the following, · The ability to dealing with the retreat problem. · The ability to choosing shorter workspace paths. The reason local minima are left out of experiments is that it turns out to be very difficult to artificially create a case of local minima with our setup, since the CRS-255 robot cannot move freely and with 6DOF. In order to evaluate the ability to deal with the retreat problem, a scenario with a large rotation needs to be considered. Specifically, Scenario I: A pure 90 rotation around the axis normal to the image. For this case, it is expected that standard IBVS will result in large and unnecessary backward movement of the camera, which can cause the robot to reach its joint limits. On the other hand, the proposed algorithm is expected to show minimal retreat. To evaluate the quality of the workspace paths, two scenarios with moderate rotation and translation are considered. Specifically, Scenario II: A combined large translation towards the object and a 30 rotation. and Scenario III: A combined large translation away from the object and 60 rotation. For these two scenarios, it is expected that the proposed method will result in shorter workspace trajectories (and, consequently shorter time to complete the task). Finally, for in the accuracy of both approaches are compared. These three scenarios are illustrated in Fig. 4.11.

78

4.8. EXPERIMENTAL SETUP

a)

b)

c)

d)

Figure 4.11 ­ Scenarios. Each row displays the robot pose and the corresponding image seen through the camera. a) The initial pose of the robot for all scenarios. b) The target pose for scenario I. c) The target pose for scenario II. d) The target pose for scenario III.

79

CHAPTER 4. APPLICATION TO VISUAL SERVOING

4.9

Experimental Results and Discussion

The performance of the proposed algorithm is compared with the standard IBVS algorithm for three scenarios. In each case, the qualitative behavior of the control as well as the orientation and translational error of the final pose and the lengths of work-space trajectories are compared. For all of these experiments, the following parameters for the two control methodologies were used. For IBVS there is only one parameters to set,  = 0.1. For the proposed algorithm, it was already decided to use, (4.31)

k

=

0,

for all k, for all k.

(4.32) (4.33)

-1 k  0,

For the choice of weighting matrices in the cost function , the following straightforward choices were made,

Qk = I, Rk = 10
-6

for all k, I, for all k.

(4.34) (4.35)

If a larger Rk was used, the input commands would be more penalized, which would result in smaller control signals. The value of 10-6 I was chosen such that the control signal (which are the robot joint velocities) fall within the operation range of robot. The control horizon for the proposed method was set to 25. This is the maximum control horizon which was achievable in 60Hz.

4.9.1

Scenario I: Large Rotations and the Retreat Problem

IBVS is known to have difficulties dealing with large rotations [120]. In our experiments, it was found that even 180 rotations can be handled easily. However, since IBVS fails completely for such large rotations, for the purposes of comparison a simpler case of 90 rotation is considered. The steps described in the previous section are completed for the scenario I, for both control methodologies, and the results are collected. Since this is a pure rotation case, there shouldn't be any translational motion. Fig. 4.12 illustrates the motion of the robot for both control methodologies. These figures are obtained by recording the joint values of the robot during the experiment, operation, and plotting the robot using the Matlab Robotics Toolbox [139]. It is observed that the standard IBVS 80

4.9. EXPERIMENTAL RESULTS

0.7

0.7

0.6 Retreat 13 cm 0.5

0.6

0.5

Retreat 1 cm

0.4

0.4

0.3

0.3

Z

0.2

Z 0 0.1 X 0.2 0.3

0.2

0.1

0.1

0

0

-0.1 -0.1

-0.1 -0.1

0

0.1 X

0.2

0.3

(a) The retreat for standard IBVS.

(b) The retreat for the proposed method.

Figure 4.12 ­ The motion of the robot for scenario I. It is observed that the retreat is significant for IBVS.

shows a large translational motion, away from the target object. Fig. 4.13 illustrates the trace of features in the sequence of images, during the application of both control methodologies. It can be seen that for the IBVS, the features try to move on a straight line in the image plane. However, this requires that the robot makes a large backward translation. This retreat of the camera usually results in joint limit violations, as it does in this experiment. Since one of the joints of the robots cannot move passed the joint limit, the features cannot keep following their straight path. At this point, any thing is possible. In most cases, one or many features leave the field of view, rendering the control incomplete. For the proposed method, on the other hand, it is seen that the image plane trajectories resemble a perfect circular motion. A comparison of the work-space trajectories presented in Table. 4.3, also shows that the trajectory length for IBVS is very large, due to very high retreat of the camera (287.0 mm compare to 37.7 mm with the proposed method). Fig. 4.14 illustrates the regulation of image feature errors to zero, by the two methodologies. Both methods have show good accuracy, regulating the features to sub-pixel levels. The orientation and translational errors of the two methods are compared in Table. 4.3, and the proposed method shows superior accuracy. Fig. 4.15 shows the robot joint velocities for both control methodologies. For the case 81

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Continues going straight

Trying to follow the feature on a straight line

The features don't follow a straight line.

Reaches joint limit

(a) The result for standard IBVS.

(b) The result for the proposed method.

Figure 4.13 ­ The image space trajectories of features for scenario I.

300
 v4 - v4

250 200 150 100 Pixel Errors [px] 50 0 -50 -100 -150
 v4 - v4  v3 - v3  v2 - v2  v1 - v1 u4 - u 4 u3 - u 3 u2 - u 2 u1 - u 1

200

 v3 - v3  v2 - v2  v1 - v1

100 Pixel Errors [px]

u4 - u 4 u3 - u 3 u2 - u 2 u1 - u 1

0

-100

-200 -200 -300 0 500 1000 Frames 1500 2000 -250 0 200 400 600 800 1000 Frames 1200 1400 1600

(a) The results for IBVS.

(b) The result for the proposed algorithm.

Figure 4.14 ­ The visual feature errors for scenario I.

82

4.9. EXPERIMENTAL RESULTS

0.15 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5 Joint Velocity Commands [rad/sec]

0.16 0.14 0.12 0.1 0.08 0.06 0.04 0.02 0 -0.02 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5

0.1

Joint Velocity Commands [rad/sec]

0.05

0

-0.05

The command is almost purely a rotation of joint 5.

-0.1

-0.15

These sharp velocities are due to joint 4 reaching its limit, as a result of large retreat.

-0.2

0

200

400

600

800 Frame

1000

1200

1400

1600

0

200

400

600

800 Frame

1000

1200

1400

1600

(a) The result for IBVS.

(b) The result for the proposed method.

Figure 4.15 ­ The joint velocity of the robots for scenario I.

of IBVS, because of the large retreat, the robot reaches the buffer zone for joint limits. Specifically, joint 4 of the robot is pushed back into the buffer zone. In order to prevent damages to the robot, when the robot is reaches the buffer zone, a safety mechanism is activated and the violating joint is quickly pushed back into the permitted region. Therefore, the large and short-lived velocities observed in 4.15a are the result of violations of joint limits, due to large retreat. It should be noted that without this security measure, IBVS would fail as soon as the robot reached the joint limits. Thus, the ability of the proposed method for dealing with large rotations, without the retreat problem was evaluated and verified.

4.9.2

Scenarios II and III: Shorter Work-Space Trajectories

For scenario II and III, the goal is to show that the resulting work-space trajectories are shorter for the proposed method compare to standard IBVS. The trajectory lengths are computed by integrating the changes of the origin of the camera frame, as read from the optical tracker data. The results are summarized in Table. 4.3. According to these results, the work-space trajectory lengths for the proposed method is shorter than those of IBVS, for all scenarios. However, for cases where the rotations are larger, the trajectory length of the proposed method becomes relatively shorter. Therefore, the most improvement is seen in scenario I. Figs. 4.16­4.21 demonstrate the image-plane trajectories, image feature errors and robot joint velocities for both control methodologies, for scenarios II and III. It is perhaps noteworthy to say point out that in Scenario III, for the proposed algorithm, one of the features leaves the field of view momentarily. This can be observed in Fig. 4.19b. Fortunately, since the EKF is implemented such that when a feature is lost, it is considered as a measurement 83

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Scenario I

Method Standard IBVS Proposed Algorithm Standard IBVS Proposed Algorithm Standard IBVS Proposed Algorithm

Orientation Error [deg] 2.05 0.42 6.5 3.5 4.9 0.46

Translational Error [mm] 3.4 2.8 24.1 17.8 24.8 7.8

Trajectory Length [mm] 287.0 37.7 115.4 90.2 122.2 101.3

II

III

Table 4.3 ­ The comparison of translational, orientation error, and the work-space trajectory length for the three scenarios.

(a) The result for the IBVS.

(b) The result for the proposed method.

Figure 4.16 ­ The image space trajectories of features for scenario I.

with a variance of infinite. This technique forces EKF to output it's predicted whenever a feature is lost. Although this technique seems straightforward, and works very well in practice too, the author does not know of any works which explicitly discuss its possibility for dealing with temporary loss of features.

84

4.9. EXPERIMENTAL RESULTS

150

 v4 - v4  v3 - v3  v2 - v2

150

 v4 - v4  v3 - v3  v2 - v2

100

 v1 - v1

100

 v1 - v1

u4 - u 4 50 Pixel Errors [px] Pixel Errors [px] u3 - u 3 u2 - u 2 u1 - u 1 0 50

u4 - u 4 u3 - u 3 u2 - u 2 u1 - u 1 0

-50

-50

-100 0 200 400 Frames 600 800 1000

-100 0 500 1000 1500 2000 2500 Frames 3000 3500 4000

(a) The result for IBVS.

(b) The result for the proposed algorithm.

Figure 4.17 ­ The visual feature errors for scenario II.

0.15 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5 Joint Velocity Commands [rad/sec]

0.15 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5

0.1

0.1

Joint Velocity Commands [rad/sec]

0.05

0.05

0

0

-0.05

-0.05

-0.1

-0.1

-0.15

-0.15

-0.2

0

200

400 Frame

600

800

1000

-0.2

0

500

1000

1500

2000 2500 Frame

3000

3500

4000

4500

(a) The results for IBVS.

(b) The results for the proposed algorithm.

Figure 4.18 ­ The joint velocity of the robots for scenario II.

85

CHAPTER 4. APPLICATION TO VISUAL SERVOING

Temporary loss of feature and graceful recovery

(a) The result for the IBVS.

(b) The result for the proposed method.

Figure 4.19 ­ The image space trajectories of features for scenario III.

150
 v4 - v4

150

 v3 - v3  v2 - v2

100

100

 v1 - v1

u3 - u 3 50 Pixel Errors [px] u2 - u 2 u1 - u 1 0

Pixel Errors [px]

u4 - u 4

50

0

-50
-50

-100
-100

-150 0 200 400 600 Frames 800 1000 1200

-150 0 500 1000 Frames 1500 2000 2500

(a) The result for IBVS.

(b) The result for the proposed algorithm.

Figure 4.20 ­ The visual feature errors for scenario III.

86

4.10. SUMMARY

0.15 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5 Joint Velocity Commands [rad/sec]

0.15 Velocity of Joint#1 Velocity of Joint#2 Velocity of Joint#3 Velocity of Joint#4 Velocity of Joint#5

0.1

0.1

Joint Velocity Commands [rad/sec]

0.05

0.05

0

0

-0.05

-0.05

-0.1

-0.1

-0.15

-0.15

-0.2

0

200

400 Frame

600

800

1000

-0.2

0

500

1000

1500

2000 2500 Frame

3000

3500

4000

4500

(a) The results for IBVS.

(b) The results for the proposed algorithm.

Figure 4.21 ­ The joint velocity of the robots for scenario III.

4.10

Summary

In this chapter, the approximate SOC algorithm developed in Chap. 3 was adapted to the problem of IBVS. The result was a powerful optimal controller for visual servoing, with modest computational load. Therefore, the algorithm was implemented in real-time, with a relatively fast frame rate of 60Hz. It was shown, through experiments and comparison to standard IBVS, that the resulting algorithm can deal with many classical problems and myopic behavior of standard IBVS, and also, result in higher control accuracy.

87

Chapter

5

Conclusion
5.1 Summary

The main focus of this thesis was to provide a tractable and efficient solution to SOC. The exact answer given by DP was shown to be too hard to solve for problems of interest, due to curse of dimensionality. Also, issues regarding the robustness of the optimal policy were raised. It was suggested that a promising path towards finding a tractable solution to SOC is through casting it as an inference problem, because problems in inference also suffer from curse of dimensionality, and in the context of inference, there exists a rich literature on how to deal with these computational complexities. Therefore, a more general version of SOC was defined, which included robust, and riskseeking SOC, and the problem was stated as an inference problem and a novel and unified solution was derived. Although this solution also suffered from curse of dimensionality, it was now possible to use approximate methods from inference. Specifically, EKF-type approximations were used and the result was an algorithm which is able to solve robust, standard, and risk-seeking SOC problems, depending on the choice of a set of parameters. This general inference based solution For a specific choice of parameters of the algorithm, the method was shown to reduce to a recent approximation to standard SOC, namely, to iLQG, and for another special case, it reduces to a recent approximate inference-based solution, with risk-seeking behavior [16]. Therefore, the proposed algorithm generalizes over these recent works, while also able to produce robust policies, therefore, completing the spectrum. Moreover, it allows for policies with time-varying behavior, which don't fit in any category of the existing methods. For a benchmark problem, a spectrum of policies with different degrees of robustness and risk-seeking were derived. It was shown that the standard optimal policy resulted from our approach exactly match those produced by iLQG, except, in less computation time. It was also shown through simulations that compared to standard optimal policies, robust policies can reduce the cost variance significantly, while the risk-seeking policies increase it. This 88

5.2. LIMITATIONS OF THE PROPOSED METHODOLOGY

means that in face of model uncertainties, the robust policies are able to accomplish the task more often than the optimal and risk-seeking policies. For an empirical application, the developed algorithm was applied to IBVS, and the results were compared against standard IBVS methods. The computational efficiency of the proposed algorithm allows it to look ahead for a long horizon and devise an optimal plan, instead of the myopic one-step ahead methods. Also, while the existing OC-based algorithms for IBVS, cannot be used in real-time, due to their use of general nonlinear programming solvers in the loop, our efficient solution can reach long control horizons in real-time. It was shown that the proposed approach results in higher accuracy of control, shorter work-space trajectories and smaller completion time, as well as escaping local minima, and overcoming the infinite retraction problem in IBVS.

5.2

Limitations of the Proposed Methodology

The approximation based on EKF uses linearized dynamics around a predicted state-control trajectory. Therefore, it is valid in a small region around this trajectory. In fact, this is precisely why it's not subject to curse of dimensionality. However, if the uncertainties or parameter perturbations are large enough to cause the system to deviate from this trajectory, the approximate policies begin to lose their optimality. This first limitation is, indeed, inherited from EKF, and they can be lessened using more advanced types of approximations. A discussion on this will be given in the next section, under future directions. A second limitation of the proposed approach is that the state and control constraints are not explicitly handled. Instead they need to be implemented as cost functions, using penalty functions, such as log-barrier penalties. However, if the feasible region of state or control space are small, using penalty functions may increase computation time. A solution for handling constraints is by using simple line-searching and backtracking techniques, also used in iLQG. A third limitation of the propose methodology is computation time. Though much improved than previous works, it may still be too much for more complicated problems in robotics, specially those which require a long horizon to accomplish a task. In these cases, a high-level planner might be needed, or the computation time needs to be further reduced. Some ideas in this direction will be explored in the next section. A fourth drawback of the proposed method, specifically for application in visual servoing is that, it's a lot more complicated than basic approaches in visual servoing. This is partially due to the fact that the visual servoing scenario considered here was a simple one, which could be handled with classical methods. Therefore, the proposed method could be potentially more useful when faced with more complex problem. Finally, the proposed assumptions for achieving robust control suffer from the fact that while different degrees of robustness can be achieved, it is not exactly known what ranges 89

CHAPTER 5. CONCLUSION

of each parameter of the model is covered. This is why considering robustness did not inflict additional computational burden. If a separate (ambiguity) range for each individual parameter was defined, the dimensions of the problem would grow by the number of uncertain parameters. Therefore, the proposed method trades computational complexity with how much control it is possible to have over what ranges of each parameter to cover.

5.3

Future Directions

There are a number of possibilities for extending the proposed solution to SOC, as well as potential applications for it. In this section, these future directions are explored.

5.3.1

Other Types of Approximations

The general proximal point solution given in Chap. 3 was approximated using EKF-type assumptions, because of the simplicity and computational efficiency of EKF. However, numerous other approximation methods are possible, given more computational resources. For instance, approximations based on unscented Kalman filtering can be easily adapted to our work. Particle filters are also another possibility. The advantages of unscented or particle filtering over EKF also extend to the proposed method here. For instance, EKF is valid locally, in the vicinity of the trajectory around which the dynamics was linearized. Unscented, or particle filtering can result in solutions which are valid in a larger region. Also, EKF assumes Gaussian noises. Should the uncertainty be hard to approximate using Gaussian, approximations based on unscented or particle filtering can improve the accuracy of the solution. Furthermore, both unscented filtering and particle filtering are more suited for handling state or control constraints. This direction is perhaps the most direct extension of the current work.

5.3.2

Reinforcement Learning

Our solution in Chap. 3 can easily be adapted for RL. Problems discussed under RL are similar to SOC, in that, they involve a sequential decision making problem, in which, an agent needs to come up with a policy to optimize a cost or utility. The difference is that, while in SOC it was assumed that a nominal model of the plant (dynamics) and a cost function are available, in RL, it is assumed that only samples of the dynamics and cost function are available to the agent. The proposed algorithm can be used in the context of RL, by evaluating it at samples, and updating a functional approximation of the policy (such as a linear basis function model). This approach is similar to the one followed in [56] and [56]. For instance, a can be used to approximate the policy function, similar to [56]. The result will be a novel RL solution which has the added flexibility of being tunable for different levels of risk-seeking vs. robustness behavior. 90

5.3. FUTURE DIRECTIONS

Also, in the context of RL, risk-seeking behavior could be more useful, as it encourages learning. Therefore, a higher-level logic can be used to tune conservativeness or riskiness of the agent's behavior. Convergence speed of this proposed method is expected to be faster than existing approaches, based on the similarity of the proposed approach to other recent inference-based methods in RL, such as  -learning described in [16]. This direction is, perhaps, the most promising direction for future research.

5.3.3

Formal Proof of Global Convergence

Deriving a formal proof of the global convergence of our general proximal point solution was not pursued in this thesis, because ultimately, by making an EKF-type assumption, any guarantee of global convergence is lost. However, while for the purposes of this thesis developing a formal proof of convergence was not necessary or helpful, a future work in the context of RL can greatly benefit from it. In the context of RL, such a proof is necessary, because a lot of works in RL already enjoy such formal guarantees of convergence. Fortunately, a proof can be developed by extending the results in [16]. The reason is that our solution reduces exactly to the solution in [16], for a specific choice of the parameters (when k =  > 0, for all k , which is the risk-seeking case). Therefore, for the risk-seeking case, the same proof as in [16] can be used. In the case of robust SOC, that is, (when k < 0 ), the same proof can be extended by making some assumption about the admissible range of k (i.e. from zero up to the breaking point). Another possibility for deriving convergence results in by using the connections of the presented work with minimax stochastic games with relative entropy bounds and use the monotonicity properties and other useful results given in [103, 140]. This may provide a more rigorous proof than the somewhat unconventional proof given in [16]. However, using these monotonicity results may require a more rigorous reformulation of the problem within the measure theory framework, and therefore, may require some extra work. However, from a theoretical point of view, this can be a good direction with great potentials.

5.3.4

A Distributed Implementation

Similar to approximate methods in inference, a distributed implementation of our algorithm is possible. Specifically, since an asynchronous update mechanism is allowed, all messages can be computed in parallel. This can result in a much faster version of the proposed algorithm, which can be used for longer control horizons in real-time. Such an algorithm can be used for more complicated tasks which require longer plans to accomplish the task. A distributed implementation can be based on distributed and parallel implementations of message-passing algorithms for inference [141, 142]. 91

CHAPTER 5. CONCLUSION

5.3.5

Application to Advanced Robotics Problem

Since the proposed approach constitutes one of the most powerful approaches for handling nonlinear and uncertain dynamics, in large state and control spaces, it can potentially be used for hard robotics problem. For instance, motor control of humanoids may be a good area to try the propose algorithm, because of the large number of DOF of the robot, and also because of the large, continuous state space.

92

Appendix

A

Entropy
A.1 Kullback­Leibler Divergence ­ or Relative Entropy

In probabiliy theory, Kullback­Leibler divergence also known as relative entropy is an asymmetric measure of difference between two probability distributions (or more generally, two measures on a probability space). It frequently appears in various fields such as statistical physics, information theory and variational approaches to inference [48]. Definition 1. Kullback­Leibler Divergence of probability distribution q (x) from p (x) is defined as: ^ KL (p (x) q (x)) = p (x) log p ( x) dx. q (x) (A.1)

In general, KL (p q ) is not equal to KL (q p). Also, KL-Divergence is always non-negative, with zero occuring iff q (x) = p (x). If p (x) and q (x) are unnormalized distributions, we have the more general definitions: ^ KL (p (x) q (x)) = p (x) log p ( x) dx - q (x) ^ [q (x) - p (x)] dx. (A.2)

Definition 2. The Conditional Kullback­Leibler Divergence (or conditional relative entropy) of q (x | y ) from p (x | y ) is defined as: ^ ^ KL (p (x | y ) q (x | y )) = p (x | y ) p (x, y ) log dxdy = q (x | y ) ^ p (y ) ^ p (x | y ) log p (x | y ) dxdy. q (x | y ) (A.3)

While Definition 2 is for a bivariate case, generalizations to multivariate distributions are simple to imagine.

93

Appendix

B

Multivariate Gaussian Functions and Distributions
We use two notations for describing normal distributions. We use two formats for representing Gaussians: A normal form, represented by parantheses and a canonical form, represented by square brackets, defined as follows,

^ , ) N (x | x  , P] N [x | x

= =

(2 ) || (2 )
d

d

-1 2

1 ^ )T -1 (x - x ^) , exp - (x - x 2
1

(B.1) (B.2)

1 T -1  P x  exp - 2 x

|P-1 | 2

1  , exp - xT Px + xT x 2

where d is the dimension of x,  is the covariance matrix and P is called the precision matrix. We can convert one form to the other using,

^ , ) N (x | x  , P] N [x | x

=

^ , - 1 , N x | - 1 x
-1

(B.3) (B.4)

= N x|P

, P x

-1

,

The above definitions are well-defined if the covariance (or precision) matrix has full rank. Degenerate Gaussians are Gaussians with a singular covariance (or precision) matrix We can extend the above definitions for degenerate Gaussians as follows:

^ , - N x|x x  , P- N x|x

= =

(2 ) 1 2 · · · r
r

d

-1 2

1 -1 -1 (2 ) - 1 2 · · · r

1 ^ )  ^) , exp - (x - x x (x - x 2 1 1 -2  ) P (x - x ) , exp - (x - x 2

(B.5) (B.6)

-1 -1 -1 - where 1 2 · · · r are eigenvalues of - x , and 1 2 · · · r are eigenvalues of P . The superscript '' denotes the Moore­Penrose pseudoinverse. In the following, whenever we have singular covariance (or precision) matrices we use generalized pseudoinverse and the product of eigenvalues instead of

94

inverse and determinant, respectively. For the product of Gaussians we have,

 1 , P1 ] N [x | x  2 , P2 ] = N [x | x 1 + x  2 , P1 + P2 ] N N [x | x
1 1 P- 1 x

(B.7) +
1 P- 2

|

1 1  2 , P- P- 2 x 1

,

(B.8) (B.9) (B.10) (B.11)

1 1 1 -1 ^ 1 + - ^ 2 , - ^ 2 , 1 +  2 ) , N x | - N (^ x1 | x 1 x 2 x 1 + 2 1 ^1 - 1 x

^ 1 , 1 ) N (x | x ^ 2 , 2 ) = N (x | x ^ 1 , 1 ) N [x | x  2 , P2 ] = N (x | x +
1  2 , - x 1

N x|

^1 | + P2 N x

1  2 , 1 P- 2 x

+

1 P- 2

.

(B.12)

Linear transformation of Gaussians,

N (Ax + b | a, )

= |A | = |A|

-1

-1

N x | AT -1 (a - b) , AT -1 A . N x | AT (a - Pb) , AT PA .

N x | A-1 (a - b) , A-1 A-T

(B.13) (B.14)

N [Ax + b | a, P] = |A|

-1

(B.15)

For conditionals and marginalization, we have,

p (x1 , x2 )  p (x1 | x2 ) and p (x1 )

= = =

N

x1 x ^1 11 | , x2 x ^2 21

12 22

,

(B.16) (B.17) (B.18)

1 1 N x1 | x ^1 + 12 - ^2 ) , 11 - 12 - 22 (x2 - x 22 21 ,

N (x1 | x ^1 , 11 ) .

And for cononical form,

p (x1 , x2 )  p (x1 | x2 ) and p (x1 )

= = =

N

1 x1 x P11 | , 2 x2 x P21

P12 P22

,

(B.19) (B.20) (B.21)

 1 - P12 x2 , P11 ] , N [x 1 | x

1 1  1 - P12 P-  2 , P11 - P12 P- N x1 | x 22 x 22 P21 .

95

Appendix

C
ck (xk , uk ) = - 1 T 1 xk Qk xk + uT Rk uk + xT k Cxu,k uk 2 2 k T   k . xT kx k + uk u

Deriving Messages
We start by defining the quadratic (or quadratized) cost function as,

(C.1)

Therefore, the reward variables at temperature  are given by, Pr (Rk = 1 | Xk = xk , Uk = uk )  N xk uk |  -1 k x u k ,  -1 Qk CT xu,k Cxu,k Rk . (C.2)

Stacking xk and uk into a vector and calling it zk gives more compact equations. Thus,
-1 Pr (Rk = 1 | Xk = xk , Uk = uk )  N zk |  -1 z k, 

Qk CT xu,k

Cxu,k Rk

(C.3)

In the case of linearized dynamics we have, fk (xk+1 | xk , uk ) = N (xk+1 | Ak xk + Bk uk + ak , w ) . where, (C.4)

Ak Bk ak

= = =

 fk (xk , uk , wk )  xk  fk (xk , uk , wk )  uk

,
xk =^ xk ,uk =^ uk ,wk =0

(C.5) (C.6) (C.7)

,
xk =^ xk ,uk =^ uk ,wk =0

^ k , 0) - (Ak x ^ k + Bk u ^k) . fk (^ xk , u

Again, in terms of zk one has,

96

fk (xk+1 | xk , uk )

= N

xk+1 |

Ak Fk

Bk Bk Fk

xk uk

+ ak , w

(C.8) (C.9) (C.10)

= N xk+1 |

zk + ak , w Bk zk + Pw ak , Pw .

= N xk+1 | Pw The update equation for the dynamics is,
(i+1) fk

=

fk Tk mfk µk+1 Tk mµk fk
(i)

(i)

.

(C.11)

The message from µk+1 to fk is,

mfk µk+1

(i)

= N xk+1 | x ^fk µk+1 , fk µk+1 = N xk+1 | x fk µk+1 , Pfk µk+1 =
-1 T Zµk+1 k+1

(C.12) (C.13) (C.14)

(k+1 )

and the message from fk to µk is,

mµk fk

( i)

= = =

N (zk | ^ zµk fk , µk fk ) N [zk |  zµk fk , Pµk fk ]
1 T- Zfk (k ) . k

(C.15)

(C.16)

The message from rk to µk is,

i) m( µk rk

1 - = T- e k

ck 

(C.17) Qk CT xu,k Cxu,k Rk . (C.18)

-1 = N zk |  -1 z k, 

Starting from the numerator,

fk Tk mfk µk+1 = N xk+1 | Pw = N xk+1 | Pw ×N Ak Ak Bk Bk

(i)

Ak

Bk

zk + Pw ak , Pw (C.19)

-1 -1 ×N xk+1 | k x fk µk+1 , k Pfk µk+1 -1 -1 zk + Pw ak + k x fk µk+1 , Pw + k Pfk µk+1

zk | x ^fk µk+1 - ak , w + k fk µk+1 .

(C.20)

For the parition function to be well-defined and finite, the following should hold. This may or may not limit the admissible range of, k , when it's negative.

97

APPENDIX C. DERIVING MESSAGES

-1 P w + k Pfk µk+1

0,

(C.21)

and, w + k fk µk+1 For the denominator we have, 0. (C.22)

Tk mµk fk = N  N zk | , AT k BT k

(i)

Ak AT k BT k

Bk

zk | x ^fk µk+1 - ak , w + k fk µk+1
-1

(C.23)

w + k fk µk+1 w + k fk µk+1
-1

x ^fk µk+1 - ak Bk (C.24)

Ak

Therefore,

mµk fk = N zk |  zµk fk , where,

(i)

Pxx,µk fk Pux,µk fk

Pxu,µk fk Puu,µk fk

,

(C.25)

Pxx,µk fk Pxu,µk fk Pux,µk fk Puu,µk fk  zµk fk

-1 = AT k k w + fk µk+1

-1 -1 -1 -1

Ak , Bk , Ak , Bk , x ^fk µk+1 - ak x ^fk µk+1 - ak .

(C.26) (C.27) (C.28) (C.29) (C.30)

=

AT k BT k AT k BT k

= BT k = =

-1 k w -1 k w -1 k w

+ fk µk+1 + fk µk+1 + fk µk+1

-1 k w -1 k w

+ fk µk+1

+ fk µk+1

-1 -1

The update equation for policy is,

µk

(i+1)

=

µk Tk mµk fk Tk mµk rk Tk mfk-1 µk
(i)

(i)

(i)

(i)

.

Suppose that from the last update of the policy we have, µk = N uk | Kk xk + uk
(i) (i) (i) ol(i)

, µ(i) .
k

(C.31)

Before deriving the numinator, let us rearrange µk in terms of zk ,

98

N uk | Kk xk + uk

(i)

ol(i)

, µ(i) = N
k

-Kk -Kk

(i)

I zk | uk

ol(i)

, µ(i)
k

(C.32)
µk

=N

(i)

I zk | -(1 i) uk
µk T

ol(i)

, -(1 i)
i) , -K( k T

(C.33) I
(i) -(1 i) -Kk µk

(i)  N zk | -Kk

I

-(1 i) uk
µk

ol(i)

I (C.34)  (C.35)  .

  (i)T  (i) (i)T ol(i) Kk -(1 -Kk -(1 i) Kk i) uk µk µk     , = N zk | (i) ol(i) --(1 I T -(1 i) Kk i) uk 
µk µk

(i)T -Kk -(1 i) µ

-(1 i)
µk

k

Now, starting from the numinator,

i) µk Tk mµk fk Tk m( µk rk

(i)

(i)

= µk e

(i) -

ck -k log Zf (k ) k k

,

(C.36)

we have

i) µk Tk mµk fk Tk m( µk rk

(i)

(i)

= N uk | Kk xk + uk
(C.37.I ) -1  × N zk | k zµk fk ,

(i)

ol(i)

, µ(i)
k

-1 k Pxx,µk fk -1 k Pux,µk fk (C.37.II )

-1 k Pxu,µk fk -1 k Puu,µk fk

-1  × N zk | k zµk fk ,

-1 k Pxx,µk fk -1 k Pux,µk fk (C.37.III )

-1 k Pxu,µk fk -1 k Puu,µk fk

.

(C.37)

First, we multiply the two first of the product above,

(C.37.I ) × (C.37.II ) =

   (i)T (i)T ol(i) (i) -Kk -(1 Kk -(1 i) uk i) Kk µk µk     , × N zk | ol(i) (i) I T -(1 --(1 i) uk i) Kk
µk µk -1  × N zk | k zµk fk , -1 k Pxx,µk fk -1 k Pux,µk fk



-Kk

(i)T

-(1 i)
µk

-(1 i)
µk

 

(C.38) (C.39)

-1 k Pxu,µk fk -1 k Puu,µk fk

.

(C.40)

Therefore, we have,

99

APPENDIX C. DERIVING MESSAGES

(C.37.I ) × (C.37.II )    (i)T ol(i) -Kk -(1 i) uk µ k  + -1 N zk |  ol(i) k zµk fk , I T -(1 i) uk µk  (i)T   (i) (i)T -1 -1 Kk -(1 -Kk -(1 i) Kk i) k Pxx,µk fk k Pxu,µk fk µk µk   , + . (i) -1 -1 k Pux,µk fk k Puu,µk fk --(1 -(1 i) Kk i) 
µk µk

(C.41) (C.42)

(C.43)

Continuing with the third term in the product,

-1  -1 i) µk Tk mµk fk Tk m( zk , k µk rk  (C.37.I ) × (C.37.II ) × N zk | k 

(i)

(i)

Qk Cux,k

Cxu,k Rk

, (C.44)

which after some simlifications becomes,

-1 k Pxx,µk fk -1 k Pux,µk fk

(i) µk Tk mµk fk Tk mµ = k rk    (i)T ol(i) -Kk -(1 i) uk µk -1     + -1 N zk | zk , ol(i) k zµk fk + k  -(1 i) uk µk   (i)T (i) (i)T -1 -1 -Kk -(1 Kk -(1 i) i) Kk k Qk k Pxu,µk fk µk  µk  + + ( i ) -1 -1 -1 -1 k Cux,k k Puu,µk fk  (i) - (i) Kk µk µk

( i)

(i)

(C.45) (C.46)
-1 k Cxu,k -1 k Rk



(C.47)

.

Therefore,

Pfk-1 µk =

(i+1)

Qk + k Kk

(i)T

-(1 + Pxx,µk fk i) Kk
µk (i)T

(i)

+ Cxu,k + Pxu,µk fk - k Kk

-(1 i)
µk

Kk

(i+1)

(C.48)

x fk-1 µk = xµk fk + x  k - k Kk = xµk fk + x  k + Kk
(i)T

(i+1)

(C.49) -(1 i) uk
µk ol(i)

+ Kk

(i+1)T

k -(1 i) uk
µk (i+1)T

ol(i)

 µk fk + u +u  k -(1 i) uk
µk ol(i)

(C.50) (C.51)

(i+1)T

( uµk fk + u  k ) + k Kk

- Kk

(i)T

.

Therefore, the updated policies becomes, N uk | Kk where,
(i+1)

xk + uk

ol(i+1)

, µ(i+1) ,
k

(C.52)

100

µ(i+1)
k

= = =

-1 -1 k Rk + -(1 i) + k Puu,µk fk µk

-1

(C.53) (C.54) (C.55)

Kk uk

(i+1)

µ(i+1)
k

-1 -1 -(1 - k Pux,µk fk - k Cux,k i) Kk µk

(i)

ol(i+1)

µ(i+1)
k

-(1 i) uk
µk

ol(i)

-1 -1   µk fk + k + k u u k .

101

Appendix

D

Kalman Filtering
D.1 The standard Kalman filter.

KF, presented by Kalman in his seminal work [7], is probably the most significant result in the theory of filtering. Here we derived it from a Bayesian point of view. First, let's assume a linear dynamics and measurement equation, as in, xk+1 = Ak xk + Bk uk + wk . (D.1)

yk = Hk xk + vk .

(D.2)

And we further assume that noises are normal and i.d.d. , with distributions wk  N (wk | 0, w ) and vk  N (vk | 0, v ). To find the recursive solution, we assume that the belief at time bk-1 (xk-1 ) is available, ^ k-1 , x-1 ) , bk-1 (xk-1 ) = N (xk-1 | x (D.3)

and find the belief at bk (xk ), given bk-1 (xk-1 ), the latest measurement, yk and the previous control signal, uk-1 . The linear dynamics of the plant results in the following conditional probability, p (xk | xk-1 , uk-1 ) = N (xk | Ak-1 xk-1 + Bk-1 uk-1 , w ) . We can now perform the prediction step: ^ b- k (xk ) =
xk-1

(D.4)

^k-1 , x-1 ) N xk | Ak-1 xk-1 + Bk-1 uk-1 , w N (xk-1 | x

(D.5) (D.6) (D.7)

^ k-1 + Bk-1 uk-1 , Ak-1 x-1 AT = N xk | Ak-1 x k-1 + w
- ^- = N xk | x k , xk

The linear measurement equation results in the following conditional, p (yk | xk ) = N (yk | Hk xk , v ) . (D.8)

102

D.2. THE EXTENDED KALMAN FILTER

We can predict the measurement given the predicted state, ^ p yk | uk-1 , Ik-1 =
xk

p (yk | xk ) b- k (xk )
- y ^k , Sk

(D.9) (D.10) (D.11)

^ k , Hk w HT = N yk | Hk x k + v = N yk |

We can now find the joint distribution of yk , xk given all previous measurements and controls, p yk , xk | uk-1 , Ik-1 = N xk yk ^- x k - y ^k , - xk Hk - xk
T - xk Hk Sk

.

(D.12)

From Appendix B on properties of joint Gaussian random variables, we can find p (xk | yk , Ik-1 , uk-1 ),
- ^- p xk | yk , uk-1 , Ik-1 = N xk | x ^k , xk , k + Kk yk - y

(D.13)

where,

x k Kk x ^k

= = =

-1 xk HT k Sk ,

T - xk - Kk Sk Kk ,

(D.14) (D.15) (D.16)

- ^- x ^k . k + Kk yk - y

Therefore, we only need to replace the variable yk , with the value of the latest measurement, yk , to find the belief, i.e.,

bk (xk )

= = =

(xk | Ik )

(D.17) (D.18) (D.19)

p xk | yk , uk-1 , Ik-1

- ^- ^k , x k . N xk | x k + Kk yk - y

This completes the derivation of KF.

D.2

The Extended Kalman Filter

We assumed a linear dynamics to derive KF. However, for many practical problems the dynamics is nonlinear and, therefore, Kalman filter cannot be readily applied. We could simply linearize the plant and apply the Kalman filter, as a local approximation of the complete Bayes recursion. If the region where the minimization is accurate is large enough this approximation is useful. Another choice is EKF which uses the two following approximations:

E [f (x)] cov (f (x))

= f (E [x]) , = f (x) cov (x)f (x) .
T

(D.20) (D.21)

103

APPENDIX D. KALMAN FILTERING

That is to say, the covariance is approximated as if the plant was linearized, but the mean is calculated using the nonlinear dynamics. With these assumptions, the prediction step becomes,
- ^- b- k (xk ) = N xk | x k , x k ,

(D.22)

where,

^- x k - xk Fk - 1

=

= Fk-1 x-1 FT k -1 +  w , =

f (x ^k-1 , uk-1 , 0, k ),

(D.23) (D.24) .
xk-1 =x ^k-1 ,uk-1 =uk-1 ,wk-1 =0

 f (xk-1 , uk-1 , wk-1 , k )  xk-1

(D.25)

The update step becomes,
- ^- ^k , xk , bk (xk ) = N xk | x k + Kk y k - y

(D.26)

where,

- y ^k

= = = =

^- g x k , 0, k ,
T - xk - Kk Sk Kk , -1 xk GT k Sk ,

(D.27) (D.28) (D.29) .
xk-1 =x ^k ,vk =0

xk Kk Gk

 g(xk , vk , k )  xk

(D.30)

104

Bibliography

[1] H. Sussmann and J. Willems, "300 years of optimal control: From the brachystochrone to the maximum principle," IEEE Control Systems Magazine, vol. 17, no. 3, pp. 32­44, 1997. (Cited on page 1.) [2] B. V. G. R. . M. E. Pontryagin, L., The Mathematical Theory of Optimal Processes. New York City, NY: Pergamon Press, 1962. (Cited on page 1.) [3] R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957. (Cited on pages 1 and 14.) [4] R. Bellman, "On the theory of dynamic programming," Proceedings of the National Academy of Sciences of the United States of America, vol. 38, no. 8, pp. 716­802, 1952. (Cited on pages 1 and 14.) [5] R. E. Kalman, "On the general theory of control systems," IRE Transactions on Automatic Control, vol. 4, no. 3, pp. 110­111, 1959. (Cited on page 1.) [6] R. E. Kalman, "Contributions to the theory of optimal control," Boletin Sociedad Matem´ atica Mexicana, vol. 5, no. 2, pp. 102­119, 1960. (Cited on page 16.) [7] R. E. Kalman and R. S. Bucy, "New results in linear filtering and prediction theory," Journal of Basic Engineering, vol. 83, no. 3, pp. 95­108, 1961. (Cited on pages 1 and 102.) [8] P. H. Zipkin, Foundations of inventory management, vol. 2. New York City, NY: McGraw-Hill, 2000. (Cited on page 2.) [9] J. Stein, Stochastic Optimal Control and the U.S. Financial Debt Crisis. SpringerLink : B¨ ucher, Berlin, Germany: Springer, 2012. (Cited on page 2.) [10] W. Powell, Approximate Dynamic Programming: Solving the Curses of Dimensionality. Wiley Series in Probability and Statistics, Hoboken, NJ: Wiley, 2011. (Cited on pages 2, 3, and 4.) [11] M. Toussaint, "Probabilistic inference as a model of planned behavior," K¨ unstliche Intelligenz, vol. 3, no. 9, pp. 23­29, 2009. (Cited on page 3.) [12] E. Todorov, "General duality between optimal control and estimation," in Proc. 47th IEEE Conference on Decision and Control, pp. 4286­4292, IEEE, 2008. (Cited on pages 4 and 6.)

105

BIBLIOGRAPHY

[13] H. J. Kappen, "Linear theory for control of nonlinear stochastic systems," Physical Review Letters, vol. 95, pp. 200­201, Nov 2005. (Cited on pages 4 and 5.) [14] E. Todorov, "Efficient computation of optimal actions," Proceedings of the National Academy of Sciences, vol. 106, no. 28, pp. 11478­11483, 2009. (Cited on page 4.) [15] M. Toussaint and A. Storkey, "Probabilistic inference for solving discrete and continuous state markov decision processes," in Proc. 23rd International Conference on Machine Learning, pp. 945­952, ACM, 2006. (Cited on page 3.) [16] K. Rawlik, M. Toussaint, and S. Vijayakumar,"On stochastic optimal control and reinforcement learning by approximate inference," in Proc. International Conference on Robotics Science and Systems, MIT Press, 2012. (Cited on pages 3, 4, 5, 6, 7, 18, 19, 20, 21, 34, 40, 42, 43, 48, 50, 54, 56, 88, and 91.) [17] M. Hoffman, N. de Freitas, A. Doucet, and J. Peters, "An expectation-maximization algorithm for continuous markov decision processes with arbitrary rewards," in Proc. 12th International Conference on Artificial Intelligence and Statistics, (Clearwater Beach, FL), pp. 232­239, 2009. (Cited on pages 4, 6, and 56.) [18] C. Andrieu, A. Doucet, S. Singh, and V. TADIC,"Particle methods for change detection, system identification, and control," Proceedings of the IEEE, vol. 92, no. 3, pp. 423­438, 2004. (Not cited.) [19] M. Hoffman, H. Kueck, N. de Freitas, and A. Doucet, "New inference strategies for solving markov decision processes using reversible jump MCMC," in Proc. 25th Conference on Uncertainty in Artificial Intelligence, (Montreal, Canada), pp. 223­231, AUAI Press, 2009. (Cited on page 44.) [20] D. Barber and T. Furmston, "Solving deterministic policy (PO) MDPs using expectationmaximisation and antifreeze," in Proc. European Conference on Machine Learning, vol. 50, (Bled, Slovenia), p. 64, 2009. (Cited on page 4.) [21] T. Furmston and D. Barber, "Variational methods for reinforcement learning," in Proc. 13th International Conference on Artificial Intelligence and Statistics, vol. 9, (Sardinia, Italy), pp. 241­ 248, 2010. (Not cited.) [22] H. Kappen, V. G´ omez, and M. Opper,"Optimal control as a graphical model inference problem," Machine Learning, pp. 1­24, 2009. (Cited on pages 4, 6, and 53.) [23] Q. Liu and A. Ihler, "Belief propagation for structured decision making," in Proc. Uncertainty in Artificial Intelligence, (Catalina Island, CA), 2012. (Cited on pages 19, 40, and 56.) [24] M. Toussaint, A. Storkey, and S. Harmeling, Expectation-Maximization methods for solving (PO) MDPs and optimal control problems, ch. 18, pp. 388­413. Cambridge University Press, bayesian time series models ed., 2011. (Cited on pages 3, 4, and 56.) [25] E. Theodorou, J. Buchli, and S. Schaal, "Reinforcement learning of motor skills in high dimensions: A path integral approach," in International Conference on Robotics and Automation, (Anchorage, AK), pp. 2397­2403, IEEE, 2010. (Cited on page 3.)

106

BIBLIOGRAPHY

[26] J. Kober and J. Peters, "Policy search for motor primitives in robotics," Machine Learning, vol. 84, no. 1-2, pp. 171­203, 2011. (Not cited.) [27] M. Toussaint and C. Goerick, "Probabilistic inference for structured planning in robotics," in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 3068­3073, IEEE, 2007. (Cited on pages 19 and 48.) [28] M. Toussaint, "Robot trajectory optimization using approximate inference," in Proc. 26th Annual International Conference on Machine Learning, pp. 1049­1056, ACM, 2009. (Cited on pages 4, 18, 19, 34, 48, and 49.) [29] D. Verma and R. P. Rao, "Planning and acting in uncertain environments using probabilistic inference," in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2382­2387, IEEE, 2006. (Not cited.) [30] M. Toussaint, N. Plath, T. Lang, and N. Jetchev, "Integrated motor control, planning, grasping and high-level reasoning in a blocks world using probabilistic inference," in Proc. IEEE International Conference on Robotics and Automation, pp. 385 ­391, may 2010. (Cited on pages 3 and 5.) [31] R. Howard, Dynamic programming and Markov processes. Cambridge, MA: Technology Press of Massachusetts Institute of Technology, 1960. (Cited on page 3.) [32] C. Derman, Finite State Markovian Decision Processes. Mathematics in Science and Engineering, Waltham, MA: Academic Press, 1970. (Not cited.) [33] D. Heyman and M. Sobel, Stochastic Models in Operations Research: Stochastic optimization. McGraw-Hill Series in Quantitative Methods for Management, New York City, NY: McGrawHill, 1984. (Not cited.) [34] R. Bellman, "A markovian decision process," Indiana University Math Journal, vol. 6, pp. 679­ 684, 1957. (Cited on page 3.) [35] M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics, Hoboken, NJ: Wiley-Interscience, 2005. (Cited on page 3.) [36] L. P. Kaelbling, M. L. Littman, and A. W. Moore, "Reinforcement learning: A survey," Journal of Artificial Intelligence Research, vol. 4, no. 1, pp. 237­285, 1996. (Cited on page 4.) [37] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, vol. 1. Cambridge, England: Cambridge University Press, 1998. (Cited on page 4.) [38] P. J. Werbos, Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard University, 1974. (Cited on page 4.) [39] P. J. Werbos, "Approximate dynamic programming for real-time control and neural modeling," Handbook of intelligent control: Neural, fuzzy, and adaptive approaches, vol. 15, pp. 493­525, 1992. (Cited on page 4.) [40] D. Bertsekas and C. White,"Dynamic programming and stochastic control," IEEE Transactions on Systems, Man and Cybernetics, vol. 7, no. 10, pp. 758­759, 1977. (Cited on page 4.)

107

BIBLIOGRAPHY

[41] J. Tsitsiklis, "Asynchronous stochastic approximation and q-learning," Machine Learning, vol. 16, no. 3, pp. 185­202, 1994. (Cited on page 4.) [42] D. Bertsekas and J. Tsitsiklis, Neuro-Dynamic Programming. Athena Scientific optimization and computation series, Nashua, NH: Athena Scientific, 1996. (Cited on pages 4 and 17.) [43] D. Bertsekas, Dynamic Programming and Optimal Control, vol. I & II of Athena Scientific optimization and computation series. Nashua, NH: Athena Scientific, third ed., 2005. (Cited on pages 4, 11, 13, 14, and 21.) [44] J. Si, A. Barto, and W. Powell, Handbook of Learning and Approximate Dynamic Programming. IEEE Press Series on Computational Intelligence, Hoboken, NJ: Wiley-IEEE Press, 2004. (Cited on page 4.) [45] D. Bertsekas, Dynamic Programming and Optimal Control, vol. I of Athena Scientific optimization and computation series. Nashua, NH: Athena Scientific, fourth ed., 2012. (Cited on pages 4 and 17.) [46] C. D. Charalambous, F. Rezaei, and A. Kyprianou, "Relations between information theory, robustness,and statistical mechanics of stochastic systems," in Proc. 43rd IEEE Conference on Decision and Control, vol. 4, pp. 3479­3484, IEEE, 2004. (Cited on page 4.) [47] D. Wolpert, "Information theory­the bridge connecting bounded rational game theory and statistical physics," Complex Engineered Systems, pp. 262­290, 2006. (Cited on pages 6, 19, and 54.) [48] M. Mezard and A. Montanari, Information, Physics, and Computation. Oxford, UK: Oxford University Press, 2009. (Cited on pages 4 and 93.) [49] Statistical Physics of Inference And Control, Sept. 2012. (Cited on page 4.) [50] K. Dvijotham and E. Todorov, Reinforcement Learning and Approximate Dynamic Programming for Feedback Control, ch. Linearly Solvable Optimal Control, pp. 119­141. Hoboken, NJ: Wiley-IEEE Press, 2012. (Cited on pages 4, 5, and 53.) [51] E. Todorov, "Linearly-solvable markov decision problems," Advances in Neural Information Processing Systems, vol. 19, p. 1369, 2007. (Cited on pages 4, 5, and 20.) [52] M. Toussaint and C. Goerick, "A bayesian view on motor control and planning," From Motor Learning to Interaction Learning in Robots, pp. 227­252, 2010. (Cited on page 5.) [53] K. Friston, "What is optimal about motor control?," Neuron, vol. 72, no. 3, pp. 488­498, 2011. (Not cited.) [54] S. Schaal and N. Schweighofer, "Computational motor control in humans and robots," Current Opinion in Neurobiology, vol. 15, no. 6, pp. 675 ­ 682, 2005. <ce:title>Motor sytems / Neurobiology of behaviour</ce:title>. (Not cited.) [55] S. Schaal and C. Atkeson, "Learning control in robotics," Robotics & Automation Magazine, IEEE, vol. 17, no. 2, pp. 20­29, 2010. (Cited on page 5.)

108

BIBLIOGRAPHY

[56] M. Gheshlaghi Azar, V. Gomez, and H. J. Kappen, "Dynamic policy programming," Journal of Machine Learning Research, vol. 13, pp. 3207­3245, 2012. (Cited on pages 6, 54, and 90.) [57] P. A. Ortega and D. A. Braun, "Thermodynamics as a theory of decision-making with information-processing costs," Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science, vol. 469, no. 2153, 2013. (Cited on pages 6, 34, 54, and 55.) [58] P. A. Ortega and D. A. Braun, "A minimum relative entropy principle for learning and acting," Journal of Artificial Intelligence Research, vol. 38, no. 2153, pp. 475­511, 2010. (Cited on pages 20, 34, 54, and 55.) [59] D. Ortega and P. Braun, "Information, utility and bounded rationality," Artificial General Intelligence, pp. 269­274, 2011. (Cited on page 6.) [60] L. Hansen and T. Sargent, Robustness. Princeton, NJ: Princeton University Press, 2007. (Cited on pages 6, 19, 24, and 54.) [61] E. Todorov and W. Li, "A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems," in Proc. American Control Conference, pp. 300­306, IEEE, 2005. (Cited on pages 7, 17, 48, 71, and 73.) [62] J. Hill and W. T. Park, "Real-time control of a robot with a mobile camera," in Proc. 9th International Symposium on Industrial Robots, (Washington D.C, USA), pp. 233­246, March 1979. (Cited on pages 7 and 58.) [63] F. Janabi-Sharifi and M. Ficocelli, "Formulation of radiometric feasibility measures for feature selection and planning in visual servoing," Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 34, pp. 978­987, April 2004. (Cited on page 7.) [64] F. Janabi-Sharifi and M. Marey, "A kalman-filter-based method for pose estimation in visual servoing," Robotics, IEEE Transactions on, vol. 26, pp. 939­947, October 2010. (Not cited.) [65] F. Janabi-Sharifi, L. Deng, and W. Wilson, "Comparison of basic visual servoing methods," Mechatronics, IEEE/ASME Transactions on, vol. 16, pp. 967 ­983, oct. 2011. (Not cited.) [66] L. Deng, F. Janabi-Sharifi, and W. Wilson, "Hybrid motion control and planning strategies for visual servoing," IEEE Transactions on Industrial Electronics, vol. 52, pp. 1024 ­ 1040, aug. 2005. (Cited on page 7.) [67] P. Whittle, Optimal control: basics and beyond. Hoboken, NJ: John Wiley & Sons, Inc., 1996. (Cited on page 13.) [68] S. Marcus, E. Fern´ andez-Gaucherand, D. Hern´ andez-Hernandez, S. Coraluppi, and P. Fard, "Risk sensitive markov decision processes," Progress in Systems and Control Theory, vol. 22, pp. 263­280, 1997. (Cited on pages 14 and 43.) [69] M. Zyskowski and R. Diersing, "Infinite-horizon, multiple-cumulant cost density-shaping for stochastic optimal control," in Proc. American Control Conference, pp. 1488­1493, IEEE, 2011. (Cited on page 14.)

109

BIBLIOGRAPHY

[70] M. Zyskowski, M. Sain, and R. Diersing, "Weighted least-squares, cost density-shaping, stochastic optimal control: A step towards total probabilistic control design," in Proc. 49th IEEE Conference on Decision and Control, pp. 1417­1422, IEEE, 2010. (Cited on page 14.) [71] B. Lindoff, J. Holst, and B. Wittenmark, "Analysis of approximations of dual control," International Journal of Adaptive Control and Signal Processing, vol. 13, no. 7, pp. 593­620, 1999. (Cited on page 16.) [72] D. Bertsimas, D. B. Brown, and C. Caramanis,"Theory and applications of robust optimization," SIAM Review, vol. 53, no. 3, pp. 464­501, 2011. (Cited on pages 16 and 24.) [73] M. G. Lagoudakis and R. Parr, "Least-squares policy iteration," The Journal of Machine Learning Research, vol. 4, pp. 1107­1149, 2003. (Cited on page 17.) [74] E. Theodorou, Y. Tassa, and E. Todorov, "Stochastic differential dynamic programming," in Proc. American Control Conference, pp. 1125­1132, IEEE, 2010. (Cited on page 17.) [75] G. N. Iyengar, "Robust dynamic programming," Mathematics of Operations Research, vol. 30, no. 2, pp. 257­280, 2005. (Cited on pages 18 and 24.) [76] A. Nilim and L. El Ghaoui, "Robust control of markov decision processes with uncertain transition matrices," Operations Research, vol. 53, no. 5, pp. 780­798, 2005. (Cited on pages 18 and 24.) [77] J. Yedidia, W. Freeman, and Y. Weiss, "Understanding belief propagation and its generalizations," Exploring Artificial Intelligence in the New Millennium, vol. 8, pp. 236­239, 2003. (Cited on pages 19, 40, 42, and 56.) [78] A. Iusem, B. Svaiter, and M. Teboulle,"Entropy-like proximal methods in convex programming," Mathematics of Operations Research, vol. 19, no. 4, pp. 790­814, 1994. (Cited on pages 20, 27, and 28.) [79] R. Rockafellar, "Augmented lagrangians and applications of the proximal point algorithm in convex programming," Mathematics of Operations Research, vol. 1, no. 2, pp. 97­116, 1976. (Cited on pages 20, 27, and 28.) [80] E. A. Feinberg,"Optimality of deterministic policies for certain stochastic control problems with multiple criteria and constraints," in Mathematical Control Theory and Finance, pp. 137­148, Berlin, Heidelberg: Springer, 2008. (Cited on page 21.) [81] M. M. Fard and J. Pineau, "MDPs with non-deterministic policies," Advances in Neural Information Processing Systems, vol. 21, p. 1065, 2009. (Cited on page 21.) [82] S. Filippi, O. Capp´ e, and A. Garivier, "Optimism in reinforcement learning and kullbackleibler divergence," in Proc. 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), 2010, pp. 115­122, IEEE, 2010. (Cited on pages 23 and 26.) [83] R. I. Brafman and M. Tennenholtz, "R-MAX-a general polynomial time algorithm for nearoptimal reinforcement learning," The Journal of Machine Learning Research, vol. 3, pp. 213­231, 2003. (Cited on pages 23 and 26.)

110

BIBLIOGRAPHY

[84] A. Ben-Tal and A. Nemirovski,"Robust solutions of linear programming problems contaminated with uncertain data," Mathematical Programming, vol. 88, no. 3, pp. 411­424, 2000. (Cited on page 24.) [85] A. Ben-Tal, D. den Hertog, A. De Waegenaere, B. Melenberg, and G. Rennen, "Robust solutions of optimization problems affected by uncertain probabilities," Management Science, vol. 59, no. 2, pp. 341­357, 2013. (Cited on page 24.) [86] H. David, C. Strauss, and D. Rajnarayan, "Advances in distributed optimization using probability collectives," Advances in Complex Systems, vol. 9, no. 04, pp. 383­436, 2006. (Cited on page 27.) [87] D. Rajnarayan, D. Wolpert, and I. Kroo, "Optimization under uncertainty using probability collectives," in Proc. 12th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference, (Victoria, British Columbia, Canada.), 2006. (Cited on page 27.) [88] A. Iusem,"Augmented lagrangian methods and proximal point methods for convex optimization," Investigaci´ on Operativa, vol. 8, pp. 11­49, 1999. (Cited on page 27.) [89] M. Teboulle, "Entropic proximal mappings with applications to nonlinear programming," Mathematics of Operations Research, vol. 17, no. 3, pp. 670­690, 1992. (Cited on pages 27 and 28.) [90] J. Bierkens and H. J. Kappen, "Probabilistic solution of relative entropy weighted control," pre-print, 2012. arXiv:1205.6946 [math.PR]. (Cited on page 28.) [91] D. P. Bertsekas, "Multiplier methods: a survey," Automatica, vol. 12, no. 2, pp. 133­145, 1976. (Cited on page 29.) [92] L. Hansen and T. Sargent,"Robust control and model uncertainty," American Economic Review, pp. 60­66, 2001. (Cited on pages 30 and 54.) [93] L. Hansen, T. Sargent, G. Turmuhambetova, and N. Williams, "Robust control and model misspecification," Journal of Economic Theory, vol. 128, no. 1, pp. 45­90, 2006. (Cited on page 30.) [94] J. Yedidia, W. Freeman, and Y. Weiss, "Constructing free-energy approximations and generalized belief propagation algorithms," IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282­2312, 2005. (Cited on pages 33 and 56.) [95] M. Wainwright and M. Jordan, "Graphical models, exponential families, and variational inference," Foundations and Trends R in Machine Learning, vol. 1, no. 1-2, pp. 1­305, 2008. (Cited on page 33.) [96] J. Bezdek and R. Hathaway, "Some notes on alternating optimization," Advances in Soft Computing­AFSS 2002, pp. 187­195, 2002. (Cited on pages 37, 40, and 56.) [97] H. Cox, "On the estimation of state variables and parameters for noisy dynamic systems," IEEE Transactions on Automatic Control, vol. 9, no. 1, pp. 5­12, 1964. (Cited on page 44.)

111

BIBLIOGRAPHY

[98] E. Wan and R. Van der Merwe, "The unscented kalman filter for nonlinear estimation," in Proc. Adaptive Systems for Signal Processing, Communications, and Control Symposium, pp. 153­158, 2000. (Cited on page 44.) [99] R. Van Der Merwe, A. Doucet, N. De Freitas, and E. Wan, "The unscented particle filter," Advances in Neural Information Processing Systems, pp. 584­590, 2001. (Cited on page 44.) [100] T. P. Minka, "Expectation-propagation for approximate bayesian inference," in Proc. 17th Conference on Uncertainty in Artificial Intelligence, pp. 362­369, Morgan Kaufmann Publishers Inc., 2001. (Cited on page 44.) [101] H. Kappen, "Path integrals and symmetry breaking for optimal control theory," Journal of Statistical Mechanics: Theory and Experiment, no. 11, pp. P110­121, 2005. (Cited on page 53.) [102] K. Dvijotham and E. Todorov, "Linearly solvable markov games," in Proc. American Control Conference, pp. 1845­1850, IEEE, 2012. (Cited on page 54.) [103] I. Petersen, M. James, and P. Dupuis, "Minimax optimal control of stochastic uncertain systems with relative entropy constraints," Automatic Control, IEEE Transactions on, vol. 45, no. 3, pp. 398­412, 2000. (Cited on pages 55 and 91.) [104] A. Ypma and T. Heskes, "Iterated extended kalman smoothing with expectation-propagation," in Proc. IEEE 13th Workshop on Neural Networks for Signal Processing, pp. 219­228, 2003. (Cited on page 57.) [105] P. Corke, "Visual control of robot manipulators - a review," in Visual Serving: Real -Time Control of Robot Manipulators Based on Visual Sensory Feedback (K. Hashimoto, ed.), vol. 7, pp. 1­31, Toh Tuck Link, Singapore: World Scientific Publishing Co, 1993. (Cited on pages 58, 59, and 65.) [106] N. Papanikolopoulos, P. Khosla, and T. Kanada, "Vision and control techniques for robotic visual tracking," in Proc. IEEE International Conference on Robotics and Automation, pp. 857­ 864, IEEE, 1991. (Not cited.) [107] F. Chaumette and S. Hutchinson, "Visual servo control. i. basic approaches," IEEE Robotics & Automation Magazine, vol. 13, no. 4, pp. 82­90, 2006. (Cited on pages 58, 59, and 64.) [108] F. Chaumette and S. Hutchinson, "Visual servo control, part ii: Advanced approaches," IEEE Robotics & Automation Magazine, vol. 14, no. 1, pp. 109­118, 2007. (Not cited.) [109] S. Hutchinson, G. Hager, and P. Corke, "A tutorial on visual servo control," Robotics and Automation, IEEE Transactions on, vol. 12, no. 5, pp. 651­670, 1996. (Cited on pages 58, 59, 64, and 65.) [110] M. Sauvee, P. Poignet, E. Dombre, and E. Courtial, "Image based visual servoing through nonlinear model predictive control," in Proc. 45th IEEE Conference on Decision and Control, pp. 1776­1781, 2006. (Cited on pages 59, 66, and 72.) [111] G. Allibert, E. Courtial, and F. Chaumette, "Predictive control for constrained image-based visual servoing," IEEE Transactions on Robotics and Automation, vol. 26, no. 5, pp. 933­939, 2010. (Cited on pages 59, 66, and 72.)

112

BIBLIOGRAPHY

[112] G. Chesi and K. Hashimoto, Visual Servoing Via Advanced Numerical Methods. Lecture Notes in Control and Information Sciences, Springer, 2010. (Not cited.) [113] G. Allibert, E. Courtial, and F. Chaumette, "Visual servoing via nonlinear predictive control," in Visual Servoing via Advanced Numerical Methods (G. Chesi and K. Hashimoto, eds.), vol. 401 of Lecture Notes in Control and Information Sciences, pp. 375­393, Berlin, Germany: Springer, 2010. (Cited on pages 59 and 66.) [114] B. Siciliano and L. Sciavicco, Robotics: modelling, planning and control. Berlin, Germany: Springer, 2009. (Cited on pages 60 and 75.) [115] D. Lowe, "Object recognition from local scale-invariant features," in Proc. 7th IEEE International Conference on Computer Vision, vol. 2, pp. 1150­1157, Sept 1999. (Cited on page 61.) [116] F. Hoffmann, T. Nierobisch, T. Seyffarth, and G. Rudolph, "Visual servoing with moments of SIFT features," in Proc. IEEE International Conference on Man and Cybernetics., vol. 5, pp. 4262­4267, IEEE, 2006. (Cited on page 61.) [117] J. Heikkila and O. Silven, "A four-step camera calibration procedure with implicit image correction," in Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, (Puerto Rico), pp. 1106­1112, IEEE, 1997. (Cited on pages 61 and 75.) [118] F. Chaumette, P. Rives, and B. Espiau, "The task function approach applied to vision-based control," in Proc. 5th International Conference on Advanced Robotics, (Washington, D. C), pp. 1392­1397, IEEE, 1991. (Cited on pages 63 and 64.) [119] P. Rives, F. Chaumette, and B. Espiau, "Visual servoing based on a task function approach," in Experimental Robotics I (V. Hayward and O. Khatib, eds.), vol. 139 of Lecture Notes in Control and Information Sciences, pp. 412­428, Springer, 1990. (Cited on page 63.) [120] F. Chaumette, "Potential problems of stability and convergence in image-based and positionbased visual servoing," in The Confluence of Vision and Control (D. Kriegman, G. Hager, and A. Morse, eds.), vol. 237 of Lecture Notes in Control and Information Sciences, pp. 66­78, Springer London, 1998. (Cited on pages 66 and 80.) [121] K. Hashimoto, T. Ebine, and H. Kimura, "Visual servoing with hand-eye manipulator-optimal control approach," IEEE Transactions on Robotics and Automation, vol. 12, no. 5, pp. 766­774, 1996. (Cited on page 66.) [122] M. Sznaier and O. Camps, "Control issues in active vision: open problems and some answers," in Proc. 37th IEEE Conference on Decision and Control, vol. 3, pp. 3238­3244 vol.3, 1998. (Cited on page 66.) [123] Y. Bar-Shalom and E. Tse, "Dual effect, certainty equivalence, and separation in stochastic control," IEEE Transactions on Automatic Control, vol. 19, no. 5, pp. 494­500, 1974. (Cited on pages 68 and 69.) [124] Y. Bar-Shalom and E. Tse, "Caution, probing, and the value of information in the control of uncertain systems," in Annals of Economic and Social Measurement, vol. 5, pp. 323­337, NBER, 1976. (Not cited.)

113

BIBLIOGRAPHY

[125] A. Feldbaum, "Dual control theory i," Automation and Remote Control, vol. 21, no. 9, pp. 874­ 1039, 1960. (Cited on page 68.) [126] D. P. Joseph and T. J. Tou, "On linear control theory," American Institute of Electrical Engineers, Part II: Applications and Industry, Transactions of the, vol. 80, no. 4, pp. 193­196, 1961. (Cited on page 68.) [127] E. Tse, "Adaptive dual control methods," in Annals of Economic and Social Measurement, vol. 1, pp. 65­84, 1974. (Cited on page 68.) [128] H. Unbehauen, "Adaptive dual control systems: a survey," in Proc. IEEE Adaptive Systems for Signal Processing, Communications, and Control Symposium, pp. 171­180, IEEE, 2000. (Cited on page 69.) [129] B. Wittenmark, "Adaptive dual control methods: An overview," in Proc. 5th IFAC symposium on Adaptive Systems in Control and Signal Processing, Citeseer, 1995. (Cited on page 68.) [130] A. De Luca, G. Oriolo, and P. Robuffo Giordano, "Feature depth observation for image-based visual servoing: Theory and experiments," The International Journal of Robotics Research, vol. 27, no. 10, pp. 1093­1116, 2008. (Cited on page 69.) [131] A. Davison, I. Reid, N. Molton, and O. Stasse, "Monoslam: Real-time single camera slam," Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 6, pp. 1052­ 1067, 2007. (Not cited.) [132] J. Civera, A. J. Davison, and J. Montiel, "Inverse depth parametrization for monocular slam," Robotics, IEEE Transactions on, vol. 24, no. 5, pp. 932­945, 2008. (Cited on page 69.) [133] A. Bemporad and M. Morari, "Robust model predictive control: A survey," in Robustness in Identification and Control (A. Garulli and A. Tesi, eds.), vol. 245 of Lecture Notes in Control and Information Sciences, pp. 207­226, Springer London, 1999. (Cited on page 72.) [134] G. Allibert, E. Courtial, and Y. Toure,"Visual predictive control for manipulators with catadioptric camera," in Proc. IEEE International Conference on Robotics and Automation, pp. 510­515, 2008. (Cited on page 72.) [135] Thermo-CRS, A255 Robot System User Guide UMI-A255-400. Thermo Scientific, Mississauga, ON, Canada, 002a ed., July 2002. (Cited on page 75.) [136] Quanser, QuaRC CRS CataLyst-5T Open-Architecture Manual. Quanser Inc., Markham, Ontario, Canada, 2008. (Cited on page 75.) [137] Point Grey, Richmond, BC, Canada, Camera Model: http://ww2.ptgrey.com/IEEE-1394/fireflymv. (Cited on page 75.) FFMV-03M2M-CS.

[138] N. D. Inc., Optotrak Certus Motion Capture System. Northern Digital Inc., Waterloo, ON, Canada. (Cited on page 75.) [139] P. I. Corke, Robotics, Vision & Control: Fundamental Algorithms in Matlab. Berlin, Germany: Springer, 2011. (Cited on page 80.)

114

BIBLIOGRAPHY

[140] C. Charalambous and F. Rezaei, "Stochastic uncertain systems subject to relative entropy constraints: Induced norms and monotonicity properties of minimax games," Automatic Control, IEEE Transactions on, vol. 52, no. 4, pp. 647­663, 2007. (Cited on page 91.) [141] J. Gonzalez, Y. Low, and C. Guestrin, Scaling Up Machine Learning, ch. Parallel Belief Propagation in Factor Graphs, pp. 190­217. Cambridge, UK: Cambridge University Press, 2012. (Cited on page 91.) [142] N. Ma, Y. Xia, and V. Prasanna, "Data parallelism for belief propagation in factor graphs," in Proc. 23rd International Symposium on Computer Architecture and High Performance Computing, pp. 56­63, 2011. (Cited on page 91.)

115

