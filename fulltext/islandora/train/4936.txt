DESIGNING AND EVALUATING A COMPOSITION SOFTWARE INTERFACE WITH VIBROTACTILE NOTATION SYSTEM
By

Somang Nam H.B.Sc., University of Toronto, 2013

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science

in the Program of Computer Science

Toronto, Ontario, Canada, 2016

© Somang Nam 2016

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS

I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

DESIGNING AND EVALUATING A COMPOSITION SOFTWARE INTERFACE WITH VIBROTACTILE NOTATION SYSTEM
Somang Nam Master of Science, Computer Science, Ryerson University, 2016

Abstract
Vibrotactile stimulation can be used as a substitute for audio or visual stimulation for people who are deaf or blind. In order to do this, new tools must be developed and evaluated that support the creation and experience of vibration on the skin.

In this paper, a vibrotactile composition tool, the "Beadbox ", along with the results of a user study will be described. Beadbox is a vibrotactile notation system and software tool, which helps users to create and record a vibrotactile art composition. It allows users to control the four essential vibrotactile technical elements: (1) frequency; (2) intensity; (3) temporal information; and (4) spatial information consists of how the vibrotactile signal is distributed on the human body. A user study designed to evaluate the usability and support for creative expression of Beadbox. Results from the user study indicate that the Beadbox is easy to use, and viable for vibrotactile composition.

iii

Acknowledgements
Firstly, I would like to express my sincere gratitude to my advisor Prof. Deborah Fels for the continuous support of my study and related research, for her patience, motivation, and immense knowledge. Her guidance helped me in all the time of research and writing of this thesis. I could not have imagined having a better supervisor and mentor for my study.

Besides my advisor, I would like to thank the rest of my thesis committee: Prof. Alireza Sadeghian, Prof. Frank Russo, and Prof. Alex Ferworn, for their insightful comments and encouragement, but also for the hard question which incented me to widen my research from various perspectives.

In addition, I would like to thank my father and mother for their love, support and encouragement during this process. I would like to thank my grandparents. I would also like to thank my fiancée, for all her love and support.

I want to also express my sincere gratitude to my people who were there for me: 88 friends, MSM members, TKPC, U of Toronto hackathon crew, IMDC lobbies and Vibrafusion crew. I could not have completed this document without their help, support, and their love. I would like to thank my participants for their quality feedback and their enthusiastic attitude. I could not have done it without them. Funding was graciously provided by the Social Sciences and Humanities Research Council (SSHRC).

Finally, I would like to thank God for all the processes, wins and mistakes I went through.

This thesis is dedicated to my parents.

iv

Table of Contents
AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS .......... ii Abstract .......................................................................................................................... iii Acknowledgements ........................................................................................................ iv Table of Contents ............................................................................................................ v List of Tables .................................................................................................................. ix List of Figures .................................................................................................................. x List of Appendices ......................................................................................................... xii Chapter 1. Introduction....................................................................................................1 1.1. Research Questions .............................................................................................3 1.2. Research Contributions ........................................................................................4 1.3. Thesis Outline .......................................................................................................6 Chapter 2. Literature Review ..........................................................................................7 2.1. Human Vibrotactile: System and Mechanics........................................................8 2.1.1. Mechanoreceptor ...........................................................................................8 2.1.2. Vibrotactile Perception ...................................................................................9 2.1.3. Creative process and vibrotactile.................................................................13

v

2.1.4. Vibrotactile Output........................................................................................17 2.2. Timeline Interface ...............................................................................................19 2.2.1. Audio Editors ................................................................................................20 2.2.2. Video Editors: ...............................................................................................24 2.2.3. Animation Editors .........................................................................................26 2.3. User Interface Design .........................................................................................28 2.3.1. HCI and UI ....................................................................................................29 2.3.2. Visual Perception in Interface Design ..........................................................33 2.4. Media Protocols ..................................................................................................38 2.4.1. AIFF ..............................................................................................................39 2.4.2. WAV..............................................................................................................39 2.4.3. MP3 ..............................................................................................................39 2.4.4. MIDI ..............................................................................................................40 Chapter 3. System Overview ........................................................................................45 3.1. System Design....................................................................................................45 3.2. The Beadbox.......................................................................................................47 3.2.1. Vibrotactile Notation .....................................................................................48

vi

3.2.2. User Interface ...............................................................................................52 3.3. Vibrotactile Instrumental Digital Interface (VIDI) ................................................57 3.3.1. Introduction ...................................................................................................58 3.3.2. Standard VIDI File Format ...........................................................................59 Chapter 4. Methodology, Results and Discussion........................................................62 4.1. User Study ..........................................................................................................62 4.1.1. Pre-study questionnaire ...............................................................................62 4.1.2. Post-study questionnaire..............................................................................63 4.1.3. User Study Procedure ..................................................................................63 4.1.4. Participants ...................................................................................................64 4.1.5. Data analysis methods .................................................................................65 4.2. Results: ...............................................................................................................67 4.2.1. Forced-choice questions ..............................................................................67 4.2.2. Qualitative Data ............................................................................................69 4.3. Discussion ..............................................................................................................70 4.3.1. Usability and usefulness of the Beadbox ........................................................70 4.3.2. Viability .............................................................................................................76

vii

4.3.3. Technical Aspects............................................................................................80 4.3.3.1. VIDI Protocol .............................................................................................80 4.4. Limitations ..............................................................................................................81 Chapter 5. Conclusions and Future work .....................................................................85 5.1. Conclusions.........................................................................................................85 5.2. Future work .........................................................................................................86 References ..................................................................................................................110

viii

List of Tables
Table 1: Usability constructs of various standards Table 2: Correlation of light frequency/colour hue and audio frequency/music scale. Table 3: Themes and definitions used for thematic analysis. Table 4: Results of chi-square test and descriptive of all Likert-scale type questions for significant results . A rating of one is strongly disagree and five is strongly agree.

ix

List of Figures
Figure 1: Logical mapping in between sensory domains.

Figure 2: Contours of equal sensation level at 250 Hz, with different intensi ty levels from 5dB to 55 dB. Figure 3.a: The Hapticon Editor has a frequency pattern editor and other simple buttons. Figure 3.b: The posVibEditor consists of three parts: a) manager; b) pattern editor; and c) multichanneltimeline interface. Figure 4: A screenshot of the VibScoreEditor with three different windows opened up: a) displays the overall composition; b) shows the clefs manager; and c) allows to set frequency values and patterns. Figure 5: Screenshot of the TactiPEd. 1) file navigator; 2) shows the timeline and actuator map; 3) sequence number labels; 4) actuator visualization; 5) frequency samples display and 6) actuator controls. Figure 6: The Emoti-Chair with its cover on and off. Figure 7: The spatial arrangement of transducers (left) and the vibrotactile composition suit (right). Figure 8: Screenshot of Cakewalk : an audio editor with traditional western music notation (with stave and notes) display and piano roll display. Figure 9: Screenshot from Cakewalk : The user interface for EQ control with knob visual design. The knobs would increase/decrease the value when user turns clock-wise/counter clock-wise. Figure 10: A screenshot of sample wave editor, Audacity. Figure 11: A screenshot of sample MIDI editor, FL studio. Figure 12: A screenshot of sample video editor, Adobe Premiere. In the multi -timeline display panel, notice that the video clips are displayed with its thumbnails while audio samples are displayed with the frequency graph. Figure 13: Screenshot of 3D animation editor, Blender3D. Figure 14: A screenshot of sample animation editor, Adobe Flash Figure 15: The human (user) centred design cycle Figure 16.a: Visual metaphor of a recycle bin used as an icon. Figure 16.b: A recycle bin in real life. Figure 17: Colour circle, and chromatic musical scale circle. Figure 18: Proposed space for the associations between pitch-light intensity and loudness saturation. Figure 19: Simple MIDI file structure. Figure 20: System diagram of the Beadbox. Figure 21: The Beadbox Overview.

x

Figure 22: Simple voice coil visual metaphor. Figure 23: Colour brightness gradation (dark to light as frequency increases) by sample frequency level. Figure 24: Sample Bead showing lower intensity coefficient. Figure 25.a: A note with a certain duration changing from a low frequency to high frequency. Figure 25.b: A note with a certain duration changing from one track to another track. Figure 25.c: A note with certain duration from a low intensity to a high intensity. Figure 26: The Beadplayer frame. Figure 27: Bead Palette with new Bead button, intensity and frequency slider. Figure 28.a: The icons are arranged to match the Emoti-chair's (left) voice coil positions. The icons for channels 2, 3, 4, 5 and 8 are turned on, which means the notes on these tracks are now playing. Figure 28.b: The icons are arranged to match the vibrotactile belt. It only has four channels of output so that there are only four icons in the visualization window. Figure 29: Two overview panel columns visualized the Beadplayer frame. Figure 30: VIDI File Structure. Figure 31: VIDI header byte information. Figure 32: Track component byte information. Figure 33: Bead Message byte information. Figure 34: Frequency of comments from participants in each theme. Figure 35.a: The Bead (on the left) has no connected Bead at the moment. When user dragged the Bead from left to the right, it created a connected Bead at where user dropped. Figure 35.b: The Bead at scene 1, already has a connection with another Bead. When user dragged the Bead to the lower track, it relocated the Bead. Figure 35.c: Similar to Figure 34.b, the Bead at scene 1 has connection. When user dra gged the Bead to the right, it relocated the Bead. Figure 35.d: This scene describes the click and drop command. User first clicked the Bead on scene 1 to select, then clicked on the empty space which relocated the Bead at dropped position. Figure 36: A composer is using the Beadbox while sitting on the Emoti-Chair. Figure 37: Common waveform types and their matching notation ideas. From top to bottom: sine wave, square wave, triangle wave, and sawtooth wave.

xi

List of Appendices
Appendix A: MIDI Specification 1.0 Appendix B: VIDI Specification 1.0 Appendix C: Ethics Approval Letter Appendix D: Study Consent Form Appendix E: Study Questionnaire Appendix F: Frequency Graph for Forced choice questions

xii

xiii

Chapter 1. Introduction

"Great work s of art are only great because they are accessible and comprehensible to everyone." LEO TOLSTOY , What Is Art?

Art and technology are consistently related to each other in many aspects throughout history. Both share a core creative process. With the advent of technology for creative activities, the process of creative expression has been propelled to new heights. For instance, digital graphics hardware and software have enabled art and graphics to be drawn with relative ease, digital cameras have enabled high-definition photography to be captured at the click of a button, and digital instruments have enabled more capabilities in music creation for a multitude of purposes. Additionally, software tools have been developed with interactive interfaces designed from a usercentred perspective. One of the newest additions to the suite of creative tools and techniques is vibrotactile art.

Vibrotactile technologies provide the users to create and transmit vibrations, and enable audiences to experience/feel a multitude of vibrations through tactile sense rather than through audio/visual media. This can be possible by use of gestures (hands, feet, body, facial expressions, eye movement, etc.) or even without such interfaces. One can use these technologies to enable users to input and output various vibrations for a wide range of educational and recreational purposes by using existing vibrotactile output device (i.e. Emoti-chair). Furthermore, Virtual Reality (VR) technology can be used in conjunction with vibrotactile technology to deliver multi-sensory feedback as demonstrated in recently introduced 4-D movies in some theatres (i.e. haptic chairs when there is a shock in the movie, wind blowing, strobe lights, etc.) [1, 2, 3, 4]. The "fourth-

1

dimension" does not refer to the scientific fourth dimensional space, but is a marketing term to emphasize the uniqueness of the effect. Vibrotactile technology can be more inclusive for a wider range of people by providing accessibility to an experience that would be impossible to experience otherwise by people who are deaf or hard of hearing.

Vibrotactile stimulation can be used as a substitute for audio or visual stimulation for people who are deaf/hard of hearing or blind/low vision. In 2008, a vibrotactile system called the Emoti chair was designed specifically to allow deaf users to experience audio music using the tactile sense [5]. Since that time, artists have become interested in creating vibrotactile media as a new art form [6]. However, in order to do this, there are new tools needing development and evaluation in order to provide for this form of expression. In addition, there is no notation system for vibrotactile stimuli in which these expressions can be recorded and/or shared with audiences

Currently, vibrotactile output is provided using motors, pins, voicecoils/audio speakers, piezoelectric vibrators or other physical stimuli. However, often these output displays are controlled with sound (which can be designed and manipulated by users) or by low-level programming of circuitry which is not usually accessible by end-users interested in working only in the tactile domain. An example which could be considered similar to this challenge was analogue music. Until the invention of a standardized and formalized music notation system with notes, rests, key signatures, etc., there was no formal language to tie together different forms of music that could be universally understood by all end-users/musicians regardless of the musical instrument that was played.

In this thesis I take an inductive research approach to the investigation of a vibrotactile, time-based notation system by introducing the concept of the "Beadbox ". The Beadbox is a software tool that utilized a novel vibrotactile notation system which allows artists to compose vibrotactile expressions using standardized mechanisms for expressing their ideas. In doing so, other artists can reproduce those vibrotactile compositions, and they can be represented on a variety of output devices. This concept is similar to the idea of another time-based media music

2

notation which contain concepts of note values, key signatures, timing indicators, etc. Instead, Beadbox concepts consist of frequency and intensity levels, speed c ontrol, distribution on output channels (spatial variable) and in time (temporal variable). The Beadbox is evaluated in user studies, where participants are asked to use the software to create a vibrotactile composition. For each user study session, pre and post questionnaire are used to gather subjective data regarding their impressions and opinions. Qualitative and quantitative data are analyzed to determine system usability of Beadbox and its user interface, viability of Beadbox as a vibrotactile composi tion tool, and how novice users interact with the system.

1.1. Research Questions
The research questions I will attempt to answer with this research are the following: Is Beadbox and its notation system usable by a variety of artists' perspectives? a. Is Beadbox a viable and effective tool in controlling the four factors (frequency level, intensity level, temporal information, and spatial information) of vibrotactile stimuli? b. Does logical mapping for the visual/auditory substitution apply in the visual/auditory/tactile domain? 2. What are the processes employed by novice vibrotactile artists in creating a vibrotactile composition? 3. Is the notation system of the Beadbox a workable system for vibrotactile art composition?

1.

3

1.2. Research Contributions
The research contributions of this thesis are:

1) A theoretical contribution where a visual-tactile sensory substitution model emerged from
a combination of auditory-visual, visual-tactile and auditory-tactile translations (See Figure 1).

Figure 1: Logical mapping in between sensory domains.

An algorithm was developed based on this model of sensory substitution for frequency, amplitude, space, time and duration factors of perceptual information/stimuli. The existing models of visual-auditory substitution for frequency and amplitude, visual-tactile

representation of the spatial dimension, and auditory -tactile for timeline and duration factors were used.

2) This model was instantiated as the Beadbox, which is a software tool to help create vibrotactile art with a timeline interface. The software allows control over the four vibrotactile factors: frequency and amplitude (or vibrotactile intensity) levels, spatial and

4

temporal information. The user interface of system has limits on each component control therefore a vibrotactile composition from the Beadbox focused on human tactile-visual perception.

3) The Beadbox uses a unique notation system for authoring vibrotactile patterns to allow users to design and record. It is necessary to have a specification for a media to be understood and shared by people. For example, the notation system of vibrotactile would be similar to the relationship of staff notation system for music, the colour palette for painting, materials for sculpture, or other method of specifications for different creative practices. There is no standardized notation system for vibrotactile, which has different domains to use than any other existing media. The composition process using the vibrotactile notation system is made independent from the hardware, while the characteristics of vibrotactile stimuli are provided in the fundamental unit, "Bead". The design of notation system focused on delivery of tactile information through visual perception.

4) The vibrotactile instrument digital interface (VIDI) is introduced as a device independent vibrotactile protocol designed to provide a standard mechanism for connecting and communicating between computers, vibrotactile instruments, output devices, and other related devices.

5

1.3. Thesis Outline
This thesis is divided into the following 5 chapters:

Chapter 1: Serves as an introduction to the thesis document. This section details the background and motivation to this research.

Chapter 2: Provides an in depth literature review of background work surrounding this research topic.

Chapter 3: Describes the technical aspects of the designed system in detail.

Chapter 4: Describes the evaluation method used to assess Beadbox, present and discuss the results of the user study in relation to the research questions. In addition, the limitations of the research are presented.

Chapter 5: Presents the contributions of the research and conclusion. It also suggests some possible future work.

6

Chapter 2. Literature Review
In this chapter, I will first present a wide spectrum of literature related to the human vibrotactile system and its mechanics. I will then discuss the collective contribution of current timeline-based media software suites, interface design, media input/output protocols, and creative processes in developing a formal notational system for creating time-based art. There are a number of tools used to generate and compile time-based media such as music, video and animation and an existing body of work regarding timeline media composition and these editing tools. Since vibrotactile creations are time-based, these tools are examined in terms of their utility for common interface design strategies.

The use of the vibrotactile modality as a medium for creative expression is just entering the realm of possibilities as enabled by technologies such as the Emoti-chair [5], Gunther's Vest [7], haptic displays such as Tactons [8], Haptic touchpads [9], Haptic Mouse [10], Haptic Braille [11] [12], and the Haptic pen [13]. While the Emoti-chair is based on a concept of transforming and transmitting non-acoustical sound patterns into vibrotactile equivalents, known as sensory substitution [14], it has not been widely adopted by artists likely due to its complex usage prerequisites and its novelty. Finally, this chapter contrasts and compares various components and characteristics of several digital transmission protocols to assist in choosing the most appropriate mechanism upon which to build a unique and effective vibrotactile application layer software suite. When music entered the digital realm, new digital protocols such as Musical Instrument Digital Interface (MIDI) were created in order to translate the acoustic to digital music. In addition, new protocols were created such as .wav and .aiff for processing analog music into digital music. MIDI is different from .wav or .mov because it is a digital only format for music notation and instrumentation. MIDI has technical definitions for each music/audio characteristic so that digital music can be composed and played through any MIDI enabled device or played directly from a MIDI instrument. Analog to

7

digital audio formats such as .wav or .aiff translate the acoustic music events (such as timing) and frequencies directly. Currently, vibrotactile displays are a combination of analogue and digital (A/D) systems where digital, computer-based controls and signal processing techniques are used to allow users to use computers to drive analogue motors or voice coils (or other vibrating devices) in a tactile output device. In order to allow the analogue to digital process to occur, a communication protocol is required. For my thesis, it is important that the communication protocol be device independent to allow for a variety of possible analogue devices similar to other media A/D systems such as digital audio. By reviewing and comparing existing digital media protocols, insights can be gained regarding the development of a similar protocol for the vibrotactile media. Given the time lag between research and market availability, certain technologies fail to appear in reviews of academic literature. As such, it is important to cast a wide net in reviewing the technology literature, and web based resources have been used to supplement traditional sources of information from academic journals and conference papers.

2.1. Human Vibrotactile: System and Mechanics 2.1.1. Mechanoreceptor
The basic mechanics of the human vibrotactile system are made possible by mechanoreceptor cells in the skin, which transmit the frequency, amplitude, and duration of vibration applied to the skin into an electrical signal whose frequency and duration is preserved in the pattern of neural firing transmitted to the person's brain [15]. Four types of mechanoreceptor cells responsible for processing vibrotactile stimuli are present in the human body: Meissner's corpuscles (MC), Pacinian corpuscles (PC), Ruffini corpuscles (RC) and Merkel's disks (MD). Miessner's corpuscles are sensitive in the lower frequency ranges between 10 Hz - 50 Hz and are mainly found in the fingertips and lips (e.g., in the dermis layer of non-hairy skin) [16]. Pacinian

8

corpuscles number about 2000 in an adult body and detect vibrations in the 1 Hz to 1000 Hz range and are most sensitive at around 250 Hz [17, 18]. Merkel's disks help discern pressure, while RCs are most responsive to sustained pressure. Pacinian corpuscles are fast adapting and considered to be largely responsible for the detection of vibration, whereas MDs and RCs are slow adapting and not viewed as having a significant role [16]. Experiments have revealed that as PCs absorb more energy (due to stimulation) over time, they became less responsive [17, 19]. The target tactile system in my thesis is the PCs because they are intended to detect vibrations, and have the widest frequency range and distribution in the body. As such, there will be a frequency limitation of 1000 Hz for the composition tool created for my thesis.

2.1.2. Vibrotactile Perception
Many researchers have claimed that there are four essential parameters of a vibrotactile stimulation frequency, intensity (amplitude), spatial information, and temporal information [8, 18, 20]. Spectral coding with different types of waveform can also be considered as an additional parameter [21]. For each parameter of the vibrotactile stimulation, the coding of information to be transmitted cannot simply be based on ensuring a detectable stimulus [20]. Instead, researchers have recommended that intensity coding is limited to a maximum of four levels, while frequency coding is limited to nine distinct frequencies each at least 20% away from its neighbour (i.e., 60Hz, 72Hz, 87Hz, etc.). Other recommendations have include at least a 10ms gap between successive signals for temporal coding, and consideration of actuator density, which is high on hands, feet, and face, for location coding. There are a number of researchers who have investigated human perception of vibrotactile stimuli. Verrillo was an early vibrotactile researcher and carried out important fundamental work. He conducted a series of experiments in order to examine the relationship between vibrotactile stimuli and skin type, contactor area, frequency, spatial information, and vibrotactile perception.

9

Human skin can be classified into two types ­ hairy skin and non-hairy skin (glabrous) [22]. Skin with hair either has actual hair or hair follicles. Non-hairy skin (e.g. fingertips) has neither. Hairy skin is less sensitive to vibrotactile stimuli than non-hairy skin, and so a higher energy signal must be transmitted to hairy regions in order to achieve the same effect from those of non-hairy regions [22]. Experiments have shown that non-hairy skin is most sensitive at around 250 Hz, while hairy skin has most sensitivity around 220 Hz [23] and this must be taken into account when determining the frequency ranges used in vibrotactile displays. The human sense of touch, thus, is limited to discerning frequencies between 20 Hz and 1000 Hz with a peak at around 250 Hz, while the human ear can perceive frequencies between 20 Hz and 20,000 Hz [24]. Another issue is the contactor diameter and placement of the vibration devices. Branje et al. [25] claim that contactors over 100 mm in diameter are effective for producing distinguishable independent signals on human back, which is non-glabrous skin, but the researchers used a single contactor. Weinstein [26] discusses his findings of the human tactile spatial acuity using two-point discrimination experiments. These experiments assumes that two stimuli points can be distinguishable from one stimulus point but when the two stimuli are too close together, the brain will perceive them as one. This is known as the minimum distance (threshold) of the spatial discrimination ability of the human skin. From his experiments, the minimum distance threshold for the human back is 39 mm with 1 mm contactors [27]. Vibrotactile art composers must consider these factors especially when the audience uses different types of output devices. The arrangement of output should also be considered in composition. In case of the Emoti-chair, the contactor size and spacing is such that audiences can feel different levels of vibrotactile stimuli from each different contactor pair located on their back. While many experiments relating to human perception of vibrotactile stimuli are limited to testing on the fingertips several studies suggests that it is feasible to apply the vibrotactile stimulation to other skin areas. Mahns et al. [28] reported that there is a significant similarity on the detection thresholds between the fingertip and the forearm skin, although there is some a difference

10

between them. The researchers used a 4 mm diameter contactor and tested at 20 Hz, 50 Hz, 100 Hz, and 200 Hz on the fingertip and the forearm. Higher frequencies were not tested. Several studies warned about the negative effects of low frequency vibration on the human body. Studies on the effect of vibration frequencies below 100 Hz indicated that frequencies less than 20 Hz might cause discomfort to humans [29]. Although the results differed for different levels of intensity and duration, lower frequency vibrations might cause tiredness, restlessness, irritability, and somnolence [30]. For the temporal coding, Pockett [31] concluded the average perceivable duration of vibration felt by the human skin can be around 80 ms not 500 ms as originally proposed by Libet [32] Cohen et al. [33] conducted further experiments to find how the duration of vibrotactile stimuli influences frequency discriminability. The results indicated that the minimum duration of vibration to determine tactile presence is 50 ms. Discrimination ability dropped sharply after 30 ms. Therefore, these suggested that a minimal vibrotactile notation unit must last at least 50 ms (0.05 seconds). There are several studies which examined the detection of variations in tactile stimuli intensity. In most studies concerning tactile intensity, the unit of intensity was expressed in decibels 1 (dB). Harris et al. [34] found that a human perceives an increase in frequency when there is an increase in vibration amplitude. Goff [35] conducted experiments to examine the human tactile system for vibrotactile frequency discrimination for two different intensity (amplitude) levels: 20 and 35 db. He found that participants perceived lower frequency vibration more intensely than higher frequency when intensity remained the same; thus claiming that this effect would help people better differentiate frequencies. Conversely, an increase in intensity reduced the ability of frequency

1

A measurement used to compare the ratio of intensities of two signals [147].

11

differentiation. Verrillo [36] investigated the subjective magnitude of vibrotactile perception from intensity change. The result suggested that the growth of sensation is proportional to the physical intensity. Figure 1 displayed the contours of equal sensation magnitude for vibration in different decibels, which arranged the intensity required to obtain a constant sensation level as a function of frequency. Verrillo [36] interpreted that the derived curves are similar to the equal loudness contours in audition.

Figure 2: Contours of equal sensation level at 250 Hz, with different intensity levels from 5dB to 55 dB [24, p.371]

There are no specific studies or published on a limit of vibrotactile stimuli that humans could perceive without discomfort. However, there is a specification for a recommended limit of auditory intensity where a human experiences discomfort. The European Organization for Nuclear Research (CERN) [37] suggests that any noise above 60 dB would cause a human to be uncomfortable. Similarly, the World Health Organization [38] suggests reducing community or life noise to below

12

50 dB, since any noise above 50 dB can cause annoyance to people. For this thesis then, I suggest a maximum level of intensity of 40 dB in order to avoid causing harmful effects from vibration. Vibrotactile artists may need to consider these factors especially when the audience uses different types of output devices. The arrangement of output should also be considered in composition. In the case of the Emoti-chair output display used in my thesis, the contactor size is suitable to transfer effective vibrotactile stimulation and audiences will feel the vibrotactile art on their back [14]. In designing a physical system such as the Emoti-chair, it is important that vibrotactile stimuli occur within the perceivable frequencies of 20 ­ 1000 Hz and that frequency differences be at least 40 Hz in order for the human skin to distinguish the various stimulation occurrences [39]. The frequency, amplitude and stimuli detection time factors are also crucial when developing a notational system and corresponding software tool to control a vibrotactile output system such as the Emoti-chair since the lower and upper boundaries must be defined to work with the human tactile perceptual system. Thus, for my thesis these tactile limits are defined as: 1) frequency limits will be between 100 Hz to 1000 Hz; 2) amplitude range is below 40 dB, with a flexible control provided.; 3) the minimum notation time unit is 50 ms; and 4) composers of vibrotactile art needs to consider the minimum contact distance, which can vary for different contactor size but the average suggested distance is about 40 mm. Displaying these maximum and minimum values and assisting artists in understanding how to working within them is one goal of the compositional tool developed for my research.

2.1.3. Creative process and vibrotactile
The Creative Processes paradigm in conjunction with vibrotactile in Human Computer Interaction (HCI) has created a rich source of literature, including the design of various systems in a wide range of products encompassing the entertainment, gaming, and etc. This section will

13

provide a review of the general guidelines for designing creative process support tools, of some examples of existing tools and of other existing vibrotactile authoring tools. There is some research that is related to creative process support tools. Shneiderman [40] focused on how to create or develop a system which can support the creative process of humans. Creativity takes many forms such as paintings, music, plays, etc. while it can also be part of the design culture of engineering, architecture, software development, and user interface design. Candy and Edmonds [41] suggested that it would be necessary to have observation and interaction with creative users over the long term in order to maximize the usability of the system, since quantitative metrics of creativity are difficult to obtain. Therefore, any creative support tools need to be evaluated with user studies with not only quantitative methods but also qualitative methods. Resnick et al. [42] focus on `composition tools', which they define as a system and environment that people use to generate, modify, interact and play with. They suggest some design principles for creative process support tools as follows: 1) Support Exploration: it is important to allow users to try out different alternatives. 2) Low Threshold, High Ceiling, and Wide Walls: Tools should make it easy for novices to get started but also possible for experts to work on advanced projects. 3) Make It Simple 4) Choose Black Boxes Carefully: One of the most important decisions is the choice of the primitive elements that users will manipulate. 5) Balance user suggestions with observation and participatory process. 6) Iterate for improvements. [46, p.2-11]

The vibrotactile composition tool developed in this thesis is a support tool to help vibrotactile artists express their creativity with vibrotactile art. The design process of the system should carefully consider these principles and conduct user studies for evaluation and improvements.

14

There exists several haptic 2 authoring tools that allow for vibrotactile composition of some sort. The Hapticon Editor [43] offers users a way to create a "hapticon", which is an icon that can provide users with a single vibrotactile stimulus in order to communicate a simple idea similar to visual/auditory icons. A second system, the posVibEditor [44], allows prototyping for vibrotactile pattern by using a timeline with graphical representation of waveforms. Figures 2.a and b show the screenshot of the two editors. These two editors have a similar display system, which allows users to edit the frequency patterns of the stimulus in detail. Notice that the multichannel timeline interface uses sample clips of frequency patterns with different durations created in the pattern editor.

Figure 2.a: The Hapticon Editor has a frequency pattern editor and other simple buttons [47, p.3].

Figure 2.b: The posVibEditor consists of three parts: a) manager; b) pattern editor; and c) multichannel timeline interface [48, p.4].

The VibScoreEditor [45] is a vibrotactile authoring tool which uses a western music notation score as the user interface where each musical note has a single frequency associated with it. However, it can be complex to use if the user does not have background knowledge of music composition structure and format. Also, the researchers of the VibScoreEditor does not review the

Haptic was defined as "A tactile feedback technology which recreates the sense of touch by applying forces, vibration, or motions to the user" [142]. Haptic field includes the vibrotactile system but is not limited to it. The authoring tools reviewed in this section mainly focused on a vibration editor, which has similar characteristics of vibrotactile.
2

15

human vibrotactile perception, nor conduct any user studies so it does not have a proof to be a good usability. The minimum duration of the note is about 1 ms, which is far less than 50 ms so it will be hard for a human to perceive. Similarly the frequency range does not have a maximum limit meaning that composers could use frequencies that are not detectable by the skin. For example, the sample levels used the note value 9 has 4500 Hz, which is beyond the level of vibrotactile perception. Figure 3 displays the screenshot of the VibScoreEditor.

Figure 3: A screenshot of the VibScoreEditor with three different windows opened up: a) displays the overall composition; b) shows the clefs manager; and c) allows to set frequency values and patterns [49, p.306].

The last vibrotactile authoring tool to be reviewed is the TactiPEd [46]. It provides a visualization of spatial information as well as frequency, duration and intensity in histogram format via a timeline. As seen in Figure 4. the column on the left supports the visualization of output orientation, which can be rearranged by users to fit their output device. Each sequence has its own visualization for its output, which is coloured in a different hue. However, there is no visualization for the intensity level and the timeline measure is difficult to match with the actual frequency samples.

16

Figure 4: Screenshot of the TactiPEd. 1) file navigator; 2) shows the timeline and actuator map; 3) sequence number labels; 4) actuator visualization; 5) frequency samples display and 6) actuator controls [50, p.8]. These existing authoring tools have different approaches to their interface designs. Each of them has advantages and disadvantages. The vibrotactile composition tool to be developed in this thesis project will consider the advantages provided by these tools and attempt to avoid their pitfalls.

2.1.4. Vibrotactile Output
There exist several vibrotactile output devices, which allows users to feel vibrations that would result from the compositions produced by the vibrotactile authoring tools. Karam et al. [14] developed a sensory substitution vibrotactile interface called the Emoti-Chair. The Emoti-Chair had a chair form to display tactile feedback from sound information. The system allowed vibrotactile perception of auditory music using voice coils as the vibrotactile actuators. The voice coils were

17

arranged in eight channels where each channel consists of two voice coils running the length of a chair (as seen in Figure 5). The voice coils then output the vibration that the audiences could feel while seated in the chair.

Figure 5: The Emoti-Chair with its cover on and off [14, p.433].

Another vibrotactile output device was the vibrotactile composition suit. Gunther and O'Mondrian [7] created a suit that could be used in vibrotactile music composition. The suit was a full body stimulator with thirteen transducers worn against the human body (see Figure 6) Similar to the Emoti-Chair, the suit used flat speakers as actuators. The high frequency transducers positioned as close to glabrous skin as possible and, a low frequency woofer was placed at the back of the body. The researchers used MIDI as the composition interface, however they suggested that it remained awkward and better compositional tools must be designed. For this thesis project, the Emoti-Chair was used in the user study, which allowed the participants to test their compositions by simply sitting in the chair.

18

Figure 6: The spatial arrangement of transducers (left) and the vibrotactile composition suit (right) [7, p.9].

2.2. Timeline Interface
Time-dependent data can be defined as data indexed with respect to time [47]. Vibrotactile art is time dependent in that is changes over the duration of a piece, similar to how music changes over the duration of a song. Each epoch of time contains a piece of vibrotactile information. Thus, digital manipulation of a vibrotactile information is also semantically time based, and a compositional tool should have a timeline based interface to reflect the time-based nature of the stimuli. However, the visualization process of time-dependent data can be difficult, especially when there are other variables that may change over time. For example, the vibrotactile needs to have at least three variables to visualize including frequency, intensity level, and temporal location. Creating a visualization that shows these elements and their links to each other is a more complex interface than a single one-to-one mapping for a single variable. Muller and Schumann [47] suggest that important elements to address when considering time-based data visualization are:

19

A well designed visualization can aid in answering the following questions for unknown temporal data:         Does a data element exist at a specific time? (Existence of a data element) When does a data element exist in time? Is there any cyclic behavior? (Temporal location) How long is the time span from beginning to end of the data element? (Temporal interval) How often does a data element occur? (Temporal texture) How fast is a data element changing or how much difference is there from data element to data element over time? (Rate of change) In what order do data elements appear? (Sequence) Do data elements exist together? (Synchronization) [51, p.737]

User interaction with time-based data often requires complex user actions such as multivariable manipulations and is typically associated with a high learning curve for the user to become proficient [48]. Timeline based editors have a rich user interface (UI) in order to provide sufficient controls for manipulating the elements over time. These are domain agnostic tools with the only commonality being that the data is indexed on a temporal basis [49]. There are a plethora of tools to assist this process, and many of which are in the open source domain. For each use, these tools can be divided into three categories of media: audio, video, and animation. In this section, I present the characteristics of existing media editors that employ a timeline based interface and media, and identify what is common and what is unique in each. This is important since vibrotactile art is a time-based media and the advantages from other media authoring tools will inform the development process of the vibrotactile composition tool.

2.2.1. Audio Editors
For professional sound editors, a Digital Audio Workstation is an essential tool which runs off a local workstation (i.e. Audacity, FL Studio, Cakewalk, etc.) [50, 51, 52, 53]. Many newer audio editing tools run on remote hosted servers using cloud technology, and thus allow users to save their data online and interact with remote users through the same hosted portal [53]. The interface functions of these tools usually include the basic tasks of editing, sound level adjustment (e.g.,

20

volume, gain, etc.) and automation, file format conversion, multitrack editing and audio filtering effects. Audio editing tools typically use visual equivalents to paper-based or physical musical devices such as piano rolls, timeline visualization, a symbolic notation system such as notes and rests, and/or the traditional western music stave [51, 52]. Figure 7 depicts an audio editing tool with western music notation system.

Figure 7: Screenshot of Cakewalk: an audio editor with traditional western music notation (with stave and notes) display and piano roll display [52]. Major user actions can be accomplished with non-text interaction such as clicking on buttons, drag and drop actions, and scrolling to navigate. Text input has been made almost redundant, except in instances of file naming or title manipulation, or when the user needs to adjust values with fine grained accuracy [54]. The interface control components may also be haptic/physical based design. For example, there are knob buttons for controls that have the appearance of analogue buttons which function as it would work in physical device (ex. Figure 8). These functionalities typically aim to reduce the learning curve for the user.

21

Figure 8: Screenshot from Cakewalk [52]: The user interface for EQ control with knob visual design. The knobs would increase/decrease the value when user turns clock-wise/counter clock-wise.

Many digital audio editors allow files to be saved in various digital audio formats including WAV, MP3, MIDI, etc. In order for a digital audio file to be played for an audience to hear, digital audio editors allow a direct connection to input/output devices which can interpret/play the various digital audio file formats. For example, MIDI keyboards can be connected as an input device to record what is played as a MIDI file, and users may then convert the recorded project into WAV file. The difference between these file formats/protocols is further discussed in section 2.4. Lastly, there are some differences in between digital audio editors and MIDI editors. While both editors have user interfaces to provide functionality for audio manipulation, their visualization method is different. WAV editors often use the raw audio data, which are displayed as frequency spectrum graph. Figure 9 shows a screenshot of WAV editor with visualization.

22

Figure 9: A screenshot of sample wave editor, Audacity [50].

MIDI editors use a piano roll display or the western music stave to visualize the basic elements of music notation such as notes, key signature, rests, etc. Another difference is with the control functionalities. Since MIDI protocol has a definition for each note, it is possible to create a defined visualization symbol per a note. Therefore, it is possible to manipulate any one defined note (i.e. setting pitch, timbre, etc.). Figure 10 is the screenshot of MIDI audio editor.

23

Figure 10: A screenshot of sample MIDI editor, FL studio [51].

2.2.2. Video Editors:
Timeline based video editors allow the user to access a multi-panel control displays to manage moving images and sound data (i.e. Adobe Premiere Pro, CyberLink, iMovie, etc.). This design strategy takes advantage of the notion that user interface (UI) components with similar functionalities work most efficiently when grouped together. Advanced video editing tools are collated under a specific window, which may be resized and relocated on the screen [55, 56]. The UI reflects user maturity in this application domain space and even freeware tools are competing in terms of intuitive tool placement and performance with their competitors [57]. Figure 11 is a screenshot of the sample video editor.

24

Audio Edit Panel

Output Display

Multi-Timeline Display

Figure 11: A screenshot of sample video editor, Adobe Premiere [56]. In the multi-timeline display panel, notice that the video clips are displayed with its thumbnails while audio samples are displayed with the frequency graph.

Certain common UI features appear in most video editing tools. One of them is to have a multiple timeline display along the bottom half of the screen [57, 58, 59]. This type of timeline UI usually combines the audio track with the video track, which enables the user to match time cues. The top left hand side is usually reserved for the actual video being edited, while the top right hand side of the screen is where the user is allowed to make modifications (e.g., drag and drop new clips, etc.). Video editing typically includes audio and video manipulation, which may be complex because of the connection and synchronization between the two media sources. As a result, the UI is very flexible allowing users to edit each media source separately or together. However, this type of functionality increases the complexity of the UI because there are many different elements to control. Thus, these editors typically allow users to hide, lock or show various media and/or editing functions.

25

2.2.3. Animation Editors
Animation editors (i.e. Adobe Flash, Animatron, Maya, etc.) involve the addition of single or multiple audio/video effects layer(s) to an existing (preloaded/background) video clip [60, 61]. The animation editing process includes advanced filter effects such as a flashing light pixel, fade in, and fade out, as well as many other audio-visual effects. The UI of many animation tools tends to be focused on the functionalities rather than a simple user interface, especially for 3D animation tools [62, 63]. This is due to the special characteristics of 3D animation, since they require accurate values and controls for the animation model. Figure 12 is a sample screenshot of 3D animation editor.

Model editor toolbar

3D model display

Navigation tools
Timeline and frames.

Figure 12: Screenshot of 3D animation editor, Blender3D [63]. One common feature in timeline animation editors is the ability to add new timelines for each animation effect layer, and thus, the mixing of two audio clips with three video layers will require a total of five layers that the user will be able to manage. Figure 13 is a screenshot of a sample animation editor showing these layers.

26

2D edit tools

Current frame display

Timeline and layers

Figure 13: A screenshot of sample animation editor, Adobe Flash [64].

After reviewing various timeline editors, there are five essential, common functionalities of such tools that are used in the vibrotactile interface in my research: 1. Record and play back: Basic multimedia tools to record and to play are essential and fundamental as they are used to navigate the time based media whenever users want to play and create their compositions. 2. Timesynchronized data sources: A timeline's essence is the ability to get all the media sources to work in tandem. 3. Annotation of data: In time-based media files, there are metadata or tags to provide additional information such as lyrics in music or subtitles in video. In case of the vibrotactile art, spatial information of the output arrangement and tempo falls in this category since they are essential information in vibrotactile composition.

27

4. Multi-track timeline interface: This will facilitate the addition and manipulation of more than one unique media source which then need to be managed and collated. 5. Object Centered Notation System: Each tool has its own visualization of the fundamental component 's notation. For example, FL studio uses piano-roll-like horizontal bars to visualize music samples, video editors often use thumbnails to represent clips, and etc. By convention Western music is represented using the Western music notation system of notes, rests, key signatures and staves; digital audio is represented by frequency and amplitude information, animation is represented by images in key frames, etc. Therefore, the user interface of vibrotactile composition tool also needs an object centered notation system to visualize the factors of vibrotactile.

2.3. User Interface Design
User Interface Design (UID) is the design of user interfaces for a system with the focus on maximizing user experiences [65]. UID is often employed in the development of new interactive products and services [66]. The methodologies used in UID have gained much academic legitimacy over the years for understanding and evaluating how the design process and the selection of design methods will meet user needs. Some commonly used methods include task analysis [67], GOMS technique (Goals, Operators, Methods, and Selection rules) [68], heuristic evaluation and user studies [69] that can be used in academic research as well as practical approaches taken by designers. In this section, a general literature review of user interface and interactive design will be presented.

28

2.3.1. HCI and UI
Human Computer Interaction (HCI) is the study of interactions between humans and computers [70]. Humans have limited capacity for information processing [71], and this has important implications for the design of the information processing systems they use. Computers are comprised of various elements, each of which affects the user in different ways. Interaction is defined as the dialogue in between the data input by the human user and the output from the computer. Human-computer interaction takes place at the user interface, which is the environment setup for communication between computer, and human perception and recognition [72]. The user interface consists of hardware and software tools where input tools include the computer itself as well as a keyboard, mouse, touch screen, infrared (IR) sensor, camera (computer vision), etc. Output tools include visual, haptic, sound, and vibrotactile stimulation. Software tools include programs and applications that are created by computer programmers to allow humans to carry out tasks with computers. The user interface has an important role to play as it exists in between the hardware and software in order to support the efficient and effective communication with human users. A poor quality user interface can cause difficulties accomplishing tasks, frustration and ultimately abandonment of the task or software tool altogether. A poor user interface can cause economic damage (i.e. increased labour costs due to inefficient work), while a good user interface can improve the work flow of its user, support reduced fatigue, and economic profit due to efficiency [73]. The role of the user interface is to help users interact with the computer by maximizing its usability. The term usability was coined in the early 1980s, replacing the term "user friendly " [74]. The ISO/IEC 9126-1 standard defines usability as, "the capability of the software product to be

29

understood, learned, used and attractive to the user, when used under specified conditions"3 [75] Further, the definition for ergonomics of human-computer interaction can be found on the ISO924111 which defines usability as "the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use" [76]. Usability is the major factor which influences whether the user (human) interface is effective or not. Therefore, it is also important to measure usability of a system to determine whether the user interface is usable, and if not where and how to make improvements. The theoretical design approach for this thesis project is User Centred Design (UCD), which focuses on the needs and limitations of the end users of the system at each stage of the design process [77]. UCD is a cyclical process with multiple stages of system development involving analyzing needs and context, creating a solution and evaluating it with users in small iterative cycles as shown in Figure 14. Designing the user interface is not complete until it meets the usability requirements for the solution and the goals and needs of users.

3This

is an older version of definition, which now is replaced by ISO 25010:2011 [75]. This thesis uses this older definition because it is the definition used by many academic reference sources. The newer definitionof usability is "A set of attributes that bear on the effort needed for use, and on the individual assessment of such use, b y a stated or implied set of users" (Ch.4.3) which has five sub characteristics: Understandability (Appropriateness recognizability), Learnability, Operability, User Interface Aesthetic, and Usability Compliance (User Error Protection and Accessibility).

30

Figure 14: The human (user) centred design cycle [77, p. 589].

Many researchers have focused their efforts on determining appropriate usability constructs. Table 1 displays some of these standardized usability constructs as described by various researchers and the ISO standard [78]. The empty cells indicate that a particular construct is missing from that source. The table suggests that there are three common constructs among all references: 1) efficiency; 2) learnability; and 3) satisfaction. This suggests that the evaluation of the usability of the vibrotactile user interface should include these consider at least the three common attributes in development process to have a good usability.

31

Table 1: Usability constructs of various standards [78, p. 161].
Shackel (1991) B.Schneiderm an (1992) J.Nielsen (1993) Preece et al. (1994) ISO 924111 (1998) Constantine & Lockwood (1999) Efficiency in use Learnability ISO/IEC 9126 (2001/2011)

Effectivene ss (speed) Learnability (Time to learn) Learnability (Retention) Effectivene ss (Errors) Attitude

Speed of performance Time to learn

Efficiency of use Learnabilit y (Ease of learning) Memorabili ty Errors/safe ty Satisfactio n

Throughp ut Learnabili ty (Ease of learning)

Efficiency

Operability Learnability

Retention over time Rate of errors by users Subjective satisfaction

Throughp ut Attitude

Satisfactio n (Comfort and acceptabili ty of use)

Rememberabil ity Reliability in use User satisfaction

Understandabil ity Usability Compilance Attractiveness

The attractiveness construct was recognized at a later date in the history of usability measures. It was defined as visual appeal/aesthetic point of view to attract its user; extending from the satisfaction attribute [79]. Nielsen also included aesthetic design in his usability heuristics for user interface design [80]. The visual appearance not only assisted users in focusing on the system but also helped them recognize certain components, providing better readability, and even better work flow organization [81]. Researchers [81] examined the relationship between user perception on the aesthetic appeal of a system and the usability of it, and found that there was a strong relationship (r=0.71). This relationship suggested there was a high value placed on appearance by users and that interface designs must take this construct into consideration. As such, the vibrotactile composition tool developed in this thesis had a focus on visual appeal in order to better support its usability. There are many tools and ways to evaluate the usability of a system. One of these tools is the System Usability Scale (SUS) [82]. The SUS uses the usability constructs of ISO 9241-11, measuring effectiveness, efficiency, and the satisfaction [76]. The SUS is a ten-item Likert-scale questionnaire [83]. Brooke [82] calls the SUS a "quick and dirty" method to measure usability yet it is robust and appropriate. The design of SUS recognizes that there is no absolute measure of

32

usability since it depends on the context of the system and user pairing. Therefore, the SUS establishes general and common evaluation categories. Brooke [82] also claims that the SUS is a reliable, low-cost usability measuring tool that can be used globally. User interface design has an important role in the relationship between humans and machines, especially for software tools and applications. As a human computer interaction researcher, one should take considerable time designing the user interface, while ensuring end user concerns are addressed. The Beadbox design processes were based on the foundation of user-centred design in an attempt to make the Beadbox tool usable, acceptable and functional for users.

2.3.2. Visual Perception in Interface Design
Visual perception is the ability to interpret meaning from visual information that humans gain through their sight [84]. Seeing is a process where light/images are processed through the eye structures and represented in memory and the visual cortex, but without meaning [85]. The process of cognition then makes meaning from the images stored in memory [86]. The other sensory systems (e.g., auditory, tactile, olfactory and taste) follow a similar process which makes sensory substitution possible [87]. Sensory substitution can appear across sensory systems, such as visual to tactile or auditory to tactile. Each input stimulus can be substituted by another stimulus because the brain can perceive equivalencies for alternative stimuli. For example, a bright object may be seen as having similar "loudness" properties of sound. A visual metaphor also known as analogical juxtaposition work is a graphic and/or visual image or representation of a person, place, thing, or idea that conveys a specific message [88, 89]. It has been used extensively in product and service advertising. Adding text to images is a common technique to reinforce the visual message being conveyed. Product designs based on visual metaphors may also influence user behavior. For example, interface components such as buttons,

33

labels, icons or other graphics can be representative of certain functions or objects and the visual representation conveys that functionality. Figure 15 is an example of this case; a real garbage container can be represented in a computer interface as a virtual trash can. Like throwing away garbage which is removed from the people's presence by garbage collectors, computer users can place any files (using drag and drop, delete command, etc.) into this icon when they want to remove/delete those files.

Figure 15.a: Visual metaphor of a recycle bin used as an icon4.

Figure 15.b: A recycle bin in real life.

There are other ways that visual information can be used to express non-visual concepts. One of the common way is the map. A map is a visualization of geographic/topographic information, which the reader can perceive and recognize the match between reality and the visual information. Another popular way of using visual cue as expression of non-visual object is the graphs. Graphs are used to compare or to visualize relationships between quantitative information.

4

Recycle bin picture is copyrighted by epsos from Flickr (https://www.flickr.com/photos/epsos/5575089139) and Windows icon is copyrighted by Microsoft Corp.

34

Auditory-visual perception forms the basis of work conducted in mapping a system of colour to musical notes [90]. Visual colour percepts are defined as hue (main wavelength), saturation (amount of hue), and light (amount of brightness). Colour abstraction of musical notation has been the topic of previous research [91]. A musical octave can be equally divided into 12 half steps which can then be associated with the equal spacing of twelve hues of a colour circle. In addition, complementary hues (such as Red-Green pair, or Yellow-Violet pair) are placed directly across from each other. Hence, there appears to be a one-to-one mapping between music notations and a colour representation. Maryon [91] came up with the hue-husic tone mapping which are displayed in Figure 16. Table 2 displays some of the similar colour/frequency of light to chromatic music scales/audio frequency mappings sorted by different researchers.

Figure 16: Colour circle, and chromatic musical scale circle [91, p. 104].

35

Table 2: Correlation of light frequency/colour hue and audio frequency/music scale [91, p. 104].

Psychological studies have shown that the mapping of colours and sounds may have cultural connotations, which impact decisions with respect to how colour to sound mapping may be used, and its relevance in globally-distributed products [92]. For example, it has been shown that participants tended to associate higher sound pitch with lighter colours [92], which may facilitate systematic mappings between lightness of colour and pitch. This correspondence may also extend to rising and falling melody since change of melody also means change in pitch (audio frequency) over time. Giannakis [90] and Marks [93] have found the same result. Figure 17 depicts the relationship between pitch (audio frequency) and loudness (sound intensity), as well as lightness of colour and saturation. The results suggest that it is possible to also use brightness of colour for visualization of frequency but there may be interference with the visualization of intensity. To resolve this confusion, a vibrotactile notation system can use a different parameter such as size an object to visualize the intensity.

36

Research on the effect of visual looming 5 stimulus on human infants found that they preferred to look at looming visual objects when presented with looming sound and similarly with receding visual-audio stimuli [94]. The effects were selective for structured sounds and not random noise, but the results suggested that there existed a positive correlation between audio and visual stimuli. Zhou et al. [95] investigated how auditory stimulation and visual stimulation related. They claimed that the object looming towards an observer would trigger the brain to elicit an escape response because the brain would process the information as an impending collision. The researchers also claimed that the change of distance from a sound source to an observer would signal the brain in a similar fashion; a closer loud sound will signal the brain as a warning.

Figure 17: Proposed space for the associations between pitch-light intensity and loudnesssaturation [90, p. 63].

There were also studies examining the relationship between musical timbre and visual information. Ward et al. [96] defined timbre as "the aspect of sound that enables a perceiver to

5

According to Merriam -Webster dictionary, to come into sight in enlarged or distorted and indistin ct form often as a result of atmospheric conditions [145]

37

judge that two sounds are different when they are otherwise matched for pitch and loudness" [p. 3]. Researchers found that the timbre can be expressed as different colour hues. Adeli et al. [97] claimed that there is a strong association between timbre of sounds and visual shapes. Soft timbres were associated with blue, green or light grey rounded shapes, while harsh timbres were associated with red, yellow or dark grey sharp angular shapes. This timbre differentiation of human perception might also be applied to visual-vibrotactile cues. It was important that my research was as inclusive of as many people as possible. The colour choices for the system interface needed to be as accessible to people of different colour vision types. As a result, the Color Universal Design (CUD) [98] was selected from which to choose accessible interface hues. The chosen colours were to be used to distinguish different source of output (tracks), which could be arranged for different types of output device. Throughout this section, we reviewed what human computer interaction is, and how user interface takes a role in the usability of a system. It was the role of researchers to maximize the usability of a system so that users were able become efficient, satisfied, and learn the functionality and operational characteristics as quickly as possible. In addition, the design process must be user centred and consider how users perceive the interface components. The vibrotactile composition tool and its notation system discussed in Chapter 3 of this thesis attempted to follow a user-centred design process using concepts from sensory substitution.

2.4. Media Protocols
Digital audio files (e.g., .wav, .midi and .mp3) are organized and structured so that a media player can interpret them. If the player "speaks" the language that the files are recorded in, it can reproduce the audio signal and make sound. If the format or protocol is different, it results in neither music nor sound. Hence, understanding the major audio file formats is essential prior to designing a sound based tactile system. In the following sections, I will examine the following file formats:

38

AIFF [99], WAV [100], MP3 [101] and MIDI [102] in order to motivate the unique application protocol that was developed for the vibrotactile art composition files.

2.4.1. AIFF
Audio Interchange File Format (AIFF) is an early file format standard for sampled sound, which Apple Inc. developed in 1988 [99]. It is an uncompressed file that allows for multiple files to be streamed from disk to application. It uses a lossless 6 file format and as a result produces larger size files than lossy 7 formats. While an .aiff file holds audio data, it can also include loop point data and musical notes that may be used by hardware samplers and musical applications.

2.4.2. WAV
Waveform Audio File Format (.WAV) is a raw file format which is the most detailed and richest of the available formats since all the data can be recorded at the chosen bit rate and sampling speed without a compression scheme [100]. However, WAV formats use large amounts of memory (i.e. a 4-5 minutes of WAV sound can consume 40­50MB of memory) [103].

2.4.3. MP3
MP3 is the third layer of the MPEG file format, which is an audio coding format. The MPEG format was developed by the Moving Picture Experts Group and MP3 was part of the MPEG-1 and MPEG-2 standard. The MP3 is a compressed lossy file format with perceptual coding [104]. Perceptual coding uses psychoacoustic models to discard sound information which most humans

6 7

Lossless compression keeps every bit of data from original source; relevant data are not lost [146].

Lossy compression reduces the file size by eliminating redundant information, hence relevant data are lost [146].

39

do not hear (e.g. humans cannot sense frequencies below 20 Hz nor above 20 kHz). As a result, file sizes are much smaller than .aiff or .wav. Because of their small file size, MP3 files are ideal for listening on a computer or a portable player. They also can retain much of the quality of the WAV file. Aside from size, the other advantage of MP3 is that it is almost universally recognized [103]. Just about any media player or portable audio player can recognize and play an MP3 song, since it is n public ISO standard [105]. However it is not a popular format for artists, because its small size facilitates ease of illicit copying and distribution [106].

2.4.4. MIDI
In 1982, electronic musical instrument manufacturers were able to standardize a media interchange transport protocol, known as Musical Instrument Digital Interface (i.e., MIDI) [107]. MIDI allowed electronic music files to have a set of common attributes for the creation, movement and reproduction of music from one location (e.g., Company A's music instrument) to another location (e.g., Company B's playback device). MIDI became ubiquitous and relates to instruments (MIDI equipment), music files (MIDI file format) and the transport protocol itself (MIDI compatible). While its universal adoption is unparalleled, MIDI does not represent music or an audio file per se, but it is a set of instructions using definitions for a MIDI instrument. MIDI files are comparatively smaller than compressed music files (i.e., .mp3 or .wav files) [103]. MIDI can perform instrument stacking by having a multi-timbre process so that multiple tracks with distinctive types of instruments can be described in a single file. However, MIDI files have certain limitations. Since MIDI files contain music instructions (i.e. musical notes), the reproduced sound is dependent on the quality of the playback instrument, MIDI plug-in, or sound card. Hence, the MIDI file being recreated on a professional electric piano will be perceived very differently than that being played on a desktop computer with an outdated sound card.

40

I have chosen to use MIDI in the design of my vibrotactile notation system. MIDI defines the audio signal into digitized data information. For example, it expresses frequency information into pitch value as:

Where f is frequency, and d is frequency data value, 440 Hz is a standard Concert A, 12 is the semitones above the frequency and 69 is the number of semitones (e.g., nine semitones above middle C, which is 60) [108]. This then provides a simple conversion from frequency into digital data, which can also be used in vibrotactile audio frequency information transfer as well by taking the digital data (MIDI pitch value). By doing this conversion, it allows for the vibrotactile frequency to be decodable with a MIDI protocol. Similarly, the definition of each message can be reproduced to fit in the vibrotactile data transmission, since there are similarities in between audio and vibrotactile data. Another reason for the choice of MIDI for the vibrotactile protocol is the possibility of extending from the existing MIDI instruments to vibrotactile instruments. If the new vibrotactile protocol is designed based on MIDI, it will likely be easier to modify MIDI instruments to vibrotactile instruments. All these factors led me to choose MIDI as the transport protocol for this project. As such, a detailed review of the MIDI protocol is required.

2.4.4.1. Standard MIDI File (SMF) Structure
A file structure defines the basic rules of organizing and storing information/data in a computer file. In this section, I will discuss the basics of the Standard MIDI File (SMF) Structure and the closely related MIDI Tracks. The MIDI file is organized in chunks, and the organization, placement and management of these chunks defines the file structure (see Figure 18). The major SMF chunks are labelled as a header chunk and one or more track chunks. The Header Chunk

41

contains overall file information, while the Track Chunk contains track specific information where a track is a separately accessible and controllable location for housing recorded audio data [109].

Figure 18: Simple MIDI file structure [109, p. 47].

The standard SMF is big-endian and contains three sub parts of Track ID. The header chunk consists of a four ASCII letter mark (MIDI Track Header, "MThd"), a header length indicator, the MIDI file format, the number of tracks in the file, and a timing value specifying time division units (see Appendix A.1 for details about the MIDI header chunk byte format).

42

Similar to the header chunk, each Track Chunk consists of a four ASCII letter mark (MIDI track, "MTrk"), a track length indicator specifying the size of the track, and the track event data in variable length. Each track event consists of a timestamp called delta time and one of the three event types: Meta event, MIDI event, or System event. The delta time is a mark for each event, and represents the time difference between two events. Starting from tick 0, each event will have a timestamp. For example, given event A happened at tick 3 and event B will happen at tick 6. Then the timestamp of event B is 3 (6-3). The timestamp has a variable length to store a large amount of temporal data. To do this, the first bit of each byte indicates how many bytes will be following in the timestamp. Meta events are non-MIDI data of various sorts consisting of a fixed prefix, type indicator, a length field, and actual event data. A meta event has a constant format of byte message starting with 0xFF followed by meta type, variable length, and event data bytes. A MIDI event has various types of channel messages, which carry the actual action such as note-on to place a note, note-off for ending a note, and pitch bend for bending, etc. System event messages are not exclusive for channels but for the whole MIDI system in general (see Appendix A.2. for details about the Track Chunk and MIDI event byte formats.) Many SMF parameters are of variable length (e.g., delta time values) and use the least number of bytes necessary to store the value. This variable length value uses the lower 7 bits of a byte. The high order bit is a marker bit. All the bytes have this `marker' bit except for the last byte. The last byte has the highest order bit cleared. The bytes are arranged from most significant to least significant and from left hand side to right hand side.

43

2.4.4.2. MIDI Streams
MIDI files are composed of one or more MIDI streams. Each stream includes time information associated with each event. Various sound features, as well as timestamp and signature information, are also included. Track names and other additional data or information may be stored with the MIDI data. MIDI supports multiple tracks and multiple sequences.

2.4.4.3. MIDI Messaging and its Structure
MIDI devices communicate with each other via MIDI messages. These are structured packets, each containing specific type of information arranged in a specific order to form the message. Since MIDI is an industry standard, this structure is available to all manufacturers, engineers, and others. A MIDI message has two parts that represent the status and data. The Status Byte consists of 8 bits. The most significant bit (MSB) is set to 0. The next 3 MSBs identify the message, while the 4 least significant bits (LSB) define the channel (i.e., 16 combinations; one for each channel). There are two Data Bytes of which the most significant bit of the data byte is set to 0 (see Appendix A.2. for details and byte information)

44

Chapter 3. System Overview
This chapter provides a detailed overview of the process to construct the Beadbox. The purpose of the Beadbox is to facilitate the creation of vibrotactile interactive art by controlling four essential variables: (1) frequency, (2) intensity, (3) spatial distribution of the signal among the vibrotactile actuators and (4) temporal information. All of these components are the foundation of a vibrotactile signal and can be controlled by a user to produce patterns over time and space that can be felt by human skin [18]. The Beadbox proposes a unique notation system in order to allow users to control these variables and produce these patterns. Users can record a vibrotactile composition, play the piece while they are creating, edit the file, and save the finished piece. It also provides an output format that allows a composition to be played on multiple devices including specialized devices designed for vibrotactile output as well as sound-based devices such as speakers and midi instruments, which enables it to provide the essential information for the vibrotactile art.

3.1. System Design
The system design goal is to have an easy to learn and use method of creating a vibrotactile piece, particularly for novice users. Having a dedicated software application for vibrotactile composition means that users do not need to use other alternative software such as audio or video compositional tools in which a translation is required to convert audio/video formats into vibrotactile formats and methods. Neither audio nor video can represent vibrotactile because they have different characteristics and factors to consider such as spatial information of output. In addition, these alternative tools require background knowledge of their particular media such as sound or images and the need to think about how to modify them to fit a vibrotactile pat tern.

45

The Beadbox uses a unique notation system and assists users to perceive the information through visual elements. Each user interface (UI) component is designed to be user friendly by considering psychological research results application. The Beadbox not only supports the composition process but also has a unique file protocol and structure, called Vibrotactile Instrument Digital Interface (VIDI), which translates the Beadbox data into bits. The protocol gives definitions of signal elements o that the user can save and open a standard file, and opens up the other possibilities of a new vibrotactile instruments. Figure 19 shows the overall system diagram of the system.

While the Beadbox requires no special or additional accessories to run and create a vibrotactile composition, the user must install some tools to play their compositions on a hardware device. The Beadbox was implemented in Java 1.8 and used Audio Stream Input/Output (ASIO) API with the ASIO4ALL driver and jASIOhost library [110, 111]. The output device for the user study was the Emoti-Chair [14] with a FirepodTM connection.

Figure 19: System diagram of the Beadbox.

46

3.2. The Beadbox
The major requirement of the Beadbox development process was the acknowledgement that the notation system must symbolize the essential components of vibrotactile information rather than audio, visual or other stimuli. As the user interface for Beadbox was chosen to be based on a basic personal computer system with a visual display, the vibrotactile information and its manipulation had to be visualized. In this section, the major components of the Beadbox will be described.

As seen in Figure 20, the interface is based on a time-based media editing model as described by [47]. This display and editing strategy was chosen because vibrotactile art is also a time based media similar to other media forms such as music movie, animation, and etc.

Figure 20: The Beadbox Overview

47

3.2.1. Vibrotactile Notation
The notation design is the major component of the whole interface design. To build up a notation system, a basic information unit must be defined. The basic unit in this system is called a "Bead". A Bead is developed to deliver essential information for vibrotactile art composition. It is similar to a "note" in music notation, providing composers with a mechanism to express and record their ideas [112]. By having a Bead a trained artist would be able to interpret the composition as well as reproduction.

3.2.1.1. Development of basic informational unit: Bead
In order to devise a visualization of a Bead, the basic unit of the Emoti-chair output, a voicecoil, was used as an underlying metaphor as seen in Figure 21. A Bead then is represented by two concentric circles where the outermost circle is described by darker, heavier line.

Figure 21: Simple voice coil visual metaphor [88, 89].

48

3.2.1.2. Design of visualization of tactile variables of frequency, intensity and duration.
Prior research has been carried out to investigate models of sensory substitution where different properties of perceptual systems can be used to represent various physical parameters such as frequency. These models were used to inform the representation of the vibrotactile parameters. The first mapping that has been explored was colour as a visual representation of auditory frequencies [92]. The second model was the relationship between the visual brightness of a colour and the audio frequency [90, 91, 93]. Researchers did not find a discrete and well-defined relationship between the light frequency and audio frequency, but they were able to isolate a relationship between brightness of a colour mapped and audio amplitude (perceived as volume) as well as audio frequency [93]. They found that if a colour is brighter, then people tend to indicate that the sound stimuli would be higher pitch, and louder volume. If two variables have an overlapping information it is possible that users would have confusion, compared with a one-to-one mapping. Therefore, brightness (or the level of white in a colour) was assigned to visualize the frequency level of the vibrotactile stimuli. Figure 22 depicts a sample scale of the notation with different frequency level information.

Figure 22: Colour brightness gradation (dark to light as frequency increases) by sample frequency level.

49

Other researchers have found that there was a relationship between audio loudness and the size of a visual object. Louder sounds were seen to be related to the visual looming object by [94] where looming signals cause an avoidance response, resulting similar effect both in visual and auditory stimulation. Intensity represents how concentrated the energy is in given spatial limitation and can be written as Power Area

Intensity =

, where the most common units W/m 2 or W/cm2 [113]. For the Beadbox, the output intensity may be relative (subjective) to the operating systems volume control (in logarithmic scale), or the size of output actuator. The intensity coefficient interface has a range that scales linearly in the Beadbox from 0 to 100. As a result, I chose to use the diameter of the inner circle of the Bead as a way of representing amplitude whereby a smaller diameter would represent the lower intensi ty coefficient level. Figure 23 shows an example of the changes in diameter of the inner Bead circle showing lowering intensity coefficient.

Figure 23: Sample Bead showing lower intensity coefficient.

50

3.2.1.3. Time and duration.

The last factor is timing information. To control the timing of the note, the user can drag and drop the Bead in the desired location along the timeline (details of the timeline and tracks are described in Section 3.2.2.1).

The default duration of a Bead is set as 50 ms, which is the minimum stimulus duration a human needs to discriminate different frequencies [33]. From this point, the duration of each Bead can be controlled by connecting a start point to an end point. As seen in Figures 24.a-c, duration is expressed as the lines drawn between two Beads placed at different points on the timeline.

Figure 24.a: A note with a certain duration changing from a low frequency to high frequency.

Figure 24.b: A note with a certain duration changing from one track to another track.

Figure 24.c: A note with certain duration from a low intensity to a high intensity.

51

This way, the Bead is interpreted and expressed as a single connected Bead which will be expressed without pause or cut. This visualization follows the classic technique of time-dependent data visualization, which gives the user information about duration with the length of the note itself [47]. Duration can be assigned to two Beads on different tracks or two notes from different intensity or frequency levels. When there is a connection between different tracks, there will be constant decrement at a linear rate on either frequency or intensity over time from the start Bead to the end Bead (Figure 24.b). Similarly, a connection between two different intensities or frequencies will give a transition as well (Figure 24.a and Figure 24.c).

3.2.2. User Interface
The user interface (UI) was designed to interact with users of the Beadbox while composing a vibrotactile art piece. Efficiency of the composition process was the first factor to consider for UI development. The Beadbox has four main UI components. Bead Player is main editor with timeline where users can place the Beads, manipulate, and play. Bead Palette is where users can create a Bead to place by using its sliders for frequency and intensity level. Stimulator Arrangement visualization panel is an interactive frame where users can drag symbols around to match with the output arrangement. Lastly, Overview Panel is to provide a simple click navigation as well as summarization of current composition. Figure 20 depicts each components with the label on it. In this chapter, it will describe in detail about each major UI components.

3.2.2.1. Bead Player

As depicted in figure 25, the Bead player is main composing space where users can place their notes created from the palette. The play frame design follows the basic interface of existing media editors or players such as Window's Media Player, Apple's iMovie/Garage Band, Adobe's

52

Premiere, etc. The play frame is divided into tracks where the number of tracks corresponds tothe number of output channels (i.e. if there are eight channels of output then there will be eight tracks in the play frame). Each track gets a designated colour picked from the Color Universal Design (CUD) suggested scheme [98]. The design of the tracks follow the recommended timeline visualization method as suggested by [114]. Users can place Beads onto any track. Once a Bead is placed on a track it will be assigned the colour corresponding to that track.

There is a measurement bar at the top portion of the frame, which displays the time in milliseconds from 0 ms to 1000 ms. The bar moves along the timeline when the play button is pressed. Whenever this bar reaches a Bead, the output visualizer will show that Bead colour (see Figure 25). The play frame is scrollable left to right or vice versa, which is designed to assist users in navigating compositions that are longer than one page and/or that have more tracks than 8 channels. Given that a single Bead has a duration of 50 ms, a page can take 20 Beads max or represents a 1.0 second portion of the total composition.

Figure 25: The Beadplayer frame.

53

3.2.2.2. Bead Palette
A palette interface allows users to control/assign the frequency and intensity level of a Bead. As shown in figure 26, there is a slider for each control where users can dynamically see the changes they make. The intensity coefficient can be adjusted in linear scale on top of the sound control in operating system. The intensity ranges from 0-100 with linear fashion as labeled because the human perception of volume is in linear fashion while intensity level (or loudness) has logarithmic fashion. The frequency level ranges from 100Hz to 1000Hz in increments of 1 Hz. The frequency range represents the range of the human perceptual system for touch, which has been reported to between 20Hz to 1000Hz with a maximum sensitivity of around 250Hz [22]. Instead of 20Hz as the lower limit of the frequency range, 100Hz is chosen because of the possible negative effect of the lower frequency stimulation (see section 2.1.2). When user changes frequency value in the palette, the Bead will still be in grayscale to display its brightness. The Bead wears s olid colours corresponding to a track when it is placed in the Bead player. Additional details of track colouring appear in section 3.2.2.3.

Figure 26: Bead Palette with new Bead button, intensity and frequency slider.

54

3.2.2.3. Vibrotactile Stimulator Arrangement Visualization
The top right panel of the user interface relates to the visualization of the vibrotactile display arrangement. This visualization uses the mini-map technique [115, 116], which provides spatial information that matches with the actual physical setup or layout. Whenever there exists an available output channel, a track is made available in the player frame as well as a single output icon in the arrangement window. The display in the arrangement window represents the position of the output stimulators. Users are allowed to drag and drop the icons to match the arrangement, matching it to the actual physical positioning. As seen in Figure 27.a, the output stimulators are arranged in a chair form. When a Bead on a track is activated, the corresponding icon turns on the track colour in the arrangement window a note gets played icons will turn on their lights corresponding to the channel track where the note is playing. When the note is not playing, the icons are turned off to empty.

Figure 27.a: The icons are arranged to match the Emoti-chair's (left) voice coil positions. The icons for channels 2, 3, 4, 5 and 8 are turned on, which means the notes on these tracks are now playing.

55

Figure 27.b: The icons are arranged to match the vibrotactile belt. It only has four channels of output so that there are only four icons in the visualization window.

3.2.2.4. Overview Panel

The overview panel is located below the player frame and function buttons. The purpose of this panel is to show the composition in summary form as a one dimensional stac ked display as outlined by [117] (see Figure 28). The idea of this overview panel is to have a UI component for easier access and providing a way to make and copy the patterns. Users can navigate through their pages by clicking on each column. The user may copy or clear the whole page by accessing a menu with right clicking. When there is a colour present in a block of the stacked display, there is a Bead has been placed in that particular track. The column represents one page (or 1.0 seconds) of a composition. Each overview panel page can display a total of 15 columns (or 15 pages of the composition), and it scrolls as the Beadplayer page scrolls through (See label Overview Panel in Figure 20). When the Beadplayer page is at the 16th second mark of composition (the 16 th page of the composition), then the overview panel refreshes and provides a new set of 15 columns.

The colour for each track is from Color Universal Design (CUD) [98]. The basic premise is that it is possible to select certain hues within each colour range that would lead to colour combinations and, in turn, would be discernible to subjects of all colour vision types. The previous researches until the works of CUD, the Primary Colours (Red, Green, Blue) along with Black ­ RGB + Black ­ were used to denote various pieces of information, data, etc. without much thought to the

56

actual hue of each colour. Thus, an individual who is visually challenged with respect to colour would have much difficulty in discerning between Blue and Green. With a proper CUD application, the Blue and Green Hues would be chosen such as to mitigate this effect as much as possible. Thus, it is possible to provide distinct colour watches for those who suffer from colour blindness.

Figure 28: Two overview panel columns visualized the Beadplayer frame.

3.3. Vibrotactile Instrumental Digital Interface (VIDI)
The user interface of the Beadbox allows users to configure and compose vibrotactile expressions. However, those expressions must be consumed by an audience via a vibrotactile output system such as the Emoti-chair [5]. It is important a file protocol be developed to facilitate

57

communication between the Beadbox information and the output device. As these output systems are developed, having a standard communication and file protocol will allow them to read/write data in a device independent manner. In this section, the VIDI protocol developed for this purpose is explained. The goal of VIDI is to support device independence so that it can be used with multiple different output system.

3.3.1. Introduction
The VIDI was built based on the Musical Instrumental Digital Interface (MIDI) format. MIDI is the technical standard protocol that provides definitions of musical constructs such as notes/frequencies, time signature, volume, instrument type or for communication between digital 8 midi output devices such as digital instruments and speakers and computers, or other musical devices [107]. It also allows musicians to compose digital music that can be played through various midi devices or instruments. Unlike other protocol, the MIDI protocol allows users to compose music by using the musical construct definitions. Those compositions can then be played on any midi instrument or device. How that music sounds depends on the instrument rather than the file itself. For example, the same file can be played on a synthesizer as on a computer/speaker combination. The composition will just sound different. In comparison, an .mp3 file cannot be played through an instrument but rather it must be played as a digital file through speakers. Detailed structure, protocol messages, and byte information can be found in section 2.4.4 and Appendix A.

As the vibrotactile system is different from the audio system, although there is some similarity in technical characteristics of each system (e.g., frequency), using music and music

The instrument itself can process digital information but the sound (music) that can be heard by people is produced using an analog device such as a speaker. A midi instrument acts as a sound based digital/analog converter so that people can hear the music [107].

8

58

concepts directly is insufficient. VIDI was thus developed to allow to create a composition without analogue input, similar to MIDI which can construct a composition by using defined messages and structures. MIDI is the technical standard describing protocol, interface, and connectors to communicate in between digital music devices. Compare to MIDI, other media protocols does not support the same level of definitions especially in musical components such as note, pitch bend, track controls, etc. Thus MIDI was chosen as the technical model to replicate and develop VIDI layer upon. As a result, a VIDI file can be played on MIDI players but not vice versa yet. In this chapter, the VIDI protocol and file format is introduced.

3.3.2. Standard VIDI File Format
The VIDI file format aims to store the information about a vibrotactile composition including: 1) the spatial information of output voice coils; 2) tempo; 3) number of tracks; and all the Beadspecific definition consisting of connected pair of Beads, track number, frequency and intensity level of each Beads. As seen in Figure 29, the file structure consists of two high level constructs, the Header and the Tracks.

Figure 29: VIDI File Structure.

59

3.3.2.1. Header

The Header contains information about the overall file and non-track/Bead specific information such as the total number of tracks defined as well as the tempo and spatial information (x,y coordinates). The Header starts with an English word "MThd", wh ich is encoded with ASCII code [118]. Next four bytes are the size of the header. The next two bytes represent the file has multiple tracks, two bytes for number of tracks, and two bytes for tempo. Followed by a meta message of the first track component, spatial information of each output relative coordination is added in variable length. This is to enable the compatibility to be playable in MIDI protocol, thus borrowing first part of the initial track component. It is desired to have a unique message header in future updates of the VIDI protocol. Figure 30 depicts the byte information of the VIDI header.

Figure 30: VIDI header byte information.

3.3.2.2. Tracks
Each track component starts with an English word "MTrk" (in ASCII [118], 4 bytes), followed by the total number of bytes contained in the track component, and then the series of track/Bead information. Each Bead information contains data for starting time, frequency, intensity, and end time. When Beads are connected in time (shown as two parallel lines between Beads), four bytes are added to the Bead definition data. The message protocol also can accommodate cases when two Beads are connected in the same track, two connected Beads are located in different tracks, or a Bead without any connections.

Since a VIDI message is based on top of the MIDI, the message also was built on top of existing MIDI event messages. The major reason is to provide an easier way of development and understanding when someone wants to create a VIDI instrument by ex tending from MIDI

60

instruments. A Bead Message carries the information of a single Bead and its information (frequency, intensity, starting time). The connected Bead information (location/time mark and track of the end Bead) are carried by next three bytes, direction of the connection has two bytes, and the track of the connected end Bead for three bytes; finally, the message package ends up with End of a Bead with three bytes. Figure 31 shows the byte information of track component and figure 32 depicts the byte information of each Bead Message. More details of what bytes mapped into represent each Bead message parts can be seen in Appendix B.

Figure 31: Track component byte information.

Figure 32: Bead Message byte information.

61

Chapter 4. Methodology, Results and Discussion
This chapter describes the methodology of my user study, and presents the results and analysis of the user study data. Qualitative and quantitative methods are employed to analyze the data and discuss the results with respect to the research questions. For the quantitative statistical analyses, the level of significance was set to be p < 0.05 which describes the statistical error made in testing a null hypothesis when it is concluded that the result is positive. The numbe r of participants (N) for each statistical test is 30.

4.1. User Study
A user study was conducted to answer the research questions. It aimed to collect usability and use data from participants who carried out representative tasks with BeadBox. Qualitative and quantitative data were collected using written a pre and post-study questionnaire (see Appendix E), and a live audio/video log of the actual system collected during the study.

4.1.1. Pre-study questionnaire
The pre-study questionnaire was used to collect demographic data and their musical experience (see Appendix E.1). There were total seven questions. The first 3 questions asked about gender, age range, and their highest level of education. The second set of 3 questions were to ask about participant level of music training, their specialized creative practice, whether they are amateur or professional, and lastly how do they plan for their vibrotactile creation in the study session. The last question asked about participant's plan for vibrotactile comp osition was left with a textbox where one can write a paragraph about it.

62

4.1.2. Post-study questionnaire
There were 26 questions in in the post questionnaire organized into two sections. The first section consisted of the ten general usability questions comprising the validated System Usability Scale (SUS) (Section 2.3.1). The SUS uses 5-point Likert Scale type questions to evaluate two validated constructs measuring usability and learnability [119]. The second part of the post-study questionnaire had six Likert-scale type questions that were focused on evaluating the viability of the Beadbox as a compositional tool dedicated to the creation of vibrotactile art. Four questions asked users about the level of difficulties in controlling the four basic components (frequency, intensity, timing and position) of their vibrotactile art piece and one question asked about the readability of the arrangement of the voice coils from the interactive visualization panel. Finally, there were five open ended questions that asked about how the actual composition was like when they played for themselves, what their intentions were, what were their expected reactions of audiences, what they liked and disliked about the Beadbox.

4.1.3. User Study Procedure
The user study was approved by the Ryerson Research Ethics Board (REB) committee (see Appendix D) for consent information and the Ryerson REB approval of the study protocol). Participation in the study was voluntary. Each participant was asked to sign the consent form in order to participate in the study.

The study began with a brief background explanation of the BeadBox user interface was provided to each participant. As participants had no background in vibrotactile art nor the system, descriptions of the vibrotactile hardware (the Emoti-chair) and the concept of vibrotactile art were also provided. Next, users were asked to complete the pre-study questionnaire. Participants were then given a brief demonstration of the Beadbox software, its notational system and 5-10 minutes to explore its functionality and user interface. During this time, they could ask questions about any part of the study including having further explanation of the interface components or functionalit y.

63

The next part of the study involved user using the Beadbox system to create a short vibrotactile art piece. Participants were asked to create/compose 8-10 pages of vibrotactile patterns while speaking aloud their thoughts. During the composition session, the screen of their actions was video recorded, as well as their audio (voice). Upon completing the compose session, the participants were asked to complete the post-study questionnaire.

4.1.4. Participants
Thirty people (20 female, 10 male) participated in the Beadbox user study to evaluate the usability and viability of the software. The age range of the participants varied between 18-64, with 19 in 18-24 age category, eight in 25-34 and one in each of the other three older age ranges (3544, 45-54, 55+).

Twelve participants reported being university graduates; nine participants were high school graduates; four participants were current college or technical school students; three participants had graduated from graduate school with either a mas ters or PhD; and two participants were college graduates.

Thirteen participants reported high school training was their highest level of musical training; eight participants reported having no musical training; five participants reported grade school level as their highest musical training; two participants completed university level of musical training; and two participants were professional level musicians.

Twenty-two participants were amateur artists; three participants were full-time professional artists; three participants were part-time professional artists; and two were non-artists.

For types of creative practices, participants could select more than one specialized As a result, art practice was grouped into four categories. Group 1 = "Visual Artists: including Painting, Photography, Printmaking, Sculpture, Performing Art or Installation, Group 2= "None", Group 3 = "Musical Artists: Music", and Group 4 = "Combined art specialists: participants who specialize in

64

multiple genres." Six participants were i n Group 1; six participants were in Group 2; five participants were in Group 3; and thirteen participants fell in Group 4.

4.1.5. Data analysis methods
Quantitative questionnaire data were analyzed using chi-square test, Kruskal-Wallis test and descriptive analyses to assess user responses. A thematic analysis [120] was used to analyze the written statements from the open ended questions and the recorded audio/video data in study. The themes were developed using an open coding strategy [121] and their definitions are shown in Table 3. To assess the reliability of the themes and definitions, 20% of the participant's responses were chosen. Two independent raters then rated each sample according to the themes in Table 3. An Intra-class Correlation Coefficient (ICC) was generated for both ratings. For all themes the ICC was greater than 0.840. This indicates that the raters were in strong agreement. The remaining data were coded by one rater.

65

Table 3: Themes and definitions used for thematic analysis. Theme Definition / Examples Music Concepts The Beadbox interface is related to the music concepts such as rhythm, sound output as melody, intensity as accent, etc. "I want the audience to feel lik e they're at a party where the music is ridiculously loud and you can feel bass drop." User interface and interaction components influence the work process; simple design; easy to use; visual aspects; "The colour coding system for intensity, different track s, etc. was very helpful ." "It's quite colourful and visual, Easy to use." The quality of being suited to serve a purpose well or not; Having a way to perform certain functions. "I didn't lik e certain aspects of the functionality (i.e., could not "Undo"). "I didn't lik e the fact that I couldn't align the notes on different lines (automatically)." It affects to the creativity when users using the Beadbox to compose. "It expanded my creativity" "It did limit my creativity because I was not comfortable with the vibration." Bugs or other technical issues. "I didn't lik e the screen setup, and glithces." Emoti-Chair Related to the Emoti-Chair rather than the Beadbox. "I lik ed the chair vibrations."

Interface Elements

Functionality

Creativity

Technical Issue

66

4.2. Results: 4.2.1. Forced-choice questions
A chi-square test was carried out for all Likert-scale post-study questions to compare the participant responses to questions with chance. The degrees of freedom were 4 for all questions. There was a significant difference between responses and chance for 13 out of the 16 Likert -scale questions with p < 0.05 (see Table 4).

A Kruskal-Wallis non-parametric test was applied to the art practice groupings to determine whether there was a difference in the ratings between these groups. The degrees of freedom was 3 for all questions. The result of the Kruskal-Wallis test showed that there was a significant difference in the complexity rating between the different artist specializations,  2 (3) = 7.94,  = 0.047, with a mean rank of 10.17 for Group 1 (Visual artists), 22.25 for Group 2 (No artistic background), 12.10 for Group 3 (Musical artists), and 16.15 for Group 4 (Combined art specialist).

67

Table 4: Results of chi-square test and descriptive of all Likert-scale type questions for p < 0.05. A rating of one is strongly disagree and five is strongly agree.

Question
A.1. I think that I would like to use this system frequently A.2. I found the system unnecessarily complex A.3. I thought the system was easy to use A.4. I think that I would need the support of a technical person to be able to use this system A.5. I found the various functions in this system were well integrated A.6. I thought there was too much inconsistency in this system A.7. I would imagine that most people would learn to use this system very quickly A.9. I felt very confident using the system A.10. I needed to learn a lot of things before I could get going with this system B.5. It was difficult to control the frequency level of the beads B.6. It was difficult to control the intensity level of the beads B.9. It was difficult to make the duration on each beads B.10. It was difficult to assign beads to each track

ChiSquare 16.00 30.33 29.33 17.00 35.67 14.67 33.33 21.00 13.33 32.33 28.33 13.33 24.33

Mean 3.43 1.63 4.33 2.10 3.63 2.13 4.37 4.00 2.07 1.73 1.70 2.27 1.83

SD 0.971 0.718 0.711 0.995 0.809 0.973 0.928 0.871 1.015 1.048 0.915 1.015 1.117

Mode 4 1 5 2 4 1 5 4 1 1 1 3 1

PValue 0.003 0.000 0.000 0.002 0.000 0.005 0.000 0.000 0.01 0.000 0.000 0.01 0.00

68

4.2.2. Qualitative Data
All comments from participant's responses to the open ended questions and audio/video recordings were coded into the defined themes. The number of occurrences per each theme is depicted in Figure 33.

50 45 40 35 30 25 20 15 10 5 0

44

Frequency of Comments

26 22
18 18 16

3

4

5

Themes

Figure 33: Frequency of comments from participants in each theme.

A one-way analysis of variance (ANOVA) was used to find any significant differences between the themes for the number of comments participants made. There was a statistically significant difference between groups (F (8,111) = 3.166, p = 0.003). A post hoc, Tukey's HSD test was performed to find means that are significantly different from each different themes. There was a significant difference between the Interface Elements theme (M = 1.69; SD = 0.736) and the

69

Creativity Positive theme (M = 1.00; SD = 0) (p<0.05). There were no significant differences in other paired themes.

4.3. Discussion
The discussion of user study results and participant comments are in relation to answering these research questions: Is Beadbox and its notation system usable by a variety of artists' perspectives? a. Is Beadbox a viable and effective tool in controlling the four factors (frequency level, intensity level, temporal information, and spatial information) of vibrotactile stimuli? b. 2. Does logical mapping for the visual-auditory substitution apply in tactile domain?

1.

What are the processes employed by novice vibrotactile artists in creating a vibrotactile composition?

3. Is the notation system of the Beadbox a workable system for vibrotactile art composition?

4.3.1. Usability and usefulness of the Beadbox
The definition of usability according to the ISO/IEC 9126-1 standard is "the capability of the software product to be understood, learned, used and attractive to the user, when used under specified conditions" [74, p. 7]. This definition can be evaluated using three factors: learnability, functionality and its aesthetic. From [74], learnability is defined as the capability of a software product to enable the user to learn how to use it. Functionality is defined as the essential purpose(s) of any product or service. The term aesthetic is defined as a set of ideas or opinions about beauty. The definition of the usability uses attractiveness, which is described as how visual factors and

70

appearance of the software attracts its user [74]. This discussion section focuses on these three aspects of the Beadbox. Responses to the three learnability questions (question 4: ` I think that I would need the s upport of a technical person to be able to use this system', question 7: `I would imagine that most people would learn to use this system very quick ly' and question 10: `I needed to learn a lot of things before I could get going with this system' in the SUS) showed that participants thought Beadbox was easy to learn. Given the relatively short amount of time participants were given to learn the concept of vibrotactile art (30±10 min.), each participant learned how compose rapidly, and said that their composition was created as intended. According to Maguire [77], a usable system allows the user to concentrate on the task, which also means it is functional with reduced errors and reinforce learning for reduced training time. Some related comments mentioned: "It was easy to learn how to use and flexible." "I lik e how it has a low learning curve for a relatively new art form" For functionality, participants also agreed that the Beadbox was easy to use and that is was not unnecessarily complex. The notion of complexity, however, was related to the level of artistic knowledge and practice. Participants who specialized in any artistic genre thought that using the Beadbox was less complex than those who had no specialization. Although not significant, participants in visual art and music thought it was less complex than those from other fine arts, including performing art, and dance. In addition, fifteen out of forty-four comments in the Interface Elements theme related to Beadbox being easy to use. Participants' responses to open -ended questions included that Beadbox was very easy to use exemplified by: "Very easy to use, instant results on what you've made." "I lik e how simple the program is to use, generally. It is ve ry easy to understand and fairly intuitive. After a little trial and error, it was pretty easy to create a track ." The goal in the design of Beadbox was to implement a simple, minimalistic interface. Large buttons were designed to be suitable for a touch screen, which makes also allows for easy, faster

71

access with a smaller scale mouse pointer. According to the Fitts's law, the time required to move a pointing device to a target is a function of the distance to the target and its size [122, 123]. It is faster to acquire closer and larger UI components (buttons and sliders) than ones which are further apart and smaller. Each Beadbox component only has one function, making simpler t o use. Participants mentioned how the Beadbox provided instant feedback (playback) which helped them to connect the vibrotactile sensations from the Emoti-chair (playback device) and their visual composition. Since vibrotactile art was already a new genre for all participants, it was important for them to learn the relationship between their Bead settings and the actual output. After a three to four trials of feeling the output from their Beads, users were able to make the connection between the output and their Bead patterns. Users could then create a composition with a physical understanding of how they composition would feel on the skin. One possible reason why participants thought the Beadbox was easy to use, would be the notation system. The notation system has straight forward interface, with visual metaphor and low functionality per component. Direct manipulation of the Bead made users recognize its change simultaneously with dynamic display.

Participants also agreed that they wanted to use the Beadbox frequently and that the various Beadbox functions were well integrated. Out of eighteen comments listed under the Functionality Positive theme, seven mentioned that the Beadbox was able to function so that each participant could create a vibrotactile art piece as they intended. The initial development of the Beadbox focused on providing a simple process for users to create a vibrotactile composition. Even though this was eventually achieved, some functions needed to be modified during the study cycles in order to addresses functionality issues that arose during the studies. During the first round of user studies, there were fourty-five comments in the Functionality Negative theme, most of which related to secondary functionality. For example, two participants asked if it is possible to have sample audios or different types of waveform in Beadbox. This secondary functionality would be one possible development in the future, since it requires a new notation development to put the waveforms and sample audios. Another secondary functionality participants commented was about

72

page level manipulation, such as a function to add a page between pages, page copy, page paste, etc. The page level manipulation would support users to edit faster, especially when it allows users to skip repeating mundane tasks when editing page level (i.e. lower intensity of all Beads in a page, creating repetitive vibrotactile patterns). Thus the page level manipulation also needs to be in the future update. Lastly, there was a comment regarding page scrolling. The Beadbox displayed one page at a time, and supports scrolling for each page. It was designed to be this way so that a page could be equivalent one second (20 Beads fits in a page). However, participants thought it would be better to have a scrollable feature, which makes it possible to browse between pages. Generally, the comments made in Functionality Negative theme were unrelated to the core functionality, but were for better usability by making process shorter than current version of the Beadbox. Many will have to come in the future update.

One main process of the composition process was placing a Bead. The process was originally designed to have two steps: 1) Clicking the New Bead button; and then 2) clicking on the Bead Player to place it. However, eleven out of thirteen participants from the first user study round wanted fewer steps because they thought this part of the process was too repetitive. HCI design guidelines proposed by Sutcliffe [124] suggest that an important aspect of good interface design is to consider the potential stress and fatigue caused to users when using a system. Sutcliffe said, "Mundane, non-stimulating tasks are liable to cause user fatigue" [124, p. 44]. Therefore, modifications to Beadbox were required. First, a shortcut for creating a Bead was implemented using a simple keyboard command. This was more efficient than working solely with a pointing device, i.e. mouse [125]. When the user holds down a key and clicks anywhere on the Bead Player frame, the Bead Palette displays a Bead that is ready to be placed. The user then clicked on the destination to place the bead. However, this method required two clicks, which led other participants to suggest another improvement; use the existing shortcut key as a switch. The new process was then to place a Bead with one click, as long as the shortcut key was pressed. These changes eliminated redundant actions.

73

Another change involved the drag and drop command, which was often misused by participants when they wanted to relocate a Bead that had already been placed. Instead of clicking to select and drop, participants tended to drag from one position to another to relocate the Bead. As a result, there was a change in user command using drag and drop and shortcut keys. This change was made to adjust this user behavior. Figure 34.a-d describes the different scenarios of changed interactions.

Figure 34.a: The Bead (on the left) has no connected Bead at the moment. When user dragged the Bead from left to the right, it created a connected Bead at where user dropped.

Figure 34.b: The Bead at scene 1, already has a connection with another Bead. When user dragged the Bead to the lower track, it relocated the Bead.

Figure 34.c: Similar to Figure 34.b, the Bead at scene 1 has connection. When user dragged the Bead to the right, it relocated the Bead.

74

Figure 34.d: This scene describes the click and drop command. User first clicked the Bead on scene 1 to select, then clicked on the empty space which relocated the Bead at dropped position.

Other functional improvements that were implemented after the first round of user studies included adding copy, paste, page clear, and multi-select functions as many participants commented that these functions would be helpful in reducing extra steps. As a result of these changes, the number of occurrences in the Functionality Negative theme was reduced to eighteen in the second phase. After these modifications, many of the comments in the second phase mentioned that the composition process was simple. However, participants continued to suggest new functionality including implementing different waveform types (e.g., sawtooth or square), an auto alignment function, and page manipulations. These functions could be implemented in future releases. Example comments from the post-modification user studies include:

"The copy and paste function is actually very user friendly" "It was difficult at first to figure out how to get the track to loop, but the ability to copy pages helped significantly." In addition to functionality, participants also made comments on the visual aspects of Beadbox. Thirteen out of forty-four comments contained in the Interface Element theme were related to visual appeal. Participants liked that they could interact with a variety of simple shapes and colours. Participants said they really appreciated the colour arrangement and aesthetics, for example: "The colour coding system for the intensity, different track s, etc. was very helpful." "The colourful circles were most appealing to me." "The colourful screen mak es it fun to play with." "It wasn't solely for the aestheti c value, but an interactive piece that allows the participants to be part of an immersive space."

75

One participant asked if it was possible to change the colour for each track. The colour for each track was selected based on the safe colour scheme recommended by the Color Universal Design (CUD) research team [126], designed to ensure that people with different colour deficiencies could distinguish all of the colours Therefore, it was not possible to change the track colour at this stage, but future version could enable this option.

Some participants mentioned that the circular shaped design of a Bead and its simple layout on the Beadplayer improved learnability of the Beadbox . It also enabled multiple approaches to the compositional process. For example, participants who specialized in visual art commented that they focused more on the visual display of the Bead patterns rather than thinking about the vibration output. They tried to draw something using Beads, even after they knew that vibrations would result from those patterns. From the screen recordings and their composition piece, it seemed that they tried to draw a gun, stairs, a house or a smiley face. A future research direction could be exploration of the relationship between visually appealing compositions and their vibrational impact on audiences. Artists from different genres may be able to connect their specializations and the creation of vibrotactile art.

4.3.2. Viability
This section will discuss viability of the Beadbox for vibrotactile art composition. Similar to other art genres it seems possible for vibrotactile art to be used as an expression of an artist's intention [127]. In order to achieve this goal, Beadbox must be able to support the creativity of artists without limiting it, and it must be able to transfer the artists' intentions to audiences. Participant's positive evaluation of the overall usability of Beadbox and specifically the use of frequency and intensity controls, their duration, and the bead assignment suggests that the Beadbox is a viable tool for vibrotactile composition.

76

Creativity is an important factor to consider when artists use an art medium t o express themselves. For a tool to be viable, it must not limit, but rather support creativity so that the artist can communicate with their audiences. There were eighteen comments which were listed under the Creativity Positive theme, and only three comments listed under the Creativity Negative theme. For example, from the positive theme: "If anything it enhanced my creativity." "I do not think that Beadbox limited my creativity. In fact, it was something so new that it rather increased my creativity allowing something new to be created."

One of the common reasons participants provided for their positive response was the visual notation system. Visual notation was also mentioned as a positive aspect of the usability of Beadbox. The simple design of the notation system as well as the Beadbox interface may have supported participant's creative notions and even encouraged enthusiasm. Participants who had specialized in the visual arts, in particular, concentrated on the visual appearance, i.e. shapes and figures. The user interface of the Beadbox helped creativity by providing not only software functionality but also a new way of interaction. Users may create a visual art piece with the notation system, which also can be felt on the vibrotactile output. Resnic k [42] [66] indicate that one consequence of one important requirement of an interface supporting creativity is to be able to try out alternatives which was realized in Beadbox. While Bead placement for configuring the vibrotactile pattern is the primary process that of Beadbox, participants who concentrated on visual patterns could still create a vibrotactile composition.

Vibrotactile art was a new art form for most of the participants. However, participants were able to connect their previous art practice with learning this new art form. They tried to find the similar characteristics to draw on when creating vibrotactile art. With this process, the new art form itself supported the creativity of new vibrotactile artists.

77

With respect to limiting creativity three participants commented that the Beadbox limited their creativity because of the vibrotactile system and technical limitations. Every participant was informed about the basic concepts of the vibrotactile system and the reasons for the limitations in frequency range, time per Bead, intensity range, etc. However, two participants still wanted to use frequencies below 100 Hz, and/or above 1000 Hz. Unlike the possibility of integrating personal colour choices for each track that visual artists found useful, the limited frequency range of Beadbox is necessary since human skin can only perceive frequencies between 20 Hz and 1000 Hz [17].

The third participant said the vibration from the Emoti-chair was an uncomfortable experience, so he had to limit his compositions to low intensity levels. The Beadbox uses intensity levels as a constant multiplier, which amplifies the output from the default system setting. Approximately 4% of U.S. population considered as specific phobic, which includes Haphephobia [128]. Haphephobic person reacts badly to being touched on the skin which is likely the issue with this one participant. The Beadbox allows adjusting intensity levels for the Beads so that it can accommodate different vibrotactile preferences. Using creativity support tools may restrict one's imagination to only what is possible with the tools [129]. The Beadbox may limit creativity by restricting ranges of intensity and frequency levels. It also has other technical limitations such as having the Emoti -chair for its only output source, limited track numbers, limited colours to express, etc. However, participants were able to create various compositions within such constraints.

There are also claims of how constraints help creativity [130]. Sternberg [131] emphasized how important it was to have constraints in creative process. "To be creative is to be free to choose among alternatives. Next, I shall consider constraints on creativity - for what is not constrained is not creative. " [p.202]

Many participants compared their first experience of vibrotactile art to music, irrespective of their specialization and professional background. Comments listed under Music Concepts

78

mentioned how the vibrotactile art they composed using the Beadbox could be related to musical concepts. For example: "I placed beads at the same time regularly to have beats lik e dance electronic music." "I want to mak e a continuous bass pattern, `do doo doom doom.'" "Lik e music, it would be great to have a function that can mak e a sample repetitive."

Sound and vibration are time-based media [8, 132] and therefore music is the closest genre of expression to tactile stimulation. It is expected that participants attempti ng to compose vibrotactile art would try to draw on those similarities. In addition, participants with a musical background focused on the audio output which was also expected. However, some participants thought that the system was "off-pitch" (e.g., as one participant commented "The Beadbox is definitely off-pitch"). The meaning of on or off-pitch was unclear in the context of vibrotactile output. It may mean that there is a possible frequencies that causes discomfort to human body or perception. The ability to create a rhythmic pattern is also similar between music and vibrotactile stimuli. Participants with a musical background tended to match music concepts with the Beadbox constructs. They tried to make melodic lines with the change of frequency levels. Users had to match the Beads at specific temporal location to create rhythmical patterns. Dancers who usually dance to music beats, tried to mimic bass rhythms by putting high intensity Beads at lower part of the Emoti-chair. Lastly, the participants created a composition with repetitive subparts. Repetition is part of many different art genres (i.e. music and its verse-chorus form, pattern repetition in visual arts, etc.). Participants used the copy and paste function, or tried to mimic the patterns they had created in their vibrotactile composition because they thought that the vibrotactile art would need a repetitive like a song in music. This kind of structure can be found in music, which is called `refrain' or chorus part. The role of refrain in a song usually contains a main rhythmic idea, which can possibly be the reason why participants liked to have one in their vibrotactile composition.

79

4.3.3. Technical Aspects 4.3.3.1. VIDI Protocol
The VIDI protocol was originally designed to be a unique communication protocol between devices contained in the vibrotactile system. The existing MIDI protocol [107] as a root, so that it could be expandable to MIDI instruments and players. Reasons to choose the MIDI protocol over other media protocol was the potential opportunities of inventing and developing new instruments. There exists many MIDI instruments on the market, which enabled the popularization of digital music community. By choosing the MIDI protocol as a root for the VIDI protocol, one can possibly develop a vibrotactile instrument from a MIDI instrument. Therefore, the VIDI files have extra bits of the prerequisites from the MIDI. The major difference comes from the Bead information data message, which has more data bytes than a single MIDI message. A VIDI message has more data per a Bead than a single musical note, requiring more bytes to transfer the data. Another reason can be the bytes added for MIDI encoding. To enable a VIDI message decodable in MIDI, it needs to fit in the prerequisites of MIDI protocol and this results more data bytes added to the VIDI message.

There are a number of improvements that could be made to the VIDI protocol in order for it to be standalone and efficient. One of the crucial factors is the data size. Currently, VIDI files are not abstract, containing extra bytes to be decodable in MIDI protocol. Other factor can be efficiency for communication in between devices. A well-organized protocol will result easier accessibility for developers and especially for the novice learners and creators who want to create a vibrotactile applications or instruments.

Another direction for assessment and improvement is to evaluate VIDI for other types of input/output devices. One beta test was carried out with a mobile tablet to explore whether it could be used for composition and control of the Emoti-chair output device. Since it was only possible for the Emoti-chair to be connected to a desktop computer, remote broadcasting from the mobile

80

device was inevitable. There was a private server used to transfer data from the mobile device and the receiver on the desktop. In the testing environment, mobile device sent the header data, and the information from each Bead was encoded into VIDI protocol. The receiver on the desktop then took the signal (Bead information sent from the mobile device) to distribute to the Emoti -chair. This process successfully allowed the tablet to control the Emoti-chair and although there was a delay of 3-5 seconds, no Bead data errors were transmitted using VIDI protocol. The delay may have come from the network setting, or the encoding/decoding time. Further investigation is required to examine what caused the delay and whether there are any other sources of error. However, this early beta test showed that there is promise for the VIDI protocol to be used with other input/output devices. Future research should be carried out with existing digital instruments and yet -to-be developed vibrotactile systems.

4.4. Limitations
One of the main limitations of this research was time. It is common for an artist to take a long time to adapt to and perfect new techniques or tools to suit their particular artistic needs and style at a level so as to be able to confidently express their intentions fully. However, in my user study, participants only had one hour to explore and use Beadbox. As such, there was likely a novelty effect that influenced the results. A longitudinal study would allow artists to become accustom to the tool in order to give feedback that is specific to their own needs. This type of study could also include design iterations based on the feedback collected during the study.

Another limitation was the number of participants. I was able to recruit only 30 participants due to time limitations. The number of participants limited the statistical analyses as well as the generalizability of the results. It also limited the demographic distribution of participation. Initially, I wanted to recruit the same number of participants in the artist and non-artist groups. However, the number of participants in each demographic was unequal due to scheduling limitations and

81

responses to recruitment efforts. Another plan was to compare artist feedback from different art genres. From my results there seems to be a trend in learnability and adaptability. Artists who had previous experience in certain art forms seemed to connect their knowledge to the new art form, but there was insufficient participant numbers in those groups to have enough confidence in the results. A future study could aim to specifically examine differences in approaches and use of Beadbox between different types of artists and recruit accordingly. Also, a longitudinal study could be conducted examining how vibrotactile art is integrated into different art genres and evolve as an independent genre.

While the Beadbox was setup to provide controls for all four vibrotactile factors, it does not provide an option to use different waveforms; it only used a sine wave. According to [20], human skin may perceive different types of waveforms stimulation (e.g., square or sawtooth wave). In future releases, there could be extended options to allow for different waveforms and perhaps alter the vibrotactile experience.

The questionnaire results only provided certain type of feedback as the majority of questions were forced choice. Having either more open-ended questions or the opportunity to interview participants could have provided more detailed fee dback eliciting responses to "why" type questions and richer data. These types of responses may contribute to clearer conclusion.

There were some limitations involving the hardware for the output device and the output device itself. The Emoti-chair was the only output device used in the user study. Users were required to sit in the Emoti-chair in order to feel the playback from their compositions. The form factor of the chair was a sling-type chair (see Figure 27.a left, section 3.2.2) that forced people to sit in a semi-reclined position with their arms outstretched to reach the keyboard (see Figure 35).

82

Figure 35: A composer is using the Beadbox while sitting on the Emoti-Chair.

This seating position may have hindered some of the perception to feel the vibrotactile stimulation and made it more difficult to interact with the computer located on a desk in front of the Emoti-chair. Using a tablet could have changed this limitation as users could have positioned a tablet closer to their bodies while seated in the reclined position. Each participant had different thickness to their clothes, which can limit the vibration senses by the voice-coils (thicker clothing dampens the vibrations).

Another limitation was the number of voice coils that could be controlled. The Emoti-chair had eight pairs of voice coils, which enabled to control over only eight channels of output by the Beadbox. Expanding either the quantity of possible channels/more voice coils or separating the pairs into individual, independent voice-coils could expand the number of possible output channels.

83

This could provide a broader range of controls and possibility increase the opportunities for expanded and varied creative compositions.

Finally, there were limitations of VIDI protocol. The current protocol focused on supporting for the communication between the Beadbox and the Emoti-chair while also complying with the MIDI protocol in order to support MIDI devices (although the two protocols differ in important ways). Future researchers and updates of VIDI must refine and evaluate to verify the VIDI protocol to make it more efficient.

84

Chapter 5. Conclusions and Future work
5.1. Conclusions
Several conclusions can be made based on the evidence and subsequent analysis in this thesis.

1.

The Beadbox seems to be a viable tool for vibrotactile art composition. The Beadbox provides control over the four vibrotactile components: frequency level, intensity level, timing, and spatial information display in a way that is useful to users. The Beadbox can be used as a standalone tool for creating vibrotactile art, providing less complexity for artists, including novice users.

2.

The usability of the Beadbox was found to be engaging and easy to learn. The vibrotactile notation system that was the foundation of Beadbox enabled vibrotactile art composition without necessarily having prior knowledge of other art genres. It suggested the possibility of replacing audio editors for composing or editing vibrotactile stimuli by providing less complexity in creating vibrotactile compositions and a more direct link between vibrotactile data and hardware. The user study participants indicated that they were satisfied with Beadbox's user interface and the visual notation system but wanted additional functionality such as different types of waveforms, page level editing, more tracks, etc.

3.

It was anticipated that musicians would find it easier to learn than other artists bec ause of the similarities between music and the vibrotactile art as both are time-based media. Time-based media share similarities in technical parameters of frequency (pitch), intensity (volume) and patterns (rhythm). This research showed that having a creative background affected learnability as musicians and other related artists found Beadbox easier to learn than people with no art background. However, the number of participants for the user study was insufficient to show any clear relationship between music experience and ease of use of Beadbox. A larger number of users was required to determine the extent of influence between experience in a specific artist genre and using Beadbox to compose vibrotactile art.

85

In addition, there was no assessment of the quality or audience engagement in any composition produced by users. 4. The approaches of visual artist was unanticipated. The visual artists seems to be focused on the visual aspects of the composition rather than the output stimulations. This is different from another approaches from musicians, who often focused on creating `rhythm' pattern. The Beadbox seems to support both approaches, which allows the artists without breaking their own art domain. However, since this finding was unanticipated, this finding needs to be investigated further in the future research with appropriate evaluation and tests. 5. Recalling the principles of designing creative process support tools by Resnick et al. [42], the Beadbox is expected to be a valid tool for authoring vibrotactile composition. User study results revealed that the Beadbox is easy to learn, where users were able to explorer from testing a single Bead to creating a full length composition as their intention. The participants also strongly agreed to that the interface of Beadbox is simple while there were multiple repetitive upgrades from each study and observation. 6. The device independent communication protocol, Vibrotactile instrument digital interface (VIDI) is a functional protocol to transfer vibrotactile information. The Beadbox uses VIDI standard file format, which saves and loads each file that the other composers created. It needs to be improved in efficiency, and also needs to be evaluated between device communications.

5.2. Future work
The Beadbox and its notation system show promise in supporting vibrotactile art. However there are considerable avenues for future research to explore. For addressing the issues and feedback for Beadbox that arose from the user study, the following improvements are recommended:

86

1.

The user study results analysis seem to suggest that visual artists such as painters concentrated on the visual appearance of their composition within the notation system, and the vibrotactile output was matching with what they intended to express. This specific finding can be a confirmation of the visual-tactile relation from the developed model, where the visual stimulation can deliver the vibrotactile stimuli output. However, the user study results of this research could not claim it as a contribution due to insufficient time and small sample size. Therefore, it is a future work for someone else to confirm this.

2.

As mentioned in the limitations section, one improvement would be to provide the option to choose different waveform types and their representation within Beadbox. Several researchers have indicated that it is possible to perceive different types of vibrotactile waveforms [133]. There are four periodic waveforms: sine wave, square wave, triangle wave, and sawtooth wave [134]. Other waveforms are considered as composite function of periodic functions and multiple other functions. Using the Beadbox representation interface style, proposed representations are depicted in Figure 36.

Figure 36. Common waveform types and their matching notation ideas. From top to bottom: sine wave, square wave, triangle wave, and sawtooth wave.

87

3.

Another aspect to be considered for the future work would be improvement of the visual interface. For the Beadbox prototype I used the default Java 2D drawing class, which could be upgraded by extending them to graphic libraries such as OpenGL, D3.js, etc. The graphical upgrade may provide an improved colour palette and better legibility in the visual appearance.

4.

Another future research direction could be the extension to mobile devices. Currently, I have explored using the Beadbox with a tablet. This involved transmitting the data from the tablet to a server, and then playing the transferred data on the Emoti-chair. While the composition did play on the Emoti-chair there was a delay of at least 3-5 seconds. This is obstacle concern since playback should be synchronized with the timeline bar on the Beadbox display so that composers would know what is happening at spec ific moment. This will allow dynamic change and performance as well. Future work may include extension of the user interface to mobile devices such that it can playback immediately with no delay and it can broadcast from one source to multiple output devices.

Other future work relates to the creation of e instruments that can take advantage of and use the VIDI protocol. Currently, the Emoti-chair is the only true vibrotacile device that can fully use the VIDI protocol, although it is possible for other devices such as midi synthesizers to interpret the signals, they are not vibrotactile devices. As vibrotactile art becomes popular, there should be instruments to perform with, as well as to provide access to compositions by audiences. Finally, longitudinal studies could be carried out in order to better understand the compositional process for vibrotactile art and generate improvements or new functionality for Beadbox.

88

89

Appendices Appendix A. MIDI Specification 1.0
A.1. Header Chunk
The header chunk is of course at the beginning of the MIDI file. It is there to help describe the file in three ways. Format of the header chunk is:

[4D 54 68 64] [00 00 00 06] [ff ff] [nn nn] [dd dd]

The first four bytes [4D 54 68 64] are translated directly to "MThd". Right after are the four following bytes [00 00 00 06] and this will always be the same. It represents the four byte size of the header. Next we have 2 bytes [FF FF] which represent the file format. There are three such formats. They are:

0 => single-track

1 => multiple tracks, synchronous

2 => multiple tracks, asynchronous

Single track is fairly self-explanatory - one track only. Synchronous multiple tracks means that the tracks will all be vertically synchronous, or in other words, they all start at the same time, and so can represent different parts in one song. Asynchronous multiple tracks do not necessarily start at the same time, and can be completely asynchronous. The next two bytes [nn nn] is the number of tracks in the midi file. Finally, the last two bytes [dd dd] is the number of delta-time ticks per quarter note.

90

A.2.Track Chunk
Type 1: Meta Event
Meta event package = 0xFF+<meta event type>+<variable length>+<event data bytes>

1) Type

Meta event types: Event Type Event

0x00

Sequence number

0x20

MIDI channel prefix assignment

0x01

Text event

0x2F

End of track

0x02

Copyright notice

0x51

Tempo setting

0x03

Sequence or track name

0x54

SMPTE offset

0x04

Instrument name

0x58

Time signature

0x05

Lyric text

0x59

Key signature

0x06

Marker text

0x7F

Sequencer specific event

0x07

Cue point

2)

Variable Length

Length of Meta event data expressed as a variable length value.

3)

Event data bytes

The input data bytes. Values within acceptable range can be given from the sender.

91

Type 2: Midi Event
1) MIDI Event structure

Each MIDI event has structure of following format:

<Status Byte> + <Channel Byte> + <DataByte1> + <DataByte2>,

While DataByte2 may be omitted depending on the channel message type.

MIDI Event Byte Level Information

Name

Lenght And Range

Description

StatusByte ChannelByte DataByte1 DataByte2

[0-F] [0-F] [0-FF] [0-FF]

Type of Midi message MIDI Channel Number First midi message data byte Second midi message data byte

92

2)

Channel Messages Channel messages are transmitted on individual channels to individual devices rather than

a global broadcast to all devices on the MIDI network. Channel voice messages instruct the destination instrument to assign a certain sound, turn on and off the notes, and change the sound of the currently active note(s). Below table lists details of each action message.

Voice Message

Status Byte

Data Byte1

Data Byte2

Note Off Note On Polyphonic Key Pressure Control Change Program Change Channel Pressure Pitch Bend

8x 9x Ax Bx Cx Dx Ex

Key number Key number Key number Controller number Program Number Pressure Value MSB

Note Off Velocity Note On Velocity Amount of Pressure Controller Value None None LSB

Figure A. Byte diagram for MIDI event message example of Note On channel message.

93

Type 3: System Event
First set of system messages are for system exclusive messages.

Status F0 F1 F2 F3 F4 F5 F6 F7

Name System Exclusive Time Code Song Position Pointer Song Select (undefined) (undefined) Tune Request EOX (End of System Exclusive)

Data data, then EOX or any status byte one byte two bytes: lsb msb one byte: song number 0 - 127

no data

Below are the realtime messages, which are system messages without data bytes, and only affect the playing of the song.

Status F8 F9 FA FB FC FD FE FF

Comment Clock (undefined) Start Continue Stop (undefined) Active Sensing System Reset

94

A.3.MIDI pitch value to frequency [135]

95

Appendix B. VIDI Specification 1.0 B.1. Track Component
<Delta Time*> <0x90> <Pitch_frequency> <Intensity> <0xE0> <Marginal_frequency> <0xA0> <Connected End Bead time position> <0xC0> <Direction sign of connection> <0XB0> <Track of the connected end Bead> <Delta Time + 50> <0x80>

*Delta Time contains the temporal location of a Bead (x coordinate), and comes into every single message. Delta Time has a variable length value. It is the time difference in between two message events. In our case, its measure is milliseconds. Each Bead message starts with a delta time, which is the time difference from previous Bead, and ends at delta time+50 (because 50 ms is the default duration of a Bead)

96

B.2. Bead Message Byte Description.

VIDI Message Bytes Parts

Data Description

<0x90> <data1> <data2> (10010000 + data1byte + data2byte)

Data1 here is the closest frequency to MIDI pitch in frequency. Since a frequency of a Bead is limited to a range of 0 to 1000 Hz, it would not fit in the MIDI data byte limit (0128). Therefore it splits into two Parts. Data2 is the intensity value of a Bead. Intensity has a range of 0-100, which fits in the MIDI data byte limit (0-128). Thus it is transferred directly in byte format. As this was built upon MIDI, one could find the closest audio frequency from a MIDI pitch value. (See Appendix A.3) Each pitch value can only have one closest "rounded" frequency, thus marginal value can be filled in the next message Part below.

<0xE0> <data1> <data2> (11100000 + data1byte + data2byte)

The marginal value of a Bead frequency information can be filled in with this value. Bead Frequency = Pitch Frequency + Bending Frequency. i.e. 800Hz = 784Hz (75,G5) + 16 Since each data byte has a limit (0-128), the maximum number of placeholder is two digits. Thus each data byte takes power of 10s. Data1 represents 1000s and 100s. Data2 represents 10s and 1s of the frequency.

<0xA0> <data1> <data2> (10100000 + data1byte + data2byte)

This Part is for the coordinate of the connected Bead (end Bead). Each Bead can only make a single connection with another Bead, and the connected pair of Beads can only remain in the same page. Given a single page holds 20 Beads, the maximum placeholder can be dealt under

97

four digits. Same mechanics as previous Part, Data1 represents 1000s and 100s whild Data2 represents 10s and 1s.

<0xC0> <data1> (11000000 + data1byte)

Given a pair of connected Beads, A and B. This is an indicator for the direction between two Beads. Data1 is 0 when A->B; 1 when A<-B

<0xB0> <data1> <data2> (10110000 + data1byte + data2byte)

Track of the connected end Bead Data1 : 1000s and 100s; Data2 : 10s and 1s

<0x80> <data1> <data2> (10000000 + data1byte + data2byte)

End of a Bead. The delta time for this one has its starting time + 50ms. Both data1 and data2 can be default values (or zeros) since they are just dummy fillers to be able to compiled by MIDI protocol.

98

Appendix C. Ethics Approval Letter

REB 2014-305 Project Title: The Beadbox (Vibro-tactile Composition Tool) Dear Somang Nam, The Research Ethics Board has completed the review of your submission. Your research project is now approved for a one year period as of Nov 10, 2014.The approval letter is attached in Adobe Acrobat (PDF) format. Congratulations and best of luck with the project. Please note that this approval is for one year only and will expire on November 10, 2014. Shortly before the expiry date a request to complete an annual report will be automatically sent to you. Completion of the annual report tak es only a few minutes, enables the collection of information required by federal guidelines and when processed will allow the protocol to remain active for another year. Please quote your REB file number (REB 2014-305) on future correspondence. If you have any questions regarding your submission or the review process, please do not hesitate to get in touch with the Research Ethics Board (contact information below). No research involving humans shall begin without the prior approval of the Research Ethics Board. Record respecting or associated with a research ethics application submitted to Ryerson University. Yours sincerely, Toni Fletcher, MA Research Ethics Co-Ordinator on behalf of Lynn Lavallée, Ph.D. Chair, Research Ethics Board Associate Professor Ryerson University EPH-200C 350 Victoria St., Toronto, ON (416)979-5000 ext. 4791 lavallee@ryerson.ca rebchair@ryerson.ca http://www.ryerson.ca/research ___________________________________________________________ Toni Fletcher, MA Research Ethics Co-Ordinator Office of Research Services Ryerson University (416)979-5000 ext. 7112 toni.fletcher@ryerson.ca http://www.ryerson.ca/research

99

Appendix D. Study Consent Form

CONSENT AGREEMENT SUMMARY

1. You will have the opportunity to participate in an evaluation of our vibrotactile 9composition10 tool. 2. The researchers are interested in your experience and opinion of software that aims to help vibrotactile composition. 3. Agenda: Consent form, pre-questionnaire, video recording of the user using the software, post questionnaire. 4. Participation is voluntary and you can stop at any time. 5. Everything you say and do will remain confidential.

9

A type of sensory substitution system that applies vibration to skin to feel through audio frequency output speaker.
10

An art piece which plays vibro-tactile.

100

Consent Agreement
Principal Investigator: Faculty Supervisor: Somang Nam, Ryerson University (416) 979-5000 ext. 2523 or somang.nam@ryerson.ca Deborah Fels, Ph.D., P.Eng. Ryerson University (416) 979-5000 ext. 7619 or dfels@ryerson.ca

Somang Nam is a Ryerson University student in the School of Computer Science currently studying towards his Master degree in Computer Science. His supervisor is Deborah Fels. This research is for Somang's master's thesis, which is a r equirement for his graduation. Project Title: Beadbox: VIBRO-TACTILE COMPOSITION TOOL

You are being asked to participate in a research study. Before you give your consent to be a volunteer, it is important that you read the following information and ask as many questions as necessary to be sure you understand what you will be asked to do. Purpose of the Study: The Beadbox is a software that provides a user friendly interface for vibrotactile composition. It was developed to help the user for controlling the four essential components in vibrotactile composition. These components include frequency level (pitch), intensity (volume), spatial information (visualization of speaker arrangement), and timing. The purpose of this study is to determine whether or not the Beadbox11 is viable with a high degree of usability in making a vibrotactile art piece. Description of the Study: First, you will be asked to complete a pre-study questionnaire to collect some background information. Once it is completed, there will be a short description of the software interface. Then you will be asked to make a vibrotactile composition using the Beadbox. You will be asked to speak out what you do, what you see, and any of your thinking process. You will be videotaped when you are using the Beadbox so that we can record any commentary you make during the study. Lastly, you will be asked to complete a post-study questionnaire on your general opinion and overall impressions of the Beadbox. The study will be approximately 1 hour in length including 10 minutes of pre-study questionnaire, about 25-30 minute of video recording while you using the Beadbox and a 20-25 minute of post-study questionnaire.

11

A proposed software that aimed to help and enhance the process of vibrotactile composition

101

Risks or Discomforts: The risks associated with the study are minimal. You might feel uncomfortable or fatigued while responding to the individual questions or questionnaires. If you feel tired or uncomfortable, you may take a break to rest or discontinue participation in the study either temporarily or permanently. You may feel uncomfortable being videotaped. We will turn on the camera during the pre-study questionnaire so that you can become use to it being on. If that does not help, then we can either finish the study without camera recording or stop the whole study. Benefits of the Study: It is not foreseen that you will personally benefit from participation in this study. However, the results from this research may contribute to the development of Beadbox which may in turn help potential users who want to create vibrotactile composition. Confidentiality: All data will remain confidential; will be secured at the Inclusive Media and Design Centre at Ryerson University and destroyed after five years. Furthermore, only the principal investigator and faculty supervisor of this study will have access to the data for analysis purposes. Data will only be presented in summary form and no one individual will be identified. Number codes will be used to link data with personal information. We will also be recording the study on video. We will not use this footage in any public setting, and the footage will be stored on our password protected lab servers located at Ryerson University. You have all the right to review/edit the recordings or transcripts as well. Costs and/or Compensation for Participation: There are no costs associated with your participation. You will be compensated with $20.00 in cash for completing the entire study. If you choose not to finish the study, you will still be given the incentive for your participation.

Voluntary Nature of Participation: Participation in this study is voluntary. Your choice of whether or not to participate will not influence your future relations with Ryerson University. If you decide to participate, you are free to withdraw your consent and to stop your participation at any time without any penalty. At any particular point in the study, you may refuse to answer any particular question or stop participation altogether.
Questions or Concerns about the Study:
We sincerely appreciate your co-operation. If you have any questions or concerns, please do not hesitate to contact Somang Nam by email somang.nam@ryerson.ca and at 416-979-5000 ext. 2523 or Deborah Fels at 416-979-5000 ext. 7619. This study has been reviewed by the Ryerson University Research Ethics Board. If you have questions regarding your rights as a participant in this study please contact: Research Ethics Board c/o Office of the Vice President, Research and Innovation Ryerson University 350 Victoria Street Toronto, ON M5B 2K3 416-979-5042 rebchair@ryerson.ca

102

Principal Investigator:

Somang Nam, Ryerson University (416) 979-5000 ext. 2523 or somang.nam@ryerson.ca Deborah Fels, Ph.D., P.Eng. Ryerson University (416) 979-5000 ext. 7619 or dfels@ryerson.ca Beadbox: Vibrotactile Composition Tool Consent Form to Participate in Study

Faculty Supervisor:

Project Title:

Agreement: Your signature below indicates that you have read the information in this agreement, have had a chance to ask any questions you have about the study, and know that your participation is entirely voluntary. Your signature also indicates that you agree to be in the study and have been told that you can change your mind and withdraw your consent to participate at any time. You have been given a copy of this agreement. You have been told that by signing this consent agreement you are not giving up any of your legal rights. ____________________________________ Name of Participant (please print) _____________________________________ Signature of Participant _____________________________________ Signature of Investigator Your signature below indicates that you agree to be video-taped during the study. ___________________________________ Signature of Participant ____________________ Date __________________ Date __________________

103

Appendix E. Study Questionnaire E.1. Pre-Study Questionnaire

Pre-questionnaire for the Beadbox : Vibrotactile Composition Tool
The purpose of these questions is to collect general information about you and your general experience in musical or artistic background. It should take less than 10 minutes to complete this questionnaire. Thank you in advance. * Required Please write your participant number * Ask your researcher for your participant number. 1. What is your gender? * Mark only one oval. Male Female Other: 2. What is your age? * Mark only one oval. 18-24 25-34 35-44 45-54 55-64 65+ 3. What is the highest level of education that you have completed? * Mark only one oval. High school graduate Some college or technical school College graduate University graduate Graduate school or above 4. What is your level of music training? * Mark only one oval. Grade school High school University level Professional level None

104

5. What kind of creative practices have you specialized? * Any art form, or art creation experienced. Check all that apply. Painting Sculpture Photography, Printmaking Installation Film Performing Arts Music None 6. What level of artist are you? * Mark only one oval. Part time professional Full time professional Amateur Other: 7. What do you want to create today? * Please describe briefly any plan or blueprint you are thinking of

105

E.2. Post-Study Questionnaire

Post-questionnaire for the Beadbox : Vibrotactile Composition Tool
The purpose of this questionnaire is to understand the usability and viability of the Beadbox software and to understand overall experience of yours. The questionnaire will take about 20 25 minutes. Part A is for general and overall usability test based on SUS (System Usability Scale) © Digital Equipment Corporation, 1986. Part B is Beadbox specific questionnaires for viability test. Terminology review: - A Bead is the circle shaped notation which represents the frequency and the intensity level. - A String is a series of beads, where each String arrangement to each voice coil. - A Voice coil is the output device which contacts to the skin of audience.

* Required Please write your participant number Ask your researcher for your participant number. A.1. I think that I would like to use this system frequently * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.2. I found the system unnecessarily complex * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.3. I thought the system was easy to use * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.4. I think that I would need the support of a technical person to be able to use this system * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

106

A.5. I found the various functions in this system were well integrated * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.6. I thought there was too much inconsistency in this system * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.7. I would imagine that most people would learn to use this system very quickly * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.8. I found the system very cumbersome to use * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.9. I felt very confident using the system * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

A.10. I needed to learn a lot of things before I could get going with this system * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

B.1. Were you able to make what you planned? * Mark only one oval. Completely Somewhat Not at all I changed my mind. B.2. Did Beadbox change what you wanted to make? * Describe briefly, point form is okay. B.3. Did Beadbox limited your creativity? If so, how did it limit your creativity? * Describe briefly, point form is okay.

107

B.4. Did Beadbox supported your creativity? If so, how did it support your creativity? * Describe briefly, point form is okay. B.5. It was difficult to control the frequency level of the beads * Mark only one oval. 1 Strongly disagree 2 3 4 5 Strongly agree

B.6. It was difficult to control the intensity level of the Beads * Mark only one oval. 1 2 3 4 5 Strongly disagree Strongly agree B.7. It was difficult to read the arrangement of the Voice coils * Top-Right panel that you controlled the icons to match the physical arrangement. Mark only one oval. 1 2 3 4 5 Strongly disagree Strongly agree B.8. It was difficult to place the Beads at right time * Mark only one oval. 1 2 3 4 Strongly disagree

5 Strongly agree

B.9. It was difficult to make the duration on each Beads * Mark only one oval. 1 2 3 4 5 Strongly disagree Strongly agree B.10. It was difficult to assign Beads to Strings? * Mark only one oval. 1 2 3 4 Strongly disagree

5 Strongly agree

B.11. How did your piece work when you tried it on the Emoti-chair? Was it the same as your intention? Or was it something you did not intend?

B.12. What emotions did you want to convey for an audience? B.13. How did you want the audience react? B.14. What did you like about the Beadbox? B.15. What did you not liked about the Beadbox? B.16. What would you do in the future with the Beadbox? Any plans or interests that you would like to investigate further in the future?

108

Appendix F. Frequency Graph for Forced-Choice Questions

109

References

[1] [2] [3] [4] [5]

Cj4dx.com, About 4DX | 4DX, 2016. B. F. Institute, 4DX: Here come the feelies, 2014. S. Herald, Are 4D movies the next big thing?, 2012. A. N. Gilbert, What the nose knows, Crown Publishers, 2008. M. Karam, F. Russo and D. Fels, "Designing the model human cochlea: An ambient crossmodal audio-tactile display," Haptics, IEEE Transactions on, vol. 2, no. 3, pp. 160169, 2009. C. Branje, The Vibrochord - Investigating a Vibrotactile Musical Instrument, University of Toronto, 2015. E. Gunther and S. O Modhrain, "Cutaneous grooves: composing for the sense of touch," Journal of New Music Research, vol. 32, no. 4, pp. 369-381, 2003. L. M. Brown, S. Brewster, H. C. Purchase and others, "A first investigation into the effectiveness of tactons," 2005, pp. 167-176. L. B. Rosenberg and J. R. Riegel, "Haptic feedback for touchpads and other touch controls," Google Patents, 2002.

[6] [7] [8] [9]

[10] K.-U. Kyung, D.-S. Kwon and G.-H. Yang, "A novel interactive mouse system for holistic haptic display in a human-computer interface," International Journal of Human-Computer Interaction, vol. 20, no. 3, pp. 247-270, 2006. [11] H. Nicolau, J. a. Guerreiro, T. Guerreiro and L. Carri\cco, "UbiBraille: designing and evaluating a vibrotactile Braille-reading device," 2013, p. 23. [12] C. Jayant, C. Acuario, W. Johnson, J. Hollier and R. Ladner, "V-braille: haptic braille perception using a touch-screen and vibration on mobile phones," 2010, pp. 295-296. [13] J. C. Lee, P. H. Dietz, D. Leigh, W. S. Yerazunis and S. E. Hudson, "Haptic pen: a tactile feedback stylus for touch screens," 2004, pp. 291-294. [14] M. Karam, C. Branje, G. Nespoli, N. Thompson, F. A. Russo and D. I. Fels, "The emotichair: an interactive tactile music exhibit," 2010, pp. 3069-3074. [15] Vázquez, Yuriria; Emilio Salinas; Ranulfo Romo, "Transformation of the neural code for tactile detection from thalamus to cortex.," Proceedings of the National Academy of Sciences, vol. 110, no. 28, pp. E2635-E2644, 2013.

110

[16] D. Purves, G. J. Augustine, D. Fitzpatrick, L. C. Katz, A.-S. LaMantia, J. O. McNamara, S. M. Williams and others, "Mechanoreceptors Specialized to Receive Tactile Information," Sinauer Associates, 2001. [17] R. T. Verrillo and S. J. Bolanowski, "Tactile responses to vibration," Springer, pp. 11851213, 2008. [18] G. A. Gescheider, Psychophysics: the fundamentals, 2013. [19] R. T. Verrillo, "Vibration sensation in humans," Music Perception, pp. 281-302, 1992. [20] J. B. Van Erp, "Guidelines for the use of vibro-tactile displays in human computer interaction," 2002, pp. 18-22. [21] F. A. Russo, P. Ammirante and D. I. Fels, "Vibrotactile discrimination of musical timbre.," Journal of Experimental Psychology: Human Perception and Performance, vol. 38, no. 4, p. 822, 2012. [22] R. T. Verrillo, "Vibrotactile thresholds for hairy skin.," Journal of experimental psychology, vol. 72, no. 1, p. 47, 1966. [23] G. Von Bekesy, "Similarities between hearing and skin sensations.," Psychological Review, vol. 66, no. 1, p. 1, 1959. [24] B. C. Moore, An introduction to the psychology of hearing, 2012. [25] C. Branje, M. Maksimouski, M. Karam, D. Fels, F. Russo and others, "Vibrotactile display of music on the human back," 2010, pp. 154-159. [26] S. Weinstein, "Intensive and extensive aspects of tactile sensitivity as a function of body part, sex and laterality," 1968. [27] K. Myles and M. S. Binseel, "The tactile modality: a review of tactile sensitivity and human tactile interfaces," 2007. [28] D. Mahns, N. Perkins, V. Sahai, L. Robinson and M. Rowe, "Vibrotactile frequency discrimination in human hairy skin," Journal of Neurophysiology, vol. 95, no. 3, pp. 14421450, 2006. [29] M. Schust and others, "Effects of low frequency noise up to 100 Hz," Noise and Health, vol. 6, no. 23, p. 73, 2004. [30] M. J. Griffin, Handbook of human vibration, Academic press, 2012. [31] S. Pockett, "On subjective back-referral and how long it takes to become conscious of a stimulus: a reinterpretation of Libet's data," Consciousness and Cognition, vol. 11, no. 2, pp. 144-161, 2002.

111

[32] L. Benjamin, W. J. Elwood W, F. Bertram and P. Dennis K, "Subjective referral of the timing for a conscious sensory experience," pp. 164--195, 1993. [33] B. Cohen and J. H. Kirman, "Vibrotactile frequency discrimination at short durations," The Journal of general psychology, vol. 113, no. 2, pp. 179-186, 1986. [34] J. A. Harris, E. Arabzadeh, A. L. Fairhall, C. Benito and M. E. Diamond, "Factors affecting frequency discrimination of vibrotactile stimuli: implications for cortical encoding," PLoS One, vol. 1, no. 1, p. 100, 2006. [35] G. D. Goff, "Differential discrimination of frequency of cutaneous mechanical vibration," Journal of experimental psychology, vol. 74, no. 2, pp. 294-299, 1967. [36] R. T. Verrillo, "Subjective magnitude functions for vibrotaction," Man-Machine Systems, IEEE Transactions on, vol. 11, no. 1, pp. 19-24, 1970. [37] Safetyguide.web.cern.ch, Safety at CERN -- 25. The Physical Work Environment, 2015. [38] Who.int, Guidelines for Community Noise - Chapter 4, 2015. [39] R. T. Verrillo, "Psychophysics of vibrotactile stimulation," The Journal of the Acoustical Society of America, vol. 77, no. 1, pp. 225-232, 1985. [40] B. Shneiderman, "Creativity support tools: A grand challenge for HCI researchers," Springer, pp. 1-9, 2009. [41] L. Candy and E. A. Edmonds, "Supporting the creative user: a criteria-based approach to interaction design," Design Studies, vol. 18, no. 2, pp. 185-194, 1997. [42] M. Resnick, B. Myers, K. Nakakoji, B. Shneiderman, R. Pausch, T. Selker and M. Eisenberg, "Design principles for tools to support creative thinking," 2005. [43] M. J. Enriquez and K. E. MacLean, "The hapticon editor: a tool in support of haptic communication research," 2003, pp. 356-362. [44] J. Ryu and S. Choi, "posVibEditor: graphical authoring tool of vibrotactile patterns," 2008, pp. 120-125. [45] J. Lee, J. Ryu and S. Choi, "Vibrotactile score: A score metaphor for designing vibrotactile patterns," 2009, pp. 302-307. [46] S. Paneels, M. Anastassova and L. Brunet, "TactiPEd: easy prototyping of tactile patterns," pp. 228-245, 2013. [47] W. M\"uller and H. Schumann, "Visualization methods for time-dependent data-an overview," 2003, pp. 737-745. [48] A. Sethi, Multimedia Education: Theory and Practice, International Scientific Publications, 2005, p. 105.

112

[49] B. L. Harrison, R. Owen and R. M. Baecker, "Timelines: an interactive system for the collection and visualization of temporal data," 1994, pp. 141-141. [50] Audacityteam.org, Audacity: Free Audio Editor and Recorder, 2015. [51] Image-line.com, FL Studio 12, 2015. [52] Cakewalk.com, Cakewalk - The World's Best Software For Recording And Making Music On PC And Mac, 2015. [53] Creative.adobe.com, Adobe Creative Cloud Audio Editor Audition, 2015. [54] B. Adam Patrick, "Can We Afford These Affordances? GarageBand and the Double-Edged Sword of the Digital Audio Workstation," Action, Criticism, and Theory for Music Education, vol. 14, no. 1, p. 44, 2015. [55] A. (Canada), Apple (Canada) - Final CutProX - Overview, 2015. [56] Adobe.com, Video editing software Adobe Premiere Pro CC, 2015. [57] W. Brandon, "Want Hollywood results on a budget? Here's the best free video editing software," Digital Trends, [Online]. Available: http://www.digitaltrends.com/computing/best-free-video-editing-apps/. [Accessed 22 12 2015]. [58] windows.microsoft.com, Movie Maker - Microsoft Windows, 2015. [59] G. Siegchrist, Jaycut Offers Easy, Powerful Online Editing, 2015. [60] S. Mahbubur Rahman, "Design and Management of Multimedia Information Systems," in Opportunities and Challenges, IGI Global, 2000. [61] "A complete list of free and paid 2d animation software programs," 2D Animation Software Guide, [Online]. Available: http://www.2danimationsoftwareguide.com/. [Accessed 22 12 2015]. [62] Autodesk.com, 3D Animation And Modeling Software | Maya | Autodesk, 2015. [63] B. Foundation, blender.org - Home of the Blender project - Free and Open 3D Creation Software, 2015. [64] Adobe.com, Flash animation software | Download free Flash Professional CC trial, 2015. [65] C. Lewis, C. Brand, G. Cherry and C. Rader, "Adapting user interface design methods to the design of educational activities," 1998, pp. 619-626. [66] B. Shneiderman and S. Ben, Designing the user interface, 2003. [67] J. T. Hackos and J. R, User and task analysis for interface design, Wiley New York, 1998.

113

[68] B. E. John and D. E. Kieras, "The GOMS family of user interface analysis techniques: Comparison and contrast.," ACM Transactions on Computer-Human Interaction (TOCHI), vol. 3, no. 4, pp. 320-351, 1996. [69] J. Nielsen and R. Molich, "Heuristic evaluation of user interfaces.," in Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 1990. [70] K. Tripathi, "A study of interactivity in human computer interaction," International Journal of Computer Applications, vol. 16, no. 6, pp. 1-3, 2011. [71] R. Marois and J. Ivanoff, "Capacity limits of information processing in the brain," Trends in cognitive sciences, vol. 9, no. 6, pp. 296-305, 2005. [72] Lap.umd.edu, Human Computer Interaction, 2015. [73] D. Hill, "Designing for Human-Computer Interaction: some rules and their derivation," 2012. [74] N. Bevan, "International standards for HCI and usability," International journal of humancomputer studies, vol. 55, no. 4, pp. 533-552, 2001. [75] Iso.org, ISO/IEC 25010:2011 - Systems and software engineering -- Systems and software Quality Requirements and Evaluation (SQuaRE) -- System and software quality models, 2015. [76] Iso.org, 2015. [77] M. Maguire, "Methods to support human-centred design," International journal of human-computer studies, vol. 55, no. 4, pp. 587-634, 2001. [78] A. Seffah, M. Donyaee, R. B. Kline and H. K. Padda, "Usability measurement and metrics: A consolidated model," Software Quality Journal, vol. 14, no. 2, pp. 159-178, 2006. [79] P. I. Santosa, K. K. Wei and H. C. Chan, "User involvement and user satisfaction with information-seeking activity," European Journal of Information Systems, vol. 14, no. 4, pp. 361-370, 2005. [80] Nngroup.com, 10 Heuristics for User Interface Design: Article by Jakob Nielsen, 2015. [81] N. Tractinsky, A. S. Katz and D. Ikar, "What is beautiful is usable," Interacting with computers, vol. 13, no. 2, pp. 127-145, 2000. [82] J. Brooke, "SUS-A quick and dirty usability scale," Usability evaluation in industry, vol. 189, no. 194, pp. 4-7, 1996. [83] R. Likert, "A technique for the measurement of attitudes.," Archives of psychology, 1932. [84] ScienceDaily, Visual perception, 2015.

114

[85] T. P. o. t. Senses, Transformations for Perception and Action, 2015. [86] R. Spence, Information visualization, 2001. [87] P. Bach-y-Rita and S. W. Kercel, "Sensory substitution and the human--machine interface," Trends in cognitive sciences, vol. 7, no. 12, pp. 541-546, 2003. [88] V. S. Williams, "Creating Effective Visual Metaphors," 2012. [89] C. Forceville, Pictorial metaphor in advertising, 1996. [90] K. Giannakis and M. Smith, "Imaging soundscapes: Identifying cognitive associations between auditory and visual dimensions," Musical imagery, pp. 161-179, 2001. [91] A. Wells, "Music and visual color: A proposed correlation," Leonardo, pp. 101-107, 1980. [92] J. L. Caivano, "Color and Sound: Physical and Psychophysical Relations*," Color Research \& Application, vol. 19, no. 2, pp. 126-133, 1994. [93] L. E. Marks, "On cross-modal similarity: the perceptual structure of pitch, loudness, and brightness.," Journal of Experimental Psychology: Human Perception and Performance, vol. 15, no. 3, p. 586, 1989. [94] A. S. Walker-Andrews and E. M. Lennon, "Auditory-visual perception of changing distance by human infants," Child Development, pp. 544-548, 1985. [95] L. Zhou, J. Yan, Q. Liu, H. Li, C. Xie, Y. Wang, J. L. Campos and H.-j. Sun, "Visual and auditory information specifying an impending collision of an approaching object," pp. 720-729, 2007. [96] J. Ward, B. Huckstep and E. Tsakanikos, "Sound-colour synaesthesia: To what extent does it use cross-modal mechanisms common to us all?," Cortex, vol. 42, no. 2, pp. 264-280, 2006. [97] M. Adeli, J. Rouat and S. Molotchnikoff, "Audiovisual correspondence between musical timbre and visual shapes," Frontiers in human neuroscience, vol. 8, 2014. [98] Y. G. Ichihara, M. Okabe, K. Iga, Y. Tanaka, K. Musha and K. Ito, "Color universal design: the selection of four easily distinguishable colors for all color vision types," 2008, pp. 68070-68070. [99] A. I. F. F. "AIFF", Audio Interchange File Format: "AIFF", 2015. [100] Msdn.microsoft.com, Multiple channel audio data and WAVE files - Windows 10 hardware dev, 2015. [101] R. Raissi, The Theory Behind Mp3, 2015.

115

[102] J. Burg, J. Romney and E. Schwartz, Chapter 6: MIDI and Sound Synthesis - Digital Sound & Music, 2015. [103] R. C. Williams, Windows XP digital music for dummies, 2004. [104] N. Jayant, J. Johnston and R. Safranek, "Signal compression based on models of human perception," Proceedings of the IEEE, vol. 81, no. 10, pp. 1385-1422, 1993. [105] K. Brandenburg and G. Stoll, "ISO/MPEG-1 audio: A generic standard for coding of highquality digital audio," Journal of the Audio Engineering Society, vol. 42, no. 10, pp. 780792, 1994. [106] A. Rathbone, Mp3 for Dummies with Cdrom, John Wiley \& Sons, Inc., 2001. [107] Midi.org, MIDI Manufacturers Association - Introduction to MIDI, 2015. [108] Newt.phys.unsw.edu.au, Note names, MIDI numbers and frequencies, 2015. [109] D. S. &. Music, 6.3.1 MIDI SMF Files - Digital Sound & Music, 2014. [110] Asio4all.com, ASIO4ALL - Universal ASIO Driver, 2015. [111] GitHub, jasiohost-Java ASIO, 2013. [112] Merriam-webster.com, notation | a system of marks, signs, figures, or characters that is used to represent information, 2015. [113] Physics.info, Intensity - The Physics Hypertextbook, 2015. [114] G. M. Karam, "Visualization using timelines," 1994, pp. 125-137. [115] R. P. Haining, Spatial data analysis: theory and practice, 2003. [116] V. Zammitto, "Visualization techniques in video games," Proceedings of Electronic Information, the Visual Arts, and Beyond, pp. 267-276, 2008. [117] D. Keim and others, "Information visualization and visual data mining," Visualization and Computer Graphics, IEEE Transactions on, vol. 8, no. 1, pp. 1-8, 2002. [118] Asciitable.com, Ascii Table - ASCII character codes and html, octal, hex and decimal chart conversion, 2015. [119] J. R. Lewis and J. Sauro, "The factor structure of the system usability scale," pp. 94-103, 2009. [120] R. E. Boyatzis, Transforming qualitative information: Thematic analysis and code development, Sage, 1998.

116

[121] G. W. Ryan and H. R. Bernard, "Techniques to identify themes," Field methods, vol. 15, no. 1, pp. 85-109, 2003. [122] P. M. Fitts, "The information capacity of the human motor system in controlling the amplitude of movement.," Journal of experimental psychology, vol. 47, no. 6, p. 381, 1954. [123] I. S. MacKenzie and W. Buxton, "Extending Fitts' law to two-dimensional tasks," 1992, pp. 219-226. [124] A. Sutcliffe, Human-computer interface design, Springer, 2013. [125] R. C. Omanson, C. S. Miller, E. Young and D. Schwantes, "Comparison of mouse and keyboard efficiency," 2010, pp. 600-604. [126] K. I. M. Okabe and I. Kei, "Color universal design (cud): How to make figures and presentations that are friendly to colorblind people, 20 08," 2015. [127] C. Branje and D. I. Fels, "Playing vibrotactile music: A comparison between the Vibrochord and a piano keyboard," International Journal of Human-Computer Studies, vol. 72, no. 4, pp. 431-439, 2014. [128] Encyclopedia.com, Phobias Facts, information, pictures | Encyclopedia.com articles about Phobias, 2015. [129] B. Shneiderman, "Creating creativity: user interfaces for supporting innovation," ACM Transactions on Computer-Human Interaction (TOCHI), vol. 7, no. 1, pp. 114-138, 2000. [130] P. D. Stokes, Creativity from constraints: The psychology of breakthrough, Springer Publishing Company, 2005. [131] R. J. Sternberg, The nature of creativity: Contemporary psychological perspectives, CUP Archive, 1988. [132] Tate.org.uk, Conservation  time-based media, 2015. [133] S. J. Bensma\ia and M. Hollins, "Complex tactile waveform discrimination," The Journal of the Acoustical Society of America, vol. 108, no. 3, pp. 1236-1245, 2000. [134] Y. Wei and Q. Zhang, Common Waveform Analysis: a new and practical generalization of Fourier analysis, Springer Science \& Business Media, 2012. [135] Global.oup.com, Refining Sound: Chapter 7, 2015. [136] R. T. Verrillo, A. J. Fraioli and R. L. Smith, "Sensation magnitude of vibrotactile stimuli," Perception \& Psychophysics, vol. 6, no. 6, pp. 366-372, 1969. [137] R. T. Verrillo, "Effects of aging on the suprathreshold responses to vibration," Perception \& Psychophysics, vol. 32, no. 1, pp. 61-68, 1982.

117

[138] R. T. Verrillo and A. J. Capraro, "Effect of stimulus frequency on subjective vibrotactile magnitude functions," Perception \& Psychophysics, vol. 17, no. 1, pp. 91-96, 1975. [139] R. T. Verrillo, "Effect of spatial parameters on the vibrotactile threshold.," Journal of experimental psychology, vol. 71, no. 4, p. 570, 1966. [140] R. T. Verrillo, S. J. Bolanowski and G. A. Gescheider, "Effect of aging on the subjective magnitude of vibration," Somatosensory \& motor research, vol. 19, no. 3, pp. 238-244, 2002. [141] R. T. Verrillo, "Comparison of vibrotactile threshold and suprathreshold responses in men and women," Perception \& psychophysics, vol. 26, no. 1, pp. 20-24, 1979. [142] G. Robles-De-La-Torre, "International Society for Haptics: Haptic technology, an animated explanation," Isfh. org, 2010. [143] Iso.org, ISO/IEC 9126-1:2001 - Software engineering -- Product quality -- Part 1: Quality model, 2015. [144] J. X. Maier, J. G. Neuhoff, N. K. Logothetis and A. A. Ghazanfar, "Multisensory integration of looming signals by rhesus monkeys," Neuron, vol. 43, no. 2, pp. 177-181, 2004. [145] Merriam-webster.com, Definition of LOOM, 2015. [146] D. Codecs, Digital Audio File Formats: Lossy and Lossless Codecs - For Dummies, 2015. [147] Indiana.edu, What is amplitude?, 2015. [148] I. Hattwick, I. Franco, M. Giordano, D. Egloff, M. M. Wanderley, V. Lamontagne, I. Arawjo and M. Martinucci. [149] A. Baijal, J. Kim, C. Branje, F. Russo, D. Fels and others, "Composing vibrotactile music: A multi-sensory experience with the Emoti-chair," 2012, pp. 509-515.

118

