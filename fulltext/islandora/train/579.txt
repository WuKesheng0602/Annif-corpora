Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2003

Object extraction in video sequences based on spatiotemporal independent component analysis
Zhenhe Chen
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Electrical and Computer Engineering Commons Recommended Citation
Chen, Zhenhe, "Object extraction in video sequences based on spatiotemporal independent component analysis" (2003). Theses and dissertations. Paper 136.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

OBJECT EXTRACTION IN VIDEO SEQUENCES BASED ON SPATIOTEMPORAL INDEPENDENT COMPONENT ANALYSIS
by
ZHENHE CHEN
Bachelor of Engineering South China University of Technology, China, 1996 A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of rvlaster of Applied Science in the Program of Electrical and Computer Engineering. Toronto, Ontario, Canada, 2003

©

Zhenhe Chen 2003

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

UMI Number: EC52879

INFORMATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

®

UMI
UMI Microform EC52879 Copyright 2008 by ProQuest LLC. All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code. ProQuest LLC 789 E. Eisenhower Parkway PO Box 1346 Ann Arbor, MI 48106-1346

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Instructions on Borrowers
Ryerson University requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date.

IV

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Abstract
Zhenhe Chen, Master of Applied Science, Electrical and Computer Engineering, Ryerson University. Video object extraction is one of the most important areas of video processing in which objects from video sequences are extracted and used for many applications such as surveillance systems, pattern recognition etc. In this research work, an object-based technique based on the spatiotemporal independent component analysis (stICA) is developed to extract moving objects from video sequences. Using the stICA, the preliminary source images containing moving objects in the video sequence are extracted. These images are processed using wavelet analysis, edge detection, region growing and multiscale segmentation techniques to improve the accuracy of the extracted objects. A novel compensation method is applied to deal with the nonlinear problem caused by the application of the stICA directly to the video sequences. The recovered objects are indexed by the singular value decomposition (SVD) and linear combination analysis. Simulation results demonstrate the effectiveness of the stICA-based object extraction technique in content-based video processing applications.

v

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Acknowledgment
I have enjoyed and benefited from the pleasant and stimulating research environment at Department of Electrical and Computer Engineering, Ryerson University. I am grateful to my supervisor, Dr. Xiao-Ping Zhang, for having guided me with an open but practical mind, for being always willing to answer questions or discuss problems and for infusing me with his intellectual honesty. I also thank my colleagues in Communications and Signal Processing Applications Lab (CASPAL), in particular Hua, for the interesting discussions and for the active help to enhance this document. My deep gratitude is also expressed to Karthikeyan, Yuhong and Kan who supplied encouraging supports for my research. My special thanks go to each member of the dissertation jury for having accepted to evaluate this thesis. I have sincerely appreciated your comments and questions. I would like to express my warm appreciations to Bonnie for caring and being patient during the past few years. Her understanding provided the strongest motivation to finish this writing.

VI

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Contents

1 Introduction
1.1 1.2 1.3 1.4 1.5 Motivation. Review of Previous Works Objectives......... Proposed Approaches and Methodologies Overview of the Thesis . . . . . . . . . .

1

1

1 3 4 5

2

Principle Component Analysis and Independent Component Analysis
2.1 Principle Component Analysis, Singular Value Decomposition, and Whitening . . . . . . . . . . . . . . . . . . . 2.1.1 2.1.2 2.1.3 2.2 2.3 2.4 2.5 2.6 Principle Component Analysis Singular Value Decomposition Whitening . . . . . . . . .

7

7 7 8

10
10

Independent Component Analysis Comparison of PCA, Whitening and ICA . The ICA Estimation Methods Spatiotemporal ICA Summary . . . . . .

12 15 18 21
23

3

Formulation of the stICA Model for Video Sequences
3.1 3.2 Formulation of the stICA Model for Video Sequences The stICA Based Video Segmentation Approach . . .

23 26

vii

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

3.3 3.4 4

Simulation of the stICA applied to Video Processing in the First Iteration Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28 30
31

Post-processing in the First Iteration 4.1 4.2 4.3 4.4 Using Wavelet Analysis to Locate Regions of Interest Image Edge Detection with Region Growing Multiscale Image Segmentation . . . . . . . Simulations of the Post-processing Techniques in the First Iteration 4.4.1 4.4.2 4.4.3 4.5 Simulation of Wavelet Analysis to Locate ROIs .. Simulation of Edge Detection with Region Growing Simulation of Multiscale Image Segmentation

31 35 40 42 43 46 49 52 53 53 56 57 58 59 67 69 69 70 72

Summary . . . . . . . . . . . . . . . . . . . . . . . .

5

A Compensation Approach of stlCA for Practical Video Sequences 5.1 5.2 5.3 A Compensation Approach of stICA Frame Object Indexing Approach Simulations . . . . . . . . . . . . 5.3.1 5.3.2 5.4 Simulation of Compensation Approach of stICA Simulation of the Frame Object Indexing Approach

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

Conclusion 6.1 6.2 Contribution. Possible Extension

Bibliography

viii

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 1 Introd uction
1.1 Motivation
HE increasing popularity of video processing is due to the high demand for video in entertainment, security related applications, education, tele-medicine, database and

T

new wireless telecommunications. Recently, interesting research topics such as automated and efficient video content-based t~chniques are attracting much attention. Video content-based techniques are aimed at achieving significant data reduction of video by applying suitable transformation on video sequences based on their content. This data reduction has two main advantages: video databases work efficiently for searching content-based videos, and processing cost reduces dramatically. The content-based video presentation is an essential need for broadcasting services, Internet and security applications. This thesis develops a framework for automated content-based video processing based on the spatiotemporal independent component analysis (stICA). Both theoretical derivation and simulation results are provided to illustrate the effectiveness of the presented methods.

1.2

Review of Previous Works

The essence of this thesis is in applying the stICA technique to extract the objects in video sequences. A brief review of some of the works done in these fields is covered in

1

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

2

this section. Raw video clips are usually binary streams that are not well organized. To represent their contents, video clips must be decomposed into objects so analysis can be performed. The object-based technique is one way of analyzing the video clips and it is gaining importance in achieving compression and performing content-based video retrieval. Recently, there have been many video object segmentation techniques to extract or track the objects, such as transition-based [1] [2] and key frame estimation [3] [4]. The transition-based methods (also named scene change detection) look at the grayscale value difference between two image frames being considered. This process identifies any pixel as either being a "changed" or "unchanged" pixel when a function of its grayscale value difference is respectively greater than or smaller than a certain predetermined decision threshold. This kind of method often suffers from noise due to global thresholding and inaccurate moving object boundaries due to occlusion areas. Moreover, this method is very reliable for abrupt changes but not so effective for gradual changes. A video key frame is the frame that can represent the salient content of a video shot. Key frames provide an abstraction for video processing. One important class of the

methods is shot boundary based approach [3]. Another important class is unsupervised clustering based approach [4]. However, most of the key frame estimation methods perform object segmentation based on low-level image features and other readily available information instead of semantic primitives of video, such as objects of interest, actions and events. Thus it cannot satisfy the requirements of a video surveillance system. All the above object segmentation approaches are frame-based techniques. In this thesis, we introduce a novel statistical analysis method based on the stlCA. The stICA model is used to formulate the spatial and temporal independence of the different moving objects. The solution of the stICA model can therefore identify these objects. In recent years, the independent component analysis (ICA) based techniques are getting much attention in video processing. The ICA based techniques have been applied in many areas of signal processing, medical application, neural networks, information theory

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

3 and telecommunications. The ICA can be used in two complementary ways to decompose an image sequence into a set of images and a corresponding set of time-varying image amplitudes. The spatial ICA (sICA) [5] finds a set of mutually independent component (IC) images and a corresponding set of unconstrained time courses, whereas the temporal ICA (tICA) [6] finds a set of IC time courses and a corresponding set of unconstrained images. However, the sICA and tICA can only seek either the ICs of images (frames) or the time courses, respectively. As shown by McKeown [5], the sICA extracts the independent images but these images' corresponding temporal sources could be highly correlated. This is undesirable for object-based video sequence analysis, since the corresponding time courses for the independent objects should be independent as well. The stICA, the generalization of the classic ICA, can blindly separate the independent sources from their spatial and temporal mixtures. It was initially developed in functional magnetic resonance imaging

(fMRI) [8].

1.3

Objectives

The presented research focuses on the video sequences taken with a still camcorder. We assume that there is a stationary background in each frame. The objects and background can be considered the spatial ICs and the corresponding time courses can be considered the temporal ICs. The following are the objectives of our proposed framework in this thesis:
1. To verify/assess the applicability of the stICA model for video sequences.

This

involves segmenting objects of interest from a stationary background in every video frame. 2. To deal with the limitations of the stlCA model on video sequence applications, since objects of interest and their background are not linear combination. 3. To show that the algorithms proposed in this system are effective.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4 The novelty of the proposed methods in this thesis lies in the extraction of semantic

moving objects through a background separation technique in a complex environment and in the processing of every independent frame of the video sequences. The contributions of this thesis consist of 1. A new method of analyzing video sequences by the stICA model. 2. A novel compensation method to deal with the nonlinear combination problem in the stICA model for video sequences. 3. The integrated post-processing techniques based on wavelet analysis, edge detection with region growing and multiscale segmentation approaches.

1.4

Proposed Approaches and Methodologies

To achieve the goals mentioned above, the proposed system involves the following modules as stated in Figs. 1.1, 3.2, and 5.1 [9] [10]: · The stICA is applied to the video frames to separate the spatial and temporal signals. · The signals obtained after the stICA are further processed in the first iteration, where wavelet analysis, edge detection with region growing, and multiscale image segmentation techniques are employed to improve the accuracy. · In the second iteration, a compensation approach is introduced to deal with the nonlinear combination problem of the stICA. A frame object indexing technique is then performed to reconstruct the sequence of frames containing only the objects. More precise video objects are extracted in this iteration.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1.5

Overview of the Thesis

5

This thesis will summarize the ICA and other related technologies in chapter 2. In chapter 3, the stlCA model is used to formulate video sequences. Chapter 4 and chapter 5 elaborate on all the methodologies applied in the proposed two-iteration approach. There are simulation results and summaries from chapter 3 to chapter 5. Finally, chapter 6 summarizes all work included in this thesis and points out some possible future work that might improve the current stlCA model.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

6

Original video sequences

Video capture device

Video frames f.
I

Algorithm

The first iteration of algorithm

The second iteration of algorithm

Segmented frames with objects OJ only

Processed video sequences
Figure 1.1: Block diagram of the framework. i and j are the indices of frames and objects respectively.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 2 Principle Component Analysis and Independent Component Analysis

I

N this chapter, the basic concepts of principle component analysis (peA), singular value decomposition (SVD), whitening, leA and stIeA are introduced. This chapter

is a summary of the work stated in [11] [12] [8].

2.1
2.1.1

Principle Component Analysis, Singular Value Decomposition, and Whitening
Principle Component Analysis

The peA is potentially valuable for applications involving reduction of the dimension of multivariate data. We suppose that X=[Xl, "',

xnV is a zero-mean vector, and fix is a
(2.1)

vector with its mean values. C x is the covariance matrix of x such that

Since the mean of vector x is zero, i.e. fix=O, C x is given by the correlation matrix (2.2) The goal of the peA is to find an nxn orthogonal matrix W=[Wl, "', determines a linear transform of x, i.e. y=W
T

wnl

that

X.

It can be proven that such an orthogonal

transform does not change the total variance of x. This is true because the orthogonal

7

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

8
transform changes neither the angles between x and y nor the vectors' lengths, which means [12] {total variance of Xl,
... , Xn}

= {total variance of Yl, ... , Yn} = Al + ... + An,

(2.3)

where Aj (j=1, ... , n) are the eigenvalues of CX' The solution to the PCA is given by the unit-length eigenvectors el, "', en of CX' Thus we have wl=el, ... , wn=en . The detailed solution of eigen decomposition can be found in [13]. Compared with the SVD, eigen decomposition is only valid for a given square matrix [14] while the SVD is valid for any given mxn matrix [12]. Thus in practical applications, the SVD is the main tool used to perform the PCA. In the next subsection, the general idea of the SVD will be introduced.

2.1.2

Singular Value Decomposition

The SVD is one of the most widely used matrix factorizations in applied linear algebra. The SVD of A involves an m x n "diagonal" matrix L; of the form

(2.4)

where D is an r x r diagonal matrix for some r not exceeding the smaller of m and n. Let A be an m X n matrix with rank r. Then there exists an m x n matrix L; as in Eq. (2.4), where the diagonal entries in D are in the first r singular values of A,
(T12:

.. '2:(Tr>O, and there exists an mxm orthogonal matrix U and an nxn orthogonal matrix
V such that [12]
A=U~VT.

(2.5)

Any factorization A=UL;V T , with U and V orthogonal and L; as in Eq. (2.4), is called an SVD of A. The matrices U and V are not unique, but the diagonal entries of ~ are necessarily the singular values of A.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

9
Since A is an m x n matrix, AT A is symmetric and can be orthogonally diagonalized [12]. Let
[VI, .. "

v n] be an orthonormal basis for n-dimensional space (Rn) consisting

of eigenvectors of AT A. Let AI, "', An be the associated eigenvalues of AT A. Then, for

1:S i:Sn,
IIAvill2 (AVif AVi = vf AT AVi Vf(AiVi) Ai(vf Vi) = Ai,

(2.6)

where Vi is an eigenvector of AT A, and AiVi=AT AVi. The singular values of A are the square roots of the eigenvalues of AT A, denoted by 0"1, ... , an. That is, O"i=A, for 1<i:Sn, the singular values of A are the lengths of the vectors AV1, ... , Avn . There is a theorem concerning about the rank and the singular values [12]: if an mxn matrix A has
T

nonzero singular values, 0"12:·· '2:O"r>O with O"r+1 = .. '=O"n=O, then the
T.

rank of A is equal to

The SVD is based on the property of the ordinary diagonalization that can be imitated for rectangular matrices. Let us denote the symmetric matrix AT A by B. The eigenvalues of B determine how much of the energy of B is distributed along the directions specified by the eigenvectors. If BX=AX and IIxlI=l, then

IIBxl1 = IIAXII =

IAI·

(2.7)

If Al is the eigenvalue with the greatest magnitude, then the corresponding unit eigenvec-

tor

VI

identifies the direction along which the stretching effect of B is the greatest. This

is, the length of Bx is maximized when X=V1, and IIBv111=IAll, by Eq. (2.7). Lay [12] describes the relationship between the PCA and the SVD as follows: if C is an mxn matrix of observation with zero mean, and if A=(1/.vn=-I)cT , then AT A is the covariance matrix of C. The squares of the singular values of A are the eigenvalues of AT A, and the singular vectors of A are the unit eigenvectors of C. Through the SVD, the unit eigenvectors of the image matrices can be obtained. Thus the SVD is widely used to perform the PCA.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

10

2.1.3

Whitening

Whitening is a useful preprocessing technique in signal processing. The term "white" comes from the fact that the power spectrum of white noise is constant over all frequencies, somewhat like the spectrum of white light contains all colors. A zero-mean random vector

Y=[Yl, "', YnJT is said to be white if its elements Yi are uncorrelated and have unit
variances:
(2.8)

Generally, the objective of whitening is: Given a random vector x with n elements, find a linear transformation V into another vector y such that

y=Vx
has elements that are uncorrelated and have unit variances. Let us denote the covariance matrix of x by whose columns are the unit-norm eigenvectors diagonal matrix of the eigenvalues of

(2.9)

ex' Let E=[el' "', en] be the of ex' Let D=diag(Al' "', An)

matrix be the

ex'

Then a linear whitening transform is [13] [11]

(2.10)
It is easily proven that the matrix V of Eq. (2.10) is indeed a whitening transformation.

In fact,

ex can be written in terms of its eigenvector and eigenvalue matrices E

and D as

Cx=EDET [13], where E is an orthogonal matrix satisfying ETE=EET =1. It holds that:

(2.11)
The covariance of y is the unit matrix, hence y is white.

2.2

Independent Component Analysis

Imagine that there are two people speaking simultaneously in a room. Two microphones record these voices and give two time signals that can be denoted as Xl(t) and X2(t). Each

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

11 of these recorded signals is a weighted sum of the speech signals Sl(t) and S2(t) given by

the two speakers, respectively. Usually, we express them as linear combination:

(2.12) (2.13)
where the
aij

with i,j=1,2 are the parameters that depend on the distances of the micro-

phones from the speakers. It would be very useful and challenging if we could restore the original signals Sl(t) and S2(t), by using only, the recorded signals Xi(t). This is called the "cocktail-party problem" . This seems to be an impossible task since we know neither new tool to estimate both
aij

aij

nor Si (t). One relatively

and

Si

(t) relies on the use of the statistical information of

the signals Si(t). This tool is named independent component analysis (ICA). In the ICA, the observed random vector x is modelled as

x=As,

(2.14)

where the mixing matrix A is assumed to be square, i.e. the number of ICs is equal to the number of observed mixtures; and s is the original signal vector. This model can also be written as
n
X

=

Laisi
i=l

(2.15)

where ai is the column vector of A and n is the total number of ICs. By definition, elements variables such that
n
Si

are statistically mutually independent (zero mean) random

p(s)

=

IIpi(si).
i=l

(2.16)

Eq. (2.14) is the basic ICA model. The ICA model is a derivative model, which means it describes how the observed data are generated by a process of mixing the components

Si. The ICs Si are latent variables, meaning that they cannot be directly observed. Also
the mixing coefficients
aij

are assumed to be unknown. The ICA problem now becomes

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

12 the estimation of both the ICs s and the mixing matrix A using only the observation x. This is a type of blind model identification. There are some assumptions underlying the ICA method.
1. The source signals are assumed to be statistically independent.

2. The ICs must have non-Gaussian distributions. There are two ambiguities in the ICA model in Eq. (2.14):
1. The variances(energies) of the ICs cannot be determined.

Since both s and A are unknown, any scalar multiplier in one of the sources

Si

could always be cancelled by dividing the corresponding column ai of A by the same scalar. 2. The order of the ICs cannot be determined. This is also due to the indeterminacies because both s and A being unknown. We can freely change the order of the terms in Eq. (2.15), and call any of the ICs the first one.

2.3

Comparison of PCA, Whitening and ICA

To transform some given random variables into uncorrelated variables, whitening or the PCA is the straightforward method.· However, whitening or the PCA cannot recover the ICs from these given random variables. Two random variables Yl and Y2 with zero mean are uncorrelated if their covariance
IS

zero:
(2.17)

Since their mean values are zero, E{Yl}=E{Y2}=O. In this case, the covariance is equal to the correlation corr(Yl,Y2)=E{YlY2}, and uncorrelatedness is the same thing as zero correlation [11].

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

13 If two random variables are independent, they must be uncorrelated. Furthermore,
for any variables derived from certain functions of these two variables, they must be uncorrelated as well. Suppose we have two functions

f 1 and f 2 for two independent

random variables Yl and Y2 respectively, we have:

E{fl(yd!2(Y2)} -

JJ!I(Yl)!2(Y2)P(Yl, Y2)dy dY2 JJfl(Yl)!2(Y2)Pl(ydp2(Y2)dy dY2 Jit (Yl)Pl (yddYl ! !2(Y2)P2(Y2)dY2
1 1

E{fl (Yl)} E{!2(Y2)},
which verifies that the variables !I(Yl) and f2(Y2) must also be uncorrelated.

(2.18)

However, on the other hand, uncorrelatedness does not imply independence.

For

example, suppose that (Yl,Y2) are discrete values and follow a distribution such that the probability of the pair being equal to any of the following values: (1,0), (0,1), (-1,0) and (0,-1) is

i . Obviously Yl

and Y2 both have zero mean values. In this specific example,

Yl, Y2 are uncorrelated, based on the calculation as follows:
4 4

COV{Yl' Y2} = corr{Yl, Y2} = LYliY2iP(Yli, Y2i) = La' P(Yli, Y2i) = i=l i=l On the other hand,
4 4

o.

(2.19)

E{YiJA} = LyiiyiiP(Yli, Y2i) = La' P(Yli' Y2i) = a
i=l i=l

(2.20)

(2.21)

E{yIyni=E{ynE{yn. It violates the condition in Eq. (2.18), so the variables cannot
be independent. Whiteness is a slightly stronger property than uncorrelatedness. Whitening random vector y with zero mean will make its components uncorrelated and their variances equal unity. As shown in Eq. (2.8), the covariance matrix of y is E{yyT}=I. Whitening can be done by using eigenvalue decomposition of the covariance matrix as well as the SVD.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

14 How far is the whitened data from being independent? Hyvarinen et al. showed that

"whitening is only half ICA" [11]. Suppose that the data in the ICA model is whitened. Whitening transforms the mixing matrix A into a new one, .A. We have from Eqs. (2.9) and (2.14)

y = VAs = .As.

(2.22)

Since .A=VA is orthogonal, E{yyT}=.AE{sST}.AT =1, which means the searching for the mixing matrix can be restricted to the space of orthogonal matrices. Instead of having to estimate the n 2 parameters that are the elements of the orthogonal matrix A, we only need to estimate an orthogonal mixing matrix.A. This orthogonal matrix has n(n-l)/2 degrees of freedom. The complexity of the ICA problem is reduced partially, "ICA is solved on the half way" [11]. The following example shows the fact that only non-Gaussian variables are accepted in the ICA, which is explained by whitening. Assume that the joint distribution of two ICs,
81

and

82,

is the standard Gaussian distribution. Their joint probability density function 1 P(SI' S2) = -exp 2K

(pdf) is [11]

[sr + s~]
2

1 [llsI12] = -exp --- . 2K 2

(2.23)

Furthermore, let us assume that the mixing matrix A is orthogonal. For example, we could assume that this is so because the data has been whitened. Using the classic formula of transforming pdf's in [11], and noting that for an orthogonal matrix A -1=AT holds, we get the joint pdf of the mixtures
Xl

and

,X2

p(x .. x,)

~ 2~ exp [_IIA:xll']ldetATI.

(2.24)

Because of A's orthogonality, we have IIAT x1l 2=llx11 2 and Idet AI=l. Note that if A is orthogonal, so is AT. Thus
(2.25)

The orthogonal mixing matrix has no effect on Gaussian pdf, because it does not appear in the pdf. Both the original and mixed distributions are identical. The reason for such un-

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

15 identity is due to the fact that uncorrelated Gaussian random variables are independent. This tells us that the information of the les does not exceed that of whitening.

2.4

The leA Estimation Methods

· The leA by Negentropy A fundamental result of the information theory is that a Gaussian variable has the largest entropy among all random variables of equal variance [15] [16]. From this conclusion, two hints can be obtained:
1. Entropy can be used as a measure of non-Gaussianity, and

2. Gaussian distribution is the "most random" or the least structured among all distributions. Let us define negentropy J as a measure of non-Gaussianity that is zero for a Gaussian variable and always non-negative:

J(y) = H(Ygauss) - H(y),

(2.26)

where y gauss is a Gaussian random variable of the same correlation (and covariance) matrix as y, and H is the entropy. This negentropy is always non-zero and is zero if and only if y has a Gaussian distribution. Using negentropy as a measure of non-Gaussianity has its advantages. It is well justified by statistical theory [11] [17]. However, the computational complexity is very high . · The leA by Minimization of Mutual Information Another approach for the leA estimation, inspired by information theory, is minimization of mutual information. One can discover the fundamental relationship between mutual information and negentropy.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

16 The definition of mutual information I comes from differential entropy. Here we denote m random variables Yi, such that:

I(Yl,···, Ym) =

L
i=l

m

H(Yi) - H(y),

(2.27)

where i=l, ... , m and y is the vector containing Yl, ... , Ym. Mutual information is a natural measure of the dependence between random variables. In fact, it is equivalent to the Kullback-Leibler divergence [11] between the joint density f(y) and the product of its marginal densities; a very natural measure for independence.
It is always non-negative, and zero if and only if the variables are statistically

independent. To show the relationship between mutual information and negentropy, an important property of mutual information is that if an invertible linear transformation y=Wx exists then Eq. (2.27) can be expressed as [17]

I(Yl,···, Ym) =

L

H(Yi) - H(x) -log Idet WI·

(2.28)

Let us assume that Yi is whitened (Yi is uncorrelated and has unit variance) E {yyT}=WE {xxT} W T =1. We can get detl
1 = det(WE{xxT}WT )

(det W)(det E{xxT } ) (det WT)

(2.29)

and this implies that detW must be constant since detE{xxT } does not depend on W. Moreover, for Yi of unit variance, entropy and negentropy differ only by a constant and the sign, as can be seen in Eq. (2.26). Thus we have
(2.30)

where C is a constant that does not depend on W. This derivation shows that the

leA estimation by minimization of mutual information is equivalent to maximizing
the sum of non-Gaussianities of the estimates, when the estimates are constrained

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

to be uncorrelated.

17 This constraint can simplify the computation considerably.

Thus mutual information gives another rigorous justification for finding maximally non-Gaussian directions [11] [17] . · The lCA by Maximum Likelihood Estimation Starting from the density Px of the lCA model in Eq. (2.14), we can get [11]

Px(x) =1 det W 1Ps(s) =1 det W 1IIPi(SJ,

(2.31)

where W=A -1, and the Pi are the densities of the lCs. If we denote W=[W1' "',

wn]T, we have
Px(x) =1 det W 1 IIPi(wT x).
i

(2.32)

If there are K observations of x, denoted by x(I), "', x(K). Then the likelihood can be obtained as the product of this density evaluated at the K points. This is denoted by Land considered as a function of W [18]:
K
n

L(W) = II IIPi(wTx(t)) 1 detW
t=1 i=l

I·

(2.33)

Because many density functions contain an exponential function, it is more convenient to deal with the log-likelihood function
K
n

logL(W) = 'L'Llogpi(wTx(t)) +Klog 1detW
t=l i=l

I·

(2.34)

To simplify notation, we can denote the sum over the sample index t by an expectation operator, and divide the likelihood by K to obtain

K log L(W) = E{'L logpi(wT x)} + log 1det W 1
i=1

1

n

(2.35)

This expectation is not the theoretical expectation, but an average computed from the observed sample. Gradient methods are the simplest algorithms to maximize the likelihood. The BellSejnowski algorithm [6] is one of the most popular maximum likelihood estimation

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

18 techniques. Bell et al. showed that the gradient of the log-likelihood in Eq. (2.35)
is:
(2.36)

Here g(WX)=[gl (wi x), "', gn(w;x)] is a component-wise vector function that consists of the score function
gi

of the distribution of
gi

Si,

which is defined as
(2.37)

=

(1OgPi )' =Pi -.
Pi

,

This gives the following algorithm for maximum likelihood estimation:
(2.38)

As it is a stochastic version of this algorithm, the expectation is omitted. In each step of the algorithm, only one data point is used:
(2.39)

Due to the inversion of the matrix W that is needed in every step, this algorithm converges slowly. The convergence can be improved by using whitening [6].

2.5

Spatiotemporal leA

The stlCA is the generalization of the classic ICA. The distinction between the ICs and the mixing matrix is completely abolished. Considering the data with n observed vectors as its columns: X=[X1' .. " xn], and likewise for the ICs 8=[Sl' . ", sn]. The ICA model can be expressed as X=A8. Taking a transpose of this equation, we have
(2.41) (2.40)

Now we find that the matrix 8 is like a mixing matrix, with AT giving the realizations of the "ICs". In the conventional lCA model Eq. (2.14), the difference between sand

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

19
A is due to the statistical assumptions made on s. Now for the stICA, the independent

constraints are made on both A and S. The stICA was initially developed for fMRI that is a form of magnetic resonance imaging (MRI) of the brain that registers blood flow to functioning areas of the brain [7]. The fMRI signal associated with a given voxel is affected by a subject's general arousal levels, the experimental task being executed, drifting sensor outputs, and noise. Thus the signal at each voxel consists of a mixture of underlying source signals (Fig. 2.1). Stone uses the stICA to separate signal mixtures into a set of statistically independent signals [8]. He describes a matrix containing a sequence o(n fMRI mixtures X image
Xi

=

[Xl, "', xn]. Each

is an m x 1 vector. A linear decomposition into k modes is defined by a matrix

factorization like Eq. (2.40) (2.42) where S

= [Sl' "', Sk], T = [t1' "', tk] and A is a diagonal matrix of scaling paramSi

eters. The independent image vectors

are the columns of spatial images S and the

corresponding independent time courses ti are the columns of T. Using the SVD [12], fMRIs are decomposed into two parts, eigenimages U and corresponding eigensequences V: (2.43) where U is an m x k matrix of k:;. m eigenimages, V is an n x k matrix of k:;. n eigensequences, and
~

is a diagonal matrix of singular values. In order to determine the ICs S

and T, two kx k unmixing matrices W sand W T are assumed to exist such that

S=UWs ,
and

(2.44)

T=VW r ·

(2.45)

(2.46)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

20
Independent components maps Measured fMRI signals Mixing @t= 1

1

OJ
Mixing @ t= 2

·
·
· ·

·

Mixing matrix

I-il
~
~

I.

hs = H(a(UW s)),

(2.47)

· ·

·

Figure 2.1: Illustration of fMRI mixing.

Given that X=UVT=SAT T , it can be shown that WT=(WSl)T(A-l? To find the unmixing matrices W T and W s, it is necessary to simultaneously maximize a function hST of the spatial entropy

and temporal entropy

hr =
where a and
T

H(r(VW T )),

(2.48)

approximate the cumulative density function (cdf) of each of the spatial

source signals and temporal signals, respectively. The function h to be maximized is defined as:
hST(W s)

= ahs + (1 - a)hT'

(2.49)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

21 where a is a weighting factor given to spatial and temporal entropy. To optimize these two entropies by maximum likelihood estimation [6], their notations need to be changed to: (2.50) and
hT =

where

Sij

and t ij

n j=l i=l are the corresponding elements of Sand T in Eq. (2.42).

log I W

T

I +- 2:: 2:: logT{(t ij ),

1

n

k

(2.51)
ai

and

Ti

are

the cdfs of the spatial and temporal signals, respectively. Their derivatives a~ and T[ are the corresponding pdfs. One can recover the spatial signals and the time courses at the same time using maximum likelihood estimation, which is similar to the conventional IGA [11] approximation techniques.

2.6

Summary

Let us review the procedures to find the IGs from the mixed observed data. The basis of this approach is that if the model in Eq. (2.14) holds, then the IGs corresponding to the uncorrelated one-dimensional projections are maximally non-Gaussian. For an observed random vector x, a vector
Wi

is sought such that (2.52)

have a maximally non-Gaussian distributions and are mutually uncorrelated E {SiSj }=O, when i#j. A simple way to do this is to whiten the data, and then seek orthogonal, non-Gaussian projections. This is justified since uncorrelatedprojections in the original data correspond to orthogonal projections in the whitened data, and vice versa. Thus, a two-step process is used to estimate the IGs: 1. The observed vector x is transformed by a whitening process y=Vx such that the elements of yare of unit variance and uncorrelated, i.e. E {yyT}=I.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

22 2. An orthogonal matrix W that maximizes the non-Gaussianity of the elements of

s=Wy can be obtained.
For the stlCA, there are more constraints on both A and s, so the notations are changed to Sand T, respectively. The algorithm for maximizing the independence on S and T is the same as the lCA. Through the stlCA approach, the lCs in Sand T can be found.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 3 Formulation of the stICA Model for Video Sequences

F
3.1

ROM the last chapter, we can see that the ICA is an ideal tool for data analysis, especially for source data separation. In this chapter, the ICA is employed to extract

objects of interest from video sequences.

Formulation of the stICA Model for Video Sequences
... , fN ], where fi is an M x 1 column

Let us denote a video sequence with n frames as F=[fb

vector representing a frame that contains M pixels. These image vectors are constructed by taking the column-wise elements from the frame images. Thus the dimension of matrix

F is MxN. The mutual independent objects of interest are denoted as 0=[01, .",
where vector
0i 0i

OK],

is constructed in the same way as should be the same as

fi

and K 5:.N. The dimension of the object

t

M x 1. Thus the dimension of 0 is M x K. If the video

sequence is captured by a fixed camera, for example in the surveillance security system, the background is a constant vector. To simplify the work, the stationary background can be considered as a vector of 0, say
OK.

The independent temporal signals time courses

A=[a1, "', aK] affect the objects on every time unit. Again, we use the same method
to construct the time course column vector ai. In every time unit there should be time Courses affecting each object and the dimension of any time course vector ai should be

23

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

24
equal to the number of video frames, i.e. N x 1. This means that each column of A is the time signature for the corresponding objects in

o. The dimension of A is

N x K, where

K SN. Because the background is stationary, the corresponding time course vector aK
has no effect on it, which means all elements of vector aK have value 1. We have (3.1) To find out each object's effect on the video frames, we expand the matrices:

a~K
aU 0u [
a21011 a

aNI]

al1~Ml
[

a21 0 MI

aN2~M2
F.

N2 12 0

]

alKOIK

+ ... +

aIK~MK

.

(3.2)

A function 9 is assumed to describe the object oi's contribution to
matrices expansion, we can see:

From the above

(3.3) These equations reveal the fact that
0i.
ai

is the time signature for the corresponding object

We can rewrite Eq. (3.1) in vector format as:
K T F = ~Oiai.
A ' "

(3.4)

i=l

To find the element construction in j th video frame

fj (j = 1, ... , K),
0ik

we need to utilize

the linear combination relationship between the spatial elements signals
ajk

and the time sequence

from previous assumptions (Eqs. (3.1) and (3.2)):
iij =

L
k=l

K

°ikajk,

(3.5)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

25
where i=l, ... , M. This equation reveals that an element at a specific location in a frame is the linear combination of the elements at the same locations of all the independent spatial objects at a certain time moment i; i.e. the ith element in the jth video frame is the linear combination of all the ith elements in all the independent object vectors
·.. , OK
01,

at ith moment.
Independent components Video frames Mixing@,.-t_=_1_ _ _ _ _ _ _ _-,

Background

Object1

Background

Object 3

Object 1

Object1

I

Object

21

Background

Mixing Matrix

I

Objoct

21 Objoel3

1--_--, Back

round

Object 2

Object 3

Figure 3.1: Illustration of video frame construction by mixing objects.

Fig. 3.1 demonstrates how the stICA model is applied to video frames. At a certain time moment, a video frame consists of a linear combination of all the objects, including the background. For example at t=l, video frame 1 is obtained by the linear combination of the spatial

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

26 ICs on the left-hand side of Fig. 3.1. Frame 1 carries the information of the background, object 1 and object 3. In this way, different video frames are constructed. Please note that the video frames cannot be the linear combination of the ICs that we want because some of the background is blocked by the moving objects in the video frames. This condition violates the stICA assumption. Thus. we need to compensate for the background information that is lost due to object blocking. In this way, the assumption of linear combination may hold so that the stICA requirements are met. Here we denote the ideally blocked background information by Ai in ith frame fi' such that (3.6) where the dimension of

Ai

is also M x 1 and i = 1, .. " N.

Between the practical video frame model in Eq. (3.6) and the fitting model in Eq. (3.4), there is a gap

Ai that affects the accuracy of the stICA approach on video sequences.

This

problem is dealt with by our proposed methods in the following chapters.

3.2

The stICA Based Video Segmentation Approach

In this section, we will introduce an stICA based iterative approach, which can segment semantic video objects without any human intervention. To deal with the nonlinear

combination problem (shown in Eq. (3.6)), we set up a two-iteration scheme (Fig. 1.1). In the first iteration (block diagram in Fig. 3.2), the stICA model is applied to the captured video frames. The maximum likelihood estimation is employed on both spatial and temporal signals. The Bell-Sejnowski algorithm is implemented to find the unmixing matrices, where the ICs
0i

and

ai

are substituted for both

Si

and

ti

in fMRI (Eqs. (2.42)-

(2.46)). However, since the video frames cannot be the linear combination of the objects and their background, the recovered spatial signals
0i

are still coarse representation of

the objects (Fig. 3.3). Among all the recovered spatial signals, only the background image is clear. We can subtract it from all original video frames to get the preliminarily processed images which only contain objects (Fig. 3.5). Post-processing techniques are

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

27
Grey level frame
f;

Find stationary background Ok by the stlCA

Preliminarily processed images
f(Ok

Wavelet analysis with overlapping window to get the ROls

Edge detection with region growing technique

Multiscale segmentation

I
Approximately extracted objects

Figure 3.2: Block diagram of the first iteration.

then required to refine the object segmentation, which will be introduced in the next
chapter.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

28

(a)

(b)

(e)

(d)

Figure 3.3: Spatial

SOlll'C<c

signals from the first stICA processillg.

3.3

Shnulation of the stICA applied to Video Processing in the First Iteration

In our experiments, if \yithout further notice. the propused ::;ystclU is applied to the video sequence "Hall "t\lonitor" \yith
~J.28-s('cond

duration. There arc altogeth('l" 280 frames,

each of \vhich has 240 x 360 pixels and 2SG grayscale levels, i.e. there drc 2('30 illlages
generated. \Vc suppose t hat
en~ry

video franw

COllt ;lillS at

lo;)st ono ohj('ct of interest.

This means there are no pure "background" images.

A set of frame is selected from these 280 frames for further proccssmg. To avoid
interference between close objects, frames are select ed frolll the sequence at a constant

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

29
"P,'--oc-es-'.S;;"-te-'p1;-----'

I P,OCIm Step 2

i
I
1

Run,dCA
P,ace.. Step 3

FindROI

IP,oce.. Step 4
E><lracl Object,

I P,oceu Step 5
I
1

E.llactedVideo

running siICA. selected objects are captured but post-processing is performed to improve qualities ... ,

Figure 3.4: A CUI for the st ICA baticd object. extraction in \-icico sequellces.

interval. We set up a GUI (graphic user interface) that can show the processing details step by step (Fig. 3A). The program allows
OllC

to define a f1"<11ne selection intcrnll.

Based on the frallle selcdioll mte. a lHllU1Jl'r of frames is selected from the 280 frcullcs alld

number of spatial output. images

ClS

input frames.

Among the output images in Fig. 3.3. only the bnckground imnge is dear. l\Ieamvhile. there are a llumber of ulHlcsirahlc outpnt images (Fig. 3.3(b)-(d)). Tlw r('(1son is tlint th(' pixels representing objects ill the video frames are not the linear combination of the pixels representing objects and the background in recovered image sigrwls. In other words, these video frames are not a linear mixture of all the independent sources, namely the objects and background. Since the Lackground image is dear among all the outputs, we can

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

30

subtract it frolll all original video frames to get the prelimillarily proccssed imagcs which ouly contain objects as shown in Fig. 3.5.

(a)

(b)

(e)

(d)

Figure 3.5: Prclilllinaril.y processed images from the first sHeA processing sllbtrac:tiOlI.

Iu these images, we

("all SC'C

cxtcnsiw' noise. Post-processing tedllliques, which ,yill be

prescuted in the llext chapter, are thus required to refine the object segmentation.

3.4

Summary
;-)11

In this chapter,

stICA ll1odC'] is formulated for vid('o S0qllC'll(,('S. A hnl-iteration ap('hapt(~r,

proach is proposed to segment. llloving objects from a "ideo sequence. III the next

the post-processing techniques will be prescnkd. Based on the first iteration results, the nonlinear cOlllbination problem will be dealt with in the second iteration.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 4 Post-processing in the First Iteration

T
4.1

HE stleA approach alone cannot provide a satisfactory object segmentation result. Some post-processing techniques are required to fine tune the output images. In

this chapter, we introduce post-processing techniques based on wavelet analysis, edge detection, region growing and multiscale image segmentation. These methods are applied sequentially to segment the object.,of interest.

Using Wavelet Analysis to Locate Regions of Interest

As an ideal tool of image analysis, the wavelet transform performs well in characterizing singularities [19] [20]. In other words, large coefficients represent edge transitions in the wavelet domain. The wavelet transform decomposes an image into three wavelet subs'paces (LH, HL and HH) and one scaling subspace (LL). A single level of 2D wavelet decomposition is shown in Fig. 4.1. The three wavelet subspaces capture image details along the vertical, horizontal and diagonal directions, respectively. We use the HL subspace to detect the horizontal boundaries of the image objects and the LH subspace to detect the vertical boundaries. First let us focus on the HL subspace to make an illustration. As we know, image object boundaries are represented by large coefficients in the wavelet domain. Thus in the HL subspace, image horizontal edges are 31

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

32
,.----<

H

r--'

H

--"-L

-

Previous level

Next level

--'---'

H

L

---<

---

L

Figure 4.1: Illustration of one stage of 2D wavelet decomposition.

represented by large coefficients. For other image areas where there are no horizontal edges, basically there are no large coefficients in such areas in the HL subspace. Thus we can apply a slide window in the HL subspace and let it slide from one side of the image to the other side horizontally. While it is sliding, we observe the coefficients along each column and use the coefficient with the largest absolute value to represent such column. According to the wavelet features we explained before, if this coefficient has a very small value, this indicates that there are no image horizontal edges along this column. If the value is large, it means there are some horizontal edges. Through this method we can define the horizontal scope of the image ?bjects in the HL subspace, which can bring us the horizontal region of interest (ROI) (ROIHLhorizontal) in the wavelet domain. For any spatial signal after the stIeA processing, we define W as the HL subspace at the Nth level of the wavelet decomposition and
Wij

is the coefficient in that subspace,

where i, j are the indices of rows and columns of W, respectively. We also use a vector

\lI={ 'ljJl, ... , 'ljJq} to represent the ensemble of those largest coefficient values in the HL
subspace, where 'ljJj=maxi IWij I is the largest absolute coefficient value of column j in the

HL subspace. Here q is the total number of columns in the subspace, which is decided by
the level of the wavelet decomposition. For example, if the dimension of an image is r x r,

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

33
then

(4.1)

Largest absolute values of the wavelet coefficients
3.-----.-----_r-----,------.-----.------r----~r_----._----_r----_,

2
1

20

40

60

80

100'

120

140

160

180

200

Mean values of overlapping sliding windows
2.------.------.-------.-----_r------._----~--~--_r------._----_,

1.5
1

0.5
O~----~------~----~~-----L------~----~------~------~----~

o

~

~

00'

00

100

1~

1~

160

180

Mean values after comparing to threshold
2.------.------.------,r_----_r------._----~------_r------._----_,

1.5
1

0.5
O~----~------~------~~---L----~~----~~-----L------~----~

o

20

40

60

80

100

120

140

160

180

Figure 4.2: (From top to bottom) a. Maxima of absolute wavelet coefficients; b. Mean values of the maxima in the overlapping windows; c. Mean values after thresholding.

The method stated above to detect a horizontal image edge is based on the detection of large coefficients that represent image edges. The method requires a successive set of large coefficients to detect a single horizontal edge. However, if there are some small coefficients (below threshold) existing among these large ones, we may detect two or more edges where the object only has one. For example, in Fig. 4.2(a), there exist some valleys

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

34

between peaks, which without further processing would give erroneous results. To avoid wrong detection of multiple edges, we apply an overlapping sliding window in the HL subspace. While it is sliding from the left to the right, we calculate the mean value of the largest absolute values within the window and use the mean value to decide whether there is an edge or not. Using this method reduces the likelihood of erroneous results. The overlapping sliding window has two important parameters to control the sliding properties. One is the width of the overlapping sliding window frame and the other is the sliding step. We define the width parameter as 1 and the step parameter as

.

'

1. At the last several steps of window sliding, the number of 'lfJi is less than l, so there is a total of q-l+ 1 steps for the window to slide. We calculate the mean value of 'lfJi within the window at each sliding step as follows:
mk

=

E~+l-l

~=k

'lfJ.
t

1

'

k

= 1" ...

q- 1

+ 1.

(4.2)

The processing result is shown in Fig. 4.2(b). Now the object edges are represented by some large mean values and the image background is represented by some small mean values. To distinguish these two classes, we need to define some thresholds. A threshold detector is set up by comparing the mean values and the global absolute maximum value of the HL subspace wavelet coefficients:
(4.3)

·

where

Q

is an empirical constant, k

=

1,... ·,q+l-1, and i,j

=

1, .. ·,r. We compare the

mean values to the threshold. Once the first mean value that is greater than or equal to the threshold is observed, the corresponding position in HL subspace is recorded as a, the beginning of the edge. We continue to compare values until we observe a mean value
that is less than the threshold. At this point, the position in HL subspace is recorded as

b, the end of the edge. In HL subspace, the wavelet coefficient ensemble can stand for the

region containing the object horizontal edges if these coefficients meet the criteria such that (Fig. 4.2(c)): ROIHL horizontal
=

{i I a < < b} , _i_

(4.4)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

35
where i is the column index. We continue comparing values and this manner all regions containing object edge can be detected. Applying the same method in the LH subspace, the vertical ROI can be defined:
ROILHvertical

= {j

Ic < j

~

d},

(4.5)

where j is the row index; c, d are the starting and ending points of vertical edges, respectively. Thus the rectangular ROIs that contain the objects in the wavelet domain are obtained:
RO Iwavelet =

{i, j liE

RO IHL horizontal,

j E

RO ILH vertical}.

(4.6)

The corresponding ROI in the stICA processed images can be located by using the inverse calculation in Eq. (4.1). The purpose of segmenting a ROI is to decrease computational complexity for later post-processing and to reduce noise so that edge detection techniques and region-based segmentation approaches can achieve better results. Moreover, the object indexing approach that will be introduced in chapter 5 also needs the ROI to detect true objects. The ROI technique can also be applied to the original video frames as the video object tracking method. In the following two sections, two post-processing approaches are applied sequentially: one is edge detection with region growing and the other is multiscale image segmentation.

·

4.2

Image Edge Detection with Region Grewing

The ROIs detected by the presented object detection method based on the stICA represent areas of the objects of interest. However, the ROIs do not contain exact boundary information of the detected objects. The Canny edge detection technique is applied to these rectangular ROIs. This operation renders a binary image, in which Is stand for the object (foreground) and Os for the background. From the binary images, we have obtained prospective object regions from edge detection. However, not all the obtained regions are objects of interest. In the ROIs, the target

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

36

010

/
/

~ ~ ~
(b) Mark Matrix M

010

o

1

0

o

3

0

001
(a) Binary image I

~_~----if-o_3-t-°-3(c) Label Matrix L

Figure 4.3: Region growing technique to label connected pixels. a. Binary edge pixel neighbourhood; b. Mark pixel neighbourhood; c. Label pixel neighbourhood.

objects are generally larger than the other isolated regions. Thus we can discriminate the target objects from those unwanted regions through the comparison of their sizes. Here a region growing technique is introduced to calculate the connected region size. To perform this region growing operation, we fill the interior regions inside the closed edge with the value 1. These closed-edge detections are performed by the Canny technique. Marshall et al. [21] introduced a region growing approach that has the following operating procedures:
1. An initial set of small areas are iter~tively merged according to similar constraints.

2. Start by choosing an arbitrary seed

p~el

and compare it with neighbouring pixels.

3. The region is grown from the seed pixel by adding in neighbouring pixels that are similar, increasing the size of region. 4. This whole process continues until all pixels belonging to a region are processed. In a digital image, if two pixels have similar grayscale values and they are in their neighbours of eight, they are deemed in the same region. In our case, the processed

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

37 images are binary images. If two adjacent pixel values are equal to 1, they are considered to be in the same region. We assume in a binary image I with rx r size, its pixels
ii,j

and

ip,q are in the same region if
ii,j

= ip,q = 1,

where i,j,p, q ::; r,

1p - i I::; 1,

and 1 q -j

I::; 1.

(4.7)

We define two matrices with the same dimensions as I. All pixel values in these two matrices are initialized to zero. One matrix is named "Mark Matrix" and the other is named "Label Matrix". The flag with value 1 is assigned to a certain pixel in the Mark Matrix M to indicate that this pixel has been processed to avoid repeated processing. The Label Matrix L is used to assign a unique labelling integer to each isolated region. Thus the isolated regions can be distinguished by the different labelling integers. The total number of each labelling integer indicates the region size. For example, in Fig. 4.3(a), a seed pixel
ii,j

is randomly chosen, the values of its eight neighbours are checked in a
ii-I,j

clock-wise order. In this case, pixels the same region as
ii,j.

and i HI ,j+1 are equal to 1, therefore they are in

For each pixel that is in the same region as ii,j, its corresponding

element in the Mark Matrix M is set to 1 to indicate that it has been processed such that (Fig. 4.3(b))

(4.8)
Meanwhile, a labelling integer, say 3, is assigned to corresponding elements in the Label Matrix L to represent that region (Fig. 4.3{c))

(4.9)
After the labelling of L is completed, the sizes of all isolated regions can be easily calculated. In Fig. 4.3{c) the total number of labelling integers 3 in matrix L represents the size of this region. Let us explore the proposed region growing approach depicted in Fig. 4.4. First of all, in the binary image I, a seed pixel 1. Pixel value must be 1: ii,j=l;
ii,j

is selected, which must satisfy two criteria:

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

38
Choose new seed pixel i .. =1
'J

I+--------------i

mp,q' mjj are

flagged 1

Ip.q. IjJ are assigned an 1 - - - - - -.. new integer

Ip,q' I jj

are assigned the old inte er

No

I .. is assigned 'J a new integer; miJ is flagged I

No

Yes

Region size threshold detector

Figure 4.4: Block diagram of region growing technique.

2. The Mark Matrix element value cannot be 1: processed. Once a new seed pixel
ii,j

Mi,j =1=

1. Otherwise

ii,j

has been

is chosen, its eight neighbours

ip,q

(I p -

i

1:::;1, 1q -

j

1:::;1)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

39 are examined. There are two underlying possibilities.
1. If i p ,q=1 holds for any p,q

(I p

- i

1::;1, 1q -

j

1::;1), the value of the corresponding

element in the Mark Matrix M, M p,q should be checked. There are two possibilities under this condition: (a) M p,q= 1: this indicates that the pixels corresponding to M p,q and processed. Thus same value as
ii,j ip,q

have been

belongs to the same region as

ip,q,

and

li,j

is assigned the

lp,q. ip,q

(b) M p,q= 0: this implies

has not been processed. If all the neighbours of
lp,q

ii,j

have not been processed,

and

li,j

are both assign'ed a new labelling integer.
ii,j'S

2. If there is no value 1 pixel in the seed pixel means
ii,j

neighbourhood, i.e.
li,j

ip,q=O,

this

is the only one pixel in its region. M i,j is flagged to 1 and

is assigned

a new labelling integer. In this way, all
ii,j'S

neighbours

ip,q

with value 1 are identified. Their Mark Matrix

elements M i,j, M p,q are marked flag 1 after they have been processed. The corresponding Label Matrix elements
li,j

and

lp,q

are assigned the same labelling integer.

This recursive computing method is employed on every unmarked seed pixel. After the seeking is finished, all isolated regions are assigned different labelling integers by the Label Matrix L. A region size threshold detector is used to distinguish the objects of interest from any smaller size regions, which are not the objects of interest and are subsequently removed. This region growing algorithm is simple, easy to implement, and reliable. It employs two ancillary matrices, which are efficient in processing. The Mark Matrix eliminates unnecessary processing, and the Label Matrix makes calculation of the region size easy. Such implementation does not change any pixel in the binary image I.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4.3

Multiscale Image Segmentation

40

Edge detection techniques such as the Sobel method and Canny method work efficiently on sharp edges. However, the processed images after the stICA do not possess such sharp edges. This leads to some false edges that affect further processing. In the last section, we apply the Canny edge detection technique to the rectangular ROIs and then exploit the region growing method to remove small regions that are not objects of interest. This gives us the approximate regions of objects, which are called the object regions. In Fig. 4.5', the objects of interest are obtained by edge detection with region growing to remove the small regions that are disconnected with the objects. However, this approach cannot remove the regions that are connected to the objects. For simplicity, the connected regions are given a new name: connected component. Because of the false edges generated by edge detection, the region growing method cannot accurately identify the edges. Thus a multiscale region-based still image segmentation method [22] [23] [24] is employed on the object regions in post-processing. Note that here the term "multiscale" means the scales of the grayscale variance in a region. A region in this method is defined by measuring grayscale similarity and each region is labelled with a unique integer. The result of region growing shown in Fig. 4.9(c) is combined with the original image in the ROI in Fig. 4.8(a) giving the object regions in Fig. 4.1O(a). Multscale segmentation is then performed on the object regions giving the multiscale segmented regions shown in Fig. 4.10(c). Apparently, segmentation of regions 'Yith similar grayscale generally does not segment the objects of interest in images. A grayscale region may contain multiple objects, or one object may be divided to several grayscale regions. If an image has complex structure, it is difficult to find correspondence between each closed homogeneous region and a specific object. Fig. 4.5 is an illustration of the proposed approach. In Fig. 4.5(c), an object and its connected component are divided to four regions (Rb R 2 , R3 and R 4) according to their grayscale similarities. In this case, Regions Rl R2 and R3 belong to the object of interest. However, we cannot segment Rb R2 and R3 from R4 if only using multiscale segmentation.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

41

Object of interest

component

;1' i 3· i2

representthe pixels in the smoothed regions that are smaller than original objects

(a)

(b)

Object of interest

(c)

(d)

Figure 4.5: Illustration of the procedures of incorporating edge detection and multiscale segmentation. a. Regions obtained by edge detection and region growing; b. Smoothed regions obtained by Matlab Image Processing Toolbox; c. Regions obtained by multiscale segmentation; d. Objects obtained by the projecting operation between (b) and (c).

If we apply a smoothing and projecting approach on the multiscale segmentation

results, the objects of interest can be identified. This indicates that we need to distinguish which regions in Fig. 4.5( c) should belong to the object of interest. The first step is to smooth all the connected regions (Rb R2 , R3 and R 4 ) in the object regions. The

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

42 smoothed results are shown in Fig. 4.5(b). The purpose of smoothing is to reduce the

object region and make sure that there are no extra pixels outside of the objects of interest. The second step involves the removal of the connected component by the projecting operation. This involves mapping the pixels in the smoothed image (Fig. 4.5(b)) to the corresponding pixels in the segmented image (Fig. 4.5(c)). Then the regions labelled by the corresponding pixels in the segmented image are regarded as the desired parts of the object. The theoretical basis for the approach is that the connected components are relatively small and so that smoothing will effectively remove them. After smoothing, their pixels and relevant segmentation labelling information will be removed in the smoothed image plane. Thus the smoothed image only contains the pixels belonging to the object. For example, utilizing the location information of pixels ill i2 and i3 in Fig. 4.5(b) can correspondingly indicate that R}, R2 and R3 in Fig. 4.5(c) belong to the object of interest. Fig. 4.5(d) shows the segmented object of interest that contains R 1 , R2 and R3 only.

In this way, by utilizing wavelet analysis, edge detection, region growing and multiscale
image segmentation approaches on the stICA outputs, objects with shape and boundaries can be approximately extracted.

4.4

Simulations of the Post-processing Techniques in the First Iteration

The first iteration is illustrated in Fig. 3.2. In the last chapter, the inputs for postprocessing are the preliminarily processed images obtained by subtracting the recovered background from original video frames. The preliminarily processed images are processed by wavelet analysis to locate the rectangular ROIs. The ROIs can track the objects of interest, however they cannot describe the exact object boundaries. Thus edge detection and region growing approaches are necessary. They are used to outline the edges and remove the isolated small size regions. After the edge detection and the region growing, there may still be some connected components (e.g. R4 in Fig. 4.5). Connected components and object are given a new name: object regions (e.g. RI-R4 in Fig. 4.5). To remove

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

43 the connected components from an object, the multiscale segmentation technique is applied to the object regions. Through the smoothing and projecting approaches, multiscale segmented regions belonging to the object can be identified.

4.4.1

Simulation of Wavelet Analysis to Locate ROIs

Figure 4.6: An example of 2D wavelet decomposition. a. LL scaling subspace; b. LH subspace; c. HL subspace; d. HH subspace. After the subtraction of the recovered background, the preliminarily processed images contain object but with extensive noise (e.g. Fig. 3.5{a)-{d)). The discrete wavelet

transform decomposes an image into four subspaces: three wavelet subspaces{LH,HL and HH) and one scaling subspace(LL). A scaling subspace (LL) example is shown in Fig. 4.6{a). It is a low frequency approximation of the original image. The other three subspaces LH, HL and HH are shown in Fig. 4.6(b)- 4.6(d). We can see that as we stated

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

44 above, LH, HL and HH describe image details along three directions: vertical, horizontal and diagonal directions, respectively. Thus LH and HL subspaces are used to locate the vertical and horizontal edges. HL subspace wavelet coefficients are used to detect the horizontal edges. An overlapping sliding window approach is then
appl~ed.

The subspace wavelet coefficients

Wij

have

q columns such that:
q=

(~)N X r = (~) 1 x 360 =

180.

(4.10)

Thus vector ll1·has 180 elements ["pI, "', 11'180]' Each element

lPj

is the largest absolute

coefficient value of column j in matrix W. The 60th frame in the video sequence is selected to demonstrate the proposed method. The graph of vector ll1 is shown in Fig. 4.2(a). The edge detection technique is based on a set of large coefficients to detect a single horizontal edge. However, some small coefficients (below threshold) may exist among these large ones. From the curve, decision-making of edge detection might not work because of some valleys between peaks. To minimize this adverse effect, an overlapping sliding window method is employed. This method has two important parameters: a window width of 4 and a sliding step of 1 were found to provide good results in the experiments. Then the number of mean values is
q- l+1

= 180 -

4 + 1 = 177.

(4.11)

This is also the number of steps for the, window to slide from the left to the right of HL subspace. Fig. 4.2(b) demonstrates the application of Eq. (4.2). The rough curve in Fig. 4.2(a) is smoothed and is shown in Fig. 4.2(b). It clear that the result emphasizes the edges on the horizontal direction. Now the object edges are represented by some large
mean values and the image background is represented by some small mean values. To

distinguish the two classes, we need to define a threshold. The threshold is determined by comparing each mean value with the global absolute maximum value of the HL subspace wavelet coefficients. The empirical constant
Q

in

Eq. (4.3) is set at 0.685. This constant proves to be efficient in all test images. The

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

mean values

ml

to

m177

45 are compared with the threshold sequentially. Whenever a mean

value is found to be greater than the threshold, the corresponding position in the HL subspace is recorded. In the selected frames, a = 68. Comparison continues until a mean value is found to be lower than the threshold, which indicates that the horizontal edges end and this position in the HL subspace is recorded as b = 96. The results are shown in Fig. 4.2(c). The mean values out of the range (68:96) are set to be zero. Then the corresponding 68th-96th columns in the HL subspace can construct the horizontal edges of the ROI: ROI HL horizontal = {i I 68 < - i < _ 96}. (4.12)

The horizontal ROI in the original domain is illustrated in Fig. 4.7(a). Note that the horizontal ROI is located between the 136th column and the 192nd column. The locations are acquired by the inverse calculation in Eq. (4.1). Applying the same method in the

LH subspace leads to the detection of the vertical edges and the vertical ROI:
ROI LH vertical = {j 1 24:::; j :::; 89}. Combining the ROIHLhorizontal and ROILHvertical yields a rectangular ROI: ROIwavelet = {i,j 168 :::; i :::; 96,24 < j < 89}. (4.14) (4.13)

The post-processing begins with the wavelet analysis along the vertical and horizontal directions, which can accurately locate the ROIs that contain object of interest. This procedure can significantly reduce the searching range of the object and· thus reduce the computational complexity. For example, in Fig 4.7(a) and (b), we only need to consider an area containing rows 48 to 178, and columns 136 to 192. This rectangular area is 131 x57, which is only 131 x 57 240 x 360
=

8.64%

(4.15)

of the original image area. Fig. 4. 7(b) also shows that the locations of the ROIs are very accurate and the object of interest is completely included within the rectangular ROI. Fig. 4.7{c) shows a "zoom in" video frame from its original size 240x360 to 131x57.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

46 This "zoom in" operation reduces the computation complexity and makes the detection immune to the interferences generated by the stICA (consider reference to Fig. 4.6(a)).

Figure 4.7: (From top to bottom) a. Horizontal ROJ; b. A rectangular ROJ after the horizontal

and vertical wavelet analysis; c. The "zoom in" video frame 60.

4.4.2

Simulation of Edge Detection with Region Growing

The ROls detected by the presented object detection method based on the stICA describe the areas of the object of interest, but the ROIs do not contain exact boundary information of the detected objects. The Canny edge detection technique is applied to these rectangular ROls. This operation renders a binary image, in which Is stand for the

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

47
object(foreground) and as for the background. Fig. 4.8(b) shows this operation. However, in this binary image of the ROI, not all the detected regions are object of interest. For example, in Fig. 4.8(b), besides the moving human object, there are some other regions included, such as the door. In the ROls, the target objects are generally larger than the other isolated regions. Thus we can discriminate the target objects from those unwanted regions through the comparison of their sizes. For example, in Fig. 4.8(b), the size of the moving human is much larger than others'.

Figure 4.8: (From top to bottom) a. Original image in the RaJ; b. Edge detection by the Canny detector.

The region growing approach can categorize these isolated regions. To apply this technique we fill the interior regions inside the closed edge with the value 1. These closededge detections are performed by the Canny technique. Fig. 4.9(a) shows these three isolated regions in whit.e (Is), and the background in black (as). Another two matrices

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

48

Figure 4.9: (From top to bottom) a. Filling regions after edge detection; b. Labelling regions with the same integer; c. Removing regions that are not of interest by threshold detection.

with the same dimensions are defined: the Mark Matrix M and the Label Matrix L. All pixel values in these two matrices are initialized to zero. This region growing algorithm is a recursive computing method (Fig. 4.4). Its implementation is simple and reliable. In the binary image I, the first seed pixel ii,j that meets two criteria (ii,j=l, Mi,j #1) is found by column-wise searching. Its eight connected neighbours ip,q

(I p - i 1::;1, 1q -

j

1::;1) are then checked for both their pixel values and

their Mark Matrix element M p,q. After the checking is finished, the l'vlark Matrix is flagged and the same labelling integer is assigned to li,j , lp,q. One of the neighbours ip,q with

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

49 value 1 is considered to be a new seed pixel on which the same operation is implemented. This operation continues until all pixels are processed. In this way, a region finishes its growing and its corresponding Label Matrix region is assigned a unique labelling integer. For example, Fig. 4.9(b) shows three connected regions that are assigned three labelling integers. The sizes of isolated regions are easily acquired by summing up the number of each labelling integer. We get the sums of labelling integers 0 through 3 as 2023, 457, 2643 and 22. Labelling integer 0 corresponds to the background; labelling integer 2 corresponds to the moving object; and labelling integers 1 and 3 correspond to the non-target regions. Finally, the small regions corresponding to the labelling integers 1 and 3 are eliminated by a region size threshold detector (Fig. 4.9(c)). This threshold is set to 10% of the largest region size (except the background) in the whole binary image. In the test image, the threshold is set at 10% x 2643~264. After threshold detection, only the approximate object of interest remains. Thus image quality improves.

4.4.3

Simulation of Multiscale Image Segmentation

In Fig. 4.9(c), the object regions are obtained by edge detection and region growing. However, this approach cannot remove the unwanted components that are connected to the objects. Such components are caused by the false edges resulting from edge detection. The multiscale region-based still image segmentation method in [22] [23] [24] is employed on the object regions in post-processing. In thE; simulation, we apply this algorithm to the original frame in the area outlined by edge detection. Then we get the multiscale segmented regions in Fig. 4.1O(c). Since the object of interest (the human) and its connected regions are segmented into several regions based on their grayscale similarities, extra information is required to distinguish which regions should be considered as parts of the object. This information comes from edge detection of the object regions. However, edge detection can bring unnecessary connected components because of the false edges. Smoothing the edge detected regions can remove such unnecessary connected

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

50

Figure 4.10: (From top to bottom) a. Object regions in the ROI; b. Smoothed regions from edge detection; c. M ultiscale segmented regions.

components. After smoothing the regions in Fig. 4.9(c), a "slimmer" object is obtained and shown in Fig. 4.10(b). The major unnecessary connected components have been

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

51 removed. We project the pixels after the smoothing operation to the multiscale segmented
image (Fig. 4.10(c)). The regions belonging to the object are identified. The reason for smoothing the binary regions is to make sure that no pixel is projected to the connected components. An example of the extracted object from the original image is illustrated in Fig. 4.11(b).

Figure 4.11: (From top to bottom) a. Original video frame 20; b. Extracted object.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4.5

Summary

52

In summary, the first iteration includes the following steps (as shown in Fig. 3.2):
1. Use the stICA to process the selected frames from a video sequence. The prelimi-

narily processed images are obtained by subtracting the recovered background from original video frames. 2. The preliminarily processed Images are processed by using the wavelet analysis followed by applying overlapping moving windows and a threshold detector to obtain the rectangular ROIs. 3. From the ROIs, edge detection of the extracted object is performed by using the Canny method. A recursive region growing technique is employed to remove the small size regions in the ROIs. The object regions are formed in this step. 4. Multiscale segmentation techniques are applied to the object regions with the smoothing/projecting approach to identify the regions belonging to the object.

In this way, by utilizing the wavelet analysis, edge detection, region growing and
Illultiscale image segmentation approaches on the stICA outputs, the objects with specific shapes and boundaries can be approximately extracted.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 5 A Compensation Approach of stICA for Practical Video. Sequences

T

HE post-processing procedures described in chapter 4 work effectively for the objects with relatively simple background. For the objects used in the tests, the

background regions at the target object boundaries possess grayscale values that are sufficiently different to enable easy distinction. However, if both the background and the objects of interest have similar greyscale values, false regions may be identified as the objects of interest, as illustrated in Fig. 5.3(b) and (c). To deal with this problem and the nonlinear combination problem in the stICA model for video sequences, a "compensation" technique is applied to the stICA in the second iteration of our framework (Fig. 1.1). In the second iteration (Fig. 5.1), satisfactory object segmentation results are achieved by a compensation approach, a frame object indexing method and the post-processing techniques.

5.1

A Compensation Approach of stICA

The major obstacle of the stICA's application to video sequences is the nonlinear combination problem as shown in Eq. (3.6). The nonlinear property of video frames leads to the poor outputs from the stICA when it is applied directly to the video frames. Thus the complicated post-processing methods are required in the first iteration (Fig. 3.2). If we can determine the approximate region .6.i that is blocked by the object in each frame 53

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

54
The background blocked by objects obtained in the 1st interation

The blocked background regions are superimposed on original frames to constitute the compensated frames f.

1
J

Find the independent spatial images 0 by the stlCA

J
Frame object indexing

J
Use the ROls to locate objects in different spatial images o. ,

·

1
Edge detection with region growing technique

1
Multiscale segmentation

1
Accurate objects

Figure 5.1: Block diagram of the second iteration.

fi and "compensate" the blocked background back to each frame (Eq. (3.6)), then we can

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

obtain the ideal frames

fi from:

55

(5.1) where ~i'

Ai, fi

and

fi

are the Mx1 column vectors as stated in chapter 3.

If ~i is ideally located,

-Ai+ ~i

= 0, which means the video frames can fit the stICA

model. In fact, if we get the accurate blocked background information, we can outline the objects of interest and fulfil the video object segmentation task. However, we can only acquire the approximate blocked background information in the first iteration and use it for the stICA processing in the second iteration. The following steps are the procedures of the compensated frames for the stICA processing in the second iteration:
1. The blocked regions of the background are determined by the segmented objects in

the first iteration. The blocked regions are used as binary masks (Fig. 5.4) and the masks are applied to the background image obtained in the first iteration to get the · blocked background information ~i (Fig. 5.5). 2.
~i

is superimposed onto its corresponding original video frame and the compensated

frames are obtained (Fig. 5.6). Comparing Eq. (3.5) and Eq. (5.1) we obtain (5.2) where

-Ai+ ~i

is the major factor that determines the accuracy of the stICA processing

results. We apply the stICA model (Eq. (2.46)) to the "compensated" frames (Fig. 5.6). The stICA estimation algorithm stated in chapter 3 is employed again. The accurate foregrounds (objects) and background are recovered from the "compensated" frames. Fig. 5.2(b)-(d) are the results of the stICA outputs. The edges of extracted objects are much clearer than those in the first application of the stICA (Fig. 3.3(b)-(d)). Their clear edges demonstrate an improvement in the object segmentation quality.

.

------------------------------------------------

-----

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

56

(c)

(d)

Figure 5.2: Spatial source signals from the second stlCA processing:

01, 02, 03, 04.

5.2

Frame Object Indexing Approach

The stICA recovered video objects (spatial signals 0) are clear enough for edge detection. However, due to the ambiguities of the ICA [11], the order ofthe ICs cannot be determined. The order of the ICs is very important for reconstructing the video sequence containing only the objects. Thus, before edge detection, the recovered spatial objects 0 must be indexed according to the order of the video frame F. We propose an indexing method based on the SVD [12] and the corresponding weighting matrices. matrix is a kind of linear combination. We derive the frame object indexing from the compensated video sequence F and its SVD products U and V in Eq. (2.43) such that
(5.3)

Such a weighting

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Since both U and V are orthogonal [8], we could make use of these two equations
= I and U = UE 1/2. We can get the following derivation results

vrv
(5.4)

57

where E is a diagonal matrix with singular values. The multiplication of U with E 1/ 2 can only change the amplitude of U (eigenimages), but can not change the eigenimage indices. Let us suppose that V is a kxk weight matrix. Eigenimage
Ui

(i=l, .. ·,k) is

most affected by the frame that has the largest absolute element in the corresponding column of V. Once we find the indexing relationship between F and eigenimages U, we can proceed to get the
~ndexing

relationship between U and the independent spatial images O.

Referring to Eq. (2.44), we denote spatial ICs

O=UWo ,

(5.5)

where Wo is a k x k unmixing matrix. In the stICA model, 0 is generated by the multiplication of eigenimages U and the unmixing matrix Wo. The indexing relationship between U and 0 can be found in the same manner as that used for F and U. Now the object index in 0 can be referred to the order of F. Here we still need to use the ROls obtained from the first iteration to assure the quality of edge detection. Those post-processing techniques used in the first iteration, such as the edge detection with region growing and multiscale image segmentati'On, are applied to the indexed objects. In this way, the objects with accurate shape and boundaries are extracted.

5.3

Simulations

The methods we used in chapter 4 work effectively for the objects with a simple adjacent background, which means the greyscale of the background pixels are not similar to the target objects. Fig. 5.3(a) and (d) are in this category. However, if both the background

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

58

(e)

(d)

Figure 5.3: The output images from the first iteration.

and the object of interest have similar greyscale values, false regions may be identified as the objects of interest as shown in Fig. 5.3(b) and (c).

5.3.1

Simulation of Compensation Approach of stICA

Fig. 5.4(a)-(d) show the binary masks that are determined by the segmented objects from the first iteration. The blocked background regions are obtained by projecting the masks to the background we recovered in the first iteration so that the compensated video frames are the sum of original video frames and the corresponding blocked background regions. Fig. 5.6(a)-(d) are the examples of the compensated video frames. Then the stICA model is applied to the compensated video frames (Fig. 5.6(a)-(d)). As expected, the second stICA processing detects the object edges accurately. Compared

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

59

(c)

(d)

Figure 5.4: Binary masks determined by the first iteration.

with the results obtained in the first stICA processing (Fig. 3.3(b )-( d)), the edges of the recovered spatial ICs in the second stICA processing (Fig. 5.2(b)-(d)) are clearer and sharper. This represents an improvement the object segmentation quality.

5.3.2

Simulation of the Frame Object Indexing Approach

After the second stICA processing, the stICA recovered objects (spatial signals 0) are clear and the edge detection results are more accurate than the first orie. However, due to the ICA's ambiguities, the order of the ICs cannot be determined. The order of ICs is very important for reconstructing the video sequence containing only the objects.

In the simulation experiment, there are altogether four video frames defined as inputs
to the stICA. Since the SVD is the pre-processing tool of the ICA, we first use the SVD to

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

GO

(e)

(d)

Figure 5.5: Blocked background regions determined by the binary mask.

find the indexing relationship between the video frames F and the eigenimages U. In the eigenimages matrix U, the first principle component
Ul

represents the strongest energy

among all the principle components [12]. Among all the objects, the background has the strongest energy because it exists in every frame of the video sequence. Thus
Ul

should

correspond to the background (a special object). Through the observation of the elements of the eigensequence matrix V, the indices of other objects can be found.

In this case, each video frame contains only one object. So there are altogether four
objects and one background to be indexed. Since we have a total of four eigenimages after the SVD, there should be more than one object to be indexed in a certain eigenimage. We need to find the indexing information of the four objects and a background from these four eigenimages. The eigenimage
Ul

corresponds to the background. To determine

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

61

(c)

(d)

Figure 5.6: a.-d. Compensated video frames for the second stICA processing.

what the indices of the four objects are, the largest absolute coefficients in columns two to four of the eigensequence matrix V found.

0.4899 0.4343 -0.7464 0.0245] 0.4820 -0.1482 0.4756 -0.7302 0.4348 0.4129 [ 0.4948 0.6290 0.5019 0.2218 -0.1661 -0.8002 As can be seen, the third coefficient of column two has the largest absolute value in that column, which means the object segmented from the second eigenimage
U2

will be

indexed as the third frame in the video sequence. This is true because the third frame corresponds to the third coefficient and the frame has the largest contribution to the formation of the second eigenimage and to the object in it. For the same reason, the object segmented from the third eigenimage
U3

will be indexed as the first frame in the

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G2

(c)

(d)

Figure 5.7: The output images from the second iteration.

video sequence. Finally, colullln four has two large coefficients at positions two and four, which means there are two objects to be segmented from the fourth eigenimage
U4

and

their indices in the video sequence \vill be the second and the fourth frames, respectively. The indexing relationship between the Eigcnimages U and the vidco framcs F can be described as follows:

Then we use the Bell-Sejnowski algorithm in the stICA to optimize the eigenimages U and obtain the unmixing matrix W
0

such that O=UW o . In the experiment, W 0 is a

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

63

Figure 5.8: The original video sequence frames.

4x4 matrix 2.1762 -10.9408 -0.8998 -1.4259] 1.4613 -1.9929 -38.6003 2.8184 -40.57.52 2.8608 -0.5246 0.2471 [ 6.9683 35.0712 8.3672 -1.0995 For the same reason outlined ahove, the rrlatiollship betwcPIl U and 0 is

Thus we can map the relationship between F and 0 as follows:

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

64

(a)

(b)

(e)

(d)

Figure 5.9: The eigenimages:

Ul: U2, U3, U4·

04 ----+ f2

02 ----+ f3
04 ----+ f4

The object indexing relationship from F to 0 through U is illu::;trated in Fig. 5.10. In this way, the frame object order can be determined. To compare the segmented image quality in these two iterations, the Peak Signal to
Noise Ratio (PSNR) [25J [26J is employed. The PSl\R is a standard criterion for ohjective

noise measuring in video systems. For example, the image size is 10.1 x N, op( i,j) and 01'( i,j) denote the pixel amplitudes of the processed and reference images, respectively, at the

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

"

__ ;~:i
;II'

I~
I

1
~(f,)

~I.: I
0,,('3)

65

Figure 5.10: Illustration oqhe indexing relationship from F to 0 through

U.

position (iJ): PSNR = 10 . log (255)2
1 MN

Li=l Lj=l(Op(~,J) - OT(~,J»

N

M

..

.

.

2

dB.

(5.6)

Table 5.1 compares the PSNR values (dB) of the segmented object images in the two that the results obtained in the iterations from the "Hall Monitor" sequence. It shows , . second iteration (Fig. 5.7) are superior to those in the first one. (Fig. 5.3).
Table 5.1: PSNR values (dB) of the segmented images in "Hall Monitor" sequence.

Iteration Image (a) First 30.25 Second 36.36

Image (b) 27.43 39.84

Image (c) 26.12 41.72

Image (d) 34.71 40.30

In another simulation experiment, the "Computer Lab" video sequence with 4.35second duration is used. There are altogether 160 frames, each of which has 240x360

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

66

(e)

(d)

Figure 5.11: The output images from the first iteration of "Computer Lab" sequcuce.

pixels and 256 grayscale levels. \Ve suppose that every video frame contains at least one object of interest. This llleans there are no pure "background" images. A set of frames are selected from these 160 frames for further processing in the proposed system. To avoid interference between dose objects, frames are selected from the sequence at a constant interval 40. Thus there are 4 frames are selected to Le processed by the systelll each time. The approaches in first and the second iterations (Figs. 3.2 and 5.1) are applied. The output images in the t\VO iterations are shown in Figs. 5.11 and 5.12, respectively. Based on the "Computer Lab" simulation experiment, Table 5.2 gives the comparison results of the PSNR values between the first iteration and the second iteration. The oLtailled results are quite similar to the results in Table 5.1. The results after the second iteration are better than the results after the first iteration. l'vIoreover, the missing information on the object's face in Fig. 5.11(c) can be retrieved back in Fig. 5.12(c) by the proposed compensation method. This is because the compensated frames consist of

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

67

(e)

(d)

Figure 5.12: The output images from the second iteration of "Computer Lab" sequence. both background information and object information inside the edges. Thus even some information is lost in the first iteration, it can still be retrieved by the proposed methods based on the stICA model. This is an advantage of applying the stICA to video frames. Table 5.2: PSNR values (dB) of the segmented images in "Computer Lab" sequence. Iteration Image (a) Image (b) Image (c) Image (d) First 24.42 29.66 25.17 38.72 Second 26.67 31.21 31.54 40.28

5.4

Summary

To deal with the nonlinear combination problem in stICA model for video sequences, a novel compensation method is introduced. Spatial signals with clear shapes and edges are recovered from compensated video frames by the stICA. An object indexing method

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

68
is used to index the segmented objects in the original video sequence. The ROIs obtained in the first iteration are used to locate the objects in different spatial images (Fig. 5.2). Those post-processing techniques used in the first iteration, such as edge detection with region growing and multiscale image segmentation, are employed again in the second iteration to improve object segmentation accuracy. In the second iteration, the processing approaches consist of (shown in Fig. 5.1): 1. Extracting the regions of background that are blocked by the objects whose boundaries are bbtained in the first 'iteration. 2. Superimposing the regions of background that are blocked by the objects onto the original frames to obtain the compensated frames. 3. Employing the stICA to process the compensated frames to produce spatial signals with clearer edges. 4. Indexing the frame objects by the SVD and the weighting matrices. 5. Using the ROIs obtained in the first iteration to locate the objects in different spatial images. 6. Employing edge detection with region growing, multiscale image segmentation approaches to get accurate objects (shown in Fig. 5.7). Simulation results reveal that the proposed approaches along with the post-processing techniques can segment the objects of interest accurately and effectively.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 6

Conclusion

I

N this thesis, a new framework for high-level video object segmentation based on the stICA is presented. This chapter summarizes the work presented and suggest some

possible research extensions.

6.1

Contribution

The main purpose of this thesis is to verify the efficacy of the stICA model for video sequences. Based on the similarity of the independence of spatial and temporal signals in

fMRI and video sequences, an stlCA model for video sequences is formulated. When the
stICA model is applied directly to the video objects, a nonlinear combination problem will arise due to the absence of background information. To deal with the nonlinear combination problem, a novel two-iteration approach is presented.

In the first iteration, the stICA processing together with wavelet analysis, edge detection, region growing and multiscale segmentation techniques segment the objects from their backgrounds. However, some of the segmented objects cannot be extracted accurately, especially when objects have a complicated background. Thus the second iteration is necessary. The nonlinear combination problem in Eq. (3.6) is the major obstacle for the stICA application for video sequences. The problem leads to rather poor outputs from the video frames processed by the stICA. To deal with this problem, we introduce a novel

69

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

70 blocked region compensation method. The stICA model is applied to the compensated

frames and it recovers the spatial and temporal signals. Both theoretical derivation and simulation results show that this compensation technique is effective. After the second stICA processing, a frame object indexing approach is introduced to address the problem that is caused by the uncertain signal order of the ICA. This approach is based on the SVD and the weighting matrices used in the stICA algorithm. The ROls obtained in the first iteration are used to locate the objects in different recovered spatial signals. The
post-proc~ssing

techniques utiFzed in the first iteration, such as edge detection with

region growing, and multiscale segmentation are applied again. Simulation experiments show that the outputs from the second iteration are improved and the extracted objects are superior to those extracted in the first iteration. The contributions of this thesis consists of
1. A new method of analyzing video sequences by the stICA model.

2. A novel compensation method to deal with the nonlinear combination problem in

·

the stICA model for video sequences. 3. An integrated post-processing approach that consists of wavelet analysis, edge detection, region growing and multiscale segmentation techniques.

6.2

Possible Extension

There are some possibilities that may be explored in the future in order to enhance the performance of the proposed system and to extend its applicability.
1. The stICA optimization processing has the highest computational cost in the pro-

posed system. The implementation of its algorithm can be optimized to allow faster execution of the whole system. 2. Object motion analysis. The proposed method in this thesis can effectively segment different moving objects from a video sequence. If two successive frames are

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

71 compared to obtain the change of locations of the moving objects, then the objects moving velocity and direction can be predicted. This application has a promising future in the image and video processing.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Bibliography
[1] D. Lelescu and D. Schonfeld, "Statistical Sequential Analysis for Real-time Video Scene Change Detection on Compressed Multimedia Bitstream," IEEE Transaactions on Multimedia, vol. 5, issue: 1, pages: 106-117, March 2003.

[2] U.R. Gargi and S. Antani, "Performance Characterization and Comparision of Video
Indexing Algorithms," Proceedings of SPIE Conference Storage and Retreval for Image and Video Databaseds VII, San Jose, CA, pages: 290-301, 1999.

[3] P. Campisi and A. Neri, "Synthetic Summaries of Video Sequences Using a Multiresolution Based Key Frame Selection Technique in a Perceptually Uniform Color Space," Proceedings of 2000 International Conference on Image Processing, vol. 2, pages: 10-13, Septmeber 1997.

[4] H. J. Zhang, J.Y.A. Wang, and Y. Altunbasak, "Content-Based Video Retrieval and
Compression: A Unined Solution," Proc. Int. Conf. Image Processing. vol. 1, pages: 13-16, Oct. 1997. [5] M. Mckeown and M. Makeig, "Spatially Independent Activity Patterns in Functional Magnetic Resonance Imaging Data During the Stroop Color-Naming Task," Proc. Natl. Acad. Sci. USA, vol. 95, pages: 803-810, 1998. [6] A.J. Bell and T.J. Sejnowski, "An Information-Maximization Approach to Blind Separation and Blind Deconvolution," Neural Computation. Vol. 7, pages: 1129-1159, 1995.

72

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[7] Hyper

Dictionary.

functional

Magnetic

Resonance

73 Imaging,

http://http://www.hyperdictionary.com/dictionary/fMRI.
[8] J. Stone, "Spatial, Temporal, and Spatiotemporal Independent Component Analysis of fMRI Data," Proceedings of the 18th Leeds Statistical Research Workshop on

Spatial-Temporal Modeling and Its Applications, pages: 23-28, 1999.
[9] Z. Chen, X.-P. Zhang, "Video Sequences Processing Based on Spatiotemporal Independent Component Analysis," Proceedings of 2003 Canadian Conference on Elec-

trical and Computer Engineering, Montreal, Canada, 2003.
[10] Z. Chen, X.-P. Zhang, "Object Extraction in Video Sequences Based on Spatiotemporal Independent Component Analysis," Visual Communications and Image Pro-

cessing 2003. Lugano, Switzerland, 2003.
[11] A. Hyvarinen, J. Karhunen,.and E. Oja, Independent Component Analysis. John Wiley & Sons Inc., 2001. [12] D.C. Lay, Linear Algebra and Its Applications. Addison-Welsely Publishing Company, Boston, 1993. [13] K.1. Diamantaras and A.P. Kung. Principal Component Neural Networks: Theory

and Applications. Welsely Publishing Company, 1996.
[14] Eric Weisstein's World of Mathematics (MathWorld). Eigen Decomposition Theorem,

http://mathworld. wolfram. com/EigenDecomposition Theorem. html.
[15] T.M. Cover and J. A. Thomas, Elements of Information Theory. Wiley, 1991. [16] A. Papoulis, Probability, Random Variables, and Stochastic Process. McGraw-Hill, 3rd edition, 1991. [17] A. Hyvarinen and E. OJ a, "Independent Component Analysis: Algorithms and Applications," Neural Networks, vol. 13, pages: 411-430, 2000.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

74 [18] D.T. Pham, P. Garrat, and C. Jutten, "Separation of a Mixture of Independent Sources Through a Maximum Likelihood Approach," Proceedings of European Signal

Processing Conference (EUSIPCO), pages: 771-774, 1992.
[19] T.-C. Hsung, D.P.-K. Lun, and W.-C. Siu, "Denoising by Singularity Detection,"

IEEE Transactions on Signal Processing, vol. 47, No. 11, November 1999.
[20] S. Mallat and W.L. Hwang, "Singularity Detection and Processing with Wavelets,"

IEEE Transactions on Information Theory, vol. 38, No.2, March 1992.

.

'

[21] A.D. Marshall and R.R. Martin, Computer Vision, Models and Inspection. World Scientific Publishing Company, River Edge, NJ, USA, 1993. [22] M. Tabb and N. Ahuja, "Multiscale Image Segmentation by Integrated Edge and Region Detection," IEEE Transaction on Image Processing, vol. 6(5), pages: 642655, 1997. [23] X.-P. Zhang, "Target Segmentation and Extraction from Geographic Images Based on Multiscale Analysis," Proc. of 5th WSESjIEEE World Multiconference on Cir-

cuits, Systems, Communications

fj

Computers, Ret hymnon , Crete, July 8-15, 2001.

[24] X.-P. Zhang, "Multiscale Thmor Detection and Segmentation in Mammograms,"

Proc. of 2002 IEEE 'International Symposium on Biomedical Imaging, Washington
D.C., USA, July 2002. [25] I.-M. Kim and H.-M. Kim, "A New Resource Allocation Scheme Based on a PSNR Criterion for Wireless Video Transmission to Stationary Receivers over Gaussian Channels," IEEE Transactions on Wireless Communications, vol. 1, issue. 3, pages: 393-401, July 2002. [26] S. Saha and R. Vemuri, "An Analysis on the Effect of Image Features on Lossy Coding Performance," IEEE Signal Processing Letters, vol. 7, No.5, May 2000.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

