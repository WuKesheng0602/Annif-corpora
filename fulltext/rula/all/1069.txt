REINFORCEMENT LEARNING USING ASSOCIATIVE MEMORY NETWORKS
by

Ricardo Salmon Bachelor of Science, Computer Science, Ryerson University, 2007

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of l\!Iaster of Science in the Program of Computer Science

Toronto, Ontario, Canada, 2009

© Ricardo Salmon 2009

I hereby declare that I an1 the sole author of this thesis or dissertation.

I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

Date:

--'

_ __

I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other rneans, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research .

Signature: _

__

REINFORCEMENT LEARNING USING ASSOCIATIVE MEMORY NETWORKS
Ricardo Salrnon l\llaster of Science, Computer Science, Ryerson University, 2009

It is shown that associative 1nemory networks are capable of solving immediate and

reinforcement learning (RL) proble1ns by con1bining techniques from associative networks and reinforcernent learning and in particular Q-learning. The 1nodified
··.··..····.u.~.·v-u.·~.._

is shown to outperform native RL techniques on a stochastic grid world task by correct policies. In addition, we formulated a analogous 1nethod to add feature

············ ""'"+··,. . n+,rn~

as di1nensional reduction and eligibility traces as another rnechanism to help

the credit assignn1ent problen1. The network contrary to pure RL 1nethods is based on associative n1emory principles such as distribution of information, pattern cmnpletion, Hebbian learning, and noise tolerance (limit cycles, one to 1nany associations, chaos, etc).
'·"'''··"''h'''

of this, it can be argued that the model possesses more cognitive explanative

than other RL or hybrid models. It may be an effective tool for bridging the gap between biological mernory rrwdels and computational memory models.

Acknow-led en1ents

I would like to thank my supervisor Alireza Sadeghian for his supervision and allowing me the independence to research a broad range of ideas. Furthermore, without hin1 this thesis would not be possible.

In addition, I would like to thank n1y other advisor Sylvain Chartier from the University of Ottawa for his help frorn his knowledge, background and suggestions.

Also, I would like to thank Eric Harley for his help in getting rne involved in my research area frorn his course and recommendations.

Finally I would like to thank the department and my supervisor for all financial support provided.

IV

Table of Contents
Acknowledge1nents List of Abbreviations List of Figures List of Tables 1 Introduction 1.1 Proble1n . 1.2 1\!Iotivation . . 1.3 Related Work 1.4 Limitations and Key Results . 1.5 Outline . . . . . . . . . . . . .
2

IV VII

X
X
1 2

2 3 4 5
6 6 10 14 18 21 22 25 25

Literature Review 2.1 Autoa.ssociative J\:Iemory 2.2 Heteroassociative Memory 2.3 Chaos and Neuroscience 2.4 Reinforcernent Learning 2.4.1 Q-Learning . . . 2.4.2 Eligibility Trace . 2.5 Exploration and Exploitation 2.6 Associative Reinforce1nent

3 Proposed Model 3.1 Theory . . . . . . . . . 3.2 Design . . . . . . . . . 3.2.1 Value Function 3.2.2 Policy . . . . . 3.2.3 Eligibility Trace . 3.2.4 Feature Extraction 3.3 lin pleruentation

28

28
29

30
31

35 36
37
40

4 Experiments 4.1 Simulations . . . . . . . . . . . 4.1.1 Value Function . . . . . 4.1.2 Action Values Capacity .

40 40
42

v

4.2

4.3
5

Test Problems . . 4.2.1 Gridworld 4.2.2 Tetris Conclusion .

43 43

47
53
55 56 58

Conclusion 5.1 Future Work .

Bibliography A Source Code

62

VI

ist of Abbreviations

AM
BAM
RL

Associative rviernory Bidirectional Associative l\!Iemory Reinforcernent Learning Feature Extracting Bidirectional Associative l\!lemory k \Vinners Take All J\!Iarkov Decision Process Dynamic Programrning ]\;fonte Carlo Tern poral Difference Q-learning with Eligibility Trace Value function, Policy, eligibility Trace, Feature extraction

FEBAM
kWTA

lVIDP
DP

MC
TD

Q().)
VPTF

VII

List of Tables
2.1

Q-learning algorithm. . . . . . . . . . .

22

2.2
3.1

Q(..\) Algorithrn with replacing traces. .
Dynamic policy. Greedy policy. . Pseudocode of Associative 1\;Iernory reinforcement learning algorithm. Pa.rarneter settings for the Gridworld learning problen1. Paran1eter settings for the Grid \Vorld learning task. Performance on Tetris for various values of a using only the network V. The data was collected by noting the number of episodes required to reach an accurnulated total removed row of 500 averaged over 20 independent trials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24
37

3.2 3.3 4.1 4.2 4.3

38 39 41 49

51

4.4

Perforn1ance on Tetris for various values of ,\ using the Eligibility trace unit. The data was collected by noting the number of episodes required to reach an accumulated total removed row of 500 average over 20 independent trials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

4.5

Perfonnance on Tetris problem by varying the size of the projected dimension using the Feature Extraction unit. The data was collected by noting the nurnber of episodes required to reach an accun1ulated total removed row of 500 average over 20 independent trials. . . . . . . . . . . . . . . . 52

VIII

List of Figures
2.1 2.2 2.:3 2.4 2.5 Four state Hopfield network. . . . . . . . . . . . . . . . . . . Energy function of a two state flip-flop with weight set to -1. Update process without initial connections from y0 . Diagran1 of itinerancy in high dimension space [1]. . Lyapunov exponent for a 1D network with weight set at 1 and initial system state at 0.9 for 5, 000 iterations. . . . . . . . . . . . . . . . . . . . . . . .
2.6 16

7 8 13 15

Bifurcation diagram for new output function with transmission pararrwter

6 [2] .. . . . . .
2. 7

0

0

0

0

0

0

· · ·

0

·

·

·

·

·

·

·

·

·

17
18 19

Interaction between an agent and its environn1ent. Delayed reward for in a six state environment. . . Energy function of 1 dimensional state systen1 with -1 stored as return for the states

2.8
3.1

+ 1/-1.

. . . . . . . . . . . . . . . . . . . . . . . . .

29 30

3.2 3.3

Overview of the interaction between the different components. Original associative 1nemory energy function of 2 dimension network with fixed-points at the corners of the cube. Again, a network with (1, 1) and ( -1, -1) stored at - 1.9 energy level and (1, -1) and (-1, 1) at - 1.5 energy level. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32 33

3.4 3.5

States represented in high dimension and actions as orbit between states. Two diinensional network with transmission parameter (5) set to 2.0 and weights set to the 2-by-2 identity matrix with starting states of (0.4, 0.5). In contrast with rand01nness on the interval [-1.5, 1.5].

34

3.6

BAM network storing successive states iteratively. . . .

35

IX

4.1

Mean squared error of setting the energy function of arbitrary bipolar states in the network to various values in the interval [-1,1]. . . . . . . . .
41

4.2

Capacity of network as a function of different state values being stored for a fixed network size in the interval [-1,1]. . . . . . . . . . . . . . . . . 42 43

4.3 4.4

Stochastic gird world environment with an action failure rate of 20%. Performance from left to right of SARSA, Q-learning and the Associative 1\!Iemory (c:-greedy) n1odel on the stochastic grid world task n1easure by reward per episode over 200 trials and averaged over 200 episodes. . . . .

44

4.5

Gridworld policy when using dynamic policy with chaos averaged over 200 independent trials up to 200 episodes ..

46
48

4.6 4. 7

State of a game of tetris in progress. The average accumulated rows rernoved by training the network for 150 1 episodes with c: = - over time. . . . . . . . . . . . . . . . . . . . . . . . .
t

49

4.8

Tetris player making a sequence of bad moves that guarantee the bottom three rows can never be con1pleted. . . . . . . . . . . . . . . . . . . . . . 50

4.9

Tetris agent following the greedy policy split between episodes 1 - 37 and 37-46. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

X

Introduction
Associative Inemory (ANI) can be seen as a possible computational model of the brain and hmnan control for the reason that it appears to be one of the Inost important functions in many cognitive processes. 1v1any brain structures can be rnodeled as associative memories. This is evident by our ren1arkable ability at pattern recognition and pattern completion which A1v1 networks excels at. vVe know the brain is capable of many fonns of learning and specifically supervised and unsupervised learning. If these theories are correct, then our models must also be able to perforn1 the various learning tasks a human could do including reinforcement learning. Supervised learning can be thought of as being taught the correct action from a However, for unsupervised learning, no correct action is given, instead, the agent must establish order on its own by some rneasure1nent of desirability. In contrast, reinforcement learning sits between both in that there is no teacher available with the correct action, but a response is received fron1 the environ1nent that quantify the desirof taking that action in sorne state. A case can be n1ade that associative n1en1ories are well suited to model human learning because of their dynamical properties. These include the ability to exhibit attractor behaviour such as fixed-points, lilnit cycles and strange attractors which are essential to dealing with noisy inputs and have been supported by Sharda and Freeinan [3, 4] as fundamental to the way the brain stores and recalls infonnation.

1

1.1

Problem

The problem this work atternpts to address is to allow Arvi networks to perfornr reinforcernent learning on pattern sequences with associated rewards. Pattern sequences can be seen as states changing over time in response to sonre actions taken in the environn1ent. Therefore, anticipating and rnaxirnizing the rewards over states by taking the right action are in general the reinforcement learning (RL) problem. For this to be feasible, we will build on previous work in both the associative rnenrory and reinforcernent learning problem that includes solving the temporal credit assignment problem. The temporal credit assignrnent problem is to deternrine which actions are responsible for certain behaviours fron1 a sequence of actions by using states, actions, rewards and optionally a rnodel of the environrnent. There are quite a few proposed solutions and techniques within the reinforcement literature that we will leverage to our advantage such as the Q-Learning algorithm [5] and Eligibility traces [6, 7, 8) together called Q(>.) .

1.2

Motivation

So far the AM networks has been applied to perfornring supervised and unsupervised learning task. The in1portance of this work is that it will provide a unified model of learning that is capable of three types of learning: supervised, unsupervised, and reinforcenrent learning. This is sought after because cognitive science researchers are looking for solutions to apply to rnultiple perceptual and cognitive tasks without assuming an arbitrary nun1ber of rules and architectures. The significant contribution would be a working n1odel of the associative men1ory architecture applying reinforcenrent learning solutions to solve ternporal decision problems. The reason we are tackling this problenr is because we find the biological ranrifications exciting and the dynanrical nature of the model interesting enough to warrant the effort. Furthennore, the applications in psychology and neuroscience from having a unifying nrodel of learning based on associative memory networks. The explanatory power of the model would be valuable on it's own in addition to robotics control, control problems, and planning systems. These would be ideal applications especially when the problem
2

domain is new in cases such as large scale problmns where a good enough solution is desired over a computationally optimal method that rnight not be feasible or when tabular representation would be too costly storage wise or the problem cannot be represented by a small set of key features in this case we could atten1pt to reduce its dimension. The in1portance of this work, however, is to investigate the feasibility of using Hebbian [9] inspired learning rule to solve reinforcernent problen1s. Our objective, therefore, is to derive a reinforcernent learning systern by using associative n1emory network principles while n1aintaining its advantages. By using the network state, we were able to store arbitrary values in the network as the energy of that particular state, and use these values to make further estilnates. These were used as a basis to sin1ulate Q-learning action values within the Al\1 network and successfully solved a stochastic Gridwork-! problem and a modified game of Tetris. Qlearning is a con1mon algorithm used solve reinforcement learning problems. l\1oreover, we already know that associative networks can store sequences using lixnit cycles encodings [10] as well as one to n1any associations [2]. Furthennore, recent work by [11] has shown progress in using chaos as a search process through state space. This works by adjusting a pararneter of the network which has the effect of influencing the output of the network frorn cmnpletely chaotic to chaos bounded within a region to fixed-point attractors. It is proposed that this value could be reduce as we enter more desirable regions until the network converges to a fixed-point . vVe can simulate a searching process using the dyna1nics of the network itself instead of incorporating a stochastic elexnent into the network. The purpose behind this is to sirnulate exploration in the netvvork which is required to learn optin1al policies from experiences (i.e., E-greedy polices). In addition this seems like a more biologically plausible technique than more complex searching algorithn1s frmn the perspective of associative networks.

1~3

Related Work

In the past few years researchers have been exploring the idea of inserting prior knowledge into bidirectional associative me1nory (BAM) neural networks. One method by Chartier

3

[11] involved using an asyrnn1etry parameter in the output function of the network. This
has the desired effect of biasing the state space of the network into particular region for a n1on1ent during recall. Furthermore, this asymrnetry parameter can be set and controlled by the environrnent or controlled by a second network trained by a supervised learner. For instance, the XOR problern has been solved using this nrethod by Chartier by a collection of BAMs, which was never achieved before using Hebbian learning. vVe first associate each input and output pairs as usual. Then when the model failed to associate the right pair the environment would suggest a different value for the asyrnmetry paran1eter until it has perform the correct association. Then the parmneter value would be used for future recalls. The process has many siinilarities with reinforcement learning, however, only irnmediate success or failure is possible to learn at the moment . Another project by Zhu [12] was based off Palm networks. These are a special kind of associative rnen1ory model with neuron connections and neurons output only taking binary values. Zhu used two networks, one to represent the policy, the other to represent the value function. By iteratively updating both networks as a function of the other it was able to converge to the correct policy. This system was shown to perfornr well on a deterministic gridworld task against Q-learning.

1Q4

Limitations and Key Results

Our proposed model was cmnpared with two RL rnethods on a stochastic gridworld problem where the task was to find the optimal path to a correct tern1inal state. In this task, our model outperfonned both methods by a significant nrargin and validated our objective that Al\!I can be used to solve RL problerns. A lirnitation of our method is that we were not able to successfully use the dyna1nic policy as an optinral action selection procedure for both greedy and exploratory actions. Then on a rnodified game of Tetris the n1odel's performance over various ranges of the pararneters was analyzed including a pure greedy policy where we see signifi.cant improvements within two episodes. In particular, we use the ability to extract features from the original problem space to a lower dimension and use this as a basis to nrake decisions. The purpose of doing this was

4

the reduced nun1ber of operations required and in effect trading tirne for accuracy.

1.5

Outline

In the next chapter we will discuss the necessary background knowledge for our work and any related work frmn the literature. In particular we will cover associative n1emory, chaos in associative networks with applications to neuroscience, the theory of reinforcement learning and related work using associative rnernories. Afterwards we will present our work in chapter 3 with our contributions to the field and what we have accmnplished. In chapter 4 we will explore and analyze the effectiveness of our work on two problerns from the literature. The results will be analyzed and compared against prior work in the reinforcen1ent learning literature. Finally to conclude with we will sununarize our contributions and discuss possible issues and areas of future work that were not covered in this document.

5

Chapter 2

Literature Revie-w
Associative memories are an irnportant class of neural network rnodels that have seen strong growth since its original inception in a wide scope of applications. These models are a convenient tool for modeling human thinking and intelligent systems. Associative menrories can be divided into two categories: Autoassociative and Heteroassociative. In the following sections, we will briefly go into detail regarding chaos, neuroscience and reinforcement learning.

2.1

Autoassociative Memory

Since Hopfield's original paper[13] there has been significant interest in cornputational nrodels of the brain for applications in control systems, optimizations, knowledge representation and neuroscience. The autoassociative networks are one of the first wellunderstood recurrent neural networks. These networks are constructed frmn a large numbers of identical neurons (also called units) based on the Ising n1odel from physics. In these networks, the input and output units are identical. However, collectively the n1odel is able to recover data given a partial sample of sufficient size frorn the input space. In addition ernergent properties such as ability to generalize, fan1iliarity recognition, pattern cmnpletion, categorization, noise reduction and error correction were possible. This work was later extended [14] to the continuous real valued states.

6

The autoassociative network is described by a collection of n binary or bipolar state units tha.t are connected to every other unit except itself and weights are synnnetric, i.e.,
wii

= 0 and

'l.OiJ

=

Wji·

Figure 2.1 is an illustration of a 4-state autoassociative network.

neuron 3

neuron 4

Figure 2.1 : Four state Hopfield network.

T he constraints on the structure of the network guarantee convergence to a local rnininnrnr, however, this does not rule out undesired patterns. The la.ck of self-connections in each unit is necessary to avoid t he potential periodic or chaotic behaviour in the transition between states while synunetric connections are necessary to guarantee convergence. The output of each unit is deterrninistically calculated by whether the surn of all its input exceeds a certain threshold () as
;:rt+ l

= sgn(:L·rW) for
+ 1 ifai>Bi

sgn,(a.J

=

0

(2. 1)

-1

if ai <

ei
= ;r;i W

where \V is the weight rnatrix, :r is the unit state, ai and

the activation of unit i,

ei

is the threshold of neuron i. The output units have been shown to converge in

asynchronous mode as well [1.5], i.e., when each unit's state is updated independently of each other. The ccnnputation is emnplete when the systern has settled to a. stable pattern such that
:1;

=

sg'n (:rW). The particular pattern t he netvvork settles to is referred to as

the attra.ctor.
7

Hopfield ~s description of cornputation within this nrodel was to imagine a physical dynamical systen1 and each rnernory corresponds to a stable state of the systern. Isornorphic to an Ising nrodel a scalar quantity E can be used to describe the state of the systern known as the energy. The energy is proportional to the degrees of freedorn available a.t each state:

(2.2)
It can be shown that the energy of the systenr decreases or rernains the same for each

state update and fixed points corresponds to local rninirna of the energy function [13]. Furthennore, the recalling procedure would be to present a pattern ;r to the network and a search through state space \vould coincide with following the gradient of the energy function to the lowest energy state ;r* frmn the initial pattern. Illustrated in Figure 2.2 is the energy function of a 2-dirnension hopfield network with states
{[1,1],[1~-1L[-1,1],[-

1,-1]}. If we were to start in one of the higher states of [1,-1] or [-1,1] we would decent
to a lower state of either [1,1] or [-1,-1]. In the search process, a randorn neuron can be updated at each tirne step. This stable state x* would represent the output of the systen1.

-1.0

Figure 2.2: Energy function of a two state flip-flop with weight set to -1.

Training can be achieved using any rnethod capable of rnaking arbitrary points in the systen1 stable states by lowering the energy level of those states. For exarnple, a rnenlorization of vector ( 1, - 1, - 1, 1) would have the effect of pulling on near by vectors such

8

as ( 1, -1, 1, 1) and (1, 1, -1, 1). The network can also be trained using the Perceptron learning [16] or genetic algorithn1s [17]. However, a comrnon approach is to use Hebbian learning [9] and update the weights (initially set to zero) according to the rule:

'Wij *- 'Wij

+ :rfxj, i, j

=

1, ... , n and i

=/=-

j

(2.3)

for each pattern p to be stored in the network but this can also be written in matrix form as

W

=

~r:I Xr

+ xi X2 + ... + 1:'fn Xm -

rni

(2.4)

where ·m, is the nunrber of patterns. We do a subtraction at the end to remove the sun1 in the diagonals that corresponds to self connections. However, the storage capacity of an n dimensional space is limited to n1axirnum of n memories or fix points. It is known from the n1atrix W eigenvectors that to achieve this maxirnum requires n orthogonal n-dimensional vectors. \tVithout orthogonal vectors we have undesirable attractors that are fonned from interaction with other patterns and smne quite arbitrarily. Furthermore, from the systen1 of equations we have

sgn( - xW) = - sgn(xW) = - :c

(2.5)

indicating that using Hebbian learning with a syrnmetric weight matrix will always produce the pattern and its complernent as attractors in the network in addition to linear and non-linear combinations of our initial patterns. The limitation of orthogonal vectors can be relaxed by using the pseudo-inverse algorithnr for training instead. Unfortunately the pseudo-inverse is non-iterative and non-local, that is, each unit can no longer be arbitrarily updated based only on the sum of its neighbours. In addition to the abilities mention, the autoassociator networks are able to perforn1 optirnization computations. To do so, the optirnization task can be written in a forn1 ismnorphic to the energy function. So the difficulty is in encoding the constraints of the problem in the connections of the network and traversing the state space to the least energy. Local minimum in the energy function will correspond to sub-optirnal solutions

9

and the global rninimurn to optinral solutions. Such encoding has been shown to find near optimal solutions on the traveling salesrnan problen1[18] and with better results by [15] on the multi-flop problen1, eight rooks problern, and eight queens problen1. However, this n1ethod does not provide a way of solving NP problerns in polynomial time since the number of units required to solve each task scales exponentially con1pare to the size of the problem [15]. In addition, there is no guarantee of finding the optirnal solution while using gradient decent to drive the searching process as the number of local solutions grows so large. This motivated research into other methods, in particular Boltzrnann machines. Boltzmann machines use the concept of a temperature frorn statistical mechanics to allow movement to higher energy state proportional to the temperature [19). As the temperature slowly cools over tirne the probability of jumping to higher energy states decreases.

2.2

Heteroassociative Memory

Later, Kosko [10] proposed a bidirectional associative memory (BAJVI) 1nodel that associates pair of patterns. BAI\1 is the link between unsupervised and supervised 1nodels. The network architecture can be viewed as a bipartite graph and the two groups are the pair (x, y) that feedback to the other layeL Probing the network with x will retrieve the pattern y and visa versa for y will retrieve
;r

y

=

sgn( x W) and :~r = sgn(WT y)

(2.6)

The units used are identical to the autoassocative n1odel with either binary or bipolar states and symmetric connections without any self-connection. A simple Hebbian learning rule is employed as before but with vector pairs x and y

(2.7)
The same reasoning for the autoassociator can be used to show that the energy function of the network can be describe by the function

10

E(X, Y) =

-xrwy

(2.8)

If :r = y the BAM collapses into a Hopfield network of equal dimensions and si1nilar
capabilities. The BAivi is also able to store complex spatial-temporal patterns that are equivalent to Grossberg outstar avalanche coding [20] such as li1nit cycles. The temporal coding works on a sequence of patterns

by training the pairs

on the network. The issue of binary /bipolar encoding has been dealt with in [10] where it was shown that bipolar coding is better than binary coding in tern1s of strength and sign of the correction term coefficients. Since Kosko's paper there has been many variants that tries to overcon1e its initial weaknesses in storage capacity [21, 22], expressive power, biological plausibility, sylnrnetric relations and recall perforn1ance. These approaches varied in sophistication from modifying the architecture, learning rule, and output function. Changes to the architecture include adding a second layer with its own separate weight 1natrix [2] that recurrently sends inforn1ation to each other in synchronization. This in1plies that the weight matrices need not to be transposes of each other. This n1odel tries to find solutions to the non-linear equation of the form:

X= f(VY) andY= f(WX) for an output function

(2.9)

f.

By using a separate weight rnatrix, the network is now able

to perform task such as feature extraction [23], categorization [24] and other com1non unsupervised task [25].
11

If we add a context unit to the BA.!VI for sequence recognition we will be able to perform one to many associations. Each additional context unit allows us to do more associations with longer context. vVhile learning the XOR problem, a second BA.!VI can be used to provide additional information to the prirnary BAJ\ti by associating the desired bias for the correct pattern [26]. This will be relevant later for imn1ediate reinforcement learning. An in1proved learning rule by Haykin [27], i.e., Hebbian cross correlation learning based on Hebbian (but with an anti-Hebbian correction tern1) can be expressed:

W k+l
V k+l

=

W k + TJ(Yo - Yt) (xo - ~r;t)T
~-ct) (Yo - Yt)T

(2.10) (2.11)

= Vk + TJ(~-co -

where W and V represent the weight matrix, x 0 and y0 are the original patterns to be learned,
Xt

and Yt are the patterns iterated through the network t times and TJ is

the learning parameter. An autoassociative learning rule can also be derived frmn these equation if we let xo = y0 this reduces to a much sin1pler rule:

(2.12)

In fact by re1noving the initial connections for y0 we can get a new architecture [23, 24] with feature extracting (FEBAl\II) abilities similar to principal component analysis in addition if the network output is limited to bipolar states then clustering or categorization is performed. The process to derive the vectors necessary for training are illustrated in Figure 2.3. To start training, the weights are initially set to small randmn values and the output of the initial vector x 0 is used as the initial vector y0 . In this network, the srnaller the dimension of y relative to x the greater compression is achieved for the extracted features. vVe can go even further by constraining the amount of units that are able to fire in the compressed (y) layer. This would allow the network to simulate k-w'inner-take-

all (kWTA) [28] characteristics and hence categorical behaviour and as k is increased

12

Figure 2.3: Update process without initial connections frmn UO·

towards the nurnber of units in y we get back our original network.

and Giguere

[25] were able to show the netvvork achieve n1ultiple categoriza.tions and in addition being able to store exernplars. Also by constraining the :y layer, the network was able to group sirnilar topological patterns in a. neighbourhood while also reducing the di1nensions like
seU-or:qanizing feature ·maps.

The advantage of these rnethod over other rules such as Hebbian, optirnal projection and the pseudo-inverse are online lea,rning by incorporating feedback into the weight updates: 1naintaining the sirnplicity of the Hebbian rule and preserving locality. The a.lgoritlun is very sirnple in nature reducing the error between the patterns and its

state after being iterated through the network t tirnes. A fixed-point attractor is reached

iff the two tern1s are equal, i.e., :ro
A new non-linear output function

:rt and Yo

Yt·

departing from the step function is:

if ai > +1

(b

+ 1) ai ai

- :3 oai

if

vvhere

ai

is the activation of unit i written

the transrnission pararneter. Sin1ilarly if we replace :ri+l by :Yi+l and ai by Vyi. The parameters value should be set according to:

TJ<

1 ... 2(1 - 28)rna:r(JV, 111)'

S=£~ 2
1

(2.14)

13

for guaranteed convergence where Nand AJ are the nun1ber of units in each layer. The third condition in equation (2.13) allows the network to develop non-bipolar attractors for when (6+ l)ai + R

= 5ar for some real-valued constant R in addition to bipolar attractors.

This is in contrast to previous n1odels that were only capable of forming attractors on the corner of a hypercube inn-dimensions [30] or by using special encoding schemes. The result from these modifications is the BAlV1 can now associate grey level patterns such as the various shades of an image [31]. Furthennore by changing the learning rule Xu el la [32] were able to infer asynunetric relations with a BAl\ii. In a typical autoassociative and heteroassociative systern the network is trained with the following procedure: l. weights are initially set to zero. 2. randon1ly select a pair of patterns. 3. iteration pattern through the network k times (cmnmon for k = 1) using equation (2.13). 4. update weights by using equations (2.10) and (2.11) . 5. repeat steps 2-4 until desired error or number of trials is reached.

2.3

Chaos and Neuroscience

Although previous changes to the BAl\11 were driven by perfonnance, the inclusion of chaos was partly inspired by work from neuroscience [33]. There were 1nounting evidence that the brain is a dynamical system with inherit chaos. Stored patterns may be stationary, tin1e-varying, quasi-chaotic or chaotic. Different attractors are associated with different functions such as n1emory, n1otor behaviour, and classification [34, 35]. Work by Freernan, Skarda and Kay [4, 3] uncovered that chaos is fundainental to odour perception in animals. The results fron1 these findings suggest that anin1als do not directly respond to external stimuli but internal stiinuli created from chaos in the olfactory bulb, olfactory cortex, and the hippocampus. This suggests a complex interplay

14

at both n1acroscopic and microscopic scales that is in direct contradiction with the widely held static view of memories, that is, rnemories exist rnerely as fixed-point attractors in the brain. Kaneko and Tsuda [1] later described a so-called Chaotic Itinerancy (CI) as an explanation for how the brain stores memories. CI is essentially the result of destabilizing a systen1 with a stable attractor such that the attractor is neither totally ordered or disordered. A consequence of these phenmnena in high din1ensional space is that strange unpredictable orbits forn1 that give rise to states that switch between ordered motion and total chaos. Nioreover, CI has been identified in rnany real world dynan1ical systen1s, e.g., coupled rnaps, optical turbulence, dynarnics of water rnolecules, climate changes, animal brain, biochen1ical reaction dynamics in cells are arnong son1e of the areas studied.

Figure 2.4: Diagran1 of itinerancy in high dirnension space

Situations equivalent to the ones described above could arise between attractor ruins (see Fig. 2.4). For some stable strange at tractor in high dirnensional space a small perturbation in the bifurcation paran1eter could cause instability of this attractor that causes it to fragrnent into many smaller attractors with chaotic orbits. A bifurcation diagrmn shows the fixed-points and periodic orbits of a system as a function of the bifurcation parameter in this case 6. Such groups of attractor are called attractor ruins. While in an attractor ruin the orbit is chaotic but eventually escapes to another attractor ruin. But in between, motion in high dirnensional space is in some circurnstances constraint to a low din1ensional manifold. An escape is possible if not all orbits in the trajectory of the

15

basin of the attractor are attracting. The path taken is seen as a type of dynarnic rnemory or a searching process. However, this path can be differentiated from noise since its past and future orbit is entirely deterministic. That is the distinguish chaotic behaviour frmn randomness we know a chaotic systern always evolve the san1e way starting frorn any initial state. A basic method is to pick a starting state and find a near by state in the systern and study how they evolve over tilne. In a stochastic systern the difference in the two states will be randomly distributed while in a deterministic systen1 the difference will be regular or increase exponentially for a chaotic systern. That's the idea behind Lyapunov exponents which are usually used to test for stability in a systen1 by n1easurernent of the separation between two adjacent states [36]. The Lyapunov exponent is negative for stable systerrrs and positive for chaotic systems as shown in Figure 2.5.
Lyapunov Exponent

0.5
-I

-2 -3

Figure 2.5: Lyapunov exponent for a lD network with weight set at 1 and initial system state at 0.9 for 5, 000 iterations.

Earlier effort of including chaos into the BAl\!I are dmninated by more complex neuron models. These models seek to replace the sirnple neuron model with a more complex model [37, 38] that is capable of many behaviours such as periodic response and deternlinistic chaos usually as a collection of differential equations or difference equations. Crisis behaviour is the result of adjusting son1e pararneter of the network the system swaps behaviours fron1 periodic to chaotic and vise versa. Auaujo et al [38] introduced a fanrily of chaotic bidirectional associative rnemories (C-BA:NI) represented by three equations to describe the states of each neuron. This rnodel uses four additional parameters that in
16

a certain range of values enable the network is able to show chaotic or crisis behaviour. However, when all the parmneters are set to 0 the network naturally defaults back to a standard BA:NI [10]. This fan1ily includes rnodifications to Chaotic BAJVI, Delay BAl\!I and Exponential C-BAIVL Adachi and Aihara [37] were able to show that transient state space depends on initial conditions and that such states are characteristic of a searching process. One recent method of injecting chaos into BAl\!I is through the output function on recall. This can be accomplish by increasing the transmission pararneter 5 in equation

(2.13) to the chaotic region of its bifurcation diagram. Chartier et al [2, 11] were able to
demonstrate that even with chaos the output of each unit can be bounded to a region of state space. Furthermore higher values of 5 allow unconstrained wandering of state space.

Figure 2.6: Bifurcation diagram for new output function with transmission parameter 8 [2].

These results shows, that chaos can be used as a searching progress where the transrnission pararneter is set to unconstrained chaos region (e.g., 5 = 1.9 in Fig. 2.6) and reduced as the network approaches a region of state space which contains the desired at tractor (e.g., 5 = 0.1). This idea further reinforces concepts from. psychology and neuroscience that recall is not a one shot all or nothing process. But a progressive process that also needs to be taken into account by our models. However, we should note that long transient phase would not be ideal for infonnation processing especially in biological systems since the agent needs to constantly react to its environn1ent that is continually
17

changing.

2.4

Reinforcen1ent Learning

Reinforce1nent Learning is the paradign1 of cletennining how to act in situations through trial and error. In contrast to supervised learning in which a teacher has to present n1atching input-output pairs to the agent to learn, and unsupervised learning in which the agent only receives input signals and is asked to 1nake sense of the data,. Reinforcernent Learning (RL) works by placing an agent into son1e unknown dynmnical environment, in which it can perforrn a set of actions each of which will return a certain reward in the next state. This is sho\vn in Figure 2.7. Here reward is used for both positive Etnd negative reinforce1nents.
Reward rt+ 1

Action at Environment

State st+ 1

Figure 2.7: Interaction behveen an agent and its environrnent .

The goal of RL is to find a policy that 1naxilnizes the re-ward in the long tenn. A policy can be thought of as a n1apping of actions to take in any particular state. Such as pressing the elevator button corresponding to the upper floor while trying to get home. In addition, a policy is optilnal, if frorn all starting states in the envircmrnent the reward is rnaxiinized. A policy that does not change over tinw is called a stationary and inversely a policy that does change over tirne is non-stationary. However, for our 1nain focus we will only consider stationary policies. Going back to the recent exa.rnple, the optilnal action to take in the elevator would be to press the button corresponding to the flo or you live on to rninirnize the an1ount of tirne it takes to get hmne. It is noted that rnaxin1izing the reward can be perceived as n1inilnizing sonw punislunent. It is often the case that RL proble1ns are frarned as a rv1arkov Decision Process (lVIDP).
18

By using IVIDP, each problmn can techniques can be develop.
~1DP

expressed in a unifonn nw.nner and 1nore generalize is a well-known theory on how to rnake decisions in

cmnplex envirornnent when outcon1es are partially randmn and partially under control of the agent [39]. Any JVIDP can be represented as a tuple (8, T, .A, R) where 8 is the set of states, A is the set of actions, R : 8 x A __,. 1R is the reward function of per:fonning an action in a. certain state and T : 5' x A x 8 __,. [0, 1] is the state transition function that is probability of being in the successor state after perforrning an action in the current state. A
~1DP

assurnes the rna.rkov property holds in the envirorn11ent, that is the successor

state only depends on the current state and action takerL In addition there always exists an optilnal policy for lVIDPs. There are two rnain ways to find policies, direct or indirect, however \Ve will only consider indirect rnethods. Direct rnethods entails representing the policy by a set of adjustable pararneters which are changed to get a. better policy. But since no gradient infonnation is available for discrete problerns smnples of return are

used, e.g., variations of gradient rnethods, silnulated annealing, evolutionary algoritluns, etc. Indirect rnethods are lesser cmnputationally in the sense that for each sarnple the estirnates are shared by rnany policies and adjusted accordingly. These rnethods work by using states values or action values. For the state values we seek to place a muneric value on the desirability of a state when following the current policy and action values is the desirability of choosing an action in the current state and follow the current policy afterwards.

Figure 2.8: Delayed reward for in a six state environrnent.

Reinforcenwnt learning problerns can be categorized into two groups based on difficulty. The first of \vhich were nan1ed innnediate reinforcernent problerns. In these problerns the optirna.l action is the action with rnaxirnurn reward for each state. Hcn,vever, the

19

rnore general reinforcement problern considers delayed rewards as shown in Figure 2.8. In other words, a significant reward could be only presented after a long sequence of states. Where the task is to give credit to the actions responsible for the reward. This is usually term the credit assignn1ent problem. In general reinforcernent problerns the transition function, sometimes called the model, and the reward function are usually unknown. The agent must decide what infonnation it needs to store. Agents that solve the RL problems without building a rnodel are

called modelfree methods in contrast to agents using experience to build a rnodel. Three common approaches to solve the RL problem are by using Dynamic Progran1n1ing, J'\1onte Carlo simulations or Temporal Difference methods (40]. All these methods are derived frmn the Belln1an equations:

(2.15)
s'

\i\Thich indicates that the value of state s while following policy 1r is the reward R(s) plus the value of all successor states condition on the probability of reaching the said state by taking action 1r(s ). 1r( s) the action taken in state s by policy 1r. Action selections are usually sirnple con1putations based on the value functions. A common deterministic approach is to always choose the action with the greatest action value. But most often, it is suboptimal to always choose the greedy action. Instead a better approach called E-greedy is to choose the greedy action with probability 1 the time. [40] Dynamic Program1ning (DP) nwthods are derived frmn control systems and assume a n1odel of the environment. Two connnon algorithms to find solutions are Value Iteration
E

and all other actions

E

of

[41] and Policy Iteration [42]. DP methods are optimal in the sense that it does as
best as possible. However, they are not feasible for larger state space but are ideal

to benchmark other algorithms against such as Nionte Carlo and Ternporal Difference methods. The reason DP is not feasible for large state space is that it updates all state values simultaneously regardless of the current state and the action taken. In addition the transition probabilities have to be cornpletely specified that requires order

lSI

x

IAI

20

entries. For a state space with n

=

10 120 states, e.g., chess, that entails trying to solve

n equations with n unknowns. For any reasonable problern, it becomes in1practical since
it cannot be computed before hand or too large to store and maintain. Instead, it would be extrenrely efficient to only update the states that were visited after each transition. That's the idea behind Ivionte Carlo and Ten1poral Difference sampling rnethods. The Monte Carlo (MC) rnethod does not assun1e cmnplete knowledge of the environment. Instead it gains knowledge from sample experience or sequence of states, actions, and rewards. Because of this IviC methods only update their states at the end of an episode where an episode is defined as a sequence of experience that ends in a terminal state. Thus, to use MC nrethods we nrust guarantee that all experiences eventually tern1inate. However, the advantage of this approach is that it does not rely on previous state estimates for updating, also known as boosting, it is able to perform much better than other methods in non-rnarkovian environn1ents [40] . These n1ethods are ideal for situations where real work experience is costly but simulations are cheap [43].

2.4.1

Q-Learning

In contrast to l\t1C methods, Temporal Difference (TD) methods update the state or action values after each step instead of after an entire episode. These methods are quick to adapt to changes in the environment and also do not rely on a nrodel. Instead, they take advantage of boosting by using previous estimates as a basis for making new estimates. One such off-policy method is Q-learning developed by Watkins [5]. The convergent proof was later provided by [44]. The general idea behind Q-learning is to store action values instead of states. This elirninate, the requirement to construct a rnodel of the environment when selecting actions. A greedy policy can be used to select an action a that maximizes rnaxQ(s, a). So we can update action values by:
a

Qt+r(s, a)

f-

Qt(s, a)+ a[r + 1maxQt(s', a')- Qt(s, a)]
a'

(2.16)

Action values are updated by the TD rule that n1oves our estirnate of Q(s, a) closer to
r

+ 1Q( s', a') which is assumed to be a better estirnate by the fact that it includes reward
21

frorn an experience. Furthermore, s' and a' are the successor state and next action. With an appropriately sn1all value for a Q-learning has been show to converge to the optimal value function Q* with probability 1. A full listing of the Q-learning algoritlun can be found in Table 2.1.
Table 2.1: Q-learning algorithn1.

Initialize Q(s, a) arbitrarily Repeat (for each episode) : Initialize s Repeat (for each step of episode): Choose a from s using policy derived from Q(e.g., E-greedy) Take action a, observer, s' Q(s, a) f-- Q(s, a)+ o:[r + 1rnc;xQ(s', a')- Q(s, a)]
a

s

f--

s'

until s is terminal

2.4.2

Eligibility Trace

TD n1ethods, (referred to as TD(O)), are quite an iinprovement over DP 1nethods in terms of con1putational tin1e. But only one state is updated at a time and the propagation of state values when a significant reward encountered can be quite slow [40]. On average it takes n episodes to propagate a tern1inal state value back n states if the same path is taken. In contrast a MC method would update all states visited after each episode. Eligibility traces, pioneered by Sutton [6, 7, 8] are a comprise between these two extren1es. The basic idea is to assign a nurnber (trace) to each state when visited which detennines its eligibility to be updated when the agent receives a reward in the future. The 1nost recent states will have higher traces and therefore share more responsibility for any imn1ediate reward. This can be seen as a rnechanism to help solve the credit assignment problern.

if s = St and a = at

(2.17)
otherwise In (2.17) the parameter A controls how fast previous traces falls off from the current
22

state. By adjusting the paran1eter /\ towards 1 we move closer to a pure 1viC method TD ( 1) and inversely towards 0 we rnove back to the original TD (0) learning. In fact
it was shown by [40] that better perforn1ance are attained with intern1ediate values for

0 :::; ). :::; 1. To distinguish tracing rnethods fron1 non-tracing rnethods we will refer to them as TD(.A) methods and in particular Q(.A). Reme1nber Q-learning 1naintains two policies, the one it follows (behavioural) and the one which is being learned (estin1ated). Since the behavioural policy will sometimes take actions that are sub-opti1nal with respect to the esti1nated policy it may not be best to indiscrim.inately apply eligibility traces. A non greedy action might take you to a completely different set of states. In other words, we are using exploratory actions in estiinating the value of following the greedy policy! As a result if you mark every action value as eligible, you backup the effect of non-greedy actions. Watkin's [5] proposed instead to look ahead only one step pass the next exploratory action. However because of the frequency of non greedy actions at the start learning will be slow from terminating traces. Peng's Q(,.\) [45] addresses this by having no distinction between exploratory and greedy actions. It tries to balance the two by updating the policy early on using exploratory actions and later to greedy actions. The only disadvantage of this method is its complexity to implement. There is a third approach which we will be using that just applies the regular trace to TD learning called na'ive Q(,.\). Although backing up exploratory actions seerns bad, it has shown good perfonnance [46] in comparison to Watkin's Q(/\) and Peng's Q(,.\). In the previous section the traces defined, now call accumulating traces can be inlprove if you consider what happens when a state is repeatedly visited by an agent. The eligibility for that trace becon1es greater than 1 and this would cause the agent to take n1ore responsibility than required for the reward and becon1e a proble1n for convergence. To overcome this we can consider replacing the trace (replacing traces [8]) instead of incrementing by one. Hence we do:

23

1

if s = St and a = at if s
=

et(s, a)

=

0
'f Aet-1 ( s, a)

St and a

#- at

(2.18)

otherwise

Thus the update rule for Q(,\) for all states turns out to be:

(2.19)

and the error is calculated only for the current state:

(2.20)

These eligibility values are then used in the full update rule to get Q(,\) algorithm which is listed in Table 2.2.
Table 2.2: Q(-\) Algorithrn with replacing traces.

Initialize Q(s, a) arbitrarily Repeat (for each episode): Initialize s Repeat (for each step of episode): Choose a frmn s using policy derived from Q(e.g., E-greedy) Take action a, observer, s' 8 +--- r + rymaxQ(s', a') - Q(s, a)
a'

e(s, a)+--- 1 For all s, a:

Q(s, a)
e(s, a)

+--+---

{O

Q(s , a)+ are(s, a) if s = St and a ryAet-l(s, a) otherwise

=/=-

at

s

+---

s'

until s is terminal

24

2.5

Exploration and Exploitation

In RL problems we are not given the n1odel nor the reward function we must explore the environment to learn its structure. However this introduces a problern since R.L task are usually online we are faced with a dilemn1a. Whether to exploit knowledge we already have or be optimistic and keep exploring for better potential rewards. This is a problem face not just by action value agents but in addition those that build a 1nodel as well. For taking the optin1al action with respect to the agent's model of the environment may not be optimal at all because the agent's 1nodel could be incon1plete or outdated. A reasonable approach is to have an overly optin1istic start by over estimating the true state values. Using standard methods the agent will explore optimistically states only to learn its true value and possibility find the optin1al policy. This heuristic led to the development of a n1odel base learning algorithm call R-NIAX [47]. Additionally, we could use an exploratory policy such as an E-greedy for the behaviour policy while learning the optimal policy independent of the policy being followed. A provable optimal method of making the trade off between exploration and exploitation is based on the idea of computing Gittins indices [48] . Unfortunately, so far the method has not been extendable to the general reinforcement problem.

2.6

Associative Reinforcement

A sensible way to create an associative systen1 is to incorporate Reinforce1nent Learning on top of an associative 1nemory n1odel. A n1ajor system was developed by Zhu [12] using the PALlVI network. A plan1 network [49) functions like a BAl\1 with weight update rule:
N

W= V[YnX~]
n=l

(2.21)

where the operator V is boolean OR. In other words, each weight connection is limited to either 0 or 1. A single layer feed-forward PALivi network the critic (state-to-value) and learning rule:
25
Wact Wcritic

was used to store

the policy (state-to-action) with a modified online

(2.22)
Wcritic ~ Wcritic

+ ,,
L
i,j

a~ W act W critic

Wcr'itic[z,J]

..

(2 .23)

given that

Qt

= Wcrit:ic'St and Qt+l = Wcritic'St+l· Initially Wact is set to randmn values

and W critic is given s1nall positive values, e.g., 0.1. The policy affects the state values by which actions are selected so the state could be updated and the state value affects the policy by influencing which action is chosen. Furthermore, on recall the system uses the rule below to extract an appropriate action while in some state
St

at time t:

at= f(Wact · St-

8)

(2.24)

The action component uses a 'k winners take all rule' where only k bits in the output are activated
(at =

1) as the action vector. If more than k units are active a randmn

subset of k are selected. This is believed to be the source of exploration needed for optimal learning. This encoding mechanisrn forces the actions to self organize. The threshold parameter () is adaptively set to Inake sure only the top k bits are activated. Their system was reported to have good convergence and generalization with k
=

1

winner take all in cmnparison to Q-learning on a 15-by-15, detenninistic and stationary grid world task. The downside of this n1odel is that it is not based on dynamical system principles and hence cannot be used as a model of learning in the brain. However a si1nilar idea has been atten1pted in the literature recently by Chartier [26]. This approach requires adding an asymmetric parameter h to the output function from equation (2.13):
xi+ I =

h + (5 + l)ai- bar

(2.25)

The justification of this parameter is to bias the search space in the direction of the parameter h such that attractors near that region gains a larger radii during the recall process. Results show that recall performance was boosted for patterns near the region of para1neter h (i.e., patterns similar to h). A bias value was generated for each input the

26

network associated incorrectly by a secondary BAl\1 network. Chartier was able to use a collection of BAJ\!Is to learn a non-linearly separable task, the XOR problern, and solve imrnediate reinforcen1ent problems. This was thought to be irnpossible using Hebbian derived learning rules. However this approach required a secondary BAl\!I that is told the correct actions which in general is a form of supervised learning. The goal of our system is to extend the abilities of associative n1emories model to n1ore general reinforcement problems by considering ternporal sequences with delayed rewards.

27

Chapter 3

Proposed Model
The previous Inethods discussed in section 2.6 either used gradient based function approxhnators to store the value function or supervised learning by giving the correct actions to the network. \Ve propose a method based on Hebbian learning that takes advantage of AlVf networks unique ability of pattern completion. In this section, we will outline the theory behind our approach, the network architecture and ilnpleinented details. Two sainple proble1ns from the literature will also be introduced and used as a benchmark for our system in the following chapter.

3.1

Theory

The key insight was to represent the value function by the energy function of an autoassociative n1en1ory network. By using context units, we were able to store state-action pairs in the network and the energy of these states corresponds to the negation of action values. From here on, we will adapt the tern1inology of using x for the state the network is in and reserve s for the state received frmn the environment which the network associates with a reward. For now, without mentioning the detail of how the network would be trained to reach these very specific energy levels, we will briefly explain why this concept is sound. First, consider a fully trained network with the correct state-action pairs and action values (energy levels). Such as the 1-dimension network shown in Figure 3.1 that has a return

28

Energy 80

60

40 20
State

Figure 3.1: Energy function of 1 diinensional state systein with -1 stored as return for the states

+1/-1.
of -1 stored at states

+ 1 and

-1 . For sorne particular state x the network might be

in, a policy n1ust be present that chooses an action. This could work by atten1pting to c01nplete the pattern for the state-action pair (correct action vector is not available). But frorn this error correcting procedure the network converges to the action with the lowest energy which so happens to correspond to the action value with the highest return. It is by this reasoning the energy level is the negation of the action's return value.

3.2

Design

Our syste1n (V PT F) is com posed of four inter-connected components: value. function (V), policy (P), eligibility trace (T) and feature extraction (F). The cornponents work together by first receiving St from the envirornnent and the relevant features are extracted as shown in Figure 3.2. The current state is passed onto the policy where an action is taken that transfers the agent to the next state
St+l

which is filtered to

St+l Tt+l

and the are then

greedy action recmnmended is stored. These along with the current reward

used to update the action values of the old action and trace back in tin1e to update all previous experience. All the components will be discussed in Inore detail through the next 4 subsections.

29

Figure

~3.2:

Overview of the interaction between the different components.

3.2.1

Value Function

Network V is the core component of the syste1n and will be used for associating stateaction pairs to action values as shown in Figure 3.2. Network V will be represented by a single autoassociative network. Here, the environrnent state units are used as context units for the current action to disan1biguate betvv-een correct actions in different states. On each iteration the network update its estirnate of the optirnal value function by changing the weights according to the Q-learning update rule. But first the old action values are retrieved as the negation of the energy frorn the current network state ;:r0 = sa. (J(s, a) = -B(V, sa)
Q(s', a') = rnax- E(V, .s' a')
a.'

(3.1) (3.2)
(3.3)

Q(s, a)

= Q(s, a) + o:[r + [Q(s' , a') - Q(s, a)J

where it is understood that we are in state s and took action a then transitioned to state s' and the greedy action is a'. After the new estinwte of Q(s , a) is used to update the \veights as:

V

=

V

+ 17[-Q(s, a)- B(V, :ro)] · [:rox6- ~Tt :rf]
30

(3.4)

It should be noted this learning rule is a generalization of the previous learning rule
and collapses back to equation (2.12) when E(V, x 0 )
=

-Q(s, a)- 1. However that will

not normally be the case and more generally there are two fixed- points. The original at

xox'[

= Xtl:f

and the other when E(V, x 0 )

=

-Q(s, a).

Since roughly speaking the energy level of a bipolar pattern that is a fixed-point can be approxin1ated by:

E(x)

=

~[xT y- (c5 +
2
1

1)xTl¥y +
1
T
3

~c5xT(VVy) 3 + C]
2

(3.5) (3.6)
(3.7) (3.8)

=2[n-(c5+1)n+ c5x x] 2

= ~ [n - n - c5n + ~c5n]
2 2

c5n
4

which is typically around -~n for a n-dimensional network. So we should rnake sure our return values are mapped to a higher interval to avoid interference with the first fixedpoint in equation (3.4). vVe also assume for the energy function that the constant C
=

0.

For consistency with the theory of Reinforce1nent Learning theory we will consider the negation of the energy function instead:

e(x)

=

-E(:r)

(3.9)

The effect this has on the shape of the energy function can be readily seen in Figure 3.3 in comparison with a conventional trained network for the patterns (1 , 1) and (1 , -1) at energy levels -1.9 and -1.5. Note its structure compare to a conventional network trained with the san1e fixed-points. The original E(x) is scaled on the
.1: 1

and x 2 axis

until the desired energy level is reached at the bipolar points. This causes the original fixed-points to be shifted outwards to new coordinates. 3.2.2 Policy

The second cmnponent P implen1ents the policy the network follows by using the structure of the value function in the V network. By iterating the network V frmn a known initial
31

Figure 3.3: Original associative memory energy function of 2 din1ension network with fixedpoints at the corners of the cube. Again, a network with (1, 1) and ( -1, -1) stored at -1.9 energy level and (1, -1) and ( -1, 1) at -1.5 energy level. network state
Xt

=

sa0 where a0 can be some arbitrarily chosen action or
St,

0 and

s is the

current state from the environment. In addition having s clamped to

the network will

traverse the landscape of the energy function by following the path that leads to a local rninirnum but being restricted to the action subspace. An incren1ental approxi1nation to the next nearest action ak+ 1 with lesser energy is:

(3.10)

In that region, the action that minimizes the energy function will corresponds to the action with rnaxin1um return. This is analogous to having an attractor ruin in high dimensional space but orbits being limited to a srnall n1anifold. However outside changes (new state from the environment as a consequence of choosing an action) will cause the network to suddenty transition to a new stable rnanifold. We can irnagine these rnanifolds are represent by clouds in Figure 3.4 where different actions taken in a state corresponds to following an orbit to another state. It is in fact the agent's organization of the environment. 32

I

I
\

--------- ---w
S(2)

t

'·,'.,~-----·· ·· ·······

Figure

~~.4:

States represented in high dimension and a.ctions as orbit between states.

Fortunately the exact state values are not required to extract the opti1nal policy. Usually the optinu: d policy is available before the action values fully converge to the optirna.l value function so we will rnake use of this inside our policy. A second step is to take advantage of chaos as a searching process. In an e-greedy policy we usually select a random action with probability f: by using a. stochastic process to smnple fron1 this distribution. vVe propose to use an analogous rnechanisn1 with cha.os to sarnple frorn the space of actions. Consider a network Va that is trained in advance with all the valid actions the agent can perfonn. If the network is probed with an arbitrary vector it will converge to one of these actions or its cornplernent after a sequence of iterations. However if we initially set the transrnission pararneter 6 to an unconstrained chaotic region (e.g., 6
=

2) of the network's bifurcation diagra111. This will cause the

network to arbitrarily cross boundary axes and transition betvveen patterns unpredictably.

It is apparent frorn Figure 3.5 where we show a 2 din1ensional network with an initial
state of (0.4, 0.5): 6 = 2, and weights:

5000 iterations are plotted on the plane. It can be co1npared to points generated randornly in the sarne interval unifonnly where there is approxin1ately equal mnount in each quad-

rant. We will initially set the transmission parameter 6 to high values which corresponds to unconstrained chaotic region and reduce the value as the agent becomes rnore confident in its value function where the optimal action we will slowly move to a rnore stable regions. The motivation behind this is to use a naturally occurring process of the network to our advantage as it can be seen as a very plausible technique for biological models. Furthermore recalling as we have explained is not a one shot process but a slow iterative process of reconstruction. If the two ideas mention above are con1bined then the state should be stored for the chaotic orbit to be resu1ned later. A simple m.ethod to combine these actions is a linear su1n based on the paran1eter E, i.e.,
a +-- ( 1- E)· agreedy

+ E· aexplore·

A consequence of using the associative n1emory Inodel is the inverse of states and actions are also stored with the sanw energy level. That means on recall there is a possibility of retrieving the inverse action. On recall, our policy will recmnmend an action
a, to detennine the correct action we take the action ai that has the greatest absolute

correlation of all actions. i.e., nl?.X IcoTr( a, ai) I· Actions will generally not interfere with
2

each other but the correlation between actions detennine how easy it would be for the network to transition to a near by action on recall. So for neutrality each actions should be equal distance (bitwise) fron1 all other actions. One sin1ple encoding of this is the 1-of-d choice. For ad dimensional vector only one component is active while the rest is inactive, e.g., ( -1, -1, +1, -1).
Chaos Randomness

-1.0

-0.5

0.0

0.5

1.0

-1.5

-1.0

-0.5

0.0
XI

0.5

1.0

1.5

Xl

Figure 3.5: Two dimensional network with transmission parameter (8) set to 2.0 and weights set to the 2-by-2 identity matrix with starting states of (0.4, 0.5). In contrast with randomness on the interval [~ 1.5, 1.5].

34

3.2.3

Eligibility Trace

The third network T stores a trace of the nwst recent states and actions taken. Sin1ilar to an eligibility trace any re·ward receive are propagated back to previous states. The attraction of this cmnponent is that it allows the systen1 to n1ore quickly propagate changes backwards by inforn1ing earlier state-action pairs of their responsibility. In fact the mechanisrn is a technique to help solve the credit assigrunent problem discussed ea,rlier. However only a fraction of states that are recently visited will have trace of a.ny significance since the rest \Vill have near zero values. Instead, we will only keep track of states that were visited recently. The list of recently visited states can be viewed a.s a lirrlit cyele at the BAr-/I network level. So the net\vork Tis represented as a BAI\1 and for each episode the initial state is stored as a fixed-point and inductively the current state is linked to the previous which fonns a chain as shown in Figure 3.6. The initial state is n1ade a fixed-point to place a tenninal state in the chain.

Figure :3.6: BAJ\'1 network storing successive states iteratively.

The propagation of rewards back-vvards would be done after each tirne step.

(3.11)

This update is si1nilar to a regular Q-learning update of a state but here the error is stored in a tenn ll and used in all recently visited action values:

(3.12)

Also note the Qt(s, a) values are updated in network \/.

3.2.4

Feature Extraction

The last network F was inspired by the use of feature extracting abilities of the BAlVI. We want to compress the state space for large problen1s into srnaller sizes. This is a desirable property of any intelligent system since in many problems the state space is intractable to enumerate and furthern1ore only a fraction of states are necessary to visit in order to find a near optin1al policy. However it should be noted that this n1ethod builds generic states as attractors to represent con1mon states the agent encounters. In particular we would not expect rarely encountered states to be represented well. In fact , we expect our network to filter out and n1ap the most cmnmon or ilnportant states fron1 high dimensional to a low dimensional subspace. That means the previous networks n1ention (i.e., networks V, P, and T) will only see a lin1ited set of states from the original problern in a lower dimension. This brings forth another advantage of function approxirnators (FA), the ability to generalize from states, closer states will have similar state values. In fact with this extra layer our system has two levels of approximators. That is the original BAl\11 by itself can be used as a FA with fix-points defining areas of interest in state space and any near by points (basin of attractor) would naturally be attracted to the fixed-point as this is one of the fundan1ental capabilities of associative n1ernory networks. However it is known that standard RL

algorithms, in particular QL has had problems converging when using t'A compare to tabular representations and have only been proven to converge on linear approximators and usually diverge on nonlinear approximators without extensive tweaking [50, 51]. \Vithout much Inodifications as illustrated in Figure 3.2 the incon1ing state the environment is trained on the network and its extracted features
St

St

from

are passed onto

the other system as a lower din1ension state. So we will use the following equations for training weights F w and F v:

Fw(k + 1)

+---

Fw(k)

+ TJ(Yo- Yl)(.ro + x1)T + Yl)T

(3.13) (3.14)

Fv(k + 1) ~ Fv(k) + r;(xo- xl)(Yo
36

with Xo as the initial state St, :Yo= kwta(Fwxo, k), :r:r

= Fv:Yo and Yr = Fwx 1 . By deriving

y0 we used the function kwta to limit the amount of units that can fire and control the

sparseness of encoding, i.e., pick the top k units to fire. On recall, we are given a state
St

and one trial through the network we derive

St:

St

= f(W f(V f(Wst)))

(3.15)

that is later use throughout the network.

3.3

Implementation

Our algorithm is based off a connnon RL solution called Q-learning. The algorithm was in1plernented in the l\1athernatica progrmnming environment. The reason being it was very expressive and supported many required features such as rnatrix operations and graphing. The source code is also available in Appendix A and the pseudocode of our algorithn1s are in Tables 3.3, 3.1, and 3.2.
Initialize V a, aexplore in trial 0 Initialize V, s, E aexplore ,___ f(V a · aexplorei J = 2.0)
agreedy ,___ j(IO) (V a(3 ,___

·sO)

Let Ai =
J ;---

(1 -E) · agreedy + E · aexplore icov(a(3, ai) I for all i = 1, 2, ... lal
. if RandomReal(O, 1) 2
E

.

{m~xAj
J

Randomlnteger(1, Return aj

lal)

otherwise

Table 3.1: Dynamic policy. An issue encountered during implementation was that it was problen1atic finding the chaotic region for the transmission parameter since the fixed- points has been moved out of the usual interval. Instead we used a fixed network Va with the original action vectors trained and conditioned on it 's chaotic region to regenerate exploratory actions. vVe used

37

the covariance of the recalled action against all actions to infer which action is closer to the attractor during action selection for the dynarnic policy. That is the action chosen by our agent. Initialize V, s, c Let Ai = e(V, sai) for all i = 1, 2, ... [a[ m~xAj if RandomReal(O, 1) 2
~

j

Randomlnteger(l, [a[) Return aj

{

E

J

otherwise

T11ble 3.2: Greedy policy. For practical reason the trace depth was set to be
1

3

of the networks dimension. Also,

to avoid infinitely updates by progressing backward within a lirnit cycle a list of previous updates was n1aintain and only unique state-action pairs were updated.

38

Initialize V, Cw, Cv = all weights 0 Initialize F w, F v to small randmn values Set parameters a, 1, E, .A, rJ, 8 Repeat (for each episode): Initialize E = all weights 0 St <- state from environment Use St to train F w, F v St +----- recall St from Fw, Fv Repeat (for each step of episode): at +----- action recommend by policy in St (i.e., E-greedy or P-greedy) If first step: associate sa with sa using Cw, Cv Take action at, observe Tt+b St+l St+l +----- recall St+l from F w , F v at+l +----- action recommend by policy in St+l (i.e., E-greedy or P-greedy) qt +----- energy(V, Stat) qt+l +----- energy(V, St+lat+l) ~ +----- r + '"'fqt+l - qt Associate St+lat+l with Stat using Cw, Cv Let Xt =Stat E = trainAu.to(E, Xt, 1) Repeat (for each step taken): q +----- energy(V, Xt) e +----- energy(E, l~t) V = trcL'inAuto(V, Xt, q + a~e) 1 .Ae if s = St and a = at Let u = · {0 otherwise E = trainAui:o(E , Xt, ·u) Xt-1 +----- traverse cycle backwards using Xt and Cw until cycle found until s is terminal until desired number of episode

Table 3.3: Pseudocode of Associative l\!Iemory reinforcement learning algorithm.

39

Chapter 4

Experin1ents
In this section, we will support our objective with experirnental results that shows our systern performs well. To begin, we first dernonstrate our systern is capable of storing arbitrary values in the interval [-1, 1] for a set of states. This region is sufficient because any other rewards interval can be rnapped to this interval by scaling all rewards by the rnax reward appropriately. Next, we will try to experimentally detennine the capacity of action values that could potentially be stored accurately with a fixed network size. Furthermore, we will demonstrate the network on a stochastic Gridworld and rnodified Tetris problenrs. For the stochastic Gridworld task, we will do a comparison with other RL algorithms and for the Tetris game we will show the perfornrance of the system over a variety of pararneter settings.

4.1

Simulations

A number of simulations will be performed to analyze the correct behaviour of our systern and its subcomponents before it is tested on a full problern.

4.1.1

Value Function

To dernonstrate the effectiveness of the system, we will confirm our hypothesis that the energy function is a viable means of storage for action values. The purpose of this

experiment is to show the network is capable of storing the correct return values and it

40

System
Associative 1\;femory

Parameter
State dimension Action dimension
TJ

Value
40 4 10- 4 0.01 0.9 0.9
1

8
(}:

Reinforcement Learning

'Y
E

t

Table 4.1: Parameter settings for the Gridworld learning problem.

converges as a function of the nun1ber of iterations. The network size will be selected to be 44 din1ensions with 44 bipolar states. This leads to a network load of 44/44
=

100%.

The parameter values are set according to Table 4.1. The results can be seen in Figure
4.1.
Precision of Energy Function
2.0 1.5

~

1.0 0.5 0.0
0

100

200 Iterations

300

400

500

Figure 4.1: :J\;lean squared error of setting the energy function of arbitrary bipolar states in the network to various values in the interval [-1 ,1].

As we would expect the Rl\!IS (root mean squared) error is reduced to alrnost zero after 500 iterations which is roughly 12 updates per state. These results are ilnpressive and confirm that the network can store action values to arbitrary accuracy in the interval

[-1, 1]. However, as was stated earlier, ann dirnension network is limited to storing values
only in the interval [-n, n]. The sa1ne test was tried for the full range of values and we saw greater error as the network needs to be distorted at greater extren1es to reduce the error. For our purposes we will only consider the subinterval [-1, 1] since larger intervals for rewards can be easily mapped to this range and it contains less variability for the

41

values being stored.

4.1.2

Action Values Capacity

In this section we will show the capacity of the network under different load factors and how this affects the error of the stored state return values. The objective behind this sin1ulation is to justify the ratio of state values relative to the dirnension of the network. I'vioreover, we theorize that the storage capacity (error in energy function) would be more than the comrnonly used load factor of 30% used in regular associative memories since we are not fully storing the state as a fixed-point. We are storing state value which is less inforrnation. In Figure 4.2 we see an increase in error and greater variance of a fixed size network of 20 dimensions storing varying nurnber of bipolar patterns from 1 to 40. The state values were generated unifonnly in the interval [-1 , 1]. Each pattern was updated on average 50 times over .50 independent trials.

::;---- --~ ..--~-~-----r··--fH ___ _ ~lhlffiDI_ ~ 02~ WPl
0.1 ~

Capacity of Energy Function

t
f._

0.0 L.J. ..... <......
0

e-r!-m
c ......c

[!' .· ,:.· . j
40

10

20

30

. --'------'-----~---"' .... ::.1

Patterns

Figure 4.2: Capacity of network as a function of different state values being stored for a fLxed network size in the interval [-1,1].

Frmn the graph we can see that the rate of increase in error appears to be linear in the load factor up to 200% (40 patterns). These results shows our system is robust over varying load factors and it is unlikely that a particular threshold will degrade performance critically.

42

4.2

Test Problems

To show the success and generalization ability of our systern we will test it on two problems . The first is a Gridworld problern and the second is a 1noclification of the popular Nintendo gan1e Tetris. 4.2.1 Gridworld

Our first test case is a stochastic Gridworld environment from [39]. The environment will consist of 4-by-3 cells shown in Figure 4.3. The two terminal state each give a reward of

+ 1 and -1 while all other transitions are -0.02.

In this world there are 4 possible actions

{north, west, south, east} that correspondingly 1nove the agent fron1 a grid position {up, left, down, right} and remains in the smne position if rnovement is off the grid or towards cell (2, 2). However each action has a 20% chance of failure meaning the action north will rnove up with 80% success and 10% chance of n1oving left and 10% right. Taking action west would go 80% left, 10% up and 10% down and the san1e for the remaining actions. The objective for the agent is to reach the positive terminal state from the sta.rt state by moving as quickly as possible to n1inin1ize the accun1ulating negative rewards but safely and avoid falling into the negative terminal state from a faulty action.
3
+1

2

-1

3

4

Figure 4.3: Stochastic gird world environment with an action failure rate of 20%.

For this problen1 we will represent actions as 4 dimensional vectors where {north=(1, -1, -1,west=(-1, 1, -1, -1), south=( -1, -1, 1, - 1), east=(-1, -1, -1, 1)}. The 11 unique states are uniformly san1 pled fron1 {-1, 1} to produce a 40 cli1nensional vector. The remaining parameters that were used can be found in Table 4.1. Since the state vectors are not based on topographical features of the states according to its 2 dimensional representa43

tion but instead were unifonnly generated we will not apply the feature extraction on this problem. In addition, on such a small world performing trace would not help much.

Result
Vve first cmnpare our system using the c::-greedy reinforcement learning policy against two standard algorithms: SARSA and Q-lea.rning. Afterwards, we will introduce the dynamic associative policy (network P). Based on tests from 200 trials and up to 200 episodes the perfonnance of these different systems can be seen in Figure 4.4.

Figure 4.4: Performance from left to right of SARSA, Q-learning and the Associative Memory (E-greedy) model on the stochastic grid world task n1easure by reward per episode over 200 trials and averaged over 200 episodes.

The plot in Figure 4.4 shows three things for us a) how much exploration the agent
44

goes through b) the rnoment the system learns the optirnal policy, and c) the rate at which the system takes advantage of the optimal policy (exploitation). Frorn the curves we can tell that rnost of the learning in the system occurs between the first episode and before the graph of the accun1ulative rewards curve passes zero. Since on average the expected return following a purely random policy is negative returns. After which point the systenr rnainly exploit what it has learn for the ren1ainder of the trial. However the rate at which it exploit this knowledge is dependent on the
E

term that is a decreasing

function of time so in the limit the policy will become entirely greedy. For this problern we did not test the optirna.l policy, instead our unit of measurement was the average accumulative reward received. The justification for this choice was that it was not required to know the full optinral policy since sonre states has such poor expected return our agent should never go there often enough to deternrine the optin1al action. Instead the shortest path that maxirnize our return would be sufficient. The rniddle solid line shows the mean over 200 trials and each bar is the standard deviation. \Ve decided to show the accumulated rewards instead of rewards per episode because the episodes are not independently sarnpled, but are influence by pass episodes. The poor results by SARSA can be explained by trials where convergence within 200 episode has failed and the systen1 accurnulates larger negative rewards. Huge negative rewards would easily overcome any positive gains by the systern. This would explain the increased rewards midway through and then a decrease below zero which is supported by a change in variance. From inspection we know QL and Al\11 Learning has done extremely well and most likely had to have found the optimal policy consistently. This can be inferred from the plots of the snrall bounds on their standard deviations which happen to get smaller centered around the mean as all trials eventually converge to the optimal policy. Furthern1ore, the Al\11 learning algorithm achieved accurnulative reward of 0.58 compare to Q-learning at 0.47. This rnarks a difference of over 0.1 with a p-value of 4.5·10- 24 which is statistically significant for its scale and justifies our reasoning that Al\11 learning was able to learn the optimal policy faster. Our explanation for the increase in perforn1ance of Al\1 learning is the shared knowledge of state returns by distributed representation. That is, each states updates affect the estimates of other states. From this, we know the
45

A1'1 syste1n has done extren1ely well so for using the value function component (network

V).
Now we will atte1npt to use the dynaxnic associative policy outlined earlier. In QL there are two policies, the behaviour policy and the estirnated policy. For the behaviour policy, we will use a chaotic net\:vTork to select an action in each state the agent Inust follow while the optilnal action recmnmended by our estin1ated policy is found through the searching process policy described in section 3.2.2. In other words, our exploratory policy is to follow a chaotic action and the exploitive policy is to follow the reconnnended action at a local mininnun.
3

0.95

~
fo.76

2

fo.69

.. - -0.995
1.0

~

fo.995

0.60

0.125

0.0

2

3

4

Figure 4.5: Gridworld policy when using dynamic policy with chaos averaged over 200 independent trials up to 200 episodes.

Results fro1n the experim.ent can

seen In Figure 4.5. \Ve see the proportion of

tirnes the net\vork learned the correct action for each state over 200 trials. Returns close to the positive tenninal state -vvere learned more successfully than others, in fact the correct action for state (4) 1) was never achieved in any of the trials. Even by chance this should be around since there is only 4 actions. Unfortunately, as the results

show the exploratory chaotic policy could not be interleaved with the greedy associative rnernory policy successfully. \Vhat's rnissing is a function analogous to c-greedy that could over tin1e transition frorn a pure exploratory to an exploitive policy. Therefore, it was unable to exploit kno-vvledge gained frorn early exploration to its advantage and aftenvards continually explores. But, although it has not rnaxhnize its reward the correct returns are being learned nevertheless a. rnajority of the time. Additionally, it seerns the network smnetirnes forgets what it has learned previously. It has occurred before in the
46

literature and is comn1on in solutions employing function approxiination. In particular by [52] coincidentally on the Tetris problem which we willrnention shortly. Although the current network is not using feature extract it does indeed apply function approximation to fixed-points. In the next problen1, we will continue using theE-greedy policy based on the network V .

4.2.2

Tetris

Our other test case is the popular videogarne Tetris. A standard game of Tetris consist of a 10-by-20 size board and it is played with 7 different tetrominos. The basic rules are that the current tetron1ino is placed at the top center on the board. The player must guide it downward while being able to perfonn basic actions. An example of a garne in progress is shown in Figure 4.6 where the current tetrmnino is red at the top middle and the green tetrmnino on the side is the next piece awaiting to enter the game. Possible actions include shifting the tetrornino left or right, drop it to the bottom frmn it's current position or rotate it by incre1nents of 90°. vVhen a row contains the rnaximum filled cells, this row is rernoved from play and all occupied cells above are moved down a row. However, if the current tetromino cannot be placed on the board because of occupied cells the gan1e is over. The objective of the garne is to 1naximize the removed rows or equivalently play indefinitely as you will need to rernove rows to continue playing. A detail explanation of the rules can be found at [53]. A comn1on modification and for our purposes is instead of letting the agent guide the tetromino, the agent will siinply provide the position and rotatation from which the tetromino is dropped frmn the top in one action. This is an ideal problem because of the large state space and it's difficulty in conlpressing. It was shown by [54] that Tetris is NP-complete. Also by playing a gan1e using an alternative sequence of 'Z' and 'S' pieces you are guaranteed to lose using the standard board dimensions [55]. Fahey [53] conjectured that it might be possible to predict the length of a gmne and hence the nurnber of completed rows by plotting the histograrn of the remaining pile height after a completed row. However, as a consequence of that fact we will only consider a simplified example for faster training of the network.

In this document we will however limit our game to the '0' tetrmnino piece. By

47

Figure 4.6: State of a. game of tetris in progress.

limiting ourselves to a subset of the game we introduce an opti1nal strategy that an agent can use to play indefinitely. The total number of states in Tetris is approxi1nately 210 ·20 ~ 1060 . Although simpler than a regular game the agent n1ust learn to navigate the large state space and focus on the relevant features of the states. The purpose of this problen1 is to show our system on a challenging task where the state space is large enough that the extracted features would be quite beneficial and the length of episode sequence are sufficiently long to n1ake eligibility trace an influence. The representation we will be using is a 200 di1nension vector to represent the game's state. Since the orientation of the '0' piece is invariant under rotation we can reduce our action space to the width of the board which in this case is 10 cells. That is sufficient because the next piece is constant and we will not consider any look ahead strategy. A sin1ple representation was used for the action vectors. That is for action i, its corresponding 10 dimensional vector would be v such that v'i = 1 and Vj used for this problem are shown in Table 4.2 .
= -1

for all j -=1 i. The parameters

Result
In this section we will show our results and analysis of our experimental results described above.

48

System

Parameter
State dirnension Action dimension

Value
200 10 lo-r 0.001 0.01 0.9 0.9
1

Associative I\!Iemory

1]1

7]2 <5

a

Reinforcement Learning

I

E .\

t 0.9

Table 4.2: Para111eter settings for the Grid World learning task.

Reward

35000 30000 25000 20000 15000 10000 5000 20 40

60

80

100

120

140

Episode

Figure 4.7: The average accumulated rows removed by training the network for 150 episodes 1 with E = - over time.

t

49

The n1ajor problem on t his benchn1ark was that the network tends to forget what it has learned or 1nore precisely it has over learned specific action values. Since all actionvalue updates efFect all other actions as a consequence of the distributed representation. Our solution to overcorne this proble1n was to set the step size pararneter
E

to decrease

over tinw and illustrated in Figure 4. 7. It could be seen that the network quickly rea.ched a certa.in peak perforrnance where it has learned the optin1a.l policy but unexpect edly n1akes a poor Inove every so often. That actually is related to
E,

the rate at which the

network exploit kno\vledge it has. Since E is non-zero it will eventually n1ake a sub-optirnal rnove by placing a block on an even position thereby guaranteeing that row will never be ren1ove because of the cmnbination of block being used.

Figure 4.8: Tetris player making a sequence of bad n1oves that guarantee the bottmn three rows can never be completed.

Furthennore, the network develops serni-optinu1ol policy at n1ultiple heights in Tetris, which was surprising as shown in Figure 4.8. This is relat ed to the fact that the agent ca,n not recover frmn a bad m.ove although there always exist an optirnal action at each level to play indefinitely. On the san1e note, we ran the test with the smne parmneters but after 500 rows has been ccnnpleted we change frorn an policy where
E

E

= ~

policy to the full greedy

=

0 but still continue to update. \Ve speculate that the network ·would

give better performance since by that tirne a correct policy is known. The results can be seen in Figure 4.9. \Ve see that in episode #44 the policy switch occurred and the network took only tvvo rnore episodes to reach the stopping eriteria of 50,000 cornpleted
50

rows in episode #46. At this point the network will begin to approach a fixed-point for the state values being updated and should continue playing indefinitely.
Reward Reward

I~
Episode 100

~
___/

lOr---r
-+----1----L

~-

/

-----

38

40

42

Figure 4.9: Tetris agent following the greedy policy split between episodes 1- 37 and 37- 46. In the following experiments we will run the network with the default para1neters listed in Table 4.2 for up to 500 accumulated rows rem.ovecl and averaged over 20 independent trials. Each trial goes up to 200 episodes then is considered a failure if 500 completed rows was not reached. By that time we reason the correct policy should be learned. Now, consider the effect of the paran1eter a on the network performance. Different values of a are shown in Table 4.3. We see that better performance is achieve with lower values of a with sn1aller deviations from the mean at no lost of accuracy. Statistic !viean
SD

Accuracy

a= 0.10 44 16 100%

a= 0.25 43 13 100%

a= 0.50
58 41 95%

a= 0.75
48 26 100%

a= 0.90
48 34 100%

Table 4.3: Performance on Tetris for various values of a using only the network V. The data was collected by noting the number of episodes required to reach an accumulated total removed row of 500 averaged over 20 independent trials. In the next experilnent we vary the parameter A which controls the 1nagnitude of responsibility propagated back to previous states. For example as A approaches 1 corresponds to a l\1onte Carlo type of update while as A approaches 0 is the original TD update rule. In Table 4.4 we see that for lower and higher values of/\ shows better performance in the mean episode reached. This comes as a little surprising since intermediate values of A usually perform better. Similar results can be seen for the standard deviations and all variations had no failures.

51

Statistic :1\tfean SD Accuracy

A= 0.25 87 20 100%

/\ = 0.50
90 24 100%

A= 0.75 100 35 100%

A= 1.00 83 21 100%

Table 4.4: Performance on Tetris for various values of A using the Eligibility trace unit. The data was collected by noting the nmnber of episodes required to reach an accumulated total removed row of 500 average over 20 independent trials

As was discussed before we can use an additional BAl\II to extract features from the environment states to a lower dinrension before being process by the other networks. In this experiment we show the results of different dimensions mapped fronr the fixed 200 of Tetris in Table 4 ..5. Clearly the best range in this case for the Tetris problem is near
100 where in scores it has the lowest mean episode, deviation frmn the rnean and perfect

accuracy. If we go above, the extra con1putation introduced by feature extraction n1ight not be worth it for the less than perfect representation of states. In the case where the dimension is too high, the network tries to extract too many unnecessary features from the original domain. As expected, at lower di1nensions there is also a drop in performance. This might be explained by the network being less flexible in learning the correct features and its accuracy. But dealing with 200 2 weights to update in the original problenr and
1002 in the best state reduced version we may instead opted for 20 2 in a much sn1aller

network with a longer training session. The lower computational requirenrents nright be ideal in smne circumstances and with network F we are able to make such trade offs.
Statistic 1\fean SD Accuracy FE 20 96 31 95% FE 50 86 41 95% FE 100 75 23 100% FE 150 86 28 85%

Table 4.5: Performance on Tetris problem by varying the size of the projected dimension using the Feature Extraction unit. The data was collected by noting the number of episodes required to reach an accumulated total removed row of 500 average over 20 independent trials.

52

4.3

Conclusion

In this chapter we have demonstrated a working n1odel of our systen1 on various sinlulations and problems in particular a stochastic Gridworld problen1 and a modified game of Tetris. We first showed that our 1nodel can in fact store states returns using the network energy function. We later show the network is robust to varying number of returns being stored and performance gradually decrease with more states as expected. Afterwards we cmnpare the 1nodel to two familiar reinforcen1ent learning algorithn1s fron1 the literature that is SAR.SA and Q-learning of which our model is derived frmn. The results were supportive of our thesis as our model outperfonned the other methods with a p-value of 4.5 · 10- 24 , considering our system is approximating Q-learning, and Q-learning is using tabular storage. We later showed that the dyna1nic policy is incapable of combining its exploratory (chaos) cmnponent and exploitative (fixed-point convergence) component as a unifying policy. Afterwards we showed on the Tetris problem the network learning with a E-greedy policy was able to achieve up to 35, 000 cmnpleted rows but we later reveal that
E =

~

was too large a probability of Inaking a subopthnal move and was the source of 1nany bad moves. Instead we switched to a completely greedy policy after .500 completed rows and the network exhibited unexpected advance1nent by reaching 50, 000 rows in two additional episodes. Furthernwre, the network is able to learn optimal polices on states projected to lower di1nensions using the Feature Extraction network. Performance steadily decreases with dimensions as the network has less flexibility in learning the correct features fron1 the original space but the computational de1nand was also reduce with sinaller network sizes. However, surprisingly there was a decrease in perfonnance when using the Eligibility Trace network. update. Our explanation for this result is that the network is already capable of function approximation through its distributed representation. So after each update, all state values are 1nodified, however closer states should be affected 1nore in the direction of the error and move closer to this return. This follows from the fact that states are In fact performance increased as we approached a pure Ml\1 or TD

53

topographically related in the problem space and not generated independently as in our Gridworld problern. Therefore, updating a sequence of states can be seen as a fornr of eligibility trace where the pararneter ). would be correlated with the density (distance between states) of valid states in all of state space. In summary our system outperform standard tabular Q-learning on the Gridworlcl problem and therefore supports our thesis that Associative rnemories can be used as a computational model to perfonn supervised, unsupervised and now reinforcernent learning.

54

Chapter 5

Conclusion
In this work we showed that previous models of associative n1err1ories were capable of learning both supervised and unsupervised learning task with properties that makes these rnodels ideal for a computation n1odel of hurnan learning. These include dynamic properties such as establishing fixed-points, strange attractors, Hebbian rule, pattern c01npletion, noise correction, self organizing with competitive learning. Our objective was then to incorporate an additional paradigm reinforcement learning into the model while preserving its current abilities. VVe proposed a new model to solve reinforcerr1ent learning problems by an associative rnemory network. We achieved this by the novel idea of representing state returns as the negation of the network state's energy level. However this required a modification of the learning rule to correct for the new error. A modified Hebbian inspired learning rule that generalizes previous rules and as a consequence preserve the network properties. The effect this had on the energy function is to scale existing fix points to higher boundaries. Using this network we were able to outperform a native reinforcement algorithm, Q-learning, on a stochastic Gridworld task with p-value of 4.5 · 10- 24 and cornpleted up to 50, 000 rows in a rnodified Tetris garne. Three additional networks were suggested that includes a new policy based on dynanlical network principles using chaos. However, a suitable tradeoff between exploration and exploitation could not be found to get con1petitive results with the E-greedy action selection. The other was extracting features of the original problem state dimension to a lower dimension state for learning. This proved ideal as a trade off between the complexity of

55

the state space and the order of computations needed to learn correct actions. Our contribution to the research field is network V and representing states returns as the negation of the network's energy function. \Ve were able to encode Q-learning in the network and perfornr reinforcement learning. In addition, the dynmnic policy of encoding action selection as a searching process, similar to that of the 'm,ax operator used in E-greedy polices. Ideally this policy should be used since it is constructed from features native to AJVL Also, the technique of combining feature extraction into the RL algorithm instead of a separate preprocessing step that is done n1anually.

5.1

Future Work

Possible future directions we could go in includes determining a suitable tradeoff between exploration and exploitation in the dynanric policy as rnentioned before. This would be an ideal policy to use since we will be able to remove stochastic elements from the network and rely solely on chaos for any diversity. The advantage of this frorn a biological perspective is it would be n1ake more sense if we are to believe the brain is capable of being 1nodeled by a deterministic rnachine. We can also expect to extend the representation of states and actions from the discrete to include their continuous fonns since the underlying A1r1 network supports continuous states. Although discrete problems cmne up often in contrive exan1ples the real world is full of continuous problems worth exploring. One connnon method is to discretize continuous space to discrete intervals but this has a problem of scaling due to the large number of intervals required to guarantee good coverage of the original space or knowing in advance the optimal interval size. A solution based off the continuous fonn of the Bellman [52, 56] equations might be ideal in this case. Furthermore, we notice the network uses a union of the parmneters frmn associative memory and reinforcernent learning. It would be beneficial to reduce the number of parameters by cmnbining then1 or derived others from hyper parameters. \Ve have intentionally decided to focus on direct nrethods despite the advantage of having a rnodel of the environment. We will consider this a possible area for future work as a basis for planning

56

problems. In that case we will build a rnodel with a state value function instead of action values. The difficulty would be in estimating probabilistic outcomes frorn experiences. In addition to these, there are currently many open problems in RL that are under active research that we have not considered but techniques which we could incorporate into our network at a future tin1e such as hierarchical actions [57] and learning a model [43].

57

Bibliography
[1] K. Kaneko and I. Tsuda. Chaotic itinerancy. Focus Issue, 13(3) :926-936, 2003.

[2] S. Chartier and 1VI. Boukadoum. A sequential dynamic heteroassociative mernory for rnultistep pattern recognition and one-to-many association. IEEE Trans. Neural N etwork.s, pages 59-68, 2006. [3] A. Sharda and J. Freeman. How brains make chaos in order to make sense of the world. Behavioral and Brain Sciences, 10:161-195, 1987. [4] J. Freernan. The creation of perceptual meanings in cortex through chaotic itinerancy and sequential state transitions induced by sensory stirnuli. Ambiguity in mind and nature, 64:421, 1995.
[5] C. Watkins and P. Dayan. Learning from delayed rewards. PhD Thesis, 1989. [6] R. Sutton. Ten1poral credit assignrnent in reinforcement learning. PhD Thesis, 1984.

[7] R. Sutton. Learning to predict by the rnethods of temporal differences. I\lach'ine Learning, 3:9--44, 1988. [8] S. Singh and R. Sutton. Reinforcement learning with replacing eligibility traces. l'vlachine Learning, 22(1-3):123-158, 1996.
[9] D. Hebb. The organization of behavior. New York: Wiley, 1949. [10] Kosko B. Bidirectional associative memories. IEEE Trans. Syste'ms, Man and Cybernetics, 18(1):49-60, 1988. [11] S. Chartier, P. Renaud, and l\1. Boukadourn. A nonlinear dynan1ic artificial neural network model of memory. New Ideas in Psychology, 26(2):252-277, 2008. [12] S. Zhu and D. Hanrmerstrmn. Reinforcement learning in associative mernory. In Proceedings of the International Joint Conference on Neural Networks, volume 2, 2003. [13] J. Hopfield. Neural networks and physical systerns with ernergent collective cmnputational abilities. Proceedings of the National Acaderny of Sciences of the United States of Arnerica, 79(8):2554-2558, 1982. [14] J. Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Sciences of the United States of A·merica, 81(10):3088-3092, 1984. [15] Rojas R. JVeural Networks: A Systernatic Introduction. Springer, 1996.
58

[16] C. Leung. Optimum learning for bidirectional associative n1emory in the sense of capacity. IEEE Trans. System, ll!anual, Cybernetics, 24(5) :791-795, 1994. [17] D. Sherr and Jr J. Cruz. Encoding strategy for maxirnum noise tolerance bidirectional associative n1emory. IEEE Trans . 1Veural Networks, 16:293-300, 2005. [18] .J. Hopfield and D. Tank. 'neural' computation of decisions in optimization problerns. Biological Cybernetics, 52(3):141-152, 1985. [19] D. Ackley, G. Hinton, and T . Sejnowski. A learning a.lgorithrn for boltzrnann machines. Cognitive Science, 9:147-169, 1985. [20] S. Grossberg. Some nonlinear networks capable of learning a spatial pattern of arbitrary complexity. Proc. Nat. Acad. Sci., 60:368-372, 1968. [21] R. 1\ricEliece, E. Posner, E. Rodemich, and S. Venkatesh. The capacity of the hopfield associative memory. Infonnation Theory, IEEE Trans., 33(4):461- 482, 1987. [22] L. Personnaz, I. Guyon, and G. Dreyfus. Infonnation storage and retrieval in spinglass like neural networks. J. Physiq1Le Letter, 46:3.59-365, 198.5. [23] S. Chartier, G. Giguere, P. Renaud, J. Lina, and R. Proulx. Febarn: a featureextracting bidirectional associative mernory. In Proceedings of International Joint Conference on Neural Networks, pages 97-102, Orlando, Florida, USA, 2007. [24] G. Giguere, S. Chartier, R. Proulx, and J. Lina. Category development and reorganization using a bidirectional associative memory-inspired architecture. In Proceedings of ICCfv1 - Eight Internat'ional Conference on Cognitive 1\!Iodeling, pages 97-102, Oxford, UK, 2007. [25] S. Chartier and G. Giguere. Autonomous perceptual feature extraction in a topologyconstrained architecture. 2008. [26] S. Chartier, l\11. Boukadourn, and M. Arniri. Ban1 learning of nonlinearly separable tasks by using an asyrnmetrical output function and reinforce1nent learning. IEEE Trans. Neural Network, page 13, 2008. [27] S. Haykin. JVeural Networks: A CornpTehensive Foundation. New York: Wiley, 1999. [28] T. Kohonen. Self-organized formation of topologically correct feature rnaps. Biological Cybernetics, 43:59-69, 1982. [29] S. Chartier and Proulx R. Ndram: nonlinear dynamic recurrent associative rnemory for learning bipolar and non bipolar correlated patterns. IEEE Trans. Neural Networks, 16(6):1393-1400, 2005. [30] Vidyasagar :Lvl. Discrete optin1ization using analog neural networks with discontinuous dynarnics. In Proc. of International Conference A ut01nation, Robotics, Computer Vision, Singapore, 1994. [31] S. Chartier and I\1. Boukadoun1. a bidirectional heteroassociative 1nemory for binary and grey-level patterns. IEEE Trans. neuTal networks, 17(2) :385-396, 2006.

59

[32] Z. Xu, Y. Leung, and X. He. Asymn1etric bidirectional associative memories. IEEE Trans. Systems, Man and Cybernetics, 24(10):1558-1564, 1994. [33] I. Tsuda. Toward an interpretation of dynarnic neural activity in tenns of chaotic dynamical systen1s. Behavioral and Brain Sciences, 24:793-847, 2001. [34] J .. Amit. :Niodeling brain function: The world of attractor neural networks. 1989. [35] T . \Vills, C. Lever, F. Cacucci, N. Burgess, and J. O'Keefe. Attractor dynamics in the hippocampal representation of the local environrnent. Science, 308(5723):873-876, 2005. [36] Glenn Elert. The Chaos Hypertextbook. 1995. [37] NL Adachi and K. Aihara. Associatie dynmnics in a chaotic neural network. Neural lVetworks, 10(1):83-98, 1997. [38] A. Araujo, L. Bueno, and NI. Cmnpos. Dynan1ic behaviors in chaotic bidirectional associative 1nemory. Journal of Intelligent 1 Fuzzy System,s, 18:513-523, 2007. [39] S. Russell and P. Norvig. ATtificial Intelligence: A lvfodern Approach, 2nd Edition. Prentice Hall, 2003. [40] R. Sutton and Barto A. Reinforcernent Learning: An Introduction (Adaptive Computation and !fachine Learning). The :NUT Press, 1998. [41] R. Bellman. A 1narkovian decision process. Journal of 1vfathernatics and lvlechanics, 6, 1957. [42] R. Howard. Dynamic programrning and Inarkov processes. The Nl.I. T. Press, 1960. [43] L. Kaelbling and l\;1. Littinan. Reinforcement learning: a survey. Journal of Artificial Intelligence Research, 4:237-285, 1996. [44] C. Watkins and P. Dayan. Q-learning. JV!achine Learning, 8:279-292, 1992. [45] J. Peng and R. \Villimns. Incremental multi-step q-learning. In Proceedings of the Eleventh International Conference on IV!achine Learning, pages 226-232, San Francisco, 1994. Niorgan Kaufmann. [46] A. l\tfcGovern and R. Sutton. Towards a better q(lambda). Presented at the Reinforcernent learning vVorkshop, 1997. [47] R. Brafman and l'vl. Tennenholtz. r-max - a gernal polynomial time algorith1n for near-optimal reinforcernent learning. Journal of JVfachine Learning Research, 3:213231, 2002. [48] J. Gittins and D. Jones. A dynamic allocation index for the sequential design of experiinents. Progress in Statistics, 1974. [49] G. Pahn. On associative memory. Biological Cybernetics, pages 19-31, 1980. [50] N. Tsitsiklis and B. Van Roy. An analysis of ten1poral-difference learning with function approxi1nation. Automaic Control, IEEE Trans., 42(5):674-690, 1997.

60

[51] P. Stone and R. Sutton. Reinforce1nent learning for robocup soccer keepaway. Adapative Behavior, 13(3): 165-188, 2005. [.52] D. Bertsekas and .J. Tsitsiklis. Neuro-dynamic progra'mm,ing. :NUT, 1996. [.53] C. Fahey. Tetris. http://www.colinfahey.corn, 2003. [54] E. De1naine, S. Hohenberger, and D. Liben-Nowell. Tetris is hard, even to approxiInate. Technical Report JY!IT- LCS- TR-865, 2002. [55] H. Burgiel. How to lose at tetris. Mathernatical Gazette, 1997.

[56] JC Doya. Reinforcement learning in continuous time and space. ne,ural computation, 12:219-245, 2000.
[57] A. Barto and S. l\!Iahadevan. Recent advances in hierarchial reinforcen1ent learning. Discrete Event Dynarn,ic Systerns, 13(4):1573~7.594, 2003.

61

Appendix A

Source Code
Standard Functions sgn := Function[x, Map[If[# >= 0, 1, - 1] &, x]]; bound := Function[u, Map[Function[x, If[x > 1, 1, If[x < -1, -1, x]]], u]]; auto := Function[{m, u, q, \[Eta]}, With[{U = next[m, u], Q = energy[m, u]}, With[{err = Outer[Times, u, u] - Outer[Times, U, U]}, m + \[Eta] ( ( -q) - ( -Q)) err]]]; AUTO := Function[{m, u, \[Eta]}, With[{U = next[m, u]}, With[{err = Outer[Times, u, u] - Outer[Times, U, U]}, m + \[Eta]*err]]]; hetero := Function[{m, u, U, v, V, \[Eta]}, m + \[Eta]*Outer[Times, u- U, v + V]]; bamT := Function[{w, v, x, y, \[Eta]}, With[{X = next[v, y], Y = next[w, x]}, {hetero[w, y, Y, x, X, \[Eta]], hetero[v, x, X, y, Y, \[Eta]]}]] bamR := Function[{w, v, x, y}, {next[v, y], next[w, x]}]; febamT := Function[{w, v, xO, k, \[Eta]}, With[{yO = wta[next[w, xO], k]}, With[{x1 = next[v, yO]}, With[{y1 = sgn[next[w, xi]]}, {hetero[w, yO, y1, xO, xi, \[Eta]], hetero[v, xO, x1, yO, yi, \[Eta]]}]]]]; febamR[w_, v_, x_, n_, \[Delta]_: 0.01] := Nest[next[w, next[v, #,\[Delta]], \[Delta]]&, next[w, x, \[Delta]], n]; wta[v_, k_: 1] := With[{pos = Map[List, Sort[Take[Ordering[v], -k]]]}, MapAt[1 &, Map[-1 &, v], pos]]; (* \[Delta] = 1.65 unrestrained chaos; \[Delta] = i.45 restrained \ chaos; \[Delta] = 0.1 behaved *)
next[m_, v_, \[Delta]_: 0.01] :=
62

With[{mv = Map[bound, m.v]}, (\[Delta] + 1) mv - \[Delta]*mv~3]; eg := -energy[#1, #2] &; energy[m_, x_, \[Delta]_: 0.01] := With[{y = next[m, x]}, (* lowest energy is -Max[n ,
m] *)

-(x.y- (1 + \[Delta])*x.m.y + 0)/2];

(\[Delta]/2)*x.(m.y)~3

+

63

Network V

AMLearning := Function[{n, dimS, dimA, \[Lambda]}~ Module[{s, S, a, A, q, Q, r, ia, iA, v, sa, SA, \[Eta] = 10~{-4}, step = 1, ep = 1, R = 0, dim = dimS + dimA, policyz, Ar = {}}, v = ConstantArray[O, {dim, dim}]; Print ["Ep\t 11 , Dynamic [ep], "\tStep\t", Dynamic [step] , "\tRe/Ep\t 11 , Dynamic[R/(ep- 1) // N]]; policy := Function[{rn, s, \[CurlyEpsilon]}, If[RandomReal[] > \[CurlyEpsilon], With[{act = Map[energy[rn, Join[s, #]] &, actions]}, First[RandomChoice[Position[act, Max[act]]]]], Randominteger[{1, numA}]]]; For[step = 0; ep = 1, ep <= n, ep++, s = start[]; While[Not[terminal[]], ia = policy[v, s, 1/ep]; sa= Join[s, actions[[ia]]]; {r, S} = env[ia]; iA = policy[v, S, 0]; SA= Join[S, actions[[iA]]]; q = energy[v, sa]; Q = If[terrninal[], 0, energy[v, SA]]; With[{\[CapitalDelta] = r + \[Gamma]*Q - q},
v =

Nest[auto[#, sa, q + \[Alpha]*\[CapitalDelta], \[Eta]]&, v,
1]] ;

s = S; R = R + r; step = step + 1;

J;
Ar = Append[Ar, R/ep]; step = 0;
] ;

Ar]];

64

Network VPTF

AMLearning := Function[{n, dimS, dimA, \[Lambda]}, Module[{s, S, a, A, q, Q, r, ia, iA,
v, c, C, e, E, f, F,

sa, SA, \[Eta]2 = 0.001, \[Eta] = 0.00001*0.01, step= 1, ep = 1, \[Delta], R = 0, dim= dimS+ dimA, statea, Ar = {}, As = {}}, (*Print ["Ep\t" ,Dynamic [ep], "\tStep\t" ,Dynamic [step], "\tRe/Ep\t", Dynamic[R/(ep-1)//N]] ;*) Print[Dynamic[ep], "\t", Dynamic[step], "\t", Dynamic[R/(ep- 1) // N], "\t", Dynamic[R]]; {f, F} = {RandomReal[{-0.1, 0.1}, {dimS, dimState}], RandomReal[{-0.1, 0.1}, {dimState, dimS}]}; e = v = c = C = ConstantArray[O, {dim, dim}]; policy := Function[{m, s, \[CurlyEpsilon]}, If[RandomReal[] > \[CurlyEpsilon], With[{act = Map[energy[m, Join[s, #]] &, actions]}, First[RandomChoice[Position[act, Max[act]]]]], Randominteger[{1, numA}]]]; statea := Function[s, If[dimState ==dimS, s, {f, F} = Nest[febamT[#[[1]], #[[2]], s, Round[dimS/3], \[Eta]2] &, {f,
F}, 1] ;

sgn[febamR[f, F, s, 1]]]]; For[step = 0; ep = 1, ep <= n, ep++, s = statea[start[]]; While[Not[terminal[]], ia = policy[v, s, If[ep >= 90, 0, 1/ep]]; sa= Join[s, actions[[ia]]]; If[step == 0, {c, C} = bamT[c, C, sa, sa, \[Eta]2]]; {r, S} = env[ia]; S = statea[S]; iA = policy[v, S, 0]; SA= Join[S, actions[[iA]]]; q = energy[v, sa]; Q = If[terminal[], 0, energy[v, SA]]; \[Delta] = r + \[Gamma]*Q/ep - q; {c, C} = bamT[c, C, SA, sa, \[Eta]2]; e = auto[e, sa, 1, \[Eta]]; If[\[Lambda] <= 0,
65

v = Nest[auto[#, sa, q + \[Alpha]*\[Delta], \[Eta]]&, v, 1], Module[{x = sa, collect = {}}, While[Not[MemberQ[collect, x]] && Length[collect] <dimS, q = energy[v, x]; E = energy[e, x]; v = auto[v, x, q + \[Alpha]*\[Delta]*E, \[Eta]]; If[x[[1 ;; dimS]] == s && x !=sa, e = auto[e, x, 0, \[Eta]], e = auto[e, x, \[Gamma]*\[Lambda]*E, \[Eta]]]; collect= Append[collect, x]; sgn[next[c, x]]]

x = ]] ;

s
] ;

=

S; R = R + r; step

=

step + 1;

Ar = Append[Ar, R]; step = 0;
] ;

Ar]];

66

