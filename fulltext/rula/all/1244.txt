Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2009

A mobile mapping system utilizing a spherical camera
Craig Alleva
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Geographic Information Sciences Commons Recommended Citation
Alleva, Craig, "A mobile mapping system utilizing a spherical camera" (2009). Theses and dissertations. Paper 504.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

A MOBILE MAPPING SYSTEM UTILIZING A
SPHERICAL CAMERA
By

Craig Alleva

B.A.Sc Geomatics Engineering, York Toronto, Ontario, Canada, 2006

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Masters of Applied Science

in the program of Civil Engineering

Toronto, Ontario, Canada, 2009
Â© Craig Alleva, 2009

Author's Declaration
I hereby declare that I am the sole author of this thesis.

Abstract
A MOBILE MAPPING SYSTEM UTILIZING A SPHERICAL CAMERA
Craig Alleva

I hereby provide Ryerson University to lend this thesis to other educational institutions for
purposes of scholarly research only.

M.A.Sc, Civil Engineering, Ryerson University, 2009
The transportation departments belonging to respective provinces currently collect

highway management data with the use of several methods and systems which include visual field inspections, survey methods, aerial photogrammetry, as well as mobile data acquisition Furthermore, I authorize Ryerson University to reproduce this thesis by photocopying or by other systems. Spherical cameras offer an attractive alternative to standard mobile data acquisition

means, in total or in part, at the request of other educational institutions or individuals for the
purposes of scholarly research

devices for highway management systems as they provide full coverage with a single camera.
Inclusion of such a camera requires methods of determining relative, interior and exterior
orientation information, as well as bore-sight and lever arm determination. Specialized methods

of mosaicking the imagery are also required. This paper focuses on exploring these methods for spherical cameras. Several computer programs were developed to solve for relative, interior, and exterior orientation parameters. It was concluded that a spherical camera can be efficiently

utilized for highway data collection and provides full data coverage with a single camera system.

111

Acknowledgements
I would like to first thank Dr. Michael Chapman, who has provided me with tremendous support

Table of Contents
AUTHORS DECLARATION ABSTRACT ACKNOWLEDGEMENTS LIST OF TABLES ii iii iv vi

throughout my thesis, and has offered his expertise, knowledge, patience, and kindness, all of which has assisted me in the completion of this thesis. Without his guidance and help, this thesis
may not have been completed. I could not ask for a better supervisor.

LIST OF FIGURES LIST OF APPENDICES

vii viii

I would also like to extend my deepest gratitude for my fellow colleagues at PCI Geomatics,

whom offered me further guidance and assistance during the writing of this thesis. Most notably, I would like to thank James Lutes for his continued advice and his willingness to assist and
answer questions whenever I needed extra help. His expertise and experience was truly a
blessing.

CHAPTER 1 INTRODUCTION 1.1 Mobile Mapping 1.2 Principles of Mobile Mapping

1 1 2

1.2.1 Sensors 1.2.2 Positioning
1.2.3 Orientation

2 3
4

1.2.4 Full System Integration
1.3 Research Objectives.

5
7

My sincere thanks also goes out to Yubin Xin, of PCI Geomatics, for his advice and guidance in the field of Mobile Mapping, and for allowing me to bounce ideas off him.

CHAPTER 2 COLLINEARITY EQUATIONS CHAPTER 3 CAMERA CALIBRATION
3.1 Immersive Media Dodecahedron Camera 3.2 Interior Orientation Parameters
3.2.1 Principal Point Coordinate and Scale Factor Determination

9 11
11 13
13

The Department of Graduate Studies, Civil Engineering, and specifically Mary Neelands, for her countless efforts to assist me in all my administrative issues.

3.2.2 Lens Distortion Parameters Determination

14

3.3 Relative Orientation Determination
CHAPTER 4 SOLVING FOR INTERIOR ORIENTATION PARAMETERS CHAPTER 5 DIRECT AND INDIRECT GEOREFERENCING

15
27 35

Finally, I would like to thank my parents for their continued support throughout my university career, and for believing in me, and always being there when times were difficult.

5.1 Indirect Georeferencing 5.2 Direct Georeferencing CHAPTER 6 BORE-SIGHT AND LEVER ARM DETERMINATION
6.1 Bore-sight Angle Determination

35 36 37
40

6.2 Lever Arm Determination
CHAPTER 7 IMAGE MOSAICKING WITH A SPHERICAL CAMERA

41
43

7.1 Geometric Correction for Image Mosaicking
7.2 Color Balancing and Seam Line Elimination

44
48

7.2.1 Color Balancing (Histogram Matching)

49

7.2.2 Seam Line Definition

50

7.2.3 Seam Smoothing 7.3 Potential Problems with Approach CHAPTER 8 CONCLUSIONS AND FUTURE WORK TO BE DONE

50 51 53

REFERENCES

56

IV

List of Tables
Table 3.1- List of Cameras and their Associated Position and Orientation Information
Table 4.1- List of Cameras and their Associated Position and Orientation for Virtual Point

List of Figures
25 29 29
33

Figure 1.1: Angular Orientation Parameters Figure 1.2: Detailed Diagram of Elements of Georeferencing for Close-Range Case Figure 2.1: Principle of Collinearity Equations
Figure 3.1: Dodecahedron

4 6 9
11

Production Table 4.2 - List of Cameras and their Associated Interior Orientation Information for Virtual Point Production
Table 4.3 - List of Cameras and their Associated Position and Orientation as Determined By
Self-Calibrating Bundle Adjustment

Figure 3.2: Dodeca2360

12

Figure 3.3: Regular Pentagon - Side Length 1
Figure 3.4: Regular Pentagon with Local 2D Coordinate Frame Superimpose
Figure 3.5: Mathematical Relationships of a Regular Pentagon

15
16
17

Table 4.4 - List of Cameras and their Associated Interior Orientation Information as Determined by Self-Calibrating Bundle Adjustment 34

Figure 3.6: Vertices of a Regular Pentagon

18 19 20 21
21

Figure 3.7:
Figure 3.8: Figure 3.9:

Pentagon with V4 Centered on Origin, followed by Rotation of 64 Degrees
about X-Axis, followed by Rotation of 108 Degrees about the Z-Axis Upper Right Face of Dodecahedron with Reference Top Face Complete 2D Representation of Upper Portion of Dodecahedron

Figure 3.10: Completed 2D Representation of Lower Portion of Dodecahedron

Figure 3.11: Compute Vectors V3VI, V4VI, Bi-Sector

23

Figure 3.12: Front View of Dodecahedron Figure 3.13: Top View of Dodecahedron
Figure 4.1: Virtual Sphere of Points Centered at Centre of Dodecahedron

26 26
28

Figure 4.2:
Figure 6.1:

Example Image Coordinate System Utilized
Detailed Diagram of Elements of Georeferencing for Close-Range Case

30
38

Figure 7.1: Figure 7.2: Figure 7.3:
Figure 7.4:
Figure 7.5:

Proj ecting Image onto Surface of the Sphere Mosaic Generated by Immersive Media using Data Captured by Dodeca 2360... Super-Imposed Spherical Coordinate System on Mosaic
Image Edge Points Plotted onto Immersive Media Mosaic Grid
Minimum Bounding Box Encompassing Image on Mosaic

44 46 46
47
47

VI

vn

List of Appendices
APPENDIX A - NORMAL EQUATIONS STRUCTURE.
.54

1. Introduction
1.1 Mobile Mapping

In the past decade, expansion and growth of urban areas has occurred rapidly.

The

occurrence of rapid growth requires city planners to have access to detailed and recent
information related to location and control for sites under development. The high demand for

location information can also most recently be seen in Alberta, where development has been
booming over the last few years. With the rising demand of location information, Geographic The information

Information Systems which are spatially referenced have been developed.

contained within a Geographic Information System have been collected via various methods, such as surveying and GPS point collection, however these methods are often expensive and
provide only point information, rather than full coverage. An additional drawback of these

methods is that they are often time consuming, and do not answer, "increasingly complex

questions concerning the interaction of different factors in urban centers and their time

dependencies." (El-Sheimy, pg 1, 1996). Fortunately, the field of photogrammetry (both aerial
and close-range types) has developed and can provide a solution to rapid data collection which

conventional methods fail to realize. One such solution is the deployment of a mobile mapping
system.

Mobile mapping has been around for decades, but until recently has not gained as much
attention as it now does today. This was due primarily to the fact that Mobile Mapping Systems

(MMS) were, in the past, heavily restricted to areas of application in which the exterior orientation parameters of the cameras/sensors onboard the vehicle could be easily determined.
This meant that mobile mapping activities had to be conducted in such a way so that the exterior orientation parameters could be derived from ground control points. However, over the last

decade, significant advances in satellite technology, as well as inertial navigation technology, have removed this restriction from MMS's. Mobile Mapping Systems can now operate virtually anywhere, and thus the need for ground control points has vanished. Further advances in

technology have resulted in a transition from the traditional analog based methods of image
collection (i.e., film) to digital based methods of image collection. These advances have now

resulted in a mobile mapping system which contain various sensors and image capturing devices,
as well as the capability to acquire, store and process georeferenced data.(Schwarz et. al 2004).

vm

1.2 Principles of Mobile Mapping
Mobile mapping systems operate primarily on photogrammetric principles and, like

1.2.2 Positioning

Determination of the three-dimensional coordinates of the camera exposure station at the
time an image was taken is an important component of any photogrammetric survey.

traditional photogrammetric platforms, include some type of sensor used to image the area of
interest, as well as a positioning device, and an orientation device. will be discussed herein.
1.2.1 Sensors

Each of these components

Determination of the three-dimensional coordinates of the camera exposure stations, in the past, was primarily done by a method known as photo triangulation. Photo triangulation is a method in which intersection points of the same object in different images, with known threedimensional coordinates, are used in a computational process to derive the position and

In the field of photogrammetry, the most important device is the sensor for which the system uses to image the area of interest. Two different types of sensors exist; active sensors and passive sensors. An active sensor is a sensor which provides its' own source of energy to The sensor operates by emitting energy, in the form of

orientation of the images at the time in which the image was taken by the camera. This method required the knowledge of ground control points (points with known 3D coordinates) in a set of
overlapping images.

illuminate the area of interest.

electromagnetic energy, which interacts with the target (area of interest) and is then reflected
back to the sensor where it is detected, measured and then recorded by the sensor system. The

More recently, the Navstar Global Positioning System (GPS), "has become generally available and has been considered fully operational on a worldwide basis since 1993." (Ebadi, pg 6, 1997) The development of the Global Positioning System has allowed for a much easier way
to determine the exterior orientation parameters (position and orientation) of the camera

most prominent advantage of these types of sensors is that they can be operated at any time of
day. A passive sensor is a sensor which uses the sun as its source of energy. When

exposure stations at the time of exposure.

In aerial applications, the airborne kinematic GPS

electromagnetic radiation (from the sun for example) interacts with the target (area of interest), it is reflected or emitted and is detected, measured and recorded by the passive sensor. These

method has been used to determine these exterior orientation parameters each time an image is taken. In kinematic GPS, "the rover station is in constant motion ... [and] requires a stationary receiver to be located at a known base station, and an initial start-up period during which the

sensors do not have their own source of radiant energy. In photogrammetry, the sensor most commonly employed is the camera. A standard

rover receiver is held stationary."(Wolf et al, 2000). The purpose of including a "start-up" time is so that integer ambiguities can be calculated by software applications. After the unknown However,

camera works by recording, "the image of an exterior object [which] is projected upon a
sensitized plate or film, through an opening usually equipped with a lens or lenses, shutter, and

integer ambiguities have been determined, the rover station can then begin moving.

variable aperture." (Wolf et al, 2000).

Recently, the advent of digital cameras has resulted in The digital camera operates by detecting

this can only continue as long as a constant lock is maintained on the satellites. If, as in the case

these devices finding their way into photogrammetry.

of mobile mapping applications, there occurs a loss of signal lock due to the receiver onboard the
mobile platform passing under a tree or bridge, or via some other obstructions, then the rover

electromagnetic energy with the use of some type of solid-state detector. The most common type of solid-state detector currently in use is known as the charged-coupled device (CCD). These

c

station must return to a previously surveyed position. This major disadvantage limits the use of
kinematic GPS methods to areas in which there are little to no obstructions; primarily areas
considered "open areas". To overcome this drawback, a method was developed known as on-

devices operate by detecting electromagnetic radiation which is incident upon one of the CCD elements at a pixel location. The CCD element then generates an electrical charge which is in

proportion to the intensity of the electromagnetic radiation which was incident upon the CCD element. "The electric charge is subsequently amplified and converted from analog to digital

the-fly ambiguity resolution. "This method of ambiguity resolution is performed by a software technique whereby many trial combinations of integer ambiguities for the different satellites are
tested in order to determine the correct set." (Wolf et al, pg 361, 2000).

form." (Wolfetal, 2000).

The advent of the Global Positioning System has greatly enhanced the ability of exterior

taking photographs of the horizon and yields the rotation angles of the camera frame with respect to the horizon. Another method used to determine the angular orientation was the solar

orientation determination of camera exposure stations in photogrammetric applications, both in
airborne and land-based mapping. Ebadi (1997) has concluded that the use of the global

periscope. However, the angles determined were referenced to the sun. Both of these methods were considered impractical due to the achievable accuracy falling short of requirements.
Today, the approach taken to determine the angular orientation of the exposure station is through

positioning system has also helped in achieving greater accuracy of exterior orientation parameters by including the GPS measurements into the aerial triangulation adjustment. "The

combined

GPS-photogrammetric

block

adjustment

takes

advantage

of

weighted

GPS

the use of an Inertial Measurement Unit (IMU).

These EVIUs operate by using accelerometers,

observations, which significantly reduces the number of ground control points needed in a
conventional block adjustment." (Ebadi, 1997).
1.2.3 Orientation

as well as rate gyroscopes, to sense and detect how the platform on which it is mounted on is rotating in space. Once this is known, position and velocity can be determined by performing This is done through the use of an Inertial

integration on the accelerometers and gyros.

When taking photographs during a photogrammetric survey, it is always desirable to have
the optical axis of the camera to be vertical to the surface being imaged. However, despite great efforts to achieve this goal via, for example in aerial photogrammetry, the use of leveling
instruments such as level vials, as well as the use of other stabilization equipment, it is usually not the case that the optical axis will be vertical to the surface being imaged. When the optical

Navigation System (INS). An INS typically consists of a computer and an IMU, and is initially fed its' position and velocity information from a GPS. It can then use information from its' EVIU
to compute position and velocity information. 1.2.4 Full-system Integration
Advances in sensors, positioning and orientation have allowed for the potential to

axis is not vertical to the surface, the resulting photographs are called tilted photographs.

The

combine all of these technologies into a powerful mapping system that can be used to image an area and produce the precise location and orientation of these cameras in a near instantaneous time frame. In particular, the combination of GPS and INS has resulted in a very convenient
"Since both sensor systems [GPS and

angular orientation of the photographs at time of exposure is given by three parameters denoted omega (co), phi (cp), and kappa (k). Figure 1.1 illustrates these three angles:

method of determining position and orientation together.

O

K

INS] are of almost complimentary error behaviour, the ideal combination will provide not only higher positioning, velocity and attitude accuracy but also a significant increase in reliability, as both systems are supporting each other." (Cramer et al, pg 2, 2001). When there is a loss of

signal lock with the GPS system, the INS can help the situation by supplying very accurate

information about the initial position, as well as the velocity of the platform, providing
'trajectory' information about the platform. In addition to these benefits, the GPS can also assist
Fig 1.1. Angular orientation parameters. Image provided by Robert Pless, Associate Professor,
Washington University

the INS by providing, "accurate estimates on the current behaviour of its error statistics."
(Cramer et al, pg 2, 2001).

With respect to the imaging sensor (camera) used onboard the platform to map the area of interest, the benefits of an onboard integrated GPS/INS system becomes increasingly apparent. "The direct measurement of the fully exterior orientation of any sensor during data recording becomes feasible [due to the inclusion of an integrated GPS/INS], which offers an interesting

In the above figure, rotation about the x-axis (co) is known as pitch, rotation about the y-axis (q>)
is known as roll and rotation about the z-axis (k) is known as yaw/heading.

In the past, several methods existed to determine the angular orientation of the camera exposure station. One such method was the horizon camera. The horizon camera operates by

alternative to the standard indirect approach of image orientation based on classical aerial

triangulation."

(Cramer, pg 2, 2001).

However, due to the fact that the GPS/INS system is

1.3 Research Objectives

physically separated from the imaging sensor, there exist several offsets that must be determined
in order to properly identify the final positioning information of the imaging sensor's perspective center. The diagram below from El-Sheimy (1996) illustrates the relationship between the GPS,
INS and camera frames (c-frame).

The transportation departments belonging to respective provinces currently collect highway management data with the use of several methods and systems which include a visual field inspection, survey methods, aerial photogrammetry, as well as mobile data acquisition

systems.

With respect to the mobile acquisition systems, many departments own their own

c - frame

system or rent these systems so as to acquire the highway inventory data which they seek. The

INS b-frame

R

benefit of these mobile systems is that they have the ability to capture and record assets belonging to the highway, as well as road conditions of the highway. All of this can be done at a normal highway speed so as to provide the most convenient method of data collection. addition to these benefits, these mobile systems can In

also record the three-dimensional

coordinates of lanes and centerlines, as well as appurtenances, panoramic right-of-way video and pavement video. Many systems operate through the use of a series of cameras which record

images of the surroundings and derive information of interest from these images. If, instead of

Target point

using standard cameras, the inclusion of a spherical camera was used in conjunction with these

systems, the generation of high precision-three dimensional coordinate information can be
m

derived for the parameters of interest on a highway with complete coverage.

The desired

information can then be georeferenced and can then be entered into highway management

m-frame

systems to provide a much more convenient and efficient method of field crew deployment for
maintenance and upkeep of a highway.

All
Fig 1.2. Detailed diagram of elements of georeferencing for the close-range case (El-Sheimy, pg 57,1996).

information

collected

by

mobile

mapping

systems

requires

a

method

of

georeferencing this information so that it can be related to an Earth-fixed coordinate frame. Georeferencing essentially refers to the process of assigning three-dimensional coordinates to

points in an image.

Prior to the georeferencing process, these mobile systems also require the

In Figure 1.2, the offsets that must be determined between the sensor systems is the vector ab,
which is the offset vector between the INS body frame and the camera frame, and Rbc which is
the rotation between the camera's frame and the INS body frame. Both of these parameters are determined via calibration and will be discussed in later sections. The determination of these

determination of a variety of parameters inherent to the camera, and the camera's spatial location
at the time of exposure. The first set of parameters is known as interior orientation parameters,

and the second set of parameters are known as exterior orientation parameters.

This research

paper will extend current methods of determination of these parameters to the application of a spherical camera. For the purposes of this paper, a series of computer programs were written

parameters provide for the opportunity to relate the coordinates of an object point in the camera
frame to its' corresponding three-dimensional coordinates in an Earth-fixed frame. This will be discussed in later sections.

and, collectively, they provide a means of determining the following:
Â· Interior orientation of each camera which comprise the spherical camera,

Â·

Relative orientation existing between each camera of the spherical camera body,

Â·

Position and orientation of each camera at time of exposure

In addition to these programs, this research paper will propose a method of "stitching" each

2. Collinearity Equation
Before a discussion on the determination of the interior and exterior orientation

image captured by the spherical camera to build a 360-degree panoramic, "immersive" image strip. Achieving accurate color and tonal balance across the image will also be discussed.

parameters can commence, the collinearity equation must be introduced.

The fundamental
This equation

Finally, the determination of the positional offset vector between each camera perspective center and the GPS mounted on the vehicle, as well as the orientation offset between each camera and

equation utilized in photogrammetry is known as the collinearity equation.

essentially states that, "an object point, its homologous image point, and the perspective centre are all collinear" (Ebadi, pg. 12, 1997). This condition is shown in Figure 2.1, and is followed
by the collinear equations themselves.

the EVIU will also be discussed.

Image [positive)

PC Perap. Central

P=Object point J

p=Image point

Fig 2.1 Principle of Collinearity Equations. Image provided by Geometric Software

Collinearity Equations (From Ebadi, 1997)

_Q

F

_x

,

(2.1)
p _

Where

3. Camera Calibration
mn ml2 m13

M =

m21
m31

TM22
m32

m23 m33

(2.2)

3.1 Immersive Media Dodecahedron Camera

The spherical camera considered as the sensor for the mobile mapping platform proposed in this project is the Dodeca 2360 Camera, manufactured by Immersive Media Corporation. The body cosÂ® cos^r) -cosÂ® sin(Â£)
sinÂ®
Where:

cos^y) cos($ +sin^y) sinÂ® cos(Â£) cos^y) cosifc) -sin^y) sinÂ® sin^r)
-sinÂ£y)cosÂ®

sin^y) sin^r) -cos^y) sinÂ® cos^c) sin^y) cos^) +cos^y) sinÂ® sin^r)
cos^cosÂ®
(2.3)

of the camera is in the shape of a dodecahedron, "a platonic solid composed of twelve regular

pentagonal faces, with three faces meeting at each vertex." (Wikipedia) Below is a figure of a
typical dodecahedron:

(xj, yO

= the image coordinates,

(xp,yp) c m ij

= coordinates of the principal point, = the camera constant, = elements of the 3D rotation matrix,

(Xi, Yi, Zi) = coordinates of the object points,

(Xo, Yo, Zo) = coordinates of the exposure station,
M = rotation matrix,

(a),<p,K) = rotation angles,(change O in rotation matrix (2.2) to (/)) ky = the scale factor corresponding to the y axis in digital camera.

The

determination

of

the

interior

and

exterior

orientation

parameters

in

a
Fig 3.1 Image provided by Wikimedia

photogrammetric survey is often performed by solving the collinearity equations for each of

these parameters by means of a least-squares adjustment. Details on this will be explored in later
sections.

10

11

Given below is a figure of the Dodeca 2360 by Immersive Media.

vectors and angular differences between the cameras and the GPS/INS components will be
discussed in later sections.

3.2 Interior Orientation Parameters

Interior orientation parameters are the elements which define the interior geometry of any standard metric or digital camera. The calibration of a camera to determine these parameters is a process which must be performed prior to any photogrammetric survey. There are four

components to interior orientation parameters. These are listed below:
Â·
Â·

The principal point coordinates (xp ,yp),
The focal length of the lens /,

Â·

The scale factor corresponding to the y-axis,

Â·

Lens distortion parameters (5 parameters).

For the purposes of this project, the interior orientation parameters for each camera of the IMC camera must be solved for. adjustment must be utilized. In order to solve for these parameters, a self-calibrating bundle The result of the self-calibrating bundle adjustment gives the

interior orientation parameters, as well as the spatial coordinates of the camera exposure station
Fig 3.2 - Dodeca 2360 - Image Provided by Immersive Media

(X0,Y0,Z0)and the orientation of the camera at the time of exposure, denoted by {co,(p,K) (Exterior Orientation Parameters). The self-calibrating bundle adjustment will be discussed in

The Dodeca 2360 consists of the following (information provided by Immersive Media)
 Eleven CCD 1/3" sensors in a modular dodecahedral array

later sections.

3.2.1 Principal Point Coordinate and Scale Factor Determination The principal point is the point in the image which is considered the geometric centre of

 

Field of view of full array: 360 degrees horizontal, 290 degrees vertical (91.7% of sphere) Most efficient division of the sphere for consistent quality in all directions

the image. In the aerial photography case, this point coincides with the point on the ground in

In order to properly utilize the Dodeca 2360 camera system for use in a mobile mapping
system, the determination of the relative orientation (offset and angular differences) between each "face" or camera of the dodecahedron must be determined, as well as the interior orientation of each camera. Furthermore, the offset vector between each camera and the GPS, as well as the angular differences between each camera and the INS must also be determined.

which the optical axis intersects. The principal point, as well as the scale factor, ky, relate the
image coordinate system to the computer coordinate system. The scale factor ky accounts for

the y-axis scale factor as well as a reflection of (-1) to take into consideration the difference in
the image coordinate system and the computer coordinate system (one is a right handed system and the other is left handed). The transformation from the computer coordinate system to the

Determination of the relative orientation between each camera of the dodecahedron, as well as the interior orientation parameters, will be discussed herein. The determination of the offset

image coordinate system is given below from El-Sheimy (1996) below:
X Xc X ,

(3.1)

12

13

y = (yc-yP)/ky
Where:

(3.2)

causes off-center patterns of distortion.

The mathematical models used to model these

distortions are given below from El-Sheimy (1996);

Â·
Â·

(xp9yp) are the principal point coordinates,
(x, y) are the image coordinates,

Ax = xt (kxr2 + k2r4 + k3r6) + p1 (r2 + 2xt2) + 2p2xiyi Ay = y&r2 +k2r4 + k3r6) + 2Plxiyi + p2(r2 +2y2)
Where:

(3.5)
(3.6)

Â· Â·

(xc, yc) are the computer coordinates, ky is the scale factor corresponding to the y-axis.

Â·

kx,k2,k3 are related to the symmetric radial distortion,

The determination of the principal point coordinates and the scale factor is done via a self-

Â·

Pi,p2 are related to the decentering tangential distortion,
-2 , --2

calibrating bundle adjustment. This bundle adjustment makes use of the collinearity Equations
2.1 given previously. These equations are reproduced below for reference, and are slightly modified in appearance to reflect the parameters of interest which are sought after:

3.3 Relative Orientation Determination
(3.3)

*,Â·=-/ -- + Ax,

1

W

As was mentioned earlier, the determination of the offset vector(s) between each face of the
dodecahedron must be determined, as well as the angular differences between each face. In

y Â· = -/ -- + Ay.
Where:

(3.4)

order to determine these parameters, a simple "unit" pentagon is considered (i.e., the length of
each side is 1)

a= 1

Â· Â·

f = focal length of the lens, Ax and Ay are correction terms corresponding primarily to the lens distortion inherent to each camera,

3.2.2 Lens Distortion Parameters Determination Lens distortions are caused by imperfections in the manufacturing of the lens, and result in incoming light rays being bent and changing directions after passing through the lens in such a way that their direction of travel is no longer parallel to their incoming directions. They do not degrade the image quality, but they do degrade the geometric quality of the images. Two types of lens distortions which are normally prevalent are the symmetric radial lens distortions as well
Fig 3.3 - Regular Pentagon - Side Length = 1

as decentering lens distortions.

In symmetric radial lens distortion, imaged points become Decentering lens distortions

In order to determine the relative orientation existing between each pentagonal face of the

distorted along radial lines which emanate from the optical axis.

dodecahedron, the vertices of the pentagon given above must be determined.

This is done by

14

15

first superimposing a local 2-D coordinate frame on the above pentagon, centered at the centroid.
This is shown below

a/2
Fig 3.5 - Mathematical relationships of a regular pentagon

Where:
Fig 3.4 - Regular Pentagon with Local 2-D Coordinate Frame Superimposed
--

(3.7)

To compute the coordinates of the vertices, some well-known mathematical relationships for a
regular pentagon must be utilized. These are illustrated below:

r'=

--

(3.8)

With these equations in hand, and knowing that the interior angles of a pentagon are 108 degrees,
the following vertices were computed:

16

17

that vertex V4 (shown in Figure 3.7) becomes centered at the origin (this is done by subtracting

the coordinates of V4 from all the other vertices). The vertices are then rotated about the X-Axis by +64 degrees, and then rotated about the Z-Axis by +108 degrees. This gives the following

diagram (note that this is 2-D, so the effect of depth is not shown). The rotated pentagonal face
is angled "downward" (into page) by 64 degrees.

t-s-|

V2

V3 V4
\

1

\_

/

\

'" ^. V3

/
Fig 3.6 - Vertices of a Regular Pentagon

N
\

n 5

Vl1

\

\

\y 4
nl-0.4

i

V5 <C

-o"/

-0.6

-0.2

()

0.2

~HtA---j

V5 ^^^> V2

06

^8

n r

Once the vertices of the regular pentagon have been determined, it now becomes possible to determine the coordinates of the vertices of every other pentagonal face of the dodecahedron. This can be done by first introducing the Z-Axis to our already established coordinate system.
^1_

V1
Centroid-URF

Top Face -m- Upper-Right Face

The positive Z-Axis points out of the page, and completes the right-handed coordinate system.
Next, the pentagon given above is designated as the "top" face of the dodecahedron. This gives a dodecahedron whose coordinate system is centered on the top face.

Fig 3.7 - Pentagon with V4 Centered on Origin, followed by rotation of 64 Degrees about X Axis, followed by
another rotation of 108 degrees about the Z-Axis

To compute the coordinates of the vertices of each pentagonal face of the dodecahedron with respect to the coordinate frame centered on the top face, the knowledge of the dihedral
angle of a dodecahedron is utilized. The dihedral angle is the angle between any two adjacent

The rotated pentagon is then translated in such a way so as to align edge V4-V3 to edge V2-

V3. This is done by adding the coordinates of vertex V2 to all of the V coordinates. This gives
the following figure:

faces of a polyhedron.

In the case of a dodecahedron, this angle is approximately 116 degrees.

Next, the coordinates of the vertices of the top face about the x, y and z axes are rotated by various angles, and then translate these coordinates so as to obtain the coordinates of the vertices of the pentagonal faces of the upper portion of the dodecahedron. Rotation angles about the Z-

Axis are derived using geometric knowledge of a pentagon, whereas rotations about the X/YAxes are derived from knowledge of the dihedral angle. An example is given below for the

determination of the vertices of one of the faces. First, a translation is applied to the points such

18

19

Upper Portion Dodecahedron

V"2
1

V4

V3
v V"?

/
;
! V5 *-^

/

VI

--0!

15

/

-0 5

(J

05

V"4

1

i

1

^\

^' -Â·j

\,
-4-

 Top Face -m-- Upper-Right Face

Centroid-URF

---Top Face

-Upper-Right Face

Back Face

--*-Upper-Left Face

 Lower-Left Face

- Lower-Right Face

Fig 3.8 - Upper Right Face of Dodecahedron with Reference Top Face

Fig 3.9 - Complete 2D representation of Upper Portion of Dodecahedron

The equation used to calculate the coordinates of the vertices of the new face is given below:

To compute the coordinates of the vertices of the pentagonal faces on the lower portion

of the dodecahedron, the vertices of the upper portion of the dodecahedron are rotated about the
X-V4

X-Axis by +180 degrees, and translated down along the negative Z-Axis. The bottom portion of
(3.9) the dodecahedron is given in the Figure 3.10:

R7(108Â°)Rx(64Â°) Y-V4,
Z-V4.

Lower Portion Dodecahedron

Where:

cos(^)
sin(tf)
0

-sin(^)
cos(^)
0

0"
0
1

10 0 cos(z?)

0 -sin(^)
(3.10)

0

sin(^)

cos(z^)

A similar procedure is performed to compute the coordinates of the vertices of all the

-0 5

0

other pentagonal faces on the upper portion of the dodecahedron.

Figure 3.9 below shows the

05

.

pentagonal faces plotted in Microsoft Excel for the upper portion of the dodecahedron.

Bottom Face B_Lower_Left

-Â·-- B_Lower_Right -*-- B_Upper_Left

;

B_Front

--- B_Upper_Right

Fig 3.10 - Complete 2D Representation of Lower Portion of Dodecahedron

20

21

Once the coordinates of the vertices have been computed, the centroids of each pentagonal face can also be computed. The reason that the positions of the centroids are required

is because the centroid is essentially the position of the perspective centre of the camera on the respective faces of the dodecahedron.
particular face is given below:
V5
Centroid

V4

bi-sector = y axis

The equation for the computation of a centroid for a

VI
= VI

Y
z
Facel

(3.11)

V5
Facel

V2

vi

The centroids essentially provide the offset vectors between perspective centers of each camera. Together with the rotation angles used to rotate the reference face, a complete set of
As a final check to ensure that the rotation angles

relative orientation parameters are obtained.

V1
Fig 3.11 - Compute Vectors V3V1, V4V1 and Bi-Sector

utilized are correct, the orientation of the local reference frame of each individual pentagonal face is computed as follows:

e31

=

bi-sector = e4l +e3l (3.12, 3.13, 3.14)
7--7

For each face:

1)

Compute Vectors V3V1 and Vectors V4V1, then compute the bi-sector of these two vectors. Normalize the bi-sector to obtain direction vector of the Y-Axis:

Yaxis - direction= normalize(bi - sector)

(3.15)

2) Take the cross-product of e4l and e31, then normalize to obtain the direction vector for
the Z-Axis:

Zaxis - direction = normalize(e4l x e31)

(3.16)

3) Take the cross-product of the Y-axis Direction vector with the Z-axis Direction vector to

obtain the X-axis Direction:

Xaxis - direction = Yaxis X Zaxis

(3.17)

22

23

4) With the direction of each axis defined, the following matrix can be formed

(3.23)

'yx

/r=tan

(3.24)

eLF=

(3.18)
a)=tan

(3.25)

Where eLF describes the orientation of each axis on the local face, and each column in

the above matrix is the direction vector for the x-axis, y-axis, and z-axis, respectively.

These angles were computed for each face and were equivalent to the angles used to initially rotate the coordinates of the vertices of the reference face.

5)

To check the rotation angles and ensure they are correct, the rotation matrix that rotates

the coordinate frame of the top face to the coordinate frame of the face under
investigation must be determined. This is done by forming the following relationship:

Each face was assigned a "camera" number. In Table 3.1, each camera number is given with its' offset and orientation. Prior to this, the location of the reference frame was translated

downwards from the top face to the centre of the dodecahedron to simplify the situation.
eLF=R- eTF

(3.19)

CAMERA NUM

X(m)
0.000 0.941
0.000 -0.941 -0.582 0.582 0.000
0.941

Y(m)
0.000 0.306 0.990 0.306 -0.801 -0.801 0.000
-0.306 -0.990

Z(m)
1.119 0.500
0.500

Kappa(deg)
0
108 180 -108 -36 36

Phi(deg)
0 0 0 0 0 0 0
0

Omega(deg)
0
64 64 64 64 64 180 -116 -116 -116

Where:

1000

eTF is the orientation of the coordinate frame of the top face (our reference face)
and is simply given as follows:

1001
1002

1003

0.500 0.500
0.500 -1.119 -0.500

"1
eTF =

0
1

0"
0
1

1004

0 0

(3.20)

1005

0

1006
1007 1008

0
-108

R is the combined rotation matrix resulting from the consecutive rotations of Rx(co) - Ry((/)) -Rz(k) and is given below:

0.000
-0.941 -0.582 0.582

-0.500 -0.500
-0.500

180
108

0
0

1009

-0.306
0.801

cos^cos^r
R=

cos^sinr
sinaco&p
coscocosfi
(3.21)

1010

36
-36

0
0

-116
-116

1011

0.801

-0.500

Table 3.1 - List of cameras and their associated position and orientation information

Since eLF is known, and eTF is known, one must only solve for the elements of R . This is

simplified further by the fact that eTF is identity. This gives the following:

eLF=R

(3.22)

Solving for the rotation angles is therefore given as follows:

--if

24

25

J V

Each pentagonal face was also plotted in Google SketchUp according to edge-length (a=l), CCD offset (x, y, z from Table 3.1) and rotation angles (from Table 3.1).
given below:

4. Solving for Interior Orientation Parameters
The self-calibrating bundle adjustment was utilized to solve for the interior orientation
parameters of each camera of the dodecahedron. The relative orientation parameters determined

The result is

in Section 3 were used as estimates of the exterior orientation parameters for the model. For the

purposes of this research, the method employed is the Normal Equation Formation method of
solving the self-calibrating bundle adjustment. The following algorithm was programmed using the Fortran programming language and utilized for a 12 camera, 12 image, 697 point, 804
observation system (see Section 6 for results and observations). For completeness, 12 cameras,

resulting in 12 images, is chosen to reflect the entire dodecahedron. In reality, with respect to the
actual camera system, this would really be an 11 camera and, therefore, an 11 image system.

Ground control points were simulated by generating a series of virtual "spheres", each centered at the centre of the dodecahedron, and each with varying radii. Each sphere was tessellated using

different grid spacing to generate the virtual points.

Given below is an example of one of the

virtual spheres centered at the center of the dodecahedron.
Fig 3.12 - Front view of Dodecahedron

'V-A-lb

Fig 3.13 - Top view of Dodecahedron

26

27

Re-arranging Equation 4.1 and solving for focal length, a focal length of 381 pixels is obtained. For each of the virtual points on the sphere, corresponding observation image points

were generated for each camera, using the collinearity equations, and the parameters of interior
and exterior orientation given below:

CAM
1000
1001 1002 1003
1004

X
0.000 0.941 0.000 -0.941
-0.582 0.582

Y
0.000

Z
1.119 0.500 0.500 0.500 0.500
0.500

x(deg)
0.000 108.000 180.000

<p(deg)
0.000
0.000 0.000

ui(deg)
0.000

0.306 0.990 0.306
-0.801 -0.801 0.000 -0.306 -0.990 -0.306 0.801 0.801

64.000 64.000 64.000 64.000 64.000 180.000 -116.000 -116.000 -116.000 -116.000 -116.000

-108.000
-36.000 -36.000 0.000 -108.000 180.000
108.000

0.000
0.000

1005 1006

0.000
0.000 0.000

0.000
0.941 0.000 -0.941
-0.582 0.582

-1.119 -0.500
-0.500 -0.500 -0.500 -0.500

[\;>.n"\ A" \ 'Til f 

k^-vr\

1007

1008 1009
1010

0.000
0.000 0.000 0.000

36.000
-36.000

1011

Table 4.1 - List of cameras and associated position and orientation information

CAM
1000 1001
1002

F
380.000
380.000

xp

Yp
-2.000

k1

k2

k3
3.14E-21 3.14E-21 3.14E-21 3.14E-21
3.14E-21

P1
-5.63E-07
-5.63E-07 -5.63E-07

P2
2.45E-07

2.000 2.000 2.000 2.000
2.000

-6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08

2.16E-14

-2.000
-2.000

2.16E-14
2.16E-14 2.16E-14

2.45E-07
2.45E-07 2.45E-07

380.000 380.000 380.000
380.000 380.000

Fig 4.1 - Virtual sphere of points centered at centre of dodecahedron

1003
1004

-2.000 -2.000
-2.000 -2.000

-5.63E-07 -5.63E-07 -5.63E-07 -5.63E-07
-5.63E-07 -5.63E-07 -5.63E-07 -5.63E-07 -5.63E-07

2.16E-14
2.16E-14 2.16E-14

2.45E-07
2.45E-07

An assumption was made that the field of view of each camera was 80 degrees and that the format size of the CCD was 640x480. An estimated focal length for each camera was computed using the following equation (provided by Wolfram MathWorld): /ov = 2 tan
(4.1)

1005

2.000
2.000 2.000 2.000 2.000 2.000

3.14E-21
3.14E-21 3.14E-21 3.14E-21 3.14E-21

1006
1007 1008

2.45E-07 2.45E-07
2.45E-07 2.45E-07 2.45E-07 2.45E-07

380.000 380.000 380.000
380.000 380.000

-2.000
-2.000

2.16E-14 2.16E-14 2.16E-14

1009 1010 1011

-2.000 -2.000
-2.000

2.16E-14
2.16E-14

3.14E-21
3.14E-21

2.000

-6.11E-08

Where:

Table 4.2 - List of cameras and associated interior orientation information

Â·
Â·
Â·

fov = field of view = 80 degrees
D = size of the image sensor (in pixels) = 640 pixels
f = focal length

A test was required to ensure that the image points calculated fell within the format of the

camera in question. Below is an example of the image coordinate system utilized:

28

29

+240 lines

AeolC,
AslCtj

- pÂ» -1
ph ~l

Aeotj

Aeo] Cr7 "' Astj
i

Ai.eo] Ctj
i

Aiotj

T Ph~l

Aeo,.

TPh~l
A*ij V
1 ij

ph~l

dxf
dx]
(4.3)
1

y'

1 ij

-320 pixels

+320 pixels

Aiolf Aeo,.

AiolC^As^

ph-1

dx?_ Aiotj

,Mol C

AiolCJph^

1 ij

Equation 4.3 can be re-written as follows:
-240 lines

Neo,
Fig 4.2 - Example image coordinate system utilized

Neo

Neo s]
Neo  iof
Where: 

'sij

Neo Â· iot
Ns-iotj Nio,

~dxeÂ°
dx)
=

teo i

Ni
Nio

tSj

(4.4)

dx?

Jioi _

Any image points falling outside of the format size (i.e., -320px < x < +320 and -2401n < y < +2401n) were considered not to be a point for which the particular camera can see.

Neot contains coefficients of the image parameters


Once the observation points for each camera were determined, the self-calibrating bundle

Nsj contains coefficients of point coordinates Niot contains coefficients of camera calibration parameters

adjustment can be solved.

This adjustment is solved by first writing out the linearised



collinearity equations for each image i and each point j.



Neo Â· stj refers to both coefficients of image parameters and point coordinates

v, + Aeo dxeo + As dx + Aio dxi0 = Wph
2Jxl 2x6 ij 6x1 i 2x3 ij 3x1 j 2x8 ij 8x1*

2xl ij

(4.2)



Neo-iot

refers to both coefficients of image parameters

and camera calibration

parameters

Where the dxeo contains the corrections to the exterior orientation parameters of image /, dxs
6x1 3x1



Ns Â· iov refers to both coefficients of point coordinates and camera calibration parameters

contains the corrections to the coordinate of point / and dxÂ° contains the corrections to the * J 8x1
interior orientation parameters of image i, and Aeo, As, and Aio represent partial differential equations of the collinearity Equations 2.1 with respect to exterior orientation parameters,

The assumption is then made that errors in each of the point's image coordinates are

uncorrelated with any other image coordinates, including other points imaged on the same image

and other images of the same point.

However, there may be correlation error in x and y

object space coordinates, and interior orientation parameters, respectively. Wph represents the
misclosure vector.

coordinates of any single image point. When the assumption is made that the image coordinate
errors are uncorrelated, this implies that all systematic errors have been removed. The assumption stated above essentially means that the total covariance and weight
matrices which contain the covariance and the weight information for each and every image

The least-squares normal equations associated with one image of one point is then formed as follows:

coordinate, is block diagonal. Each of the blocks is a 2x2 matrix which correspond to one set of

image measurements for one of the points. If the measurements are independent of each other (as is assumed), and that each of the collinearity equations will refer to only one image and one

30

31

point, then the result is that one has the ability to add the contributions to the normal equations

saves processing time and storage space. This is because the other method of solving the self-

from each set of coUinearity equations. The following observations can thus be made regarding
the normal equations formation:

calibrating bundle adjustment would require the formation of the entire A matrix and then solving N by "brute force" multiplication. In real life situations, given a set of photos/images

and points, the entire A matrix will contain mainly zero values. This results in vast amounts of



The total Neo matrix is actually block diagonal.

Each of the 6x6 blocks refers to a

storage space being wasted on the storage of zero values.

This also results in vast amounts of

separate image, and each Neot is the sum of the Neotj sub-matrices, which are assembled from Aeotj and Ctj
to image i.
Ph -i

processing time wasted in multiplying values by zero when forming the N matrix.
The weight matrices corresponding to the exterior orientation parameters, the object

from each of the sets of coUinearity equations which are referenced

space/point parameters and the interior orientation (camera calibration) parameters are added to their particular sub-matrices (Neoi9 Nsj^Nio^ once per iteration. As well, the total corrections

Like the total Neo matrix, the total Ns matrix is also block diagonal with 3x3 blocks on the diagonals which are in reference to coordinates of a single point. Like the Neot, the
Ph
-i

to the original parameter observations, Weot, Wsj, Wiot, are multiplied by their corresponding
parameter covariance matrices (Ci
/

, Ci

, Ci

) and then subtracted from their respective teoi,

Ns  is assembled from the Astj and Ctj
referenced to point j.

from each of the sets of coUinearity equations

tSj, tioi vectors.

The results of the self-calibrating bundle adjustment are given below.



Again, similar to Neo and Ns, the total Nio matrix is also block diagonal with 8x8 blocks

on the diagonals which are in reference to the camera calibration cameras of a single camera. If all images are from a single camera, then the Mo matrix is just 8x8. Niot is
assembled from the Aiotj and Ctj
referenced to image i.
CAM
1000 1001
1002

X
0.000 0.941 0.000
-0.941 -0.582 0.582 0.000 0.941

Y
0.000
0.306

Z
1.119 0.500

K(deg)
0.000
108.007 179.996 -108.004
-35.992

<P(deg)
0.003
0.002 -0.002

w(deg)
-0.005 63.973

from each of the sets of coUinearity equations

0.990
0.306

0.500 0.500
0.500

64.056
63.999 64.007 64.008

1003
1004

0.000
0.002 -0.006

-0.801 -0.801
0.000



For each

Neo-stj

sub-matrix, it is the result of the multiplication between

Aeotj,

1005 1006
1007

0.500 -1.119
-0.500 -0.500 -0.500

35.999
0.000

0.000
-0.006

179.992 -116.008
-115.987

-0.306
-0.990

-108.006
180.004 108.001 35.997

Ctj

and Astj from one of the sets of coUinearity equations. If a point does not occur on

1008
1009

0.000 -0.941
-0.582 0.582

0.001 0.009
-0.005 0.009

-0.306
0.801

-116.008
-116.000

1010

-0.500 -0.500

one of the images, then the Neo  stj submatrix are zero.

1011

0.801

-35.979

-115.999



The Neo-io{ and Ns-io- sub-matrices are assembled in a similar fashion to that of
Neo Â· stj and for time sake, will be omitted from explanation.

Table 4.3 - List of cameras and associated position and orientation as determined by selfcalibrating bundle adjustment.

There are numerous advantages of solving the self-calibrating bundle adjustment problem by this method of normal equations formation. The main advantage is that this particular method

32

33

5. Direct and Indirect Georeferencing
CAM
1000

f
379.668
379.494

JSE.
1.973 1.998

IE.
-2.015

k1 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08 -6.11E-08

k2
2.16E-14
2.16E-14

k3

In order to obtain useful spatial information from the imagery captured by the spherical
-5.63E-07 -5.63E-07 2.45E-07 2.45E-07

3.14E-21 3.14E-21 3.14E-21 3.14E-21 3.14E-21 3.14E-21 3.14E-21
3.14E-21

camera, the imagery captured must be georeferenced.

The crucial requirement for the

1001
1002

-2.192

379.513
379.516

2.025
2.042

-1.964
-2.119
-1.995 -1.941

2.16E-14 2.16E-14 2.16E-14 2.16E-14 2.16E-14
2.16E-14

-5.63E-07
-5.63E-07

2.45E-07
2.45E-07

georeferencing process to take place is that the location of the perspective centre of each camera

1003 1004
1005

379.416
379.412

1.986
2.160

-5.63E-07
-5.63E-07

2.45E-07 2.45E-07
2.45E-07

of the spherical camera be known, as well as its' orientation. determination of these parameters:

Various methods exist for

indirect georeferencing and direct georeferencing. Each of

1006
1007

379.505 379.556
379.661

1.998
1.972

-2.058 -2.197
-1.990 -2.196
-1.994

-6.11E-08
-6.11E-08 -6.11E-08

-5.63E-07 -5.63E-07
-5.63E-07 -5.63E-07

2.45E-07 2.45E-07
2.45E-07

these methods will be discussed herein, and their potential for use in a mobile mapping system
will be investigated.

1008

2.024
2.137

2.16E-14 2.16E-14 2.16E-14 2.16E-14

3.14E-21 3.14E-21
3.14E-21

1009
1010

379.553
379.528

-6.11E-08
-6.11E-08 -6.11E-08

1.999

-5.63E-07
-5.63E-07

2.45E-07
2.45E-07

Table 4.4 - List of cameras and associated interior orientation parameters as determined by self-calibrating
bundle adjustment

1011

379.528

2.145

-1.993

3.14E-21

5.1 Indirect Georeferencing

As had been stated previously, in the past photogrammetric surveys required the establishment and presence of ground control points (GCPs). For aerial photogrammetric

The Normal Equations Structure is given in Appendix A.

surveys, for example, "Ground Control Points (GCPs) were the only required source of
information for providing the georeferencing parameters and suppressing undesirable error

propagation." (El Sheimy, 1996).

Ground Control Points are used as input to the bundle

adjustment to obtain the exterior orientation parameters for the cameras used in the survey. The
requirement for GCPs proved to be a costly and highly restrictive process; that is, it becomes

quite expensive to establish GCPs in the survey region, and in many cases it is not possible (or
rather, very difficult) to establish GCPs in certain inaccessible areas.

As part of the indirect georeferencing process, "the use of auxiliary position and

navigation sensor data" (El Sheimy, 1996) has been utilized in the past to assist in the
determination of the exterior orientation of the camera. Examples include the use of gyros,

horizon cameras and statoscopes (El Sheimy, 1996). However, even including this auxiliary data
still does not remove the need for GCPs. In the close-range photogrammetry case, the problem becomes even more demanding. Establishment of GCPs for a close-range photogrammetric survey (utilizing a system such as that

proposed in this research paper) is even more difficult and bothersome. The development of the Global Positioning System (GPS) helped ease this situation tremendously. The inclusion of a

GPS system in a photogrammetric survey (aerial or close-range) finally provided the means to

directly measure the exterior orientation parameters desired and thus could potentially eliminate

the need for GCPs. (Ackerman, 1995). However, limitations to the use of GCPs in a close-range

34

35

photogrammetry survey arise when surveys take place in urban areas, where signal loss can

6. Bore-sight and Lever Arm Determination
In order to take advantage of the availability of direct georeferencing in terrestrial mapping

frequently occur. When signal loss occurs, and GPS is the only sensor being utilized to provide positioning information, then the accuracy of the survey can plummet. Therefore, a different

applications, the determination of the rotational angles existing between the INS body frame and the camera body frame is required. In addition to this, the offset vector between the onboard

approach is required to georeference data collected in a close-range photogrammetry survey.

This approach was realized once the development of INS occurred.
process was finally possible.

A direct georeferencing

GPS and the camera perspective center must also be determined. The position of the camera perspective centre, as well as the camera orientation at exposure

time, are collectively known as the exterior orientation parameters. The interior geometry of the
5.2 Direct Georeferencing

camera sensor, as well as the lens distortion parameters are known as the interior orientation
parameters. The exterior orientation parameters are determined using an integrated GPS and INS approach. Interior orientation parameters are determined via laboratory calibration, as has been
previously discussed in earlier sections. "Georeferencing of images can be defined as the problem of transforming the 3-D

Direct georeferencing provides the means to obtain accurate positioning information

during the course of a photogrammetric survey, without the need for GCPs. This is particularly
advantageous for close-range photogrammetric surveys such as that discussed in this paper.

Direct Georeferencing of a photogrammetric survey can be realized by the inclusion of GPS and INS instruments onboard the survey platform. As was discussed in Section 1.2.4, the presence of
GPS and INS concurrently allow for the presence of signal lock to be more forgiving, as the INS
can provide trajectory information during the signal loss period, and thus provide very accurate

coordinate vector rc of the camera frame (c-frame) to the 3-D coordinate vector rm of the
mapping frame (m-frame)." (El-Sheimy, pg 53, 1996). For an integrated sensor system

consisting of a camera, GPS and INS, the determination of the coordinates of some object point appearing in the data imagery requires the determination of the coordinate vector which originates from the origin of the mapping frame to the center of the camera system must be
determined so that the vector corresponding to the location of the object point in the camera

positioning information for data collection sensors (such as the spherical camera) during GPS
signal loss in urban areas.

As noted by El Sheimy (1996) the notion of direct georeferencing "is conceptually

different from the notion that a block of connected images and sufficient ground control is
needed to solve the georeferencing problem", as was the case of the indirect georeferencing
method. Direct georeferencing is a much more flexible option for photogrammetric surveys as

frame can be transferred to the coordinate system corresponding to the mapping frame. Figure
6.1 (which was previously provided, from El-Sheimy, pg 57, 1996) details the relationship between the integrated GPS/INS system and the camera/sensor for a mobile mapping system.
This figure is reproduced below for reference:

imagery captured is not required to be connected (that is, overlapping). As was also alluded to in Section 1.2.4, direct georeferencing methods require the determination of the offset vectors
between the GPS unit and the imaging sensor. In addition to this, the orientation differences Orientation differences

between the INS and the camera system must also be determined.

between the INS body system and the camera body system are known as bore-sight angles.

Positional differences between the GPS body system and the camera body system are known as
lever arms. Both of these parameters will be discussed in the next section, and methods of

determining them will also be examined.

36

37

Â·
c - frame
INS b-frame

rc is the coordinate vector corresponding to the location of the target point in the camera
frame.

However, there is one significant problem associated with Equation 6.1. with the vector rTMc(t).
. r

The problem occurs

The equation assumes that the 3-D location of the camera perspective

centre can be directly measured.

However, this is not the case.

Instead, modifications to the

equation must be made to reflect the indirect determination of the vector rTMc (t). This is done by
Target point

determining the 3D location of the INS body frame, and then determining the relatively small

translations and rotations which exist between the INS body frame and the camera frame.
The vector which corresponds to the location of the INS body frame in the mapping

frame can be denoted as rTMs (t), the rotation matrix which rotates the body frame into the

mapping frame can be denoted as RTM (t) and the constant vector between the camera frame and

Fig 6.1 -

Detailed diagram of elements of georeferencing for the close-range case (El-

the INS body frame can be denoted as ab. With these new parameters, the coordinate vector
corresponding to the location of the camera perspective centre in the mapping frame can be re
written as follows:

Sheimy, pg 57,1996).

In Figure 6.1, the position of the GPS and INS have been consolidated into a single "unit." As

the vehicle is in motion, the location and orientation of the camera frame (c-frame) changes with
respect to the mapping frame (m-frame).
(1996) is given below:

Aside from these parameters, the rotation between the camera frame and the body frame must

The georeferencing equation, as given by El-Sheimy

also be determined.

In Figure 6.1, this is denoted by Rb, is considered a constant and is

determined by calibration. With this new rotation matrix, the rotation between the camera frame
and the mapping frame RTM(t) can be re-written as follows:

Where:
point

is the coordinate vector corresponding to the location of the target point in the

If the Equations 6.2 and 6.3 are substituted into Equation 6.1, then the following equation results for the determination of the vector coordinates of a point in the mapping frame. (From ElSheimy, pg 58, 1996).
rpoM --' INS /GPS ^ ^b \LJlA *^c '

mapping frame,

rTMc (t) is the coordinate vector corresponding to the location of the camera perspective
centre in the mapping frame,

rm

_

m

, J?m(f\\vl]?brc -\-nb~\
^ u

1

(6 4)
K^'^J

sl is a scale factor which is "specific to a one-point/one-camera combination which

This equation implies that the coordinate vector of an object point in the mapping frame

relates the image coordinates to the object coordinates." (El-Sheimy, pg 54, 1996),
RTM(t) is the rotation matrix which rotates the camera frame into the mapping frame,

can be determined by the real-time measurement of the vector r^s/GPS and rotation matrix

RTM(t), as well as two sets of parameters determined via calibration (Rb and ab) and a quantity
which can be determined in post mission (rc). The scale factor sl changes with distance from

38

39

the camera to the object of interest, and is determined via measurement. The parameters Rbc and
ab are known as bore-sight angles and lever arms, respectively.
quantities will be discussed herein.

Â·

RTM (t) is the rotational offsets between the body frame and the mapping frame (known quantity, measured by Inertial Measurement Unit component of INS)

Methods to determine these
Â·
Â·

Rhc is the bore-sight angles which are required (unknown)
rc is the vector of the object point in question, given in the camera frame.

6.1 Bore-sight Angle Determination

As mentioned in the previous section, the bore-sight angles are the small rotational

The observation equation is linearised for each GCP observation for each camera with

offsets existing between the INS body frame and the camera body frame, denoted in Figure 6.1
as Rbc. Two methods of determining the bore-sight angles will be discussed.
Method 1

respect to the unknown quantities (Rhc) to be estimated. A least-squares adjustment is performed
and the result gives the bore-sight angles for each camera in the dodecahedron array.
Method 2

In order to determine the bore-sight angles, a calibration which includes the presence of Ground Control Points is required. These can be acquired for a particular area which will be chosen for the calibration, using surveying techniques and/or GPS methods. Care should be

An alternative approach to the method given above to obtain the bore-sight angles

between each camera in the spherical camera body and the INS body frame can be obtained by
following Method 1 for one of the cameras (for simplicity sake, choose the top camera). Since
the relative orientation parameters existing between each camera in the dodecahedron array are already determined, only the bore-sight angles between the INS body frame and the camera on

taken to ensure that accurate GCPs are collected in the calibration area. Once the GCPs have
been established, they must be imaged using the mobile mapping platform. This can be done by driving the mobile mapping platform through the calibration area, and capturing images of the surrounding calibration field using the dodecahedron camera. Observed image points for each
GCP appearing in the images are then acquired directly from the images.

the top face of the dodecahedron must be determined, and then those bore-sight angles are

adjusted according to the relative orientation information previously obtained.

Although this

method is less intensive, it may not be as accurate as Method 1, and would be dependant on how accurately the relative orientation parameters related to the dodecahedron array were determined.

The bore-sight angles are then, "calculated by iterative least squares adjustments of the

linearised collinearity equation of the GCPs." (Holzwarth et al, 2005). The mapping equation
which maps the 2D image space points to the 3D mapping space points is given below, from
Zagajewski, et al.(2005), and modified to reflect the parameters in Figure 6.1:
point pc

6.2 Lever Arm Determination

After a calibration has been performed and a bundle adjustment has been performed to

obtain the exterior orientation parameters of each camera, the offset vector between the INS body frame and the cameras can be determined. From El-Sheimy (1996), the equations required

Where:

is me location of the GCP in the mapping frame (known quantity, from calibration
survey)

to retrieve the offset vectors are given below:
A INS ~Xci
aci ~
nb --
-y m

V m

rmc (t) is the location of the perspective center of the camera in question (known quantity,
determined from bundle adjustment)

XINS ^INS
V m
ym

ym

ym

1ci

(6.6)

ym

sl is the scale factor as previously defined

A INS ~ AGP5
aGPS ~
b _

Vm

T)b
m

XINS ~ IGPS

ym

ym

(6.7)

^INS

ym

ym

40

41

Where:

7. Image Mosaicking with a Spherical Camera
abci is the offset vector between each camera and the INS, expressed in the INS body
frame,

Â·

Often times during the course of a photogrammetric survey, it is not possible to

completely image an area under investigation with a single image.

Examples of this include

Â·

abGPS is the offset vector between the INS and GPS, expressed in the INS body frame,

agricultural surveys and geological surveys (in the aerial photogrammetric case), where the
region being surveyed is so large that the field of view of the image acquisition sensor is not adequate enough to capture the entire area in a single image. In cases such as these, it often

Â·
Â·

Rbm is the rotation matrix which rotates from the mapping frame to the body frame,
XTMNS, YTMs, ZfNS are the coordinates of the INS in the mapping frame,

becomes necessary to acquire multiple images of the survey region, and then fuse these images

Â·
Â·

XTM, FC7, Zc7 are the coordinates of the camera perspective center in the mapping frame,
XqPS , YTMPS, ZTMPS are the coordinates of the GPS in the mapping frame.

together to form one large image.

This large image is the equivalent image which would be

obtained by an imaging sensor with a much larger field of view. This image fusing process is known as image mosaicking, and is crucial if one intends to compose a completely immersive image using a spherical camera.
should contain the following:
Â· Geometric corrections

Again, this can be done to determine the offset vectors between the INS and each camera in the
spherical camera configuration. Similarly to the section on bore-sight angle determination, one

From Hall (1996), a typical image mosaicking application

could alternatively determine the offset vectors between the body frame and a single camera, and then propagate this offset vector according to the relative orientation parameters inherent to the
dodecahedron body.

Â·

Gray level corrections and seam elimination.

Geometric corrections are essentially a series of corrections or processes by which an image
point is mapped to a new location (Hall, 1996). In the case of the spherical camera, one must

view the world as a "spherical" world, where each point in the real world, appearing in the image, must be thought of as residing on the surface of a "sphere" in the real world, centered at the centre of the dodecahedron. This is because in close-range photogrammetry cases, it is often
very difficult to obtain "depth" information (i.e., how far away an object is from the camera). In the aerial photography case, depth information is readily available; however this is much more

difficult to obtain in close-range photogrammetric surveys. A proposed method for mosaicking
imagery captured by the spherical camera will be discussed in this section.

Aside from the mosaicking/geometric correction aspect, portions of the images which overlap with each other must be dealt with. Finally, a colour-balancing and seam line

elimination method must be employed. This is often required due to the fact that each camera's radiometric specifications may differ from one another's. Thus, if colour-balancing and seam

line elimination is not performed, the image mosaic may appear "patchy", and each scene of the
mosaic may be easily delineated from one another. This is an undesirable effect of image

mosaicking, and must be dealt with. Possible methods for colour-balancing imagery captured by the spherical camera will also be dealt with in this section.

42

43

The equation used to compute the coordinates of the edge points on the surface of the sphere is
7.1 Geometric Correction for Image Mosaicking
In order to mosaic imagery captured by the spherical camera to produce a 360 degree
given below:

panoramic image, the images are projected onto the surface of a sphere. In order to do this, the assumption that each camera in the dodecahedron array is centered at the same point (i.e., at the
center of the dodecahedron) is made. In reality, this is not the case; however this assumption can be made given the knowledge that in reality, the perspective centers of the cameras in the dodecahedron array are offset from each other by only a few centimeters. If a sphere is chosen to project our images onto in such a way so that the radius of this sphere is large enough to compensate for the error in making the assumption that each camera is centered at the same
point, then this error can be considered negligible.
Where:

Y

r-M ROT
SPH PC

y

(7.1)
img

Z

-f

X,Y,Z are the Cartesian coordinates of the image point projected onto the sphere
r is the radius of the sphere being projected onto

MR0T is the rotation matrix

Â·
Â·

x, y are the image coordinates in the image coordinate system
f is the focal length of the camera

The process for projecting an image onto the surface of a sphere is given below for one of
the cameras. The same process can be repeated for each image of the scene from each camera to
form the panoramic strip.

2) Once the coordinates of the edges of the image have been projected onto the sphere, they are

converted into spherical coordinates (^,/l,r) as follows:
(7.2)
(7.3)

1) For each pixel which comprises the image in question, the positions of the points located on
the surface of the sphere are computed. This is illustrated in Figure 7.1, where the camera is

centered at the center of the dodecahedron (which is also centered at the center of the sphere), and the corner coordinates of the image are projected onto the sphere.

R

(7.4)

3) Since the coordinates of the edges of the image, projected onto the sphere, are now available

in spherical coordinates, the information provided in Figure 7.2 can be used (provided by

Immersive Media).

Figure 7.2 depicts a mosaic generated using data from the Dodeca 2360.

Figure 7.3depicts a spherical coordinate system superimposed over the mosaic. Angles {d) are along the X axis and range from -180 degrees to + 180 degrees. Angles (X ) are along the Y axis
and range from -90 degrees to + 90 degrees.

Fig 7.1 - Projecting image onto surface of the sphere

44

45

90

-180

+180

Fig 7.2 - Mosaic generated by Immersive Media using data captured by Dodeca 2360
-90

-r+900

Fig 7.4 - Image edge points plotted onto Immersive Media mosaic grid

A minimum bounding box which completely encompasses the image plotted onto the immersive
media mosaic grid is formed. This is shown in Figure 7.5:

-180'

(0,0)

,

+90

+180'

Fig 7.3 - Super-imposed spherical coordinate system on mosaic

-180

+180

On Figure 7.3, the edge points of an image can be plotted (projected onto the sphere in spherical coordinates). This is shown in Figure 7.4:

-90

Fig 7.5 - Minimum Bounding box encompassing image on mosaic

4) For each pixel in the minimum bounding box, the corresponding spherical coordinates from

the Immersive Media image are converted back to Cartesian coordinates X,Y,Z according to the
following:

(7.5)

46

47

Y = rsinz?sin/l Z = rcos/l

(7.6)
(7.7)

possible method to colour-balance a mosaic generated from a spherical camera.
discuss the seam line elimination, as well as seam line smoothing.

It will also

7.2.1 Colour Balancing (Histogram Matching')

These points are then converted to image x, y using the collinearity equations and the exterior
orientation and interior orientation defined for the camera whose image is under investigation. A test is then performed to check if the image x, y computed falls within the image under

As made note of in the previous section, often times when creating a mosaic from various sets of imagery, the result is a patch-work like appearing mosaic, and "the creation of spurious artificial 'edges' at the seams between the pictures sections (tiles). These artificial edges occur when there are perceptible differences between the tiles within the region of overlap." (Milgram, 1975). In order to obtain a visually appealing mosaic, these visual differences between imagery
must be dealt with. In the case where imagery has been obtained via airborne or satellite

investigation.
omitted.

If it does, it is included in the mosaic in its respective position otherwise it is

This procedure can then be repeated for each image captured by the Dodeca 2360, to form the mosaic. One can then proceed to color balancing and seam line elimination techniques
to improve the visual quality of the mosaiced image. This will be discussed in the next section

methods, colour balancing can become even more bothersome due to atmospheric effects, clouds, snow, water, etc. all of which cause grey level differences between imagery in the mosaic. In the close range photogrammetry case (such as our spherical camera configuration),

7.2 Color Balancing and Seam Line Elimination

these effects are not an issue since the system is not imaging through the atmosphere. However, there will be numerous sharp terrain changes occurring all around the spherical camera, and as
such colour balancing is still necessary.

When mosaicking imagery, it is often the case that imagery acquired may have been acquired under different conditions (i.e., different day, time, season, year, etc). In the case of a

spherical camera, it may also be the case that one camera may view something that has a shadow, but from another camera's point of view that shadow is not present. When this occurs, the radiometric properties of a scene may differ from one image to another. This is dependent on when the imagery was acquired relative to the other imagery in the full "data set", as well as the various cameras which were used to acquire the data. When these radiometric differences are

For decades, researchers have worked extensively to develop good colour balancing
techniques. Among the various colour balancing methods in existence, one of the most

commonly utilized methods employed today is based on histogram matching techniques.
Milgram, 1975, provides a method which takes advantage of histogram matching to obtain a colour balanced mosaic. This method will be described herein, as the approach taken can be

not accounted for, the resulting mosaic is not "visually appealing" in that one can clearly

applied to a spherical camera setup. The first step in Milgram's colour balancing approach is to locate the overlap region for

perceive seam lines.

The over-all effect is that of a "patch-work" looking mosaic, where each

image in the mosaic can be clearly delineated from another. In some cases, the effect is not very noticeable. As Guindon, 1997 notes, "images of agricultural areas are comparatively simple to visually integrate primarily because such scenes are 'busy', i.e. they contain a patchwork of

the imagery under investigation.

Once the overlap region has been identified between two or As

more images, a grey-level histogram for the overlap region for each image is obtained.

Milgram notes, one would ideally hope that the histograms in the overlap region for each image

small areas of diverse reflectance, thereby making it difficult to visually follow an inter-scene
boundary." At the same time however, Guindon, 1997 makes mention of the fact that a mosaic of an agricultural area, especially when the images composing the mosaic are acquired during

would match. However, this is normally not the case due to differences in the imaging sensor, as
well as atmospheric effects, etc. To compensate for these differences, a zeroth order, or first

order adjustment method is employed that essentially shifts the histograms in such a way as to make their averages coincide. Although this method may help to balance the colour, tonal and

different growing seasons, contain very little information continuity across the entire scene. Therefore, in order to obtain a visually appealing mosaic, one must perform some colourbalancing / radiometric correction / seam line elimination techniques. This section will discuss a

contrast visual appearance of the imagery, it is still not perfect as the edges of the imagery will

48

49

still be visible. As such, Milgram notes, this step should be used as a pre-processing step to the
seam definition and smoothing step. 7.2.2 Seam Line Definition

mosaic image in which colour, tonal and contrast balancing has been achieved, and where seam
lines are effectively hidden:

Â·

For seam point s, let Ds = the minimum edge difference at seam point s

The seam line definition process, as outlined by Milgram, is meant to, "choose points in the overlap region which define where one tile ends and the other tile(s) begins." (Milgram,

Â·

H = --^-is then the average difference in grey level corresponding to overlap mismatch
existing between left and right sides of composite line.

1975). One method of doing this is to scan the overlap region, one "row" at a time, and choose
one point per line in the overlap region. In Milgram's algorithm, image information from to the left of the point in question comes from the line segment of the left hand tile, whereas the
information that comes to the right of the point will come from the right hand tile. From

Â·

"Smoothing at s involves apportioning H among the neighbours of s to the right and left
so that there is a gradual transition or ramp effect." (Milgram, 1975)

The slope of the ramp indicates how many of the neighbouring points are affected by the
smoothing process. If one wishes to hide the seam line in the best possible way, a "ramp" with a
more gradual slope can be applied. However, the trade-off here is that the information around

Milgram, 1975, the seam point for a single line is chosen as follows:
Â· Let Lp.^Z^be the grey level values of the current line in the left hand images' overlap
region

the seam point is "tainted" as the contrast level of these neighbouring points are altered. This method offers a suitable approach for hiding the seam line(s) in a mosaic. For a

Â·

Similarly, let Rl9...,RkbG the grey level values of the current line in the right hand
images' overlap region.

spherical camera setup, the same approach could be applied, and performed on a "sequential"
basis. This means that in the event that three images, for example, overlap the same area, then
one can perform the seam line identification and feathering methods for two of these images, and

Â· Â·

K = width of overlap region, in pixels A seam point for each line is then chosen in such a way as to minimize the presence of an

artificial edge. The edge measure, as given by Milgram, is reproduced below, where the sums of the differences are computed over w points.
where D} is minimal, j ranges from w/2 to K - w/2:
w/2

then use the results to identify the seam line between the results from the previous two images,
and the third image, and finally feathering. Such an approach can be repeated for all overlapping
areas in the imagery.

The goal is to identify the index j

D--

\ ^I t
i=-w/2+1

j?

I

n o\

7.3 Potential Problems with Approach

Although the methods of mosaicking and colour balancing presented in this section
appear to be sufficient, there are several shortcomings which need to be addressed. The first
The random positioning of the edge,

This seam point definition scheme results, "[in] a succession of points whose horizontal positions are unrelated to one another." (Milgram, 1975).

issue that arises occurs when the cameras of the dodecahedron array view an object in the scene from different angles whereby the amount of sunlight incident on the surface of the object and
recorded by the sensor varies with respect to the angle at which it is received by the sensor. In such instances, it may be possible that the same object appears to be a different object in a set of
overlapping images. If the histogram colour balancing technique described in the preceding

"is known as 'feathering', and help[s] to reduce the visual cues which would normally attract the
eye to the seam."
7.2.3 Seam Smoothing

After the seam points have been identified as outlined in Section 7.2.2, a smoothing

process is required so that any sudden changes in grey level values surrounding the seam point

section is applied, the differences in grey level between the two images in the overlapping regions may differ significantly enough that the correction applied to one of the overlapping
images may overcompensate for the effect.

are diminished.

The procedure given below, as outlined by Milgram, 1975, provides a final

50

51

Other issues which may crop up occur when an object appears in the overlap area of one

8. Conclusions and Future Work to be Done
This research paper focused on various methods and approaches one would need to
explore when considering the use of a spherical camera in mobile mapping applications, as opposed to the standard "multiple camera" approach. Such an approach would be very

image, but not the other. Such instances may occur when one camera captures the presence of a
person walking on the street, and that same person may not appear in one of the other images. When comparing the grey level differences between the two images in the overlapping region,
the problem becomes more apparent. In such cases, a special modification to the colour

beneficial for transportation departments, who routinely operate mobile data acquisition systems which rely on a series of cameras mounted on the vehicle. This type of system would be

balancing technique described previously will need to be employed so as to remove varying grey level pixels between the two images in the overlap region. Such algorithms which may be

beneficial in providing the respective departments with complete data coverage, something for
which traditional methods fail to realize. provide only part of the picture. than continuous coverage. The standard approach (multiple cameras) serves to

employed could be those used routinely in satellite and airphoto mosaics, where clouds may be
present in one image and not the other. An example of this technique can be viewed in Scholten,
1996, where Scholten explains a method to eliminate spurious pixels and obtain proper grey

Surveying techniques provide only point information, rather

Only through the use of a spherical camera can the "complete

level values for pixels in overlap areas using a weighting function. Guindon, 1997, also outlines
a method to deal with colour balancing in similar areas using grey level scattergrams.

picture" be efficiently formed. The combination of this system with existing sensors in use by
transportation departments creates a truly robust and complete data acquisition system.

A final issue which could arise occurs when considering the mis-registration between two

This paper demonstrated methods of developing a "math model" for a spherical camera such as the Dodeca 2360 by considering the geometric shape for which it is based on. Results of
this method were tested against various plotting software, and further confirmed through the use of a self-calibrating bundle adjustment. The bundle adjustment software developed not only

overlapping images in the scene. In the previous sections, the assumption was made that all of
the cameras in the dodecahedron array could be considered to be centered at the same point.
However, this is not the case, and as such this discrepancy will result in a mis-registration between the two images, making colour balancing and seam line elimination more difficult. One potential solution to this problem could be to down sample the images just enough so as to compensate for the mis-registration error. An example of this would be to take 4 pixels in the affected area, and make it one pixel in a "new" down-sampled image. Care would have to be

served this purpose, but also provided a means of determining interior and exterior orientation
information for such a camera system.

Future work required as outlined in this project includes the computer implementation of

the mosaicking and colour balancing techniques for use with the spherical camera proposed. The
ground work has essentially been laid for a software developer to program the mosaicking and

taken to ensure that the down-sampling process would not destroy the desired accuracy and
spatial resolution in the final product.

colour balancing techniques, in combination with the self-calibrating bundle adjustment program developed already, and the bore-sight and lever arm determination algorithms, to complete the
software package. In addition, further refinements to the mosaicking method to compensate for the assumption made of all cameras being located at the same point should be explored, as well as other methods to eliminate spurious grey level pixels attributed to sudden changes in
overlapping imagery should also be undertaken.

52

53

PROTECTIVE PARAMETERS
PHOTO NUMBER

point coord.-s

PHOTO NUMBER

FAKAH. FOE SELF-CALIB

APPENDIX A

NORMAL EQUATIONS STRUCTURE

SURVEYING ENGINEERING

THE UNIVERSITY OF CALGARY

54

55

9. References
Cramer, M. and Stallmann, D. (2001), "On the use of GPS/Inertial Exterior Orientation
Parameters in Airborne Photogrammetry", The Institute for Photogrammetry,
University of Stuttgart.

Milgram, D.L. (1975), "Computer Methods for Creating Photomosaics", IEEE
Transactions on Computers, Nov. 1975, Volume C-24, Issue 11, Pages 1113-1119

Scholten, F. (1996), "Automated Generation of Coloured Orthoimages and Image
Mosaics Using HRSC and WAOSS Image Data of the MARS96 Mission",

Ebadi, H. (1997), "GPS Assisted Aerial Triangulation", PhD Thesis, Department of
Geomatics Engineering, University of Calgary, Canada.

International Archives of Photogrammetry and Remote Sensing. 1996. Vol.
XXXI, Part B2, pages 351 - 356

El-Sheimy, N. (1996), "The Development of VIS AT - A Mobile Survey System for GIS Applications", PhD Thesis, Department of Geomatics Engineering, University of
Calgary, Canada.

Schwarz, P. and El-Sheimy, N. (2004) "Mobile Mapping Systems - State of the Art and
Future Trends", Department of Geomatics Engineering, University of Calgary,
Canada.

Guindon, B. (1997), "Assessing the Radiometric Fidelity of High Resolution Satellite
Image Mosaics", ISPRS Journal of Photogrammetry and Remote Sensing, 1997,
Vol 52, pages 229 - 243

Wiedemann, A. More, J. and Tauch, R. (2003), "Archimedes3D - An Integrated
System for the Generation of Architectural Orthoimages" FPK Ingenieur GmbH,
Feurigstr 54, 10827 Berlin, Germany.

Guindon, B., et al, (2003), "From Need to Product: A Methodology for Completing a
Land Cover Map of Canada with Landsat Data", Canada Centre for Remote Sensing, Canadian Forest Service, University of Lethbridge, Pacific Forestry
Centre. Preprint edition.

Wolf, P. and Dewitt, B. (2000), "Elements of Photogrammetry with Applications in

GIS", 3rd edition, USA: McGraw-Hill, ISBN 0-07-292454-3

Hall, R.W. and Gumustekin, S. (1996), "Mosaic Image Generation on a Flattened

Gaussian Sphere", Proceedings 3rd IEEE Workshop on Applications of Computer Vision,
Dec 2-4, 1996. Pages 50-55

Holzwarth, S., Muller, R. and Simon, C. (2005), "Determination and Monitoring of
Boresight Misalignment Angles During the HYMAP Campaigns HYEUROPE

2003 and HYEUROPE 2004", Proceedings of 4th EARSel Workshop on Imaging
Spectroscopy, pages 91 - 100.

56

57


