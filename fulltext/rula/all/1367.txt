Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2009

The New Public Management and Post-Secondary Institutions: the Evolution of the Accountability Regime of Ontario Universities
Sabita Ramlal
Ryerson University, sabita_ramlal@hotmail.com

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Higher Education Administration Commons, Public Administration Commons, and the Public Policy Commons Recommended Citation
Ramlal, Sabita, "The New Public Management and Post-Secondary Institutions: the Evolution of the Accountability Regime of Ontario Universities" (2009). Theses and dissertations. Paper 977.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

THE NEW PUBLIC MANAGEMENT AND POST-SECONDARY INSTITUTIONS: THE EVOLUTION OF THE ACCOUNTABILITY REGIME OF ONTARIO UNIVERSITIES

By Sabita Ramlal BSc, University of the West Indies, 1990, Trinidad MA, Institute of Social Studies, 1996, The Hague. A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Arts in the Program of Public Policy and Administration Toronto, Ontario, Canada, 2009

© Sabita Ramlal 2009

AUTHOR'S DECLARATION

I hereby declare that I am the sole author of this thesis or dissertation. I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

Signature

I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Signature

111

iv

ABSTRACT

An Abstract of the thesis of Sabita Ramlal for the Master of Arts in Public Policy and Administration at Ryerson University, presented August 27, 2009. Title: The New Public Management and Post-Secondary Institutions: The Evolution of the Accountability Regime of Ontario Universities. The adoption of New Public Management (NPM) principles, practices and values in the Ontario public sector was one phase in the evolution of the accountability regime for universities and the post-secondary education (PSE) sector since the early 1990s. New Public Management advocates the adoption of private sector management principles and practices in the public service that re-defines citizens as customers, and emphasizes efficiency and effectiveness. It also places a high value on transparency and accountability to government. The research found that the evolution of the accountability regime for universities in Ontario was significantly influenced by NPM which has meant growing demands for universities to demonstrate value for money invested by government through institutional and system level performance measures and results. The findings in the comparative analysis of public policy and administrative reform of higher education in other jurisdictions show that the impact of NPM was even greater in the jurisdictions reviewed than in Ontario. The information and insights gathered from key informants in the PSE sector support the central argument and demonstrate the differing views on changes in the accountability regime and the challenges facing the PSE sector as a result of NPM-influenced reforms. The research also reveals a multiplicity of NPM-inspired accountability mechanisms: financial compliance focusing on administrative issues; voluntary accountability initiatives; and government-mandated performance measurement in terms of achievement of government's policy goals of increased quality, access, and accountability. The changing nature of the accountability regime demonstrated an effort by the government to assert itself over autonomous entities over which it had very little control since the 19th century. It was concluded that the changes made by government in the accountability regime while not

v

dramatic but incremental has had some impact in terms of holding universities more accountable. This incremental change in the accountability regime over several governments resulted in a disconnected web of accountability mechanisms which was not always effective. The paper concludes that there is a need for a rationalisation of the accountability framework that provides accountability to government and taxpayers without the costly burden of meaningless performance measurement and reporting.

vi

ACKNOWLEDGEMENTS

I wish to thank my supervisors Prof. Carolyn Johns and Prof. Patrice Dutil for their guidance in completing the thesis. I also wish to thank all the interviewees who volunteered to share their knowledge and experience gathered over many years in the post-secondary education sector. I would also like to thank Aaron and Kush for their support in writing this thesis.

vii

viii

TABLE OF CONTENTS AUTHOR'S DECLARATION .................................................................................................................iii  ABSTRACT................................................................................................................................................. v  ACKNOWLEDGEMENTS .....................................................................................................................vii  LIST OF FIGURES ...................................................................................................................................xi  LIST OF APPENDICES .........................................................................................................................xiii  INTRODUCTION....................................................................................................................................... 1  METHODOLOGY ..................................................................................................................................... 4  CHAPTER 1: THEORETICAL AND COMPARATIVE CONTEXT .................................................. 8  NPM and Accountability .......................................................................................................................... 8  NPM and Public Policy in Higher Education ­ A comparative overview .............................................. 11  CHAPTER 2: THE BEGINNINGS OF UNIVERSITY ACCOUNTABILITY IN ONTARIO TO THE 1980s ................................................................................................................................................. 27  1972 Wright Commission on PSE ...................................................................................................... 30  1981 Report of the Committee on the Future Role of Universities in Ontario ................................... 31  1984 Commission on the Future Development of the Universities .................................................... 32  Accountability through Legislation .................................................................................................... 32  Reporting to the Minister on enrolment and grants ............................................................................ 33  Influence of the Provincial-Auditor In Increasing Accountability...................................................... 34  Self-regulation in Academia and Quality Assurance of Programs ..................................................... 35  Voluntary Accountability Measures ­ the Council of Finance Officers (COFO) Report................... 36  CHAPTER 3: THE IMPACT OF NPM - ACCOUNTABILITY MEASURES, 1990 to 2003........... 37  The Rae Government and Accountability........................................................................................... 38  A concern with self-regulation in quality assurance ........................................................................... 38  PSE Review: Report of the Task Force on Advanced Training, 1993................................................ 38  ix

PSE Review: Taskforce on University Accountability and the Broadhurst Report, 1993.................. 39  OCUA Advisory Memorandum on Resource Allocation for Ontario Universities, 1995 .................. 40  The Harris/Eves Government's approach to university accountability and NPM .............................. 42  Increased self-regulation in quality assurance for university programs.............................................. 45  PSE Review: Report of the Smith Advisory Panel, 1996 ................................................................... 46  Performance-based funding and Key Performance Indicators (KPIs) ................................................ 47  The Rising Influence of the Auditor-General - a Proponent of NPM practices.................................. 50  New Accountability Legislation ......................................................................................................... 53  CHAPTER 4: THE AGE OF MULTIPLE ACCOUNTABILITY DISORDER, POST-2003 ........... 55  The 2005 Rae Review on PSE and its impact on expanding NPM in the accountability regime....... 56  Legislative Changes ............................................................................................................................ 57  Aligning the university agenda with government policy goals........................................................... 58  1)  Policy Goal: Improving Accountability ...................................................................................... 59  2)  Policy goal: improving access to university education ............................................................... 64  3)  Policy goal: improving quality in university education .............................................................. 66  Academic accountability for quality in Teaching and Learning ......................................................... 67  Self-regulation and accountability for quality assurance and system review ..................................... 70  Role of the Higher Education Quality Council (HEQCO) in Quality and Accountability ................. 73  4) Policy Goal: Value-for-Money through Research and commercialization..................................... 76  5) Value-for-Money through administrative efficiencies in universities ............................................ 82  The Rise of Parliamentary Officers in Administrative Accountability............................................... 85  Voluntary Accountability Initiatives and Reporting or more MAD-ness? ......................................... 86  CONCLUSION AND PROSPECTS FOR CHANGES IN THE ACCOUNTABILITY REGIME... 92  APPENDICES ......................................................................................................................................... 102  BIBLIOGRAPHY ................................................................................................................................... 116  ENDNOTES ............................................................................................................................................ 126  x

LIST OF FIGURES

Figure 1: Total Full-time Equivalent Enrolments, 1979-80 to 2008-09

................................ 32

Figure 2: University Grants per FTE, 1979-80 to 2008-09

.......................................... 45

xi

xii

LIST OF APPENDICES

Appendix 1: Core Interview Questions .................................................................. 107 Appendix 2: Key Accountability Actors for Ontario Universities 2008 ........................... 111

Appendix 3: Accountability Mechanisms for universities (government, or legally mandated, self regulatory and voluntary ............................................................................ 113 Appendix 4: Reporting Requirements to Ministry of Training, Colleges and Universities ........ 115 Appendix 5: List of Acronyms ............................................................................ 117

xiii

xiv

INTRODUCTION The adoption of New Public Management (NPM) principles, practices and values in the public sector in Ontario significantly influenced the evolution of the accountability regime for universities and the postsecondary education (PSE) sector. Accountability mechanisms increased in an incremental fashion from very little accountability during the early years of publicly funded universities to increasing accountability mechanisms from the 1990s to the present. This thesis examines the evolution of accountability of universities in Ontario with a particular focus on the influence of New Public Management (NPM) including the emergence of new accountability goals, language, requirements and new players in the accountability regime. The accountability regime was examined in the context of three themes which have been highlighted over the years in the accountability debate in higher education related to the rise of NPM: (i) monitoring quality (quality assurance and quality in teaching and learning); (ii) research & commercialization, and (iii) administrative efficiency. The first two themes focused on accountability of academia and the latter on the financial/legal accountability of the administration. The research also looked at the broad range of accountability mechanisms, including legislated, government-mandated, and voluntary initiatives.

Given that higher education falls under provincial jurisdiction under the Canadian constitution, this study focused on the provincial government of Ontario but the role of the federal government, which has its own sphere of influence through research funding was also reviewed. Universities are part of the broader public sector in Ontario but are autonomous, independent institutions, each established by an Act of the provincial legislature except for Queen's University which was established by royal charter prior to Confederation. Each university has its own Board of Governors and is directly accountable to the legislature through annual financial reporting. This institutional autonomy has traditionally created a tension with the idea of accountability to the government. A university, however, is not only externally accountable to the legislature and government, but also to other stakeholders, internally to student and employees' groups and externally to donors, the community and society. The federal government and private donors provide funds for research, private donors also provide donations for endowments, and students provide some funding through tuition fees. Over the years, as indicated in the various PSE reviews, the challenge for government has been to ensure that a university's institutional goals align with government policy goals, and operating within the government's established accountability framework.

1

Both a former provincial politician and a former government bureaucrat both noted that establishing an accountability framework "is an evolutionary process." As the universities' functions and enrolments expanded along with government funding, so did the demands for greater accountability from government. The evidence reveals that the demand for greater accountability had been a constant with every Government over the last few decades. The `form' that accountability mechanisms took, however, was shaped by government's introduction of NPM-influenced policy from the 1990s. This influence of NPM accountability in Ontario was consistent with the experience of other Organisation for Economic Co-operation and Development (OECD) jurisdictions such as New Zealand, the U.K, the USA and Australia.

Interviewees who were all key stakeholders in the Ontario university sector were asked their views on the accountability regime as it evolved over time and what changes they would like to see in the future. Their insights helped to reinforce or clarify the findings of the documentary analysis. (See Appendix 1 for a list of questions posed to interviewees.) They put forward arguments that demonstrated either their support or rejection of current NPM-influenced accountability mechanisms in Ontario, in varying degrees. Government, university officials on both the administrative and academic sides, student representatives and other PSE experts all agreed that universities have to be accountable but there was also disagreement over the degree and form that accountability mechanisms should take. The government focused on summative accountability (focusing on point in time performance measures) and other stakeholders focused on what can be called formative accountability (peer assessment and improvement), as being more appropriate for universities.

Summative and formative assessments are used in education to assess student progress. Assessment is used to refer to all activities related to gathering information to gauge progress in meeting specific targets. In a broader sense, these concepts can be applied to an accountability framework for universities. There is a focus on measures and numbers in summative accountability - financial accountability (debt levels, the state of endowments, annual financial statements) and performance measures at the institutional-level. Summative assessments are done periodically to determine progress at a particular point in time as an accountability measure. Key performance indicators that aim to judge university performance through measures such as graduation rate and graduate employment rate at a particular point in time are summative. NPM focuses on the summative form of accountability which emphasizes measurement and quantitative numbers as the way of assessing outcomes. The information obtained from this type of assessment is important to help evaluate university improvement goals but are very limiting. Formative accountability includes the Senate/academic community who are responsible for progress indicators at the 2

program-level. For learning and teaching, in particular, as Garrison and Ehringhaus put it, " because they are spread out and occur after instruction every few weeks, months, or once a year, summative assessments are tools to help evaluate the effectiveness of programs, school improvement goals, alignment of curriculum, or student placement in specific programs. Summative assessments happen too far down the learning path to provide information at the classroom level and to make instructional adjustments and interventions during the learning process. It takes formative assessment to accomplish this."

A balanced accountability framework comprises both summative and formative assessments. Targets for improvement can be both formative and summative. When information is collected in a formative way it has to be shared and used to inform the process for improvement. When a comprehensive accountability regime balances formative and summative achievements, there is a clearer idea of whether targets and standards are being met. On the teaching and learning side of the university, for instance, as more is learned about how students engage in the learning process, instruction can be adjusted to ensure that all students continue to achieve by moving forward in their learning.

Since the 1960s, accountability requirements for universities have continuously increased as the demand for a university education expanded and enrolments increased. As the philosophy of New Public Management (NPM) took hold in the Ontario public sector in the 1990s, performance or results-based management became part of the accountability landscape, when government demanded greater effectiveness and efficiency from all public sector entities. In 2005 there was a new development triggered by the release of the report of the Rae Review on PSE. The McGuinty government adopted some of the report's recommendations related to funding and improving accountability for the PSE sector. For the first time, the Government committed multi-year funding to the PSE sector (universities, colleges and training centers) which was a significant investment after a period of declining investment in PSE. Along with this investment came new accountability reporting. The current government of Ontario, like many other OECD governments, believed that investment in PSE was a key component in economic development in today's knowledge economy. The number and quality of PSE graduates was seen by policy-makers as the way to improve innovation and competitiveness in the new global economy. Government had to decide how to divide its budget among competing policies and had to justify its expenditures to a greater extent. The new accountability demands clearly reflected NPM focusing on results, value-for-money and performance measurement in Ontario stem from a recognition of the importance of the university sector in economic development, as well as the need to justify its billion

3

dollar investments in the PSE sector. This was a shift from viewing PSE as a social good and even a luxury to which few had access, to an economic good which should be accessible to all qualified students who would form the basis of the new knowledge economy.

Although some of the Rae Report recommendations were adopted post-2005, there was not a significant change to PSE legislation or regulations. All the same reporting requirements remained in place, however, new performance contracts were introduced for individual institutions which put more attention on academic accountability. The research found that over time the accountability framework for universities had become more complex and multi-faceted. The thesis demonstrates that there has been a gradual managerial accountability creep on universities with the government making in-roads on the traditional autonomy of universities. The influence of NPM philosophy and practice lingers today in the language and mechanisms being used for accountability. Based on findings, the thesis concludes with some analysis of the possibility of implementing a more coherent, consolidated accountability framework which takes into account all government-mandated requirements and voluntary initiatives that universities undertake, without creating a bureaucratic, costly burden for institutions, where performance reporting becomes an end in itself; an exercise that may not impact on strategic directions or the management of universities.

METHODOLOGY Research on public policy and accountability in PSE is a slowly emerging area of research in Ontario and Canada compared to other jurisdictions which have more sophisticated accountability frameworks. The research is both interesting and important to public policy and administration for a variety of reasons. A 1981 review on PSE in Canada by Sheffield for the Social Sciences and Humanities Research Council (SSHRC) and the Canadian Society for the Study of Higher Education (CSSHE) 1 , found that there was scholarly research being conducted on PSE, mainly in the Faculties of Education but also in faculties of economics, sociology, psychology, history, government departments and agencies. It also found most of the academic researchers conducted PSE research as an area of secondary research. It was also found that more long-term studies including inter-provincial and those of a national scope were needed on PSE including research on how higher education institutions were responding to changes in their social and economic environment in terms of their university mission and their response to the needs of government and citizens. This research will contribute to the body of research on public policy and higher education institutions in Ontario and Canada. 4

Research questions The main questions the research asks are, 1. To what degree and to what extent did NPM philosophy impact accountability mechanisms for PSE, specifically universities, in Ontario? 2. How has the accountability regime of Ontario universities evolved over time? 3. To what extent are existing accountability measures for universities voluntary or government mandated? 4. What has been the impact of the Rae Review on the accountability framework for Ontario universities? 5. To what degree, and in what form, are universities responding to calls for greater accountability to government? 6. Is there room for improvement of the existing accountability framework? 7. Are universities more accountable today? 8. What should a consolidated accountability regime look like in the future?

Data Collection and Analysis Procedures Documentary research and secondary literature review Through documentary research and review of literature of both published and unpublished materials from government, non-profits and academia, policy trends regarding external accountability of universities that have been implemented by government were tracked. Document analysis included government PSE sector reviews as they related to universities. While the focus was external accountability the influence of internal accountability within the institutions on external accountability was acknowledged. This literature review included a comparative analysis of accountability mechanisms in other jurisdictions that have been impacted by NPM, which put the state of accountability in context and allowed an assessment of the degree to which Ontario universities were influenced by NPM. Voluntary accountability mechanisms implemented by the university sector (formative accountability) were also reviewed. The insight

5

accumulated from the secondary data collection on Ontario and other jurisdictions informed the interview process.

Primary research - Key Informant Interviews Interviews were conducted to reinforce the findings in the literature review as well as to fill in the gaps missing in the literature. Primary data were collected through interviews with eleven (11) key informants who worked in the PSE sector or had previously been involved in PSE policy in Ontario. Interviewees comprised experts on university issues including a former provincial politician (current federal politician), two Ministry of Training, Colleges & Universities officials, a Higher Education Quality Council (HEQCO) official, a Canadian Federation of Students-Ontario student representative, an Ontario Confederation of University Faculties Association (OCUFA) representative, a COU official, a VicePresident ­ Academic, a Head of University Planning, a university Vice-President - Finance & Administration, a university Vice-President - Research. Audio taping was used when interviewing and the interviews were fully transcribed. The interviews were in person ranging from ½ hour to1 ½ hours. Interviewees were asked some core open-ended questions and then specific questions related to the insight they might have based on their particular position. See Appendix 1 for core questions administered to interviewees. The Research Ethics Board required that the interviews be anonymous so there was no attribution from the interviews.

The questions sought to obtain information about the opinions and attitudes towards the changing accountability regime and the impact of NPM to supplement and expand on the findings in the literature. The interviewees were of a high calibre and a rich source of information and their insights helped to reinforce or clarify the findings of the documentary analysis. Quotations were selected from interviewees where they expressed a strong opinion and articulated well their view on government policy and approaches to accountability especially NPM measures. Interviewees were not expected to know about NPM as a philosophy so they were not directly asked about NPM instead the researcher looked for references to the language of NPM in the responses of interviewees.

Participant Observation As the researcher worked in the university sector for the Council of Ontario Universities during the data collection period of the thesis, this provided inside access to information through meetings and conferences for the thesis, as well as facilitated access to interviewees. Permission was granted from the researcher's former employer to utilize unpublished but public information that would be useful.

6

The strategy for analysis of information from the documentary research, interviews and observation was to look for classic NPM terms such as efficiency and effectiveness, results-based management, performance measurement and reporting, outputs and outcomes, value-for-money, performance indicators, and accountability for results, to assess how much NPM values had become part of the accountability landscape.

7

CHAPTER 1: THEORETICAL AND COMPARATIVE CONTEXT

NPM and Accountability Accountability has various meanings, not always consistent, in the literature but the paper focuses on administrative accountability and refers sometimes to the issue of political accountability. Accountability has been defined as "the process whereby those to whom authority has been conferred or delegated and /or responsibilities assigned must justify, explain or defend their actions (or those of their subordinates) to a superior authority that has the obligation to hold account all those on whom it has bestowed authority and responsibilities"[Aucoin and Jarvis, 2005, 91]. Paul Thomas defined accountability as "...where an authoritative relationship exists, processes for reviewing performance operate, there is a recognized obligation to take corrective action when untoward events occur and there is the potential for penalties and rewards to be applied depending on whether the outcomes achieved fell below, matched, or exceeded agreed upon expectations" [Thomas, 2008, 258]. A former provincial politician who was interviewed noted, "the principles of accountability and transparency are hallmarks of public administration."

The calls for greater accountability in higher education are related to the introduction of what has been called New Public Management (NPM) reforms in government in many OECD countries which gave rise to the `audit' or `evaluative' State. NPM advocates a particular definition or type of accountability which emphasizes summative accountability. The major concepts that differed from the traditional bureaucracy were the expectation for public servants to exercise more control and discretion in decision-making; contracting-out of services; and measuring performance through outputs and outcomes. NPM advocated for the adoption of private sector management principles and practices in the public service that redefined citizens as customers, value for money reporting, emphasized quality and performance management with a focus on public reporting, and accountability for results. As Saint-Martin puts it, NPM is "a loose term used as shorthand for the set of ideas and techniques imported from business management that governments have increasingly used since the 1980s to reform the public sector" [SaintDenis, 2004, 2]. Performance management (outcomes or results-based management) is a private sector concept, borrowed by NPM, which involves a process of assessing progress toward achieving predetermined goals, establishing criteria for measuring if those goals had been achieved, then reviewing issues with regard to inputs, processes, outputs and outcomes to improve achievement of organizational goals. Key concepts in performance management are: 8

· · ·

Efficiency which is related to minimizing costs and maximizing the use of resources to create specific results to meet established goals; Effectiveness which means the results achieved are meaningful and are meeting the university's objectives as well as the government's policy objectives; and, Performance measures and indicators which demonstrate results.

Performance (results-based or outcomes-based) planning and management involved four steps: a. Determining outcomes or results that government wants to achieve; b. Determining the inputs, processes and outputs required to achieve these outcomes; c. Conducting an evaluation using performance indicators and reporting on success in achieving outcomes; and, d. Using the evaluation in a continuous improvement process to improve results.

This approach to public policy and administration and the accountability model associated with it "are embedded in broader ideological mechanisms associated with the political-economic transition from welfare state to global capitalism" [Shanahan, 2009, 7]. NPM was implemented in different degrees in the 1980s and 1990s in the United Kingdom, New Zealand, Australia, United States of America and to a lesser extent, Canada. These jurisdictions traditionally influence Canadian public policy and their accountability mechanisms for universities will be discussed later. Managerial forms of accountability were introduced in the public service in light of a growing pessimism that Parliament (Congress in the case of the USA) was unable to exert its power to make government and the bureaucracy accountable to the public. The old Weberian bureaucratic model of public administration epitomized by the British bureaucracy was challenged through the rise of neo-liberal economics in the West and the NPM movement. NPM had its origins in different disciplines which resulted in an "an ambivalence in its approach to accountability"[Thomas, 2008, 265]. The different influences included:  Public choice theory which views the bureaucracy with suspicion and the need for politicians to assert control over self-interested bureaucrats;  From business management literature came ideas of management independence from interfering politicians and a focus on citizens as `customers' who have more service options;  Accountability is viewed as a goal in its own right "to prevent the abuse of power and to ensure fairness, and as a means to promote efficiency and effectiveness in government, as well as to assure the public that they are receiving value for money" [Thomas, 2008, 266].

9

As Thomas noted, "NPM brought new perspectives and practices into the perennial debates over accountability, but it fell far short of resolving the so-called accountability problem" [Thomas, 2008, 266]. This definition clearly linked accountability to performance management and was most suited for the purposes of this paper, which explored if and how universities had been affected by NPM to meet accountability requirements. The limitations of the NPM approach to accountability in higher education are also recognised. The political accountability structure (legislature representing the public) and the administrative accountability structure of the government of the day which together form the major part of the accountability framework within which universities operate. Other stakeholders that hold universities accountable are students who pay tuition fees and donors (including other governments), and a more diffuse accountability to internal stakeholders (faculty, unions) and other external agents such as accreditation bodies. NPM put the focus on accountability to the `customers' (in this case students, parents, taxpayers) whereas a traditional view focuses on political accountability to Parliament. See Appendix 2 for an outline of the accountability relationships and actors involved for universities.

Thomas, among others, supported the view that in cases where political accountability through Parliament broke down, managerial accountability based upon NPM thinking became more relevant. The research suggested managerial or administrative accountability was seen as the mechanism for government to gain some control over autonomous and independent entities such as universities through performance measurement, which revealed the contradiction in NPM, as one of its goals was to allow more control to public servants in decision-making. As Thomas noted, "Cynicism about politics and pessimism about Parliament's capacity to enforce accountability have contributed to the introduction of numerous managerial forms of accountability based upon NPM ideas" [Thomas, 2008, 270]. Thomas and others view managerial accountability derived from NPM thinking as an approach that could "supplement and complement ministerial responsibility but it is not an appropriate or complete substitute for the traditional model of accountability based upon responsible ministers being forced to either boast or to confess before the public's elected representatives in Parliament" [Thomas, 2008, 254]. Thomas also used the concept of `multiple accountabilities disorder' (MAD) by which he meant that when faced with multiple and sometimes inconsistent accountability requirements, the lines became blurred as to who was really accountable as well as weakening organizational performance by having a negative impact on efficiency and effectiveness.

The thesis focused on managerial or administrative accountability of universities as it relates to NPM. NPM-influenced accountability strategies in this thesis include performance management procedures setting targets; measuring performance through performance indicators; establishing new reporting 10

structures and government agents/agencies to monitor and advise on university performance; demonstrate results through performance indicators that are not only inputs and outputs-based but also outcomes-based to show a return on investment or value for money; demands to increase efficiency and effectiveness; and demands to provide more student-focused service. These strategies are also defined as being summative in nature as it focuses on measuring, ranking and make judgments based on numbers.

NPM and Public Policy in Higher Education ­ A comparative overview
The trajectory of expanding accountability influenced by NPM in higher education in Canada and Ontario was influenced by developments in other jurisdictions from the late 1970s, 1980s, and 1990s. This section provides a comparative context of recent accountability developments in higher education. The USA, UK, New Zealand and Australia were included in this review as these were the countries that first undertook changes in the accountability of universities, heavily influenced by the philosophy of NPM. These countries especially the USA and UK have always influenced government policy developments in Canada and Ontario, and it will be shown that there was some influence and lessons from these jurisdictions in terms of what policies could be useful and what should be avoided given their negative impact on the university sector.

Some in academia saw global competition as the reason behind the focus of governments on educational systems as producers of human capital for the new economy and "outcomes-based education, and educational standardization in the name of the market and economic competitiveness crept into discussions of educational accountability" [Shanahan, 2009, 6]. In 1969, the OECD established a program for Institutional Management in Higher Education (IMHE), to promote research, information exchange, training and professionalism in the management of higher education institutions (HEIs). At the 1981 OECD Intergovernmental Conference, there was concern about a "crisis of higher education" involving both "public confidence in their performance" and "an internal crisis of purpose" [OECD, 1987]. In the 1980s, the OECD noted: ... universities in OECD countries today confront common problems which derive from a single central fact: they are being called upon to play an ever more important part in the restructuring and growth of increasingly knowledge-based national economies, at the same time as they are under pressure from cuts in public spending, demographic downturn, diminished legitimacy, and the consequences of rapid growth in the 1960's and 1970's... [OECD, 1983].

11

The economic and social environment of OECD countries in the 1980s generated talks about resource constraints and demographic change impacting universities. With the rise of China and India threatening their industrial capacity to produce low-value added products, OECD countries felt pressured to expand their knowledge-based capital as a means of economic competition and thus were placing more demands on public universities, even in a situation of steady-state or decreased government funding for universities. Universities in OECD countries were being asked to provide more accountability through performance reporting in terms of quality in teaching and learning; research and commercialization; and administrative efficiency. Universities were expected to take on a much broader role aimed at greater collaboration and alignment with their government's economic, social, and human capital goals.

While in the last few decades, university education was seen as a means to promote social mobility, the recent demands for greater accountability from governments now had a more economic focus. The shift from a Fordist to post-Fordist business paradigm, from manufacturing to the services industry requiring PSE knowledge, resulted in a focus on investing in PSE as a key to global economic competitiveness. Governments, understandably, were asking questions about whether their investments in higher education were paying off, given the competition for public funds among various policy issues - education, health, foreign policy, social policy, industrial development, national security, etc. In the knowledge economy, post-secondary graduates were more important to economic growth, hence the pressure on the State to bridge the distance between the government bureaucracy and the `ivory towers' of universities. More jobs now required higher education, as low-skilled jobs disappeared. Governments also viewed university research output as a driver of innovation and competitiveness. Narayan [2008] referred to commercialization as the "third mission" of many universities.The demands for universities to play a stronger role in the economy was therefore the driving force behind the calls for greater accountability and value for money for government investments.

Burke and Minassians noted that while NPM ideas were developed and promoted by researchers in academia, it was in relation to business and other sectors, not to universities. The adoption of NPM initiatives by governments, however, affected higher education operations and processes. Speaking in the American context they noted: Severing the connection with external accountability would also cut off a major impetus for institutional improvement. Along with most institutions, colleges and universities have a poor record of reforming themselves without external pressure. In a knowledge-and-information age when state success depends on the results 12

from higher education, accountability and improvement become one and inseparable [Burke and Minassians, 2002, 31]. As mentioned above, the mechanism for greater accountability under NPM was performance management. There were three types of performance-based policy government can put in place in higher education according to Burke and Minassians: i. Performance reporting ­ publishes the `results' of state/provincial higher education systems, sectors and campuses. ii. Performance budgeting ­ state/provincial governments and coordinating bodies for higher education or university systems consider `performance' in funding allocations to institutions. iii. Performance funding ­ ties limited sums of money directly to institutional results.

They noted the purpose of all three initiatives was to have universities demonstrate external accountability, improve institutional performance, and respond to government's policy agenda especially regarding economic development. One key aspect of performance management was developing a means of measuring performance using indicators shifting away from inputs and processes to outputs and outcomes. Inputs-based performance indicators of institutional performance had traditionally included enrolment rates and entering grades of students and outputs-based indicators traditionally included graduation rates and employment rates. Outcomes-based measures included measures such as learning outcomes, quality of student experience and engagement indicators - student-faculty interaction, active and collaborative learning, educational experience, academic challenge, and support services to students. A distinction is sometimes made between performance measures and performance indicators where with the former, "the numerical indicator is a precise or robust measure of the factor of interest (e.g. student numbers, building condition, financial outcomes)" whereas performance indicators (teaching quality, student satisfaction, or sustainability) do not purport to measure anything. [Committee of University Chairmen, 2006, 12] Performance measures seem to be the focus of NPM's summative approach to accountability whereas performance indicators could be more reflective of formative accountability which involves ongoing assessments for continuous improvement. However, the term is sometimes used interchangeably to mean both. The following section looks briefly at higher education performance models and accountability in specific jurisdictions ­ the USA, England, Australia, New Zealand, and Canada, to inform the discussion of university accountability in Ontario.

13

NPM and accountability in Higher Education in the USA In the USA, hundreds of billions of dollars are spent on PSE each year, more than any other country with the federal government contributing more than one-third of the total investment including grants and contracts for research and development, facilities, equipment, fellowships, etc. The USA has a more coordinated approach to higher education accountability at the state and national level than Canada. The US Government Accountability Office (GAO) conducts research at the request of Congress on issues such as university funding, commercialization efforts, etc. According to the GAO's website, under a cross-servicing arrangement, the Department of Health, Education, and Welfare (HEW) is responsible for auditing federal funds provided to most of the institutions and for providing the audit results of these to funding agencies. Publicly funded universities in the USA receive operating funds from state governments who are responsible for the higher education system. The federal government is not involved in regulation of universities because ach state is responsible for implementing and managing the higher education system. The USA was one of the first countries in the 1960s and 1970s to attempt to monitor performance of publicly funded higher education institutions given the huge investments of taxpayers funds. The USA avidly adopted NPM in the public sector, supported by the "movements to reinvent government and re-engineer business" [Burke and Minassians, 2002, 5]. NPM was also reflected in their approach to higher education. In the 1990s, when NPM philosophy reigned, the trend was `accountability for results' as opposed to the traditional `accountability for expenditures', which coincided with the rapid development of information technology. The calls for greater accountability in higher education came during the time of a national recession in the 1990s and reduced funding from government after a period of sustained increases.

Performance budgeting, performance funding and performance reporting were implemented by different states in varying degrees. Performance reporting became the most popular form of demonstrating accountability to state and federal governments, as it seemed to be the least costly to both governments in terms of tying funding to performance and for institutions, in terms of effort, time and financials risk. Burke and Minnassians noted that "Performance reporting promises that publicizing the results of higher education at the state, system, and campus levels on priority indicators can enhance external accountability improve institutional performance, further state needs, and possibly even increase state funding" [Burke and Minassians, 2002, 1]. Since 2000, biennial `Measuring Up' report cards were produced voluntarily by each state and a national report card for higher education was also produced, coordinated by the National Center for Public Policy and Higher Education, an independent nonprofit organization. According to the `Measuring Up' website 2 , the report cards served "to provide the general

14

public and policymakers with information they can use to assess and improve postsecondary education in each state." There are four performance categories for grading each state's performance in higher education - Preparation, Participation, Completion, and Benefits for which grades are calculated by comparing each state's current performance to that of the best-performing states. This comparison provided a basis for evaluating each state's performance within a national context and encourages each state to "measure up" to the highest-performing states.

In 2006, the US federal government released the Report of the Spellings Commission, "A Test of Leadership: Charting the Future of U.S. Higher Education", outlined the concerns of government and its expectations for higher education regarding quality of entrants, access, cost and affordability, financial aid, quality of learning, transparency and accountability, and innovation. According to Basken ,"Colleges, as a group, had limited enthusiasm for the secretary's efforts" [2008]. While state and federal governments had been putting money into the higher education sector, the Spellings Report indicated they were not getting the results that they wanted. Nothing much was done to implement the report's recommendations and Spellings publicly expressed disappointment with the lack of political response to the report. Even though US universities were highly advanced in performance reporting compared to other jurisdictions, the Spellings report noted that transparency and accountability for measuring institutional performance was inadequate. Specifically, the report called for colleges and universities to move away from measuring institutional quality in terms of financial inputs and resources and to provide better data for policymakers to assess the payoff for investing in higher education including comparable evidence on student learning outcomes; comparable data on benchmarks of institutional success (student access, retention, learning and success, educational costs, productivity); data on non-traditional students (part-timers, transfers) that was not being captured; and a transformation of accreditation that did not focus on process but bottom-line results for learning or costs.

Business leaders had also expressed concern that colleges "are still not paying enough attention to meeting the nation's economic needs" [Basken, 2008, 2]. Thus in the USA, there was a strong push for universities to respond to the country's economic needs but for some in academia, the business of a university should be not be exclusively about producing graduates who meet the workforce needs of corporations since students could also be of societal benefit working in government and the non-profit sector, helping to build social capital. Burke and Minassians came to a conclusion about conflicting developments in the US PSE system which seems applicable to other jurisdictions such as Canada:

15

First the attitude of state policymakers toward higher education shifted from seeing it as a private benefit for graduates to a public benefit for states. At the same time, they reduced state funding for public higher education and raised the cost to students [Burke and Minassians, 19]. Burke and Minassians also noted that, as government funding decreased, universities were moving from government-funded to government-assisted, a situation that was already the case in Ontario, as we shall see later.

NPM and Accountability in Higher Education in the United Kingdom The UK was one of the first jurisdictions to embrace neo-liberalism and NPM. The UK has a national system of higher education but with separate funding councils for England, Wales, and Scotland overseeing higher education. There are also seven Research Councils for the UK which operate independently of government, from which universities get their research funding. Except for the older institutions such as Oxford and Cambridge, fundraising is not a major activity of UK universities and most of their funding comes from government. The advent of Thatcherism in the 1980s brought NPM into the UK public sector with accountability requirements that shaped higher education in no small measure. Performance reporting and funding was introduced where universities had to report on various performance indicators. The focus was on measuring outputs which linked back to funding.

The Higher Education Funding Council for England (HEFCE) has a statutory responsibility for quality in teaching, which it oversees through teaching quality assessments done by an external Quality Assurance Agency (QAA). The QAA does quality assurance for the entire UK and is a member of the European Association for Quality Assurance in Higher Education (ENQA). ENQA provides quality assurance guidance to EU post-secondary education, which promotes credit recognition of graduates and mobility of labour. In 1997, the Dearing Report, a series of reports on "Higher Education in the Learning Society", was presented to government with 93 recommendations on funding, research, and teaching quality and standards. The report was the result of the 14 month work of a National Committee of Inquiry into Higher Education, whose aim was to investigate how to create a society that is committed to learning throughout life over the following two decades. This report had a major impact on higher education policy and practice in the UK. A national white paper on the future of higher education in England was released in 2003, which looked at issues of fair access, expansion, exchanges and knowledge development with industry, research excellence, teaching and learning excellence and funding.

16

The controversial Research Assessment Exercise (RAE) in the UK provides a peer review exercise to evaluate the quality of research in higher education institutions, measuring research outputs and based on quality ratings, funding from the Research Councils are determined for each institution. The RAE 3 was introduced in 1989 was "the principal means by which institutions assure themselves of the quality of the research undertaken in the HE sector." It introduced an explicit and formalized assessment process of the quality of research and further exercises were held in 1989, 1992 and 1996 which were more comprehensive and systematic, and another RAE was done in 2008 but with some modifications. Shattock notes that "the RAE is not, as national research assessments are in some other countries, an exercise associated with quality assurance carrying reputational consequences only" but "a resource allocation device" for grants. The rigor with which the UK holds its researchers accountable seems to be having some positive effects, from the political perspective, in terms of improvements in productivity and research outcomes. In April 2002, the UK's House of Commons Science and Technology Select Committee published a report that noted,

The RAE has had positive effects: it has stimulated universities into managing their research and has ensured that funds have been targeted at areas of research excellence. But it also stands accused of distorting research practice, ruining academic careers and contributing to the closure of university departments. It has imposed a huge administrative burden on those assessed. The RAE in its present form is generally agreed to have had its day [Select Committee on Science and Technology Second Report, 2001, 1].

HEFCE notes that "a review of research policy and funding revealed that on many measures the work done by
university researchers in this country is among the best in the world." On the other hand, it created an

environment of competition and poaching for research experts, which continue to disadvantage smaller institutions and polarize research and non-research institutions. The RAE has been severely criticized by academics in the UK and elsewhere including Ontario. Critics noted that the RAE created a competitive culture where the larger universities benefited more from such a process of performance management, measurement and rewards, and the smaller universities were the losers. As the Ontario Confederation of University Faculty Associations (OCUFA) put it, the initiative created "a self-fulfilling prophecy: universities that score well get more funding and continue to score well or even improve. Others fall further behind" [OCUFA, 2006, 2]. Critics referred to RAE as " a routine operation of surveillance and assessment dependent on coercion and consent" and "expressed bitter resentment about the inordinate administrative requirements necessary to comply with performance models, and strongly objected to the amount of time taken away from academic work" [Atkinson-Grosjean, 2000, 14]. 4 17

There has been much criticism of accountability requirements in England and the UK where it was felt there was too much detailed assessment from the central government. Scotland and Wales started making changes to their higher education system which differs from England, as the local government gained more political independence from England. There have been ongoing review and adjustments of accountability requirements of the PSE sector. For example, the Higher Education Funding Council of England (HEFCE) implemented a new accountability framework in 2008 and released a new report on a review of quality assurance. The QAA was also under review to maintain its membership in the European body and the RAE exercise will be revamped after the 2008 exercise. A new Research Excellence Framework is planned to succeed the RAE and "will be much more metrics based, will be more heavily steered by government and less likely to reward excellence wherever it is found" [Shattock, 2009].

NPM and Accountability in Higher Education in New Zealand New Zealand has a national system of higher education or tertiary education as it is called. In the 1990s, New Zealand avidly implemented NPM strategies in its public sector, based on the changes happening in the UK. Universities have many accountability requirements including legislation and regulations and performance reviews involving many Key Performance Indicators (KPIs). They are accountable under the Public Finance Act 1990 and must have sound management and report by outputs. In November 1998, the New Zealand government released a white paper which called for " a high-performing tertiary sector." The Tertiary Education Commission (TEC) provided funding to tertiary education institutions and was the lead agency for managing relationships with the tertiary sector and for policy development. The (TEC) was implementing a new way of planning, funding and monitoring tertiary education in conjunction with the Ministry of Education and the New Zealand Qualifications Authority. There was a standardised regime through the Tertiary Advisory Monitoring Unit (TAMU), whereby the TEC also monitored the financial, leadership and governance performance of all thirty-one tertiary education institutions. Regular performance reviews were mandated to ensure efficiency objectives were met and these involved the extensive use of performance indicators. Under the Education Standards Acts 2001, the University Council had to operate "in a financially responsible manner that ensures the efficient use of resources and maintains the institution's long term viability".

Universities are required to provide reports to different agencies including an annual Statement of Service Performance, indicating how objectives have been met which was reviewed by the Government's Audit Office. There was a lot of research being done with private sector funding which was making 18

accountability and reporting relationships more complex and demanding. According to Narayan, commercialization of universities posed a new challenge to New Zealand's public sector accountability model, as funding from non-government sources increased, creating new accountability relationships. He noted that New Zealand's existing accountability mechanisms were not designed for collaborative research programs and recommended that "... annual reporting should be extended to incorporate `networked' accountability mechanisms reflecting a much broader understanding of the commercial and economic benefits of research commercialization" [Narayan, 2008, 2].

The Crown, through the TAMU, could intervene and take over a university's operations if it did not meet the fiscal performance requirements of the government. Institutions could not run a deficit or government could take over running of the institution and if there was a surplus, government would claw back funding. Academic staff were graded and given a score on a scale of "0 to 4". Research funds were distributed based on performance with each university receiving a research score. Research funders paid full overheads, all direct costs plus 104% of salaries. Operating funding was provided on a competitive basis based on enrolment unlike Ontario where enrolment growth is managed so that the division of the enrolment pie tends to be similar every year taking into account individual full-time equivalents (FTEs) growth. Academics in New Zealand have been critical of the performance measures and accountability requirements for the higher education sector in New Zealand.

NPM and Accountability in Higher Education in Australia Australia also adopted NPM initiatives in government with the "managing for results" philosophy, with a focus on value-for-money, outputs and outcomes as (opposed to inputs and process), client-orientation, and performance measurement and evaluation adopted as a means of accountability. Australia as a federation, has adopted a national funding and accountability framework. All 37 public and the two private universities receive federal (or Commonwealth) funding under the Higher Education Support Act (HESA) 2003 and other legislation. Universities are self-governing within a regulatory and reporting framework that includes state/territory and local governments, with all but one university established under state/territory legislation. All universities are self-accrediting providers of education established by or under Commonwealth, State or Territory legislation and responsible for maintaining the quality of their own academic standards. Quality assurance in the higher education system is based on a partnership between the Commonwealth (federal), State and Territory Governments and the higher education sector. The Australian Universities Quality Agency (AUQA), established in 2000, is responsible for auditing the quality of Australian universities and the Commonwealth, State and Territory authorities responsible for

19

accrediting universities. Its audit reports are publicly available on the AUQA website. The National Protocols for Higher Education Approval Processes were approved in 2000 to ensure consistent criteria and standards across Australia in the recognition of new universities, the operation of overseas higher education institutions in Australia, and the accreditation of higher education courses to be offered by non self-accrediting providers. The Australian Qualifications Framework (AQF), established in 1995, provide for nationally recognised pathways between awards offered in Australia's vocational education and training and higher education sectors, bringing together the qualifications issued by different sectors into a single comprehensive system of titles and standards. The AFQ also maintains a public register of "post compulsory education providers and accreditation authorities" which is a key part of the Australian higher education quality assurance framework. Universities produce performance reports and are peerevaluated on processes and outcomes which result in public reports and rankings. Some performance funding is provided based on these rankings.

The government of Australia was in the process of a major review of higher education in 2008, the first undertaken since 2002. The purpose of the review, according to the government's website, was to consider the issues and challenges facing Australia's higher education sector, to inform the Government's policy agenda for higher education for the next two years, and to develop a long-term vision for higher education for the future. The discussion paper highlighted the government's concern with globalization and the contribution of higher education to social and economic prosperity including its role in innovation. The paper foresaw the development of the higher education sector as "an industry in its own right" which is a policy direction many countries are now taking including Canada. The attraction of international students is a major source of revenue for universities and creates a competitive climate. In 2005, Australia had the highest percentage of international students among OECD countries. The government reported that higher education was "the largest services export and third largest export sector" [Discussion paper, 4]. Universities increased their share of revenue from non-government sources through foreign students and the creation of private sector-type operations or what in Canada are referred to as `ancillaries'. Like other countries, accountability requirements increased while government funding went down.

NPM and Accountability in Higher Education in Canada and the provinces The concerns about `accountability' of government in Canada are not new. It has been noted that "the Legislature is the proper forum for public accountability" [Ontario Financial Review Commission, 1995, 1], but in practice, the legislature was not able to hold the government fully accountable as the public 20

service and broader public sector (BPS) became more complex, expanding with various types of agencies, and along with this came the growing power of the Executive. In the Canadian context, parliamentary accountability was where Parliament held Ministers accountable through such mechanisms as `question period'. Political accountability entailed the electorate holding the government accountable through elections. Administrative or performance accountability was where Ministers held the Deputy Minister accountable for the conduct of the business of the public sector and this type of accountability was utilised to hold arms length entities in the BPS, such as universities, accountable. Some of the philosophy of NPM can be seen in the results-based accountability initiatives of the federal and provincial governments particularly related to research and student funding.

The Canadian constitution, (Part VI, Sec 93) assigns legislative responsibility for non-Aboriginal education to the provinces. This resulted in limiting the ability of the federal government to directly influence university education. Provinces provide the majority of funding to universities for base operating and capital projects. The federal government has a long history of involvement with universities through its research granting agencies and student funding such as the Canada Student Loan program. Although the federal government reduced funding to universities in the form of transfers to provinces as part of deficit reduction cuts in 1995, from 1997-2007 it reinvested more than $11 billion in universities and intervened in provincial jurisdiction through a `boutique' of programs such as the Millennium Scholarship Fund and Canada Research Chairs program under its research and innovation agenda related to the knowledge economy [Morgan 2008, 179]. During this period, the primary vehicle it used to do this was the Canada Health and Social Transfer (CHST). In 2004 transfer payments to the provinces were split out as the Canada Health Transfer (CHT) and Canada Social Transfer (CST), primarily to promote accountability for increased transfers for health care spending. In addition to these direct and indirect transfers to universities, the federal government also funds capital and infrastructure projects such as the recent federal matching of provincial capital funds earmarked for university projects as part of an economic stimulus strategy, given the recession of 2008-09.

Canada ranks among the top four OECD countries in tertiary education investment as a percentage of GDP, keeping fairly steady according to the latest OECD data, 2.1% in 1995; 2.3% in 2000, and 2.6% in 2005. In 2005, Canada ranked 3rd behind, Switzerland (2nd) and the USA (1st), for annual expenditure on tertiary educational institutions per FTE student for all services. However, between 2000 and 2005, total spending on tertiary education in Canada increased at a slower pace (17%), lower than the increase observed in the OECD on average (30%) [OECD, 2008, Table B1.5]. Universities in Canada are 21

generally seen as having a good record in terms of participation, access and quality. However, the Canada Council on Learning (CCL) noted in its 2007 report, that "Canada's federal, provincial and territorial governments invested $36 billion in post-secondary education in 2006-2007. Despite this significant expenditure, there are no pan-Canadian goals or objectives for the sector, or ways to assess how effectively this money is invested" [CCL, 2007, 9]. In other words, the PSE sector needed to justify the investment of taxpayer dollars and the contribution to the economy, in order to maintain or increase financial support. Performance management practices such as performance measurement and evaluation were supposed to provide evidence to government and tax-payers of value for money.

The federal government provides funding for university research through its Tri-council agencies ­ Social Sciences and Humanities Research Council (SSHERC), National Science and Engineering Research Canada (NSERC), and Canadian Institutes of Health Research (CIHR). There is no coordinated national policy on PSE nor is there a national accountability framework, whereas other countries have set national goals for PSE, whether or not PSE was devolved within its federation such as the USA, UK and New Zealand. The Canadian Council on Learning (CCL) has set as one of its goals, doing an annual national report to assess progress and performance in PSE. Because of the structure of PSE administration, the CCL noted, "Canada lacks mechanisms at the national level to ensure coherence, coordination and effectiveness on key priorities, such as quality, access, mobility and responsiveness" [CCL, 2006, 4]. While participation rates increased dramatically in university enrolment since the 1940's, there was room for further improvement especially expanding enrolment in STEM disciplines (Science, Technology, Engineering and Mathematics), in post-graduate studies, and among disadvantaged groups ­ low income, Aboriginal Peoples, and persons with disabilities.

One former provincial politician interviewed for this study noted "when you read about what other countries are doing and the investments they are making, it seems that Canada at the federal level really needs to wake up". He noted in the USA, the federal government takes a much greater interest in higher education issues. He felt it was a myth that the federal government did not have any role in PSE since it transfers funds through the federal transfers, funds research through the granting councils, funds student aid through Canada Student Loan and the Millennium Scholarship Fund and has had a strong relationship with the universities over the last sixty years. He pointed out the "strange anomalies in funding" in Canada and in the governance systems with dramatically different levels of tuition. There was reference to the fact that Quebec allowed students from France to study at the same level of tuitions as students from Quebec but students from other Canadian provinces did not have that right. A university

22

institutional research & planning representative interviewed for the study felt that while "there should be more involvement, just in terms of the strategic importance to the country of post-secondary education", given the constitutional issue of the provinces having responsibility for education, it would be difficult for the federal government to get involved in direct funding of PSE and coordinating higher education issues on a national level. There is no accountability mechanism from one level of government to the next for specific purpose funding for transfer payments.

Research, accountability and NPM Since the 1950s, the federal government has had a direct funding role in university research. In Canada, universities and hospital research institutes perform a large proportion of the total research effort compared to other G7 countries, since the private sector in Canada conducts less research than other OECD countries. Given the role of science and technology in the knowledge-based economy, research and innovation leading to commercialization gained a new prominence in discussions around research in higher education. The idea of commercializing university research has been around for a while, since the establishment of the federal Centres of Excellence in the 1980s, "when a consensus emerged among government, industry, and universities that knowledge and highly qualified people are two main components in achieving economic growth" [Bell, 1996, 1]. A former provincial government official interviewed for this research, noted that government interest in research and commercialization tended to be sporadic over the years. The federal government provides the major part of funding and coordinating of PSE research through the tri-Council funding agencies. These agencies cover direct costs of sponsored research, some graduate fellowships, and some overheads. Canada seems to be an anomaly in not funding the salary component of funding research as is done in other countries such as the USA and New Zealand. A university institutional planner interviewed for this research, noted that federal funding of salaries of principal investigators on research projects would be helpful but indicated that the cultural issues in Canada that have been built around collective agreements and the 12-month academic year for faculty salaries would make it problematic to implement. The Indirect Costs Program of the federal government, started in 2001, helps fund some hidden costs of research projects funded by the Tri-council agencies. 5 . The secretariat of the Canada Research Chairs program, housed at SSHRC, administers the Indirect Costs program and is responsible for conducting performance measurement, evaluation and audits of grant recipients of indirect costs funding and for reporting on the program to the Minister of Industry, Treasury Board Secretariat and, ultimately, Parliament.

23

Both the provincial and federal governments have indicated an interest in greater commercialization of research since the link has been made between the level of research activity and economic development such as happened with Stanford University and Silicon Valley. A 2001 paper from the Association of Universities and Colleges of Canada (AUCC), which represents universities at the national level, noted that "Canadian universities believe that with the support of the private sector and the federal government, they can triple the direct results of committed of commercialization over a ten-year period" [AUCC, 2001, 3]. The federal government adopted this target in `Canada's Innovation Strategy'. Technological innovation and development is seen as a key to productivity and competitiveness of the economy which explains the pressure being put on universities to focus on commercial research. Commercialization is "the process that seeks to transform new knowledge and technology into new economically successful products, processes or services or to add value to established ones..." [OCUR, 2005, 2]. Provincial universities are not held to a strict accountability mechanism for research funding such as exists in other jurisdictions like New Zealand and the U.K. A university representative from Ontario noted that "the RAE in the UK has gone completely overboard" and is counterproductive and suggested Ontario would want to avoid a similar situation.

A key accountability mechanism in PSE, a national accreditation system does not exist in Canada outside of the professional accrediting bodies, whereas the USA has specialised regional accreditation and national accrediting bodies. A university representative noted that "a big challenge in Canada is because we have 10 different provincial systems and AUCC refuses to be considered an accrediting body". CCL noted, "Canada does not have a comprehensive and coherent approach to quality assurance that has the confidence of all PSE stakeholders" [CCL, 2008/09, 11]. It is therefore difficult for other countries to assess the Canadian university system regarding quality assurance. The AUCC website attempts to put together information from all the provinces in lieu of a national system but the reality is there are different rules in the quality assurance processes in each province, which a university representative referred to as "a bit of a mish mash."

Other provinces In 1999, the Council of Ministers of Education (CMEC) noted that PSE institutions were accountable for their performance in 6 areas: quality; accessibility; mobility and portability; relevance and responsiveness; research and scholarship; and accountability [CMEC, 1999, 6]. For accountability, in particular, they noted "PSE institutions and governments are openly accountable to the public in relation to mandates and outcomes and for reassuring citizens, and students in particular, that resources are

24

allocated to achieve maximum value and sustainability of postsecondary education" [CMEC, 7]. Government policy on accountability for universities varies across provinces. British Columbia (BC), Alberta, Manitoba, Ontario and Quebec governments have established their own performance indicators. The use of NPM terms such as inputs, outputs, outcomes, results-based performance, performance measurement and indicators, efficiency and effectiveness were not traditionally associated with education as a public good and with universities, but they have become part of the language being used by provincial governments in demanding greater accountability from large transfer payment recipients such as schools, colleges, universities and hospitals. At the same time, more universities, individually and at a system level, are voluntarily self-assessing their performance through performance management indicators. There have been recent voluntary initiatives by universities to collaborate on performance indicators across Canada. This may be the start of movement towards national reporting which the CCL recommended, and in which the federal government may be interested, but the politics of PSE is challenging outside the research domain and would require cooperation of all provinces, territories, the federal government and national NGOS such as the Association of University and Colleges (AUCC).

There are government-mandated performance management requirements in other provinces. The BC government requires universities to prepare three-year institutional accountability plans. The BC Ministry of Advanced Education sets university specific, multi-year performance targets but funding is not linked to key performance indicators (KPIs). Alberta universities have to submit to the provincial government, 4year business plans outlining goals, objectives, performance indicators and targets. Government has mandated five performance indicators linked to a performance funding envelopes. Universities in Manitoba are not required to submit annual plans but Manitoba Advanced Education and Literacy requires institutions to report on two indicators ­ student outcomes and costing. Quebec's Commission on Education requires the submission of various graduation data, persistence, and financial indicators as well budget and enrolment plans, but no requirement exists to submit annual plans and there are no standard provincial objectives. Conférence des recteurs et des principaux des universités du Québec (CREPUQ) and the universities are discussing the use of a common set of indicators called "EDUCQ".

NPM-influenced policy measures in the USA and in Commonwealth countries have and continue to influence public policy for PSE institutions in Ontario. The review of other jurisdictions' accountability requirements puts in context the state of accountability of universities in Ontario. Interviews and the literature review, both revealed that, while NPM helped shape the accountability framework of Ontario's universities, Ontario has not gone as far as other jurisdictions in adopting NPM mechanisms such as widespread performance measurement and reporting at the system and institutional levels as well 25

allocation of resources by government based on performance. The accountability mechanisms in the UK, New Zealand, Australia and the USA, resulted in increases in government spending to maintain an accountability bureaucracy, more competition among universities and faculty, an expansion of private sector-funded research (excluding the UK which still depends heavily on government for research funding), and attempted to measure the quality of the education and the value of research to the economy based on established metrics. NPM shaped the form of accountability in these jurisdictions in a way that was sometimes detrimental to collegiality in the case of the RAE exercise in the UK. The theoretical grounding of NPM which came out of the private sector and the focus on value for money and measurement did not always show itself to be appropriate. While administrative efficiency and quality assurance may have improved, measuring teaching and learning, and research outcomes seemed to ignore the longer term impact that graduates and basic research have on society, which is beyond auditing or measurement. NPM therefore has its limitations for application to universities but this has not stopped governments from trying to find ways to measure outcomes for its large investments in PSE.

The next three chapters describe the evolution of the accountability regime for universities over three time periods: pre-1990s before the rise of NPM philosophy; 1990 to 2003 ­ the critical years of NPM and experimentation under the Rae NDP government and the Harris/Eves Conservative government; and 2003 and beyond when the McGuinty Liberal government came into power and the Rae review on PSE was appointed. It will be shown that different aspects of the external accountability regime ­ legislative, financial, and administrative ­ evolve in each of the time periods, depending on the changes in government regimes and public policy and NPM becomes more significant from the 1990s.

26

CHAPTER 2: THE BEGINNINGS OF UNIVERSITY ACCOUNTABILITY IN ONTARIO TO THE 1980s Ontario has the largest university system in Canada including 19 universities ­ Algoma, Brock, Carleton, Guelph, Lakehead, Laurentian, McMaster, Nipissing, Ottawa, Queen's, Ryerson, Trent, Toronto, Waterloo, Western, Windsor, Wilfrid Laurier, York, UOIT, and the Ontario College of Art & Design, and various affiliated colleges. However, given that they are dependent on public funding, "their freedom of action in practice is less a question of legal status than of what conditions, if any, are attached to the receipt of public funds" [Cutt, 1990, 33]. Funding is determined at a system level based on an enrolmentdriven formula. Universities present some interesting challenges in determining the appropriate balance between accountability and autonomy. As universities are part of the broader public sector (BPS), and are accountable under their individual Acts directly to Parliament, they are autonomous institutions.

A few years after the establishment of Upper Canada, crown land was set aside as an endowment for grammar schools and a provincial university. Accountability of post-secondary institutions was not a major policy issue for government in the first 100 years after confederation when a post-secondary education was still seen as a privilege of the elite. From the early 19th century until the 1950s, the State did not expect a lot from post-secondary institutions as this was the era where they were very autonomous institutions and received support from the Church and private benefactors. King's college which was affiliated with the Anglican Church and established by royal charter in 1827, benefited from the university portion of the endowment. Other denominational colleges (Catholic, Methodist, Presbyterian) sprouted and got a share of the public endowment. In 1849, King's College changed from denominational to secular and later became University of Toronto in 1887. In 1868, Premier Macdonald terminated all grants to denominational colleges who operated for a while without public funds. The accountability of these colleges was to donors (churches and benefactors), and to the professions that relied on colleges to train their members (medicine, dentistry, engineering, law) and to some extent, the student body. In 1901 Toronto's endowment was supplemented by annual grants as it began expanding and in 1914, the grant was fixed at $500 000.

Except for University of Toronto, until the end of World War II, Ontario's PSE institutions were basically private, serving a small elite group. In 1920, based on the recommendation of the Royal Commission on University Finances, government began to provide annual funding to Queens and Western, in addition to University of Toronto. The latter two gave up their denominational status to access public funds. With the

27

expansion of PSE to the masses in the 1950s and 1960s which was financed by public funds, government became more involved in university education. In this period, new universities were formed and former religious institutions were secularized to qualify for public funds. Prior to the 1950s there was no provincial government office or agency responsible for Ontario universities. Such decisions as were required, usually concerning funding, emanated from an informal process involving personal contact among the premier, the university presidents and, sometimes, those chairing the various boards of governors [Graham, 1989, 2]. Funding requests initially were made directly to the Premier or was responsible for education. In the early 1950s, an advisor to the Minister of Education was appointed, followed by several other advisors until late 1956 when, an internal and relatively informal government committee was formed consisting of the Provincial Treasurer, the Minister of Education, and senior treasury and education officials. This committee went through several phases until in 1961 the Advisory Committee on University Affairs (ACUA) was formed with, for the first time, some members from outside government (although not academic members [Graham, 2-3]. From the 1950s after World War II to the mid-1970s, enrolment expanded rapidly. This is the period when "it came to be recognized that, as the system developed in size and complexity, a more formal structure was required in order to provide increased public accountability..." [Fisher, 1981, 36]. During the 1960s, the sector also became organized to deal with government. The key players in university governance at this time, were ACUA set up in 1961, predecessor of the Ontario Council on University Affairs (OCUA); the Committee of Presidents of the Universities of Ontario (CPUO) set up in 1962, predecessor of the Council of Ontario Universities (COU); and the Department of University Affairs, later Ministry of Training, Colleges and Universities, set up in 1965. A formula was established in 1967 for the distribution of grants to universities which stayed in place during the 1970s. The sector worked with government to establish a funding formula which the universities felt would remove the subjectivity of the existing process where government reviewed budget requests and provide more independence of universities.

In 1967, Premier William Davis set out the objectives for the Ontario university system: · · · · The provision of skills and knowledge that will allow graduates to play a vital role in our society; The promotion of the powers of the mind so as to create men and women with love for learning and the motivation to seek new knowledge throughout their lifetimes; The search for truth and new understanding beyond the frontiers of present knowledge; The transmission of our common culture both to its student body and to the wider community; and, 28

·

The provision of graduates whose attitudes are consistent with the free society in which we live.

Over time, these broad objectives did not change but the university sector did. The enrolment growth in the 1960s was as a result of the maturation of the baby-boom generation. In the 1960s, the Progressive Conservative (PC) government built and equipped seven new universities. In that period, full grants were provided for every student in the university. In 1971, the Ministry of Colleges and Universities was created with responsibility

The State began to get more involved in the postwar PSE system and primary accountability was political through legislation and annual reporting directly to the legislature. Administrative accountability through the Minister became gradually more important during the 1980s onwards as enrolment expanded and government moved to `system' planning. Trick defines system planning as "the process by which government (or some other central authority) sets expectations for the university system as a whole and establishes appropriate regulations or incentives to ensure those expectations are realized" [Trick, 2005, 193]. There was some also some voluntary accountability through sector level financial reporting during this period. This was also the era where faculty was the administrators of universities so they were accountable to themselves. Given the autonomy of universities, the State has had to grapple with system planning and meeting its own provincial goals for PSE through the universities. According to the Bovey report, "Between 1962-63 and 1982-83, total full-time enrolment increased from 39 000 students to 179 000, a more than four-fold increase , while part-time enrolment rose from 14 000 to 105 000, a seven-fold increase" [Bovey, 1984, 3]. University enrolment in Ontario continued to increase dramatically over the last few decades which demanded greater expenditures of public funds as outlined in Figure 1 below on page 32. With PSE massification, "accountability emerged as a major education policy concern" [Shanahan, 2009, 4]. During this period, enrolment expansion and the increased government funding that came was the major reason that accountability for the university sector increased.

29

(Source: MTCU USER System) Excludes ineligible Full-time Equivalents (FTEs); Includes undergraduate (All Terms) and graduate FTEs (Fall & Summer)

Given the growth and changes in the university system outlined above, it was not surprising that the sector was the subject of many government reviews, as government attempted to increase its influence over traditionally autonomous institutions. Some of the most notable in this period included: the 1972 Commission on PSE in Ontario chaired by Douglas T. Wright; the 1981 Committee on the Future Role of Universities in Ontario chaired by Harry K. Fisher; and the 1984 Commission on the Future Development of the Universities of Ontario chaired by Edmund C. Bovey.

1972 Wright Commission on PSE In the early 1970s, public sector revenues were not increasing and government was looking for ways to rationalize spending. The PC government of Bill Davis set up a review of PSE, chaired by Douglas T. Wright and produced the report, "The Learning Society". The terms of reference involved advising on the effective development of PSE to 1980, and further to 1990 in terms of student needs and preferences; demand for programmes; the nature and type of institutions needed to meet present and future needs; facilities required; the functions and interrelations among bodies and institutions involved in the administration and development of PSE; principles on student transfers; and costs and financing PSE as it related to equality of educational opportunity and resources of the provincial government. The Commission made a total of 126 recommendations. Recommendations 42 -48 focused on PSE for Aboriginal Peoples including establishing special PSE programs and the set up of an Advisory Committee on PSE for the Native Peoples of Ontario. Most of the earlier recommendations focused on alternative forms of PSE outside of universities to make PSE more accessible to working people and focused on labor market needs. Recommendation 90 proposed the establishment of four governmental agencies by 30

law to deal with the planning, coordination and funding of PSE. This was not accepted by government except for the Ontario Council for University Affairs (OCUA), which was eventually set up but as a strictly advisory body. OCUA, an intermediary advisory body between the government and the university sector, assisted the Ministry responsible for universities with quality oversight, when it was set up in 1974 and evolved out of the previous ACUA.

Recommendation 96 also recommended the set up of an Ontario Committee on PSE with no administrative or executive authority to sponsor and conduct research on PSE and publish annual reports. Rec. 99 also noted "All post-secondary institutions should be operated with a maximum of local autonomy" but at the same time recommendation 100 asked that students and faculty have "direct and significant representation on the governing bodies" [Wright, 1972, 199]. In terms of access and financing, Rec. 111 proposed formula income based on projected enrolment and Rec. 109 was that "the goal of the provincial government's financing should be universal access .... for all who wish and are able to benefit from them." Recommendation 112 proposed 3-year grant announcements and freedom for institutions to set tuition levels, something that still does not exist. These themes would echo in subsequent reviews. It was only in the appendix of the report that reference was made to measuring PSE outputs in terms of efficiency and effectiveness by looking at the number of students and years spent in PSE. The report recognized that this was a measurement of activities as opposed to how well goals had been achieved. The report noted that the fact that questions about the purpose of activity in PSE institutions and how they were achieved was being posed throughout the Western world showed "the social importance of post-secondary education" [Wright, 1972, 227]. The language at this time still framed PSE as being of social importance as opposed to economic importance.

1981 Report of the Committee on the Future Role of Universities in Ontario The PC government of Bill Davis set up another Committee, chaired by H.K Fisher, during a period of economic recession. The Committee reviewed whether universities had met their objectives in the previous two decades and what needed to be done for the rest of the 1980s to serve the population of Ontario. The report made some recommendations which focused on issues of access, funding levels and allocations based on three scenarios, and system structure. The report recommended funding that supported modest real growth and acknowledged that at that time total operating expenditure per FTE student at $5 010 was lower than the rest of Canada ($5, 270) in 1978-79. It recommended more funding to improve accessibility and noted that: "If the resources are not available, expanding accessibility and

31

increasing emphasis on research will be counter-productive. Some degree of accessibility will have to be sacrificed in order to preserve quality" [Fisher, 1981, 20]. It also noted that changing the way in which funds were allocated to universities would not solve the problem of inadequate funding which already existed at the time. In terms of system structure and governance, it acknowledged that universities were not government agencies which made them unique among publicly funded PSE institutions, which derived from their need for academic freedom. Their autonomy supported academic freedom but there was a limit to their autonomy, to the extent they had to be accountable for expenditure of public funds. It recommended a much stronger role for OCUA in advising on university funding as well a recommendation to have results of COU graduate appraisals made available to OCUA for monitoring program quality, which was actually implemented. It was felt OCUA would have a more objective view than COU, which represented the interests of its members, in providing advice to government. The main recommendation of the report was that universities needed more funding to at least cover the cost of inflation, and if not possible close some universities. The idea of closing universities was rejected by Cabinet.

1984 Commission on the Future Development of the Universities Still under PC leadership, this Commission chaired by Edmund C. Bovey, provided what it called a strategic plan of action for the future development of universities for the rest of the 1980s and 1990s, and made 51 recommendations on issues of policy: accessibility and demand, quality in instruction and research, balance and differentiation, adaptability; inter-institutional planning and co-ordination; and funding, the latter being key to implementing the plan of action. Government, however, was unable to provide the additional funding proposed in the report, as happened with past reports. The recommendation on quality was noteworthy but while the presidents welcomed the idea of increasing quality, faculty and student representatives felt that this "challenged the long-standing commitment to accessibility" [Trick 2005, 204]. The ideas of restricting enrolment and increasing tuition fees were rejected by Government.

Accountability through Legislation The main accountability mechanisms for a university in this period, was through its own Act and the requirement to provide annual financial audited statements to Parliament. Each university having its own enabling legislation resulted in considerable autonomy for universities in the conduct of its affairs in all matters. The Governing Body, established by an act of Parliament, was the senior governing body that oversaw the academic, business and student affairs of the university. The president and senior staff were 32

accountable to the governing body or board under its establishing Act. Universities were set up over a long period so each Act may have clauses different from the others and variations in their Acts. These variations allow for differentiation among institutions but all had the same accountability to Parliament. Queen's was established by Royal Charter of Queen Victoria in 1841, 26 years before Canadian confederation. Classes were first held in 1842. Some of the university Acts are private acts of the Legislature introduced by a back bencher, so are not in the consolidated statutes, and others are public, introduced by the Minister of the day so a university representative noted "that's why it's not really appropriate to use the word `system' for Ontario's universities. It is a `sector', there's nothing really systematic about it." Historically, the Legislature had the power to confer degree granting powers directly as each Act was introduced into Parliament. In 1984 the Degree Granting Act was introduced to monitor degree granting to reduce the growth of "degree mills". Either the Legislature or the Minister could confer degree granting authority but "once a university had received its Act, the Legislature had no ongoing role in quality oversight" [OCUFA, 2007, 4].

Reporting to the Minister on enrolment and grants The other primary accountability mechanism since the mid-1980s was through Ministry operating grant regulations to account for transfer payments. The Ministry had enrolment reporting requirements for the main operating grants allocated to each institution, based on enrolment, which was calculated on an annual basis as well as reporting for any targeted envelopes. Up until the 1980s, the Minister's office did not think it had direct authority over universities. This was confirmed by a former government official who noted "to get a sense of the evolution of the accountability regime for universities, one had to look at the dialogue that was taking place between the Ministry of Training, Colleges and Universities (and its predecessors), and the provincial auditor (now Auditor-General) since the early 1980s", when the then provincial-auditor asked the Ministry the question, "To whom are the universities accountable?" According to a former government official, the Ministry's response to the provincial auditor was that universities are accountable to the legislature because the legislature establishes each university and gives its board of governors and senate or governing council the authority to run the institution. The Fisher report noted: While government is required to have the Ministry estimates approved in the legislature, institutional internal governance, management, budgets and academic programs are neither scrutinized by the legislature nor under the direct control of the Ministry [Fisher, 1981, 39].

33

Some Acts have the requirement that an institution should submit any reports that the Minister may request to the Legislature. As a former government official noted, "the Acts have been passed over a long period of time, so not all of them have the same provision but at a certain point in time government started putting in some reporting requirements in the legislation." Thus government began to influence the content of the establishing Acts of universities to provide more accountability to the Minister. The government during this period also attempted to influence universities to meet its priorities through specific programming. In 1989, OCUA was asked to work with the sector to create spaces in specific areas - teacher education, health sciences, French language programs, science and engineering.

Influence of the Provincial-Auditor In Increasing Accountability From the 1980s, a key parliamentary officer, the Provincial Auditor became instrumental in the way government's accountability requirements for universities changed over the years. There was a gradual shift in government policy over the years towards demanding greater accountability for broader public sector entities. The Office of the Provincial Auditor made a great contribution in terms of linking administrative accountability to political accountability through its intermediary role among the Public Accounts Committee (PAC) of Parliament, the Ministry of Training, College and Universities (MTCU) and the universities themselves.

In 1988, the Provincial Auditor undertook inspection audits of the accounting records and procedures of universities for the first time, but only had the authority to audit Ministry transfer payments. The audit included Trent in 1988, Guelph in 1989, and the University of Toronto in 1990 but no issues of consequence was found in any of the audits. The audit allowed limited access to information - operating funds and some capital funds. The auditor found accountability for the large sums of transfer payments universities received was inadequate with insufficient reporting caused by lack of a well-defined accountability relationship with government. Trick noted that the Liberal government at the time felt accountability lay with governing boards and "showed no special support for the Provincial Auditor's initiative" [Trick, 2005, 241]. Since 1988, the PAC supported the conclusions of the Provincial-Auditor's report regarding a need to amend the Audit Act to facilitate greater audit powers over more entities such as universities, however, no amendment of the Audit Act took place to grant powers for VFMAs. The Provincial Auditor found, at the time, that the Ministry did not believe it had the legislative authority to hold universities accountable and that their accountability was to the public directly.

34

Self-regulation in Academia and Quality Assurance of Programs According to Lee Harvey, there were four broad types of quality assurance processes: accreditation, audit, assessment and standards checking, which may sometimes overlap. According to his definition:

Accreditation is about providing a stamp of approval that the program or institution meets or exceeds minimum expectations. Audit is a process of review of procedures in place but it is not accompanied by any threshold judgment. Assessment judges the level of inputs, processes or outputs. Standards checking examine output standards and the means by which output standards are assessed internally; it includes external examination of academic achievement or professional competence and performance indicators or student evaluations of service provision [Harvey, 2008, 11]. Universities do not have a full-fledged national or provincial accreditation system, as in other countries. Ontario's universities may have their professional programs accredited through their respective provincial or national accreditation bodies, but for mainstream degrees, a self-regulating standards checking and audit process was in place for new and existing programs.

Ontario has one of the oldest quality appraisal processes in the world as well as one of the most independent, self-regulating university systems. The Ontario Council on Graduate Studies (OCGS) was set up in 1966 with the responsibility for conducting program approvals of proposed graduate programs and reviews of existing graduate programs. In 1968, OCGS began appraisal of new undergraduate programs in publicly-assisted universities and in 1982, it started appraisal of existing programs. Before the set up of OCGS, universities were totally self-regulating. The reviews or appraisals aimed "to promote, maintain and improve the quality of graduate education in Ontario and to provide the government, the universities and the public with the necessary assurance of this quality" [OCGS By-laws and procedures 6 , 11]. The government and the universities were informed of the results of the reviews. An OCGS quality appraisal was required before government would provide funding for a new graduate program but the government accepted Senate approval for undergraduate programs.

OCGS had full responsibility for graduate program planning in the 1960s. However, government took over some responsibility and set restrictions on graduate enrolment during the 1970s. In the early 1980s, OCUA got more involved in reviewing quality appraisals. A sub-committee of OCUA, the Academic Advisory Committee established in 1982, composed of seven academics appointed by order-in-council carried out four major reviews: 35

· · · ·

A review of the Council of Ontario Universities' annual compilation of graduate macro indicators; A review, from time to time, of the operation and effectiveness of COU's program-quality appraisals; A review of the results of COU's appraisals and assessments and of the implications for continued funding of any existing graduate programs; and, A review of proposals for any new graduate programs as well as undergraduate professional, quasi-professional, and special programs, according to OCUA's criteria.

But some felt at the time that it duplicated the work of OCGS. It was "something of a `buffer's buffer' " [Graham, 6].

Voluntary Accountability Measures ­ the Council of Finance Officers (COFO) Report A key accountability report was a voluntary sector-level financial report that was introduced in 1971-72 through the Financial Report of Ontario Universities 7 developed by COFO, an affiliate of the COU. 8 While the report was not mandated, government relied on it as a key accountability tool to provide data to verify what funds was flowed, how it was allocated at each institution and to help government budget for the next fiscal year.

This section showed that the university sector began to expand and the State increased its funding to universities as they were now being seen as critical agents of prosperity. Along with more tax-payers dollars came growing concern for accountability for universities. From very little accountability there evolved a growing demand for financial compliance backed by support from the auditor-general. As the 1980s came to a close, and the demand for PSE continued to increased, the need for funding of enrolment expansion to make PSE accessible became a recurring issue which were raised in the various PSE reviews during this period and would continue and intensify into the 1990s. The issue of quality improvement was not yet a major issue for both government and universities, although it had been raised by two PSE reviews.

36

CHAPTER 3: THE IMPACT OF NPM - ACCOUNTABILITY MEASURES, 1990 to 2003 It was during the 1990s, that the influence of NPM on PSE and the growth of accountability mechanisms became more apparent. During this period of increasing globalization and neo-liberal economics, Thatcherism and Reaganomics, NPM became prominent in the public sector and more accountability requirements were put in place by the provincial government. Primarily this was evident in terms of more targeted funding and reporting for enrolment but affected mainly the administrative side of institutions with some minor attention to the academic side. Accountability was infused with the NPM language of measuring and quantifying to justify expenditures which impacted the form of accountability. Similar to the USA, the calls for greater accountability in higher education came at a time of economic recession and reduced funding from government.

University accountability became a recurring theme with the Rae and Harris/Eves governments during the 1990s, as they struggled to exert control over these autonomous entities. During the early 1990s economic recession, government was looking for ways to achieve efficiencies and justify expenditures. In fact, cuts in funding took place in 1993 and 1996 under both regimes. Accountability demands, however, did not have a direct relationship with funding levels but increased with both increase and decrease in funding to the university sector. The rise of NPM helped shape the accountability requirements of universities, in this period where emphasis shifted from basic legislative annual reporting and financial compliance to value-for-money, from enrolment reporting to performance-based reporting. According to Shanahan, "accountability frameworks became infused with market discourse, market principles and market mechanisms" [Shanahan, 2009, 5]. This period also marked some interest in accountability from the academic side of the university as opposed to only the administrative side. The concerns surrounding PSE led to four reviews in the 1990s, two under the Rae government and two under the Harris government. While both Rae and Harris showed concern with university accountability, no major reporting changes were implemented under the Rae government, given it was the height of the economic downturn. With the dawn of the Harris regime, new accountability requirements were introduced along with funding cuts to the entire education sector, and with large corporate tax cuts, the Harris government did not leave enough funds in the coffers for improving funding to PSE.

37

The Rae Government and Accountability The Rae government came into office during an acute economic downturn in the economy. In the early 1990s, during the time of the Rae government, audited financial statements and the provision of enrolment data to the Ministry continued to be the primary accountability mechanism for universities to government. PSE and accountability was a policy concern for the NDP government and "were the first to tackle institutional accountability, even considering mechanisms that challenged university autonomy" [Shanahan, 2009, 7]. As the recession continued, per FTE funding for universities continued to decline over the 1990s. (See Fig. 2 on page 43) Trick pointed out, the Rae government was interested in `reshaping the university sector.' In 1993, government asked universities to establish protocols for membership of its boards. The Rae government also requested OCUA to advise on the issue of improving accountability for program quality and established two task forces and a Steering Committee on University Restructuring to address various PSE issues. The Committee was set up to find ways of costcontainment, improving quality and providing access for under-represented groups. According to Trick, the initiative met with opposition from the university sector and was derailed by the deteriorating fiscal situation and the work of the committee was suspended in 1994.

A concern with self-regulation in quality assurance In 1991, the Minister requested OCUA to provide advice on establishing a program review system to ensure public accountability for academic quality in universities. OCUA reported back that the appraisal process of the OCGS provided accountability for graduate programs. Concern was raised by OCUA over the lack of a "province-wide systematic quality review process at the undergraduate level." A recommendation was made by OCUA to set up a public-appointed Academic Quality Audit Committee to oversee undergraduate programs. This recommendation was never implemented in the face of a lack of support from the university sector, according to a former government official. Universities subsequently undertook to establish their own Undergraduate Program Review Audit Committee (UPRAC) with responsibility to establish standards for universities to review the academic quality of their undergraduate programs.

PSE Review: Report of the Task Force on Advanced Training, 1993 This report to the Rae government focused on training the workforce for reviving the economy. The six recommendations called for recognition of both academic and vocational education, partnerships among

38

universities, colleges, and employers, and elimination of barriers to inter-sectoral credit-transfers, a policy issue that still remains unresolved.

PSE Review: Taskforce on University Accountability and the Broadhurst Report, 1993 The Ministerial Task Force on University Accountability, chaired by William Broadhurst, also set up by the Rae government, comprised of different stakeholders to specifically review the university accountability framework. Trick noted that the Task Force was set up "as a substitute for amending the Audit Act to expand the powers of the Provincial Auditor" [Trick, 2005, 243]. The Task Force noted that its work was the result of an attempt to discuss and resolve concerns of both government and the university sector over accountability and suggested " ... a substantial strengthening of university accountability policies and practices" [Broadhurst, 3]. It addressed the issue of governance, in particular, and the expectations for university Boards in addressing both internal and external accountability. The governing body was seen by Broadhurst as the "locus of institutional accountability" but found that "governing bodies are too often insufficiently involved in monitoring performance" [Broadhurst, 3].

To enhance accountability, the Report proposed : (i) a strengthened accountability framework supervised by the governing body; and (ii) an independent, external monitoring agency. The institutional accountability framework would be "transparent, cost-effective and responsive to the stakeholders" and "appropriate to the nature of the university and respect academic freedom and institutional autonomy" [Broadhurst, 3]. The report suggested that to meet its obligations to both internal and external stakeholders, the governing body of each institution needed to review its constitution, organization, support, powers, relations with the senate, senior administration, faculty, staff and students. It went on to make detailed recommendations on the `attributes' of a governing body. Whether any of these detailed recommendations had already been adopted by universities was not identified in the report. The recommendation of the establishment of an Accountability Review Committee (ARC) within the then existing OCUA to monitor and report on the effectiveness of accountability frameworks in universities was never adopted. Activities of the proposed ARC would have included receiving a bi-annual accountability report from universities and conducting on-site reviews in a seven-year cycle, which would be presented to the Minister.

The report also made recommendations on various accountability mechanisms which would be subject to approval by the institutions' governing body including: 39

     

A clear mission statement, financial and academic plans which included progress assessment; Improved use of `management indicators' to allow governing bodies to better monitor activities and hold their institutions accountable; 9 Annual reporting on admission standards that allows access to all qualified students; Reporting on the quality of undergraduate and graduate programs including internal and external reviews and the steps being taken to address identified deficiencies; Monitoring of policy governing terms and conditions of academic appointments especially with regards to resources implications. Sound financial management and budgeting including deficit management, consultation on ancillary fees, auditing, management of capital and physical assets including inventory and monitoring of deferred maintenance, and adequate insurance coverage; Appropriate management and control systems for managing endowment and other restricted funds; Monitoring of institutional research policies and practices to ensure consistency with mission statement and academic plan; Proper human resources management policy; and, Community relations and responsiveness to local community needs.

   

One issue that was not within the terms of reference of the Task Force was funding of universities to satisfactorily achieve their missions. A former government official during the Rae regime noted that this report guided the sector on the issue of accountability. According to a former government official, "one of the most important influences on the accountability regime prior to the conservative's coming into power and what was driving Broadhurst was the notion that the accountability regime should be formative, in nature". According to him, a formative accountability regime meant the focus was on institutional improvement as opposed to summative accountability which was judgmental. He noted that the Conservative government that came into power in 1995 introduced a summative approach to accountability focusing on performance measurement.

OCUA Advisory Memorandum on Resource Allocation for Ontario Universities, 1995 In 1993, OCUA was asked to provide advice to the Ministry with regards to funding for "increased accessibility, greater emphasis on teaching, enhanced credit transfer among universities and between universities and colleges, program rationalization, and enhanced accountability for the resources allocated to teaching, research and community service" [Smith, 96]. OCUA raised the issue of increasing enrolment and its impact on quality if there was no new funding from the government. This is an issue

40

that would continue to be problematic for government and universities and is a recurring theme in the accountability debate which started with massification of PSE. Government funding allowed creation of new spaces for enrolment expansion but funding was not always sufficient to fund areas of quality improvement including student services, new classroom and building facilities, more faculty hires to reduce the student-faculty ratio, program improvement, etc.

The Advisory Memorandum from OCUA provided recommendations that were far-reaching and proposed a change in the existing funding allocation mechanism for universities which was a "historically-determined allocation system based on institutional income and expenditures over 25 years ago" to a cost-based funding system. [OCUA, 1995, 14] It brought great opposition from the sector who saw the proposals as intruding on university autonomy and an intention to micromanage. It also recommended new accountability mechanisms such as developing new accountability provisions with clear expectations and terms and conditions attached to operating grants. These would be in addition to recommendations of the Task Force on University Accountability that was also underway at the time. The allocation formula would be "based on factors more reflective of the level of activity or outcomes underlying the key outputs or missions of each university than is currently the case" [OCUA, 1995, 2]. It also recommended that to implement the recommendation, several initiatives would have to take place including implementation of a previous recommendation of a system of self-regulated academic quality reviews for all programs in universities to improve the quality of the student learning experience. This responsibility was later undertaken by COU. It also recommended a review of peer-adjudicated research overhead funding and review of the appropriate research activity/output measurement. This advisory was using the language of NPM to cover accountability not only for funds administration but for academic performance in terms of program quality and research performance. When the report was released the Conservatives were already in power and rejected the advice of OCUA.

The Education Quality and Accountability Office (EQAO) The EQAO was set up by the Rae government in 1995 to address educational quality and accountability in elementary and secondary schools and the PSE sector. The EQAO did start work with the school system but met with opposition from the university sector and "no action was taken to develop this role" [Trick, 2005, 245].

In summary, although there was a review of PSE specifically on the issue of accountability, no major changes to the university accountability framework were implemented under the Rae government given the economic crisis at the time. Despite the recession, the Rae government increased operating grants to 41

universities with no strings attached. The government, according to an interviewee and former provincial politician, was not pleased that the funds went into salary increases for faculty and did not impact teaching quality through new hires and improved student-faculty ratios. The Rae government had great plans for the university sector and PSE but they did not have the time, resources, nor support from the sector to implement many of them as the Conservatives took over the reins in 1995. This period showed a significant shift with accountability of universities high on the agenda of the Rae government. The regime did not have time, however, to implement any new accountability mechanisms for the sector so the impact of NPM was not significant.

The Harris/Eves Government's approach to university accountability and NPM In 1995, the PC government of Mike Harris came into power and purported with his "Common Sense Revolution" to run Ontario like a business. This was the period where NPM philosophy was at its height in public administration, which was reflected in the policies and programs of the government. At this time, government was seen as inefficient so policy measures focused on efficiency as the primary goal and value-for-money alongside cuts in government expenditures. A government official noted that "every new government likes to pretend there was no accountability before they arrived." With the Harris government, there was "accountability lust" and a university representative (and former government official) noted that during this period there were "pressures to move ahead on accountability." The Harris government also introduced business planning into the public service, based on the Party's election promise to run Ontario like a business. The focus was on cost-cutting and cost efficiency for the entire public sector given his election promise to cut government expenditure by 30%. Most of the accountability measures for universities under Harris were incremental. The Harris government conducted two reviews on PSE to look at specialization and restructuring as cost saving mechanisms. Universities were subjected to both cost-cutting and demands for more accountability. Universities were seen as more instrumental to economic development and funding and accountability reporting for both operating (the Access to Opportunities Program) and capital (Superbuild) focused on program disciplines that were felt to be more relevant to the economy.

42

(Source: Operating grants - COU Resource Documents; FTE enrolment ­ MTCU USER system) N.B. FTEs includes undergraduates for All terms and graduates for Fall & Summer terms)

Cuts were made to university funding even as enrolment was increasing. Per FTE funding declined further under the Harris Government at the same time that the double cohort was entering universities. See Fig 2 (on page 45 above) which shows the fall in per student funding under the Harris government. Part of the Common Sense Revolution included de-regulating tuitions fees, allowing universities in 1998 to increase tuition fees to compensate for the loss in government funding with a proviso to set-aside 30% of tuition increases for scholarship and bursaries. In 1995, universities also suffered from a reduction in federal transfers with the introduction of the Canada Health and Social Transfer (CHST). This was a single transfer replacing individual transfer envelopes for higher education, health, and social programs. The provinces now had more discretion about how these funds were allocated, with the results that PSE funding went down, losing out to health care.

As a cost-saving measure, in 1997, the Harris government announced replacement of a 5-year secondary school program with a 4-year program. This meant in 2003, two cohorts would graduate from high schools and would be looking for spaces in PSE institutions, which would have huge impacts for universities. Accountability and quality was also part of the agenda of the Harris government. In the period leading up to the double cohort, according to a university representative, institutions had to sign agreements and set targets, and there was a continuing increase in the number of special purpose funding envelopes and reporting requirements. An interviewee who was a former Ministry of Training, Colleges and Universities official and a university representative at the time of the interview noted "some of the

43

reporting requirements were absolutely inane because they bore no relationship to how the funds were actually being used in the institutions".

Capital funding declined during this period but under pressure to provide spaces for the double cohort, the SuperBuild program was announced in the 1999 Budget allocating $742 million to PSE for infrastructure. However, universities had to provide matching funds which was allocated on a competitive basis preferring infrastructure targeted for engineering, business and computer science. According to a study of the Canadian Centre for Policy Alternatives, SuperBuild was a mechanism that forced universities into partnerships with the private sector and favored projects for programs that the government felt were more important to the labour market and economic development. OCUA was one of the agencies shut down by the Harris government in 1996, a victim of the move towards `smaller government'. The 2001 budget promised $283 million by 2003-4 when the university sector indicated it needed a minimum of $600 million to maintain enrolments [CCPA, 2001]. Table 1 on page 47 below shows that in nominal dollars, provincial funding started falling in the early 1990s and accelerated during the Harris years. Revenues from tuition fees also continued to increase under the tuition de-regulation policy of the government. Students had to pay more towards their education.

In 2004-05, funding was almost the same as in the late 1980s in real terms [COU Briefing Notes, 2003]. In 1991-92, provincial funding of operating funds was 74.1% but by 1996-97 had dropped to 59.4% and further to 50.6% in 2001-2002. Ontario government funding for universities was 10th out of ten provinces on a per student basis in 2001-2002, and had been 10th on a per capita basis since 1993-94, during the economic recession [COU Resource Document, 2007].

44

Table 1: Provincial Operating Grants and Tuition Fee Revenue, 1987-88 to 2004-05
Year Operating grants 1987-88 1988-89 1989-90 1990-91 1991-92 1992-93 1993-94 1994-95 1995-96 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 $1,451,592 $1,554,947 $1,676,864 $1,822,796 $1,945,074 $2,009,133 $1,875,973 $1,853,629 $1,822,477 $1,550,722 $1,550,972 $1,596,514 $1,640,581 $1,726,897 $1,732,385 $1,885,835 $2,215,681 $2,464,517 Actual $ 000s Tuition Fees Grants + Fees Operating grants Constant 2004-05 $ 000s Tuition Fees Grants + Fees

$343,728 $379,690 $421,021 $474,561 $532,429 $596,665 $630,966 $677,179 $744,393 $846,891 $920,047 $1,025,491 $1,179,444 $1,271,826 $1,406,606 $1,593,702 $1,834,014 $1,939,815

$1,795,320 $1,934,637 $2,097,885 $2,297,357 $2,477,503 $2,605,798 $2,506,939 $2,530,808 $2,566,870 $2,397,613 $2,471,019 $2,622,005 $2,820,025 $2,998,723 $3,138,991 $3,479,537 $4,049,695 $4,404,332

$2,205,506 $2,269,138 $2,322,809 $2,402,951 $2,465,725 $2,503,934 $2,308,182 $2,267,354 $2,184,536 $1,825,662 $1,803,957 $1,838,188 $1,848,231 $1,891,148 $1,856,655 $1,962,457 $2,264,397 $2,464,517

$522,250 $554,083 $583,202 $625,603 $674,948 $743,609 $776,336 $828,323 $892,276 $997,043 $1,070,120 $1,180,726 $1,328,727 $1,392,794 $1,507,507 $1,658,454 $1,874,338 $1,939,815

$2,727,756 $2,823,220 $2,906,011 $3,028,554 $3,140,672 $3,247,543 $3,084,517 $3,095,677 $3,076,812 $2,822,706 $2,874,077 $3,018,914 $3,176,958 $3,283,942 $3,364,162 $3,620,911 $4,138,736 $4,404,332

(Source: COU Resource Document 2007)

Increased self-regulation in quality assurance for university programs When OCUA was abolished in October 1996 by the Harris government, the COU took over the quality assurance monitoring role for undergraduate education and "established guidelines for the conduct of periodic quality reviews of undergraduate programs, and committed to a system of regular audits of the Ontario universities' policies and procedures for these reviews" [UPRAC website]. UPRAC was set up for only existing programs, but in 1997 the UPRAC guidelines were amended to include an approval process for new undergraduate programs. MTCU approves funding of non-core new undergraduate programs based on specific criteria including demand for the programs. The UPRAC website also noted that "the UPRAC audit guidelines are designed to satisfy the need for public accountability identified in the report of the Task Force on University Accountability (the Broadhurst Report) and by the former Ontario Council on University Affairs in its Advisory Memorandum, OCUA 93-VI Academic Audit Review." 10 . During an audit, universities must demonstrate to UPRAC that they have a policy and

45

practice that meet the UPRAC minimum requirements. Three auditors conduct each audit and three universities are audited each year. UPRAC conducts audits of selected existing undergraduate programs on a 7-year cycle which according to COU, "is concerned with process rather than direct assessment of academic quality. Its contribution to public accountability rests upon its demonstration that the quality of every program in the university is regularly reviewed according to transparent and sound procedures and standards that are verified by external audit." The auditors, comprising retired academics from Ontario universities, provided a copy of the report to the university, all other universities in Ontario and MTCU. One year after the report, the audited university has to provide information to MTCU and OCAV as to how it has responded to recommendations.

Universities in Ontario went from no government oversight in quality assurance before the 1970s, to some oversight with the advent of OCUA in 1974 until its demise in 1996. Universities thereafter were again totally self-regulating as far as the program quality assurance process. The difference was that from individual institutions being totally responsible for quality assurance, there was now an external sector level entity, in OCGS, overseeing quality assurance. Government did not second guess the decisions of the university quality assurance process when it came to new programs, and there was no external review overseen by government of existing programs. This was a contradiction to the increased calls for accountability, but the government probably also recognised that state control of quality assurance would be an expensive undertaking in a time when government was cost-cutting.

PSE Review: Report of the Smith Advisory Panel, 1996 In 1996, David C. Smith, Chair of the Advisory panel, "Excellence, Accessibility, Responsibility", was appointed by the Harris government to advise on how the PSE sector could be restructured to deal with funding reductions in 1996-97. The final report did not directly address this issue. The report covered PSE in its broadest sense including universities, community colleges and private sector colleges and its recommendations addressed ways to improve quality and accountability. Recommendation 13 of the report, (like the Broadhurst report) suggested the establishment of an advisory body "... to provide analysis of post-secondary education to help assure governments, students, private organizations and other groups that critical assessments, independent reviews and advice are an ongoing feature of Ontario's post-secondary system" [Smith, 1996, 13]. One responsibility of this proposed Body

46

was " ... to monitor, assess and report upon the adequacy of quality assurance and accountability processes for both colleges and universities" [Smith, 13]. The report suggested de-regulation as, a necessary condition for institutional development and adaptation, which will lead to expanded opportunities for learning. This position is practical, not ideological. Universities and colleges perform best in education and research when they have a large measure of autonomy, reinforced through full accountability exercised through their governing bodies [Smith, 1996, 23]. In other words, de-regulation did not necessarily mean less accountability. Recommendation 17, echoed the Broadhurst report, by suggesting governing bodies must evaluate performance of faculty for teaching and research and have processes for taking corrective measures. The Smith panel was also in favor of private universities that were strictly regulated but opposed giving colleges of applied arts and sciences degree-granting authority.

Performance-based funding and Key Performance Indicators (KPIs) A former government official noted that "things started to change on the government side in terms of expectations, in the late 90s with the advent of the performance indicators". There was a drive for more accountability and "to have performance measures for universities as well as other parts of the broader public sector." The Conservative government introduced performance measurement and performance funding for colleges and universities. The Provincial-Auditor in 1996 had recommended that the colleges establish measurable performance goals and report on their achievement. In 1998, a small performance funding envelope was introduced by the Ministry, which used employment and graduation rates to calculate allocations. According to another former government official, bureaucrats were asked to look at accountability practices in other jurisdictions. For instance, performance funding was introduced because a number of US states and some other jurisdictions had introduced funding tied to performance and certain measures. One former government official felt for this accountability measure "they wanted to have teeth in it by putting dollars to it" but according to another former government official, the amount of funding actually tied to performance was very small. Performance funding was 1% of total operating grant. One former government official felt the amounts were too marginal to be an incentive to institutions and were "probably not good measures either".

Universities have had to report KPIs (borrowed from the colleges sector) to the Ontario government annually since 1998-99. The KPIS were outputs-based including the employment rate after six months and two years, and graduation rate. The employment rate and graduation rate was used as the basis to 47

allocate performance funding. Another former government official noted that when the KPIs were developed they were not intended for use as the basis for performance funding. The central government ignored some of the key underlying problems related to the KPIs because "politically the idea of funding based on performance played well." He noted the government "developed schemes that amplified minor differences into major differences so that they could ensure that there was a broader and a wider distribution of funding" based on the KPIs. It was a matter of expediency rather than accountability to use the KPIs but he noted further: It really had no relationship to the actual measures in terms of accountability. It ignored the fact that two of the indicators are based on surveys with very low response rates and high statistical errors so that they treat each observation as being the exact observation and they are not. It also ignores the fact that on the graduation information, it throws in apples and oranges. Academics in Ontario expressed their displeasure with the idea of performance indicators which were "imbued with a consumer ideology that encourages the view of education as a commodity" [Shanahan, 2009, 11]. A government official acknowledged that the universities had objected to the KPIs and had given very good arguments for not using them. For instance, these measures did not necessarily reflect the quality of education of students as some universities were located in low employment areas. Some universities had professional schools (doctors, dentists, lawyers, pharmacists) where the employment rate was 100% and other universities were geared for undergraduates and did not have the same employment rates. The inability to find employment after graduation was also reflective of the economic state of the province and the demand and supply of labour. Despite these objections, the KPIs stayed in place and showed very high employment rates for universities, which then used as a marketing tool for recruitment. The government felt that students as consumers needed information on where to invest their money for a PSE education and the KPIs provided the relevant information. University accountability framework report In 1999, the Ministry of Training, Colleges and Universities (MTCU) was created which reflected the importance being placed on post-secondary education. Previously, the Ministry was part of the Ministry of Education. In 2000, a proposed university accountability framework was prepared for a joint Ministry/COU Task Force set up to address the Provincial-Auditor's 1999 report on "The Accountability Framework for University Funding". The framework was never officially adopted but was used by MTCU as a guide in its work with universities. The report defined `accountability framework' based on an outline from a 1999 Leaders Symposium on "Making Accountability Work, 11 co-hosted by the Ministry of Finance. The key elements are summarized in the context of universities, below:

48

Performance measurement:       Timely dissemination of information were critical; Goals needed to be established and expectations defined before selecting performance measures; Must be relevant to clients (students), providers (funders), and sensitive to system-wide and institution-specific benchmarks; Enhances administrative performance and improves service delivery by providing information for decision-making; Focuses on results for clients (government, students, donors); A tool that provides "the proper focus and common vision of an enriched accountability regime" whose benefits must be greater than its administrative costs.

Stakeholder needs:   Meet the needs and expected benefits of all stakeholders based on the relative importance of each stakeholder (funders, students, governing body, community etc); Benchmarks or minimum standards are important for stakeholders to be able to compare organizations.

Flexibility and acceptability:   Allow professionals to address issues of quality-of-service, professional ethics, and continuous improvement of best practices; Given both institutional and system considerations, there must be a blend of consistency and diversity that allows comparability yet allows for individual institutions to adopt goals and performance measures suited to their own mission and circumstances; Be fair, predictable, consistent, transparent, and politically neutral; There should be continuous renewal to accommodate changes in circumstances and technology.

 

The University Accountability Framework report acknowledged the previous work of the 1993 Broadhurst report and reiterated much of what was raised in that report, including issues related to university governance; program quality; monitoring financial health; accountability for tuition fees and revenues; data on accessibility, participation, enrolments, and capacity; linking institutional objectives to performance indicators, and linking system-level performance indicators to provincial objectives. The concern was about `strengthening' accountability which meant providing better and more information to government which could include already existing reports to government on a systematic instead of an ad hoc basis.

49

Targeted PSE funding for the labour market In 1998, the Ontario government introduced a funding envelope through the Access to Opportunities Program (ATOP) for universities to provide education in the technological areas ­ computer science and software engineering. This was a direct imposition of its market-oriented philosophy on universities, which ignored the high demand for PSE in the humanities and social sciences and was implemented in response to a report from Nortel as well as the Canadian Advanced Technology Alliance about labour shortages in the Information Technology & Telecommunications industry, which both proposed university funding to expand programming these areas. The Harris government felt intervention was necessary to meet the needs of students and Industry. One government agency official did not believe that universities should be specifically educating students for Industry and the perceived needs of the labour market. It was interesting that the government offered ATOP to all universities and all opted to participate.

Investing in Students Taskforce report, 2001 Appointed in 2000 by the Harris government, the Investing in Students Taskforce's mandate echoed themes of previous taskforces including advising the Minister on quality, affordability, accountability, as well as exploring best practices and options for shared services (e.g., in IT, procurement, data collection) to increase administrative efficiencies. With all the reviews on PSE, government selectively chose or as Doucet [2004, 7] put it, practiced the art of cherry-picking advice as to what they wished to implement and generally chose to ignore recommendations around increased funding. The report also raised the issue of the costs of over-reporting which would become a continuing concern in the sector: There should be a balance between the need for institutions to provide clear and transparent accountability on the one hand and the time has come to streamline processes and reporting requirements on the other" [ISTF, 64].

The Rising Influence of the Auditor-General - a Proponent of NPM practices During the 1990s, the auditor-general continued to have an impact on the public policy discourse on accountability and noted:

all transfer-payment partners should be subject to legislated public accountability, including performance reporting and a full-scope legislative audit regime, because provincial monies flowing to grant-recipient organizations continue to represent the single most significant demand

50

on the province's financial resources, with about 50% of total government expenditures flowing to grant-recipient organizations [Provincial Auditor Report, 2002, 18]. Their individual Acts made universities accountable to their governing bodies but it still was not clear to the Provincial Auditor to whom the governing bodies were accountable. He was instrumental in terms of advising government to demand more accountability from universities. The Broadhurst report acknowledged the influence of the Auditor-General: The perceived need to strengthen the accountability of Ontario Universities has been stimulated by reports of the Provincial Auditor and the Standing Committee on Public Accounts [Broadhurst, 1993, p.13]. A government official also remarked on the impact of the Auditor-General in questioning the use of public funds by universities. It was during this time, she noted that universities became concerned about intrusion into `academic freedom' and `autonomy'. The auditor-general became an internal advocate for NPM, as was the case with the federal auditor-general. The contradictory nature of the role of the auditorgeneral as both NPM advocate and compliance auditor is noted by Saint-Martin speaking about the federal Office of the Auditor-General, which is also applicable to the Auditor-General of Ontario, "the OAG represents a Janus-like or `schizoid' institution that simultaneously pushes both for greater managerial decentralization and more controls and regulations in the administration of government" which shows "the tensions and contradictions in the role of the office as these are shaped by various institutional forces as well as by the knowledge base and ideas on which the it relies to fulfill its mission" [Saint-Martin, 2004, 4].

In response to the auditor-general's reports from the inspection audits at Trent (1988), Guelph (1989) and Toronto (1990), in 1991, Management Board of Cabinet in proposed extension of the Auditor's powers under the Audit Act to include "access to all information and records considered necessary by the Auditor in order to conduct an audit and report fully", [Broadhurst, 14] as well as the power to perform value-for-money audits. The idea of Value-for-Money Audits (VFMAs) in universities was controversial with the Ministry "fearing auditing of policy" and university stakeholders "... concerned about the effectiveness criterion included in this auditing" [Broadhurst, 31]. The specter of compromising `institutional autonomy' and `academic freedom' was also raised by the university community through the creation of criteria to measure efficiency and effectiveness.

Almost ten years later, the Auditor-General's 1999 Annual Report [ch: 3, sec 1] focused on university funding. The Auditor-General audited the universities branch of newly created MTCU 12 ' using both compliance and VFMA with the objective of assessing the extent to which it was promoting the 51

achievement of objectives of program quality; access; responsiveness to changing educational needs; cost effectiveness and sound financial management. But the AG still had no power to conduct VFMAs of universities. The universities and its association, the Council of Ontario Universities (COU) cooperated with the AG's request for information in a limited review of governance and accountability processes. The AG concluded in his 1999 report that: Although the Ministry has recently set some measurable objectives for postsecondary education and begun to collect some related performance information, these initial steps are not sufficient for the Ministry to determine how well the university system is meeting provincial needs and contributing to the achievement of postsecondary education objectives [AG Report, 1999, 217]. The AG continued to lobby for his office to have expanded audit powers and after a couple of failed Bills and a change of government, Bill 18, the Audit Statute Law Amendment Act, amended the Audit Act (now the Auditor General Act), November 30, 2004. The most significant amendment contained in Bill 18 was the expansion of the Auditor General's value-for-money audit mandate to include the thousands of organizations in the broader public sector including universities, after 13 years of lobbying. Providing value-for-money was a cornerstone of NPM.

The faculty representative interviewed for this study, noted that the academic community "supported the provincial auditor-general having access to the university books because it needs to be accountable to its funder". The academic representatives had met with the auditor-general and indicated their support of financial auditing of universities, "as long as he did not interfere with academic freedom because he is not there to measure the quality of academic programs but rather is the money being spent for the purposes for which it was intended to be spent". The universities do well in financial compliance, according to the faculty representative and the academic supported the fact that there was too much reporting without any use of the information by government. The academic community also supported academic accountability but felt there was already significant accountability measures internally within universities and the sector ­ peer adjudicated and externally adjudicated (OCGS, UPRAC). Harris himself in his election campaign expressed his interest in eliminating tenure for faculty. But a faculty representative noted that becoming tenured involved a rigorous process and student evaluations/surveys were also are part of internal accountability mechanisms. They believed the government was quite aware of all the internal accountability mechanisms but discounted them as being relevant and reliable.

52

New Accountability Legislation Universities became covered by the Public Sector Salary Disclosure Act, 1996 which required the release of names, salaries and benefits of employees with salaries exceeding $100 000. The intention was to also include universities under a proposed Public Sector Accountability Act 2001. The 1997 Ontario budget had first raised the idea of a Public Sector Accountability Act and after advice in 2001 from the Ontario Financial Review Commission, the government introduced the Act in Spring 2001. The proposed Act would apply to and have far-reaching consequences for agencies and municipalities, school boards, hospitals, universities and colleges and to very large agencies in the social services sector. The Act would require balanced budget operations, business planning, and performance targets and measurement to assessing inputs, outputs and outcomes, improved service delivery, efficiency, effectiveness, accountability. There was objection from the sector including OCUFA who noted that "according to Sec. 11 (2), failure to comply with the Act could result the government withholding funds." However, this legislation was never passed.

The Harris government also introduced the Post-secondary Education Choice and Excellence Act, 2000 which "strengthened enforcement and virtually eliminated the role of the legislative Assembly in degree granting" [OCUA, 2007, 5] while expanding the power and scope of the Ministry's role in degree granting. The Act expanded the powers of the Minister in consenting to degree granting powers and reduced the power of the Legislature. According to OCUFA: the Minister of the day became the primary gatekeeper for new degree-granting authority and, through a new body, the PEQAB, the determiner of program quality for new programs. All public universities were exempt from the scrutiny of the Board ... [OCUFA 2007, 5]. The Postsecondary Education Quality Assessment Board (PEQAB), an arms-length advisory agency, was set up to make recommendations to MTCU on applications for ministerial consent under the terms of the Act. Ministerial consent was required by all public or private degree-granting organizations, based outside the province to offer all or part of a degree program in Ontario, as well as all private organizations in Ontario, and by all Ontario public organizations not empowered to grant degrees by Ontario statute. Consent was also required to use the word "university" relating to an educational institution in Ontario. So government became involved in quality control but not related to quality assurance for Ontario universities.

The period of the Rae and Harris/Eves Governments saw a marked increased in the concern with university accountability which included four reviews on PSE during this period. Ironically, it was a

53

period of economic decline which resulted in funding cuts to universities. Even as enrolment continued to increase (See Table 1 and Figure 2 on page 45 above) per FTE funding fell under the Rae and even more so under the Harris/Eves government during a period of economic recession. One would have to surmise that the accountability concerns were tied to declining funds in the government coffers but under Harris the decline in funds was partially due to tax cuts to corporations. It was the Harris government who introduced NPM-related performance measures to the sector, as it attempted to emulate other jurisdictions such as the USA. Unfortunately, the funding associated with performance-funding were too small to be an incentive to universities, nor did it seem relevant to the KPIs of graduation rate and graduate employment that were being used. The sector also consolidated its control of the quality assurance of programs with the demise of OCUA, as government did not have the expertise or funds to attempt to control or monitor quality assurance. NPM's influence was more apparent with regards to the administrative side with the sector now subject to VFM audits. NPM did have some influence on the accountability regime but it was not as marked as with other jurisdictions.

54

CHAPTER 4: THE AGE OF MULTIPLE ACCOUNTABILITY DISORDER, POST-2003 Though some have argued that NPM is dead, this does not seem to be the case in Ontario or for that matter in other jurisdictions. NPM left a legacy in government to obtain accountability through measurement of outcomes using performance indicators. This section looks at the issues of accountability under the McGuinty government who was elected in 2003, in the wake of the public's disenchantment with the PC Government's Common Sense Revolution. In Ontario, there have been many Accountability Directives which apply to all Ministries and a majority of agencies. In December 2004, the McGuinty government in Ontario enacted the Fiscal Transparency and Accountability Act (FTAA) which the government noted in the 2008 Budget, was "...part of its commitment to enhancing accountability, increasing transparency and improving its financial reporting". In August 2007, the government introduced a new transfer payment accountability directive to "provide stronger assurance that public funds are being spent by organizations for their intended purposes."

The State struggled to establish system planning for universities in the 1980s, 1990s up until the Harris/Eves regime. The McGuinty government made a greater effort at system planning and establishing specific policy goals, targets, funding, and accountability mechanisms for the sector from previous regimes. This government wanted to ensure its policy priorities as outlined the Reaching Higher Plan (RHP) were implemented by universities. All aspects of universities including academia were under scrutiny in terms of outcomes and accountability. The auditor-general also continued to flex his new VFM muscles in whatever administrative area he thought should be subject to review for cost-savings and efficiency.

In October 2003, the newly elected Premier Dalton McGuinty wanted to be known as the "Education Premier". Investing in skills and knowledge was one part of his five-point plan for the economy. This section focuses on the 2005 Rae review of PSE and the roll out of policies based on the review as it impacted accountability for universities. Given the large investment of $6.2 billion in the PSE sector in 2005, the government felt the need to justify its policy by demanding accountability for the incremental funds invested above the normal operating grant to the higher education sector. The increase in funding from the McGuinty government can be seen in recent years with the impact of the Reaching Higher funds from 2005-06. (see Fig. 2 on page 43) Accountability requirements increased under McGuinty along with the new funding. The expansion of the accountability regime is reflected in the increase in the number of accountability actors and new accountability mechanisms, as well. 55

The great value placed on PSE as part of economic competitiveness indicated the shift from viewing PSE as a social good to an economic good where it had an economic value and had to show a return on investment. Politically, however, it was difficult to sell to the public and competed with health and other sectors for limited tax dollars. One university representative put it this way: The selling point for universities is this is an investment. You are investing in people. You can't say that about hospitals. Hospitals and the health sector are consumption items. You are basically maintaining people.... you sort of fix them up and they go back to what they were doing. What you hope with education, is they don't go back to what they were doing. That they're going to do something more. This is going to have a return for them and for society.

The McGuinty government committed to providing public progress reports on its performance outlining results in achieving government goals and targets since 2004. According to the government website, "The results-based planning processes help to ensure that all provincial programs achieve desired results and outcomes, and that these programs are delivered in an efficient and cost-effective manner." Each Ministry has to do results-based planning annually. The government extended these expectations for performance measurement and public performance reporting to other agencies and entities in the broader public sector such as hospitals, colleges and universities to provide more accountability to the government and taxpayers. Results-based management, performance measurement and reporting have implications for accountability where the focus shifts from inputs and process to results or outcomes. This administrative form of accountability reflected the lingering legacy of NPM in the Ontario public service.

In both the period of funding decline in the mid-1990s and the period of funding increase from 2005-06, new accountability mechanisms including performance measures were introduced. (on page 45 above) shows nominal grants per full-time equivalent (FTE) and grants per FTE in 2008 dollars between 1979 and 2009. In 2008 dollars, accounting for inflation, grants had been falling from a high of 12,000 per FTE in 1979-80 to just over 8,000 per FTE in 2008-2009. The 2005 Rae Review on PSE and its impact on expanding NPM in the accountability regime In the 2004 Budget, the McGuinty government announced the set up of the Rae review whose mandate was to provide evidence-based, realistic recommendations on system design and funding of the PSE system [Rae Report, 2005, 1-2]. System design would need to promote the building of a skilled workforce and scholars, and integration and articulation of the system in a cost-effective way. A new funding model 56

would consider accessibility, affordability, quality, appropriate cost-sharing, student assistance and accountability in PSE. Interviewees agreed that what drove the set up of the Rae review was that the current government saw the importance of PSE in the economic competitiveness of Ontario. The Premier, in his election platform also committed to focusing on PSE as a key policy concern. The Rae Review was set up, one interviewee noted, "because the Premier did not think the tuition freeze was sustainable so government needed some policy advice on moving beyond that election promise" and it was felt "higher education was not responding to the needs of the 21st century".

The Rae Review has been considered by some to be more influential than any other PSE review given the translation of its recommendations into actual government policy including funding and administrative initiatives [Clark and Trick, 2006, 1]. If one focused on the fact the government finally increased funding to PSE, after successive reviews on PSE made this recommendation which government failed to implement, then the Rae Review was the most influential. However, as with other governments, the recommendations were selectively implemented with the aim of keeping more control in the hands of government without giving over power to a new agency or enshrining new government obligations in legislation. Also, after declining funding in PSE, under the Harris/Eves government and the explosion in demand for university spaces, the McGuinty government would have been hard pressed not to increase funding in PSE.

NPM did not disappear with the departure of the PC government. The McGuinty government took performance measurement further than the Harris government. Some of the recommendations of the Rae Report were adopted as outlined in its Reaching Higher Plan [Ontario Budget, 2005] 13 . The report and resulting PSE policy reflects the ideas and language of NPM including demonstration of accountability through results, outcomes and value-for- money invested in PSE. The major accountability changes that were implemented stemming from the Rae Review were administrative - the introduction of Multi-Year Accountability Agreements and the requirement for report-backs, and the set up of the Higher Education Quality Council (HEQCO).

Legislative Changes Except for the legislation to set up HEQCO, there was no major legislation introduced, only amendments of existing legislation for MTCU and no changes to university statutes. The Rae report had recommended the set up of a new legislative framework which would "set out the parameters of the student assistance 57

program, the frameworks for revenue ­including tuition ­ and accountability, and mandated public reporting of performance and results" [Rae, 2005, 39]. According to a former provincial politician, this would promote a transparent governance model but government preferred to do without legalization, so they can have more flexibility in funding and policy or "do whatever the hell they want." So for instance, given the current budgetary difficulties, the government did not have to continue multi-year funding, if it did not want to. Government officials noted that another layer of legislation would be too complicated as each university already has its own Act of Legislature "so to have an over-arching Act would be a very big and complicated and controversial thing to do." The amendments made to a few pieces of legislation including the Ministry of Training, Colleges and Universities Act, the Colleges Act, and the Act to establish the HEQCO along with the introduction of Multi-year Accountability Agreements (MYAAs), was felt to be sufficient.

Aligning the university agenda with government policy goals The three main policy goals as articulated in the Rae review and the Reaching Higher Plan [Budget 2005], echoed themes of previous PSE reviews which included: 1) Accountability ­ improving accountability for use of public funds and achievement of government goals was a policy goal in itself; 2) Access - increased enrolment at the undergraduate and graduate levels including first generation student (parents did not attend university) and Aboriginal Peoples; and 3) Quality - improvement in quality of university education in terms of learning and teaching. These goals were by no means new government concerns about PSE since previous government reviews have raised all these issues. The McGuinty government set these targets as part of its own policy and what was different this time was that this government expected the universities to align their objectives (regardless of individual institutional missions) to the government policy agenda. PSE investment was not high on the public agenda but the McGuinty still made the investment and new accountability requirements was a way of justifying this expenditure to the public. But the faculty representative noted from their polling, "it is clear the public is not aware of the amount of money that was spent; it is not aware of the investment in student aid. It is mainly aware of tuition increases."

The government also had a policy agenda for university research which was not part of the mandate of the Rae review. The policy focus was on 4) research and commercialization and value-for-money for university research funding. This focus on research and commercialization was also becoming more apparent at the federal level. Another ongoing accountability concern of government and the AuditorGeneral and was 5) administrative efficiencies in universities to reduce costs and free up resources for 58

direct academic activities. Accountability was therefore a goal in itself, but was also being framed in terms of achievement of other policy goals of the government. These policy goals, which are discussed below, all contribute to a government agenda that seems to reflect a NPM philosophy that government investments must show value for money to the economy and to taxpayers which must be demonstrated through measurement of outcomes and reporting on these outcomes. At the same time, voluntary accountability mechanisms began to grow in the sector partially as a counteraction to new government accountability mechanisms.

1)

Policy Goal: Improving Accountability

From the review of the secondary literature and interviews with stakeholders indicated that reporting requirements through the Minister was expanding over time as more targeted funding envelopes were set up. Interviewees all agreed that this reporting needed to be re-visited for rationalization. All the accountability reporting previous to Rae, remained in place subsequent to 2005. A former government official noted that the evolution of the accountability regime occurred every time new funding was introduced, "elaborate reporting requirements for that additional money have been developed and old requirements have never been sort of eased." Based on the Rae report, additional reporting through Multi-Year Accountability Agreements (MYAAs) and new funding envelopes were set up to achieve the policy goals.

Reporting Requirements for grants A 2006 review of reporting requirements of universities by a Joint MTCU/COU Working Group on Reporting Requirements (WGRR) found a considerable number of reporting and audit requirements, which institutions found burdensome and costly in administering in terms of time and money including payments of fees to accounting firms to audit very small transfer payments. In addition to annual financial audits, there were at least forty reports and two external audits for all institutions. Some programs require multiple reports per annum which could result in up to ninety reports and four external audits per annum. Appendix 3 and 4 for details the extensive reporting required by MTCU which sometimes duplicates reporting. This does not include reporting requirements to other provincial Ministries and agencies (Ministry of Health, Ministry of Labour, Ministry of the Environment, Privacy Commissioner, etc), federal government granting councils, federal agencies and private donors. A university representative noted that "on the accountability side of things there has to be a balance between the cost of collecting information and the importance of the information in being able to ensure that institutions are operating

59

in an accountable fashion." This issue of the costs of over-reporting was previously raised by the Investing in Students Taskforce in 2001.

A university representative noted, "the amount of effort and reporting mechanisms set up in Ontario right now is inversely proportional to the amount of money being allocated. It is absolutely bizarre the amount of reporting and details that you have to do for 100 000 dollars on some of these grants versus on the basic grants which is really driven by our audited enrolment." In the 1990s under the Harris government, funding to universities was cut while university accountability to government was strengthened through the Minister and MTCU, more funding envelopes, performance funding and key performance indicators were introduced. Under the McGuinty government funding went up and new accountability requirements were also introduced to justify the expenditure of the incremental funds.

A HEQCO representative did agree that the financial and auditing requirements could be as minimal as possible, given that other reporting requirements were being introduced. Effort had to be made to reduce the financial reporting requirements without compromising accountability. On the other hand, an academic/administrator suggested, the possibility of there being an integrated form of accountability that is quality oriented and not activity oriented, outcomes oriented rather than process oriented", instead of the current system which is "a kind of patchwork of accountability measures" may not be feasible. He suggested it could be the existing system with some reduced reporting might be the best approach given the complexity of issues involved ­ teaching, research, quality assurance of programs, etc.

Multi-Year Accountability Agreements (MYAAs) In 2005, MYAAs for universities were introduced in the government's Reaching Higher Plan (RHP) along with performance reporting to account for use of the multi-year funding pledged in the 2005 budget and achievement of government goals of access, quality and accountability. This was the first time multiyear funding was provided by government to universities and colleges. The MYAA was essentially a performance contract between government and each institution. In previous years, the main focus of the broad accountability framework before the Reaching Higher Plan was on the financial aspect, that is, compliance auditing and later value-for-money auditing and reporting that money was spent appropriately. The MYAAs introduced a new element in accountability in terms of multi-year funding and performance evaluation regarding the core business of a university which focused on the academic side - learning, teaching, research and knowledge creation. This was a major challenge for government

60

but they believed universities had to be accountable for performance in terms of each institution-specific mission as well as broader government policy agenda. This attempt under Reaching Higher to get the universities to align their priorities with government priorities for PSE was fairly new for a government.

The McGuinty government's "approach of collapsing system level accessibility, accountability and funding goals into one mechanism in the form of MYAAs, may have strengthened the influence of government priorities on institutional behavior" [Shanahan, 2009, 7]. The MYAAs and report-backs were posted online for public access. Each university president had to sign a MYAA which committed each university to complete multi-year action plans to provide report-backs for every year of the RHP years (2005-06 to 2009-10), that demonstrated to government, how an institution had met its objectives in the plan and contributed to overall PSE system goals (access, quality, accountability) and the government's policy agenda, using institution-specific qualitative/quantitative inputs and outputs measures. The first year (2005-06), an interim agreement was signed, followed by the 2006-07 to 2008-09 agreement, and extended to 2009-10, providing specific operating funds for each institution. Some allocations were withheld until report-backs were received by MTCU. A university representative noted, "I don't think that anybody feels that those are accomplishing what it is they are intended to accomplish". The MYAAs also only applied to the incremental new money under Reaching Higher and not the overall operations of the universities. For this university representative, "it makes no sense to go through this elaborate ritual for something that does not actually deal with the entire operations of the university". For the university representative, there seemed to be a focus on accountability for incremental funding and incremental performance measures -for example, the increase in faculty numbers, the increase in numbers of underrepresented students ­ when the government should not be focusing on $50 million incremental dollars but asking instead "What are we getting for the $3 billion we are spending on the university sector?"

There is an ongoing dialogue with both the colleges and universities and the government believes, "we are in agreement we are all pointing in the right direction and we are all starting to think alike which is really scary". According to a Ministry representative, the government wanted to " move the yard stick" with regard to targets and outcomes; however, a university representative noted that institutions were wary of providing stretch targets which if not met, might incurr penalties. He noted further, given the current economic environment, if there was going to be a next round of MYAAs, institutions would be very hesitant about the contents. Plans would have to be flexible given the changing times where there was grant cuts, changes in tuition policy, drop in endowments, etc. Institutions would have to determine

61

priorities and how they were going to track whether or not they were improving and how they would achieve objectives.

The MYAAs and report backs were additional reporting requirements that were layered on the existing accountability requirements. A government official noted that the intent would be to "roll up a lot of the reporting into the MYAAs" depending on what the final performance measures were but this did not mean that reporting would be brought down to a manageable level. The government officials agreed that rationalizing reporting would take time and "many of our reports are very specific and the government's interested in the data that we get in these reports so we are not quite ready to let go of that." They did note that they were open to looking at where the best data was coming and use that as the main reporting mechanism, and it may not necessarily be through the MYAA but those areas still had to be identified.

The universities did not agree that the MYAAs should be used as a data collection mechanism. According to a university representative, the data was not consistent as universities were not required to use the same definitions and formats. They also tried not to put quantitative information or quantitative targets as much as possible in the MYAAs but focused on qualitative data. Report backs had been problematic because when not all goals have been achieved, there were a lot of questions from the Ministry. Some institutions set stretch targets for themselves and then were unable to achieve them and then the Ministry wanted explanations. The MYAA process was seen as punitive so "if you miss a target suddenly it could almost be the inquisition", thus there was no incentive to put in stretch targets for improvement. This created a tension between the Ministry and universities that did not exist before. According to a university representative, "the MYAAs and report backs have become time-consuming and a member of the public looking at one would be totally flummoxed but there is a lot of room for improvement." He noted, "There's so much information and it's so diverse that it's very difficult for anybody to make any sense of what's going on". The other aspect of the MYAAs, a university representative noted was that, "they were designed to tell good stories. And the good stories are hard to come by when the operating grant is flat lined and student numbers are up".

A government official noted that the MYAAs were introduced because the government was looking for ways to report on progress. They were supposed to show the results and return on investment for taxpayers from the RHP investments. This was a reflection of the results-based management strategy implemented by the province. A government official noted that "we are redefining our relationship with the institutions." A university representative noted the MYAA did not turn out as expected by some in the 62

university community. He agreed that universities needed to report on achieving government objectives but it "if those objectives are a very small portion of what the institutions plans are it should be acknowledged." The government noted that the MYAAs were "... viewed in most of the Ministry as a cornerstone piece and it's what supports access and quality and making sure that we address those two key issues." They made it clear that the MYAAs were not perfect and part of an `evolutionary process' which required collaboration with the sector to improve. Total agreement from the sector was not required as long as their input was constructive in providing feedback on, ... what is doable, what makes sense in their context, what makes sense in terms of their mission and mandate and working closely with HEQCO in terms of what kind of information we need to proceed and working closely with gov't to understand what kind of needs the government has to ensure itself that its getting value for money and that the institutions are accountable for the money we give them. They did note that they were not sure the information received back from the MYAAs was the right one for good decision-making since, "maybe because we weren't as clear or specific or as well defined as we could be".

The faculty representative noted that government was looking at accountability in a different way through the MYAAs. Previously when government's share of university funding was even larger, the assumption was made that money would be well spent and there was no attempts at quantifiable measures until the KPIs of the Harris government. Faculty was completely opposed to the KPIs but not accountability, that is, "accountability measures had to be real and they had to measure what they purported to measure." According to the faculty representative, "the Liberal government wanted universities to define quality and faculty wanted the government to define quality and in the end nobody did." The sector wanted government to say what it wanted from the universities and then measures could be discussed, but from the findings they were making an assumption that the government was clear on what it wanted, when it was not. According to him, the Liberal government's approach to accountability through the MYAAs, "was about trying to satisfy its own political objectives. It was a political exercise rather than a real one", which has caused distress within the sector. It created splits in the universities between administrators and academics. The government expected the university administration to have broad-based discussion on MYAAs, but the feedback from faculty across the university sector, according to the faculty representative, was that this did not happen.

63

If Senate was consulted, it was felt that was sufficient, but according to the faculty representative, as part of the corporatization of the university, the Senate has lost power over the years and "academic decisions are being defined as business decisions and those are being made by the Board." Some Senates are now seen by faculty as tools of the administration that "represent the corporate interests rather than the academic interests". Faculty had no commitment to MYAAs since it was not a democratic process: "The CEO does not discuss his plans with the person on the floor". For the faculty representative, corporatization was arising out of under-funding of institutions. The impact of having to use endowments in the face of declining funding and go out to the private sector meant the role of the president has changed to CEO. There was a change in the view of the universities, "where universities are seen as a tool of the economy in terms of supplying labour, in terms of supplying research, and in terms of corporate spin-offs."

According to the student representative, students felt there was not enough accountability to them by the university administration. The role of the student over the past 15 years, a student representative noted, has been changing where students have to pay more tuition fees and are treated like consumers instead of partners or members of the community. This reflected the corporatization of the university and the student representative felt, "accountability should not be narrowly framed around consumer rights." This increasing corporatization had been an evolutionary process and was occurring internationally. This could explain, though, why student expectations had changed in terms of being more involved in university operations as they contributed more to the operating budget.

The government's next steps were to review and analyse the report back for the next generation of MYAAs including dialoguing with the sector. It also asked to HEQCO to provide advice on system wide measures, targets, defining `quality' and `access' and accountability frameworks. They indicated that the targets of the MYAAs had to align with the performance targets of the entire province as part of the `Knowledge and Skills Strategy.' The question now remains, "Will government continue to provide multiyear funding which was welcomed by universities to allow for proper strategic planning for new programs, hires, etc?" The current financial crisis which started in 2008 and the economic recession could provide some challenges for government to provide multi-year funding beyond the RHP.

2)

Policy goal: improving access to university education

The Ontario government had indicated the need to increase the university educated population, which was a cornerstone for economic development and productivity improvement. The 2008 Budget indicated a

64

target of 325 075 full-time undergraduate and graduate students for the 2008-09 academic year with an assumption of a 1% change in enrolment change. The RHP did achieve its goals of increasing full-time enrolment through increased funding in the 2005 budget. Enrolment for both undergraduates and graduates went up. Actual FTE enrolment in 2008-09 (all terms for undergraduates and Fall/Summer for graduate students) stood at 363 867. (See Fig. 1 on page 30) Full-time enrolment increased from 35% in 2002-03 to 39% in 2008-09. However, graduate targets were not initially met by some universities and they were aiming to meet those targets before the RHP funding period ended in 2009-10. Universities were able to deliver to Government the creation of more spaces and an increase in enrolment as indicated in their MYAAs and in Fig. 1, although for some goals for increase in graduate enrolment were not quite met. However, the demand for undergraduate spaces were underestimated and funding had been inadequate to continue to meet current and projected enrolment demands.

The Ontario government indicated concern about rising costs (tuition fees and accommodation costs) as a barrier to accessible university education and asked for commitments from universities to participate in a Student Access Guarantee (SAG) to provide low interest loans to students so every eligible, desiring a university education could have access to one; a familiar access policy goal from previous government administrations. The SAG was in the process of being worked out so the impact would have to be assessed in the future. Tuition regulation by the Ontario government required tuition guarantees from universities as a part of the accessibility requirement. Institutions had to practice restraint in tuition increases since fees impacted access to a university education, especially by disadvantaged groups. Universities had to report back on compliance with the SAG.

Improving access for under-represented and disadvantaged groups Another feature of the Rae review and the McGuinty PSE policy was a focus on university access for under-represented and disadvantaged groups. Universities were expected to identify strategies to improve access to disadvantaged and underrepresented groups such as person with disabilities, aboriginal peoples, French language and first generation students (families where the parents of a student did not attend university) and report back through their MYAAs and other reporting mechanisms. While university education had expanded over the decades and had become more accessible to the general population, government had recognised the failure of certain groups in society to sufficiently access higher education. Separate funding envelopes were provided in the past and new funding was provided for persons with disabilities, French-language students and aboriginal peoples. First generation student enrolment was also included as a targeted group for whom access should improve. Targeting under-represented groups was

65

not a totally new accountability requirement but there was more policy emphasis and institutions had to track and report back on numbers enrolled. There has been no system research yet on whether universities have been successful since 2005 in increasing student populations among the above-mentioned groups. Some more work is needed in this area to develop a system measure to track participation.

The issue of access to PSE by visible minorities was not clearly articulated by the Rae report nor was it articulated by the Ontario government as being an issue for accountability. In Ontario, the issue of visible minorities does not seem to be a major concern, generally, since large proportions of the undergraduate and graduate population are visible minorities but further research is needed to assess which specific ethnic/racial minorities in Ontario are not accessing PSE. The issue of access to PSE for newcomers was also not raised in the Rae report, of whom many are denied recognition of their university credentials by Ontario universities, and are required to re-take courses and programs they have already done in their countries of origin. Not much has been done by government to hold universities accountable in this area of PSE. The government, clearly felt that more could be done to improve access for disadvantaged and universities did not object to that policy goal. What the universities objected to was the split out of funds into targeted envelopes which increased administrative costs and sometimes duplicated reporting.

Unlike previous regimes, one of the access goals for the McGuinty government was the expansion of graduate enrolment. Accessibility, in the past, was framed in terms of undergraduate education since there was no high demand for graduate education [Trick, 2005, 197]. The RHP introduced specific targets and funding envelopes for masters and PhD enrolments for the system and each institution established their own graduate targets.

3)

Policy goal: improving quality in university education

As recommended by Strategy 2 in the report of the Rae Review [2005, 2], the Ontario government focused on quality improvement as a key goal for higher education and clearly communicated this to universities through the RHP objectives. For 2005-06 to 2008-09, $619.4 million was allocated by government as `quality funding'. Clark and Trick noted that the focus on quality in higher education "has been nearly invisible in the Ontario government's postsecondary education policies" [Clark and Trick, 8]. It was only mentioned briefly by OCUA in its 1995 memorandum to government, discussed earlier. The sector (government and universities) was consulting on the appropriate measures for improvement in

66

education quality. This focus on accountability on the academic side is fairly new stemming from the Rae review.

The RHP did not make the distinction that there were two aspects to quality in higher education ­ quality assurance of programs and quality in learning and teaching. The former evolved as a self-regulatory process in Ontario and the latter was really the basis of the quality agenda in the RHP. Harvey explained it as quality in higher education being concerned with "the nature of learning" and "quality assurance is about convincing others about the adequacy of the processes of learning" [Harvey, 2008, 11]. Harvey also noted that quality assurance processes were usually disconnected from quality in teaching and learning processes, and this was demonstrated in the case of the RHP. The RHP and MYAAs showed this disconnect as it focused only on quality in terms of learning and teaching. Government had expectations for universities to improve the student experience through the hiring of new faculty to assist in improving the student-faculty ratio, improving learning outcomes, graduation rates and retention including the provision of adequate student support services to improve the quality of the student experience. In terms of actual changes since the implementation of the RHP for PSE, universities indicated the difficulty in trying to demonstrate `a quality effect' from the RHP over the short period of two years (2005-06 to 2007-08) in their report-backs, especially since the level of funding was only sufficient to address enrolment demands. Some new hires had been made but with the retirement of professors and increase in enrolments, indications are there were not significant changes in the student-faculty ratios since faculty had also retired, so the net impact of new hires was less than expected. Some research needs to be done in this area to see the net impact of the Reaching Higher Plan funding on quality. Responsibility for `quality of learning and teaching' and `quality assurance of programs' lie primarily in the hands of the university sector and are discussed more extensively below.

Academic accountability for quality in Teaching and Learning Universities have always been autonomous and independent from government but academics see increasing accountability requirements for quality improvement through the use of performance indicators in the academic arena (as opposed to the administrative area) as an encroachment on their academic freedom and university autonomy, which they felt was necessary for the independent creation of knowledge. The interviewees were asked if they saw any conflict between `university autonomy', `academic freedom' and accountability. One university representative noted that they did not need to be in conflict but "one always has to achieve an appropriate balance." He noted that accountability was also not just an issue for universities to account to government but was multi-dimensional. Government had to

67

be accountable to students for proper funding when government has increased accountability measures over the years while decreasing funding. Since the 1960s, the issue of trade-off between access and quality in education in a scenario of limited funding has been raised by institutions and PSE review commissions.

Academic freedom had not been threatened by any accountability measures proposed by either the government or universities, as far as this university representative was concerned. A former provincial politician noted that the independence of universities and "the freedom of individuals to pursue their own interest and their own knowledge and their own scholarship" make it difficult to impose a rigid or centralised accountability regime. He believed there is a tension and potential for conflict between accountability and autonomy/academic freedom as government focuses on results and assessing overall how and what institutions are doing in a way that allows the province, to know what the return is to the public. An academic/administrator noted "there has to be some sort of balance struck between giving people enough freedom, giving universities enough freedom to operate, to be innovative, to develop, to grow, to not be restricted to single sets of ideas while still being accountable for the money they receive."

The Rae report noted, "Quality standards and measures need to be reviewed and further developed at the sector, institution, program and student level to improve the quality of the student experience and success" [2005, 30]. Ontario universities, through OCAV, have indicated, in principle, a shift from a faculty-centered paradigm to a student-centered paradigm focusing on learning outcomes. The Council of Ontario Educational Developers, a new COU affiliate to OCAV was formed to provide advice and training on teaching and learning issues. Each university has Teaching and Learning Centers and this affiliate would voluntarily coordinate the work on teaching and learning issues to develop programs on a province-wide basis on specific aspects of learning and teaching. OCAV also has formed a Committee on Quality Assurance and Teaching & Learning. OCAV organized workshops across Ontario campuses for Academic VPs, Teaching and Learning Developers and other staff. The UPRAC review and audit process would have to change to incorporate these new expectations. This was a voluntary initiative that academia had taken without the need for any government intervention. This purported paradigm shift is fairly new and it will be interesting to see if there is any real impact on student learning outcomes in the future.

The academic/administrator noted that structural changes or system changes in the promotion, tenure and renewal processes and induction of new faculty and orientation of them was important in aiding the paradigm shift. For example, one could require new faculty, to take a one semester course in university 68

teaching. He noted, one also had " to change the internal rules in the institutions, the reward rules, the promotion, tenure, renewal rules so that they are explicitly linked to teaching excellence." Most universities have those provisions such as in collective agreements, policies, senate bylaws but " in some instances, they just need to be given greater weight or recognition." But he noted change in the learning and teaching culture had to happen at the system level, individual institution level, faculty level, department level, and individual faculty member level and cannot be imposed from outside but must be intrinsically part of the culture of universities. He also noted that "there is lots of room to increase the quality of teaching, the quality of learning in our institutions". One important way to improve quality is to increase faculty hires and the student-faculty ratio which would contribute to better student engagement but he acknowledged there were other tactics and strategies to improve quality "which isn't just adding resources, just adding space, or adding professors, or providing more equipment". One key area for improving quality was the use of teaching only professors who were committed to excellence in teaching. The academic/administrator noted that they should have certain qualities including a willingness to reflect upon and improve their teaching; understand the scholarship and research of learning and teaching and how to implement it and even how to carry it out in respect of their own teaching; and sufficiently expert in the field in which they were teaching. He did acknowledge that some government oversight was not unwarranted in that government needed to be satisfied that self-regulation was sufficient. Universities had good practices in place, were vigilant, and mindful that they were actually doing the necessary assessment and evaluation, monitoring and improvements to improving quality in learning and teaching. He noted that change was happening but it was progressive over the last decade, not immediate change. The government tended to look at change and results in the short-term over a year to five years until the next election rather than over a period of years but changing academic practices to develop a culture of quality required time.

Another university representative noted that academic accountability had to be done through a peer system not by bureaucrats. He suggested that Ontario would not want to set up a system like ACQUA in Australia which is "pretty intrusive." Academics, he noted, evaluated students and evaluated research, so there was no reason to involve people from outside the academy. He felt they could evaluate their performance through best practices to determine quality, not value for money. This demonstrates the tension between current models of accountability in academia and NPM values and principles. He explained the difference between performance measurement in administrative accountability and performance evaluation in academia by noting,

69

The problem is we measure what's measurable. I'm not sure that the process should be one where you are expected to be able measure it. The process should be one where you are expected to be able to evaluate it. I think there's a difference. Academia did believe that one can quantitatively measure quality in PSE. As noted by Shanahan, "While defining `quality' is problematic, measuring quality is fraught with problems of accuracy and adequacy" [Shanahan, 2009, 14]. The faculty representative noted that "Quality is not a public issue at all. It's a government issue." Academics were wondering what was not being done in terms of quality that needed to be done given the level of funding. "The idea that you have to measure something to death to arrive at an obvious conclusion just seems to be more of a political process than a meaningful policy process." According to Lee Harvey, "Quality in PSE can be thought of as the improvement in learning outcomes and ultimately final outcomes (such as earnings, health, life satisfaction, civic engagement) associated with the PSE experience" [Harvey, 11]. The problem is that those final outcomes cannot be measured or traced back to any specific institution. This is the reason for the concern with measuring outcomes in academia since the benefits graduates derive and society derives from a well-educated citizenry are much more intangible and long term and beyond measurement.

Self-regulation and accountability for quality assurance and system review The quality assurance piece of universities was virtually ignored in the RHP. As discussed previously, other jurisdictions have a much more active role by government in quality assurance. While in the earlier years, OCUA had some role in quality assurance, today government has totally withdrawn from quality assurance for university programming, which is a contradiction to the increasing call for accountability in all other aspects of universities business. This section discusses the latest development in quality assurance in the sector as it is a key part of the accountability regime of universities, even though it is self-regulated but it is important for demonstrating quality to students, both locally and abroad.

Ontario universities undertook to revise the quality assurance process for degrees, which were based on the recommendations of the Van Loon report, a study commissioned by the COU in 2007 to review the quality assurance process. The universities themselves felt there was a need to revise the processes of the OCGS to be less bureaucratic and more in line with other jurisdictions worldwide. These proposed changes would speed up program approvals. A university official noted that the commissioning of the Van Loon Report and the review of the quality assurance of programs process currently underway were totally unrelated to the Rae Review and any recommendations made to government.

70

There had been three major reviews of the quality assurance process for university programs - in 1985-86, 1998-99 and 2006-07. The first two reviews found no need for major changes but the last one did: "The current OCGS process, while venerable and rigorous, is quite unusual compared to university quality assurance processes elsewhere in the world" [Van Loon, 2007, 5]. The idea was to make the undergraduate and graduate processes similar, which was the general practice in many other jurisdictions, "where responsibility generally rests with the individual institutions which are then subject to periodic audits to evaluate how well their own internal processes are operating against a set of publicly available guidelines" [Van Loon, 5]. The program appraisal process is currently under review by OCAV through a Quality Assurance Transition Task Force which is proposing the creation of an Ontario University Quality Assurance Council (OUQAC) supported by a Quality Assurance Secretariat 14 which will be independent and assume the quality assurance duties of UPRAC and OCGS. This bears no relationship to HEQCO, which was set up to conduct research and provide policy advice to government on access, quality and accountability. HEQCO has no role in quality assurance of university programs. According to its terms of reference, the Task Force activities involved: · Defining new guidelines for the appraisal and approval of new graduate and undergraduate programs, using existing UPRAC Guidelines and OCGS By-laws and Procedures as a guide. Guidelines will include criteria and guidelines for the review of the provisions for student academic support. Defining guidelines for processes and procedures for institutional reviews of existing graduate programs and undergraduate programs based on current UPRAC procedures and the principles in the existing OCGS by- laws and guidelines but not limited by them. Guidelines will include criteria and guidelines for the review of the provisions for student academic support. Defining guidelines for the audit of: i) compliance of institutional review policies and processes with the Ontario University Quality Assurance Council (OUQAC) guidelines for institutional graduate and undergraduate program reviews; ii) compliance of institutional program review practices with institutional review policies; and iii) quality assurance processes for student academic support services. Proposing an arrangement for the periodic audit of the OUQAC quality assurance enterprise. Consider and advise COU on the requirements of a Quality Assurance Secretariat in light of the existing structures and experiences of the OCGS and UPRAC secretariats and the phases of a transition to the Quality Assurance Secretariat.

·

·

· ·

The proposed changes came about as a result of the recommendation from the 2007 Van Loon Report which found that the process for approval of new programs was cumbersome and slow. 15 The new undergraduate programs would have an external review process similar to graduate programs. The process would stay the same for existing undergraduate programs and new graduate programs. The process for existing graduate programs would be modeled after the undergraduate process for existing 71

programs, where reviews were institutionally based and external reviewers would be appointed by the Senate of the institution concerned. The goal was to make the approval process more efficient and effective but with more institutional autonomy. Key issues for the task force included: · · · · How to treat core and non-core programs. A core program does not require government approval for funding but a non-core program does. What is the percentage of change in a program to make it `new'? When will a program require an external reviewer? If a program is found to be of poor quality through a periodic review by an external reviewer, currently through the OCGS process, the university has agreed the program will not be offered. How will the university be held accountable under the new system when its reviews are received by its Senate instead of an external body? How will the system, government, students, know recommendations will be implemented? The new approvals process was supposed to be in place by 2010. The key accountability question is "How does a self-regulated system have the same rigor as an externally or government regulated system?" Lee Harvey noted to an OCGS workshop 16 that the Van Loon Report tried to include too many elements of quality assurance from other jurisdictions. He suggested the revised quality assurance system be kept simple and focus on the key issues of concern. In moving forward with changes in Ontario university quality assurance, the university sector may want to bear in mind provincial and federal government concerns related to: harmonizing quality assurance mechanisms (policies and procedures; standards; regulation; processes ­ assessments, audits, accreditation; evaluation) across Canada to allow for credit recognition and transfer among provinces which is still problematic; and, internationalization and attracting foreign student enrolment which is impacted by the higher education quality assurance system (s) in place in a country.

A university administrator cautioned that the new quality assurance process could be costly for universities to implement if not done right, given the breadth and number of undergraduate programs, where definition of a new program and the expectations around quality assurance become very important. He also raised the issue of possible standards variation for graduate programs. He was also concerned about the creation of an undergraduate bureaucracy in each institution which would add to internal bureaucratic processes. He did not think there was a need for graduate and undergraduate quality assurance process to be the same and a new process should not end up costing universities more. He felt

72

that while there might be some operational concerns about OCGS, the solution is not to replace it with an entirely different process.

An academic/administrator noted that, "only experts can review expert work" and in the same manner that professionals regulate their own professions, universities were well-placed to self-regulate including utilizing peers to audit the quality assurance process. The implication is that there was no need for State intervention in quality assurance. Other countries and provinces vary in the degree of government involvement in quality assurance university programs, as previously discussed. Quebec is also selfregulating for program quality and Ontario was modeled after the Quebec system. Government seemed to have no desire to be involved in quality assurance, but is aware of the proposed changes and a government representative indicated any new changes must not decrease the rigor of the quality assurance process.

Another quality initiative was the creation of a document, `Guidelines for University Undergraduate Degree Level Expectations (UUDLES)' by OCAV, which was endorsed by COU in December, 2006. This document was created in response to a national initiative to state degree expectations and is "... a framework to reflect expectations of performance by the graduates of the Baccalaureate/Bachelors programs of Ontario's publicly assisted universities" [OCAV, 2006, 1]. Universities in Ontario uses the Guidelines to develop their own degree level expectations in line with their own "mission, ethos, values and culture."

Role of the Higher Education Quality Council (HEQCO) in Quality and Accountability Perhaps the most important institutional indicator of the growth/expansion/complexity of the evolution of the accountability regime for the university sector was the establishment of the Higher Education Quality Council (HEQCO) in 2005. In the mid-1990s, the Rae government's aim to have the Education Quality and Accountability Office oversee accountability and quality in Ontario's universities came to nought. It took more than a decade to establish an agency to set up organization for higher education which was partially done by OCUA in the past. While OCUA was a buffer agency between government and the sector, HEQCO would be more research and advisory. OCUA also had some role in quality assurance monitoring which was taken over by COU.

HEQCO was established as an independent crown agency as a result of a Rae report recommendation. It had its own enabling Act ­ the Higher Education Quality Council of Ontario Act, 2005, 73

to monitor and report on performance measurement in the area of quality, and develop the critical knowledge base necessary to guide the system towards improved quality" and to "... assist the Minister of Training, Colleges and Universities in improving all aspects of the postsecondary education sector, including improving the quality of education provided by colleges and universities, access to postsecondary education and accountability of postsecondary educational institutions [Public Appointments Secretariat, 2009 17 ]. It is an advisory body to Ministry of Training, Colleges and Universities on the key policy issues of access, quality and accountability. It is fully funded by the province, and accountable to its own Board of Directors appointed by the Lieutenant-Governor and has a small research Staff to carry out the work of the Council. Its functions are to conduct research and provide recommendations to the Minister on targets, performance measures, design of PSE models, to evaluate the system and report to the Minister and other related matters. The Act forbids senior PSE institutional representatives from serving on the Council. It has no direct authority over higher education entities.

HEQCO establishes an annual research plan and has been soliciting feedback from the higher education sector through workshops and surveys as well as commissioning research on various quality issues on PSE. The government has input into HEQCO's annual Review and Research Plan which centers around four themes: participation/accessibility; educational quality, system design and accountability. The Research Plan reports on findings from HEQCO's research activities and updates the Council's research priorities and planned activities. Government requires universities to cooperate with HEQCO to provide data for research on `quality' in higher education. This policy was based on Strategy 7 of the Rae Report which requires commitment from governments and institutions "to continually evaluate and review progress" [Rae, 2005, 35]. HEQCO's focus is on the academic side of accountability, which is something new. The main focus of university accountability in the government, thus far, had been on administrative issues. HEQCO is now engaging the university community to conduct research on higher education with regard to improving quality in learning and teaching, as well as student services. A university representative agreed with the current role of HEQCO as a research entity and preferred it did not get actively involved in policy-making and operational matters. But he noted that the government seemed to want HEQCO to be more proactive and provide policy advice as it moves forward on its policy agenda. For instance, HEQCO was asked to review the first generation of MYAAs and the Student Access Guarantee policy which was implemented under the Reaching Higher plan. The university representative also found the research coming out of HEQCO to be useful in clarifying the issue around accountability in terms of quality ­ student experience, student advising, learning and teaching, etc.

74

During the Rae Review consultations, some in the university community recommended against the set up of a post-secondary advisory body that was independent of MTCU. It was felt that this extra layer of bureaucracy was not necessary to accomplish the goals set out by the Rae Report and better use could be made of government funds. Both the government representatives and the university representatives interviewed for this study, noted that this stemmed from a fear that a body similar to the defunct OCUA, would remove the direct working relationship that university representatives have with the Ministry. But HEQCO, according to a university representative, " hasn't created any problem in the governmentuniversity dynamic." The Board should be representative of the community it was researching including students and staff of colleges and universities, parents and administrators but the Board is comprised differently. The Research Advisory Board would allow input into the work being done and allow the stakeholders to talk to each other. Government's view of HEQCO was different which was for it to provide quick policy advice while HEQCO thought the advice should be research-based to come up with a meaningful solution and that took time. The faculty representative noted that the academic community supported the set up of an arms-length body during the Rae review since "the Conservative government decimated the policy capacity within the Ministry itself and handed over a lot of the policy function to COU." He noted that given that the government had no obligation to take HEQCO's advice, "the concern is that an arms-length body can really become an orphan, disowned by government and disowned by its stakeholders." He noted that stakeholders must feel that HEQCO was an important vehicle for them otherwise "HEQCO will be cast off by government".

HEQCO was also working with academia in universities through OCAV on a few discipline-based pilot projects with the intention of doing more based on outcomes. The aim was to look at the degree expectations process and audit process to see how one could have expectations which were supposed to be part of a program actually translate into learning outcomes. A HEQCO representative did not see this agency as "actually taking operational responsibility for quality control". It did not have the staff or expertise to do that task and so that remained with the sector itself through OCGS and COU to manage. It saw its role as giving advice to government based on policy research. A university representative, however, noted while the academically oriented research output from HEQCO was useful and needed, "it is not set up to do policy research and provide policy advice to government."

The HEQCO representative saw the relationship of universities to government as a principal-agent type relationship. This language stems from business management theory: "The central dilemma investigated by principal agent theorists is how to get the employee or contractor (agent) to act in the best interests of 75

the principal (the employer) when the employee or contractor has an informational advantage over the principal and has different interests from the principal" [Sappington, 1991]. Principal-agent theory sees a relationship in economic terms and defined by a contract, in this case the MYAA. It is unclear whether the Higher Education Quality Council could serve as an external auditor of the entire process of program approvals in Ontario universities, once the new process is in place. HEQCO could be involved in the periodic audit of the OUQAC quality assurance enterprise and a representative from HEQCO has indicated that "I could see us being auditor of the auditors." The HEQCO representative indicted that while it did not have the in-house expertise, it could be done "by periodically bringing in an internationally recognised team .... to evaluate the evaluation processes" to see if the content of undergraduate or graduate programs, and the processes are reflective of international best practices. HEQCO could commission international experts to do a report and then transmit the report with comments to COU and the government and the public. A university representative, however, did not think HEQCO would need to have a role in quality assurance. He felt either COU handled all quality assurance or HEQCO would have to take over the whole task of quality assurance. It remains to be seen what the legacy of HEQCO will be and its own value-added to the accountability and performance measurement debate in PSE in Ontario. Some interviewees also hoped the agency would not expand to being a large bureaucracy, to the extent of the TEMU in New Zealand. HEQCO could do a lot more, as far the academic community was concerned, including having more of a consultative approach. A HEQCO interviewee also agreed that reporting requirements dealing with financial compliance for universities could be rationalised, but noted that was something on which the auditor-general could advise since this was not within the mandate of HEQCO. As it evolves, some stakeholders hope that HEQCO will broaden its research agenda and provide critical policy advice to the sector that might not necessarily coincide with what the government of the day may wish to hear.

This section on quality demonstrated that only part of the quality issue is being directly addressed by government in its accountability agenda. The quality assurance aspect was left entirely up to the sector to self-regulate since the government had neither the willingness, nor personnel nor financial resources to take over quality assurance in PSE. Governments in other jurisdictions discussed earlier do take a more active role in quality assurance but there was no interest shown in Ontario to follow suit.

4) Policy Goal: Value-for-Money through Research and commercialization In discussing the impact of NPM and an accountability regime for universities, one cannot forget the research side of the university mission. Academics as researchers are being asked by government to 76

demonstrate value-for-money through their research in terms of commercialization and the economic benefits to be derived therefrom. This was most apparent in jurisdictions like Australia, the UK and New Zealand where researchers' performance were measured and their institutions rewarded with more funding or where they were becoming more dependent on private sector funding for research which has implications for research ownership and accountability relationships. Ontario has only more recently started focusing on commercial research and the role universities can play in economic development through research and development. This section explores the increasing policy focus on commercial research in the university and the implications for basic research, teaching, researchers and the university.

Research funding to universities was traditionally the purview of the federal government but provincial governments over time have developed a role in research funding focusing on downstream activities. Canada lags behind other OECD countries in private sector investment in research and development. In the shift from Fordist to post-Fordist economies, more and more research and development has been has been outsourced to universities. The government saw the university sector as being crucial in research and development for economic development. The Ontario Centres of Excellence (OCE) were created in the 1980s, according to the OCE website, "to strengthen research linkages between academia and industry". The provincial government introduced the university incentive research fund which was intended to link up universities with private sector to commercialize research. The provincial government also implemented the Research Overhead and Infrastructure Envelope (ROIE) in the 1980s to fund some research costs not funded by the federal government's research granting agencies. Since 2001, the Ontario provincial government, as has other governments, has re-focused its effort to create a strategic direction for research & development and its role in economic development leading to increased calls for more commercialization and demonstration of value-for-money for research dollars. Recommendation 25 of the Rae report suggested a stronger role for the provincial government in research including the set up of an Ontario Research Council to advise on research priorities and funding, in partnership with the federal government, where required. [2005, 89] Rae, however, did not focus on research and commercialization as being a priority for universities.

In 2005, the McGuinty government established a Ministry of Research and Innovation (MRI) reporting directly to the Premier. The establishment of MRI came out of the 2005 provincial budget and is aligned with the Reaching Higher Plan. This two-tier level of policy and research funding is creating "a pervasive sense of overlap, duplication and spillovers between the two levels of government" [Wolfe, 1995, 2]. The same funding projects now require interfacing with two levels of bureaucracy. The Ontario Council on University Research (OCUR) had been recommending that both federal and provincial governments have 77

the same application and accountability requirements to reduce red tape. In 2008 an Ontario Innovation Agenda (OIA) was established whose priorities for funding did not always coincide with the federal government's priorities. This affected university research funding, for instance CFI funding was no longer automatically matched by MRI unless it met Ontario's research priorities. Also for national networked projects where more than province, institution and funder were involved, the provincial government did not need to be supportive if it was not on the agenda of the Ontario government. This increasing conflict and multiple accountabilities to federal, provincial and private funding posed serious challenges for researchers who were being asked to be provide more demonstrated value-added to the economy. The provincial government recently reviewed its basic research funding programs and was in the process of reviewing its commercialization programs to make sure they were aligned with the OIA.

Different funding schemes were set up to encourage commercialization and Ontario universities were seen as having a major role to play in innovation and competitiveness. The dominant opinion in the academic community was that the focus on commercialization was "a distortion of the core university mission". The Ontario Research Commercialization program was set up in 2005 as "a three-year, $31.4 million project aimed at accelerating the movement of university and hospital research to the marketplace." 18 The 2008 Budget also announced a "10 year tax exemption for new corporations that commercialize intellectual property developed by qualifying Canadian universities, colleges or research institutes" [2008 Ontario Budget, 17]. The push for commercialization was also coming from the federal government and one research representative noted, "the federal government is drifting to a focus whereby every research dollar must have some value attached to it." University spending on research was higher than the OECD average and the private sector spending was much lower than the OECD average, so government was subsidizing private sector research through the universities. Some interviewees felt government should be asking the private sector to do more of its own research. Universities obtaining research funding from private sector also politicizes the research process and has significant impact on academic freedom since the research is proprietary. According to the faculty representative, the research belongs to the private company funding it, which goes against academic freedom.

Over the years, OCUR has had various committees looking at the issues of commercialization. OCUR has asserted that "the mission of universities is not to compete in the marketplace" and that universities can play a role on three levels: · · Maintain the flow of fundamental research; Recruiting the brightest researchers and providing conditions for them to excel; and,

78

·

Provide some bridging elements through their education mission.

One representative of the academic research community noted that "value-for-money is knowledge creation" and universities "produce knowledge and how it gets used depends on government, business, and ordinary citizens and you cannot put a value on knowledge creation." He noted that commercialization was just a very small piece of university research. OCUR has, however, taken the initiative to establish a Research and Commercialization Sub-Committee whose objective is to build a commercialization roadmap that outlines the steps required in the commercialization process including identifying the gaps in funding, expertise, legal and technical supports to take an idea form lab to market, "so government will know how the system works and it is no just about funding." However, universities felt that basic research was essential for innovation and in trying to focus on commercialization, basic research results should not be undervalued or dismissed, as it was the basis of new knowledge.

Universities have expressed a willingness to work with all levels of government "to continue to define a more effective set of performance metrics for commercialization activity" [OCUR, 8]. This included learning from other jurisdictions 19 . The sector admitted that "Faculty and researchers are still largely unfamiliar with the processes of commercialization and with the full range of contribution needed to bring ideas to the marketplace" [OCUR, 4]. Universities felt they need further assistance from government and may require some formal training for researchers. A representative of the academic research community noted that the government's focus on commercialization had validated the interest of faculty who were already conducting such activities but also created movement in that direction from other faculty, and the administration was more supportive of activities. However, he noted, "the real question is, is this where we are really adding value"? He did not agree this was the role for universities. The universities could not become generators of jobs through commercialization and were not "the silver bullet solution to the economy." For him, the universities role was in training, which was the biggest part of economic development; then commercially sponsored research with universities (Ontario's share now comprise 40% in Canada or approximately $700 million); and commercialization was and should be the smallest part of the university role. He noted the key to getting university ideas out to business was building university-business relationships to build trust and encourage collaboration to work on problems that applied directly to commercial ventures. To this end, OCUR had also "commissioned a consultant's report on ways to enhance relationships between companies and universities to promote more collaborative research." For him, this was where universities could bring value to Industry: Government does not know how much work is being done with Industry and would like to see more is done, but tax cuts is not the way to encourage it but through direct investments. But

79

research should not be driven by Industry, for example, high value research in the automotive industry. Policy approaches, such as, the Research Assessment Exercise is England was seen counterproductive and unwelcome in Ontario.

On the other hand, a former provincial politician believed that it was not the business of universities to commercialize research: "You know we've been trying to make these elephants dance for a long time. We beat up on the institutions because they don't do a good job at it. But they don't know how to do it and there is no reason why they should know how to do it and to some extent it takes the universities away from their basic job which is to pursue research where it takes them, and to do basic research and to do big science." He felt that "the responsibility lies in our capital markets and the lack of venture capital" and government needed to revise the tax system regarding capital gains among others to incentivize research commercialization. For him, "the real issue is not so much the culture of the universities" but the business culture which had to be more dynamic and productive in contributing to research and commercialization. The student representative felt that tax payers were subsidizing Industry through the universities. Government advisors suggested that government needed to introduce policies to make universities focus on commercialization to enhance the research and development capacity of industry. But this created a situation of the public bearing the risks of research and Industry reaping the benefits. According to a student representative, if the government wanted a research and development policy for the private sector, "it should do so and not ask the public to be underwriting the risks that are involved with transforming universities to become these patent machines, all the while hiving off knowledge from the public or making knowledge more expensive."

Ontario universities have, however, admitted that "an accountability mechanism is required to document the benefits of the commercialization of the results of university research" [OCUR, 7]. In the past standard performance indicators for university research and commercialization included revenues from licenses, number of patents, and sale of equity in spin-off companies. Ontario universities felt this was " too narrow a range of outputs....when estimating the impact on Ontarians' quality of life, and a resulting expectation that universities should focus primarily on these narrow considerations" [OCUR, 2]. The Ontario government wanted to develop an `Ontario Innovation Scorecard' to measure results which would cover "conventional innovation metrics, such as dollars invested, publications, patents, licences, start-up companies, highly qualified people trained and venture capital investment" [OIA, 27]. By extension, universities would no doubt have to keep track of such metrics as part of the commercialization 80

effort. An academic representative noted that universities recognise that they need to be a part of "improving the quality of life of the countries, the provinces, the cities, the towns in which they operate." Their primary role is "improving the quality of the people". In terms of research, he noted "the most important reason for universities to do research is not for the innovation output but for the output of innovative people." A university administrator commented that given the lack of private sector involvement development in research and development, it was not surprising that government had turned to universities to produce research that was transferable to the market. However, he recommended a consortia approach to commercialization rather than having individual universities try to address the issue and "it can be over sold in terms of what we can actually deliver". The university research representative cited the example of Korea which was investing 5% of GDP in upstream research activities, whereas Canada was only around 2% of GDP and the focus seemed to be recently on downstream activities. There needed to be an understanding that more investments were needed in R & D including universities but the narrow VFM focus on commercialization and downstream activities would not provide the long term economic returns for which government was looking rather investments in upstream research was the recommended policy approach. The academic research representative noted that the research community was trying to understand the policy goals of the government so that university researchers could address some of their concerns which were not quite clear as yet and provide appropriate advice in terms of both upstream and downstream research activities for universities.

According to the university research representative, approximately a quarter of research expenditures at universities in Canada were funded by federal research grants, 10-15% by provincial research grants, but the majority of research expenditures were funded by universities themselves which meant operating dollars were being used to fund research. Only with the Canada Research Chairs and Global Research Chairs salaries were covered under grants, so unlike other jurisdictions, there are many indirect research costs which universities have to cover. This is an area for possible exploration as a way for the federal government to provide better funding to PSE which would release provincial funds to focus on hiring faculty for learning and teaching excellence. Focusing on VFM through commercial research is only a small part of the university mission which is learning and teaching excellence and improving basic research, but he agreed that "institutions are provincially funded and they should be in a position to be able to deliver on the agenda of the government." He cited the example of Israel, where researchers were able to do any kind of research and if something of commercial interest was discovered, the government took over the process of funding and managing the commercialization effort. This is an example that may warrant investigation for lessons learned, since some university researchers may not have the expertise or

81

interest in being actively involved in commercialization. However, that does not mean that universities and researchers should lose out on revenues generated from commercialization of their inventions and discoveries. Ultimately, the interviewee noted that politics is a reason for the demand for short-term pay back for investments, whereas "on the research side it does take years and years of research in some areas before you start seeing results and government always want results tomorrow because the next election is in the next year and a half and that creates some additional problems".

5) Value-for-Money through administrative efficiencies in universities The issue of accountability through administrative efficiency was consistently highlighted as a primary objective in previous PSE reviews. Cost containment in administration was a mechanism to have more funds available for the academic mission in a situation of under-funding of universities. When academics invented business management concepts and theories it was intended for the business sector. It was not intended for use in the public sector and BPS organizations but NPM brought these concepts over to the public sector. Universities had voluntarily been adopting various private sector management practices to create savings and efficiencies but were now being requested by government to be more efficient in terms of financial management, and in areas such as procurement and space management. This was in addition to the financial oversight of the auditor- general which was in place for years. As indicated in the 2008 Budget, in the past four years, the government established an annual savings target of approximately one per cent of total expenditures for the public service and broader public sector (BPS): "Collectively, BPS organizations purchase over $10 billion in goods and services. Government wants the public service and BPS to operate efficiently and effectively as a government priority" [Ontario Budget, 2008].

As costs went up, institutions were being asked to be more efficient so that funds were spent on education and not on administration. While administrators were responsible for implementing administrative efficiencies, a university administrator indicated the need for the support of faculty. For example, after an audit of three universities in 2008, the auditor-general felt there was room for improvement in space management and requested information from all universities around space allocation. A representative from academia agreed that universities had to be more efficient which could require re-configuration of existing space and better space utilization - using more of the timetable, more hours in the day, more days in the week, etc. Faculty, he noted who were "committed to excellence will on balance, overall, take seriously the need to make changes in what they are doing to accommodate student learning."

82

As public funding remained insufficient for universities and the opportunity to raise extra revenue through tuition increases were limited, institutions were exploring other ways of budgeting and managing finances, focusing on costs and allocation policies that encouraged efficiencies. For example, University of Toronto moved to a decentralized budgeting and cost allocation model in 2007-08 for its operating budget, which used a charge back system. The objectives of the new model, according to the administration, were to allocate budgets to faculties in a manner that best supported academic priorities; transparency and clear delineation of revenue and expense by faculty; Incentives - allocations linked to revenues and costs; and engagement ­ reviewing the process for both shared-services and academic budgets. This practice of decentralized budgeting models were quite common in other jurisdictions such as New Zealand whose institutions had become very adept at charge back models but still found it difficult to develop outcomes-based indicators, which were being demanded by Government. It was felt by both the faculty and student representative interviewed for this study, that this was a means of downloading responsibility for budgets in light of insufficient resources and that increasing administrative duties were also taking away teaching and research time from faculty.

A financial representative for universities noted that the sector, from a financial perspective, was overaudited and being heavily unionized, unions kept a close watch on expenditures, and "there has not been a record of abuse" in the sector. Quantitative accountability through compliance and VFM audits by universities was therefore not an issue. He was of the view that "we are in a quantitative mindset right now and we will as a natural piece of evolution come to a more balanced kind of reporting." The academic side had room for improvement and he noted that Ontario could learn from lessons from other jurisdictions such as the UK and New Zealand as to what works and what does not work in the quest for accountability from Ontario universities. For instance, some governments required faculty assessments for each member and people are rewarded for good performance, something he thought should be in place in Ontario. He was of the view that if policies borrowed from other jurisdictions were good and in the best interest of the public, it did not matter if universities did not want to implement certain initiatives. Government would always have an agenda for PSE and universities who receive public funding will have to meet that agenda. How this was done was the key. He noted, to deal with multiple reporting for special envelopes, "funding should be as unfettered as possible with some goals and targets and let institutions report back on that instead of spending all of our time tracking the nickels and dimes". The government needed to get together with the sector to consult on how it should be done. His view of the increasing power of the auditor-general was seen as positive, since he could highlight to government the needs of

83

universities, for example, for better use of space by faculty and labour productivity, or the deficiency in buildings from under-funding etc.

The administrative representative felt that the high unionization of university employees, especially faculty, created a structural imbalance in the sector where extra funds would always be regarded as money for salary increases. It was clear from the findings, that there was a divide between university administration and faculty. One reason was that where once faculty used to become administrators as part of their institutional responsibility and then return to academic work, now academics were becoming career administrators and not returning to the "academic fold". The faculty representative noted that academics were concerned about these new administrators and the "increasing corporatization of the universities where the president sees himself or herself not as the academic head of an institution but the CEO of a large organization." This corporatization had given rise to demand for large salaries and retirement packages from university presidents which faculty did not find necessary.

Government also set up the Ontario Education Marketplace (OECM), an activity of OntarioBuys, to encourage greater administrative efficiencies from the education sector. Similar initiatives were implemented in other jurisdictions and in the health sector in Ontario. OntarioBuys was a government program with a goal "to reduce the time and money spent by the BPS on procuring goods (more than $4 billion in the health care sector alone), and funnel savings back into front-line services. OntarioBuys has demonstrated that adopting supply chain leading practices can improve the efficiency and effectiveness of Ontario's BPS" [Budget 2008]. Currently, Ontario school boards, colleges and universities tend to each purchase their own supplies and many use manual (paper-based) purchasing systems. OntarioBuys funded the establishment of the OECM to facilitate group purchasing and to introduce an integrated electronic marketplace. As noted in the budget, the expected results are that, Educational professionals can focus more on teaching, research and student services. Savings and productivity gains are expected to exceed $250 million over five years and will be redirected to supplies, equipment and other student needs [Ontario Budget 2008]. The OECM used the already effectively implemented U-shop at University of Toronto as a model for developing the electronic marketplace. The project has been slow to get off the ground even though implementation was fully funded by OntarioBuys and participation was voluntary. Many universities expressed concern that a proper business case needed to be presented to them where the targets for spend and savings were realistic. They also wanted assurance they would not be left with liabilities for a project

84

that was the brainchild of the government. The OECM has now been set up as an independent entity with the education sector players as customers. It has yet to demonstrate results.

Ontario universities have had to become more efficient in its administration, to free up financial resources for the educational mandate following years of underfunding of the university system which has been well documented by the COU. A survey of the numerous initiatives at the institutional level to improve efficiencies is out of scope for this paper but has been the subject of some research by the COU. 20 Most of these demands for administrative efficiency have been driven by reports from the auditor-general, in addition to government policy initiatives.

The Rise of Parliamentary Officers in Administrative Accountability Parliamentary officers perform a watch-dog role, are independent of the government of the day and report to the Legislature. Two of these officers play an essential role in holding universities accountable. The increasing influence of the Auditor-General in administrative accountability over time was apparent and the influence of this Office continued under the McGuinty government. The government also extended the authority of the Information and Privacy Commissioner to include universities.

The Auditor-General was now able to not only do traditional financial compliance audits but Value-forMoney-Audits (VFMAs), as well. The Public Accounts Committee (PAC) to whom the AG reports, broadened its scope of authority to examine public expenditures for transfer payments recipients, such as school boards and universities. As a result in 2007, a VFMA was done on three Ontario universities ­ McMaster, Guelph and Carleton - focusing on facilities management, particularly, space utilization. The AG recommended that the audited institutions update their facilities maintenance database 21 more frequently and re-inspect the facilities regularly. He recognized the long-standing issue of insufficient funding for academic infrastructure but noted "...we're not saying who should be paying for it. We've just raised the alarm bells" [Auditor General, 2007]. Representatives for the audited universities appeared before the Standing Committee on Finance and Economic Affairs' pre-budget consultations to discuss the issues raised by the AG and the need for funding deferred maintenance as part of maintaining and improving the quality of the university experience. The AG's office had been a key influence on changes in legislation and public sector accountability policy and practice. The annual reports continued to be a source of considerable interest from Government for making changes in accountability requirements. A university representative, however, noted that the auditor-general could only address the administrative

85

side of accountability: "If you want to get at the issue of value, you have to get at the academic side of the house and you can't do that with accountants. You can't do that with bureaucrats."

In addition to the increasing accountability mandate of the AG, as an officer of parliament, another agent of accountability gained authority over universities. The Office of the Information and Privacy Commissioner was established in 1990 by the passage of the Freedom of Information and Protection of Privacy Act (FIPPA) but did not apply to universities. However, since June 10, 2006 Ontario universities were covered by FIPPA which supported access to University records and protection of privacy. This was an accountability measure announced in the 2005 Ontario Budget as part of the RHP. The main purposes of the Act were: · · To provide the public a right of access to university information subject to limited exemptions; and, To protect the privacy of individuals with respect to personal information about themselves held by universities and to provide individuals with a right of access to that information.

Some records accessible under the Act include: those containing a person's own personal information; most university administrative records; records about the subject matter or amount of funding of University research; and records of University staff employment expenses. A few type of records, however are specifically excluded so the Act does not apply to them. A few other types are covered by the Act but exempt from disclosure to protect public concerns, privacy, university operations or other important interests. So far institutions have had quite a number few FIPPA requests which are mentioned in the Commissioner's annual report. Information was key to accountability and the requirement for universities to provide information under FIPPA made them more accountable to the public.

Voluntary Accountability Initiatives and Reporting or more MAD-ness? The proliferation of accountability reporting was not all government-mandated. Another significant development over the last five years in accountability is that individual universities, as well as the sector, have opted to voluntarily increase public reporting, collecting data that may or may not be performance indicators, which gave information on various aspects of the institutions that might be of interest to government, students and potential students and the general public. For a long time, the sector had a Committee on Accountability which provided advice to the sector and MTCU on accountability issues. But the sector and individual universities have been providing more and more performance information

86

on their websites for the public to access. For purposes of this analysis, the growth and investment in many of these voluntary accountability mechanisms was viewed as being an additional component of the accountability regime, one that universities participate in voluntarily in reaction to the rise of the consumer orientation of NPM initiatives promoted by the government and broader society. This section reviewed some of the voluntary accountability components which some or all of the universities in Ontario are using.

Student surveys Standards checking through student evaluations of faculty were common at universities but recently, independent student surveys were being used to get student feedback on various issues including assessing the quality of the student learning and engagement experience. Participation by all universities in student surveys started off as a voluntary exercise so it was included in this section. However, the government liked the idea and participation became mandatory for three surveys ­ the Canadian Graduate and Professional Student Survey, National Survey on Student Engagement, Consortium for Student Retention Data Exchange - which were included in the MYAAs. The surveys allowed institutions to benchmark themselves against peer institutions including US public peer institutions. The data from the surveys provided feedback from students that informed what type of interventions were needed to improve programs and services including student services, teaching, research, facilities, etc. Some academics expressed their reservations about the limitations of the use of student surveys, as there were factors intrinsic to the student and external factors that affected student satisfaction and student engagement. The increase of instrumentalism in university students, where students simply go to university as a means of improving their chances in the labour market, also impacted on the outcomes of surveys. Student representatives thought these surveys did serve a purpose but the danger was that, "Politicians start speaking about slight improvements as a demonstrable improvement in quality." They were intended to provide data for institutional improvement but with the government making them part of the MYAAs, their results could be used to compare institutions which was not the intention.

Canadian Graduate and Professional Student Survey (CGPSS) Most Ontario universities along with universities from other provinces participated in the CGPSS which was first administered in 2007 to graduate and professional students to obtain feedback on their university experience. The survey was based on a similar MIT survey and administered via the Council of Ontario Universities who hired a third party provider to conduct the survey. The Canadian Graduate and Professional Student Survey (CGPSS) was an initiative of Ontario universities but universities in the rest of Canada are also participating. Each university in Ontario participates and receives individual as well as 87

system level results to assist in improving university planning, programming and services. Aggregate results are posted on individual university websites at their discretion and shared with the Government as required.

National Survey on Student Engagement (NSSE) The NSSE survey was one of the instruments used in the US university system as part of a Voluntary System of Accountability (VSA) 22 . The survey is completed by a random sample of students in first-year and seniors and assesses quality of student engagement and experience of undergraduates. The survey is administered by the NSSE Institute at Indiana University and all Ontario universities opted to use it as well as universities from some other provinces. 23 The first Ontario Consortium participation was in 2006 and again in 2008. The survey provides scores on five benchmarks of effective educational practice: Student-Faculty Interaction (SFI); Active and Collaborative Learning (ACL); Enriching Educational Environment; Level of Academic Challenge (LAC); and Supportive Campus Environment (SCE). In 2006, the Ontario universities consortium scored lower than US public universities in the benchmark scores of SFI and ACL but scores were generally better than the `Rest of Canada' grouping. The government has included the NSSE survey in its MYAAs as a performance measure. An academic/administrator noted that while NSSE was useful, the universities recognized its limitations but they government also had to do so as: it only measures half of what universities contribute to student engagement. The other half is what students themselves bring to student engagement. And so in different student populations you have different degrees to which the students themselves are responsible for the survey results that demonstrate that they are engaged. HEQCO has provided some funding to specific institutions that will be using their NSSE data to assess the impact of interventions on quality of the student experience. The government has welcomed participation in NSSE and other surveys and has written it into the MYAAs. Universities have committed to continue participating on a triennial basis to allow enough time for interventions at institutions to show some shift in scores.

Consortium for Student Retention Data Exchange (CSRDE) Some attention was also being paid to retention issues and most Ontario institutions now participated in the CSRDE 24 survey from the University of Oklahoma which provided information on retention rates for participating institutions. Use of data from the surveys for quality improvement interventions and changes

88

are dependent on their acceptance by faculty and stakeholders across individual university campuses. This will require concerted effort by the administration as this state of acceptance has not yet been reached.

Universities also have to provide data to Statistics Canada for specific surveys such as the `Universities and Colleges Academic Staff Survey (UCASS)' which surveys full-time faculty counts and the survey of
Intellectual Property Commercialization in Higher Education Sector. The implementation of student surveys

were voluntary but the government subsequently incorporated conducting the NSSE, CSRDE and CGPSS surveys into the MYAAs. So they went from being voluntary for formative purposes in terms of improving universities to being government-mandated for summative purposes.

Common University Dataset Ontario (CUDO) Voluntary initiatives which have been implemented as new mechanisms for accountability shows promise for further development. At a sector level, the Council of University Planners and Analysts (CUPA) undertook to work on establishing common data sets for the system. In 2006, CUDO, a new online tool was made available for students, parents and the public complementing the information already offered by Ontario's universities. It was based on a similar US initiative of data-sharing. Some of the information included: number of degrees awarded, student enrolment and entering averages ­ all by program; number of students living on campus and activities offered; student satisfaction NSSE scores; first-year tuition and ancillary fees by program; number of teaching faculty; undergraduate class size, by year level; research awards granted; and graduation rates and employment rates by program.

CUDO was deliberately not called performance indicators since some of the data was simply informative data of interest to different publics and did not measure anything. Improvements were underway to make CUDO more user-friendly. Other universities across Canada showed interest in CUDO and an initiative, still in a preliminary stage, was being spearheaded by the University of Calgary called the Common University Data for Canada (CUDC), which encouraged use of a common set of performance indicators in universities across Canada. The CUDC was considering using indicators from CUDO, BC and Quebec.

Each university has established `Accountability' pages and an Academic Integrity Website. A university representative noted that "the initiatives with NSSE and CGPSS and CSRDE, all of those initiatives were at the behest of the institutions, as ways of getting at improving accountability, looking at improving our performance in terms student engagement, student experience, retention, in a consistent manner that

89

allowed us to be able to evaluate how well we are doing but also to look at as a tool to be able to use for interventions to see whether or not it can lead to improvement." He did not think that government gave them enough recognition for their efforts in formative accountability as opposed to the government's interest in summative accountability. However, the government officials noted that the voluntary accountability exercises "were a great development in accountability and transparency, where all sorts of data are now available to the public and the data they collect is driving institutional improvement." One interviewee advocated the use of more formative accountability as it is "more relevant for the business of universities which is education." He found `formative' accountability more useful to the university sector for improving quality in learning and teaching rather than imposed `summative' accountability mechanisms. While the government acknowledged the usefulness of the voluntary reports and data to them, it has not reduced any reporting requirements in a manner where no accountability would be lost. Since universities were complaining of over-reporting from the government, one has to wonder why this voluntary reporting increased. On can surmise that this could be an extension of multiple accountability disorder whereby universities were trying to be proactive in the face of increased state-enforced accountability requirements. It could also be a response to have some measure of control of the reporting of university information provided through FIPPA requests at the same time addressing requests from private sector organisations such as Macleans who aim to profit from dissemination of university information.

The McGuinty government took the accountability regime a step further in terms of NPM - setting and monitoring targets for policy goals of quality, access, and accountability, introducing performance contracts and results-based report backs; a new focus on quality in academia ­ learning and teaching outcomes, students services and supporting; monitoring access for disadvantaged groups; the set up of a new Ministry to fund and promote commercial research whose mandate included universities; and the set up of a new agency to advise on policy goals and assist in research and monitoring of accountability requirements. This was the price for incremental funding which increased enrolment numbers in terms of the access goal but was insufficient to significantly impact quality. A university representative felt, however, things could be worse as other jurisdictions have even more onerous accountability requirements such as England and New Zealand. He did not feel that the performance measures in these other jurisdictions had helped the quality of university education and research and "neither does it give the taxpayers a better understanding of what's going on or a better understanding of what they are getting for their money."

90

The growth of accountability mechanisms under the McGuinty government was incremental as well since there was no major legislative changes to require more accountability. The MYAA reporting is tied to funding investments from the 2005 budget and allows for institution-specific qualitative reporting. In this regard, there is much leeway for institutions to report. The overall basic operating grant is still guaranteed regardless of the MYAA. Appendix 2 shows the number of actors involved in the current accountability framework which shows that while the main accountability is to the Parliament (and officers of Parliament) and various arms of the provincial government, there are also accountability relationships with the federal government, students, faculty, private donors, faculty, unions and the broader community Appendix 3 gives an overview of the change in accountability measures over time and the level where operational responsibility lies for each area of university business including governance, teaching, research, and administration. It shows that accountability mechanisms have increased mainly at the administrative level but there are new requirements for researchers under MRI. In terms of teaching and learning, it is still a self-regulated through OCGS, UPRAC and internal mechanisms area but the KPIs are the only external government indicator that can hold academia accountable for the quality of education, except that employment rates and graduation rates may have little to do with quality of education obtained and it has nothing to do with improvements during the academic year for students. This is where formative accountability initiatives within the institution are more useful for teaching and learning.

91

CONCLUSION AND PROSPECTS FOR CHANGES IN THE ACCOUNTABILITY REGIME

University representatives (administrator and faculty) viewed the accountability regime as `unfocused' and government objectives as unclear. The question was raised by presenter Paul Stenton at an OCUFA conference: "Where is the intersection between Government and institutional priorities?"[Stenton, 2009]. The stakeholders in the university sector need to answer this question and clarify a few issues in terms of consistent funding, goals and objectives, the definition and form of accountability and then move forward to developing a more rational accountability regime or framework. A former provincial politician noted, "There has to be a cultural shift in the Ministry as well as elsewhere about its relationship with the universities." The autonomy of the institutions has to be recognised and they should not be micromanaged. Graham noted that "two interesting issues loom on the horizon: (1) To what extent will public concern for accountability, quality, and effectiveness alter the structure of higher education; and (2) How are the values of higher education to be asserted and measured if social utility and labour-force criteria are the paramount tests?" [Graham, 1989, 7] These issues remain relevant today. One has to wonder if the public is really concerned about university accountability or is it just government, and is it to the extent that universities are seen as instrumental in producing the labour force for the knowledge economy in the 21st century. The faculty and students representatives did not think the public was interested in anything except tuition fees. HEQCO needs to help the government define how to attain their policy objectives in terms of increased access, quality and accountability. There also needs to be an understanding of the full breadth of the regime and the various agents of accountability with jurisdiction over universities not just the part for which Ministry of Training, Colleges and Universities is responsible.

The evidence indicated that the university accountability regime evolved over time from the early days to the present, from minimal accountability requirements to an un-integrated, multi-layered accountability regime. While the expansion of enrolment and the increase of government funding gave rise to the demand for more accountability in the 1960s and 1970s, the adoption of NPM practices in the public sector influenced the `form' of the accountability framework in the past 15 years. The political economy of neo-liberalism in the 1990s was the backdrop for advent of NPM in Ontario with the Rae and Harris/Eves governments which gave rise to summative forms of accountability. The Minister responsible for universities gained more regulatory power over universities, new actors emerged on the scene such as parliamentary officers - the auditor-general and later the information & privacy commissioner, and a new 92

Ministry of Research and Innovation. The auditor-general as a proponent of NPM values and philosophy, had a major influence in convincing successive governments that greater accountability mechanisms were required of universities in the form of VFM audits, greater efficiencies and use of performance measures. The Office of the Auditor-General itself moved from having limited powers to greater auditing powers over universities. The changes were incremental, wide but not deep, expanding across various actors asking for accountability, requiring more time and resources dedicated to reporting, but not demanding too much in the nature of that accountability in terms of performance measurement and reporting. This has resulted in `multiple accountability disorder.'

The research showed Ontario's universities have a patchwork of accountability mechanisms, involving both voluntary and mandated aspects addressing both the academic and administrative sides of universities. While most accountability initiatives were government-mandated (summative accountability), more and more voluntarily instituted initiatives by universities (formative accountability) were being implemented, which the government saw as useful but was slightly dismissive of their efforts. The different arms of the provincial government ­ Parliament, the Auditor-General, MTCU, MRI, the Information & Privacy Officer, other ministries, and the federal government are the major actors in accountability for universities. No one government agency has been assessing what really comprises the accountability framework for universities. Academics argue that the role of the university is much broader in society and that pressuring universities to focus on economic and commercial value of its graduates and research did not augur well for the future. There was disagreement over the form and content that accountability mechanisms should take with a distinct distaste expressed on the academic side at the idea of summative, NPM-related measures - performance measurement, measuring learning and teaching, measuring the contribution to commercial research, while ignoring the immeasurability of the overall contribution of a university to the community and society, as well as the lifelong contribution of graduates to society.

The accountability framework as it now stands was the result of changes in governments, policy, economic circumstances and expectations of universities and their role in society, as well as changes in ideas about the administration of the bureaucracy including NPM. It was quite clear, however, that while the `accountability framework' had expanded over time, Ontario universities were not as regulated and subject to as much performance measurement requirements as universities in other jurisdictions such as the USA, the UK, Australia and New Zealand where NPM has left a profound impact of all aspects of 93

university life. Though some have claimed that after the 1990s, NPM no longer held sway over public policy and administration, the review of other jurisdictions and Ontario showed this was not the case. Under the Rae and Harris governments, there were some implementation of NPM-influenced measures such as KPIs, the EQAO, and encouragement of partnerships with the private sector. The McGuinty government implemented changes in its multi-faceted accountability framework for universities after the Rae Review, which expanded the regime even further in terms of performance measurement, reporting and value-for-money which show the lingering influence of NPM. The pressure of global competition has seen a renewed emergence of the language of NPM in government policy and public administration. Government goals for PSE have not essentially changed from what Bill Davis first proposed in 1967. Today the focus has shifted, however, which has affected expectations for what universities are accountable and how they demonstrate that accountability which has included NPM initiatives such as MYAAs and KPIs. PSE were seen as not only a social good but an economic good that are key to economic competitiveness and so governments since the 1990s have been demanding measurable returns on its investment of public dollars. Accountability requirements have been increasing at the same time that government funding (operating dollars) per FTE have been declining. However, as long as universities are dependent on government for the majority of their funding, they will be obliged to meet government's policy goals. Over the years, universities struggled with government's attempts to increase accountability mechanisms which they saw as a threat to their autonomy, with governments sometimes succeeding and sometimes retreating. All interviewees, however, agreed that accountability did not have to conflict with university autonomy or academic freedom. The reason that the McGuinty government was able to implement some new accountability arrangements, however, was because it brought significant new funding dollars which were badly needed by the sector, after it suffered significant funding cuts in the 1990s. There was also still an increasing demand for university spaces which universities could not afford.

The findings from the literature review and the interviews, all indicated that universities are accountable to multiple stakeholders, the provincial government being the main stakeholder because most of the funding for basic operating for Ontario universities comes from the provincial government, but there was no consensus to what degree. While the government is keen to `measure' the impact of its policy and funding initiatives, the sector is not so keen on the methodology being used including KPIs and MYAAs. Higher education institutions that received public funds through direct government grants and from consumers through tuition fees are under greater pressure from governments to improve accountability mechanisms. Consumers - students and parents - want to know which institution to choose based on the 94

quality of education being provided. Employers are also asking questions about the quality of graduates and their ability to meet the challenges of today's workplace. For an intangible product like education, it is difficult but not impossible to answer these questions. Accountability of publicly-funded universities is no longer only about political/legislative accountability with financial compliance sufficing. Good governance and performance management in all aspects of the institution including the academic side is under scrutiny from government. But most of all universities are now expected to contribute more explicitly to meeting the social and economic policy objectives of government. The challenge for universities is to make government appreciate that their individual missions differ and their goals and objectives are much broader than the narrow policy agenda of the government of the day and accountability requirements.

Universities are more accountable today than they were even a decade ago not only to government but students who are contributing more tuition, and to private donors as universities are forced more and more to bolster their income in light of growth in the sector and the government's inability to fully fund it. Administrative accountability of universities to the government has increased over time aided by initiatives that were influenced by NPM. The government no longer accepts, as it once did, that universities are not accountable to them. The old argument of `autonomy' and `academic freedom' no longer stand up as a reason to not properly account for what the public is getting for the large transfer payments that universities receive from government. No doubt, as attempts are made by government to strengthen the accountability framework, there will continue to be push back from the sector if initiatives seem too intrusive or inappropriate. Sector representatives have articulated the need to use more formative approaches to accountability instead of summative measures which attempts to attach numbers and measurements to the core work of universities which is teaching and research and judge performance based on those measurements. An example is already in place for the quality assurance process which is a self-regulated peer review process that allows time for change and improvement. For instance, interviewees from the sector had several questions around the current university accountability regime and the need for clarification in re-defining the accountability framework. · · · · · · · What does government want universities to do? How will government know when universities are successful doing it or not? What are government's specific objectives for universities- e.g., quality, access, cost reduction, market responsiveness, stronger governance? What does the government think that the public needs to know about universities? How is accountability being defined and whose definition is being used? How should quality in PSE be defined and what are the issues involved? What form should an accountability framework take? 95

· · · · · · · · ·

What are the accountability mechanisms to implement it? If government had to have performance measures, what would be the most suitable? How does it track whether or not it is improving and demonstrate that improvement? Can one really measure the value of what universities produce in terms of graduates, research and community contribution? Where is Ontario going as a sector and how well is it doing in comparison with other jurisdictions? What is the individual institution's purpose and priorities? How it is going to achieve its objectives? Is funding keeping up with enrolment demand? Is funding available for quality improvement- new building and repairs; new faculty hires, etc?

Ideas diverged around all these questions and issues which the sector needs to answer to move forward.

The first generation of MYAAs is coming to end and it is uncertain whether there will be a next generation of MYAAs, if there is no multi-year funding beyond the Reaching Higher Plan. There was disagreement over the content of the MYAAs. All players recognised that these agreements needed to be adjusted through dialogue and discussion. The MYAAs could be made simpler and there was also a feeling in the community that it should not be a data collection mechanism since there were other avenues to collect data such as the modification of CUDO to meet government data needs. The public was not aware of the MYAAs as an accountability tool so it should be geared for government use. In rationalizing the MYAAs, KPIs could become part of the MYAAs but there were also some indicators that may not need to be in the MYAAs. A university representative noted that this should be the focus of an accountability regime and if that was done there would be no need for multiple funding envelopes with different reporting requirements. The MYAAs are reflective of the government policy agenda but do not reflect everything that universities actually do. A university representative recommended `fitness for purpose' be utilised, a term used in Australian higher education. Accountability should not only be defined in terms of the government agenda but the individual institutions' agendas which are diverse with differentiated missions. Whether it was intentional or not, the MYAAs not only held the universities accountable, but also the government in terms achievement of its own goals and the provision of adequate funding. As the government conducts a review of the MYAAs to see what lessons can be learnt going forward for a possible next generation of MYAAs, it should bear this in mind.

There seems to be a case of `multiple accountabilities disorder' in the university sector and there should be some rationalization and consolidation of performance reporting. There was more than enough financial accountability (summative) and the process of trying to achieve academic accountability ought to center around formative accountability and continuous improvement. There needs to be some 96

consultation on the extent and nature of the rationalization. Some cost-benefit analysis needs to be done to determine a way to integrate mandated (political, administrative) and voluntary accountability mechanisms to create a more realistic and coherent accountability framework that provides true accountability ­ financial and academic - to all funders, that is, government, students and donors. This may mean government having to give up some of its attachment to some redundant reporting as well as some NPM-influenced accountability mechanisms. Government needs to discuss these issues with the sector to find and implement meaningful solutions.

Higher education legislative framework The recommendation from the Rae report to set up a new legislative framework for higher education could be re-visited to see if it would create more transparency and coherence for the system on the part of government and the universities in terms of funding and accountability.

The new quality assurance process for programs As far as the core business of the universities, both academia and the government agree, universities themselves are the best ones suited to evaluating their performance through a peer review system on quality assurance. There were two concerns with the current OCGS process which were the frequency of reviews and the follow up done by institutions based on the OCGS reports. According to a university representative, " ... there is a challenge to assure that there is a rigorous review of quality but without all the sort of needless bureaucracy and rules and so on that have accumulated over time with the OCGS process". The need was identified to develop a "a quality culture that does not depend on bureaucratic processes." The new process that will be introduced to assure quality for both the undergraduate and graduate programs should take into account from lessons from other countries to strike the right balance. The Government representative had full confidence in the quality assurance system and did not see a need for government to take a more active role in quality assurance. Government does not have the qualified manpower to conduct quality assurance and if it wanted to intervene in quality assurance, it would be a big cost to tax payers. For instance, as happened with Britain's Higher Education Commission, a new bureaucracy would be needed with academics being hired to engage in onerous quality assurance exercises currently handled by the sector, itself. Government does, however, need to take note of the proposed changes and whether accountability for quality of programs would be reduced. The OCGS was fairly independent. The challenge in the new self-regulatory system is to ensure that if the university Senate became more involved in quality assurance, the process was not compromised.

97

Quality education and academic accountability A former provincial politician noted that assessing for quality in terms of student experience, student engagement, quality of teaching, and learning outcomes is not an infringement of academic freedom. The key question to be asked is: "What is done with the assessments afterwards?" Leaving aside the issue of funding and more resources, as suggested earlier, academia can use different strategies and techniques to improve quality in learning and teaching. The challenge for government is to understand the nature of academia and to what extent government accountability structures are appropriate for assessing university performance when attempting to align the university mission with the government policy agenda. Part of improving quality in academia is sufficient funding for hiring core tenured faculty to improve the studentfaculty ratio and to have a cadre of highly-qualified teaching and research faculty. More work in the area of learning and teaching needs to be done which is where quality improvement can happen, but it has to be done by universities and not imposed from outside. Faculty needs to be consulted and engaged by the university administration as well as government, if there is to be success in moving further on the government's quality agenda. The growing divide between university administration and faculty, over the corporatization of the university, needs to be overcome in this process.

Research and Commercialization Recommendations from the university sector on the future of commercialization, was for government to invest direct cash into university-industry collaborations (not tax credits) and the metrics for that would be the value of the contracts with the companies. The government could also fund start-up companies and the metrics would be how long they lasted and how many people they hired, and the value of sales from university derived products. The private sector had to be more proactive as well by investing in venture capital to commercialize research. Government should put the onus on the private sector to invest in more commercial research & development through specific private sector policies as the core research mission of universities is basic research.

The Future of HEQCO During the Harris years the policy capacity of the Ministry was reduced but some interviewees felt HEQCO can fill this void. While it has begun conducting high level policy research through contracted consultants in the PSE sector, its role should be to provide more practical policy advice based on the accumulated research. Some hard statistical data on the sector which is currently lacking and which was previously produced by organizations such as OCUA and COU, could also be collected by HEQCO. It should operate independently and not only within the purview of government policy. So it should also be

98

able to critique government policy without fear of reprisal. It should also conduct broad consultations on issues and consider a wide range of viewpoints in providing policy advice to government.

Internal accountability - students and faculty There are two aspects to university accountability ­ internal accountability to stakeholders - faculty and students and external accountability to government, private donors and the public. While the thesis focused on external accountability to government, it was clear that the issue of internal accountability regarding resource allocation and funding impacts external accountability. The growing divide between faculty and university administration created by the corporatization of the university needs to bridged to provide greater internal accountability. Students also want accountability from administration because they pay more tuition fees than previous years or also want to be treated more as partners than consumers. An accountability framework needs to take account of the internal community as well as the external community. Democratization of the internal decision-making process needs to be re-visited to have greater broad-based faculty and student participation in decision-making and accountability activities. Consultation on the next generation of MYAAs, if there is one, needs to real at the sectoral and institutional levels for faculty and students. Government expectations for consultation on campus should not be limited to the Senate or the governing body. Government funding A university representative noted that accountability is a two-way street and that "obviously the public does not understand enough to know, for instance that what students are getting today are not nearly as good as what students were getting 30 years ago". While over time enrolment has increased as well as grants, per student funding has been declining for years. The government officials acknowledged that government has an obligation to fund the universities to achieve their strategic provincial targets: "This is a two-side story. This isn't just about the gov't asking institutions to do something. That's why it's an agreement that both parties sign." Funding needs to be consistent and sufficient to meet the twin goals of increased access and quality but this is a challenge especially in the current economic circumstances. A former provincial politician noted that there are two choices open to government regarding funding, that is, "they are given greater freedom with respect to tuition fees and essentially able to plan their economic future" or "their entire economic well being is completely dependent on what happens to the government". Without consistent funding, universities find it is difficult to plan for programs and faculty hires. A university administrator noted that there was also a structural issue in universities where the balance of power lies with faculty in collective bargaining, so any new funding universities get for quality

99

improvement becomes part of the bargaining agenda for increased salaries. This is an issue for which faculty has not taken responsibility in terms of addressing funding shortages in universities.

The current tuition fee framework and the funding formula need to be re-visited. Private universities, which had been considered by the Harris government, as a potential option dealing with government funding problems was not an option. Fees would be too high and if public universities were privatized, according to a former provincial politician, compensating the public for the full value of the investments would create a huge debt for an institution attempting to privatize. Access and quality do not have to compete with each other. Proper taxation can fund PSE expansion and quality improvement. Tax cuts are seen as a way of getting re-elected but government needs sufficient revenue to fund health, PSE and other social goods. A former provincial politician noted that the government was advised that any new accountability arrangements " ... should speak both to the assurances of levels of funding as well as assurances about how to go forward". He noted that client group such as hospitals, health care systems, municipalities, and universities who rely heavily on public funding should say "we are accountable for this, where's your accountability?" On the other hand, given the massive demand for university education and the benefits reaped by graduates, it may not be realistic to ask tax-payers and government to return to the funding rates of bygone years when there were lower enrolment rates. Income-contingent re-payment of fees after graduation needs to be explored as a rational and realistic option.

Voluntary accountability reporting The sector has invested time and effort into voluntary accountability mechanisms starting with the COFO report and later on with through other mechanisms such as CUDO, NSSE, CSRDE, CGPSS and other student survey results, academic integrity web pages and accountability pages, etc. The Government needs to acknowledge the value of this data as providing accountability to government and the public, as is the case in the USA and other countries. This reporting should be taken into account when rationalizing accountability requirements. This formative accountability needs to compliment the summative accountability initiatives on which government seems focused.

Role of the federal government There was some agreement among interviewees on the role of the federal government in PSE in the future. In terms of funding research, there should not too much focus on VFM and commercialization. The possibility of funding salaries through research grants for research professors would allow provincial resources to be used for hiring teaching faculty to improve the quality of education. The federal 100

government could also provide funding for infrastructure but the issue still remains whether the province would flow the monies to the universities. The federal government also has a role to play in coordinating PSE at a national level, reducing fragmentation and facilitating the creation of a more coherent national level of university education and PSE. The dedicated transfer for PSE should be reinstated to help address the persistent underfunding of PSE institutions, which is the key to improving access and quality. The Rae report recommended that the federal government set up a separate fund to help higher education institutions with equipment and campus improvements. The federal government should have some role in creating a more harmonized and rational PSE system in Canada, to be competitive on a national level with other countries.

101

APPENDICES

102

103

Appendix 1 ­ Core Interview questions INTRODUCTION 1. What is your background? How did you come to your position or interest in the PSE sector? 2. Do you think `autonomy' and `academic freedom' are in conflict with demands for greater accountability from any government? 3. (a) Do you think Ontario has been influenced by developments in other jurisdictions regarding performance reporting and accountability of universities in the past and in the present? (e.g., UK, USA, Australia, New Zealand). (b) How do you think Ontario compares currently with other provinces and countries such as the UK, USA, Australia, New Zealand in terms of robustness of an accountability regime [n.b. accountability reporting is retrospective control vs. prospective which limits decision-making autonomy). PAST ACCOUNTABILITY REGIME 1. How do you perceive policy on accountability measures for universities in Ontario evolving over the past couple of decades?

2. Do you think there are more accountability requirements now than there were 5 or 10 years ago? Why? CURRENT ACCOUNTABILITY ISSUES/CHALLENGES 1. Who is demanding more accountability?- Government, students and parents, other funders- from universities? Why? 2. What do you think are the current policy / accountability goals for PSE sector? Do you think enough is being done on the part of government and universities to achieve these goals? a. Are you familiar with the report of the Rae review on PSE? If Yes, What do you know about it? b. Do you think there have been significant enough increases in accountability requirements for universities since the Rae review report of 2005 to say there is a new accountability regime?

104

c. If yes, to what extent do you see the Rae Review impacting government policy on higher education and current performance management and accountability requirements for universities? d. Are you familiar with the Multi-Year Accountability Agreements and if so, what do you think of their implementation as a performance measurement tool for universities? e. What do you think about the establishment of the new Higher Education Quality Council (HEQCO) and the current work it is doing on PSE? 3. Are there other accountability requirements (provincial or federal, other ministries) that you think are significant for universities that were set up that are unrelated to Rae recommendations? a. Are you familiar with any mechanisms for quality assurance for university programs in Ontario (Ontario Council on Graduate Studies review of graduate programs; UPRAC audits for undergraduate programs, etc)? Do you think these are sufficient for public accountability? b. Do you think there is too much focus on value for money and commercialization of university research as an accountability requirement for government investment in university research, than in the past? If no, why? If yes, what do you think about this policy? 4. (a) Is the accountability framework in its broadest sense (taking into account political and administrative measures) sufficient for universities to justify use of public funds? If yes, explain why. If no, what needs to change/improve for better accountability? (b) Do you think universities in Ontario have enough accountability requirements (voluntary and government-mandated) on the academic side, in both teaching and research, to justify the expenditure of public funds? Why yes or no? FUTURE DIRECTIONS 1. What would you like to see in change in terms of performance management and reporting as an accountability mechanism for universities as far as government policy and public administration in the future, if anything? Less/More reporting, etc?

105

2. Do you see the role of the new Higher Education Quality Council (HEQCO) in accountability and performance management of universities, changing in the future? If no, why? If yes, how? 3. Any other comments on past, present and future of accountability and PSE in Ontario?

October 14, 2008

106

107

Appendix 2 ­ Key accountability actors for universities in Ontario, 2008

Parliament
(Establishing Acts of the Legislature) Public Accounts Committee & other committees

Ministry of Health*

Ministry of Training, Colleges, Universities

***Ministry of Research & Innovation

Private funders /Donors; Other govt's.

HEQCO

University Governing body

Federal Gov't

Provincial Parliamentary OfficersAuditorGeneral; ***Information & Privacy Commissioner

Students

**Tri-Council funding agencies - NSERC, SSHRC, CHIR

Other provincial/ federal ministries & agencies Community; General Public

Unions; Faculty Associations Professional/ accreditation Bodies

*Only applicable for universities with teaching hospitals or receiving MOHLTC funds. ** Smaller institutions may not have much research funding ***All actors existed in some form before the 1990s except for MRI (2005). The Information & Privacy Commissioner only received
authority over universities in 2005.

108

109

Appendix 3 ­ Accountability mechanisms for universities (government or legally mandated, self-regulatory and voluntary) ACCOUNTABILITY PRE-1990s AREAS GOVERNANCE ­ · Academic plans Governing Body · Financial Plans (Budget) · Strategic Plan · Business Plan ADMINISTRATION ­ President and Administrative Staff · Enrolment and other grants reporting to MTCU · External Audit reports · Audited Annual Financial statements · Reports to other Ministries · Compliance audits by auditorgeneral · Endowment funds reports to donors1 1990-2003 · · · · Academic plans Financial Plans (Budget) Strategic Plan Business Plan · · · · 2003 - 2008 Academic plans Financial Plans (Budget) Strategic Plan Business Plan

· Enrolment and other grants reporting to MTCU · External audit reports · Audited Annual Financial statements · Student loan default rate* · Reports to other Ministries · Compliance audits by auditorgeneral · Value for money audits by auditorgeneral · Endowment funds reports to donors1

· Enrolment and other grants reporting to Ministry of Training , Colleges and Universities · External audit reports · Multi-year Accountability Agreements and report backs · Audited Annual Financial statements · Student loan default rate* · Reports to other Ministries · Compliance audits by auditorgeneral · Value for money audits by auditor-general · Report to Information & Privacy commissioner · Report to Employment Equity officer 2 · CSRDE scores · Common University Dataset 2 Ontario 2 · Accountability Web page · Endowment funds reports to donors1

110

ACCOUNTABILITY PRE-1990s AREAS TEACHNG AND · OCGS appraisals1 · UPRAC reviews1 LEARNING ­ Academic Heads

1990-2003 · Employment rate (after 6 months, 2 years)* · Graduation rate* · OCGS appraisals1 · UPRAC reviews1

2003 - 2008 · Employment rate (after 6 months, 2 years)* · Graduation rate* · OCGS appraisals1 · UPRAC reviews1 2 · Academic Integrity page 2 · CGPSS scores · Student engagement ­ NSSE 2 scores · Reporting to Tri-Council agencies; MRI; sponsors and donors · Reporting to Ministry of Research and Innovation

RESEARCH ­ VicePresident - Research

· Reporting to Tri-Council agencies; MRI; sponsors and donors

· Reporting to Tri-Council agencies; MRI; sponsors and donors

Self-regulatory -1

Voluntary ­ 2

The other requirements are mandatory under legislation or policy directives.

*Current MTCU Key Performance Indicators

111

Appendix 4 ­ Reporting Requirements to MTCU
Reporting Requirement for all institutions Capital Plan and Investment Program Facilities Renewal Program Preliminary Fall Enrolment Preliminary Winter Enrolment Actual Enrolment for Four Terms Tuition Fee Monitoring Tuition Fee Set-Aside Report Accessibility Funding for Students with Disabilities Quality Assurance Fund Performance Funding (KPIs) Student Support Grants / Bursaries Ontario Student Opportunity Trust Fund Annual Report on Existing Programs New Proposed Undergrad., Graduate and Doctoral Program Approval Form Multi-Year Accountability Agreement & Report Back Total Reporting Requirements for some universities Major Capital Support Program Bilingualism Grants Report Ontario Graduate Scholarships in Science and Technology Access to Opportunities Program Reports Aboriginal Education and Training Strategy Educ-Action Interpreters Fund Women's Campus Safety Grant # of Reports 1 2 1 1 4 1 1 1 2 1 2 12 (monthly) + Note Disclosure 1 1 Yes Yes External Audit

1
33 # of Reports 2 + 12 (monthly) 1 1 2 1 2 2 1 Yes 2 External Audit Yes (upon completion of project)

112

Graduate Nursing Compressed Degree French as a Minority Language Programs Total Institution-specific Reporting Requirements York Deaf Education Algoma Extraordinary Grant Hearst Extraordinary Grant Guelph Ontario Veterinary College French (Ottawa) Medical School Ottawa Teachers of Francophone Deaf Pupils Midwifery (McMaster, Ryerson, Laurentian) Trent/UOIT Toronto OISE/UT Toronto OISE/UT French Language Initiative UOIT Transition Fund and Operating Fund Northern Ontario Medical School Waterloo Optometry Total
*Also every 3 years, 2 external OASP audits have to be done.

1 2 27 # of Reports 1 1 5 1 1 1 1 1 2 1 4 3 1 23 0 2 External Audit

(Source: Joint MTCU/COU Working Group on Reporting Requirements, June 2005)

113

Appendix 5 ­ List of Acronyms 
ACUA ADM ATOP AUCC AUQA BIU BOG CAUBO CAUT CFI CGPSS CHST CHT CSRDE CST CIHR CPUO CO COFO-UO COU CSAO CUDO CUPA Advisory Committee on University Affairs Accumulated Deferred Maintenance Access to Opportunities Program Association of Universities and Colleges of Canada Australian Universities Quality Agency Basic Income Unit Basic Operating Grant Canadian Association of University Business Officers Canadian Association of University Teachers Canada Foundation for Innovation Canadian Graduate & Professional Student Survey Canada Health and Social Transfer Canada Health Transfer Consortium for student retention data exchange Canada Social Transfer Canadian Institutes of Health Research Committee of Presidents of the Universities of Ontario Colleges Ontario (formerly Association of Colleges of Applied Arts and Technology of Ontario) Council of Finance Officers - Universities of Ontario Council of Ontario Universities Council of Senior Administrative Officers Common University Dataset Ontario Council on University Planning and Analysis

ENQA
EQAO FTE HEFCE HEQCO MCU

European Association for Quality Assurance in Higher Education
Education Quality and Accountability Office Full-Time Equivalent Higher Education Funding Council of England Higher Education Quality Council of Ontario Ministry of Colleges and Universities

114

MET MRI MTCU MYAA NSERC NSSE OCAV OCGS OCSA OCUA OCUFA OCUR OECD OECM PEQAB

Ministry of Education and Training Ministry of Research & Innovation Ministry of Training, Colleges and Universities Multi-year Accountability Agreement Natural Sciences and Engineering Research Council National Survey of Student Engagement Ontario Council of Academic Vice-Presidents Ontario Council on Graduate Studies Ontario Committee on Student Affairs Ontario Committee on University Affairs Ontario Confederation of University Faculty Associations Ontario Council on University Research Organisation for Economic Co-operation and Development Ontario Education Collaborative Marketplace Postsecondary Education Quality Assessment Board

QAA
RAE RHP SSHRC TEC TAMU UPRAC

Quality Assurance Agency
Research Assessment Exercise Reaching Higher Plan Social Sciences and Humanities Research Council Tertiary Education Commission Tertiary Advisory Monitoring Unit Undergraduate Program Review Audit Committee

115

BIBLIOGRAPHY

Association of Universities and Colleges of Canada. "A Primer on Performance Indicators." 1995. 16 Aug. 2008 <http://www.aucc.ca/search/index_e.html>. Association of Universities and Colleges of Canada. "Commercialization and Research." Ottawa. 25 Mar. 2001. Association of Universities and Colleges of Canada. "Federal Government Roles and Responsibilities in Higher Education and University Research." AUCC Submission to the Government of Canada. Ottawa. 18 Sept. 2006. Association of Universities and Colleges of Canada. "The Commercialization of University Research." 16 Aug. 2008 <http://www.aucc.ca/search/index_e.html>. Association of Universities and Colleges of Canada. "Momentum: The 2005 report on university research and knowledge transfer." 2005. 16 Aug. 2008 <http://www.aucc.ca/momentum/en/report/index.html> . Atkinson-Grosjean, Janet and Garnet Grosjean. "The Use of Performance Models in Higher Education: A Comparative International Review." Education Policy Analysis Archives 8.30 (2000). 16 Aug. 2008 <http://epaa.asu.edu/epaa/v8n30.html >. Aucoin, Peter and Mark D. Jarvis. "Modernizing Government Accountability: A Framework for Reform." Ottawa: Canada School of Public Service, 2005. Aucoin, Peter. "the new public management: Canada in comparative perspective." Montreal: Institute for Research on Public Policy, 1995. Australia. Department of Education, Employment and Workplace Relations. "Review of Australian Higher Education". Canberra: Government of Australia, 2008. 06 Aug. 2008 <http://www.dest.gov.au/HEreview>. Basken, Paul. "Three Years Into Fight for Accountability, Spellings Again Seeking Traction." The Chronicle for Higher Education. 18 July 2008 <http://chronicle.com/daily/2008/07/3879n.htm>. Berger, Joseph and Anne Motte. "Mind the Access Gap: Breaking Down Barriers to Post-Secondary Education." Policy Options. Montreal: Institute for Research on Public Policy, 2007. Beach, Charles M., et al, eds. Higher Education in Canada. Kingston: John Deutsch Institute for the Study of Economic Policy, Queen's University, 2005. Beach, Charles M., A Challenge for Higher Education in Ontario. Kingston: John Deutsch Institute for the Study of Economic Policy, Queen's University, 2005. Bovey, Edmund C. et al. "Ontario Universities: Options and Futures." The Commission on the Future Development of the Universities of Ontario. Toronto: Province of Ontario, 1984. Brzustowski, Tom. The Way Ahead: Meeting Canada's Productivity Challenge. Ottawa: University of Ottawa Press, 2008. 116

Burke, Joseph C. and Henrik Minassians. "Reporting Higher Education Results: Missing Links in the Performance Chain." New Directions for Institutional Research. San Francisco: Wiley Periodicals Inc., 2003. Cameron, David M. and Royce, Diana. "History of Postsecondary Education in Ontario." (excerpt from the Postsecondary Panel Report ­ Excellence, Accessibility, Responsibility") Toronto: Province of Ontario, 1996. Cameron, David M. "Ontario's Rae report: investing in growth." (Book Review). Canadian Public Administration. 48.2 (Summer 2005): 280-288. Canadian Centre for Policy Alternatives. "Tories get priorities wrong in Harris budget." 10 May, 2001. 06 Aug. 2009 <http://www.policyalternatives.ca/editorials/2001/05/editorial297/?pa=>. Canadian Council on Learning. "Post-secondary education in Canada: Meeting our Needs." 28 Aug. 2009 < http://www.ccl-cca.ca/pdfs/PSE/2009/PSE2008_English.pdf>. Canadian Council on Learning. "Post-secondary education in Canada: Strategies for Success." Report on Learning in Canada '07. 2007. 18 Aug. 2008 <http://www.cclcca.ca/CCL/Reports/PostSecondaryEducation?Language=EN>. Canadian Council on Learning. "Canadian Post-secondary Education: A Positive Record ­ An Uncertain Future." 2006. 18 Aug. 2008 <http://www.cclcca.ca/CCL/Reports/PostSecondaryEducation/Archives2006/>. Canadian Federation of Students. "Public Risk, Private Gain: An Introduction to the Commercialization of University Research." Ottawa, 2007. 02 Aug. 2009 < http://www.cfs-fcee.ca/html/english/research/submissions/public_risk.pdf>. Clark, Ian D. "A Jurisdictional Competitiveness Framework for Thinking about System Capacity in Higher Education. Invited Paper for the Higher Education Quality Council of Ontario. Toronto: Province of Ontario, 6 Nov. 2007. Clark, Ian D. and Trick, David. "Advising for impact: lessons from the Rae review on the use of specialpurpose advisory commissions." Canadian Public Administration. 49.2 (Summer 2006): 180-196. Clifton, Rodney and Hymie Rubenstein. "Improving University Teaching: A Performance-Based Approach." Policy Options. Montreal: Institute for Research on Public Policy, 1998. Committee of University Chairmen (CUC). "CUC Report on the Monitoring of Institutional Performance and the Use of Key Performance Indicators." Nov. 2006. 16 Aug. 2008. <http://www.shef.ac.uk/cuc/pubs/KPI_Booklet.pdf>. Council of Ontario Universities. "Comparing Ontario and American Public Universities." Toronto: COU, 2000. Aug. 16 2008 <http://www.cou.on.ca/content/objects/Ontario%20vs%20US%20peers.pdf>. Council of Ontario Universities. "Briefing Notes." 2003. 06 Aug. 2009 <http://www.cou.on.ca/_bin/briefsReports/onLine/briefingNotes2003.cfm>.

117

Council of Ministers of Education, Canada (CMEC). "A Report on Public Expectations of Postsecondary Education in Canada." Feb. 1999. 31 Aug. 2008 <http://www.higher-ed.org/resources/Canada_Report_1999.pdf> Council of Ministers of Education, Canada (CMEC). "Ministerial Statement on Quality Assurance of Degree Education in Canada." Feb. 1999. Council of Ontario Universities (COU). "Proposed University Accountability Framework." Toronto: COU, 2004. Council of Ontario Universities. Common University Data Ontario. Toronto: COU, 01 April 2008 <http://www.cou.on.ca/_bin/relatedSites/cudo.cfm> . Council of Ontario Universities. "A Strong Ontario Needs Strong Universities." Biennial Report 20022004. Toronto: COU, 2004. Council of Ontario Universities. "University Access, Accountability and Quality in the Reaching Higher Plan." Progress Report. Toronto: COU, 2006. Council of Ontario Universities. "The Ontario University Sector: Overview and Issues, 2007-2008." Toronto: COU, 2008. Clifton, Rodney and Rubenstein, Hymie. "Improving University Teaching: A Performance-Based Approach." Policy Options. Montreal: Institute for Research on Public Policy, 1998. 19 July 2008 <http://www.irporg/po/archive/jun98/clifton.pdf>. Cutt, James and Rodney Dobell. Public Purse, Public Purpose: Autonomy and Accountability in the Groves of Academe. Montreal: The Institute for Research on Public Policy, 1992. Cutt, James. "Universities and Government: A Framework for Accountability." Halifax: The Institute for Research and Public Policy, 1990. Davenport, Paul. "Universities, Innovation and the Knowledge-based economy." CERF / IRPP Conference on "Creating Canada's Advantage in an Information Age." Montreal: Institute for Research on Public Policy, 2000. 16 Aug. 2008 <http://www.irporg/fasttrak/index.htm>. Dearing, Ron Sir. "Higher education in the learning society." Report of the National Committee of Inquiry into Higher Education. London: Government of England, 1997. 18 Aug. 2008 <http://www.leeds.ac.uk/educol/ncihe/>. Downey, James. "Accountability versus Autonomy." Meeting of Vice-Presidents Conference Board of Canada Quality Network for Universities. November 13, 2008. 01 Aug. 2009 <http://www.heqco.ca/enCA/Stay_Informed/Speeches%20and%20Presentations/Documents/Acc ountability%20versus%20Autonomy.pdf>. Doucet, Michael J. "Ontario Universities, the Double Cohort, and the Maclean's Rankings: The Legacy of the Harris/Eves Years, 1995-2003." Toronto: Ontario Confederation of University Faculty Associations, 2004. Finnie, Ross and Alex Usher. "Measuring the Quality of Post-secondary Education: Concepts, Current Practices and a Strategic Plan." Ottawa: Canadian Policy Research Networks Inc., 2005.

118

Fisher, H.K . et al. "The Report of the Committee on the Future Role of Universities in Ontario." Toronto: Government of Ontario, 1981. Government of Ontario and CCAF-FCVI. "Making Accountability Work." Summary of Proceedings Leaders' Symposium. Toronto, Feb. 1999. 01 July 2008 <http://www.fin.gov.on.ca/english/publications/2000/ccafsume.pdf>. Graham, Colin. "The Ontario Council on University Affairs: A Buffer Agency." The Philanthropist 8.4. (1989). 18 July 2009 <http://journals.sfu.ca/philanthropist/index.php/phil/article/viewArticle/285>. Harvey, Lee. "Placing Canadian Quality Assurance Initiatives in an International Context". Ontario Council on Graduate Studies Workshop. Toronto, 2008. England. Higher Education Funding Council for England. "Higher Education in the United Kingdom". London: Government of England, 2004. 30 Aug. 2009 <http://www.hefce.ac.uk/pubs/hefce/2004/HEinUK/HEinUK.pdf>. England. Higher Education Funding Council for England. "Accountability for higher education institutions: New arrangements from 2008." London: Government of England, 2007. 12 Aug. 2008 <http://www.hefce.ac.uk/pubs/hefce/2007/07_11/>. Higher Education Funding Council for England. "The Future of Higher Education." Report to Parliament of England. January 2003. 14 Aug. 2008 <http://www.dcsf.gov.uk/hegateway/strategy/hestrategy/>. Ontario. Higher Education Quality Council of Ontario (HEQCO). "Priorities and Research Agenda for the Higher Education Quality Council of Ontario: A Discussion Paper." Toronto: Province of Ontario, 2007. Higher Education Statistics Agency (HESA). "Performance indicators in higher education in the UK 2006/07." 05 June 2008. 12 Aug. 2008 <http://www.hesa.ac.uk/index.php/content/view/1166/141/>. England. Higher Education Funding Council for England. "Report of the Quality Assurance Framework Review Group." 2008/21. Bristol: Government of England. 12 Aug. 2008 <http://www.hefce.ac.uk/pubs/hefce/2008/08_21>. Hood, Christopher. "A Public Management for All Seasons." Public Administration 69. (Spring 1991): 3-19. Horn, Michel. "The Wood Beyond: reflections on academic freedom, past and present." The Canadian Journal of Higher Education 30.3. (2000): 157. Jones, Glen et al. "Marshalling Resources for Change: System Level Initiatives to Increase Accessibility to Post-secondary Education". Montreal: The Canada Millennium Scholarship Foundation, 2008. Jones, Glen A. "Ontario Higher Education Reform, 1995-2003: From Modest Modifications to Policy Reform." Canadian Journal of Higher Education 34.3. (2004): 39-54.

119

Lynn Jr., Laurence E. "New Public Management: Reform, Change, and Adaptation." Public Management: Old and New. (2006): 104-135. Lysons, Art, et al. "Comparison of measures of organisational effectiveness in U.K. higher education." Higher Education 36. (1998): 1­19. McAllister, Ian. "Growth and Governance of Canadian Universities: an Insider's View." Canadian Public Administration. 47.3 (Fall 2004): 418-421. Manning, Nick. "The New Public Management & its Legacy." The World Bank Group. 13 July 2008 <http://web.archive.org/web/20051112074952/www1.worldbank.org/publicsector/civilservice/de bate1.htm>. McKinnon, K.R. et al. "Benchmarking: A Manual for Australian Universities." Higher Education Division. Department of Education, Training and Youth Affairs. Feb. 2000. <http://www.dest.gov.au/archive/highered/otherpub/bench.pdf> 30 Aug. 2008. McMurtie, Beth. "U.S. Could Look to Europe for Accountability Ideas." The Chronicle of Higher Education. 54.38. (2008). McClellan, Ray."Accountability Initiatives in the Ontario Government." Current Issue Paper 175. Toronto: Ontario Legislative Library, 1996. Miller, Charles and Geri Malandra. "Accountability/Assessment." Issue Paper 2. Commission on the Future of Higher Education, 2006. 12 Aug. 2008. <http://www.ed.gov/about/bdscomm/list/hiedfuture/reports/miller-malandra.pdf>. Morgan, Clara 2008. "Federal Higher Education Policies and the Vanishing Public University", in Allan Maslove ed. How Ottawa Spends 2008-09: A More Orderly Federalism? (Montreal-Kingston: McGill-Queen's University Press), 179-98. Narayan, Anil. "Research Commercialization in New Zealand Universities: Public Sector Accountability and Reporting Challenges." AUT Business School. Auckland University of Technology. 16 August 2008. <http://www.afaanz.org/>. Ontario. Ministry of Research and Innovation. Results-Based Plan 2007/08. Toronto: Government of Ontario, 2007. Ontario. Task Force on University Accountability. University Accountability: A Strengthened Framework. Toronto: Government of Ontario, 1993. Ontario. Ontario Budget, 1999. Toronto: Government of Ontario, 1999. 01 July, 2008 <http://www.fin.gov.on.ca/english/budget/ontariobudgets/1999/>. Ontario. Ontario Budget, 2005. Toronto: Government of Ontario, 2005. 01 April 2008 <http://www.fin.gov.on.ca/english/budget/ontariobudgets/2005/pdf/statement.pdf>. Ontario. "Chapter II: Ontario's Economic Outlook and Fiscal Plan." Ontario Budget, 2008. Toronto: Government of Ontario, 2008. 01 April 2008 <http://www.ontariobudget.ca/english/chpt2.html>.

120

Ontario. Ministry of Training, Colleges and Universities. "Report of the Joint MTCU/COU Working Group on Reporting Requirements." Toronto: Government of Ontario, 2005. Ontario. Ministry of Training, Colleges and Universities. Proposed University Accountability Framework: Accountability for University Operating Funding in Ontario. Report of the Joint Ministry-COU Task Force on the Provincial-Auditor's 1999 Report. Toronto: Government of Ontario, 2000. Ontario. Ministry of Training, Colleges and Universities. Results-Based Plan. Toronto: Government of Ontario, 2005/06, 2006/07, 2007/08. Ontario. Office of the Auditor-General of Ontario. "Ch 2: Improving Transparency and Accountability." 2007 Annual Report. Toronto: Province of Ontario, 2007. 08 April 2008 <http://www.auditor.on.ca/en/reports_en/en07/314en07.pdf >. Ontario. Office of the Auditor General of Ontario. "Ch 2: Towards Better Accountability." 2006 Annual Report. Toronto: Province of Ontario, 2006. 13 July 2008.<http://www.auditor.on.ca/en/reports_en/en06/200en06.pdf>. Ontario. Office of the Auditor General of Ontario. "Ch. 2: Towards Better Accountability." 2005 Annual Report. Toronto: Province of Ontario, 2005. 13 July 2008 <http://www.auditor.on.ca/en/reports_en/en05/2en05.pdf>. Ontario. Legislative Assembly of Ontario: Standing Committee on finance and economic affairs. Prebudget consultations - Official Report of Debates (Hansard). First Session, 39th Parliament. Toronto: Government of Ontario, 28 January 2008. Ontario. Provincial Auditor of Ontario. 2002 Annual Report "Ch 2: Towards Better Accountability." Toronto: Government of Ontario. 12 July 2008 <http://www.auditor.on.ca/en/reports_en/en02/2en02.pdf>. Ontario. Provincial Auditor of Ontario. 1999 Annual Report "Sec. 3.13 - Accountability Framework for University Funding." Toronto: Government of Ontario. 12 July 2008 <http://www.auditor.on.ca/en/reports_en/en99/313en99.pdf>. Ontario. Provincial Auditor of Ontario. 2000 Annual Report "Ch. 2: Towards Better Accountability." Toronto: Government of Ontario. 12 July 2008 <http://www.auditor.on.ca/en/reports_2000_en.htm>. Ontario. Provincial Auditor of Ontario. 2001 Annual Report "Sec 4.13: Accountability Framework for University Funding." Toronto: Government of Ontario. 12 July 2008 <http://www.auditor.on.ca/en/reports_en/en01/413en01.pdf>. Ontario. Provincial Auditor of Ontario. 2003 Annual Report "Ch 2: Towards Better Accountability." Toronto: Government of Ontario. 13 July 2008 <http://www.auditor.on.ca/en/reports_en/en03/2en03.pdf>. Ontario. "Portals and Pathways: A Review of Postsecondary Education in Ontario". Report of the Investing in Students Task Force. Toronto: Government of Ontario, 2001.

121

Ontario. Ontario Council on University Affairs. "Resources Allocation for Ontario Universities, Advisory Memorandum 95-III." Toronto: Government of Ontario, June 1995. Ontario Confederation of University Faculty Associations. "Closing the Quality Gap: The Case for Hiring 11, 000 Faculty by 2010". Toronto: OCUFA. 6.1 (2005). Ontario Confederation of University Faculty Associations. "The Measured Academic: Quality controls in Ontario Universities." OCUFA Research paper. Toronto: OCUFA, May 2006. Ontario Confederation of University Faculty Associations. "University Degree Program Academic Quality Assessment in Ontario" Working Paper Series. Toronto: OCUFA, April 2007. Ontario Confederation of University Faculty Associations. "Tories' Public Sector Accountability Act (Bill 46) undermines universities' autonomy." Toronto: OCUFA. 08 Aug. 2008 <http://www.yufa.org/archive/2001/tories.html>. Ontario Confederation of University Faculty Associations. "Performance indicator use in Canada, the U.S. and abroad." OCUFA Research paper. Toronto: OCUFA, May 2006. 16 Aug. 2008 <http://notes.ocufa.on.ca/OCUFARsrch.nsf/9DA1693CDC3D700F852573DB006561FC/8D135 A01CDA95D56852573DA00788F7C?OpenDocument>. Ontario Confederation of University Faculty Associations. "University Funding ­ Operating". Briefing Note. Toronto: OCUFA, Jan. 2009. Ontario Council of Graduate Studies. "Terms of Reference of Transition Task Force on Quality Assurance." Toronto, 2008. Ontario Council on University Research. "Toward Improved Commercialization of Research Outcomes: A Discussion paper by the Ontario Council on University Research". Council of Ontario Universities. March 2005. (unpublished) Ontario Council on University Research. "Response to the Ministry of Research and Innovation's Strategic Plan". Toronto: Council of Ontario Universities, Jan. 2007. (unpublished) Ontario Council on University Research. "Response to the Federal Government's Science and Technology Strategy." Toronto: Council of Ontario Universities, Aug. 2007. (unpublished) Ontario Council of Academic Vice-Presidents (OCAV). "Guidelines for University Undergraduate Degree Level Expectations." Toronto: Council of Ontario Universities, Sept 2007. Ontario. Ontario Financial Review Commission. "Beyond the numbers: A new financial management and accountability framework for Ontario." Toronto: Government of Ontario, Nov. 1995. Ontario. Wright, Douglas T. et al. "The Learning Society: Report of the Commission on Post-Secondary Education in Ontario." Toronto: Government of Ontario, 1972. Oldford, Stephanie. "Exploring Options for Institutional Accreditation in Canadian Post-Secondary Education." (MA dissertation, School of Public Administration, University of Victoria, 2006). Organisation for Economic Co-operation and Development. "Education at a Glance". OECD Indicators, 2006. 122

Organisation for Economic Co-operation and Development. "Education at a Glance". OECD Indicators, 2006. Organization for Economic Co-operation and Development (OECD). "In Search of Results: Performance Management Practices." Paris, 1998. Organization for Economic Co-operation and Development (OECD). "Distributed Public Governance: Agencies, Authorities and Other Government Bodies." Paris, 2002. Organization for Economic Co-operation and Development (OECD). Universities Under Scrutiny. Paris, 1987. Organization for Economic Co-operation and Development (OECD). "Higher Education Management and Policy." Journal of the Programme on Institutional Management in Higher Education. Special Issue: Higher Education and Regional Development. 20.2 (2008). Parliament of the United Kingdom. "Science and Technology Committee - Second Report." Session 2001-02. 10 April, 2002. 20 Aug. 2009 < http://www.publications.parliament.uk/pa/cm200102/cmselect/cmsctech/507/50702.htm>. Pitman, Walter et al. "No Dead Ends: Report of the Task Force on Advanced Training to the Minister of Education and Training." Toronto: Government of Ontario, April 1993. Price Waterhouse Associates. "A Study of Management and Accountability in the Government of Ontario." Toronto, Jan. 1985. Rae, Bob. "Ontario: A Leader in Learning." Ministry of Training, Colleges, and Universities. Toronto: Government of Ontario, Feb. 2005. Reeleder, David et al. "Accountability agreements in Ontario hospitals: Are they fair?" Journal of Public Administration Research and Theory. 18.1. (2008): 161-176. Robertson, Jane et al. "For Cash and Future Considerations: Ontario Universities and Public-Private Partnerships." Ottawa: Canadian Centre for Policy Alternatives, 2003. Sappington, D. "Incentives in principal agent relationships." Journal of Economic Perspectives. 3.2 (1991): 45-66. Savoie, Donald J. "Searching for Accountability in a Government without Boundaries." Canadian Public Administration (2004): 1-26. Shah, Anwar., ed. Performance Accountability and Combating Corruption Public Sector Governance and Accountability Series. Washington: The World Bank, 2007. Shanahan, Theresa. "Accountability initiatives in higher education: An overview and examination of the impetus to accountability, its expressions and implications." Keynote Address. Conference on "Accounting or Accountability in Higher Education?" Toronto: Ontario Confederation of University Faculty Associations (OCUFA), January 23-24, 2009.

123

Shanahan, Theresa, and Glen A. "Shifting Roles and Approaches: Government Coordination of Postsecondary Education in Canada from 1995 to 2006." Higher Education Research and Development 26.1(2007): 31-43. Sims, Harvey. "Public Confidence in Government, and Government Service Delivery." Ottawa: Canadian Centre for Management Development, Mar. 2001. Smith, Dan."A decade of doing things differently: universities and public-sector reform in Manitoba." Canadian Public Administration. 47.3 (Fall 2004): 280-304. Smith, David C. et al. "Excellence Accessibility Responsibility: Report of the Advisory Panel on Future Directions for Postsecondary Education." Ministry of Training, Colleges and Universities. Toronto: Government of Ontario, Dec. 1996. Shattock, Michael. "The Impact of the UK Research Assessment Exercise." International Higher Education Boston: Center for International Higher Education (CIHE). 56. (Summer 2009). 30 Aug. 2009. <http://www.bc.edu/bc_org/avp/soe/cihe/newsletter/Number56/p18_Shattock.htm>. Smith, David C. "How will I know if there is quality?" Report on Quality Indicators and Quality Enhancement in Universities: Issues and Experiences. Toronto: Council of Ontario Universities, March 2000. Snowdon, Ken. "Without a Roadmap: Government Funding and Regulation of Canada's Universities and Colleges." Ottawa: Canadian Policy Research Networks. Research Report 31. (Winter 2005). Steele, Ken. "2007 The year in Review." London and Toronto: Academica Group, 2007. 16 Aug. 2008 http://www.academicagroucom/sites/academicagroucom/files/2007_Year_in_Review.pdf>. Saint-Martin, Denis. "Managerialist advocate or "control freak"? The Janus-faced Office of the Auditor General. " Canadian Public Administration. Vol. 47.2. (Summer 2004) p. 121-141. The Hanover Research Council. "Unit-based Cost Allocation Models at Institutions of Higher Education." Boston, July 2007. The Hanover Research Council. "Balanced Scorecards in Higher Education." Brief to McMaster University. Boston, Jan. 2008. The National Center for Public Policy and Higher Education. "Measuring Up 2006: The National Report Card on Higher Education." 2006. 31 Aug. 2008 <http://measuringuhighereducation.org/_docs/2006/NationalReport_2006.pdf>. Thompson, Laura. "Auditor's report examines U of G practices." Guelph Mercury 12 Dec. 2007. Thomas, Paul G. "The Swirling Meanings and Practices of Accountability in Canadian Government." Power, Professionalism and Public Service: Essays in honour of Kenneth Kernaghan Eds. David Siegel and Ken Rassmussen. Toronto: University of Toronto Press, 2008.

124

Thomas, Paul G. "Ministerial Responsibility and Administrative Accountability." New Public Management and Public Administration in Canada Eds. Mohamed Charih and Arthur Daniels. Ottawa: Institute of Public Administration of Canada, 1997. Thomas, Paul G. "The past, present and future of officers of Parliament." Canadian Public Administration. 46.3. (Fall 2003): 288-314.

Tibbetts, Janice. "Benchmarks urged for higher education: Council on Learning Report questions value of university." National Post 12 Dec. 2007. Toma, Douglas J. "Expanding peripheral activities, increasing accountability demands and reconsidering governance in US higher education." Higher Education Research & Development 26.1. (2007): 57-72. Tornatzky, Louis G. et al. "Innovation U.: New University Roles in a Knowledge Economy." Southern Growth Policies Board, 2002. 07 April 2008 <http://www.southern.org/pubs/pubs_pdfs/iu_report.pdf>. Trick, David. "The Politics of Government-University Relations." (Phd Dissertation, University of Toronto, 2006), 1-425. University of Alberta. "University Plan, 2007-2011." 23 Mar 2007. 31 Mar 2008 <http://www.uofaweb.ualberta.ca/strategic//pdfs/2007-11_UnivPlan_Final.pdf>. USA. U.S. Department of Education. "A Test of Leadership: Charting the Future of U.S. Higher Education." A Report of the Commission Appointed by Secretary of Education Margaret Spellings. Washington: Government of USA, 2006. Van Loon, Richard. "The Ontario Graduate Program Appraisal Review." Toronto: Council of Ontario Universities. 12 Nov. 2007. Wolfe, David A. "The Role of the provinces in PSE Research Policy". A Discussion Paper for the Council of Ministers of Education. Toronto: CMEC. Mar. 1998. 20 Aug. 2008 <http://www.cmec.ca/postsec/wolfe-e.pdf>.

125

ENDNOTES

1

The Canadian Society for the Study of Higher Education (CSSHE) was founded in 1970 for people conducting or using research in postsecondary education, whose purpose is "the advancement of knowledge of postsecondary education through the dissemination through publication and learned meetings." See http://measuringuhighereducation.org for more about the US report cards. See http://www.rae.ac.uk/ for more on the Research Assessment Exercise of the United Kingdom. See http://www.indirectcosts.ca/about/index_e.asp for more on federal funding of indirect costs of research.

2

3

5

See http://ocgs.cou.on.ca/content/objects/By-Laws%20&%20Procedures%20January%202008.pdf for OCGS bylaws and procedures for appraisals. See http://data.cou.ca/WDS/ for copies of the Financial Report of Ontario Universities. Council of Ontario Universities.
9 7

6

The Broadhurst report made reference to using some of the indicators developed by the COU Committee on Accountability, Performance Indicators and Outcomes Assessment. See http://www.cou.on.ca/_bin/affiliates/associations/upracmain.cfm for more on the work of UPRAC.

10

The Ministry of Training, Colleges and Universities (MTCU) was created in June 1999 by the Harris government. It used to be part of Ministry of Education and Training (MET). MET was created when the Ministry of Colleges and Universities (MCU) was integrated with Ministry of Education in 1993 by the Rae government.
14

12

A change in the Constitution of OCAV in October 2007 created a Quality Assurance Committee of Ontario which would oversee the quality assurance process. The COU conducted a review of its activities from 2005-2007 and Phase 3 involved a review of the OCGS.

15

OCGS conducted a Workshop on Quality Assurance, September 16, 2008 in Toronto as part of the review of the quality appraisal process. See Public Appointments Secretariat website for more on the HEQCO appointments, http://www.pas.gov.on.ca/scripts/en/BoardDetails.asp?boardID=141400 See website of the Ministry of Research and Innovation: http://www.mri.gov.on.ca/english/programs/ORCProgram.asp For more information, see the report on Quality and Productivity in Ontario Universities at http://www.cou.on.ca/content/objects/QPTF%20Report%20March%202006.pdf
21 20 18 17

16

The universities monitor their facilities condition via a common database, VFA, which shows the current deferred maintenance bill at $1.6 billion. See http://www.voluntarysystem.org/index.cfm See www.nsse.edu for more about the NSSE survey.

22 23

126

24

See http://csrde.ou.edu/web/index.html for more about the CSRDE survey.

127

