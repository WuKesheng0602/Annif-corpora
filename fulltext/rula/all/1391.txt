Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2010

An empirical application of data envelopment analysis in credit rating
Mariya Demirova
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Mechanical Engineering Commons Recommended Citation
Demirova, Mariya, "An empirical application of data envelopment analysis in credit rating" (2010). Theses and dissertations. Paper 981.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

An Empirical Application of Data Envelopment Analysis in Credit Rating
by Mariya Demirova B.Sc (Hons) York University, 2008 A Thesis Presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Applied Science in the program of Mechanical Engineering

Toronto, Ontario, Canada, 2010 © Mariya Demirova, 2010

Author's Declaration

I hereby declare that I am the sole author of this thesis.

I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

Signature_______________

I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Signature________________

ii

An Empirical Application of Data Envelopment Analysis in Credit Risk

Mariya Demirova Master of Applied Science Graduate Program in Mechanical Engineering Ryerson University 2010

Abstract Data Envelopment Analysis (DEA) is a nonparametric optimization technique that evaluates the relative efficiency of decision-making units and is used in this thesis as an empirical estimator of credit rating. The purpose of this research is to combine different DEA models and technique and obtain the best model that captures different aspects of credit risk. Various models are evaluated by combining four Slack DEA models with Principal Component Analysis (PCA), Absolute Weights Restriction, and Stochastic DEA. We found that Goal Vector Approach Stochastic PCA (SGV+PCA), applied to a sample consisting of five sectors, is the best model. SGV+PCA DEA model obtains a high correlation with Standard & Poor's (S&P) credit rating and with Market Price; it also classified twelve bankrupted companies within the 17% of the less efficient companies in the sample, suggesting that the model is a good financial health estimator and is a potential tool for credit rating analysis.

iii

Acknowledgements

I would like to express my sincere and thankful feelings to Dr. Mohamed Wahab Mohamed Ismail for his continuous support, guidance, encouragement, and patience through the Master Program. His guidance and suggestions always helped and inspired me during the study process.

Words cannot explain my gratitude to my parents Entcho Demirov and Irina Demirova and to my sister Kalina Demirova for all their love, support, guidance, and understanding, that I have been so lucky to receive. Thanks to you I passed through so many difficulties and managed to became the person I am today.

I would like to also thank my friends Ahmed El Saadany, Mehrnoosh Azem, Brandon Slopack, and Genadi for all their strong support through the last years.

iv

Table of Contents
Author's Declaration ....................................................................................................................... ii Acknowledgements ........................................................................................................................ iv Table of Contents ............................................................................................................................ v Nomenclature ............................................................................................................................... viii List of Tables ................................................................................................................................ xii List of Figures .............................................................................................................................. xiii CHAPTER 1: INTRODUCTION ................................................................................................... 1 CHAPTER 2: LITERATURE REVIEW ........................................................................................ 6 2.1 2.2 The beginning of DEA .................................................................................................... 6 Expansion of DEA techniques ......................................................................................... 7 New types of DEA models ....................................................................................... 8 New DEA techniques ............................................................................................... 8 Combination of DEA with other known procedures .............................................. 10 DEA properties and concerns ........................................................................................ 11 Other frontier efficiency estimation techniques ............................................................. 12 Credit Rating and DEA ................................................................................................. 13

2.2.1 2.2.2 2.2.3 2.3 2.4 2.5

CHAPTER 3: DATA ENVELOPMENT ANALYSIS ................................................................. 16 3.1 3.2 3.3 3.3.1 3.3.2 CCR model: Multiplier form .......................................................................................... 16 CCR model: Envelopment form ..................................................................................... 18 Alternative DEA models ............................................................................................... 19 BCC model............................................................................................................. 19 DEA slack models.................................................................................................. 19

3.3.2.1 The Additive model ............................................................................................... 19 3.3.2.2 The Slack-Based Measure of efficiency model (SBM) ......................................... 22 3.3.2.3 Measure of Inefficiency Proportions model (MIP) ................................................ 23 3.3.2.4 3.4 3.5 3.6 The Goal Vector Approach (GV) .................................................................... 24

Principal Component Analysis ...................................................................................... 25 Principal Component Data Envelopment Analysis (PCA-DEA) ................................... 26 Stochastic DEA .............................................................................................................. 28
v

3.6.1 3.6.2 3.6.3

Interpretation of SDEA ........................................................................................... 29 SDEA model formulation ....................................................................................... 31 Deterministic equivalent ......................................................................................... 33

CHAPTER 4: MODEL FORMULATION ................................................................................... 36 4.1 Variables selection ......................................................................................................... 37 Number of variables and DEA ................................................................................ 38 Selected variables for the analysis .......................................................................... 39 Correlation between variables................................................................................. 42 Grouping variables into inputs and outputs ............................................................ 43

4.1.1 4.1.2 4.1.3 4.1.4 4.2 4.3 4.4

Sectors ............................................................................................................................ 44 Radial vs slack models .................................................................................................. 48 Application of DEA techniques ..................................................................................... 51 Goal Vector Approach and Goal Vector Approach +Weights Restrictions .......... 52 GV+PCA and GV+PCA+WR ................................................................................ 54 PCA dropping Rule ........................................................................................ 58 Stochastic Goal Vector (SGV) ................................................................................ 59 Stochastic Goal Vector + Principal Component Analysis (SGV+PCA) ............... 61

4.4.1 4.4.2 4.4.2.1 4.4.3 4.4.4

4.5 Returns to scale ................................................................................................................... 62 4.6 Performance evaluation validation ................................................................................. 63 Market value ........................................................................................................... 64 Comparison to expert ­ Standard & Poor's (S&P) ................................................ 64

4.6.1 4.6.2 4.7

Statistical analysis of correlation.................................................................................... 65 Correlation .............................................................................................................. 65 P-value of correlation .............................................................................................. 67 Correlation confidence interval............................................................................... 67 Kolmogorov-Smirnov test (K-S test) ...................................................................... 68

4.7.1 4.7.3 4.7.3 4.7.4

CHAPTER 5: RESULTS AND DISCUSSION............................................................................ 69 CHAPTER 6: CONCLUSION AND FUTURE RESEARCH ..................................................... 86 6.1 Conclusion........................................................................................................................... 86 6.2 Future Research .............................................................................................................. 87

APPENDIX I ................................................................................................................................ 90
vi

A.I.1 The CCR model ............................................................................................................... 90 A.I.2 The BCC model ............................................................................................................... 93 A.I.3 A.I.4 A.I.5 Returns to Scale (RTS) ............................................................................................... 94 Variables weights restrictions ..................................................................................... 98 Advantages and disadvantages of DEA.................................................................... 101

Advantages .............................................................................................................................. 101 Disadvantages ...................................................................................................................... 102 A.I.6 Negative data in DEA ............................................................................................... 104

A.I.7 Principal Component Analysis - Methodology.............................................................. 105 APPENDIX II ............................................................................................................................. 108 A.II.1 A.II.2 Identification of RTS for points that are not on the frontier ................................... 108 Formulation of the multiplier's dual ....................................................................... 109

A.II.3 Deterministic equivalent of Stochastic DEA formulation ............................................ 113 REFERENCES ........................................................................................................................... 119

vii

Nomenclature

n

Number of units in the sample Number of inputs Number of outputs Input matrix

m
s
xij

yrj

Output matrix

i

Index for inputs Index for outputs Index for number of units in the sample Weight attributed to each input in the multiplier form

r

j
vi
ur


Weight attributed to each output in the multiplier form Radial efficiency Unknown optimization variables of the envelopment form Vector of ones Input slack



e
s-

viii

s+

Output slack Non-Archimedean element Matrix of input data Matrix of output data Unknown variable, defining the returns to scale for the multiplier form Constant Value of lower assurance region restriction Value of upper assurance region restriction Range for the i -th input


X
Y

uo

c



Ri-
+ Rr

Range for the r -th output Reciprocal of Ri-
+ Reciprocal of Rr

gi-

+ gr


ej
L
U

Value of assurance region restriction Efficiency of unit j

Upper bound of weights restrictions Lower bound for weights restrictions

ix

t

Vector of dual variables, reflecting the upper weights restriction constraints in the multiplier form for inputs

b

Vector of dual variables, reflecting the upper weights restriction constraints in the multiplier form for outputs

t pca Vector of dual variables, reflecting the upper weights restriction constraints in the
multiplier form for inputs, involved in PCA

b pca Vector of dual variables, reflecting the upper weights restriction constraints in the
multiplier form for outputs, involved in PCA
W

Correlation matrix for outputs Eigenvector matrix for W Eigenvector, obtained from W Matrix of transformed by PCA input observations

L

l

X PC YPC
p

Matrix of transformed by PCA output observations

Number of dropped principal components for the outputs Number of dropped principal components for the inputs

q

+ -  PC , PC

PCA slack variables for inputs

- + pc , pc

PCA slack variables for inputs

x


r y

Level of significance Vector of random outputs Vector of random inputs

i x



Number of input variables transformed through PCA Number of output variables transformed through PCA Expected value operator


E

 -1
µ

Inverse of the normal cumulative distribution Average value of a variable Correlation operator



 pca Standard deviation of PCA components
 ) / 2 quintile of the standard normal Z(1- p  )/2 The (1 + p

y

Vector of outputs standard deviations

xi

List of Tables
Table 4.1 Input/output variable selection...................................................................................... 42 Table 4.2: Variables correlation matrix ........................................................................................ 43 Table 4.3 Sectors selection ........................................................................................................... 45 Table 4.4 Correlation between Market Price and model variables ............................................... 50 Table 4.5 Standard & Poor's rating scheme description............................................................... 66 Table 5.1.1 P-value of correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency .............................................................................................................................. 76 Table 5.1.2 P-value of Kolmogorov-Smirnov test for the significance of correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency ......................................... 77 Table 5.1.3 95% Confidence interval for the correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency............................................................................................ 78 Table 5.2.1 P-value of correlation between S&P credit rating and Goal Vector Approach Stochastic PCA efficiency ........................................................................................................... 82 Table 5.2.2 P-value of Kolmogorov-Smirnov Test for the significance of correlation between S&P credit rating and Goal Vector Approach Stochastic PCA efficiency .................................. 82 Table 5.2.3 95% Confidence Interval for the Correlation between Standard & Poor's Credit Rating and Goal Vector Approach Stochastic PCA efficiency .................................................... 83 Table A.1 Comparison of used variables in different credit rating analysis ............................... 117 Table A.2 Nomenclature of financial ratios ................................................................................ 117

xii

List of Figures

Figure 3.1

Translation of origin for (a) Additive model (b) BCC model ................................. 21

Figure 3.2 Axis rotation for a) dispersed data b) moderately dispersed data ............................. 26 c) highly correlated data................................................................................................................ 26 Figure 3.3 Stochastic efficient frontier ......................................................................................... 31 Figure 5.2 Normal probability density function of best practice output and observed output ..... 33 Figure 4.1 Flowchart of the organization of the model formulation............................................. 37 Figure 4.2 Performance of dataset, divided into sector vs performance of dataset, consisting in only one sector .............................................................................................................................. 47 Figure 4.3 Correlation between Market Price and efficiency for four slack models .................... 49 Figure 4.4 Evaluation of six final models used for credit rating assessment ................................ 52 Figure 4.5 Convergence of upper weight restriction .................................................................... 53 Figure 4.6 Correlation between Stochastic DEA and Market Price for different levels of alpha. 60 Figure 5.1.1 Correlation between Market Price and efficiency for Sector 1 ................................ 71 Figure 5.1.2 Correlation between Market Price and efficiency for Sector 2 ................................ 73 Figure 5.1.3 Correlation between Market Price and efficiency for Sector 3 ................................ 74 Figure 5.1.4 Correlation between Market Price and efficiency for Sector 4 ................................ 75 Figure 5.1.5 Correlation between Market Price and efficiency for Sector 5 ................................ 76 Figure 5.2.1 Correlation between S&P credit rating and efficiency for Sector 1 ......................... 79 Figure 5.2.2 Correlation between S&P credit rating and efficiency for Sector 2 ......................... 79 Figure 5.2.4 Correlation between S&P credit rating and efficiency for Sector 4 ......................... 80 Figure 5.2.5 Correlation between S&P credit rating and efficiency for Sector 5 ......................... 81 Figure A.1 CCR model: An example............................................................................................ 90 Figure A.2 BCC model: An example............................................................................................ 94 Figure A.3 Interpretation of returns to scale ................................................................................. 96

xiii

CHAPTER 1: INTRODUCTION

One of the most popular ideas in finance is that the terms and conditions of borrowing should be related to the riskiness of the borrower. The reason is obvious ­ lenders are rewarded with a higher rate for a higher risk that the borrowers do not repay. Similarly, lower risk companies pay lower interests rates and enjoy a higher potential for investors. This idea, however, is not that easy to implement, because of the difficulty to measure the precise riskiness of the borrowers' assets. This difficulty creates the need for a specialized assessment of credit riskiness, in order to estimate the creditworthiness of borrowers and to eliminate the asymmetric information between borrowers and lenders. The first published credit rating can be traced back to 1909 when John Moody estimated the credibility of the US railroad bond. He was followed by Henry Poor in 1923. They created their own credit agencies, which today have grown into the biggest and most successful credit rating evaluators, and are among the few statistical organizations, recognized by the US Securities and Exchange Commission. These two companies' overall performance has been subjected to many discussions and debates. The ratings of both agencies tend to be very similar, which raises some concerns about their independence. While the correlation between bankruptcy prediction and credit ratings tends to be positive, both companies failed to predict some of the major global financial crisis. Examples include bankruptcies of several companies during the East Asian crisis in 1997 and 1998, which was followed by excessively conservative ratings for the East Asian companies, contributing, as claimed by several authors (e.g. Ferri et
1

al., 1997), to an amplification of the crisis. Another example is the high credit rating of the subprime securitizations or Residential Mortgage-Backed Securities (RMBS) whose downgrade was one of the factors that initiated the global economic crisis in 2007. Another concern about the credibility of credit rating companies is the use of the so called black boxes. A black box is a computer program that receives an input and produces an output, but the model or technique, used for the final result remains unknown to the public. The biggest disadvantage of the black box use is the impossibility to observe the process and the parameters that produce the final result and the fact that they could change with time. For the last twenty years, researchers expanded the credit risk analysis by introducing more sophisticated and early-warning systems of analysis. Earlier, most financial institutions used the 4 Cs credit model - borrower character, capital (leverage), capacity (volatility and earnings) and collateral. Sommerville and Taffler (1995) proved that the model is a worse performer than the multivariate credit scoring system (MCSS) analysis that consists of a weighted combination of selected variables, producing a credit risk score, which is compared to a critical benchmark. The four categories of the MCSS have been widely in use since then. These four models are: the linear probability model, the logit model, the probit model, and the discriminant analysis. Another category of credit rating models is the "risk of ruin" models, which calculate the probability that the market value of the company assets falls below its debt obligations to creditors. In calculating the probability, those models estimate the volatility of the market value of the company's assets. Examples of "risk of ruin" models include the option pricing model (OPM), the Black and Scholes (1973), and the Hull and White (1990) model. Another approach is
2

the fixed income portfolio analysis, developed by Markowitz (1959) that maximizes the expected value of portfolio return for a given level of risk or minimizes the estimation of risk for a given level of return. The importance of credit risk evaluation contributed to the development of a variety of sophisticated models and significant advancements in that direction. While most of the existing models concentrate on more complex statistical techniques assuming a precise type of variables relationships and distributions, little research has been done on the application of comparative models in credit rating. The purpose of this thesis is to fill this gap by exploring the potential use of DEA in credit rating. The research questions are: Is DEA efficient in estimating financially healthy and unhealthy companies? Are the estimates of DEA credit rating are consistent with time? Is DEA a good performer of credit rating compared to other credit rating evaluators? Can DEA capture potential default of companies? We will try to answer to these questions and fill part of the gap of the potential use of DEA for credit rating analysis. The main advantage of DEA is that it does not concentrate on process function assumptions and on finding a universal relationship for all the units in a sample, but it allows for every unit to have a different production process (the process that converts inputs into outputs) and it evaluates the efficiency for every unit in a dataset by defining the most efficient units and then comparing the rest of the units to the efficient ones. The efficiency evaluation of inefficient companies is determined entirely by DEA defined efficient companies, rather than by theoretical assumptions and implications. That is why the practical approach of DEA can be a reason for choosing it as an effective credit risk evaluator.
3

DEA is found to be a one of the good estimators of credit rating by classifying correctly most of the financially unhealthy companies (e.g. Paradi et al., 2004). In most of the literature, the research is limited by simply applying one of the three main DEA models ­ the Charnes, Cooper, and Rhoades (CCR) (e.g. Pille and Paradi, 2001, Emel et al., 2003, and Zhou et al., 2008), the Banker, Charnes, and Cooper (BCC) (e.g. Chen, 2008) or the Additive model (e.g. Premachandra et al., 2009). In this thesis, however, we contribute to the research done on DEA in credit rating, by applying techniques as slack DEA models, Absolute Weights Restriction Constraints, Principal Component Analysis, and Stochastic DEA that according to our knowledge have never been applied in the literature in credit rating. We test the performance of these techniques on 235 companies from years from 1995 to 2006 and find the Stochastic Goal Vector Approach + Principal Component Analysis model (SGV+PCA) as the best. This model achieves a high correlation with the credit rating, evaluated by the Standard & Poor's (S&P) rating agency. It also achieves a rather high correlation with companies' Market Price, providing evidence that SGV+PCA model can be used by companies interested in financial market investments. Finally, the model classified companies, which filed for bankruptcy, among the most inefficient companies in the sample two to three years before the bankruptcy occurred, providing evidence that SGV+PCA model could also be used for a bankruptcy prediction. The reminder of the thesis is organized as follows. Chapter 2 presents the literature review. Chapter 3 describes the main features of DEA models and different types of DEA. Chapter 4 presents the PCA-DEA. Chapter 5 explores the Stochastic DEA (SDEA). Chapter 6 explains and provides details about the formulation of the different

4

DEA models, used to evaluate credit rating. Finally, Chapter 7 summarizes the results, and Chapter 8 describes the conclusions, and identifies possible areas of future research.

5

CHAPTER 2: LITERATURE REVIEW

2.1

The beginning of DEA

The first traces of Data Enveloped Analysis (DEA) can be found in Farrell (1957), who introduced the concepts of non-parametric production function and efficient frontier. He was the first to combine the idea of "productivity" with the concept of "efficiency". Farrell, however, failed to generalize the multiple outputs model with constant returns to scale. This problem was solved by Charnes et al. (1978) who introduced the (Charnes, Cooper, and Rhodes) CCR model, and marked the beginning of DEA. The CCR model assumes constant returns of scale, which implies that all decision making units in the sample operate at an optimal scale. CCR measure of efficiency is also known as technical efficiency: a measure of the production process or decision making units' capacity to produce maximum output from minimum input. CCR model was later extended by Banker et al. (1984) in the Banker, Charnes, and Cooper (BCC) model, in which they transformed the shape of the production frontier from linear to concave. The new frontier allows treatment of units, characterized with variable returns to scale and provides an estimate for technical and scale efficiency which measures whether units operate at their most efficient size. The BCC and CCR models compose the group of radial DEA models. The radial efficiency represents the amount by which all the inputs have to be decreased and all the outputs have to be increased simultaneously in order to obtain full efficiency. The

6

technique behind the DEA optimization process is to identify the radially "efficient" units in the sample and compare the rest to them. The possibility to treat simultaneously multiple inputs and multiple outputs, and the absence of a production function requirement encourage the use of DEA in many types of research. The latter characteristic is particularly useful when the relationship between variables is hard to estimate or even unknown. DEA is a useful tool for many managerial decisions. By identifying the efficient units, it can provide some helpful insights on how to reduce inefficiency and production costs or increase service, without increasing utilized resources, by simply changing the combination of inputs.

2.2

Expansion of DEA techniques

During the years following the birth of BCC and CCR models, the research on DEA bloomed in theoretical and methodological fields and in the variety of its practical applications. DEA is used in many areas such as manufacturing (e.g., Mahadevan, 2002, Wahab et al., 2008), air transport (e.g., Tongzon, 2001), telecommunications (e.g., Cooper et al., 2001a), banking (e.g., Vassiloglou and Giokas, 1990), healthcare (e.g., Jacobs, 2001), education (e.g., Athanassopoulos and Shale, 1997), library services (e.g., Hammond, 2002), mining operations (e.g., Thompson et al., 1995), financial services (e.g., Smith, 1990), agriculture (e.g., Reig-Martìnez and Picazo-Tadeo, 2004), public sector (e.g., Smith and Mayston, 1987), etc. The success of DEA was shortly followed by numerous introductions of new DEA models, new DEA inspired techniques, and combinations of DEA with already existing approaches.
7

2.2.1

New types of DEA models

Charnes et al. (1985) introduced the Additive or Pareto-Koopmans model, creating a new DEA group - the Slack based models. Contrary to the radial models, the optimized objective function value is a sum of all individual variable inefficiencies. This is done by calculating the sum of every variable slack, which represents the distance of the inefficient Decision Making Units (DMUs) to the nearest efficient one. A disadvantage of the Additive, (as well as BCC and CCR) however, is that it is not unit invariant, i.e., the efficiency for variables, measured in different units is not identical to the efficiency of variables measured in the same units. Consequently, several slack models possessing the unit invariant property were introduced. The most popular are the Slack Based model of Efficiency (Tone, 2001), the Mix Efficiency Model (Cooper et al., 2000), and the Goal Vector Approach model (Thrall, 1996).

2.2.2

New DEA techniques

Some interesting and useful DEA techniques include the super-efficiency model (Andersen and Petersen, 1993), the cross-efficiency analysis (Sexton et al., 1986), the worst practice DEA (Paradi et al., 2004), and the layering technique (Divine, 1986). The super-efficiency DEA excludes the DMU under evaluation from the reference set during the process of efficiency optimization. By doing so, the unit cannot be a reference for itself and hence, it is compared only to the rest of the units in the sample. The method is useful to evaluate the efficiency of outliers in the case their removal is favorable for the

8

analysis. The super-efficiency method is also used when the number of units in the sample is considered as not sufficient. One of the most intriguing DEA properties is its ability to choose the weights of inputs and outputs in a way that represents the DMU in its best possible light. This procedure, however, could lead to some undesirable results. The cross-efficiency method can avoid those results by replacing the weights of the DMU under evaluation by the other units' weights. Therefore, for every DMU, the DEA model is run for a number of times equal to the number of units in the sample, changing the combination of weights on each run and obtaining a number of efficiency evaluations equal to the number of units in the sample. The final efficiency measure is then obtained by the average of the efficiencies, calculated at every step. Worst practice DEA is another technique that can be found useful in several situations. Here, the technical efficient frontier is constructed by the most inefficient points, i.e., the points that are best in being worst. Subsequently, every unit is compared to the worst DMUs. The model is useful in cases when finding the most inefficient peers is more important that finding the efficient ones. The layering or peeling technique can be applied on any type of DEA model. Using this approach the units on the frontier are removed and the model is run without them. On the next step, the new efficient points are removed again and the model is run without them and so on. This allows for the creation of groups with similar efficiency.

9

2.2.3

Combination of DEA with other known procedures

The success of DEA in different fields was followed by the introduction of different combinations between DEA and several statistical approaches. Some authors used a mixture of DEA and Discriminant Analysis (DA) (Sueyoshi, 1999; Sinuany-Stern and Friedman 1995). Discriminant Analysis is used to divide a set of observations into two or more categories, characterized by similar structure. Usually, the purpose of DEA in DA is to classify points that are hard to classify by Discriminant Analysis in any of the two categories. DEA has been successfully combined with Canonical Correlation Analysis for example by Friedman and Simuany-Stern (1996). In his work, the eigenvalues of the principal correlation component are used as values for the weight restrictions in DEA. The idea of combining Principal Component Analysis with DEA (PCA-DEA) was developed by Ueda and Hoshiai (1997). Principal Component Analysis (PCA) is a good technique that "cleans" the variables from random noises and decreases the number of dimensions, included in the optimization. PCA-DEA consists in creating new uncorrelated principal components and running the DEA optimization with the transformation through PCA data. The approach is useful in the cases where variables are highly correlated or the number of variables is high. It can also be used simply as a tool that cleans the data from random and unnecessary information. An alternative approach dealing with highly correlated variables is to simply ignore them and not include them in the analysis. Jenkins and Anderson (2003), however, show that omitting even highly correlated variables could lead to a major difference in the efficiency scores.

10

DEA was also successfully combined with linear regression (Brockett et al., 2004), Fuzzy logic (Lertworasirikul et al., 2003), Neural Networks (Wu et al., 2006), Logistic regression (Cook and Bala, 2007) and others.

2.3

DEA properties and concerns

DEA has some pleasant statistical properties. First, DEA estimators are consistent. Second, they also converge to their true value faster than the estimators of any other frontier analysis approach (Grosskopf, 1996). Third, Banker (1993) shows that "the DEA estimation is equivalent to maximum likelihood estimation." And finally, the DEA estimates are unbiased when no form of production function is being assumed (Kittelsen, 1999). DEA has also some disadvantages that need to be considered carefully. A disadvantage of the radial models is the impossibility to tackle negative data. Ali and Seiford, (1990) partially solved it, but only for the use of variables containing zero values. Charnes et al. (1991) also relaxed the requirement of positive data, but the decision making units were still obligated to contain at least one positive input and one positive output. The solution of the problem was for the first time theoretically developed by Pastor (1994). He was the first one to define the translation invariant property (the property that allows efficiency evaluation of variables, containing negative data by simply adding a positive constant to the variables) for the two radials (CCR and BCC models) and the Additive Model. He proved that the last one is translation invariant, the CCR is not and the BCC is in

11

between. The Additive Model is a frequent choice in several studies, because of its translation invariant property. Another DEA disadvantage is considered to be the fact that DEA is extremely sensitive to outliers and measurement errors. This problem can be detected by simply removing the outliers from the sample (e.g., Avkiran, 2006). This will, however, deprive the outliers from an efficiency measure. An alternative solution can be the use of super-efficiency analysis.

2.4

Other frontier efficiency estimation techniques

The method closest to DEA, which evaluates efficiency, is the Stochastic Frontier Approach (SFA), known also as the "composed error" approach, which was formulated by Lovell et al. (1977) in its current version. The production function is assumed to be known, and it is usually the Cobb-Douglas function. The error term is divided into two independent components: one accounting for the usual statistic noise, found in any variables' relationship, and the other representing the error term of technical inefficiency. The final inefficiency is expressed as a combination of the mean and variance of the production function distribution. SFA and DEA can be regarded as mutually exclusive and are often used as comparative methods for efficiency evaluation. The Distribution Free Approach (DFA) was formalized by Berger (1993), but similar ideas can be found in the work of Schmidt and Sickles (1984). DFA consists in the evaluation of cost and profit functions, which are estimated by input and output costs. Similarly to DEA, DFA also uses a production frontier, which is composed of "efficient"
12

units and performs as a point of comparison for inefficient ones. The deviation from the frontier, however, is a sum of inefficiency term, assumed to be constant with time, and a random error term. The last one represents the potential of measurement errors or random fluctuations in inefficiency and is assumed to average out to zero within the time period. Stochastic DEA (SDEA) was first developed by Charnes and Cooper (1963). A major contribution to SDEA can be found in the work of Sengupta (1987), Sengupta (1990), and Sengupta (1998). Stochastic DEA allows for the possibility of stochastic variations among the data. SDEA is implemented by constructing confidence intervals around each unit on the efficient frontier. The confidence interval reflects the likelihood that the observed variable value could be different from the actual variable value and the confidence interval width depends on the variability level of the data. Stochastic DEA can be used in the cases, where exist uncertainty about the true value of the data observations. Such examples include measurement errors, missing values, estimated variables, etc.

2.5

Credit Rating and DEA

Most of DEA research in credit rating has been somehow limited in simply applying the three main models - CCR, BCC, and the Additive model using different combinations of financial ratios. This creates a potential for more research in this credit rating area involving a high variety of DEA techniques. Paradi et al. (2004) used a combination of worst practice DEA and layering technique yielding 100% bankruptcy and 78% nonbankruptcy prediction. Barr et al. (1994) used DEA to estimate managerial efficiency and
13

then include it into the widely used CAMELS model. Pastor (2002) created a credit loan banking system composing of two stages. In the first stage the efficiency of loaners is estimated by DEA, and in the second stage, the obtained efficiency is used as an independent variable in a logit regression, where the dependent variables are macroeconomic indexes. Involving the second stage allows estimating whether inefficiency is due to poor management skills or to bad economic factors. Notice that DEA theory offers a wide variety of practices and applications that were developed in the last decade. Surprising, however, is the fact that many of these techniques have not been applied in Credit Rating. As mentioned earlier in the chapter most literature on DEA on Credit Rating simply applies one of the three main models: the BCC, the CCR, and the Additive. All of these models have some disadvantages that need to be considered by combining them with some DEA techniques and simply applying these three models can lead to undesirable results. A limitation of the existing literature on the application of DEA in Credit Rating is that very few of them include the possibility of stochastic variation of the variables, which is rather restrictive, given the volatile nature of this area. In this research, however, two of the six developed models, include the implication of stochastic variation of the data. Another limitation of the existing literature on the application of DEA in Credit Rating is the fact that according to our knowledge none of the paper considers the correlation between variables. The variables, used in the analysis, are usually financial ratios and hence normally one balance index can take part in the formulation of different financial ratios, leading to a correlation between variables. We fix this problem by including PCA technique in three of the six final models. PCA is a method that creates new uncorrelated axes which
14

"cleans" the data. Hence, in the next chapters we will develop six different models, that consider different aspects of these issues and we will show that DEA can be indeed a good credit rating tool. Among these six models, the model that simply applies a DEA Slack model is shown to have a lower level of performance, justifying our attempt of an exploration and combination of different DEA techniques.

15

CHAPTER 3: DATA ENVELOPMENT ANALYSIS

3.1

CCR model: Multiplier form

Let us consider a sample of n decision making units that can be companies, hospitals, airports, governmental agencies, etc, and whose efficiency evaluation is the goal of the optimization problem. Every Decision Making Unit (DMU) is characterized with a particular unknown production function that consumes m inputs and produces s outputs. Similar to every production process, the goal of each unit is to produce high amounts of outputs, using low amounts of inputs. DEA uses the ratio of the output linear combination to input linear combination as a measure of efficiency. The units that reach a maximum value of this ratio are defined as efficient. In order to maximize the linear combinations of inputs and outputs it is necessary to introduce unknown variables. Let xij be the input observation matrix, where xij = ( x1 j , x2 j ,..., xmj ) for j = 1,..., n . Let yrj be the output observation matrix, where yrj = ( y1 j , y2 j ,..., yrj ) for j = 1,..., n . The CCR model is given as follows: Maximize subject to:

 = u1 y1o + ... + us yso
v1 x1o + v2 x2 o + ... + vm xmo = 1
u1 y1 j + ... + us ysj  v1 x1 j + ... + vm xmj

j = 1,..., n,

(3.1)

v1 , v2 ,..., vm  0 , u1, u2 ,..., us  0.
16

In Problem (3.1)  is the radial component of efficiency, vm represents the weight attributed to each input variable, and ur represents the weight attributed to each output variable. For each DMU in the sample, it is necessary to run a separate optimization model by changing the values in the objective function for the corresponding DMU under evaluation. Notice that Problem (3.1) is the output-oriented BCC model. If we substitute

= v1 x1o + ... + vm xmo and we substitute the first constraint the optimization function by  1 we will obtain the input-oriented BCC of Problem (3.1) by u1 y1o + u2 y2 o + ... + us yso =
(for an example see Cooper et al., 2007). An important DEA characteristic is that the feasibility region, composed of the area determined by the second set of equations in Problem (3.1), is composed of real data observations, representing all the units in the sample. Hence, the notion of efficiency does not depend on any posterior information about the production function behaviour or the characteristics of the variables, but rather on how well a particular unit managed to produce a maximum amount of outputs, investing a minimum amount of inputs, compared to the other units in the sample. In that sense, the estimate of efficiency for DEA is somehow relative, because a DMU can be defined as efficient in one sample but its status can be changed by only including a unit which uses fewer inputs, producing the same amount of outputs. .

17

3.2

CCR model: Envelopment form

The dual of the Multiplier problem in (3.1), also known as CCR Envelopment Form is expressed as: Minimize subject to:


 xo - xi11 - ... - xij  j  0
yr1 j + ... + yrj  j  yro

i = 1,..., m, r = 1,..., s, j = 1,..., n.

(3.2)

 j  0,

In Problem (3.2)  j are the unknown variables of the Envelopment Form. See Appendix II on how to obtain the Envelopment Form from Problem (3.1). The interpretation of  j in Problem (3.2) is easier and more intuitive than the interpretation of the weights in Problem (3.1). Note that when  j = 1 in the second set of constraint in Problem (3.2), the input values of DMU o are compared to the input values of DMU j . Let us observe the following example:
* * * = 0.7143, E = 0.2857,  * A = 0.8571, D j = 0, for j = B, C, F,G.

* * and E The positive values of D demonstrate that point A is compared to a point, lying

on the line connecting points D and E.

18

3.3
3.3.1

Alternative DEA models
BCC model

The BCC model is other popular radial DEA model. The only difference between the BCC and the CCR model is that the first one contains the additional constraint e = 1 . BCC is given by: Minimize subject to:



 xo - X   0,
Y   yo ,
e = 1,   0.
(3.3)

3.3.2

DEA slack models

An important part of DEA is the so called slack models. The following sections present four DEA slack models: The Additive model, The Slack Based model, The Measure of Inefficiency Proportion model, and the Goal Vector Approach model

3.3.2.1 The Additive model The Additive model is given by:

Maximize subject to:

= z es - + es +
X  + s- = xo ,
19

Y  - s+ = yo ,

(3.4)

e = 1,

  0, s -  0, s +  0.
The inclusion of constraint e = 1, is optional and gives the opportunity to the researcher to choose between Constant Returns to Scale (CRS) and Variable Returns to Scale (VRS). The efficient frontier of the Additive model has exactly the same form as the BCC model when constraint e = 1 that is included in Problem (3.4) and the EF of the Additive model has exactly the same form as the CCR model when constraint e = 1 that is not included in Problem (3.4). To understand how the measure of efficiency is defined in the Additive model, let us observe Figure 3.1(a). The Additive model calculates the efficiency as the sum of the distances from the unit to the efficient frontier, expressed by the vector of slacks s - for the inputs, and the vector of slacks s + for the outputs. An efficient point is on the frontier and hence receives an efficiency measure of zero. The further the point is from the EF, the higher is the optimal value in Problem (3.4). A disadvantage of the Additive model is that it does not distinguish between fully efficient and partially efficient units as do the BCC and CCR models (see Appendix).

20

Figure 3.1

Translation of origin for (a) Additive model (b) BCC model

Definition 1. Additive model efficiency. A DMU is Additive-model efficient if and only if s - = 0 and s + = 0 . One of the most fundamental properties of the Additive model is the translation invariance property. Definition 2. Translation invariance. Given any problem, a DEA model is translation invariant if translating the original input and/or output data values results in a new problem that has the same optimal solution as the old one. By looking at Figure 3.1 (a) we can observe that changing the origin from O to O' does not alter the Additive model efficiency, because the distances, or equivalently the slacks, from the inefficient point to the frontier do not alter. All slack-based DEA models are translation invariant. The measure of efficiency changes, however, for the radial models.

21

Moving the y-axis by adding a constant, changes the efficiency measure from QR/DQ to PR/PD. The property of translation invariance is extremely useful in the presence of negative data in the sample. BCC and CCR models cannot be used in this case, even if some techniques described in the Appendix can partially solve the problem. The dual of the Additive model is given by: Minimize subject to:

w = vxo - uyo + uo
vX - uY + uo e  0,
u  e , v  e , uo free in sign.

(3.5)

The variable uo reflects constraint e = 1, and can be omitted when CRS are being assumed.

3.3.2.2 The Slack-Based Measure of efficiency model (SBM) The next model is very useful when variables are measured in different units. The SBM model is said to be units invariant. The units invariant property ensures that the objective function of the problem obtains the same optimal value, if the one uses the original data, or the one uses data where some of the variables are multiplied by a constant. Note that each slack is divided by the corresponding observed value of the variable. Since both numerator and denominator are measured in the same units; the units are canceled and

22

hence, the problem of difference in measurement units among variables is solved. Therefore, the SBM model is both units and translation invariant.

Minimize

1 m -  i=1 si / xio m = 1 s 1 +  r =1 sr+ / yro s 1-
= xo X  + s - , = y Y  - s+ , o
  0, s -  0, s +  0.
(3.6)

subject to:

The objective function can be transformed, obtaining a linear optimization model (see for example, Morita et al., 2005).

3.3.2.3 Measure of Inefficiency Proportions model (MIP) The Measure of Inefficiency Proportions Model was introduced by Charnes et al. (1985). For this type of model, every slack is divided by its corresponding input or output value, adjusting the difference in measurement units.
- + s sio sro + Maximize x y = i 1= r 1 ro io m

xio subject to: =

- ,  xij  j + sio j =1

n

i = 1,..., m,

(3.7)

= yro

+  yrj  j - sro j =1

n

r = 1,..., s,
23

- +  j  0, sio  0, sro  0.

Note that similar to the Additive model, an efficient unit has an objective function equal to zero. Another property of this model is that the range of all possible DMU inefficiencies is between zero and one because the slacks are always smaller than the corresponding input/output value.

3.3.2.4 The Goal Vector Approach (GV) The Goal Vector Approach was proposed by Thrall (1996) and is given by:

 m - - s + + Maximize   gi si +  g r sr  = r 1  i 1=  xio  xij  j + si- =
j =1 n

subject to:

i = 1,..., m,

yro  yrj  j - sr+ =
j =1 n

n

r = 1,..., s,

(3.8)

  j = 1,  j , si- , sr+  0.
j =1

In Problem (3.8), gi- =

1 1 and g r+ = + , where Rr+ is the range of the r th output and - Ri Rr

Ri- is the range of the i th input. Goal Vector Approach model is also units and translation
invariant. An alternative to this model is the replacement of the range with an interquartile range or the standard deviation of the variables. For further details see Cooper et al. (1999).
24

3.4

Principal Component Analysis

When real data is used with the attempt to be fitted in a model, a common problem is that the data may contain a certain amount of noise and unnecessary information, which are not connected to the patterns and trends in the data, and do not contribute to the analysis. A different difficulty encountered in research is the problem of dimensionality or as defined by Bellman (1961), "the curse of dimensionality", which can be experienced when too many dimensions are used in the model. In statistical terms, it is explained by the fact that the estimation of the parameters converge very slowly to their real value when the dimension space increases. Both of these problems can be resolved by the Principal Component Analysis method. PCA is a statistical technique, whose aim is to find patterns in the data and reduce data dimension. The PCA method composes new axis, and represents each unit of the sample as a linear combination of these axis. The new axis are uncorrelated and their directions are defined in a way that the first component accounts for a maximum amount of variance in the data, the subsequent component accounts for less variance, until reaching the last components which explain only insignificant amount of variability in the sample, and hence can be dropped from the analysis, reducing the variables' dimension in the model. PCA is more efficient when variables are highly correlated. An example, shown in Figure 3.2, helps understand the reason behind this. The example compares the rotation of the axis for three different types of data: data in Figure 3.2(a) is rather dispersed, data in Figure 3.2(b) is moderately dispersed, and data in Figure 3.2(c) is highly correlated. Note that in Figure 3.2(a) and Figure 3.2(b) the data needs to be expressed by two components. Note that in Figure 3.2(a) the two components express relatively the same amount of
25

information but in Figure 3.2(b) the first component expresses higher amount than the second component. And finally in Figure 3.2(c) it is possible to express the main trend in the data, using only one component. This example includes only two dimensions, however for a high number of dimensions, and highly correlated data, PCA can be found very useful.

Figure 3.2 Axis rotation for a) dispersed data b) moderately dispersed data c) highly correlated data

3.5

Principal Component Data Envelopment Analysis (PCA-DEA)

After the transformation of the variables through PCA, we are ready to run the DEA model with the new PCA inputs and outputs. Notice that the PCA transformation does not alter the properties of the DEA models. Therefore, all the characteristics for the Additive model, BCC, and CCR such as, for example, translation invariance, units invariance, and returns to scale remain identical for the PCA-DEA (Adler and Yazhemsky, 2010). PCA can be performed on all variables or on a selected group of variables. This choice is arbitrary and left to the user. It is considered that the variables converted through PCA, should represent similar themes or should be highly correlated.

26

Note that the efficiency score of PCA-DEA, obtained including all PCA variables is exactly the same as the efficiency score, obtained by DEA. The choice of a slack model is particularly comfortable when applying PCA, because after the transformation many of the values of the obtained components are negative. The slack models are translation invariant and, therefore, the user can continue with the analysis. The negative data can be eliminated by adding a positive constant or as suggested by Adler and Golany, (2002) negative data can simply be kept as it is. If negative data is kept the slacks of the slack models should be represented as a sum of two positive slacks. The PCA-DEA for the multiplier form of the Additive model is given by:
+ - + - t -1 1 et s - + et s + + et L- X ( PC -  PC ) + e LY ( PC -  PC )

Maximize subject to:

X  + s+ = Xo, Y  - s- = Yo ,
+ - )= YPC  - ( PC YPCo , -  PC

+ - X PC  - ( PC -  PC )= X PCo , 1 + - L- X ( PC -  PC )  0, -1 + - LY ( PC -  PC )  0,
+ - + - s + , s - ,  PC ,  PC ,  PC ,  PC ,   0.

(3.9)

27

In Problem (3.9) X PC and YPC represent the input and output variables, transformed through PCA. Note that after dropping p components for the inputs, X PC becomes a
n × (m - p ) matrix and after dropping q components for the outputs, YPC becomes a n × (r - q ) matrix. LX is a m × m matrix of the eigenvectors for the inputs and LY is a r × r
1 -1 matrix of the eigenvectors for the outputs. L- X and LY are the inverse matrices of LX and

+ - ,  PC are m ×1 vectors representing the slack variables for the PCA transformed LY .  PC
+ - inputs and  PC ,  PC are r ×1 vectors representing the slack variables for the PCA

transformed outputs. The slacks keep their dimension even after the elimination of principal components. The last two sets of constraints in Problem (3.9) reflect the fact that the PCA transformation can alter the magnitude of slacks. Hence, in order to obtain slack variables, equivalent to the initial DEA model, it is necessary to multiply the vector of slacks by the inverse of the eigenvector matrix.

3.6

Stochastic DEA

One of the weaknesses of DEA is that it does not include the possibility for data stochastic variations. As mentioned earlier, the Efficient Frontier (EF) plays an important role in the estimate of efficiency and small changes of efficient units' values can alter the results for the whole sample. Until this point, we considered only deterministic type of data. However, this assumption does not always reflect reality because of possible:

28

· ·

Measurement errors. This is the most frequent mistake in many types of research. Approximation of variables. In some cases variables are not easy to observe and variables approximation are used as estimates. Such examples are inflation rate, unemployment rate, level of education for different countries, number of people living in a city, etc.

·

Average Estimates of Variables. Sometimes, the data sample is composed by a time series of observations, and users prefer to consider the average of the time series as a variable representation.

·

Missing Observations. In many cases, it is possible that a missing value of a certain variable is substituted by an estimate, provided by user.

·

Partial Representation of the Variables. In some cases, variables can be observed for only a certain time period. This is the case for the financial ratios, considered in the thesis. The variables, used in the DEA models, are financial variables statements, presented in companies' balance sheets, which are evaluated at the end of the calendar year. Therefore, it is possible that some variables represent the financial state of the company at that exact moment and that they fail to capture some general aspects of the company' behaviour throughout the year.

3.6.1

Interpretation of SDEA

The difference between DEA and Stochastic DEA is that the last one allows to units to violate the production frontier for a number of times, chosen by the user SDEA moves
29

the production frontier (which in the case of Stochastic DEA is known as stochastic frontier) versus the production possibility set. The magnitude of the frontier' shift depends on the data variability and on a confidence level  , set by the user. The level of

 determines how strict should be the impact of stochastic variation on the stochastic
frontier. For example, confidence level of  = 5% implies that five percent of observations in the sample are allowed to overpass the frontier. Note that the stochastic frontier is no longer composed of efficient units, and units classified efficient by DEA obtain a value of stochastic efficiency greater than one for radial models and less than zero for slack models. Let us consider the following example illustrated in Figure 3.3. The example shows the SDEA efficiency evaluation of three units, investing two inputs to produce the same amount of output. One can note that by adding a stochastic term, the stochastic frontier moved to the north-east side of the production possibility set. The efficiency score for point A for the DEA model is equal to OB/OA. The SDEA efficiency score for the same point is OB/OA. Point B is efficient for the DEA and SDEA but in the second case, it obtains efficiency score higher than one. Point C is not efficient for DEA, but it is efficient for SDEA.

30

O
Figure 3.3 Stochastic efficient frontier

SDEA efficiency is never lower than DEA efficiency. Note also that higher variability in the variables results in higher amount of efficiently classified units, because of the further shifting of the stochastic frontier. Let us develop the analytical formulation of SDEA. We will follow the approach developed by Cooper et al. (2001b).

3.6.2

SDEA model formulation

In order to implement SDEA formulation model, it is first necessary to assume that each input/output observation follows a certain random distribution that can be determined
31

from past observations or from theory applied in the specific sector. Therefore, let
i = ( x 1i ,..., x mi ) be the vector or random inputs and y r = y 1r ,..., y  sr x
t

(

)

t

be the vector of

random outputs. Now, as previously defined, let x j = x1i ,..., xsi

(

)

t

be the vector of

observed input values and y j = y1 j ,..., yrj be the vector of observed output values. The difference with the previous deterministic DEA models is that the vectors of observed inputs and outputs actually represent the expected values of the random vectors
i = x 1i ,..., x mi and y r = y 1r ,..., y  sr x

(

)

t

(

)

t

(

).
t

We assume that the inputs and outputs are

jointly normally distributed. The Stochastic DEA is given by:

Maximize

i 1= r 1 =

 si- +  sr+
r = 1,..., s,

m

s

Subject to:

n    +  rj  j - sr  ro  =- P  y 1 j y j 1 =       n  ij  j + si-  x io  =- P  x 1 j  j =1   
n

i = 1,..., m,

(3.10)

  j = 1, sr+  0,
j =1

si-  0, j = 1,..., n r = 1,..., s, i = 1,..., m,

In Problem (3.10)  j is level of significance and is chosen by the user. The first set of constraints in Problem (3.10) guarantees that the probability observed output exceeds the best practice output is equal to . Best practice in this case is associated to the points, forming the stochastic frontier and is a relative measure of efficiency. Figure 5.2 shows
32

the distribution of the differences of observed output values and best practice values. Since outputs are assumed to be normally distributed, the distribution of the differences is also normal. One can notice that the area under the distribution, where this difference is negative, is equal to .

Figure 5.2 Normal probability density function of best practice output and observed output

3.6.3

Deterministic equivalent

In order to obtain the deterministic equivalent of Problem (3.10), it is necessary to standardize the constraints. This is first done by subtracting from both sides (right and

33

left) of the expressions in the brackets in the constraints in Problem (3.10) the expected value of the difference between observed input/output and best practice input/output. Then, the expressions are divided by the standard deviation of the same difference. However, in order to imply the standardization of the variable, it is necessary to assume that its parameters (mean and standard deviation) are known a priori. If parameters are unknown, it is necessary to use the Student's t-statistics, where population parameters are estimated. For this research, however, distribution parameters can be estimated, because of the large sample size. Therefore, we will assume that inputs and outputs are normally distributed with known parameters. After standardizing and simplifying (3.10) we obtain its deterministic equivalent. See the Appendix III on how to obtain Problem (3.11) from Problem (3.10).

Maximize:

i 1= r 1 =

 si- +  sr+
n

m

s

subject to:

sr+ -  yrj  j - yro =  -1 ro ( ) r
j =1

r = 1,..., s,

s +  xij  j - xio =  -1 io ( ) i
- i j =1

n

i = 1,..., m,

(3.11)

  j =1, sr+  0, si-  0,
j =1

n

j = 1,..., n, r = 1,..., s, i = 1,..., m.

Where,

 io ( ) =

k o j o

ij , x ik ) + 2o   j Cov( x ij , x io ) + o2Var ( x io ),   k  j Cov( x
j o

(3.11.1)

34

 ro ( ) =

k o j o

 rk , y  rj ) + 2o  k Cov( y  rk , y  ro ) + o2Var ( y  ro ),   k  j Cov( y
k o

(3.11.2)

The Optimization Problem in (3.11) is obviously non-linear. However, an assumption of zero covariances in Equation (3.11.1) and Equation (3.11.2) eliminates the problem of non-linearity.

35

CHAPTER 4: MODEL FORMULATION

In previous chapters, we described the theoretical aspects of Data Envelopment Analysis, Principal Component Analysis and Stochastic DEA. The purpose of this chapter is to apply those three techniques and develop final set of models. Then, performance of the models is compared and the best model is selected for analysis in the next chapter. Figure 4.1 shows the steps and procedure to follow in order to develop the models. In the first step, we choose the variables, which represent different financial aspects of a company. Then, we check which dataset provides better efficiency estimates: a dataset that includes a number of companies from different sectors, or a dataset, divided into five parts, each of them representing sectors with similar characteristics. We find that the data set with individual sector performs well. In the next step, we choose between the two main groups of DEA models: radial or slack. The choice for the analysis is the slack DEA models. After, we choose between four types of slack models and we find that the Goal Vector Approach model is the best among them. At the final step, we develop six different models presenting combinations of DEA techniques such as: Absolute Weights Restriction, PCA, and Stochastic DEA and then we test their performance in Chapter 5.

36

Figure 4.1 Flowchart of the organization of the model formulation

4.1

Variables selection

Selection of financial ratios that represent the financial state of a company in a best possible way is one of the most challenging parts of any credit rating analysis. Different choices of inputs and outputs, involved in DEA efficiency evaluation, can lead to completely different results and interpretations of efficiency. To date, there have not been found an universal set of variables, able to capture all the aspects of companies' financial health and different analysis tend to use different combinations of financial ratios. Table
37

A.1 and A.2 show combinations of financial ratios, used by ten different researchers, including Moody's credit rating agency.

4.1.1

Number of variables and DEA

When choosing the most appropriate number of inputs and outputs, we should take into account the following DEA properties: · A rule of the thumb is that the number of variables in DEA should be less than a third of the number of units in the sample. Therefore, as discussed in Section 6.2, the number of units is in between 37 and 39 and hence the number of variables in the analysis should be less than 12. · As the number of variables increases, the values of DEA efficiency estimations and the number of efficient DMUs increase. This can be explained by the fact that DEA is very flexible on assigning weights for the input/output ratio, as defined in model Problem (3.7). An increasing number of variables leads to more possibilities of maximizing the ratio by simply assigning a higher weight for the additional variable. · Inclusion of highly correlated variables can significantly alter the estimate of efficiency (e.g. Zhank and Bartles, 1998). Therefore, after selecting the initial values, we have to make sure that the correlation between them is not too high.

38

4.1.2

Selected variables for the analysis

In our choice of financial ratios, we will follow the approach, presented by Goss, (2009). In overall, the theories agree that the choice of the variables should represent different aspects of a company such as firm liquidity, profitability, leverage, and asset structure. Liquidity. It is usually expected that firms with low liquidity have a higher probability to experience default, because of the difficulty to survive random negative shocks. We follow Altman (1968) and Ohlson (1980) and include Working Capital to Total Asset as a liquidity measure in the analysis. · Working Capital to Total Assets (WC/TA). This ratio represents the liquidity rate of a company, or equivalently its short-term solvency. It indicates how efficient is a company in covering its own short-term financial obligations. Companies with negative ratio sign would be less successful since they do not possess sufficient funds necessary for growth. Profitability is another important aspect of financial distress. Companies that do not generate sufficient profit are not likely to survive with time and can be easily replaced by more competitive ones. Following again Altman (1968), we will use Operating Income to Total Assets as an indicator of short-term profitability, Retained Earnings to Total Assets as an indicator of long-term profitability, and Earnings before Interest Taxes to Total Assets as profitability, due to management. · Operating Income to Total Assets (OI/TA). This ratio is an indicator of profitability. According to Kaplan (1989), "Operating income equals net sales less cost of goods sold, general and administrative expenses before depreciation,
39

depletion, and amortization are deducted" (p. 224). Hence, OI/TA is a useful measure of company profit, generated from operations. It is a clean measure of assets productivity, because it is unaffected by the depreciation of gain/losses from assets sale and earnings from interest expenses. · Retained Earnings to Total Assets (RE/TA). This ratio represents the amount of earnings that is retained and reinvested in the company instead of being distributed among owners as dividends. The ratio measures the ability of the company to accumulate profit over time. A young firm would have a lower RE/TA with respect to an older firm. Hence, inclusion of this ratio into analysis penalizes the young companies and benefits the older ones. This, however, reflects reality, because the failure rate is much higher for companies in their earlier life. · Earnings before Interest Taxes to Total Assets (EBIT/TA). This ratio shows how a firm can generate profit from its assets, regardless of its size. For example an EBIT/TA ratio equal to 20%, indicates that a firm generates $1 profit from every $5 invested. A negative value of this ratio is an indication of poor management efficiency of the firm. Leverage. The term leverage is associated with the use of different financial instruments or borrowing capital for the potential future growth of a company. Following Ohlson (1980), we use the standard leverage indicator ­ Total Liabilities to Total Assets · Total Liabilities to Total Assets (TL/TA). According to the Accounting Research Foundation liabilities are "future sacrifice of economic benefits that the entity is
40

presently obligated to make to other entities as a result of past transactions and other past events." In the company balance sheet, the term liabilities include accounts payable, taxes, wages, accrued expenses, and deferred revenues. Therefore, the higher is the amount of liabilities, the higher the amount of obligations that the company needs to meet in the future. Asset structure. The Asset structure helps understand how efficient is a company in managing its assets. Following Gupta (1969), we include the Asset Turnover ratio into the analysis. · Asset turnover (AT). This ratio represents firms' operating efficiency, or equivalently, firms' ability to implement revenue from its assets. The value of the ratio depends mainly on the strategy of the company. For example, luxury stores tend to have lower Asset Turnover value than discount stores. Some authors suggest that Size is an important variable in the measure of credit efficiency (e.g. Ang et al., 1982). Larger companies are more diversified and hence less probable to bankrupt. Large firms also pay less for debt issuing and equity securities issuing. Small firms prefer to borrow short term debt and because of that they have smaller possibilities for growth. The use of the logarithm for Sales reflects the fact that the effect of Size is stronger for smaller firms than bigger ones.

41

4.1.3

Correlation between variables

Since every variable should be able to bring new information in the analysis, a desired property for each model is the independence of the involved variables. Repetition of information can be a serious issue for DEA; Jenkins and Anderson (2003) show that an inclusion of highly correlated variables in DEA can significantly alter the efficiency results. Therefore, it is necessary to make sure that the variables are not highly correlated. The variables correlation matrix is showed in Table 4.2.

Table 4.1 Input/output variable selection Input/Output Name Input 1 Input 2 Output 1 Output 2 Output 3 Output 4 Output 5 Formulation

Total Liabilities to total Assets (Total Liabilities)/(Total Assets) Asset Turnover Working Capital to Total Assets Retained Earnings to Total Assets Size Earnings before Taxes to Total Assets Operating Income to Total Assets Sales/(Total Assets) (Current Assets-Current Liabilities)/(Total Assets) (Returned Earnings)/(Total Assets)

ln (Sales)
(Net Income+Income Tax+Interest Expense)/(Total Assets) (Operating Income)/(Total Assets)

It can be seen that the correlation between EBIT and Operating Income is equal to 0.76. In order to avoid the inclusion of the same type of information twice, it is necessary to drop the variable Operating Income from the analysis. However, the variable is not

42

dropped for the PCA models, because of its ability to clean the correlation among variables.

Table 4.2: Variables correlation matrix
Variable NC/TA AT TL/TA RE/TA Size EBIT/TA OI/TA NC/TA 1 AT 0.2268 1 TL/TA -0.5102 0.0880 1 RE/TA 0.1852 0.0269 -0.4750 1 Size -0.3205 0.0070 0.2712 0.1482 1 EBIT/TA OI/TA 0.0996 0.0189 -0.2625 0.4359 0.1676 1 -0.0372 -0.0298 -0.1705 0.4379 0.1820 0.7601 1

4.1.4

Grouping variables into inputs and outputs

Once the variables are selected, it is necessary to divide them into inputs and outputs. A variable classified as an input should have a negative correlation with efficiency; thus an increase of the variable should be associated with decrease of efficiency. According to this logic, Total Liabilities to Total Assets should be classified as inputs. For example, when comparing two companies, achieving the same level of outputs, the one with a smaller amount of liabilities is considered more efficient. Similarly, a variable classified as an output should have a positive correlation with efficiency, because the higher is the output, the higher is the efficiency. Therefore, it is easy to observe that Retained Earnings to Total Assets, EBIT to Total Assets, Size, and Operating Income to Total Assets, and Working Capital to Total Assets are clearly output variables. All of them represent a measure of profitability. The only variable that raises

43

doubt about its classification is Asset Turnover. The level of Asset Turnover is rather a company's choice and represents a product of the firm's strategy. In order to understand the relationship between Asset Turnover and efficiency we proceed to the following two measures: · Correlation between Asset Turnover and Market Price: The relationship between company's Market Price and efficiency is clearly positive (an efficient company usually has a high market price and inefficient company is usually associated with low market price). Hence, the sign of the correlation between Asset Turnover and Market Price can help understand the sign of the correlation between Asset Turnover and efficiency. The value of the correlation is calculated using the Asset Turnover and Market Price observations of 235 companies and is equal to -0.17. Therefore, negative correlation is evidence that Asset Turnover should be included as an input. · Correlation between Asset Turnover and DEA efficiency, calculated using only the five variables (one input and four output), excluding Asset Turnover. The value of the correlation is calculated for the same 235 companies and is equal to 0. 5285. Therefore, Asset Turnover should be considered as an input.

4.2

Sectors

As mentioned earlier, the choice of dataset in DEA is extremely important, because inefficient units are compared to a relatively small set of efficient units and adding an

44

efficient unit to the dataset could alter significantly the efficiency estimates. Therefore, companies should be compared only to DMUs that perform in the same sector and that have similar geographical and structural characteristics, because production processes and economic conditions usually vary from sector to sector. Hence, we divide the companies, into five sectors. The companies are divided with respect to their Standard Industrial Classification (SIC) number as shown in Table 4.3. In order to keep consistency among the sector sizes, every sector contains between forty-seven and fortynine companies. Dividing the companies into sectors can be a good idea but including all the companies in one single dataset has also its advantage, which comes from the fact that the number of companies in the dataset will be raised from 37 to 235. Including more companies in dataset, leads to an increase in the number of efficient units. Therefore, the inefficient companies are compared to highly efficient performers, and hence the efficiency evaluation can be more strict and realistic.

Sector number Sector 1 Sector 2 Sector 3 Sector 4 Sector 5

Table 4.3 Sectors selection SIC number Sector description 2600-2750 2800-2842 3000-3330 3711-3825 5000-5211 Paper and allied products, printing and publishing Chemical and applied products Rubber and miscellaneous plastic products, leather and leather products Transportation equipment, instruments and related products Wholesale trade-durable and non durable products

45

In order to consider both cases, DEA efficiency, obtained from a dataset, divided into five sectors is compared to DEA efficiency, obtained from a dataset, combining all companies in one single sector and hence, we continue the analysis with the better performing dataset. The results from dataset comparison are reported in Figure 4.2. The top five lines in Figure 4.2 are associated with the five sectors, presented in Table 4.4, and the bottom blue line in Figure 4.2 is associated with the single sector, containing all the companies in the analysis. Each dot on the line represents the correlation between the value of the companies' Market Price and companies' DEA efficiency, using the Goal Vector Approach model for the corresponding year. For example the first dot of the red line expresses the correlation between the Goal Vector Approach DEA efficiency and Market Price for the forty-seven companies, included in the "paper and allied products, printing and publishing" sector for the year 1995. We follow the approach presented by Edirisinghe and Zhang (2007) and we use the Market Price index as an evaluator of DEA performance, i.e., higher correlation is associated with a better performing model. The choice of Market Price as a comparison index is explained by the fact that its value is determined by macroeconomic factors, credit rating assigned by specialized companies, such as Moody's and Standard & Poor's, the company market value and image, and others. Market Price is not an indicator that captures all the aspects of credit risk, but is one of the best indices, because it is usually evaluated by companies, willing to invest and use all available information and credit rating models, and is proved to perform adequately. In this section, we use the correlation between DEA efficiency and Market Price only to capture some general trends, helpful through the process of choosing the

46

most appropriate DEA model. Other techniques, dealing with performance validation of the selected DEA model are used in Section 4.6.

Figure 4.2 Performance of dataset, divided into sector vs performance of dataset, consisting in only one sector

From Figure 4.2, we observe that the correlations for the individual sectors (represented by the top five lines) are higher than the correlation of the dataset, combining all sectors (represented by the bottom blue line). Hence, we continue the analysis with the dataset, divided into five sectors.

47

4.3

Radial vs slack models

The two major groups of DEA models are the radial and the slack models. The choice for this analysis is the slack models. This choice is justified by the three important properties of slack models. 1) Possibility to treat negative data. Some of the financial ratios and many of the transformed PCA outputs are non-positive. 2) Slack models do not require a choice between input and output oriented models. 3) Most of slack models (excluding Additive model) are units invariant. The main model of interest is the Additive model. However, the Additive model is not units variant, and because of this disadvantage, its efficiency evaluation is compared to efficiency evaluation of the following three slack models that are units invariant: · · · The Slack Based model (SBM) defined in Problem (3.6), The Measure of Inefficiency Proportion (MIP) model defined in Problem (3.7) The Goal Vector Approach (GV) defined in Problem (3.8).

The correlation between market price and evaluated efficiency for the four slack models can be found in Figure 4.3, which is similar to Figure 4.2. The y-axis represents the correlation between one of the four models DEA efficiency and Market Price, and the xaxis represents the year. It can be observed from Figure 4.3 that Additive and Goal Vector Approach models clearly perform better than the Slack Based model and the MIP models. Hence, we do not consider these two models for further analysis.
48

The reason of the low performance of these two models can be found in the formulation of their objective functions. As can be observed, the slacks in the optimization functions of Problems (3.8) and (3.9) are divided by the value of their corresponding inputs/outputs. Five of the six variables are, however, ratios and some of them are very close to zero. Dividing slacks by particularly small inputs and outputs can lead to unreasonably high numbers that take part in the objective function, which explains the low performance of these two models and provides evidence that the models should not be used in the cases where the variables` range is between zero and one.

Figure 4.3 Correlation between Market Price and efficiency for four slack models

49

The reason why the Additive model performs better than Slack Based and the MIP models can be explained by the fact that the sum of the slacks as defined in an equation of Problem (3.4) is not adjusted to the measurement units of the variables. If variables, however, are measured in different units, their slacks are measured in different units as well. Therefore, if for example an observation of a variable is rather far from the efficient frontier, but is measured in smaller units, compared to the rest of variables, this inefficiency is not reflected with the same strength as inefficiencies of variables, measured in bigger units. Since all the variables used in DEA evaluation are ratios, except for Size, their values vary from zero to one. The variable Size, however, represents the natural logarithm of Sales and its range varies from 0 to 14.8. Hence, the effect of Size in efficiency, obtained from the Additive model is much stronger than the effect of the rest of the variables. Table 6.4 shows the correlation between Market Price and variables, used in the model. Note that the highest correlation, almost of 51%, is the one between Market Price and Size and that explains the high correlation between Market Price and efficiency evaluated by the Additive model. Table 4.4 Correlation between Market Price and model variables
Variable NC/TA AT/TA TL/TA RE/TA Size EBIT/TA Correlation -0.17851 -0.16935 -0.09073 0.299827 0.506286 0.296458

Note that Goal Model performs slightly better than Additive Model (Figure 4.3). It is also the choice of the slack model to continue with. The main reason to drop the Simple Additive is that even if the variable Size proves to be important for the Market Price

50

determination, it is better to avoid Size becoming the main factor of efficiency evaluation. The Goal Additive solves this problem, by assigning to Size an equal importance as the rest of the variables and by providing a good correlation with Market Price.

4.4

Application of DEA techniques

After we choose the Goal Vector Approach as a slack model, we can apply the DEA and weights constraints, the Principal Component Analysis (PCA), and the stochastic DEA, discussed in Chapter3. Combining and applying these techniques leads to the formulation of the following six main models, as shown in Figure 4.4. In the following sections we will discuss details about these six models. After that we will apply them and choose the best performing one. The six models are: · · · · Goal Vector (GV) Goal vector +Weights Restriction (GV+WR) Goal Vector + Principal Component Analysis (GV+PCA) Goal Vector + Principal Component Analysis +Weights Restriction

(GV+PCA+WR) · · Stochastic Goal Vector (SGV) Stochastic Goal Vector + Principal Component Analysis (SGV+PCA)

51

Figure 4.4 Evaluation of six final models used for credit rating assessment

4.4.1

Goal Vector Approach and Goal Vector Approach +Weights Restrictions

As mentioned earlier, sometimes DEA can assign very small or large variable weights, leading to the neglect or the overestimate of some variables. Hence, it would be reasonable to include upper and lower Absolute Weight Restrictions (see Section A.I.4). Efficiencies, evaluated by Goal Vector Approach with and without weights restriction, are then compared. By default, the lower limit of all Slack Models weights is set to one. Our choice about the upper limit value is twenty, because the purpose of the restriction is only to avoid too extreme values for any weights. A sensitivity analysis is run for the Goal Vector Approach Model for different values of the upper weight restriction. Then, the correlation between market price and efficiency is calculated and reported in Figure 4.5.

52

0.8

Correlation between Market Price and Goal Vector Efficiency

0.6 0.4 0.2 0 2 -0.2 -0.4 -0.6 -0.8 3 5 8 10 15 20 25 30 40 50 75 100 150

sector1 sector2 sector3 sector4 sector5

Values of Upper Absolute Weight Restriction

Figure 4.5 Convergence of upper weight restriction

It can be observed that the correlation is convergent after the chosen upper weight value of twenty. The GV and GV+WR models are defined as: · Goal Vector Approach is defined by:
+ s- t s + e R- R+

Maximize subject to :

et

X  + s- = xo

(4.1)

Y  - s+ = yo ,
e  1,

  0, s -  0, s +  0.
53

In Problem (4.1), where Ri- is the range of the i-th input and Rr+ is the range of the r-th output. The only difference with the standard Goal Vector Approach is that constraint


j =1

n

j

= 1 , used to define the returns to scale nature, is substituted by constraint


j =1

n

j

1.

The reason of the use of this constraint is explained in Section 4.5. · Goal Vector Approach with Weights Restrictions is defined by:
+ s- t s e + - 20et t - 20et b R- R+

Maximize

et

subject to:

X  + s- - t = xo , Y  - s+ + b = yo ,

(4.2)

e  1,

  0, s -  0, s +  0, b  0, t  0.
In Problem (4.2), R - is the vector of inputs ranges and R + is the vector of outputs ranges.

4.4.2 GV+PCA and GV+PCA+WR The Vector Goal Approach model is combined with PCA and absolute weights restrictions. The negative values of the principal components are kept, as suggested by Adler and Golany (2002). The number of inputs, involved in the analysis is equal to two, and hence, the number of the transformed PCA components is also equal to two. The purpose of PCA, however, is to eliminate some of the components and dropping one of the two input principal components can lead to the loss of important information. Not
54

implying PCA to the inputs could lead to even worse result, because the objective function is a sum of all variables' slacks, and since the number of outputs is decreased by PCA, the two inputs slacks would account in a much stronger way in the objective function than the outputs slacks. In order to solve this problem, we will transform the input variables into output variables by simply subtracting them from a positive constant. Since the range of the two inputs (Asset Turnover to Total Assets and Total Liabilities to Total Assets) varies from zero to one, the value assigned to this constant is one. The transformed variables (from inputs into outputs) will still bring the same type of information. For instance, if the Asset Turnover to Total Liabilities value for DMU1 is equal to 0.1 and for DMU2 is equal to 0.2, and all the rest of variables have identical values, then DMU1 has a smaller amount of input and hence is more efficient. After transforming the inputs into outputs, DMU1 will have a value for the obtained output (10.1) equal to 0.9 and DMU2 will have a value for the obtained output equal to 0.8, and thus, DMU1 will still be classified as more efficient than DMU2. Note that this transformation is not allowed for the radial models, but since the inefficiency of variables for Goal Vector Approach model is expressed as a sum of adjusted input and output slack, the efficiency estimations before and after the transformation are exactly the same. This transformation, however, leaves the model without any input variables. This is not consistent with DEA theory and can lead to improper efficiency evaluation. Hence, an artificial input is created by assigning an input value of one for each variable. Therefore, we assume that each unit invests one unit of input and achieves different amounts of outputs. The companies classified as efficient are the ones that succeed to obtain the highest value of outputs linear combination.
55

As mentioned in Section 4.3, the variable Operating Income to Total Assets was dropped for the GV and GV+WR models, because of its high correlation with the variable Earnings before Taxes to Total Assets. The variables can be included in the GV+PCA and GV+PCA+WR models, because the most important property of PCA is to clean correlation between variables. Hence, the high value of the correlation between Operating Income to Total Assets and Earnings before Taxes to Total Assets does not present a potential repetition of information. After describing the most important details in the formation of GV+PCA and GV+PCA+WR models, it is possible to define them. · GV+PCA model is given by:
+ - [ PC ] -  PC + R pca

Maximize:

1 et s - + et L- y

subject to:

X  + s- = Xo,
+ - - ) = YPC  - ( PC YPCo , PC
- 1 + L- y ( PC -  PC )  0,

(4.3)

e  1,
+ - s - ,  PC ,  PC ,   0.

Problem (4.3) is slightly modified from Problem (3.9), and the definitions of

the

variables can be found in Problem (3.9). If  is the number of outputs that we decide to
56

transform through PCA, then s - is the number of outputs, not transformed through PCA. Therefore, the number of the second set of constraints is Problem (4.3) is equal to

s - . An important fact about Problem (4.3) is that the dimension of the vector
+ - [ PC -  PC ] is still equal to s , which in our case is seven, even if the number of principal + R pca

components is smaller than s (because some of the principal components are dropped).
+ The vector R pca represents the ranges of the original output variables and not the

principal component variables. · GV+PCA +WR model is given by:
+ - -  PC  PC 1 - 20 L- y b pca - 20t + R pca

Maximize

1 et s - + et L- y

subject to:

X  + s- - t = Xo,
+ - YPC  - et ( PC -  PC ) + b pca = YPCo ,

(4.4)

1 + - L- y ( PC -  PC )  0,

e  1,
+ - s + , s - ,  PC ,  PC , t pca ,   0.

In Problem (4.4), t is the optimization variable, representing the dual of the absolute upper weight restriction constraint, defined in the Multiplier form for the input. Similarly,

b pca , is the vector of dimension s for the optimization variables, representing the duals of
the absolute upper weights restriction constraints, defined in the Multiplier form (see

57

Section 3.4.1), for the PCA outputs. We need to multiply the vector b pca , by the inverse
1 of the eigenvector matrix L- y (defined in Chapter 3) in order to take into account the fact

that the original output variables were transformed to principal component variables.

4.4.2.1 PCA dropping Rule The main purpose of DEA is to decrease the number of variables by dropping some of the components. Therefore, it is necessary to establish a rule about how many components should be dropped. The most popular rule ­ the eigenvalue one criteria (see Section A.I.7) is rather restrictive for our analysis, because the number of variables is equal to seven. Usually, only the first two components have an eigenvalue higher than 1 and including only two components brings an undesired loss of information. Hence, we will follow the approach, suggested by Jackson (see Jackson, 1993) which keeps the components with an eigenvalue higher than 0.8. This rule, however, can lead to undesirable results, when one or two components have an eigenvalue, close to 0.8. For example if an eigenvector has an eigenvalue equal to 0.79, it will be dropped from the analysis, even if it contains important amount of information. Therefore, a variance proportion rule (see Section A.I.7) is also included in order to ensure stability. It is reasonable to be willing to keep seventy to eighty percent of the original information. Since the number of variables is seven, the sum of the principal components eigenvectors is also seven. Thus, an additional constraint is added, implying the sum of the communalities of the components kept in PCA-DEA is at least equal to 5.5. (5.5/7 = 0.78, ensuring that at least 78% of the variability is kept in the analysis).
58

Therefore, the number dropping rule is summarized as follows: first the components with eigenvalues greater than 0.8 are kept. Then, if the number of the sum of kept eigenvalues is smaller than 5.5, the next component is added even if its eigenvalue is smaller than 0.8. The process continues until the sum of principal components' eigenvalues is greater or equal to 5.5. Note that we use the eigenvalues and eigenvectors calculated from the correlation matrix rather than from the covariance matrix. This method is proposed by Adler and Golany (2002) and is better to use since correlation is not sensible to measurement units.

4.4.3

Stochastic Goal Vector (SGV)

The SGV is formed by adding a stochastic term to each of the DEA constraints. An important assumption is that the covariance between variables is close to zero. Since almost all variables have a correlation smaller than 5%, we will continue the analysis without the covariances terms in equations (3.11.1) and (3.11.2). Note also that the case of Stochastic DEA plus Absolute Weights Restrictions is not examined, because after evaluating the efficiency for both models, the values of efficiencies turned out to be extremely close. Following the optimization problem, explained in Problem (3.11), we obtain the SGV model.
+ s- t s + e R- R+

Maximize

et

subject to:

X  + s- = Xo,
(4.5)
59

Y  - s+ = yo -  -1 o ( ) ,
e  1,

  0, s -  0, s +  0.
After we assumed that the covariances between variables are equal to zero, the expressions for the covariances in the second set of constraints in Problem (4.5) is

 ro ( ) = o 2Var ( yr ) . As mentioned in Chapter 5, we have to choose the level of  and
thus, it is a good idea to observe the performance of the models for different levels of  . The correlation between Market Price and SGV Efficiency is calculated for Sector 3 for the values  = 5% ,  = 40% , and  = 75% .The results are reported in Figure 4.6.

Figure 4.6 Correlation between Stochastic DEA and Market Price for different levels of alpha
60

It is easy to observe that SGV model performs better with an alpha level equal to 5%, which is also the choice for the following analysis.

4.4.4

Stochastic Goal Vector + Principal Component Analysis (SGV+PCA)

The last of the main six models, is the SGV+PCA model, which is obtained by combining PCA technique with Stochastic DEA technique. We obtain the model, by first transforming the original variables through the PCA technique, then dropping irrelevant components, using the dropping rule, discussed in Section 4.4.2.1, and finally, including a stochastic term to each of the output constraints.

Maximize

1 + - L- y [ pc - pc ] e s +e R+ pca
t - t

subject to:

X  + s- = Xo,
+ - YPC  - et ( PC ) YPCo -  -1 r (o - 1)et  Y , -  PC =

(4.6)

1 + - L- y ( PC -  PC )  0,

e  1,
s- ,
- +  PC ,  PC ,   0.

In Problem (4.6)  Y is the vector of the original output variables' standard deviation. The dimension of  Y is ( m -  ) and the dimension of  Y is ( s - ).
61

4.5 Returns to scale
Slack models give the possibility of a choice between Constant Returns to Scale (CRS) and Variable Returns to Scale (VRS). The most popular choice, found in different researches is the one of VRS. The main reason is that VRS allow for units, characterized with Increasing Returns to Scale (IRS) and Decreasing Returns to Scale (DRS) to be allocated on the efficient frontier, which is not the case for models that assume constant returns to scale. A point with IRS or DRS can improve its efficiency by decreasing its outputs or increasing its inputs. Therefore, CRS "punish" these DMUs by assigning a lower efficiency for not seizing the opportunity of a possible improvement. In many cases, however, inputs and outputs are hard or impossible to increase or decrease. Therefore, by assigning VRS, units are evaluated only based on their actual performance. RTS in the case of this research are automatically implied by the structure of the model. As mentioned earlier, we choose to transform the inputs into outputs and to include an artificial input, containing values of one for each DMU. This transforms the last equation in Problem (3.4) to

1, which is equivalent to  ij  1, . Now referring to  ij + s - =
j =1 j =1

n

n

Figure A.3 in Section A.I.3, one can observe that including the last equation changes the efficient frontier to the segment, composed by the lines OC and CD. Because of this shape of the frontier, point A, characterized by IRS, is no longer classified as efficient and point D, characterized by DRS, is still efficient.

62

4.6

Performance evaluation validation

The purpose of this research is to imply DEA techniques into credit rating. We want to find a technique, alternative to credit rating assessed by credit rating specialized companies, such as, Moody's, Standard & Poor's (S&P), and Fitch. Using the efficiency estimates, the model should distinguish between financially healthy companies and potential of default companies. Companies classified as DEA efficient would be a good candidate for investment, while companies, classified as the DEA inefficient should be considered more carefully. But how can we be convinced that DEA allocates correctly efficient and inefficient companies. Said in other words, how can we prove whether a company is efficient or not? A company that bankrupts is clearly inefficient. But how can we define whether one company is more efficient than the other? The answer is that there is no unique way to do so. There are different methods capable to give some hints about financial state of firms, but there is no unique way to prove that a company is inefficient. Some of these hints include: · The correlation between DEA efficiency and market price. More efficient companies have a higher price on the financial markets. · The correlation between DEA efficiency and expert opinion. In this case, expert opinion is the Standard & Poor's credit rating.

63

·

Comparison between companies, classified as highly inefficient by DEA and real bankrupted companies. This is done by checking how the DEA models classified companies that later experienced bankruptcy.

4.6.1

Market value

Market value represents the price at which a stock share or a bond is sold or bought. When deciding whether to buy or not, the investors usually take into account the potential future development of a company. However, the market price of a share can be overpriced or underpriced with respect to the company's book value. The incentive justifying the choice of market value as a main DEA performance evaluator is described in Section 4.3.2. Since most of the companies in Sector 3 do not have a credit rating estimate, assigned by Standard & Poor's, we drop it for the correlation estimate between S&P credit rate and DEA efficiency.

4.6.2

Comparison to expert ­ Standard & Poor's (S&P)

Credit rating evaluation is usually performed by specialized private companies such as Moody's, Standard and Poor's, and Fitch. Highly rated firms can enjoy a higher bond price and lower borrowing costs. The role of those financial intermediaries and their rating accuracy has been a point of many debates. Some concerns about the extent to which these credit risk evaluations should be trusted, were raised after the 2008-2009 crises, where many investments even for "AAA-" rated companies turned into losses

64

(Pagano and Volpin, 2010). The (S&P) credit rating, however, remains one of the most used financial health performer and hence is used as a point of comparison for DEA evaluation. A high correlation between credit ratings assigned by S&P and efficiency evaluated by DEA models is an evidence for a potential use of DEA in credit rating. S&P credit rating consists in twenty one different categories, displayed in Table 4.5. Since S&P assigned credit rating is a categorical variable, it is first necessary to transform S&P rating into a numerical variable. This is done by assigning a value of one for the highest credit rating according to (S&P), which is "AAA". Next, a value of two is assigned for category "AA+" and so on. The most inefficient category, rated "D" by S&P receives a value of twenty-three.

4.7

Statistical analysis of correlation

In order for the correlation to be statistically significant, a further statistical analysis is performed, including the p-value of the correlation, correlation 95% confidence interval, and Kolmogorov-Smirnov Test.

4.7.1

Correlation

Correlation is used to prove the existence of relationship between two variables. Correlation does not imply causation, but an absence of correlation implies absence of linear relationship between the variables. Correlation is calculated as:

65

=  X ,Y corr = ( X ,Y )

cov( X , Y ) E[( X - µ X )(Y - µY )] =

 XY

 XY

(4.7)

In Equation (4.7)  X ,Y is the correlation between the variables X and Y , cov( X ,Y ) represents the covariance between X and Y ,  X is the standard deviation of X , µ X is the average of a variable X , and E is the expected value operator. Table 4.5 Standard & Poor's rating scheme description Category Description The best quality borrowers. Reliable and stable (many of the governments) Quality borrowers, a bit higher risk than AAA

Rating category AAA AA+ AA AAA+ A ABBB+ BBB BBBBB+ BB BBB+ B BCCC+ CCC CCCCC+ CC CCD

Economic situation can affect finance

Medium class borrowers, which are satisfactory at the moment

More prone to changes in the economy

Financial situation varies noticeably

Currently vulnerable and dependent on favorable economic conditions to meet its commitments

Highly vulnerable, very speculative bonds Has defaulted on obligations and S&P believes it will generally default on most or all obligations

66

4.7.3

P-value of correlation

P-value shows how far our test statistics is in the right hand of the distribution of the null hypothesis. The further the test-statistics is the smallest is the p-value, implying evidence against the null hypothesis. A useful property of p-value is that it does not give a Yes or No answer. It rather can measure the strength of the evidence against the null hypothesis which in our case is an assumption of no correlation between DEA efficiency and the two above mentioned measures of financial credit health.

4.7.3

Correlation confidence interval

^ x , y is asymptotically distributed with mean  x , y as defined in Equation The value of 
2 2 (4.7) and variance (1 -  x , y ) / n . This result is generally not used, because of the slowly

^ x , y to its real asymptotic value. Fisher proposes an alternative method to convergence of 
construct a confidence interval for the correlation using the z -transformation. ^ xy 1  1+  ln  ^ xy 2   1-     

z=

(4.8)

Then the upper and lower bound of an approximate (1 -  )% confidence interval for  x , y can be expressed as:
zL = z - Z (1- ) / 2 n - 3

(4.9) (4.10)

zU = z + Z (1- ) / 2 n - 3

67

In Equations (4.9) and (4.10), Z (1- ) / 2 represents the (1 -  ) / 2 quintile of the standard normal distribution, n is the size of the sample, and z is defined in Equation (4.8).

4.7.4 Kolmogorov-Smirnov test (K-S test) In statistics the Kolmogorv-Smirnov test is used to determine whether two samples come from the same distribution. It is not necessary to specify what should be the type of the distribution. The two-sample K-S test is a variation of the original test and will be used as a goodness of fit test for our research. The null hypothesis of the test states that the two variables come from the same distribution and is tested against the alternative which states that they come from different distributions. The test statistic is calculated as:

= Dn,n ' sup | E1,n ( x) - E2,n ' ( x) |
x

(4.11)

In Equation (4.11), n and n ' stand for the sample size of the two populations being tested and E ( x) stands for Empirical distribution of a random variable. An Empirical distribution is a step distribution defined as: En ( x) = (number of elements in the sample  x )/ (number of elements in the sample). The null hypothesis is rejected if:
nn ' Dn,n ' > K n + n'

Where  is the level of significance for the test, determined by the researcher and Dn,n ' is defined in Equation (4.11), and K is the critical value of the Kolmogorov distribution for level  .
68

CHAPTER 5: RESULTS AND DISCUSSION

The purpose of this research is to empirically investigate the best DEA model that could be used for credit rating. The process of obtaining and applying the best model included several steps. First, in order for the research to be empirical, we collected data of 378 companies for eleven financial indexes and then combined the financial indexes into seven financial ratios. (The data is collected from Mergent ). Since the purpose of the research is to find an empirical application of DEA into credit rating the companies were selected randomly from random sectors. In order to assure that the performance of DEA models is consistent with time, we collected data for the years from 1995 to 2006. Then, we divided the companies into sectors, representing similar production processes and economic conditions. This reduced the number of companies to 235, because we wanted sectors to have a size greater than 30. The size of the sectors was chosen by following the rule of the thumb for a minimum sample size (Cooper et al. 2000) saying that the number of decision units in the sample should be greater than three times the sum of input and output variables. The process of final models formulation included several steps. First, slack models were chosen over radial models, because of their capability to tackle negative data. Then, we tested the following slack models for their performance: the Additive, the Goal Vector Approach, the Slack Based Measure of Efficiency, and the Measure of Inefficiency Proportion model. We found that the Slack Based Measure of Efficiency and the Measure of Inefficiency Proportion models had a poor level of performance and that the Goal
69

Vector Approach and the Additive models had a higher level of performance. We decided to continue with the Goal Vector Approach, because of its units invariant property. Then, we developed six models for the Goal Vector Approach model. Each of these models is characterized by inclusion of different DEA procedures and aims to improve different aspects of the analysis. The six models are the Goal Vector Approach (GV) (used for the performance comparison between the four slack models) the Goal Vector Approach with Weights Restrictions (GV+WR), (we included the Absolute Weights Restriction technique in the analysis, because it prevents the attribution of extreme importance to some of the variables in the overall estimate of efficiency), the Goal Vector Approach + Principal Component Analysis (GV+PCA) (we included the PCA technique in the analysis, because it decreases the dimensionality of the problem and clears the data from some unnecessary information), Goal Vector Approach + Principal Component Analysis+ Weights Restriction (GV+PCA+WR) (this model is a combination between PCA and WR), Stochastic Goal Vector Approach (SGV) (we included Stochastic DEA in the analysis, because it allows stochastic variation of data, implying that observed value of variables can be different from their actual values) and Stochastic Goal Vector Approach + Principal Component Analysis (SGV+PCA) (this model is a combination between Stochastic DEA and PCA). We did not apply weights restriction to the last two models, because including weights restrictions give exactly the same results as not including them. According to our knowledge, GV, GV+WR, GV+PCA, and SGV models have never been applied in credit rating and GV+PCA+WR, and SGV+PCA have never been used in any type of research in the literature. In order to conclude whether one or more of the six models can be used for future credit rating assessment, we calculated the

70

correlation between models estimated efficiency and two indexes: Market Price, and Standard & Poor's credit rating. Therefore, for each company we evaluated a DEA efficiency measure for each of the six models and for each of the twelve years. Then, we combined the companies into five sectors and we calculated first the correlation between DEA efficiency (evaluated by the six models) for the companies in each sector and their Market Price for each year. In the next step, we selected a best performing model among the above described six models and we made a statistical analysis of its correlations by calculating the correlations' p-values, a 95% confidence interval for the correlations, and the p-values of the Kolmogorov-Smirnov test. Trends and dynamics of these correlations are discussed as follows. The correlations between market price and efficiency for the five sectors are displayed in Figures 5.1.1-5.1.5, where each figure shows the correlation for one sector.

0.8

Correlation bwtween Market Price and efficiency

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV GV+WR SGV GV+PCA GV+PCA+ WR SGVA+PCA

Year Figure 5.1.1 Correlation between Market Price and efficiency for Sector 1
71

The y-axis in Figure 5.1.1 presents the correlation between Market Price and efficiency, evaluated by each of the six models for Sector 1. Each correlation is calculated for 37 companies in Sector 1 (paper and allied products, printing and publishing) for each of the years from 1995 to 2006. One can notice that the GV and GV +WR have the poorest level of performance among the models. It can be observed also that there is a slight difference between these two models and it is hard to say which one performs better. It is also easy to observe that GV+PCA and GV+PCA+WR models perform better than GV and GV+WR models. The choice of the best performer model, however, should clearly be the SGV+PCA, because it achieves a higher correlation in almost all the cases. The correlation of the SGV+PCA model is rather high and is usually above 60% for all the years with the exception of the year 1998, when it reaches a level of 41%. The low correlation could be explained by the Asian crisis around the year 1998, when US exports to East Asia fell by 12%. Companies' Market Price for 1998, however, did not change significantly with respect to the companies' Market Price in year 1997. In contrary, DEA efficiency for inefficient companies decreased significantly for the year 1998.

72

0.8

Correlation between Market Price and efficiency

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV GV+WR SGV GV+PCA GV+PCA+WR SGVA+PCA

Year Figure 5.1.2 Correlation between Market Price and efficiency for Sector 2

Figure 5.1.2 presents the correlations between Market Price and DEA efficiency evaluated for by the six models for sector 2 (chemical and applied products). The correlations for the six models move in a similar way, but the SGV+PCA can be concluded to be the best model with a lowest level of correlation equal to 57%.

73

0.9

Correlation between Market Price and efficiency

0.8 0.7 0.6 GV 0.5 0.4 0.3 0.2 0.1 0 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV+WR SGV GV+PCA GV+PCA+WR SGVA+PCA

Year Figure 5.1.3 Correlation between Market Price and efficiency for Sector 3

We find a different situation in Figure 5.1.3, which shows the correlations between market Price and DEA efficiencies for Sector 3 (rubber and misc. plastic products, leather and leather products). We can see that the GV+PCA and GV+PCA+WR models perform better than the SGV+PCA model for the years from 2000 to 2006, with the exception of the year 2005. The correlations are stable for Sector 4 (transportation equipment, instruments and related products) as one can see in Figure 5.1.4. Again, GV+PCA is the best model with a correlation, floating around 70% and reaching a lowest level of 62%.

74

0.9

Correlation between Market Price and efficiency

0.8 0.7 0.6 GV 0.5 0.4 0.3 0.2 0.1 0 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV+WR SGV GV+PCA GV+PCA+WR SGVA+PCA

Year Figure 5.1.4 Correlation between Market Price and efficiency for Sector 4

Finally, the correlation between Market Price and DEA efficiency for Sector 5 (wholesale trade-durable and non durable products) can be found in Figure 5.1.5. Similar to Sector 1, the correlation for the GV+PCA model is above 60% with the exception of the years of the Asian crisis -1997 and 1998.

75

0.8

Correlation between Market Price and efficiency

0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

GV GV+WR SGV GV+PCA GV+PCA+WR SGVA+PCA

Year Figure 5.1.5 Correlation between Market Price and efficiency for Sector 5

The p-values for the correlations between Market Price and SGV-PCA are presented in Table 5.1.1. Table 5.1.1 P-value of correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency Sector1 Sector2 Sector3 Sector4 Sector5 1.64E-05 1.51E-08 1.81E-07 4.65E-10 8.08E-10 2.57E-07 1.44E-09 4.78E-10 1.74E-11 1.05E-07 1.11E-05 1.44E-06 4.70E-09 7.55E-07 7.45E-05 3.07E-05 2.56E-05 4.99E-07 4.72E-06 0.000144 1.00E-09 2.05E-05 2.14E-06 5.68E-09 2.89E-08 5.66E-09 2.55E-05 0.00027 5.06E-08 3.84E-05 1.24E-06 4.45E-08 0.001214 1.16E-07 1.04E-06 2.49E-08 9.60E-08 8.30E-06 4.87E-07 2.57E-07 7.36E-09 1.45E-06 0.001985 1.41E-06 2.68E-08 5.01E-08 2.27E-06 0.000258 2.68E-07 1.39E-07 1.09E-07 1.64E-05 4.56E-09 1.33E-07 4.07E-08 2.37E-05 3.69E-05 0.000238 2.75E-10 3.06E-07
76

Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

All the p-values are very close to zero bringing very strong evidence against the null hypothesis of no correlation. The p-values of the Kolmogorov-Smirnov test are presented in table 5.1.2 and confirm the same results. Table 5.1.2 P-value of Kolmogorov-Smirnov test for the significance of correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency Sector1 Sector2 Sector3 Sector4 Sector5 4.68E-22 2.96E-20 4.36E-18 1.91E-16 1.57E-18 4.68E-22 3.81E-21 6.14E-19 6.14E-19 2.96E-20 4.68E-22 6.92E-17 4.36E-18 6.14E-19 1.57E-18 4.68E-22 2.55E-15 2.95E-17 8.26E-20 2.96E-20 3.81E-21 1.07E-17 1.18E-15 2.95E-17 1.07E-17 3.81E-21 7.86E-14 1.09E-12 1.18E-15 2.55E-15 3.81E-21 4.30E-16 4.78E-10 8.26E-20 2.55E-15 1.57E-18 4.08E-13 1.09E-12 4.36E-18 4.08E-13 2.96E-20 1.57E-18 1.09E-12 6.14E-19 7.86E-14 2.96E-20 1.57E-18 6.97E-15 1.06E-20 4.30E-16 2.96E-20 4.30E-16 1.91E-16 6.14E-19 1.07E-17 1.07E-17 1.45E-14 6.14E-19 1.06E-20 1.07E-17

Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

The 95% confidence intervals are summarized in Table 5.1.3. The ranges of the intervals are rather large, which can be explained by the rather high level of significance

( = 0.05). The confidence intervals assure that if the formation of confidence interval
was evaluates for an infinite number of times, the true value of the correlation would be in 95% of the intervals. One can observe that the only sector that has narrow confidence intervals is sector 4.

77

Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

Table 5.1.3 95% Confidence interval for the correlation between Market Price and Goal Vector Approach Stochastic PCA efficiency Sector 1 Sector 2 Sector 3 Sector 4 Sector 5 0.36 0.75 0.54 0.83 0.49 0.81 0.62 0.87 0.60 0.86 0.47 0.80 0.59 0.85 0.61 0.87 0.67 0.89 0.50 0.81 0.37 0.75 0.43 0.78 0.57 0.85 0.45 0.80 0.31 0.72 0.34 0.74 0.34 0.74 0.46 0.80 0.40 0.77 0.28 0.71 0.60 0.86 0.35 0.74 0.42 0.78 0.57 0.85 0.53 0.83 0.56 0.84 0.34 0.74 0.26 0.70 0.52 0.83 0.33 0.73 0.43 0.78 0.52 0.82 0.20 0.66 0.50 0.82 0.44 0.79 0.53 0.83 0.50 0.81 0.38 0.76 0.46 0.80 0.47 0.80 0.56 0.84 0.43 0.78 0.18 0.65 0.43 0.79 0.53 0.83 0.51 0.82 0.42 0.78 0.26 0.70 0.48 0.81 0.49 0.81 0.50 0.81 0.36 0.75 0.57 0.85 0.50 0.82 0.52 0.82 0.35 0.74 0.33 0.73 0.27 0.70 0.62 0.87 0.47 0.80

The correlations between Standard & Poor's credit rating and DEA Efficiency are reported in Figures 5.2.1-5.2.6. The correlation between S&P credit rating and efficiency for sector 1 is presented in Figure 5.2.1. The correlation is very stable for the SGV+PCA model and it is within the interval of 66% and 82%. The high correlation between S&P credit rating and SGV+PCA efficiency is evidence that for Sector 1, our model could be successfully used as a credit rating tool. The correlation between S&P credit rating and efficiency is presented in Figure 5.2.2. The correlation for the SGV+PCA model is stable achieving a minimum value of 76% and a maximum value of 91%. Hence, we can conclude that for Sector 2 the SGV+PCA model gives as precise credit rating estimates as the ones provided by S&P agency.

78

0.9

Correlation between S&P credit rating and efficiecny

0.8 0.7 0.6 GV 0.5 0.4 0.3 0.2 0.1 0
1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

GV+WR SGV GV+PCA GV+PCA+WR SGV+PCA

Year Figure 5.2.1 Correlation between S&P credit rating and efficiency for Sector 1

1

Correlation between S&P credit rating and efficiency

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1995 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV GV+WR SGV GV+PCA GV+PCA+WR SGV+PCA

Year Figure 5.2.2 Correlation between S&P credit rating and efficiency for Sector 2
79

`Let us observe the correlation between efficiency and S&P credit rating for Sector 4. The SGV+PCA model achieves the highest level of correlation which is rather stable with the exception of the year 2003, where the correlation is equal to 20%.

1

Correlation between S&P credit rating and efficiency

0.8

0.6

GV GV+WR SGV GV+PCA GV+PCA+WR SGV+PCA

0.4

0.2

0
1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

-0.2

Year Figure 5.2.4 Correlation between S&P credit rating and efficiency for Sector 4

And finally, let us observe the correlation between S&P credit rating and DEA efficiency for Sector 5, presented n Figure 5.2.5. The SGV+PCA is again the most stable model and it achieves a minimum correlation value of 50% and a maximum correlation value of 81%. One can also observe that the correlation has a positive trend with time. Therefore, we can conclude that the SGV+PCA model achieves highest correlation with S&P credit rating, which is very strong and stable for Sectors 1and 2, and sufficiently strong and stable for Sectors 4 and 5. Now, let us observe the statistical significance of these correlations.
80

0.9

Correlation between S&P credit rating and efficiency

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1995 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 GV GV+WR SGV GV+PCA GV+PCA+WR SGV+PCA

Year Figure 5.2.5 Correlation between S&P credit rating and efficiency for Sector 5

Table 5.2.1 shows the p-value of the correlations between the SGV+PCA model and S&P credit rating for the four sectors and for the 12 years. Again, all p-values are very close to zero, providing evidence for the strong relationship between the two variables. This is also confirmed by the p-values of the Kolmogorov-Smirnov Test that can be found in Table 5.2.2. P-values, almost close to zero are strong evidence that the observations of SGV+PCA efficiency and S&P credit rating come from the same sample.

81

Table 5.2.1 P-value of correlation between S&P credit rating and Goal Vector Approach Stochastic PCA efficiency
Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 Sector1 1.51E-03 8.39E-06 3.78E-06 6.05E-08 2.15E-06 1.89E-05 5.72E-05 3.03E-06 2.29E-05 7.10E-07 7.14E-08 4.84E-09 Sector2 1.28E-09 1.52E-09 1.93E-07 6.09E-09 4.09E-11 1.17E-10 5.85E-10 1.04E-11 6.04E-10 4.58E-15 4.93E-12 2.87E-13 Sector4 1.20E-02 2.45E-03 2.31E-03 0.03536 4.91E-03 8.34E-04 4.95E-04 1.80E-05 2.51E-04 1.43E-03 9.71E-05 1.28E-05 Sector5 8.08E-10 1.05E-07 7.45E-05 0.00014 2.89E-08 3.84E-05 1.04E-06 2.57E-07 2.68E-08 1.39E-07 4.07E-08 3.06E-07

Table 5.2.2 P-value of Kolmogorov-Smirnov Test for the significance of correlation between S&P credit rating and Goal Vector Approach Stochastic PCA efficiency
Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 Sector1 6.78E-15 6.78E-15 6.78E-15 8.18E-16 6.78E-15 8.18E-16 5.26E-14 6.78E-15 5.26E-14 5.26E-14 8.18E-16 8.18E-16 Sector2 2.40E-15 2.40E-15 1.76E-14 4.38E-05 7.78E-10 1.32E-02 2.97E-07 1.21E-13 1.21E-13 1.21E-13 1.21E-13 1.76E-14 Sector4 2.48E-11 2.15E-03 2.10E-10 2.48E-11 2.48E-11 5.93E-01 6.40E-03 2.48E-11 2.48E-11 2.48E-11 2.48E-11 2.48E-11 Sector5 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 1.34E-08 6.97E-07 1.34E-08

82

The 95% confidence intervals of the correlations between S&P credit rating and SGV+PCA efficiency are shown in Table 5.2.3 and are rather large, which again is a result of the high significance level of the intervals. Hence, the 95% confidence intervals can be applied successfully only for Sector 2, providing very strong evidence that the true value of the correlation is within the upper and lower bound of these intervals. Table 5.2.3 95% Confidence Interval for the Correlation between Standard & Poor's Credit Rating and Goal Vector Approach Stochastic PCA efficiency Sector 1 Sector 2 Sector 4 Sector 5 0.228 0.739 0.667 0.903 0.360 0.849 0.146 0.810 0.456 0.836 0.663 0.902 0.594 0.915 0.282 0.854 0.484 0.847 0.551 0.862 0.551 0.905 0.287 0.855 0.606 0.889 0.634 0.892 0.405 0.863 0.039 0.770 0.503 0.853 0.728 0.923 0.436 0.873 0.226 0.837 0.426 0.825 0.711 0.917 0.440 0.874 0.361 0.876 0.383 0.807 0.682 0.908 0.107 0.755 0.397 0.885 0.492 0.849 0.749 0.929 0.485 0.887 0.581 0.928 0.419 0.822 0.681 0.908 -0.219 0.574 0.440 0.896 0.538 0.866 0.841 0.957 0.625 0.923 0.323 0.866 0.602 0.888 0.760 0.933 0.759 0.954 0.495 0.909 0.665 0.908 0.797 0.944 0.649 0.929 0.596 0.931

Year 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

It is important to verify how well the SGV+PCA model allocates companies that experienced default. As discussed in Chapter 3, the Additive model assigns a value of zero for the efficient units and a value greater than zero for the inefficient ones. Therefore, companies that are classified as the worst (or less efficient) in the dataset will receive the highest value of inefficiency by the SGV+PCA model. Since the bankrupted companies have proved that in reality they are not efficient, we can check how the SGV+PCA model did classify them. If the model assigns the highest value of inefficiency to these bankrupted companies two to three years before the bankruptcy
83

occurred, then this will be evidence that the model can successfully classify inefficient companies. In order to do that ten bankrupted companies are added to the samples. Their efficiency level is calculated using their financial ratio values up to three years before companies filed for bankruptcy. The results are exposed in Table 5.3. Table 5.3 DEA allocation of bankrupted companies Company Name Acme Metal Acterna Corp Chesapeake Corp Carausta Industries Drypers Corp Fibermark Inc Gaylord Container CP Golden Books Family Entmt Gentek Inc Geneva Steel Hldgs Federal-Mogul Corp CHS Electronics Inc Fleming Companies Year of Year of estimated DEA Sector S&P Moody's filing for Number Rating rating bankruptcy efficiency Classification 3 NA 4 NA 2 NA 2 NA 1 NA 1 NA 1 NA 1 NA 2 B+ 3 NA 4 BB+ 5 NA 5 NA BB+ B+ BBBB+ B B BB+ B+ B+ BB B BB1998 2003 2008 2009 2000 2004 2002 1999 2002 1999 2001 2000 2003 1996 2001 2005 2006 1998 2002 2000 1997 2000 1997 1999 1998 2001 11% 1% 17% 11% 8% 3% 3% 1% 9% 10% 4% 8% 13%

The Moody's credit risk evaluation is included as well, because S&P does not provide a credit risk evaluation for most of the ten bankrupted companies. The column "DEA classification" shows within what percent of the most inefficient companies were classified the bankrupted ones. For example, the company Acme Metal is in sector 3,
84

which contains 37 companies. After running the SGV+PCA model there were four companies that received a higher level of inefficiency. Therefore Acme Metal was classified fifth within a dataset of 37 units and hence, it was classified in the lowest 11% of the companies in the dataset. It can be observed that most of the bankrupted

companies (80%) were classified two or three years prior to their bankruptcy by SGVPCA model in the 11% of the less efficient companies in the sample, which means that SGV+PCA could be used as a bankruptcy predictor. If a definite sector is of a particular research, a further analysis on the SGV+PCA predicting bankruptcy ability can be done by adding more bankrupted companies for the sector and observing a bankruptcy threshold for the model. For example, if SGV+PCA classify fifty bankrupted companies within the 25% of the less efficient companies in the sample, then a manager can use the model and invest only in companies which are in the 75% of the efficient companies in the sample. One can observe from the table that the companies Acme Metal, Carausta Industries, Federal-Mogul Corporation, Chesapeake Corporation, and Fleming

Companies were classified by Moody's in the BB-category. These companies, however, experienced bankruptcy two years after these ratings. The SGV+PCA model classified them within the 17% of the worst companies in the sample, suggesting that it was able to capture the risk of default in a better way than Moody's.

85

CHAPTER 6: CONCLUSION AND FUTURE RESEARCH

6.1 Conclusion
The purpose of the research was to find and apply a DEA model, capable of providing an appropriate measure of companies' financial health. The role of the model would be to give useful insights for an investor, whether a company can be trusted or not. Throughout the thesis, some of the DEA techniques were discussed and those proved to work better were combined and applied. The SGV+PCA was selected as the best model and its performance was tested in a more detailed way. According to our knowledge, the combination between Stochastic DEA and PCA has never been developed in the literature and the results, obtained from its application can be concluded to be rather successful. The DEA estimates are rather stable with some exceptions that could be related to different crisis that occurred in these particular years. It is important to notice that the purpose of the research is to explore the possibility for a future implementation of DEA in credit rating. That is why we considered some of the most discussed DEA disadvantages such as treatment of negative data, units invariance of the models, extreme sensitivity of DEA to sample selection, the DEA freedom of variables weights assignment, the correlation of variables and possibility for a stochastic variation of the data and then, we choose the model that fits better to the sample of 235 randomly selected companies. We encountered the difficulty, however, to determinate whether the models perform well or not mostly because it is impossible to know the real financial state of a company, because of public unavailability of such data. We followed the
86

approach of Edirisinghe and Zhan, (2007) and we used the Market Price as an estimate or approximation of companies' financial health. The correlation with Market Price resulted to vary between 60% and 80% suggesting that the SGV+PCA indeed captured important aspects of credit risk. Then, in order to bring more evidence for the performance of the SGV+PCA model, we compared the efficiency results to S&P credit rating. The high correlation between these two indexes proves that the SGV+PCA model provides as good estimates as specialized credit ratings. And finally, to bring even more evidence for the performance of the SGV+PCA model, we checked how it allocated ten bankrupted companies two years before the bankruptcy occurred. The main contribution of the thesis can be concluded to be the evaluation of the SGV+PCA model, because it can be seen from Figures 6.1.1 to 6.2.4 that the gap between SGV+PCA correlations and correlation of the simple of Goal Vector Approach varies from 10% to 70%. The model, however, has also some limitations. It does not provide ratings that classify the companies in precise categories as the ratings of Moody's and S&P which can be a subject for a future research. We also did not compare directly the performance of these credit rating companies and SGV+PCA model. It would be useful to investigate whether SGV+PCA model was able to capture some credit risk aspects that Moody's and S&P failed to

6.2

Future Research

Therefore, the possibilities for a future research can go in many different directions. One direction could be a further investigation of the SGV+PCA model performance. The model was proved to have a good overall performance, however, a further research can
87

be done by testing the model on new sectors or more bankrupted companies. The SGV+PCA performance can be compared in a more detailed way to the Standard and Poor`s credit rating. For example, it can be seen in Table 6.3, that the SGV+PCA model classified most of the bankrupted companies within 11% of the most inefficient companies in the sector. One can observe that, for instance, the companies Acme Metal and Carausta Industries were rated BB+ from Moody and they still experienced bankruptcy in the following two years. The SGV+PCA model, however, classified these companies within 11% of the worst companies in the sample. Therefore, this example can be a suggestion that the SGV+PCA model can capture the signs of default in a better way than the Moody`s models. In order to prove that, it is necessary to add more bankrupted companies to the sample and observe carefully the SGV+PCA and Moody`s classification and the differences among them. It can be also useful to find a statistical threshold for every sector that can be used for a future prediction of bankruptcy. For example if after testing 100 bankrupted companies for Sector 2, it is found that the model classifies 95% of the bankrupted companies within 30% of the worst companies in the sample, the SGV+PCA model can be used in the following way. Each year after running the model for Sector 2, do not invest in any of the companies that are classified within the 30% worst percent of the sample. As mentioned in the introduction, S&P, Moody`s and Fitch failed to capture the bankruptcies of sme companies during the East Asian crisis and consequently the global economic crisis in 2007. It may be tested how the SGV+PCA model classified the companies that were highly rated by S&P and later experienced bankruptcy. If the SGV+PCA model classified them as highly inefficient, then the model is a better
88

performer than S&P models. It can be seen in Figures 5.1.1 to 5.1.5 that the correlation between Market Price and SGV+PCA efficiency is decreasing during the East Asian Crisis in 1997 and 1998. Again, a suggestion that the SGV+PCA model could be indeed able to capture some aspects of credit risk that the S&P models cannot. A different direction for a future research is to test the performance of the model with different combination of financial ratios. As mentioned earlier, there has not been estabished a unique combination of financial variables, which represents in a best way the financial state of a company. Hence, applying only the set of the variables, presented in Table 6.1 is a limitation, and the model performance can be evaluated for many different combinations, for example that are shown in tables A.1 and A.2. Finally, a different direction for a future analysis of the SGV+PCA model could be its combination with other DEA or not-DEA techniques. The model can be tested for different slack models, values of weights or cone ratio restrictions etc. The model also could be combined with the super efficiency analysis, worst practice DEA, the Free Disposal Hull model, the cross efficiency analysis and others. .

89

APPENDIX I

A.I.1 The CCR model
Let us consider an example of seven units that use two inputs to obtain the same quantity of an output. The example is presented in Figure A.1, and a similar example can be found in Cooper et al., (2007).

Figure A.1 CCR model: An example

The efficient frontier is constructed by the lines CF, CD, DE, and ER. The Production Possibility Set is represented by the area above the efficient frontier. Points F, C, D, E,

90

and R are radially efficient and obtain a value of one in the objective function in Problem (3.2). A is inefficient and in order to calculate its efficiency it is necessary to first project point A to the EF, which in Figure A.1 is represented by point Q. The radial efficiency for point A is given by  A = OQ/QA. Let us now observe point F. Note that C achieves higher technical efficiency, because it invests smaller amount of input 1 and equal amount of input 2, obtaining equal amount of output. Observe also that F has a positive slack and hence is not Pareto-Koopmans efficient, even if  F = 1 . Let us now observe unit G. Its projection on the efficient frontier is represented by point R. However, point R is not fully efficient and exhibits a positive slack as well. We just illustrated the two types of efficiency and the two types of inefficiency found in DEA. Units can be: · · · · Fully efficient (radially efficient with zero slacks) ­ C, D, E; Partially efficient (radially efficient with positive slacks) ­ F and R; Inefficient first type (radially inefficient with zero slacks) ­ A and B; Inefficient second type (radially inefficient with positive slacks) ­ G.

Hence, units F and G can be classified as efficient and still possess positive slacks. The Envelopment Form of DEA gives the possibility to estimate the values of the slacks. This is done in a two phase procedure. The purpose of Phase 1 is to find the radial efficiency
91

 * , which is then used in Phase 2 as a number and no longer as a unknown variable. The
purpose of Phase 2 is to maximize the value of the slacks, given the optimal level of efficiency, calculated in Phase 1. Phase 2 is given by:

Maximize subject to:

=  es - + es +
s -  * xo - X  , =
+ s = Y  - yo ,

(I.1)

  0, s -  0, s +  0.
In Problem (I.1) e = (1,...,1) is a vector of ones, so that es - =  si- and es + =  sr+ . X is
i =1 r =1 m s

a m × j matrix of input observations and Y is a r × j matrix of output observations. The two phases can be combined in only one optimization problem (see Cooper et al., 2007). This is done by the introduction of the non-Archimedean element   0. Now it is finally possible to define the concept of efficiency in technical terms. Definition A.1. (Cooper et al., 2007). If an optimal solution ( * ,  * , s -* , s +* ) of LP Problem (3.2) satisfies  * = 1 and is zero-slack= ( s -* 0, s +* 0) , then the DMU is called = CCR-efficient. Otherwise, the DMU is called CCR-inefficient. The following conditions: · ·

* =1
All slacks are zero

Must be both satisfied if full efficiency is to be attained (Cooper et al., 2007, p. 45). The significance of the slacks is important and should not be neglected by applying only

92

Phase I in the analysis. According to Cooper et al., (2004) "...a considerable part of the DEA (and related) literature continues to be deficient in its treatment of non-zero slacks even today"( p. 7). The type of DEA efficiency, defined in Definition A.1, is also known as Pareto-Koopmans Efficiency. Definition A.2. Pareto-Koopmans Efficiency. A DMU is fully Pareto-Koopmans efficient if and only if it is not possible to improve any input or output without worsening some input or output.

A.I.2 The BCC model
The following example shows how the additional constraint e = 1, in Problem (3.3) changes the interpretation of efficiency. It considers five units, which use one input to produce one output. The corresponding points are plotted in Figure A.2.

93

Figure A.2 BCC model: An example

The efficient frontier for CCR is defined by the points, allocated on the line OB. Thus, for the CCR model, only point B is efficient. Now, let calculate the measure of inefficiency for point D. The projection of point B on the efficient frontier is reflected by point Q. Therefore, the efficiency for point B is calculated by PQ/PD=2.25/4=0.5625. In the BBC model the efficient frontier is constructed by the lines, connecting points A, B, and C. Hence, BBC efficiency for point D is given by PR/PD=2.6667/4=0.6667. Here we can observe a property of the BCC model: the BCC efficiency is always higher or equal to the CCR efficiency and, hence, if a point is classified as CCR efficient, then it is also BCC efficient.

A.I.3 Returns to Scale (RTS)
Every unit in the sample is characterized by either Constant Returns to Scale (CRS), Increasing Returns to Scale (IRS) or Decreasing return to scale (DRS). The interpretation of RTS in DEA is similar to the interpretation of RTS in economics. When a unit is characterized by constant returns of scale, it reaches its optimal efficiency; that is, an increase in inputs leads to a proportional increase in the outputs. When a unit is characterized by increasing returns to scale, then it may reach a higher efficiency level by increasing its inputs, because an increase in the inputs leads to a more than proportional increase in the output.
94

Finally, when a unit is characterized by decreasing returns to scale, it can achieve a higher efficiency by decreasing its outputs, because a decrease in its output level leads to a more than proportional decrease in its input level. As discussed previously, the researcher has the freedom to add or omit the constraint

  j = 1 in
j =1

n

the Additive optimization model, defined in Problem (3.4). Its inclusion

transforms the shape of the efficient frontier and allows for point with IRS, CRS and DRS to be classified as efficient. Figure A.3 shows the two types of frontiers (with and without the constraint   j = 1 ) and explains the concept of RTS.
j =1 n

The efficient frontier for the Additive Model, not including the constraint

j = 1
j =1

n

(Additive 1) is formed by the line OBC and the efficient frontier for the Additive including this constraint (Additive 2) is formed by the lines AB, BC, and CD. It can be observed that for Additive1 model, only points lying on the line OBC, i.e., points, exhibiting constant returns to scale, are classified as efficient. For the Additive 2 model, points lying on the lines AB, BC, and CD, exhibiting respectively increasing, constant and decreasing RTS are classified as efficient.

95

Figure A.3 Interpretation of returns to scale

Point A is classified as a point, exhibiting IRS, because its production function is formed by the line AB. Observe that for points belonging to line AB, an increase in x leads to more than proportional increase in y . Similarly, point D is classified as a point, exhibiting DRT. Its production function is characterized by the line CD. For all the points on this line an increase in x leads to a less than proportional increase in y . In order to estimate the RTS of inefficient units, it is necessary to project them on the BCC efficient frontier. For example M is estimated as a unit, exhibiting DRS because its projection is on the DRS part of the frontier. Notice that points B and C are characterized

96

with CRS, even if point B, for example, is located on the intersection of an IRS part and a CRS part of the EF.

Let us observe the change in the efficient frontier if we substitute the constraint

j = 1
j =1

n

by the constraint   j < 1 . In that case the efficient frontier is constructed by the lines
j =1

n

OBC and CD, and thus, point A is no longer efficient. Returns to scale are easy to determine for the points on the efficient frontier, but not always that easy for the rest of the units. This is a consequence of the fact that a point can be equally far from two different sections of the efficient frontier as for instance is point H in Figure A.3. The following theorem is used to estimate the RTS on the efficient frontier. Theorem A.1. (Banker, 1992) Assuming that ( xo , yo ) is on the efficient frontier the following conditions identify the situation for returns to scale at this point, ·
* Increasing returns to scale prevails at ( xo , yo ) if and only if uo < 0 for all optimal

solutions. ·
* Decreasing returns to scale prevails at ( xo , yo ) if and only if uo > 0 for all optimal

solutions. ·
* Constant returns to scale prevail at ( xo , yo ) if and only if uo = 0 for all optimal

solutions.

97

* In the above theorem uo is the value of the variable uo obtained in Problem (3.5). In

order to find the type of RTS for the radially inefficient units in the sample, it is necessary to follow a different procedure, explained in Appendix I.

A.I.4 Variables weights restrictions
As mentioned in previous sections, DEA has the freedom to choose the values of variables' weights in a way that represents the unit under investigation in its best light. This DEA property, however, can lead to close to zero or extremely high weights, for some of the variables. In those cases, some variables can contribute only slightly to the estimate of efficiency and alternatively some variables can be considered in an excessive way in the optimization function. This issue can be solved by introducing assurance region constraints or absolute weights restrictions which set upper and lower limits for the variables' weights. Absolute Weights Restrictions (AWR) are simple to interpret and are given as: Li  vi  U i for i = 1,..., m.

Lr  ur  U r for r = 1,..., s. The values of Li and U r are set by the user. The Multiplier structure of Problem (3.4) includes constraints, forcing the inputs and outputs weights to be greater or equal to one. Hence, in implementing the Additive model in next chapters, we use only an upper bound for the weights. In the cases, where it is

98

necessary to use the envelopment form of the Additive model, the inclusion of AWR constraints is summarized as: Vector t is of dimension m and presents the upper weights constraints from the multiplier form for the inputs; similarly, vector b is of dimension s and presents the upper weights constraints from the multiplier form for the outputs.

Envelopment form Minimize vxo - uyo + uo vX - uY + uoe  0,

Dual form

Maximize subject to:

es - + es + - U I t - U O b
X  + s- - t = xo , Y  - s+ + b = yo ,

subject to:

u  U O,
v U I,

  0, s -  0,
s +  0, t  0.

v  e, u  e, uo free in sign.

The Assurance Region Restriction (ARR) restrictions aim to set limits to the ratios between the variables. The most commonly used are defined as:

i 

vi  i vi +1

99

 i vi +  i +1vi +1  vi +2
ARR are particularly useful when it is known a priori that some variables are more important than others in the efficiency determination. The first type of constraints is used when one needs to define the relationship between two variables and the second type of constraints can be used when one needs to define the relationship between more than two variables. The degree of importance of the variables, i.e., the values of  i ,  i , and ki are chosen by the user. Even if the choice of the values of upper and lower Absolute Weights Restrictions and Assurance Region Restrictions are left to the user, the literature provides some useful insights on how this choice can be made. · Managerial/Researcher decision or expert opinion. Sometimes the choice of weights restrictions can be justified by a priori knowledge of the importance of the variables. For instance in our research, we show that the correlation between the variable Size and market price is the highest among the correlations between the rest of the variables and Market Price. Therefore, we could be willing to insert higher values for the upper absolute weights restrictions for this variable. · Best Practice Weights. On the first stage of this approach, the DEA problems are run without weights restrictions. Then the efficient units are identified and their weights are implied to the rest of the DMUs. The same procedure can be followed by implying the weights of units that are known a priori to be best performers.

100

·

Statistic Approach. There exist different suggestions, involving statistical techniques in weights adjustment practices. A useful practice can be to determine and eliminate the weights, obtained through the unrestricted DEA models, representing outliers. An alternative method is to set the upper and lower bounds to the sum of the mean value of weights, obtained again using the unbounded DEA models, and a certain amount of variation.

A.I.5 Advantages and disadvantages of DEA
Advantages · Production function. An important advantage of DEA models is the absence of necessity of production function definition. · Sample size. In order for the efficiency estimates to be valid a rule of the thumb, defined by Cooper et al. (2000), implies that the number of DMUs has to be at least three times the sum of inputs and outputs, considered in the optimization. Many econometric credit rate models require a much larger sample size. · Multiple Numbers of Inputs/Outputs. DEA allows the use of multiple inputs and outputs. Several DEA models have been developed, allowing difference in variables measurement units. · Real Behavior. DEA compares inefficient companies to efficient ones that exist in real life and are not only theoretical.

101

·

Pareto Optimal. The efficiency obtained in the radial models is Pareto-Optimal, which is a desired property for any aggregate production efficiency.

·

Computational time. DEA is a linear model, and thus requires less computational time.

Disadvantages · Input/output Orientation. When a radial model is chosen for evaluating efficiency estimation, it is necessary to choose between input and output oriented model. · Robustness. Robustness is the ability for a model to perform adequately and with minimum damage even if some "stress" is applied to the system. In the case of financial data this could be missed variables, unnecessary variables, measurement errors, wrong returns of scale assumptions and other misspecifications. The measure of efficiency for DEA is constructed by the evaluation of the frontier which is usually composed by outliers. Therefore, a simple error measure could completely alter the obtained results. This issue can be fixed by using the superefficiency model or statistical models. · Ranking of efficient units. Efficiency, obtained by DEA is an interval variable and, hence, allows for a ranking system between inefficient units. Contrary, DMUs that construct the frontier obtain an efficiency measure expressed as one for the radial models and zero for the slack models. Therefore, it is not possible to obtain a ranking measure within the group of efficient units.

102

·

Increasing number of variables. As the number of variables increases, the size of the Production Possibility Set increases as well. This leads to a higher number of units classified as efficient and hence decreases the discriminatory power of the model. This issue should be considered carefully when the number of units is low and the number of variables is high.

·

Convexity assumption. All DEA models, except for the Free Disposal Hull model (e.g. Deprins, et al., 1984), include an assumption of convexity of the efficient frontier. This assumption implies that an inefficient point can be compared to a point on the frontier, which does belong to the observation sample and is a linear combination of two actually observed efficient units, which can be found unrealistic in some cases.

·

Choice of inputs and outputs. In DEA undesirable variables such as cost, debt, implied resources in the production are classified as inputs, and variables presenting a desired outcome such as obtained products are classified as outputs. In some cases, however, the status of a variable is not easy to define. This problem can be also encountered in cases, where nature of variables changes with time.

·

Difficult to interpret. The weights of variables assigned by DEA vary from unit to unit. Hence, it is rather challenging to find a general trend in a sample of data using DEA.

103

A.I.6 Negative data in DEA
The most important advantage of the Additive model is that it allows the inclusion of variables, containing negative observations. This is possible because of its translation invariant property. Let us define some of the methods aiming to solve the negative data problem for the radial models. · Reverse the sign. If all the observations in the variable contain non-positive data, the problem can be solved by simply reversing the sign and inverting the inputs into outputs and respectively the outputs into inputs. · Ignore DMUs with negative values. The DMUs could be simply removed from the analysis. However, attention should be paid in the implementation of this method, because usually most efficient units possess negative input values. Removing even one of the efficient units can lead to completely different result. · Add a positive constant. A different approach is to transform the negative values into positive by adding a constant. It has been argued (Ali and Seiford, 1990) that this is not a proper way to confront the negative values problem, for the radial models, concluding that the optimum values of the problems before and after adding the constant are different. · Use the BCC input-oriented model. The BCC input-oriented model is output translation invariant (Pastor, 1994). Therefore the BCC model allows for the outputs to contain non positive values, as long as the input values are all positive.

104

·

The Directional Distance Approach. The Directional Distance Approach was introduced by Portela et al. (2004). It is a radial model that is both translation and units invariant.

A.I.7 Principal Component Analysis - Methodology
The PCA procedure consists of five steps. Step 1. Standardize the variables. The first step is to normalize all the variables in the model, obtaining normally distributed variables with mean equal to zero and variance equal to one. The normalized variables create a useful way to estimate the importance of each Principal Component (PC). The sum of the variances of normalized initial variables is equal to the number of variables in the analysis. The total initial variance is then partitioned among the principal components, in a way that the first component explains a higher amount of the total variance, the second explains less amount of it, etc. The concept of total variance partition is useful when it is necessary to make a choice of how many principal components should be dropped from the analysis. For example, in the case where the initial number of variance is equal to five, the total variance is equal to five as well, and a principal component, with an eigenvalue, greater than three should be definitely kept. A component, explaining 0.3 of the total variance can be dropped, etc. It is said that in this case the component has communality equal to 0.3. Step 2. Calculate the correlation matrix. The inputs correlation matrix W is calculated as:

105

W

 1   corr ( X , X ) 2 1   = .  .   corr ( X , X  m 1) 

corr ( X1, X 2 ) .

.

. corr ( X1, X m )  . . . 1
         

. . . . .

The output correlation matrix is calculated in the same way, using the output variables. Step3. Calculate eigenvalues and eigenvectors. The normalized eigenvectors l1 , l2 ,..., lm of W and the eigenvalues 1  2 ...  m  0 of W are calculated in this step. To every eigenvalue corresponds one eigenvector with dimension equal to the number of initial variables. Higher eigenvalues correspond to higher communalities. Step 4. Form the new principal components. Let X be the initial variables matrix of dimension ( j × m) , where X = [ X 1 , X 2 ,..., X m ] and l be the eigenvector matrix of

dimension (m × m) , given by: l = [l1 , l2 ,..., lm ] . Each PC is formed by multiplying the original data matrix by one eigenvector X PCi = Xli = l1i X1 + l2i X 2 + ... + lsi X s The number of the extracted new principal components is equal to the number of variables in the analysis. Step 5. Choose the remaining components. The choice of how many variables should be eliminated is left to the user. There has not been established a unique rule of the thumb on how to decide on this matter. However, there exist different suggestions that help to deal with this problem. In the next paragraphs follows a brief description of some of them.

106

·

The eigenvalue-one criterion. The principal components are dropped if their corresponding eigenvalue is less than one. Stevens (1986) warns that the eigenvalue-one criterion should be applied only when the number of variables is less than 30 and the components' communalities are higher than 0.7.

·

The scree test. The scree test was proposed by Cattell (1966) and is evaluated by plotting the eigenvalues in the attempt to find a relatively big break between them. If such a break exists, the principals before the break are kept and the principals after the break are dropped from the analysis. According to Stevens (1986), the scree test provides reliable results when the sample contains at least 200 observations. In some cases, however, it is rather difficult to spot a break between the eigenvalues and the scree test cannot be used.

·

Variance Proportion. The method consists in adding PCs until the sum of their communalities reaches a predefined desired level. This approach was criticized by Kim and Mueller (1978), because the predetermined target of the explained variance is arbitrary and left to the user.

· Minimal Sample Size. A rule of the thumb for PCA is that the sample size
should be at least five times the number of variables in the analysis.

107

APPENDIX II

A.II.1 Identification of RTS for points that are not on the frontier
Theorem A.I can be used only for points lying on the frontier. In order to find the Returns to Scale (RTS)' nature for the rest of the points, it is necessary to follow a different procedure. Since slack models are of interest for this research, we will follow procedure that aims to determine the returns of scale for the non-frontier points in the Additive model. To determine the RTS nature of non frontier points for the BBC model, see Banker et al. (1996).
* , si-* , , and Step1. Solve the optimization Problem (3.5) and find the optimal value of uo

+* . sr

Step 2. Project the point on the efficient frontier by:
^= x xio - si-* , io
+* ^= y yro + sr , ro

i = 1,..., m,

r = 1,..., s,

* Step 3. If the value of uo is non positive, solve the additional optimization problem:

Maximize

^o u

subject to:

i 1= r 1 =

- vi xij +  ur yrj - uo  0

m

s

j = 1,..., n, j  o,

(II.1)

108

i 1= r 1 =

^io +  ur y ^ ro - u ^o = - vi x 0
^o > 0. ur  1, vi  1, u

m

s

j = o,

* If the value of uo is non-negative, in Problem (3.5), solve the optimization problem:

Minimize

^o u

Subject to:

i 1= r 1 =

 vi xij - ur yrj + uo  0
^io - ur y ^ ro + u ^o = 0  vi x
m s

m

s

j = 1,..., n, j  o,

(II.2)

j = o,

i 1= r 1 =

^o > 0. ur  1, vi  1, u
* ^o , y ^o ) ^o Step 4. If the optimum value u < 0 in Problem (II.1) then the RTS in the point ( x * ^o , y ^ o ) are constant. ^o are increasing, if the u = 0 in (II.1), then the RTS in point ( x * ^o , y ^ o ) and ^o Alternatively if u > 0 in Problem (II.2), then the RTS are decreasing in point ( x * ^o , y ^ o ) are constant. ^o finally if u = 0 in Problem (II.2), then RTS in point ( x

A.II.2 Formulation of the multiplier's dual
For a maximization linear problem of n variables and m constraints, such as :

109

Maximize subject to:

c1x1 + c2 x2 + ... + cn xn a11x1 + a12 x2 + ... + a1n xn  b1, a21x1 + a22 x2 + ... + a2 n xn  b2 ,


am1x1 + am 2 x2 + ... + amn xn  bm ,
x j  0,

(II.3)

j = 1,..., n,

The dual version of (II.3) is a minimization problem with m variables and n constraints and is expressed as: Minimize b1 y1 + b2 y2 + ... + bm ym

subject to: a11 y1 + a21 y2 + ... + am1 ym  c1, a12 y1 + a22 y2 + ... + am 2 ym  c2 ,


a1n y1 + a2 n y2 + ... + amn ym  cn , yi  0,
j = 1,..., n,

(II.4)

The output-oriented CCR model is given by: Maximize subject to: u1 y1o + ... + us yso v1x1o + v2 x2o + ... + vm xmo = 1,
u1 y1 j + ... + us ysj  v1x1 j + ... + vm xmj

j = 1,..., n,

(II.3)

v1,..., vm  0, u1,..., us  0.
110

Our purpose is to bring Problem (II.3) to the form of optimization Problem (II.4). To do that, we first have to express the equality constraint as two inequality constraints and multiply the second set of equations in Problem (II.3) by minus one. Maximize subject to: u1 y1o + ... + us yso v1x1o + v2 x2o + ... + vm xmo  1, -v1x1o - v2 x2o - ... - vm xmo  -1, u1 y11 + ... + us ys1 - v1x11 - ... - vm xm1  0, u1 y12 + ... + us ys 2 - v1x12 - ... - vm xm 2  0, (II.5)


u1 y1n + ... + us ysn - v1x1n - ... - vm xmn  0, v1,..., vm  0, u1,..., us  0. Now that Problem (II.5) has the same form as Problem (II.3), we can proceed to the dual formulation. Since the number of decision variables in the dual problem is equal to the number of constraints in the primal Problem (II.5), we will have n +2 variables in the dual problem. For each constraint of the primal problem, is necessary to insert a decision variable  in the dual problem. For every variable from the primal problem is necessary to insert a constraint in the dual problem. However, only the first two constraints will take part in the dual optimization function. The rest of the constraints in the initial problem have a right hand side equal to zero and therefore cannot be included into the
' optimization function. To distinguish this fact let assign the values 1' and 2 for the first

two constraints in Problem (II.5).
111

Minimize subject to

1' - 1'
x1o1' - x1o1' - x111 - x122 - ... - x1nn  0, x 2o1' - x2o1' - x211 - x222 - ... - x2 nn  0,

( v1 constraint) ( v2 constraint)


x mo1' - xmo1' - xm11 - xm 22 - ... - xmnn  0,

( vm constraint) (II.6) ( u1 constraint) ( u2 constraint)

y111 + y122 + ... + y1nn  y1o , y211 + y222 + ... + y2 nn  y2o ,


ys11 + ys 22 + ... + ysnn  yso ,
' n  0. 1' , 2
' , X = xij , where X is a (m × n) matrix of input observations, Let us assign  =1' -2

( us constraint)

Y = yrs , where Y is (s × n) matrix of output observations, x o the vector of DMU o inputs, yo the vector of DMU o outputs. Let  be the vector of dual decision variables
' ). Finally, by expressing Problem (II.6) in a matrix form, we (without including 1' and 2

obtain the dual CCR model. Minimize subject to:



 xo - X   0,
Y   yo ,

  0.

112

A.II.3 Deterministic equivalent of Stochastic DEA formulation
Problem (3.10) is given by;

Maximize

i 1= i 1 =

 si- +  sr+
r = 1,..., s,

m

s

Subject to:

  n +  rj  j - y  ro  sr P  y 1- j =  j =1      n  ij  j + si-  x io  =- 1 j P  x  j =1   

(II.7)

i = 1,..., m,

  j = 1, sr+  0,
j =1

n

si-  0, j = 1,..., n, r = 1,..., s, i = 1,..., m,

Now, in order to proceed, it is necessary to imply the assumption that inputs and outputs are normally distributed. Therefore if

 rj  j y
j =1

n

 ro are normally distributed, then diff and y

 rj  j - y  ro and y
j =1

n

ij  j -x io x
j =1

n

will be normally distributed as well. This is used to

further simplify the expressions in Problems (I.3.1) and (I.3.2). As known form statistics, the random term of a normally distributed random variable can be removed by subtracting its mean value and dividing the difference by its standard deviation. However, in order to imply the standardization of the variable, it is necessary to assume that its parameters (mean and standard deviation) are known a priori. If parameters are unknown, it is necessary to use the Student's t- statistics, where population parameters are estimated. For this research, however, distribution parameters can be estimated, because of the large sample size. That is why, we will assume that inputs and outputs are normally
113

distributed with known parameters. This will change the constraints in Problem (II.7) into:
 n   n   n  +  rj  j - y  ro  -   yrj  j - yro  sr -   yrj  j - yro     y  j1  j1   j 1= = =      = (II.8) P    j j = 1,..., n, o o     ( ) ( ) r r      

 n   n   n  ij  j - x io  -   xij  j - xio  si- -   xij  j - xio     x  j1  j1   j 1= = =      = (II.9) P    j j = 1,..., n, o o     ( ) ( ) i i      

 n   rj  j - y  ro   ro ( ) Var   y Where =  j =1   

(II.10)

 io ( ) =

 n  ij  j - x io  Var   x  j =1   

(II.11)

In statistics the variance of a linear combination of two random variables X and Y is

bY ) a 2Var ( X ) + b 2Var (Y ) + 2abCov( X , Y ) where a and b are expressed as Var (aX +=
constants. Therefore the variance of Equations (II.10) and (II.11) can be expressed as:

 ro ( ) =
 io ( ) =

k o j o

 rk , y  rj ) + 2o  k Cov( y  rk , y  ro ) + o 2Var ( y  ro ) r = 1,..., s   k  jCov( y
k o

(II.12)

k o j o

ij , x ik ) + 2o   j Cov( x ij , x io ) + o 2Var ( x io )   k  jCov( x
j o

i = 1,..., m

(II.13)

114

It can be noticed that the left side of the Equations (II.8) and (II.9) can be replaced by Z a standard normal distribution with mean zero and variance equal to one. This will bring these equations to:
   n   n  + - - - -  s y y s xij  j - xio           r rj j ro i  j =1   j =1      =   = and (II.14)  P Z  P Z     i r o o     ( ) ( ) r i            

Deterministic Equivalent: As known from statistics, if Z is the standardized normal distribution, then the
 (n) which will cumulative distribution function of Z will be calculated as P( Z  n) =

bring the constraints in (II.14) to:
 +  n    n  -   yrj  j - yro    sr  si- -   xij  j - xio    j =1   j =1      =   = and (II.15)       i r o o     ( ) ( ) r i            

Using the fact that when  (n) =  , then n =  -1 ( ) , where  (n) represents the cumulative distribution of the standardized normal and n =  -1 (n) represents the inverse of the cumulative distribution of standardized normal. The values of  (n) and respectively  -1 ( ) can be found in standardized tables. Now, we can transform the constraints in (II.15) to:

 n   n  sr+ -   yrj  j - yro  si- -   xij  j - xio   j =1   j =1    =  -1 and   =  -1 ( ) r i o o  r ( )  i ( )
115

(II.16)

Equations (II.16) can be expressed as:
+ sr -  yrj  j - yro =  -1 ro ( ) r j =1
n

n

(II.17)

si- -  xij  j - xio =  -1 io ( )( i )
j =1

Replacing constraints (II.17), (II.12), and (II.13) in (II.9), we can finally obtain the stochastic DEA formulation for the Additive model as:

Maximize

i 1= i 1 =

 si- +  sr+
+ sr -  yrj  j - yro =  -1 ro ( ) r j =1 n

m

s

subject to:

r = 1,..., s,

si-

-  xij  j - xio =  -1 io ( )( i )
j =1

n

i = 1,..., m,

  j = 1, sr+  0,
j =1

n

si-  0, j = 1,..., n, r = 1,..., s, i = 1,..., m,

Where

 ro ( ) =  io ( ) =

k o j o

 rk , y  rj ) + 2o  k Cov( y  rk , y  ro ) + o 2Var ( y  ro ),   k  jCov( y
k o

k o j o

ij , x ik ) + 2o   j Cov( x ij , x io ) + o 2Var ( x io ).   k  jCov( x
j o

116

Table A.1 Comparison of used variables in different credit rating analysis Author (Year) Combination of used Financial Ratios Altman (1993) WCA, REA, EBITA, MVTA, STA Chen (2008) NPT, LA, CG, OCI, TIER, ROA, RS, ART Paradi (2004) CA, REA, EBIT, CF, TA, IN, SE, CL Emel (2003) STBL, CLS, ABS, LR, OETA, NPTA Moody AI, IGS, LA, NIG, NIA, QR, REA, SG, CAS, (2000) DSC Chou (2006) NPT, LA, TIER, ROA, RS, ART, IT Ward (1995) LOPO, LTII, CAI, LTFI, STFI Min (2000) CLOE, FES, TBTA, OETA, LA, ICR Dimitras (1996) WCA, DSCR, LA, EBITA, NIA Beaver (1966) CFTD, NIA, TDTA,WCA, NOCI

Table A.2 Nomenclature of financial ratios Abbreviation NIG NIA QR SG CAS DSC FES CLOE TBTA ICR CFTD TDTA CR NOCI Description Net Income Growth Net Income/Total Assets Quick Ratio Sales Growth Cash/Assets Debt Service Coverage Ratio Financial Expenses/Sales Current Liabilities/Owners' Equity Total Borrowings and Bond Payable/TA Interest Coverage Ratio Cash Flow/Total Debt Total Debt/Total Assets Current Ratio No-Credit Interval

117

Abbreviation WCA REA EBITA MVTA STA LOPO LTII CAI LTFI STFI TA SE CL CF IN STBL CLS ABS LR OETA NPTA NPT LA CA CA CG OCI TIER ROA RS ART IT CLS

Description Working Capital/Total Assets Retained Earnings/Total Assets Earnings before Interest and Tax/Total Assets Market Value of Equity/Total Assets Sales/Total Assets Lower Operating Payment Outflows Long Term Investment Inflows Capital Assets Inflows Long-Term Financing Inflows Short-Term Financing Inflows Total Assets Shareholder Equity Current Liabilities Cash Flow from Operations Interest Expense Short Term Bank Loans/CL Current Liabilities/ Sales 1-(Fixed Assets Owners' Equity) (Current Assets-Inventories)/CL Owners' Equity/Total Assets Net Profits/Total Assets Net Profits/Sales Liabilities/Assets Current Assets Current Assets Cost of Goods Sold Operating Cash Inflow Times Interest Earned Ratio Return on Total Assets Return on Stockholders Accounts Receivable Turnover Inventory Turnover Current Liabilities/Sales

118

REFERENCES

Adler, N. and Golany, B. 2002. Including principal component weights to improve discrimination in data envelopment analysis, Journal of the Operational Research Society, 53(9), 985-991. Adler, N. and Yazhemsky, E. 2010. Improving discrimination in data envelopment analysis: PCA-DEA or variable reduction, European Journal of Operational Research Society, 202(1), 273-284. Aigner, D., Lovell, C. A. K., Schmidt, P. 1977. Formulation and estimation of stochastic frontier production function models. Journal of Econometrics, 6, 21-37. Ali, A. I. and Seiford, L. M. 1990. Translation invariance in data envelopment analysis, Operations Research Letters, 9(6) 403-405. Altman, E. I. 1968. Financial ratios, discriminant analysis and the prediction of corporate bankruptcy, Journal of Finance, 23, 589-609. Andersen, P., Petersen, N. C. 1993. A procedure for ranking efficient units in data envelopment analysis, Management Science, 39, 1261-1264. Ang, J. S., Chua, J. H., McConnell, J. J. 1982. The administrative costs of corporate bankruptcy: A note, The Journal of Finance, 37(1), 219-226.

119

Athanassopoulos, A. D., Shale, E. 1997. Assessing the comparative efficiency of higher education institutions in the UK by the means of Data Envelopment Analysis, Education Economics, 5(2), 117-134. Avkiran, N. K. 2006. Stability and integrity tests in data envelopment analysis, SocioEconomic Planning Sciences, 41(3), 224-234. Banker, R. D., Charnes, A., Cooper, W. W. 1984. Some models for estimating technical and scale inefficiencies in data envelopment analysis, Management Science, 30, 10781092. Banker, R. D., Thrall, R. M. 1992. Estimation of returns to scale using data envelopment analysis, European Journal of Operational Research, 1(9), 74-84. Banker, R. D. 1993. Maximum likelihood, consistency and Data Envelopment Analysis: A statistical foundation, Management Science, 39(10), 1265-1273. Banker, R. D., Bardhan, I., Cooper, W. W. 1996. A note on returns to scale in DEA, European Journal of Operational Research, 88(3), 583-585. Barr, R. S., Seiford, L. M., Siems, T. F. 1994. An envelopment-analysis approach to measuring the managerial efficiency of banks, Annals of Operations Research, 45, 1­19. Bellman, R. 1961. Adaptive control Processes: a guided tour, Princeton University Press. Berger, A. N. 1993. Distribution-free estimates of efficiency in the U.S. banking system and tests of the standard distributional assumptions, Journal of Productivity Analysis 4(3), 261-292.

120

Black, F. and Scholes, M. 1973. The pricing of options and corporate liabilities, Journal of Political Economy, 81, 637-654. Brockett, P. L. and Cooper, W. W., Kumbhakar, S. C. Mccarthy, D. 2004. Alternative statistical regression studies of the effect of joint and service-specific advertising on military recruitment, Journal of Operational research Society, 55, 1039-1048. Cattell, R. B. 1966. The scree test for the number of factors, Journal of Multivariate Behavioral Research 1, 245-276. Charnes, A. and Cooper, W. W. 1963. Deterministic equivalents for optimizing and satisfying under chance constraints. Operations Research, 11(1), 18-39. Charnes, A., Cooper, W. W., Rhodes, E. 1978. Measuring the efficiency of decision making units, European Journal of Operational Research, 2(6), 424-444. Charnes, A., Cooper, W. W., Golany, B., Seiford, L., Stutz, J. 1985. Foundations of data envelopment analysis for Pareto-Koopmans efficient empirical production functions, Journal of Econometrics, 30(1-2), 91-107. Charnes, A., Cooper, W. W., Thrall, R. M. 1991. A structure for classifying and characterizing efficiencies and inefficiencies in data envelopment analysis, Journal of Productivity Analysis 2(3), 197-237. Chen, H. - H. 2008. Stock selection using Data Envelopment Analysis, Industrial Management and Data Systems, 108(9), Cook, W. D. and Bala, K. 2007. Performance measurement and classification data in DEA: Input ­oriented model, Omega, 35(1), 39-52.
121

Cooper, W. W., Park, K. S., Pastor, J. T. 1999. RAM: A range adjusted measure of efficiency. Journal of Productivity Analysis, 11(1), 5­42. Cooper, W. W., Seiford, L. M., Tone, K. 2000. Data Envelopment Analysis. A comprehensive text with models, applications, reference and DEA-solver software. Kluwer Academic Publisher, USA. Cooper, W. W., Park, K. S., Yu, G. 2001a. An illustrative application of IDEA to a Korean mobile telecommunication company, Operations Research, 49(6), 807-820. Cooper, W. W., Deng, H., Huang, Z., Li, S. X. 2001b. Chance constrained programming approaches to congestion in stochastic data envelopment analysis, European Journal of Operational Research, 155(2), 487-501. Cooper, W. W., Seiford, L. M., Zhu, J. 2004. Data envelopment analysis history, models and interpretations, International Series in Operations Research and Management Science, 71, 1-39. Cooper, W. W., Seiford, L. M., Tone, K. 2007. Data Envelopment Analysis. A comprehensive text with models, applications, references and DEA-Solver software, Springer Science +Business Media, LLC, Second edition. Deprins, D., Simar, L., Tulkens, L. 1984. Measuring labor efficiency in post offices. public goods, environmental externalities and fiscal competition, Springer US, Part III. Divine, J. D. 1986. Efficiency analysis and management of not for profit and governmentally regulated organizations, Ph.D. dissertation, Graduate School of Business, University of Texas, Austinm, USA.
122

Edirisinghe, N. C. P. and Zhang, X. 2007. Portfolio selection under DEA-based relative financial strength indicators: case of US industries, Journal of the Operational Research Society, 59(6), 842-856. Emel, A. B., Oral, M., Reisman, A., Yolalan, R. 2003. A credit scoring approach for the commercial banking sector, Socio-Economic Planning Sciences, 37(2), 103­123. Farrell, M. J. 1957. The measurement of productive efficiency, Journal of the Royal Statistical Society, Series A, 120(3), 253-290. Ferri, G., Liu, L. - G., Stiglitz, J. E. 1997. The procyclical role of rating agencies: evidence from the east asian crisis, Economic Notes, 28(3), 335-355. Friedman, L., Sinuany-Stern, Z. 1996. Scaling units via the canonical correlation analysis in the DEA context, European Journal of Operational Research, 100(3), 629-637. Goss, A. 2009. Corporate social responsibility and financial distress, Ted Rogers School of Management, Finance Department, Ryerson University, Working Paper. Grosskopf, S. 1996. Statistical inference and nonparametric efficiency: A selective survey. Journal of Productivity Analysis 7(2-3), 161­176. Gupta, M. C. 1969. The effect of size, growth, and industry on the financial structure of manufacturing companies, The Journal of Finance, 24(3), 517-529. Hammond, J. C. 2002. Efficiency in the provision of public services: a data envelopment analysis of UK public library systems, Applied Economics, 34(5), 649-657.

123

Hull, J. and White, A. 1990. Valuing derivative securities using the explicit finite difference method, Journal of Financial and Quantitative Analysis, 25, 87-100. Jackson, D. A. 1993. Stopping rules in principal component analysis: A comparison of heuristical and statistical approaches, Ecological Society of America, 74(8), 2204-2214. Jacobs, R. 2001. Alternative methods to examine hospital efficiency: data envelopment analysis and stochastic frontier analysis, Health care Management Science, 4(2), 103115. Jenkins, L. and Anderson, M. 2003. A multivariate statistical approach to reducing the number of variables in data envelopment analysis, European Journal of Operational Research, 147(1), 51-61. Kaplan, S. 1989. The effects of management buyouts on operating performance and value, Journal of Financial Economics, 24(2), 217-254. Kim, J. O. and Mueller, C. W. 1978. Factor analysis: Statistical methods and practical issues, Sage Publications, Inc. Kittelsen, S. A. C. 1999. Monte Carlo simulations of DEA efficiency measures and hypothesis tests, Thesis for the degree of Dr. Polit. at the Department of Economics, University of Oslo, also presented at the Georgia Productivity Workshop, October 1994. Lertworasirikul, S., Fang, S.-C., Joines, J. A., Nuttle, H. L. W. 2003. Fuzzy data envelopment analysis (DEA): a possibility approach, Fuzzy sets and Systems, 139(2), 379-394.

124

Mahadevan, R. 2002. A DEA approach to understanding the productivity growth of Malaysia's manufacturing industries, Asia Pacific Journal of Management, 19(4), 587600. Markowitz, H. M. 1959. Portfolio selection: Efficient diversification of investments, Wiley, New York. Mergent. http://www.mergentonline.com/basicsearch.php; accessed is September, 2009. Morita, H., Hirokawa, K., Zhu, J. 2005. A slack-based measure of efficiency in contextdependent data envelopment analysis, Omega, 33(4), 357-362. Ohlson, J. A. 1980. Financial ratios and the probabilistic prediction of bankruptcy, Journal of Accounting Research, 18, 109-121. Pagano, M., and Volpin, P. 2010. Credit ratings failures and policy options, Economic Policy, 25(62), 401-431. Paradi, J. C., Asmild, M., Simak, P. C. 2004. Using DEA and worst practice DEA in credit risk evaluation, Journal of Productivity Analysis, 21 (2), 153­165. Pastor, J. M. 2002. Credit risk and efficiency in the European banking system: A threestage analysis, Applied Financial Economics, 12(12), 895-911. Pastor, J. T. 1994. New DEA additive models for handling zero and negative data, Working Paper, Depto. Est. E Inv. Oper., Universidad de Alicante, Spain.

125

Pille, P., Paradi, J. C. 2001. Financial performance of Ontario (Canada) credit unions: An application of DEA in the regulatory environment, European Journal of Operational Research, 139(2), 339-350. Portela, M. C. A. S., Thanassoulis, E., Simpson, G. 2004. Negative data in DEA: A directional distance approach applied to bank branches. Journal of the Operational Research Society 55(10), 1111­1121. Premachandra, I. M., Bhabra, G. S., Sueyoshi, T. 2009. DEA as a tool for bankruptcy assessment: A comparative study with logistic regression technique, European Journal of Operational Research, 193, 412-424. Reig-Martìnez, E., Picazo-Tadeo, A. J. 2004. Analysing farming systems with Data Envelopment Analysis: citrus farming in Spain, Agricultural Systems, 82(1), 17-30. Schmidt, P., Sickles, R. C. 1984. Production frontiers and panel data, Journal of Business and Economic Statistics 2(4), 367-374. Sengupta, J. K. 1987. Data envelopment analysis for efficiency measurement in the stochastic case, Computers and Operations Research, 14(2), 117-129. Sengupta, J. K. 1990. Transformations in stochastic DEA models, Journal of Econometrics, 46(1-2), 109-123. Sengupta, J. K 1998. Stochastic data envelopment analysis: A new approach, Applied Economics Letters, 5(5), 287-290.

126

Sexton, T. R., Silkman, R. H., Hogan, A. J. 1986. Data envelopment analysis: Critique and Extensions in measuring efficiency: An assessment of Data Envelopment Analysis, New Directions of Program Evaluation, 32, 73­104. Sinuany-Stern, Z., Friedman, L. 1995. DEA and the discriminant analysis of ratios for ranking units, European Journal of Operational Research, 111(3), 470-478. Smith, P. 1990. Data Envelopment Analysis applied to financial statements, Omega International Journal of Management Science, 18(2), 131-138. Smith, P., Mayston, D. 1987. Measuring efficiency in the public sector, Omega, 15(3), 181-189. Somerville, R. A., Taffler, R. J. 1995. Banker judgment versus formal forecasting models: The case of country risk assessment. Journal of Banking and Finance, 19(2), 281-297. Stevens, J. P. 1986. Applied multivariate statistics for the social sciences, 4th Edition, Hillsdale, NJ: Lawrence Erlbaum Associates. Sueyoshi, T. 1999. Extended DEA-Discriminant Analysis, European Journal of Operational Research, 131(2), 324-351. Thompson, R. G., Dharmapala, P. S., Thrall, R. M. 1995. Linked-cone DEA profit ratios and technical efficiency with applications to Illinois coal mines, International Journal of Productivity Economics 39, 99-115. Thrall, R. M. 1996. Duality classification and slacks in DEA, Annals of Operations Research, 66(2), 109-138.
127

Tone, K. 2001. A slacks-based measure of efficiency in data envelopment analysis, European Journal of Operational Research, 130(3), 498-509. Tongzon, J. 2001. Efficiency measurement of selected Australian and other international ports using data envelopment analysis, Transportation Research, 35(2), 107-122. Ueda, T., Hoshiai, Y. 1997. Application of principal component analysis for parsimonious summarization of DEA inputs and/or outputs, Journal of Operational Research Society, 40, 466-478. Vassiloglou, M., Giokas, D. 1990. A study of the relative efficiency of bank branches: An application of Data Envelopment Analysis, The Journal of the Operational Research Society, 41(7), 591-597. Wahab, M. I. M., Wu, D., Lee, C.-G. 2008. A generic approach to measuring the machine flexibility of manufacturing systems, European Journal of Operational Research, 186(1), 137-149. Wu, D., Yang, Z., Liang, L. 2006. Using DEA-neural network approach to evaluate branch efficiency of a large Canadian bank, Expert Systems with Applications, 31(1), 108-115. Zhang, Y. and Bartels, R. 1998. The effect of sample size on the mean efficiency in DEA with an application to electricity distribution in Australia, Sweden and New Zealand, Journal of Productivity Analysis, 9(3), 187-204.

128

Zhou, G., Min, H., Xu, C., Cao, Z. 2008. Evaluating the comparative efficiency of Chinese third-party efficiency providers using data envelopment analysis, International journal of physical distribution and logistics management, 38(4), 262-279.

129

