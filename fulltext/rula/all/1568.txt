Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2009

Calibration Techniques For Low-Cost Star Trackers
Tomas Dzamba
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Aerospace Engineering Commons Recommended Citation
Dzamba, Tomas, "Calibration Techniques For Low-Cost Star Trackers" (2009). Theses and dissertations. Paper 1128.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

CALIBRATION TECHNIQUES FOR LOW-COST STAR TRACKERS

by

Tomas Dzamba
B.Eng., Aerospace Engineering Ryerson University, 2007

A thesis presented to Ryerson Gniversity in partial fulfillment of the requirements for the degree of

Master of Applied Science
in the program of

Aerospace Engineering

Toronto, Ontario, Canada. © Tomas Dzamba 2009

PROPERTY OF RYERSON UNIVERSITY liBRARY

111

AUTHOR'S DECLARATION

I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means , in total or in part, at the request of other intitutions or individuals for the purpose of scholarly reserach.

v

ABSTRACT
CALIBRATION TECHNIQUES FOR LOW-COST STAR TRACKERS

Tom Dzamba, Master of Applied Science, Aerospace Engineering Ryerson University, Toronto~ August 2009

This study presents a series of cost-effective strategies for calibrating star trackers for microsatellite missions. VIe examine three such strategies that focus on the calibration of the image
detector~

geometric calibration of the lab setup used for

ground testing, and an optical calibration due to lens aberrations. Procedures are developed for each of these strategies that emphasize speed of implementation and accuracy, while trying to minimize manual labour. For the detector calibration, an existing calibration technique was adapted and implemented to reduce fixed pattern noise and dark current. Preliminary results show reduced variations in pixel sensitivity by approximately 21%, averaged across each pixel color given the use of a color imager. Although not substantial, this reduction in pixel variation will help preserve the Gaussian illumination pattern of imaged stars, aiding in correct centroid location. Results pertaining to the lab calibration show accurate star placement, in angular terms to 0.0073° across most of the field of view. This provides an accurate low-cost , variable solution for characterizing sensor performance; specifically pattern matching techniques. Finally, we present some initial results for lens aberration characterization. Using a Gaussian model of the star image shape gives trends consistent with astigmatism and field curvature aberrations. Together, these calibrations represent tools that aim to improve both development and manufacture of modern microsatellite star trackers.

Vll

ACKNOWLEDGEMENTS

I would first like to thank my supervisor Dr. John Enright for the opportunity and all the support he has provided throughout the last two years. The enthusiasm, guidance , and overall knowledge of the field that he demonstrates everyday, make long hours in the windowless basement of Eric Pallin Hall not just bearable but fun and exiciting. I cannot thank you enough for all your support and I look forward to my future PhD studies under your guidance. Second , I would like to thank all the professors at Ryerson who have helped me, in one form or another, throughout the years. The knowledge, personal experience and enthusiasm that you bring to the classroom, the lab, or simply the walk to nearest Starbucks, make Ryerson an amazing place to be and for this I thank you. I guess I also have to thank all of my colleagues that reside in the dark basement corridors of the EPH building at Ryerson. From PhD candidates, to potential Masters graduates, all the way to nameless undergrads who have yet to earn their place, you make each day in the lab more exiciting than the last. Whether I'm wrestling with MATLAB, writing my grammaticaly challenged publications, or getting lunch, you 've all been their to offer your support in one way or another. You guys make long days and nights in the lab bearable, and for this I thank you . This of course does not apply to anyone associated with that prank involving the alarm clock. You know who you are and my revenge is coming. Last but not least I have to thank my family: Lubomir, Lubica, Kubo, Misho, Helena, Julia and Athos Dzamba and my girl friend Ashley Daly. \iVithout all of your support , I know that I would not have been able to make it this far. Even though it may be a little technical for everyday reading, this thesis is for you.

ix

Contents
Author's Declaration Abstract . . . . . . Acknowledgements List of Tables . List of Figures . Acronyms C sed Nomenclature . iii v vii xi
Xlll XV XVll

1 Introduction 1.1 Fundamentals of a Star Tracker 1.1.1 Modes of operation . . . 1.1.2 Star imaging and centroiding 1.2 Our Prototype Star Tracker 1.3 Objectives 1. 4 Outline . . . . 2 Lab Calibration 2.1 Introduction . . . . . . . . . . . . . . . . . 2.1.1 Star Simulators . . . . . . . . . . . 2.1.2 Challenges of Accurate Projection . 2.1.3 Star Simulator . . . . . . 2.2 Model Overview . . . . . . . . . . 2.2.1 Forward Model Derivation 2.2.2 Inverse Model Derivation . 2.3 Calibration Implementation . . . 2.3.1 Calibration Points & Star Representation . 2.3.2 Test Patterns . . . . . 2.3.3 Imaging & Centroiding 2.3.4 Optimization . . . . . 2.4 Results . . . . . . . . . . . . . 2.4.1 Accuracy of calibration . 2.4.2 Mock Star Pattern Tests 2.5 Conclusion . . . . . . . . . . . .

1

2
3 3 4 6 7

9 9 10 13 16 17

19
24 25

26 26
27

29 29
30 31 33

l
X

3 Detector Calibration
3.1 Introduction . . . . 3.1.1 Basic detector operation . . . . . . 3.1.2 Fundamental detector noise sources ~oise calibration formulation . . . . 3.2.1 Correction Model . . . . . . 3.2.2 Dark Current Noise (DCN) 3.2.3 Fixed Pattern Noise Implementation . . . . . . . . . . . Results . . . . . . . . . . . . . . . . 3.4.1 Validation of illumination uniformity 3.4.2 Effectiveness of Fixed Pattern Noise (FPN) correction . Conclusion . . . . . . . . . . . 3.5.1 In1plementation K otes

35
35 36

37
41 41 42 43 45 46 47 48 49 51

3.2

3.3 3.4

3.5

4

Optical Model Calibration
4.1 Background . . . . . . . 4.1.1 Ray Optics . . . 4.1.2 Optical aberrations Proposed lens calibration . 4.2.1 Lens calibration lab setup 4.2.2 Image comparison with analytical predictions Point-Spread Function (PSF) Parameterization Survey Implementation Survey Results Summary . . .

53
54 54 55 56 57 57

4.2

4.3 4.4 4.5 4.6

60
62 63

66
67 67 68

5 Conclusions and Future Work
5.1 5.2 5. 3 5.4 Lab calibration summary . Detector Calibration Lens Calibration . . Concluding Remarks

69 70
71

Bibliography

Xl

List of Tables
2.1 2.2 2.3 2.4 3.1 Summary of techniques for validating [Ruffino & Moccia 2002] . . Investigated Grid Densities . Resultant accuracies of each test pattern Mock Star Pattern Accuracy . . . . . . . star trackers 12 27 30 32

Corrected vs. Uncorrected variation in windowed pixel response (in detector counts) . . . . . . . . . . Aberration effects on point-source PSF

48 56

4.1

xiii

List of Figures
1.1 1.2 1.3 2.1 2.2 2.3 2.4 2.5 2.6 2. 7 2.8 2. 9 3.1 3.2 3.3 3.4 3.5 4.1 4.2 4.3 4.4 4.5 4.6 4.8 4. 7 Sketch of an autonomous star tracker, [Liebe 2002]. Star Tracker focussing . . . . . Prototype star tracker camera .. . Horizontal and Vertical keystone. Projector vs. Sensor star patterns. Examples of radial distortion. Lab calibration setup. Angular coordinate definition Forward Model roadmap. . . . Residual star placement error (300-point pattern). Arc length error . . . Dihedral angle error. . . . . . . . . . . . Noise injection model for pixel detectors. Bayer-pattern on image detector. . . . . Validation of spatial illumination consistancy. Corrected vs. Uncorrected image smoothness characterization. Change in window variation due to FPN correction. . . . . . . Gaussian Imaging by a convex spherical refractive surface, [Mahajan 1998] . . . . . . . . . . . . . . . . . . . . . Aberration survey lab setup. . . . . . . . . . . . . . . Aberration Characterization road map of operations. Light source boresight misalignment. Imager misalignment sign convention . . . a x variation across sampled sensor FOV. PxY variation across sampled sensor FOV. ay variation across sampled sensor FOV.

2
4 5

14 15 16 18 19 20 31 32 33 38 45 47 49 50

55 58 59 60 61 64 64 65

XV

Acronyms Used
ACS COTS
LIS

Attitude Control System Commercial-Off-The-Shelf Lost-In-Space Field Of View Dark Current Noise Fixed Pattern Noise Liquid Crystal Display Root Mean Squared Bivariate Normal Charged-Coupled Devices Complimentary-Metal-Oxide-Semiconductors Electron Hole Pair Metal-Oxide-Semiconductor Analog to Digital Convertor Green-Red-Blue-Green Point-Spread Function

FOV

DCN
FPN

LCD RMS
BVN

ceo
CMOS
EHP

MOS ADC

GRBG
PSF

xvii

Nomenclature
Bold italics denote vector quantities. Math Bold denote matrices.
Lab Calibration: Cartesian coordinates of calibration points/ stars. s p, B Spherical coordinates of calibration points/ stars. Rotation angles of projection plane with respect to projector. </Jpz , Bpz Distance of projection plane (along projector boresight). Dpt Rotation angles of sensor with respect to projector. </J s, Bs, 7/Js Sp Position of sensor: in projector frame. Projector and sensor focal ratios. Fp , Fs Distortion parameter. K OA Intersection of optical axis with image detector. Standard deviation of circular star image. Subscripts: P, S p, s , pl

Projector and Sensor frame. Corresponding to: projector, sensor, and projection plane.

Detector Calibration: (a: b) Denote row and column index of an image. DDc Dark reference image. K Estimate of per-pixel gain variation due to FPN. ~umber of dark images averaged to form Dvc· ");umber of different sensor configurations used. ~umber of exposures averaged at each configuration. Dark Current Noise (DC~) corrected image. Spatially averaged form of e. \Vindow size. Standard deviation of pixel illumination within a local window. Optical :Model Calibration: , fJy Size of Bivariate Normal (BVN) distribution along principal axes. pX Y Correlation coefficient of BVN.

(j x

1

CHAPTER

1

Introduction

0

ver the past decade, microsatellites have become a popular alternative to conventional larger satellites for various near Earth and deep space

applications. Given their small size and low-cost, microsatellites can be constructed and deployed for a fraction of the cost of previous larger satellites. This not only allows for innovative microsatellite designs, but also promotes the development of multiple satellites systems. Many of these new designs demand high-performance Attitude Control Systems (ACSs) that are not affordable under microsatellite cost budgets. As a result, current microsatellites are generally limited to moderate performance sensor suites. Common attitude sensors include: sensors, magnetometers, and for [Larson & \Vertz 2005]. sun-sensors, short-term star trackers, horizon

measurements,

gyroscopes

Among these, star trackers have the potential to

be an enabling technology for high-value microsatellite missions. Since they provide 3-axis attitude solutions frorn a single instrument, even a modest accuracy star tracker can potentially simplify sensor fusion and integration problems. The main disadvantage of star trackers is that they remain expensive when compared to other sensors and as a result, rarely find their way onto microsatellites. In this light, one can conclude that a technical priority of star tracker development for the microsatellite market is the reduction of manufacturing costs rather than a pursuit of high precision. Elements of this cost mitigation can arise from star tracker calibration strategies applicable to the development and manufacture of modern microsatellite star trackers. A common trend in reducing the cost of high performance sensors is the use of Commercial-Off-The-Shelf (COTS) components in microsatellite sensor

2

Chapter 1. Introduction

design. These components provide reduced performance for a fraction of the cost of existing specialized components. Following the manufacturing process, various calibration techniques can then be utilized to calibrate a sensor made from COTS components as a single unit to restore overall sensor performance. This thesis describes some calibration techniques for low-cost microsatellite star trackers, usefull at various stages of instrument development.

1.1

Fundamentals of a Star Tracker

Star trackers compute the attitude of a spacecraft through a measurement of relative angular star positions. Generally, these sensors consist of three main components; a lens system, an imaging array, and a microprocessor used to analyze the image and perform subsequent computation, see Fig. 1.1. Imaged star patterns are first compared against an onboard star catalogue to determine their identity and relative angular position with respect to the sensor. The \i\Thaba problem is then solved using one of many commonly implemented algorithms to . yield a 3-axis attitude solution.

Lens

Focal Plane

Real Sky

** ** * ** ** *
low Bandw;dth
Interface

J

Figure 1.1: Sketch of an autonomous star tracker, [Liebe 2002].

1.1. Fundamentals of a Star Tracker

3

1.1.1

Modes of operation

Star trackers typically operate in two modes: initial attitude acquisition followed by attitude tracking. If no previous attitude knowledge is known, the initial acquisition mode will attempt to match an imaged star pattern by searching through the onboard catalogue. This is commonly known as the Lost-In-Space (LIS) problem and can typically be accomplished in a few seconds [Liebe 2002].
If a previous attitude update is known, the tracking mode typically assumes that

the present attitude is close to the last known attitude (less than 1 s ago), and only previously identified stars are tracked at known positions.

1.1.2

Star imaging and centroiding

In order to perform any attitude computation using the relative angular positions of stars, they first have to be found within an image. A common technique for this is to set a background image noise threshold and assume that any remaining illumination patterns are stars. Once a star has been detected, it 's relative angular position is computed by finding the brightest point of its illumination pattern or Point-Spread Function (PSF). Although many different approaches exist for finding the point of peak illumination, this study employs a first-moment centroid technique. The PSF of a star is isolated from the image by drawing a
TJ x TJ window around a detected star. The centroid coordinates with respect to

this window can then be computed as:
2:~=12:~=1 [x!( x, y)]
"\:'1 1 "\:''I u x =1 uy=1

Xc

=

J(

X,

Y

)

(1.1)

(1.2) where I( x, y) is the illumination value at row x and column y of the PSF, and
TJ is the size of the window around the PSF.

The pixels of an image detector measure an illumination value per unit

4

Chapter 1. Introduction

area. Any image produced by a detector represents a discrete representation of the incident illumination. Given an arbitrary image of a star field , the apparent size of a star is usually on the order of a single pixel. This makes the task of accurately determining the star centroid difficult, as the entire illumination pattern resides within one pixel. A common technique used to mitigate this effect is to defocus the incident star light using the sensor optics. This distributes the illumination pattern of a star over a larger region of pixels, effectively increasing the sampling of the illumination pattern, see Fig. 1.2.
Defocused Imaging 1 Tightly Focused Imaging 1

2

2

* *

* *

Focal Point

I I

··
Star #1

···
···

Pixel response Star #1 Pixel response Star #2

Pixel response

· ·

!

!

II !

··

··

Pixel response
Star #2

!

II!

···

~? Signal ambiguity with tight focus

Figure 1.2: Star Tracker focussing

1.2

Our Prototype Star Thacker

This thesis represents a companion study to an ongoing effort to develop a very small star tracker using high-quality COTS parts. Component selection for this star tracker design establishes baseline parameters relevant to the cali-

1.2. Our Prototype Star Tracker

5

Figure 1.3: Prototype star tracker camera.

bration processes, but the discussed calibration techniques should be applicable to other designs. Our chosen detector is a five-megapixel (2592 x 1944 pixel) , 7.13mm( diag.) CMOS sensor, see Fig. 1.3. To minimize mass and volume , the optics are very compact; the current design employs a 6mm (diam.) , F/2 .0 lens. The resulting Field Of View (FOV) from this design is approximately 26° x 20°. This FOV is comparable to many commerically available wide-angle star trackers , but the native resolution is higher than the current norm. This sensor is designed to track stars of visual magnitude 4.0 or brighter. In contrast with many modern star tracker designs, this sensor uses a colour (Bayer-pattern) detector.

6

Chapter 1. Introduction

1.3

Objectives

The main goal of this thesis is to explore practical calibration techniques applicable to the development and manufacture of modern microsatellite star trackers. In rough correspondence to the components of a star tracker, this thesis proposes three types of calibrations useful at various stages of instrument development: · Optical Calibration. Aberrations introduced by the star tracker's optical components shift and distort the star images on the detector. Identifying centroid locations and calculating corresponding direction vectors rely on effective modeling and correction of these aberrations. · Detector Calibration. In order to improve the star detection likelihood, accuracy dominant noise sources arising from the detector array are examined and corrected for. · Laboratory Calibration. Pattern matching algorithms are a vital software component of any star tracker and laboratory testing is an essential part of software validation. A self-calibrating projector system is proposed for laboratory tests that corrects for nonidealities in the physical setup and enables flexible hardware-in-the-loop testing. The utility of our calibration techniques stems from their ability to assist development , test and manufacture of microsatellite star tackers. As promising as these techniques may be, we must be pragmatic about assessing the costs involved in adopting these schemes. The factors most likely to increase unit costs are a) labour or time-intensive practices, b) online storage requirements (e.g. , flash memory) , and c) online computational requirements (e.g. , processing power). Offline processing and storage are plentiful and cheap - these have minimal impact on system cost. For each of the proposed calibrations an estimate of their cost impact is weighed against added performance benefits to justify the practicality of implementation.

1.4.

Outline

7

1.4

Outline

The content of this thesis is separated into three independent sections; lab calibration, detector calibration, and optical calibration. Due to the dissimilar nature of these topics, they are each discussed as a whole; (i.e. problem formulation, literature review, approach, implementation, and results), entirely within their respective chapters. Chapter 2 describes a self-calibrating projector system that compensates for nonidealities in the physical setup of the lab. Following this calibration, the lab setup will provide a means for sensor performance characterization, specifically in terms of pattern matching abilities. Chapter 3 discusses the calibration of a silicon based image detector; specifically targeting DCN and FPN. A calibration of the lens system used within the prototype sensor is discussed in Chapter 4. This calibration addresses the effects of the five classical optical aberrations on star centroid accuracy. Finally, Chapter 5 provides some conclusions and suggested directions of future research.

9

CHAPTER

2

Lab Calibration

T

his study presents a star simulator used for ground testing microsatellite star trackers. Common setups for this application involve labourous measure-

ment and positioning of components making the lab environment inflexible and the task of testing numerous sensors lengthy. The proposed method minimizes this need of manual setup through the use of a self-calibrating project-screensensor system. A general analytical model and a projected calibration pattern are utilized to determine a set of parameters that define all nonidealities in the lab setup. These parameters are then used to project deformed images that, when imaged by the star tracker, appear as desired.

2.1

Introduction

Ground testing spacecraft sensors is essential. Performance and function can be characterized and thoroughly validated within the lab environment before deployment. A large part of ground testing is associated with simulating the correct sensor stimulus. Depending of the type of sensor, this may be optical, magnetic, or inertial. In either case, great effort is usually spent in the calibration of this stimulus to ensure a high level of realism. Star trackers take images of stars in visible wavelenghts, and use these images to determine attitude. Laboratory simulation of these starfields commonly includes elaborate lab setups, resulting in various degrees of simulation accuracy. This study describes a low-cost approach to lab calibration for star tracker ground testing.

10
~1any

Chapter 2. Lab Calibration
forms of star simulation for star tracker ground testing exist. Some

are implemented physically by providing some form of optical stimulus to the star tracker. Others are implemented digitally, by creating artificial images that are then processed by the sensor software. This approach allows for testing of the sensor's star identification capability, on-board catalog, and resultant attitude computations, [Ruffino & Moccia 2002], (\iVessling & Does 1994). Multiple different lab setups exist for each type of simulation, each representing various combinations of test capability, resultant performance and ease of implementation. A brief review of these commonly used test setups is pre\iVe sented, for a more in depth review please see [Ruffino & Moccia 2002].

also present an overview of general techniques used to improve projector accuracy. Our chosen architecture consists of a conventional Liquid Crystal Display (LCD) projector and the prototype star tracker. The goal of this simulator is two-fold: i) provide realistic starfield images that can be imaged and processed by the star tracker, and ii) limit the amount of necessary lab setup. This was achieved by developing a relation between the coordinates of the star tracker and corresponding coordinates on the projector through a lab model. Projector and sensor offsets make this challenging as they introduce geometric disturbances to desired star placement, not only on the projection surface but also on the detector of the sensor. In addition, zoom and distortion effects from both In the the projector and sensor optics provide further image deformation.

model, these effects are described by a set of geometric and optical parameters. An automated calibration routine is used to compute optimal values of these parameters, resulting in a quick, flexible and accurate form of star simulation.

2.1.1

Star Simulators

The most simplistic form of ground testing is numerical scene simulation. This involves generating artificial images of starfields which are then processed to compute attitude, either on-board the sensor or with a separate computer. Although this method enables multiple aspects of the star tracker software to be

2.1. Introduction

11

tested , only a fraction of the sensor hardware; if any, is actually used. Critical errors associated with the image acquisition hardware and respective software interface can remain overlooked and potentially result in reduced performance. The next level of star tracker ground testing addresses the concern of complete hardware testing by creating an optical stimulus for the star tracker using pinholes, or high resolution displays. Light from these sources is commonly collimated to better represent stars at an infinite distance. Compared to numerical scene simulation, these techniques do provide a more complete form of component testing, however they suffer in terms of limited performance. Pinhole simulators can typically only test a single star scene at time, as a result, limiting the flexibility of dynamic simulation. Scene simulators overcome this hurdle by using high-resolution displays capable of representing multiple stars at once. Unlike pinhole star simulators, this technique does also allow for dynamic testing by manipulating the image on the display, however it is limited in terms of angular accuracy by the pixel size and separation, [Wessling & Does 1994]. Lastly, real sky tests represent the highest level of image realism, but do have drawbacks associated with proper setup. Atmospheric refraction distorts the apparent position and brightness of stars. High altitudes or elevation angles minimize these effects, but a lot of effort is necessary to produce good truth comparisons, [Bank 1997]. To keep the sensor inertially pointing, this type of testing is commonly done in an astronomical observatory to utilize the telescope tracking system. This can introduce further mechanisms of error in the form of vibrations from the the movement of the tracking system and also limits the starfields that can be imaged. Table 2.1 summarizes the test capability, resultant performance, and required setup effort of each of these techniques. The solution proposed within this study represents an approach similar to scene simulator, but utilizing a digital projector and projection screen instead of a high-resolution display. The setup does allow for both static and dynamic tests, but as is common to scene simulators, is limited in resultant accuracy. The use

·.-4 ~

0

~

ro
~

.D. ·.-4

ro 0

Table 2.1: Summary of techniques for validating star trackers [Ruffino & Moccia 2002]
Technique Numerical scene simulation (Synthetic detector pixels) References [Wessling & Does 1994] Test Capability · Robustness of star identification algorithms · Initial attitude acquisition · Onboard catalog adequacy · Static and Dynamic te~t~ · Angular measurement accuracy and stability (only static tests) · Dynamic tests (gimbaled sensor mount required) · Full FOV, multiple star tests · Static and dynamic tests Performance Notes · No end-to-end hardware testing (imaging ~ubsy~tems not tested) · Time consuming processing Accmacy

.D.
~

ro

N
Q)
~

~

..d

ro

~

0

Pinhole star

simulator~

tThomas et al. 1996] Gullapalli et al. 1993) Bank 1997]

· 1 star ~cene · Limited dynamic simulations

· 1-arc~ec (order of magnitude)

Scene ~imulators (PC-controlled high resolution displays)

lGullapalli et al. 1993J Wessling & Doe~ 1994j

· Inaccmate angular star size Discrete ~tar positions in ~cene

·

· 50-100 arc~ec (order of magnitude) · 50-arcsec separation (order of magnitude) · 1-arcsec (order of magnitude)

Real sky tests

[Jorgensen & Liebe 1996] lAlexander & Chang 1996J Bank 1997)

· Robustness of star identification algorithm~ · Initial attitude acquisition · Onboard catalog adequacy · Angular measurement accuracy and stability (static test~ only)

· Affected by external pertmbations · Limited availability of star field scene~ · Limited dynamic simulations

--

-

--

-- -

N r-1

2.1. Introduction

13

of a projector allows for a larger useable scene area and limits the size of setup components, but it also introduces some challenges associated with projectorscreen-sensor alignment. A calibration of the lab setup is required to ensure that the images perceived by the sensor are as realistic as possible.

2.1.2

Challenges of Accurate Projection

Projectors are generally designed to produce a rectangular image on a planar surface that is aligned perpendicular to the projector boresight. Gsing the focal length of the optics and the distance to the projection screen, the shape and size of the projected image is easily computed. If the projector is not aligned orthogonally to the projection surface, the incident image will be trapezoidal instead of rectangular. The discrepancy between the ideal rectangular image and the incident trapezoid is referred to as the image keystone, a general problem associated with projection. The keystone effect can be broken down into horizontal and vertical components. Horizontal keystoning occurs when the projector is misaligned with the projection surface normal along the horizontal plane, see Figure 2.1. This forms a trapezoidal image, with the parallel edges aligned vertically. The further the horizontal projector misalignment, the greater the difference in lengths of these parallel sides. Vertical keystoning can then be described in the same fashion, only involving a vertical misalignment of the projector boresight instead of a horizontal one. This forms a trapezoidal image with the parallel sides aligned horizontally and will further deform as the misalignment with the surface normal grows, see Figure 2.1. The keystone discussed thus far refers to the shape of the projected image on the planar surface. The utilized setup then calls for observing this image with the prototype star tracker. Since the projector and camera are of finite size, some physical separation is unavoidable. This introduces misalignment effects between the optical axes of the projector and sensor that introduces additional keystone in the sensor image. This secondary source of keystoning is depicted in Figure 2.2, where it is illustrated that the projection of a off-axis position, (3, will be

14

Chapter 2. Lab Calibration
Legend : - - - - Ideal

- - - Realistic

~X
z

Figure 2.1: Horizontal and Vertical keystone.

perceived as a different off-axis position, a, in the sensor frame. These two forms of image keystoning represent the geometric nonidealities that must be addressed with the proposed calibration. Additional discrepancies between the projected and observed images can arise from nonidealities in the projector and sensor optics. The zoom feature present in most available projectors changes the focal length of the system resulting in a variable magnification of the image. Furthermore, any discrepancies between the ideal focal length of a system (specified by design) and the true focal length (result of optics manufacture) can also cause image magnification. Lens distortion is another nonideality of an optical system, but compared to zoom effects , impacts the shape of the image in a more complicated manner. Although many forms of lens distortion exist, this study limits the consideration of this optical aberration to radial distortion only, which can be described as an image deformation relative to the boresight distance. If a lens is not rectilinear, imaged lines will appear as curves. Two types of image deformation can occur as a result of radial distortion; barrel distortion and pincushion distortion, see Figure 2.3.

2.1. Introduction

15

PROJECTION SCREEN
I I I

I I
/ /

/

/

/
I~

/

I
I
/

~ /

/~

I

/

I I
I /
/

/ / a- DESI.RED
SENSOR COORDI NATES
~ - NECESSA RY

'l

qROJECTOR

PROJECTOR COORDINATES

Figure 2.2: Projector vs. Sensor star patterns.

In conclusion , the required calibration of the lab setup must not only compensate for geometric nonidealities that result in image keystoning but also optical nonidealities that can deform the projected and observed images. Common keystone correction algorithms aim to improve the shape of a projected image on an imperfectly aligned surface. This is generally achieved in one of two ways: manually or through camera feedback. Low-cost conventional projectors use manual keystone adjustment to change the appearance of the projected image with respect to a human observer. The provided input varies the shape of the image that is generated by the projector. The general aim is to restore the original rectangular shape of the projected image. The accuracy 1 of this technique is limited due to two main factors. Firstly, the keystone settings on a projector are limited both in range and resolution and secondly, a human observer can only resolve the shape of an image to a limited degree.
1

More

Defined as the discrepancy between the resultant image and the desired rectangular image.

16

Chapter 2. Lab Calibration

,,.,.""''
I
I I I I I I I

,. ......

------ ..........

------------· -------------:
............ "'\
\ I I

\
\ \

\ \ \ I

I I \ \ \

I I I I
\

\

\ \ I

I

I

I

I I I I

-------------------Barrel Distortion

L------------ ------------J
Pincushion Distortion

Figure 2.3: Examples of radial distortion. expensive, higher-end projectors use a camera to provide visual feedback of the projected image shape, [Sukthankar et al. 2001]. Similar to the manual technique, the shape of the incident image is then used to vary internal projector parameters until the desired shape is reached, only in this case the process is automated. orientation. The camera is mounted to the projector and aligned with boresight to minimize any keystone effects associated with relative position and

2.1.3

Star Simulator

The proposed projector-based star simulator can be split into two main components: an analytical lab model described by a set of optical and geometric parameters and a calibration routine used to find the values of these parameters. The lab model relates the image perceived by the camera to that generated by the projector and was analytically developed using basic planar and spherical geometry. The model is based on a number of optical and geometric parameters associated with the position and orientation of the projector-screen-sensor system that must be found for each specific lab setup. A automated calibration routine was developed to simplify this procedure. The calibration routine is based on the previous discussion of projector-camera feedback systems. However,

2.2.

Model Overview

17

as previously discussed, the image is optimized for the camera view of the screen, not the projector view. The calibration procedure employs the use of a test pattern that is first projected and then imaged/ centroided by the prototype star tracker. vVe deterrnine the values of the model parameters through an error minimization between the imaged pattern and the model generated pattern. We assumed no prior knowledge of the position or orientation of either the projector or camera (prototype star tracker) , and we also compensate for optical zoom and lens distortion in addition to image keystoning. The components utilized in the development of this star simulator were a
~EC

LT245 projector, a projection screen, and the prototype sensor.

The

projector has a native resolution of 1024 x 768, a pixel size of 6.45pm, and a focal length of 22.1mm. The sensor was manually oriented to ensure that the entire projected image was within its FOV. The general layout and governing coordinate axes are defined in Figure 2.4.

2.2

Model Overview

Two versions of the lab model were developed, a forward model and an inverse model. Both are based on the same lab parameters and geometric relations, but implement these operations in a different sequence. The forward model is used within the proposed calibration routine, describing the transformation from projector coordinates to sensor coordinates. The inverse model describes the transformation from the sensor image to the projector input image. This second model is used to determine the necessary projector image for a given desired sensor image. Each is based on 14 parameters that describe the position and orientation of the sensor, the position and orientation of the projection screen, the focal ratios of the sensor and projector optics, and lastly the distortion of the sensor optics. Before beginning the model descriptions, it is worthwhile to define the co-

18

Chapter 2. Lab Calibration

PROJECTION SCREEN

Figure 2.4: Lab calibration setup.

ordinate axes that are used to relate the image coordinates to the orientation and position of the other components. Two coordinate frames are attached to the sensor: one centered on the image detector, denoted by subscript S, and another centered on the sensor intersection of the optical axis with the detector, denoted by subscript OA. Both reference frames are oriented in the same manner as showed in Figure 2.4, only their respective origins are different. A projector frame is also defined, denoted by P, whose origin lies at the center of the projector and corresponding axes line up respectively. Star trackers measure the location of a star as a direction vector; generally in angular terms (e.g. right ascension and declination). Due to the fact that we will be working with these vectors in a relative sense with respect to

2.2.

Model Overview

19

the projector and sensor, a new representation of relative direction is presented. This representation is further referred to as spherical coordinates and redefines any celestial direction using an angle from boresight, p, and a clock angle from the vertical axis , fJ. Please see Figure 2.5 for an illustration.

Figure 2.5: Angular coordinate definition

2.2.1

Forward Model Derivation

As previously described, the forward model is implemented as part of the calibration routine used to determine the model parameters. This is achieved by an error minimization between the centroids of the modeled test pattern and the

20

Chapter 2. Lab Calibration

imaged test pattern. Due to the use of a specific representation of lens distortion, the error minimization is not implemented on the raw imaged test pattern but on a partially corrected form of this image instead. This representation is used in an interest to limit numerical computation, and will become evident within the derivation of the inverse model. For an illustration of the order of operations implemented in the forward model and the point of error minimization in the proposed calibration, see Figure 2.6.

Projector input coordinates

Apply focal ratio (projector)

Compute intersection with screen (P-frame)

Rotate intersection to sensor frame

Apply focal ratio (sensor)

I

I I I _I

l-----

I I I I __________ _

Sensor output image

Distortion Free Apply Distortion Coordinates Correction (Sensor Detector) 1------1
I I

Distortion Free Coordinates (Sensor Detector)

1------,
I I I

RMS computation : : K, OA
l - - - - - _I l - - - - - _I l _____ _ 1

I:

I I

I

I I

l----- -·

Figure 2.6: Forward Model roadmap.

The analytical model begins at the projector input by specifying the direction

(pp, Bp) of a calibration point. Both the projector and sensor contain a set
of optics that have an ideal focal length,

J;,

specified by the optical design.

However, the actual value of this focal length,

JP , rnay

differ slightly from this

ideal value. This discrepancy is addressed by defining a focal ratio, F p, that will be used to scale the off-boresight angle, p, either up or down to model focal length discrepancies and zoom effects. VVith respect to the projector, this scaling

2.2.

Model Overview

21

operation is defined as follows:

PP =tan
where

,

_1

(tanpp) Fp

(2.1)

Fp-

!~

JP

(2.2)

The intersection of the direction vector with the projection surface is then found by using the direction vector and the distance to the projection screen, Dpt· The direction vector, (p'p : B'p), is first converted to Cartesian components and then traced to the projection surface. The conversion to Cartesian coordinates is described as follows:
Sp
I

r sin B'p r cos B'p [ fp

l

(2.3)

where

r = fp tanp'p

(2.4)

using the parametric equations for a plane, the intersection of the star vector can now be found. A plane is defined by a point, Ppt, and a normal, npt = [a b c]r. The point and normal used are:

(2.5)

npt

=

[

sin Gpz - sin ci> pl cos 8 pl cos ci>pz cos epz

l

(2.6)

where: Dpt is the distance along the boresight from the projector to the projection screen, and ci>pt and \lJ pl are the rotations of the projection screen about the local

X andY axes. The plane constant, d, can then be found through the parametric

22
formulation:
ax

Chapter 2. Lab Calibration

+ by + cz + d = 0

(2.7)

and so

(2.8)
Using this constant, the intersection of the vector can be found by
Sintp =

s~

with the projection screen

ts'p
d

(2.9)

where

t=

sp ·

I

(2.10)
npl

This intersection calculation gives the coordinates of the intersection point , in the proj ector frame. The next st ep is to convert these coordinates into the sensor frame using the orientation of the camera (<P s , 8 s, '11 s). This is achieved using a 1-2-3 Euler angle set , Rps , as follows: (2.11) where (2.12) and (2.13)
Sintp

is the position of the intersection point in projector coordinates, and S p

is the position of the sensor in projector coordinates. Following this, another scaling of the angular coordinate p will occur as the position of the calibration point is traced through the sensor optics. In order to implement this scaling, the coordinates of the point vector, (ps ', Bs '): (2.14) where (x 18 , y 15 ) are the x and y components of the intersection coordinates in
Sints

must be first converted to an angular direction

2.2. Model Overview the sensor frarne ,
Bints.

23

_ t 2 (X Is,Y Is ) e's-aan r r
' =tan -1( - r ) Ps
Zis

(2.15) (2.16)

The scaling operation due to the sensor optics is then:
Ps = tan- 1 (Fs tan Ps ')

(2.17)

where
F

s-

_is
f'

i~

(2.18)

f

represents the set focal length, and

is the true focal length. Given these

scaled angular coordinates, we convert them back to Cartesian coordinates using the assumed focal length of the sensor optics, to determine the ideal position of the calibration point on the imaging array:

r =is tan(ps)
r sin Os r cosOs

ss =

l

(2.19)

(2.20)

[

is

As shown in Figure 2.6, this analytical derivation of the calibration point's coordinates is then compared to a corrected version of the imaged point coordinates. The purpose of the analytical transformation applied to the actual image is to correct for the effect of lens distortion which can be specified by 3 parameters. The first two parameters, (xoA , YoA) : specify the intersection of the sensor optical axis with respect to the image detector. This parameter is important because the magnitude of radial distortion is a function of a points distance relative to the optical axis. By knowing the coordinates of this axis intersection, this relative distance can be computed for any point on the sensor detector. The third necessary parameter is the distortion coefficient, J(. This coefficient describes the magnitude and type of radial distortion. The correction

24

Chapter 2. Lab Calibration

for radial lens distortion is implemented as follows: The coordinates of an imaged calibration point relative to the optical axis are described as: (2.21) The distortion corrected coordinates of a calibration point; relative to the optical axis, are then calculated by shifting the relative imaged coordinates (lens distortion included) , s~t ' by <5: (2.22) where: (2.23)
I

T =

2

S opt x

+ S opty 2
I

(2.24)

Lastly, the coordinates are then transformed back to the sensor frame through the relation: ss =
Sopt

+ (OA)

(2.25)

2.2.2

Inverse Model Derivation

The second algorithm, titled the inverse model, was developed to implement a set of lab parameters and compute the necessary projector image for a given desired sensor response. Within the forward model, coordinates were analytically mapped from the projector input image onto the sensor imaging array. Given a set of lab parameters, the opposite relationship can now be defined: mapping desired sensor coordinates to the input projector coordinates. Given a desired star position on the sensor array, (s:;), we account for the effect of lens distortion first. This is achieved in exactly the same manner as was done within the forward model using Eqs. 2.21-2.25. Following this, the

2.3. Calibration Implementation

25

discrepancy in the sensor focal length is taken into account. The coordinates

(ss) are first converted into spherical coordinates, (ps ,Bs) using Eqs. 2.142.16 and the set focal length of the prototype sensor, f s. Using the sensor focal ratio, F 5 , the discrepancy is defined as an operation on the angular boresight
component:
,

Ps =tan

_1

(tan Ps)
F
s

(2.26)
s~,

The direction vector is first converted back to Cartesian coordinates, and the plane point, PPL , into the sensor frame:

and then
npl

traced to the projection surface. To achieve this , we rotate the plane normal,

ns

= Rspnpl

(2.27) (2.28)

Ps = RsPPpl

The intersection is then defined in the sensor frame using Eqs. 2.7- 2.10. These coordinates are then converted into the projector frame using the sensor orientation, Rps, and the corresponding sensor position, sp. In order to account for the discrepancy in projector focal length, the intersection coordinates are first converted into angular coordinates, (pp ' , Bp ') , using Eqs. 2.14-2.16. The discrepancy is then applied as:

pp =tan

_1

(tanpp ') Fp

(2.29)

The last step is to recompute the corresponding Cartesian pixel coordinates (ss) on the projector image using Eq. 2.3.

2.3

Calibration Implementation

The first step in the implementation of the lab calibration was composed of projecting, capturing, and centroiding a test pattern of points. The coordinates of the imaged points were then compared to a corresponding set formed analytically by using a coarse estimate of the lab parameters and the forward model. using

26

Chapter 2. Lab Calibration

a constrained nonlinear optimization, a set of 14 lab parameters was computed through a minimization of the Root Mean Squared (RMS) error between each set of calibration points. Once the system was calibrated, the parameters can be used with the inverse model to correct for the projection geometry when placing stars in synthetic images.

2.3.1

Calibration Points & Star Representation

As a result of the projector resolution, the accuracy of projections onto the projection surface is normally limited to the pixel level. \iVith a technique similar to defocussing star images, the accuracy of this placement can be improved to the sub-pixel level by utilizing BVN distributions instead of single pixels. This distributes an illumination pattern over a region of pixels, allowing for finer control of centroid placement. These distributions are centered at the desired coordinates and then integrated across each pixel area to acquire the necessary projector input. The intensity of a circular star image is given by the bivariate normal distribution as:

P(y , x ) = 27r<J 2
where

A IIexp
~2
~

2 (1

~P

2)

I
2

(2.30)

z = (y-~ty )
~2

2

_

2p(y-~y)(x --:~x )

+ (x -~x )
~2

(2.31)

and

p =cor (y , x) = Vy x
2

(2.32)

is the correlation of y and x , and Vyx is the covariance. The intensity value of each pixel within the illumination pattern was calculated by integrating the relation across each pixel region.

2.3.2

Test Patterns

The calibration procedure dictates the use of a projected and imaged test pattern in order to determine the lab parameters. For simplicity this study utilized the

2.3. Calibration Implementation

27

most basic of test pattern, a uniformly spaced grid of calibration points. This grid pattern was setup to correspond to the projector aspect ratio of 4:3 and was spaced uniformly across 80% of the available projection area to account for any discrepancies between the projected image and the sensor FOV. Four different grid densities were tested, to assess impact of additional calibration points on the resultant angular star placement (Table 2.2). Although this analysis is limited to only using a simply uniform grid pattern, it is important to note that the shape and spatial distribution of a test pattern vary the resultant accuracy of the parameter fit and the total time necessary for calibration. This is part of the plan of future work and will be discussed in more detail later in the chapter. Table 2.2: Investigated Grid Densities Total # of: calibration points: 48 108 192 300 horz. 6 9 12 15 vert. 8 12 16 20

2.3.3

Imaging & Centroiding

Each calibration point of test pattern was projected, imaged, and centroided individually to eliminate any confusion associated with identifying corresponding pairs. Individual images were pregenerated and stored to decrease the total calibration time. Once an image was captured, the centroid of the calibration point was found through three main steps: · Detecting the calibration point within the captured image. · Determining an appropriate window around the PSF. · Computing the first-moment centroid of the window to find the exact point location with respect to the image detector.

28

Chapter 2. Lab Calibration

The first task of detected the calibration point was achieved by thresholding the image to eliminate any nonessential background illumination and image noise. This was set to a value corresponding to 20% of the maximum illumination level of the detector, determined empirically. The image was then searched Once a bright row by row for pixels brighter than the computed threshold.

pixel was found, the pixels surrounding it were examined to check for an illumination pattern. If there was no illumination pattern surrounding the pixel, it was dismissed as noise and the search continued.

If an illumination pat-

tern was found, the pixel was assumed to be the edge of a calibration point's PSF. The accuracy of the centroid is dependent on the size of the PSF. The

larger the PSF, the more pixel values can be used within the centroid calculation. To accommodate the fact that not all PSFs imaged by the sensor will be the same, a dynamic sizing routine was developed to determine an appropriate window around each PSF. The window size is based on the dimension from the edge of the detected PSF to the brightest pixel. To find the brightest pixel, a repetitive process of comparing the brightness of one pixel to the brightest pixel of the surrounding neighborhood was used. Beginning at the edge pixel of the PSF, a 9 x 9 window (centred on the current pixel) was searched for a brighter measured illumination. If brighter pixel was found, the process was repeated with the window now being centred on the new brightest pixel. Once no brighter pixels were found, the distance from the edge pixel to the current pixel, TJ, was computed. A square 2TJ x 2TJ window was then centred around the brightest pixel, and the contained pixels values were then used in the computation of the first-moment centroid. L"sing the brightness values within the windowed PSF, I(x, as: (2.33)

y), and coor-

dinates corresponding to each pixel, the first moment centroid can be computed

2.4. Results

29

Yc -

_ 2:;:12:~:1 [y!( x , y)] '\:"'11 '\:"' 11 I( ) L...i x =1 L...iy=1 X' Y
S(x , y) = (xc, Yc)

(2.34) (2.35)

where I(x , y) is the illumination value at row x and column y and S(x , y) is the computed centroid with respect to the defined window.

2.3.4

Optimization

After acquiring the image coordinates of each point within the projected test pattern, an optimization routine was employed using the forward model to compute the desired set of lab parameters. This optimization was based on the minimization of the RMS error between the analytically generated and imaged test patterns: see Figure 2.6. This is defined by cost function Q shown below.

Q=

t
i=1
test

IIJr eal ( i) ~ I gen ( i) 11

2

(2.36)

where

lgen = f (LP)
and jector
Ireal

(2.37)

is

the and

imaged LP

pattern: the

pattern

represents

Igen IS the generated proset of 14 lab parameters:

Fp , F8 , Sp , ci> 8 , 8 8 , \IT 8 , ci>pz , 8pz , Dpz, OA, K. Given a coarse initial guess and bounds on the valid ranges of each lab parameter, a constrained nonlinear optimization was implemented using the MATLAB "fmincon" optimization
function.

2.4

Results

The accuracy of a proposed star simulator was assessed in two ways: -residual calibration error and pattern simulation error. Results of the analytical model were compared to the imaged calibration pattern to asses the validity of the lab

30

Chapter 2. Lab Calibration

model. Then in addition , a series of artificial star patterns were simulated and examined, comparing the desired arc lengths and dihedral angles to the observed results.

2.4.1

Accuracy of calibration

Each of the four calibration patterns listed in Table 2.2 were used to form a corresponding parameter fit. The residual error between each imaged and analytically generated pattern were then examined. Table 2.3 shows the mean and maximum values of residual error for each test pattern.

Table 2.3: Resultant accuracies of each test pattern Total # of calibration points: 48 108 192 300 Calibration Time: ~ 5 min ~ 9 min ~ 16 min ~ 25 min l\!Iean Angular Error (
0 )

0.0088 0.0078 0.0077 0.0073

Maximum Angular Error ( 0.0181 0.0249 0.0246 0.0313
0 )

The mean residual error is quite promising in terms of attained angular accuracy and is comparable to other more common forms of star simulators. There does not appear to be much of a correlation between test pattern population and resultant mean accuracy. In comparison , the maximum residual error values show a much stronger trend with test pattern population but not in the direction expected. As more points are added, the maximum error grows significantly. In addition to the statistical analysis shown in Table 2.3 , the spatial distribution of the residual error was examined for each test pattern. These distributions showed that as more points were added to the test pattern, small regions of the test pattern grew in residual error while the majority of the pattern showed decreases in residual error. These defined regions are visible around the sensor boresight and at the corners of the test pattern, see Figure 2. 7. As the number of

2.4. Results

31

calibration points was increased, only the relative magnitude of the residual error grew within these regions while its spatial distribution remained constant. This seems to indicate that there is structure in the residual error, possibly due to an incorrect lab model. Future investigations are required to identify and model any source (s) of these errors.
0.02 0.018 0.016 0.014 0.012 0.01
Oi (I) :8.
(ii

10

5

e
E
(I)
(.)

Oi (I) :8.

'E (I)
co 0..

·x
co

en

x
5 en
(I)

0

iii

c

(/)

0.008 "S Cl
c

ro ro

co
::3

(ij

0.006
-5

·u;
(I)

'0

0::

-10 -10
-5

0
SensorY-axis (deg)

5

10

Figure 2. 7: Residual star placement error (300-point pattern).

2.4.2

Mock Star Pattern Tests

As a second evaluation of star simulator performance, a series of artificial star patterns were projected and imaged using the computed parameter values and the inverse model. A triangular star pattern was created by equally spacing three stars within the sensor FOV. This pattern was then incrementally rotated

32

Chapter 2. Lab Calibration

about the sensor boresight to create a series of 100 test star patterns. Each star pattern was projected, imaged and centroided one star at a time, to prevent any mismatch of points. Once all three stars within the pattern were imaged, the arc lengths and dihedral angles of the pattern were computed and compared to the values associated with the generated pattern. Discrepancies between imaged and desired arc-lengths and dihedral angles were then analyzed to assess the simulator performance, see Table 2.4 for error statistics and Figures 2.8-2.9 for illustration.

Table 2.4: Mock Star Pattern Accuracy

I Arc Length Error (o) I Dihedral Angle Error ( o) I

10

20

30

40

50
Image count

60

70

80

90

100

Figure 2.8: Arc length error

2.5. Conclusion

33

0.1

~ 0.08
0

~
~

e
0.06
C])

"0

.J::.

0

0 .04

0.02

10

20

30

40

50
Image count

60

70

80

90

100

Figure 2.9: Dihedral angle error.

The data series corresponding to Table 2.4 shows sizable periodic increases in arc length and dihedral error. This is due to part of the tested star pattern moving through a region of less accurate star placement, see Figure 2. 7. Eliminating these regions should allow for maximum error values to converge to the indicated means.

2.5

Conclusion

Testing the pattern matching performance and capability of a star tracker requires accurate angular star placement throughout the sensor FOV. Lab setups associated with this type of testing commonly require a significant amount of user measurement and alignment. This generally leads to single-use lab space, that ultimately makes star tracker ground testing more difficult. The proposed star simulator addresses both of these issues in a simple, low-cost fashion. The achievable angular star placement accuracy is comparable, if not better than other common forms of star simulators. In addition , the automated calibration limits the required labor to less than 1 minute and the total calibration time to

34

Chapter 2. Lab Calibration

approximately 25 min. This makes it feasible to recalibrate the lab setup before each use, resulting in a more flexible laboratory environment. Although this calibration routine was developed for use with proposed projector-screen-sensor system, it can easily be implemented to calibrate similar forms of star simulators, specifically those involving high resolution displays. Further investigations are still required to address some remaining issues and explore further possibilities. First, the developed lab model needs to be revisited in order to identify any remaining problems or limitations. Second, the calibration procedure can still be streamlined by tuning the size, shape and method of projection/ capture of the utilized test pattern. The projection of multiple calibration points within a single image could greatly reduce the amount of total calibration time but some logic must be developed to prevent the mismatch of points. Lastly, some further investigation of star placement accuracy is required using a higher resolution projector.

35

CHAPTER

3

Detector Calibration

T

he pixels detectors common to all star tracker designs produce imperfect representations of the spatial distribution of incoming starlight. Ideally, the

signal from each pixel would vary in direct proportion to the integrated photon flux incident on the pixers collection area, but a number of physical processes create temporal and spatial variations in these signals. This noise impairs our measurement of star centroids. VVithout precise star centroid positions, the accuracy of the star tracker's attitude solution suffers. Fortunately, some of these distorting effects can be removed through careful radiometric calibration. This part of the study examines a calibration routine originally proposed by Healey, [Healey & Kondepudy 1994], extends the application to colour detectors and describes its implementation for star tracker detector calibration.

3.1

Introduction

Generally, two types of solid-state detectors are used for star trackers: ChargedCoupled Devicess (CCDs) and Complimentary-Metal-Oxide-Semiconductors (CMOS). Although many functional and performance differences exist between these two technologies, they both share similar noise models. Vve begin with a basic review of both CCD and CMOS detector operations. Following this, we present a basic overview of detector noise sources and some comments as to how they contribute to the measure of illumination.

36

Chapter 3. Detector Calibration

3.1.1

Basic detector operation

Solid state arrays are optoelectronic components that provide the ability to convert light intensity into a measurable voltage signal. Digital camera use this ability to form irnages that can be thought of as a spatial sampling of the incident light. Silicon based photodetectors convert incident photons into Electron Hole Pairs (EHPs) that form an electric charge proportional in magnitude. The CCD is then just a semiconductor architecture incorporating this technology, allowing for this generated charge to be collected, transfered and ultimately read out as a measurable voltage signal [Holst & Lomheim 2007, Janesick 2001]. Photons are converted to an electrical charge by either a i\1etal-OxideSemiconductor (MOS), also called a photogate, or by a photodiode. An applied voltage potential is used to isolate and hold the charge within the local region, commonly called a pixel well. The total number of electrons that can be Following a finite exposure time, stored within the well is proportional to the applied voltage, oxide thickness, and gate electrode area, [Janesick 2001]. the collected charge within the pixel is is read out of the well by the CCD register. This transfer process occurs by systematically manipulating the gate voltage of neighboring pixel wells. Charges are transferred from pixel to pixel in a conveyor-belt-like fashion down columns, and then across the bottom row to an output circuit for charge-to-voltage conversion and amplification [Holst & Lomheim 2007]. A circuit, commonly called an electrometer, converts the charge packet frorn the charge don1ain to voltage domain using either a floating diode or floating diffusion. precharged at a reference level. The diode , acting as a capacitor, is The transferred charge partially discharges

this capacitance resulting in a resultant signal voltage linearly proportional to the number of electrons [Holst & Lomheim 2007, Janesick 2001]. This voltage is then subsequently amplified by before it reaches the Analog to Digital Convertor (ADC). Although there are many functional and performance differences between

3.1. Introduction

37

CCD and CMOS detectors, the most evident difference is at the pixel level. Unlike CCDs, CMOS detectors perform the charge-to-voltage conversion within each pixel well. The pixel area is shared by the photodetector, the electrometer, and the add~essing/ output connection circuitry [Holst & Lomheim 2007]. This not only allows for select pixel readout, but given that the image is in digital form at the pixel level, image processing routines can be implemented on the chip. In addition, the voltages required for pixel reset and select transistor operations are generally lower than those used for CCDs, sometimes by an order of magnitude [Holst & Lomheim 2007], leading to a lower power requirement. These characteristics make the CMOS array an attractive alternative to CCDs for star tracker component selection.

3.1.2

Fundamental detector noise sources

Ideally, the signal from each pixel would vary in direct proportion to the integrated photon flux incident on the pixel's collection area, but a number of physical processes create temporal and spatial variations in these signals. These variations can change the shape and the illumination distribution of an imaged star's PSF. In turn, this impairs our measurement of star centroids which leads to reduced attitude accuracy. Before we look into correcting for detector noise , it is important to understand where it originates and the respective impacts on the measured pixel illumination. \Ve present a fundamental detector noise model, outlining the basic noise sources associated with solid-state detectors. Although discrepancies exist in the relative magnitudes of the discussed noise types, the presented model is generally applicable to both CCD and CMOS detectors. The noise of an image detector can be loosely defined as any deviation of the measured pixel illumination from the truth. As discussed in section 3.1.1, image detectors are composed of many subcomponents that each participate in the formation of the pixel illumination measurement. :Many of these subcom-

38

Chapter 3. Detector Calibration

ponents contribute some form of noise to this measurement value, beginning within the pixel well and leading all the way up to the ADC. A basic model of this process is shown in Figure 3.1 , where we define four main points of noise contribution.

Photon (Shot) Noise Dark Current Noise Fixed Pattern Noise

G

Amplifier Noise

ri l

Reset Noise

[4 ']

Quantization Noise

Figure 3.1: Noise injection model for pixel detectors. The first of these points, labelled as # 1 within Figure 3.1 , describes the measured illumination at the point of the node capacitor. At this stage, a finite amount of charge has been collected within the node capacitor following a specific integration time. This charge is about to be transferred out of the pixel well to allow for the next exposure. Three different types of noise make up this first point of noise contribution: · Photon (Shot j\"oise), a consequence of the photon counting process. · Dark Current l\ oise (DCN), caused by thermal electrons. · Fixed Pattern Noise (FPN), caused by pixel-to-pixel variations in sensitivity. Each of these noise types represent a unique form of noise contribution to the ideal signal. Their corresponding effects on the measured illumination are further discussed in the paragraphs to follow.

3.1. Introduction

39

Shot noise is associated with the random arrival of incident photons on the image detector. Since each photon is an independent event, the arrival of any given photon cannot be precisely predicted; instead the probability of its arrival in a given time period can be examined [Kod 2005]. This type of noise is most apparent when the number of collected photons is small. It 's resultant effect can be reduced by simply collecting more photons, either with a longer exposure or by combining multiple frames. Dark Current Noise (DCN) is a fundamental noise source In any siliconbased photodetector. Thermal energy generates free electrons within the silicon substrate that are indistinguishable from photoelectrons when they migrate into pixel wells. These free electrons accumulate over the exposure time within each pixel and lead to an artificial increase in sensed well illumination. The noise contribution due to dark current varies both spatiality throughout the detector and temporally, making it difficult to determine its value at any given pixel. Commonly this type of detector noise is mostly eliminated by cooling the detector. Last of these first three noise sources is Fixed Pattern Noise (FPN). This term represents a summation of many individual noise sources that each cause pixels to respond slightly differently to incident light. These individual contributions come from small variations in detector size, doping density, coating thickness, and many other effects that are a result of a variety of imperfections in the fabrication of solid-state detectors [Holst & Lomheim 2007, Healey & Kondepudy 1994, Janesick 2001]. The resultant effect of this type of noise on the measured illumination can be equated to a pixel gain that is a function of incident light intensity and varies spatially throughout the array. This is the definition of FPN that is used throughout this study and is consistent with the work of [Healey & Kondepudy 1994]. Other literature limits the term FPN to only the time-invariant component of previously described variances in pixel response. The term photo response nonuniformity is then used to describe temporal effects pertaining to the dependence of this pixel

40

Chapter 3.

Detector Calibration

gain on illumination levels. The remaining three points of noise contribution, defined as # 2-4 within Figure 3.1, describe the noise contributions of the pixel readout process. This includes the conversion of the signal from the charge domain to the voltage domain by a sense capacitor, subsequent amplification, and lastly quantization effects of the ADC. As described in 3.1.1 , the charge-to-voltage conversion process is done by measuring a change from a reference voltage. voltage has to be reset to its original value. Following each exposure, this Resistance within this process

generates a thermal noise that is commonly known as reset noise, or kTC noise. This causes the reference value to fluctuate, resulting in a random contribution to the output illumination signal. Once the signal is converted to a voltage value, it is amplified to an acceptable level for the ADC. This process of amplification introduces two types of noise: white noise and flicker noise. White noise, or commonly called Johnson noise after its pioneer [Kod 2005, Janesick 2001], is due to resistance within the output amplifier. This resistance results in another source of thermal noise which is modelled as a random fluctuation in the amplified signal. Flicker noise, also known as 1/ f noise, represents the frequency dependent noise contribution of the amplifier. Generally accepted to originate due to interface states in the image sensor silicon that turn on and off randomly according to different time constants, flicker noise is inversely proportional to the pixel read-out rate [Janesick 2001]. Lastly we have quantization noise. This is simply a result of the ADC that must represent the signal using a finite range of integer values. Inherently, this will introduce roundoff errors that result in small contributions to the output voltage. The behavior of the noise contribution can be both random or uniform depending on the ranges of expected voltages. In either case, quantization noise is not a dominant noise source.

3.2. Noise calibration formulation

41

3.2

Noise calibration formulation

Noise calibration is most effective for sources that are constant or only slowly changing in time. Of the reviewed list of noise sources , fixed pattern noise and the constant component of dark current noise make the best two candidates for correction. Fortunately these two noise sources are also generally recognized as the dominant contributors to image noise for both CCD and CMOS detectors [Janesick 2001 , Yadid-Pecht & Etienne-Cummings 2004]. Our basis for this calibration is based on a the CCD calibration scheme developed by Healey and Kondepudy [Healey & Kondepudy 1994]. Vve present an abbreviated version of their calibration procedure and describe extensions for use with colour imagers. Because our prototype camera uses a linear response CMOS detector, the noise behavior remains similar [Janesick 2001, Novak et al. ] and the same analysis can be applied.

3.2.1

Correction Model

The response of an arbitrary detector can be described by D(a , b), indexed by row a and column, b , is related to the 'true' integrated illumination, I (a , b) , collected by the detector pixels. This relationship has the form:

D(a , b)= (J((a , b)I(a, b)+ NDc(a, b)+ Ns(a , b)+ NR(a , b)) A+ NQ(a , b) (3.1)
where K is the per-pixel gain variation (i.e. , the FPN) , A is the amplifier gain, and the remaining terms are due to noise; DC~ (NDc), Shot (Ns), Reset (NR) , Quantization (N Q) , respectively. The formulation of this relation is showed in [Healey & Kondepudy 1994], but can also be rederived based on the previous discussions on fundamental detector operations and common detector noise sources. By definition, the average value of J( across the array is one. In order to minimize the effects of noise, we wish to apply a correction of the form:

D c (a , b) = D (a , b) "- Dvc (a , b) J((a , b)

(3.2)

42

Chapter 3. Detector Calibration

using estimates of the DCN component of the image, D;c (a , b) , and the FP~ gain, K. The calibration process must provide a means of estimating these quantities.

3.2.2

Dark Current Noise {DCN)

Separating the dark current components from the detector image requires an improved understanding of the model can be expressed as:
DC~

terms. Mathematically, the bias-plus-noise

(3.3)
The stochastic component of the noise model, N N
,

is treated as a zero-mean,

Gaussian random variable. If images are taken with zero incident illumination, then the entire pixel response will be due to DCN. The bias term can then be estimated by averaging a series of ndark images, Ddark:

DDc(a , b)=

LZ!!rk Ddarki (a , b)
ndark

(3.4)

Due to the random nature of N N

,

some pixels in D- D

;c would end up having

values less than zero. Since image intensities are often handled as unsigned, fixed point integers , some care must be taken to prevent underflow. Negative value pixels can be pinned to zero. This model is useful but it does not quite tell the whole story. This bias,
tint ,

while constant in short term, is actually a function of integration time, the determination of

and ambient temperature , T. Including all of these factors would complicate

b DC

.

Many detectors, including the one used in this

study, have hardware features that help avoid this problem. Our detector has a border of masked (blackened) pixels around the edge of the detector to help correct the T and
t int

dependencies. The response from these dark pixels are
Nbias ·

used to estimate the bulk changes in

Averaging images using Eq. 3.4 then

3.2. Noise calibration formulation

43

gives us the per-pixel deviation from the overall array bias. From our testing, there is some evidence to suggest that impact on the star detection problem.

iJ DC

still retains some temperature and

integration time dependence, but these effects are quite weak and have minimal

3.2.3

Fixed Pattern Noise
FP~

The challenge in estimating

is to separate the per-pixel gain variations, The calibration method outlined by Healey

K (a, b), from any spatial illumination variation caused by nonuniform illumination and surface reflectance. begins by generating a quasi-uniform illumination across a calibration card, and imaging the illumination field. Uniformity is necessary over small distances, but small variations over larger distances are actually helpful. \iVith the illuminated field filling the camera FOV, we take a series of n 2 images and calculate the DCN-correction mean:

2:::7:! 1 ( Dj(a , b)- DDc(a , b)) e(a , b) = ----'---------'-n2

(3.5)

Since we assume that we are able to remove the bias component of the DCN and the averaging process removes the zero mean components of the noise sources we are effectively measuring the expectation of the response due to the illumination, i.e. ,

e(a , b)~ E {K (a , b) I (a , b) A}

(3.6)

We then reorient the camera so the pixels see slightly different areas of the calibration card and repeat the averaging process. If we collect n 1 sets , each of n 2 images, then the results from the i-th set are denoted:

(3.7)

44

Chapter 3. Detector Calibration

Once these temporal averages have been calculated, we compute a series of spatially averaged images. These are computed using a W x W window centred on each pixel:
e~ a ,

-. ( b)

~ a+( W-1 )/ 2

~b + (W-1 )/ 2

= L.,.,q=a-( W - 1)/2 L.,.,r=b- (W - 1)/2

(vV)2

ei q ~ r

(

)

(3.8)

This spatial averaging is intended to remove the effects of the FP:\" term K (a , b). The window size must be large enough so that spatial gain variations average out , but small enough that the illumination over the integration window remains constant. Since each carnera orientation changes the detector illumination slightly, we can estimate the slope of each pixel's response to the changing illumination:

m (a, b) =

~

n1 .

f
~ =1

':_i (a b) ei (a , b)

(3.9)

Thus, we evaluate the individual pixel gain by comparing changes in each pixel to the mean change taken over the averaging window. Vve improve the quality of the fit by making a final pass through the data to remove outliers. For each pixel we compute:
(3.10)

and
a~ (a , b)= var (mi)

(3.11)

Averaging the standard deviations over the image gives the mean standard deviation, a-. We then remove from the consideration any pixel where:

II mi (a , b) -

iii

(a , b)

II > p · a

(3.12)

leaving n~ (a ,'b) acceptable values. After removing outliers, the mean slope at each pixel was recalculated giving a final estimate of K (a, b) :
(3.13)

3.3. Implementation

45

The detector in our camera is a Bayer-pattern colour imager, see Fig. 3.2. Each pixel is covered with a colour filter that gives the detector its color sensitivity. The filters are arranged in a repeating 2 x 2 , Green-Red-Blue-Green (GRBG) pattern. The lamp used in our tests provides broadband illumination, but the response between pixels of different colors will not be identical. As a consequence, Eq. 3.8 must be calculated using only pixels of the same color. This change is fairly easy to implement, but it does require some more spatial uniformity in the illumination.

Bayer pattern

Detector array

Figure 3.2: Bayer-pattern on image detector.

3.3

Implementation
DDc,
the dark reference image. To minimize stray light, we
ndark

Dark current calibration is straightforward. \Ve take a series of zero illumination images to obtain take the exposures with the lens cap on, the lights off, and an opaque box around the whole camera. For our tests,

= 100. Repeating the calibration several

46

Chapter 3. Detector Calibration
Over a range

times with the same integration time gave consistent results. of integration times (10- 1
:::; tint :::;

1second) , the spatial mean of
tint

Dvc

grew

steadily, but slowly. The net change over this

range was about 6 detector

counts (0.15% of full scale). The tight clustering in the the bulk hardware corrections are very effective.

Dvc

values together

with the very small changes over vastly different integration times suggests that

The experimental setup for the FPN calibration is a little more complicated. Broadband illumination for these tests is provided by a Xenon arc lamp. Light from the lamp passes through a diffusing opalesent glass screen and strikes a white calibration card chosen for low specular reflectivity. The camera is manually positioned with a good view of the illuminated portion of the card. The whole apparatus is mounted on an optics table for stability. To vary the illumination across the detector, the camera position was manually shifted to image a different area of the card. using the For this study n 1

= 56 different image

configurations were taken. We removed the dark current bias from each image

fJ vc

obtained above, and calculated the FPN correction,

k,

using Eq.

3.13. The array windows size was W

= 19.

The entire image acquisition process required approximately 15min of manual labour to reposition the camera in different configurations. Following this , 5min was required for automated computation.

3.4

Results

Evaluating the performance of this calibration is a challenging problem to frame, in large part due to the number of pixels in the detector. The impact of detector calibration on star detection and localization would be a good measure of effectiveness, but it is impractical to make such evaluations over each pixel in the array. Instead , we consider the statistical properties of an average image set e* (a, b) , before and after calibration. It is important to note that the images

3.4. Results

47

used to calculate e* were collected from a camera configuration distinct from those used during calibration.

3.4.1

Validation of illumination uniformity
Prior to any examination of

The implemented calibration procedure is based on having a relatively constant, or at least smoothly varying, illumination field. the noise correction effectiveness, it is worthwhile to provide at least a basic validation that this true for our lab setup. This can be done by a comparison of the variables ei (a, b) and ei (a , b) over n 1 configurations. To reiterate, ei (a , b) is the pixel illumination following dark current correction, ei (a , b) is the mean value of ei (a , b) within the pixel neighborhood and n 1 is the number of different image configurations. This comparison was quantified as a standard deviation of the difference between these values and then the entire detector was searched for the strongest and weakest representations of this spatial uniformity assumption. The results are shown in Figure 3.3.
2000

1900

~ 1800

u

e.

~
0

~

1700

!
iU
Qi

c:

1600

.!S
0..

1500
-

0 ·

Pixel illuminations (Weakest Linearity) Pixel illuminations (Strongest Linearity) Calculated Pixel Gain (Weakest Linearity) Calculated Pixel Gain (Strongest Linearity)

130 0 L - - - - - - L . - - - - ' - - - - ' - - - - - ' - - - - - - ' - - - - - ' - - - - - ' 1400 1500 1600 1700 1800 1900 2000 2100
Mean Pixel Neigborhood Values

Figure 3.3: Validation of spatial illumination consistancy. Examining both sets of data points, it can be seen that there are significant

48

Chapter 3. Detector Calibration

changes of neighborhood illumination. However, despite these changes in illumination even the weakest representation of the illumination spatial uniformity seems to exhibit a relatively strong linear relationship. This increases the confidence of the computed FPN correction as it is shown that the n1ean neighborhood illumination is in fact a good approximation of any individual pixel value.

3.4.2

Effectiveness of FPN correction

Our assumption in the FPN calibration is that pixel response variations within a small windowed region of a calibration image are due to FP:\1 effects rather than differences in illumination. Based on this assumption, effective cancellation of FPN would decrease the magnitudes of these variations, leading to a more uniform windowed response. Using a window size of

vV

= 19, we examined the

variation in windowed pixel responses throughout a new calibration image e* 1 This variation is represented as a standard deviation , a 8 (a , b), which is calculated as:

a8(a , b)=
\iVhere N
=

(3.14)

W 2 , xi is the pixel response of the ith pixel within the window and

e(a,b).
Table 3.1: Corrected vs. Uncorrected variation in windowed pixel response (in detector counts) C ncorrected Mean window a of variation win. variation 17.52 20.46 25.65 1.24 2.43 2.89 Corrected a of Mean window variation win. variation 12.91 15.85 21.56 0.83 1.42 2.20

I

Pixel Color Green Red Blue

Fig. 3.4 and Table 3.1 show the distributions of a 8 before and after FP:\" correc1

Independent of the image set used for the FPN calibration process.

3.5. Conclusion
r/)

49

·[ o.o6 1 -----r----r------r----..------r---,::::::::::::::::::::::r::=====::::::;-] a; c:::J Uncorrected
~ 0.04
]i
-Corrected

Qi

Green Pixels

~ 0.02
0

c:

u
u.
r/)

0

~

o~--~--~-

8

10

Io.o6 1--r--..----.---.--.---.-----r---,:~::::::::::==~==::::::;-J
-c

18 16 12 14 Resdiual Neighborhood Standard Deviation (Detector Counts) Red Pixels

20

22

c:::J Uncorrected
Corrected

~ 0.04
]i

~ 0.02
c:

0

n ~
u.
r/)

0

12

Qi

22 24 26 18 20 16 14 Resdiual Neighborhood Standard Deviation (Detector Counts) Blue Pixels

28

30

·[ 0.06 ,------r-----.------r::-------""lr----,:::::::::::~======:::::;.,
a>

c:::J Uncorrected
-Corrected

ci5
]i

0.04

.9 0 0.02
c:

~

0

u:

01~0----1~5_.__

35 20 25 30 Resdiual Neighborhood Standard Deviation (Detector Counts)

40

Figure 3.4: Corrected vs. Uncorrected image smoothness characterization. tion, for each color. From this data it is clear that variations within a windowed region of the image do decrease as a result of the FPN correction. To gauge how much pixels are affected by the correction, we statistically examine the difference in the windowed pixel variations shown in Figure 3.4. These results are presented in Figure 3.5.

3.5

Conclusion

Detector noise degrades our ability to properly detect and centroid stars , ultimately reducing the accuracy of a star tracker. To decrease these effects, a form of radiometric calibration outlined by Healey, [Healey & Kondepudy 1994], was adapted and implemented to reduce the effects of DCN and FPN. The success of the calibration was judged by a decrease in variation of observed measured

PROPERTY OF BYERSON UNIVERSITY LIBRARY

50

Chapter 3. Detector Calibration
Green Pixels

5

-40
1i

0.0 iii 0.04 ~

·~
0 0

"'

-35 -30 -25 -20 -5 -15 -10 Change in neighborhood standard deviation(% of uncorrected) Blue Pixels

0

5

6

.------r---:~: : ~-15.6r-------T----.8 %:
-20 -15 -10 -5 Change in neighborhood standard deviation (% of uncorrected)
0

:a c
~ J:

0.02

-45

oL---~----~---------40 -35 -30 -25

5

Figure 3.5: Change in window variation due to FPl\ correction.

illuminations within a local window , centered about each pixel. Prior to any correction, each pixel neighborhood showed variations from 17-25 detector counts, or approximately 1% of the measured illumination. Literature describes DCl\ and
FP~

to be the dominant forms of noise present within uncalibrated solid-state

imagers. The latter of which is generally outlined to fall around 1% of measured signal strength. Following the correction , neighborhood variations were reduced by 15- 26% of their original values, dropping the measured magnitude of FPN to approximately 0.8% of the measured illumination. This improvement is moderate but not insignificant. Futher study is required to determine how this improved reponse uniformity translates to improvements in star detection and star centroiding.

3.5. Conclusion

51

3.5.1

Implementation Notes

The online requirements of the DCN and FP:\1 corrections are sizeable, particularly in terms of required storage. The correction for these noise sources is applied on a per-pixel basis. Thus, at least several bytes of calibration data must be stored on board the sensor for each pixel in the array. Judging from the observed noise levels, one byte of DCN correction should be sufficient; with the use of look-up tables, one or two additional bytes would be enough to correct the FPN effects. Thus, the 5-megapixel array in our prototype star tracker would then require 10-15 MB of non-volatile storage for the calibration data. \iVhile not absurdly large, this may have significant impact on the hardware design. Computational costs will vary greatly depending on where the corrections must be applied to the entire array, or whether windowing can be used. Fixed point subtraction is sufficient for DCN correction, but FPN correction will require multiplication and possibly fixed point division. these areas will itself take processing. Confining these calculations to areas of interest, will save significant processing, but identifying

53

CHAPTER 4

Optical Model Calibration

A

ll light rays passing through an optical system experience some amount of deformation due to nonidealities of the optics used. These nonidealities Without any aberrations the

are generally described as optical aberrations.

focused image of a point source (star) would appear as a small circular spot. Due to optical effects, the Point-Spread Function (PSF) of a point source object can change in shape, size, and position. These effects impair our ability to properly measure the star centroid, degrading star tracker accuracy. This chapter focusses on how optical aberrations affect star tracker images and possible corrective measures than can be implemented to minimize their effects. We first present a brief review of optical aberrations and how they Second, As part of an overall plan to deform the PSF of a point source.

correct for these effects, this section outlines a framework that can be used to determine the specifics of present aberrations. The framework is based on the calibration of an analytical model that describes the transformation of light from a pinhole, through the sensor optics and onto the detector. This calibration is outlined as a comparison between test images and analytical predictions at the point of the lens incident light vector. To achieve this comparison, a parameterization for deformed PSFs was developed. Although limited in the ability to parameterize the effects of all aberrations, preliminary results from a survey of the sensor FOV show results that are consistent with expected trends.

54

Chapter 4. Optical Model Calibration

4.1

Background

First we present a brief review of ray optics and the five basic optical aberrations followed by an outline of the proposed aberration calibration procedure.

4.1.1

Ray Optics

This summary follows a review of Gaussian optics from ?? . In the field of Gaussian optics, rays incident on a lens surface follow the Gaussian approximation to Snell's law (i.e., for small angles). This first-order approximation is defined as:
TJ sin()

ry' sin()' ry'()'

(4.1)
( 4.2)

ry()

where TJ and ry' are the refractive indices of the two media and () and ()' are the angles of the incident and refracted rays. The rays that propagate according to this approximation are called paraxial rays. From this approximation it can be shown that the image point of an incident ray is independent of the height of intersection made with the a spherical convex refracting surface. This leads to the formulation of the Gaussian imaging equation that can be defined as:

n'

n

n'- n

S'

S

R

(4.3)

where R is the radius of curvature of the refracting surface and S' is the distance of point P~ from the intersection of the optical axis and the refractive surface. This equation shows that given an object point Po a distance S away from a refractive surface, all rays incident on this surface will converge to a point P~ a distance S' from the surface [Mahajan 1998], see Figure 4.1. In a non-ideal optical system, the refracted rays intersect the axis at slightly different points in the vicinity of P~, which can be described as the effects of optical aberrations.

VIe can now extend the reviewed principles of ray optics to waves. Given a convex spherical refracting surface as that shown in Figure 4.1, a point source

4.1. Background

55

Optical axis

{- )S

5'
Figure 4.1: Gaussian imaging by a convex spherical refractive surface, [Mahajan 1998] illumination profile will result in a spherical exit wavefront under the paraxial approximation. This wavefront is commonly known as the Gaussian reference sphere, which is centred about a point on the optical axis known as the Gaussian image point. This point is part of a plane that represents the ideal convergence of any on or off-axis illumination and is commonly called the Gaussian image plane. As additional terms are added to the paraxial approximation , the wavefront at the exit pupil ceases to be spherical and therefore not perfectly convergent to the Gaussian image point. These terms can be divided into five groups that each describe a different effect on the exiting wavefront.

4.1.2

Optical aberrations

Arbitrary, physically realizable refracting surfaces will produce very complex images but even mathematically ideal optical elements will introduce aberrant response. The five most common effects are: spherical, coma, astigmatism, field

56

Chapter 4. Optical Model Calibration
Aberration Spherical Coma Common Coefficient Effect on point-source PSF Airy pattern of concentric fringes Comet-like deformation (bright head with diminishing tail) Elliptical deformation Elliptical deformation Pincushion or Barrel deformation

As Ac Aa Ad At

Astigmatism Field Curvature Distortion

Table 4.1: Aberration effects on point-source PSF curvature, and distortion. Commonly the effect of each aberration is parameterized into an coefficient that describes its relative magnitude and the order of included terms. Some common representations of aberrations are the Siedel representation [\Velfod 1986, Born & \Volf 1991] and the Zernike representation [Mahajan 1998, Born & Wolf 1991]. Each of these aberration types represent a specific type of basis function that describes the shape of the exit wavefront. Table 4.1loosely defines these effect on an ideal point-source PSF. The coefficients listed within the table are used to denote the peak values of the aberration within an optical system. For a full review of optical aberrations and their corresponding effects , please see (Mahajan 1998, Mahajan 2001, \Velfod 1986, Born & Wolf 1991].

4.2

Proposed lens calibration

The final aim of the lens calibration procedure is to minimize the effects of aberrations on the PSFs of stars. This goal can be divided into two main tasks: · Find which aberrations are present and to what extent.
· lJ sing this information , correct both the centroid position and PSF shape.

Although neither task has been fully completed to date, a framework has been developed to accomplish the first of these two tasks and preliminary results have

4.2. Proposed lens calibration

57

been collected. This framework is based on the calibration of an analytical model that describes the transformation of light from a pinhole light source, through the sensor optics and onto the detector. This model is based on a set of optical and geometric parameters that are difficult to manually measure. Instead, a calibration procedure has been developed to compute the parameter set through a comparison of test images to analytical predictions. In this case, the test irnages are pinhole images at various known sensor orientations.

4.2.1

Lens calibration lab setup

The lab setup intended for this calibration procedure consists of a Xenon arc lamp , 25Jtm pinhole, a 3-axis motion platform and the prototype sensor. The lamp is used to illuminate the pinhole which is aimed at a set of mirrors . These mirrors are positioned to redirect the point source illumination pattern onto the star tracker which is mounted to the 3-axis motion platform to allow for controlled precise rotation about any axes. The mirrors extend the distance between the point source and the sensor optics, see Figure 4.2 for illustration. The angular positioning accuracy of this platform is 0.002° (1 - (]" ). The entire setup is mounted to an optics table for stability and vibrational isolation.

4.2.2

Image comparison with analytical predictions

Given a vector of light incident on an lens system, determining the resultant PSF would be difficult. This in turn complicates the task of comparing acquired pinhole images to analytical predictions from a known sensor orientation. Instead, we chose to make the comparison between test images and analytical predictions on the outer surface of the lens by exmaining incident light vectors. A road map for the proposed comparison is shown in Figure 4.3. Stemming from the roadmap, we have identified the necessary parameters and have begun formulating the required analytical relations. These relations are divided into two categories: finding the incident light vector from the light source

58

Chapter 4. Optical Model Calibration
Pinhole Xenon Lamp

· -- -- -------------

----------W'-~-+-t---------¥.-.t--·

Image of pinhole

Mirror

Prototype Star Tracker

3-Axis Motion Platform

Figure 4.2: Aberration survey lab setup.

(pinhole) side and finding the incident light vector from the PSF side. The first of these two formulations is straightforward. For a known sensor orientation, the incident vector on the light source side of the sensor optics is described by the set of joint angles and a translational offset of the pinhole light vector from the sensor optical axis at zero joint angles. This translational offset is orthogonal to the optical axis and is comprised of two components, Lx and Ly , corresponding to offsets of the light source along the x and y axes, see Figure 4.4. On the other (detector) side, the formulation of the incident light vector is more complicated. It is a function of the lens aberration coefficients, the position and shape of the imaged PSF and a set of imager mounting parameters. The mounting parameters can be described as a set of translation offset parameters and a rotation sequence that describe the position and orientation of the image detector with respect to the sensor. The translational offset is represented in the sensor frame as I

= (Ix, Iy , I z), and is also shown in Figure 4.4. The necessary

4.2. Proposed lens calibration

59

Joint Angles

Incident Light Vector Computation

Incident Light Vector Computation

Seidel Coefficients

Lamp& Pinhole
I I

Ray Spot Diagram

Light Source Offset

Comparison
- RMS error

Imager Mounting Parameters

Lamp to incident light vector

Image to incident light vector

Figure 4.3: Aberration Characterization road map of operations.

set of rotation angles are then described by <I> I , 8

I,

and 'l1 I , expressed as a

direction cosine matrix from the detector frame to the sensor frame, C si. The sign convention utilized for these rotations is described in Figure 4.5. The required model parameters are summarized by the following list: · Light source misaslignment with sensor optical axis (2 parameters) · Aberration coefficients of the lens system (5 parameter) · Orientation of the image detector, with respect to the sensor (optical axis) (3 parameters) · Position of the image detector, with respect to the sensor (3 parameters)
"C sing these formulations of the incident light vector from both sides of the sensor

optics, an error minimization routine can be setup to compute values for each of the 13 describe parameters. This routines guides the variation in model parameters by minimizing the RMS error between the two computed incident light vectors.

60

Chapter 4. Optical Model Calibration

X

-- -

/

z
Incident Light Vector

Figure 4.4: Light source boresight misalignment.

4.3

PSF Parameterization

Part of the formulation for the incident light vector (from the detector side) involves the position and shape of the imaged PSF. This can be related to the present aberrations and the off-axis angle of the incident vector. As a preliminary analysis, we develop a parameterization technique for aberrant PSFs and exarnine how these parameters change throughout the sensor FOV. This is beneficial in two ways: · Variations in the parameters mapped over the sensor FOV will help describe the dominant forms of aberrations. · Based on these variations, a PSF deformation lookup table can be created that can be used to improve the detection of aberrant PSFs in the presence of noise.

4.3.

PSF Parameterization

61

lens

imaging plane

Figure 4.5: Imager misalignment sign convention

A star can modelled as a point source at an infinite distance. With an ideal optical system, this source should appear as a circular PSF on the detector plane of an image sensor. As introduced above, optical aberrations alter the exit wavefront from the sensor optics, changing the PSF shape and position. These effects can be subdivided into respective contributions from each aberration. If we can parameterize the shape and position of an aberrant PSF, we can map these effects across the sensor FOV. Given some basic optical design information pertaining to our prototype star tracker's optics, we know that the dominant aberrations are astigmatism, field curvature and distortion. Thus, the developed parameterization is limited to the effects of these three aberrations on the measured PSF location and shape. Distortion is commonly approximated as a strictly position altering aberration. Field curvature and astigmatism represent shape varying aberrations, transforming circular PSFs into ellipses. Even though the process is different, they introduce similar effects. Vve can approximate these PSF deformations by fitting a Bivariate Normal equation:
(BV~)

distribution to the imaged PSF. Mathematically, this can be represented by the

f (.r, y) =

A exp (- [ _z 2 ] ) 2 1 Pxy

(4.4)

62 where

Chapter 4. Optical Model Calibration

(4.5)
A is peak magnitude defined as:

(4.6)
a-x and a- y are the standard deviations of the distribution along the principal axes

of the ellipse, (J-tx, P,y) define the ellipse centroid on the image detector and Pxy is the correlation coefficient. This coefficient affects the eccentricity of the ellipse and its orientation with respect to the x and y axes of the image detector.

4.4

Survey Implementation
The resultant PSF images were then fit to a

The aberration survey was completed by imaging an illuminated pinhole from various off-boresight angles.
BV~

distribution through a nonlinear least-squares iterative algorithm. Due to

misalignments between the sensor optical axis and light source boresight only about half of the sensor FOV was sampled. The platform was used to vary the sensor orientation allowing for the point source to be imaged through various rotations of the sensor optics. The sensor was rotated both around its boresight and through incrementally increasing off-axis angles with the point source, taking irnages at each orientation. A relatively large sample set of 756 images was taken in all, representing approximately half of the sensor FOV. Due to some residual misalignment of the sensor with the point source, the collected pinhole images represent a series of concentric rings centered about the light source boresight. Each imaged PSF was fit with a BVN distribution using a nonlinear least-squares approach. The raw detector image is first centroided to provide the approximate PSF position. The image is then windowed to a smaller image based on the size

4.5.

Survey Results
1 .

63

of the PSF

The window is divided into individual pixel illuminations that are

assumed to be representative of a BVN illumination distribution, I(i , j). These pixel illuminations are then compared against an analytical model of the BVN distribution , G(A , rJ x ,y , pxy ). A cost function, Q, can then be defined as the absolute difference in pixel illuminations:

Q = L L[G(A, CJx, CJy, PXY)- f(i, j)] 2
j

(4.7)

A nonlinear least squares algorithm was then used to compute optimal values of the BVN parameter set, through a minimization of Eq. 4. 7. An initial guess and bounds on parameter variation were used with MATLAB's lsqnonlin function to perform this optimization.

4.5

Survey Results

Shape deformation is clearly present within the aberration survey of the sensor FOV. BVN parameters fitted from the survey data show smooth variations in both the size of the PSF principal axes,
CJ x, CJy,

and their respective correlation

coefficient, PXY· All three of these fitted parameters form saddle-shaped trends when a surface is mapped to the collected data points, see Figures 4.6-4.8. The hole at the center of each of these plots is due to a lack of imaged data points within the region. This was caused due to residual misalignment between the light source boresight and the optical axis of the prototype sensor. This type of relation bet-vveen the standard deviations and the correlation coefficient describes a stretching of the ideal circular PSF into an ellipse. It is clear that this deformation grows as the distance of the imaged point from the optical axis increases. This type of PSF deformation is consistent with the effects of astigmatism and field curvature, which was expected.

This is described by the distance traveled on the detector from the edge pixel of the PSF to the brightest pixel of the PSF, see Chapter 2

1

64

Chapter 4. Optical Model Calibration
1800r--.----~--~----~----r----r----~--------~

2.5

1600
2

1400

1200

·x "'
11000
(j;
O"l

1.5

ro

.£ 800
600
0.5

400

200 600 800 1000 1200 1400 1600 1800 2000 2200
0

Imager x-axis

Figure 4.6:

O"x

variation across sampled sensor FOV.
0.8

1800~~~---T----~----~----r---~-----T----~----~

1600

0.6

1400

0.4

1200

0.2

a.

"E

"' ·x
11000
(j;
O"l

·u
0
(.)

Q)

~ 0
c
0

.£

ro

800

-0.2

~
(.)

0

600

-0.4

400

200 600 800 1000 1200 1400 1600 1800 2000 2200

Imager x-axis

Figure 4.8: pxy variation across sampled sensor FOV.

4.5. Survey Results
1800.--r--~----~--~---r---,----.---~----~

65
2.5

1600
2

1400

1200
·;;;:
(jj
IJ)

1.5

11000

.s

ro

Ol

800

600
0.5

400 200 600 800 1000 1200 1400 1600 1800 2000 2200

0

Imager x-axis

Figure 4. 7:

0' y

variation across sampled sensor FOV.

In addition to the parameters shown in Figures 4.6-4.8, the BVN parameter fit also determined the centroid of each PSF. If intersection coordinates of the optical axis with the detector were known, the fitted PSF centroids could be used to determine distortion effects. As previously described , the distortion aberration can change the position of the PSF. Depending on the magnitude, this would cause the the imaged PSF centroids to take either a pincushion or barrel-like shape, see Chapter 2. Any measurable deformation from the ideal pattern of imaged concentric rings could then be related to the relative magnitude of the distortion aberration. Unfortunately, offsets from both the lens and detector look the same as offsets of the detector platform. This makes the calculation of the optical axis intersection more difficult. The next section outlines a procedure for aberration characterization that aims to simply this problem. Small-scale variations of the fitted BVN parameters are also evident within the parameter plots of Figures 4.6-4.8. Further analysis is required to determine if these features

66

Chapter 4. Optical Model Calibration

are a result of other optical aberrations or just artifacts of image processing.

4.6

Summary

A method to parameterize the shape and position of a deformed PSF has been developed. Using this parameterization and the BVN fitting technique described in section 4.4, aberrant PSF images have been fit with sets of BVN parameters. Trends in these parameters reveal shape deforming effects representative of astigmatism and field curvature aberrations. These forms of deformations agree with optical specifications provided by the lens manufacturer describing astigmatism and field curvature as two of the dominant aberrating effects. As part of the goal to correct for all present optical aberrations, an approach has been developed to determine the aberration coefficients. An optical model has been outlined that can serve as an analytical tool to relate imaged PSFs to corresponding joint angles. Further work is still needed to define the specific analytical relations associated with PSF deformation due to aberrations.

67

CHAPTER

5

Conclusions and Future Work

T

he goal of this study was to examine several practical calibration techniques applicable to the development and manufacture of modern microsatellite

star trackers. Three such calibrations were examined pertaining to the lab setup used for sensor ground testing, the image detector and the sensor optics. This chapter provides an outline of key features and results observed within each developed calibration as well as some suggestions of future work. The thesis ends with a discussion on how and when these calibration techniques would be used within the various stages of sensor development and testing.

5.1

Lab calibration summary

A new form of star simulator was presented involving the use of an LCD projector. The outlined setup procedure minimizes the need of manual setup, a common drawback of most of the present star simulators, through the use of a self-calibrating projector-screen-sensor system. An analytical model of lab was developed relating projector coordinates to sensor coordinates. A projected test pattern was then used together with a developed calibration procedure to numerically determine necessary parameters of this lab setup. Using these values together with the lab model, projector images were modified to compensate for geometric and optical nonidealities of the lab setup. This allows for the desired image representation from sensor perspective. The main focusses of this calibration were to maximize the angular placement accuracy of points within the sensor FOV and minimize the required

68

Chapter 5.

Conclusions and Future Work

amount of labor required for setup. Prelirninary results of show a resultant angular star placement accuracy of 0.0073°. This value is comparable to and in some cases better than other similar forms of star simulation. In addition , the entire calibration procedure required less than 1 minute of labor and approximately 25 min of automated calibration. This makes it feasible to recalibrate the lab setup before each use, resulting in a more flexible laboratory environment. Although this calibration routine was developed for use with proposed projector-screensensor system, it can easily be implemented to calibrate similar forms of star simulators, specifically those involving high resolution displays. Further investigations are still required to address some remaining issues and explore further possibilities. First , the developed lab model needs to be revisited and examined for potential model changes that would better account for increased regions of error near the boresight and at the corners. Second, the calibration procedure can still be streamlined by tuning the size , shape and method of projection/ capture of the utilized test pattern. The projection of multiple calibration points within a single image could greatly reduce the amount the total calibration time but some logic must be developed to prevent the mismatch of points. Lastly, some further investigation of star placement accuracy is required using a higher resolution projector.

5.2

Detector Calibration

Pixel detectors common to all star tracker designs produce imperfect representations of the spatial distribution of incoming starlight. This affects our ability to properly centroid stars, degrading star tracker accuracy. both CCD and CMOS imagers. These imperfect representations stem from various types of detector noise, generally common to Following an overview of fundamental noise sources, an existing procedure for radiometric camera calibration developed by Healey was adapted and implemented for our current star tracker.

5.3. Lens Calibration

69

This form of detector calibration focussed on removing the dominant noise sources associated with uncalibrated imagers: DC)J and FPN. Following the calibration , images could be corrected on-demand, given some fixed parameter storage requirements. The effectiveness of this correction was then examined through a statistical analysis of the variation within small pixel windows. Prior to correction, a 19 x 19 window of pixels generally saw a variation within the range of 17-25 detector counts, or approximately 1% of the measured illumination. Following the correction, this variation was reduced to 13-21 detector counts, or approximately 0.8% of the measured illumination. Although these improvements are small, the implementation of this correction can improve our ability to detect and centroid stars. A further analysis of how these changes in pixel variation relate to improvements in star detection and centroid accuracy, could provide some insight as to whether the large storage requirements are worth it.

5.3

Lens Calibration

Aberrations corrupt the PSFs of stars imaged by the sensor. As part of an overall plan to correct for these effects, this section outlined a framework that can be used to determine the specifics of present aberrations. The framework is based on the calibration of an analytical model that describes the transformation of light from a pinhole, through the sensor optics and onto the detector. This calibration is outlined as a comparison between test images and analytical predictions at the point of the lens incident light vector. To achieve this comparison, a parameterization for deformed PSFs was developed. Although limited in the ability to parameterize the effects of all aberrations, preliminary results from a survey of the sensor FOV show results that are consistent with expected trends. Future work is required to expand the PSF parameterization to include all aberration effects, and subsequently relate these values to the incident light

70

Chapter 5.

Conclusions and Future Work

vector. This relation can then be used to determine the specific details of the aberrations present, inorder to implement some form of PSF centroid and shape correction. Although in this case, the FOV survey was just used for basic This places the total required time for image parameterization validation, no other images would be required for the complete calibration of the lab model. acquisition at approximately 35min. Assuming a similar optimization time to the calibration of the star simulator, the entire lens calibration procedure could be completed in less than 40min.

5.4

Concluding Remarks
Each

Individually, these techniques are not necessarily ground-breaking.

represents a conventional or commonsense approach to calibration. Together however, they represent a set of tools that we feel are particularly practical and cost-effective for improving the engineering of microsatellite star trackers. Both the detector and lens calibration can improve the speed and accuracy of a star track attitude solution. The calibration of the image detector reduces small noise contributions throughout the image, making it easier to find dim stars and more accurately centroid bright stars. Similarly, the calibration of the sensor optics allows for the formulation of a PSF deformation lookup table. If the specific type of PSF deformation for a certain part of the detector is known, it is easier to detect aberrant PSFs hidden amongst image noise. Both of these calibrations can be useful when selecting components for a prototype sensor or even for just improving the performance of existing sensors. The developed lab calibration can be used to simplify hardware-in-theloop testing. This type of testing is used to validate the sensor as a whole. The quick and automated calibration of the developed star simulator, promote frequent testing that is useful for rapid sensor development. In addition , the short simulator calibration time allows for the quick testing of multiple sensors.

BIBLIOGRAPHY

71

Bibliography
[Alexander & Chang 1996] Alexander, J.\i\1., & Chang, D.H., "Cassini star tracking and identification algorithms, scene simulation, and testing," Proceedings of SPIE - The International Society for Optical Engineering, Vol.

2803, 1996, Pages: 311 - 336. [Bank 1997] Bank, T., "Characterizing a star tracker with built in attitude estimation algorithms under the night sky," Proceedings of SPIE - The International Society for Optical Engineering, Vol. 3086, 1997, Pages: 264-

274. [Born & \i\Tolf 1991] Born, M., & \i\Tolf, E., Principles of Optics. Cambridge University Press, 1991. [Gullapalli et al. 1993] Gullapalli, S.i\"., Flynn Doe, D.J., Kissih, F.J., Gauthier, A.G., & Kenney, T.M., "ASTRAl solid state star trackers for Martin Niarietta's modular attitude control system module," SPIE Proceedings, Vol. 1949, 1993, Pages: 127 - 137. [Healey & Kondepudy 1994] Healey, G.E, & Kondepudy, R., "Radiometric CCD Camera Calibration and Noise Estimation," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 16, March, 1994, Pages: 247

- 253. [Holst & Lomheim 2007] Holst, G.C., & Lomheim, T.S., CMOS/ CCD Benson;
and Camera Systems. SPIE Press, 2007.

[Janesick 2001] Janesick, J.R., Scientific Charge-Coupled Devices. SPIE Press, 2001. [Jorgensen & Liebe 1996] Jorgensen, J.L., & Liebe, C.C., "The advanced stellar compass, development and operations," Acta Astronautica, Vol. 39, 1996, Pages: 9 - 12.

72

Bibliography

[Kod 2005] CCD Image Sensor Noise Sources. Online, January, 2005. Revision 2.1 MTD/ PS-0233. [Larson & \iVertz 2005] Larson, \iV.J., & \iVertz, J.R., Space Mission Analysis and
Design, Pages: 354 - 380, Space Technology Series. Microcosm Press,

California, USA, 2005. [Liebe 2002] Liebe, C.C., "Accuracy Performance of Star Trackers- A Tutorial ,'l
IEEE Transactions on Aerospace and Electronic Systemsl Vol. 38, April,

2002, Pages: 587 - 599. [Mahajan 1998] Mahajan, V.l\., Optical Imaging and Aberrations- Part 1: Ray
Geometrical Optics. SPIE Press, 1998.

[Mahajan 2001] Mahajan, V.N., Optical Imaging and Aberrations- Part 2: Wave
Diffraction Optics. SPIE Pressl 2001.
[~ovak et al. ]

1\ovak, C., Shafer, S., & \iVilson, R., ," .

[Ruffino & Moccia 2002] Ruffino, G. , & Moccia, A.l "Laboratory test system for performance evaluation of advanced star sensors ," Journal of Guidance,
Control, and Dynamics, Vol. 25, 2002, Pages: 200 - 208.

[Sukthankar et al. 2001] Sukthankar, R, Stockton, R.G., & Mullin, 1\II.D., "Smarter Presentations: Exploiting Homography in Camera-Projector Systems," Proceedings of the International Conference on Computer Vision, Vol. 3086, 2001, Pages: 247 - 253.

[Thomas et al. 1996] Thomas, V.C., Blue, R.C., & Procopio, D., "Cassini stellar reference unit: performance test approach and results," SP IE - The International Society for Optical Engineering, Vol. 2803, 1996, Pages: 288 -

298. [\iVelfod 1986] Welfod, \iV.T., Aberrations of optical systems. Adam Hilger Ltd, 1986.

Bibliography

73

[Wessling & Does 1994] Wessling, F.C., & Does, A.V., "The star field simulator for the spacelab instrument pointing system fixed head star trackers," Acta

Astronautica, Vol. 2221, 1994, Pages: 116 - 127.
[Yadid-Pecht & Etienne-Cummings 2004] Yadid-Pecht, Cummings, R.,

CMOS Imagers:

& EtienneFrom Phototransduction to Image
0.,

Processing. Kluwer Academic Publishers, 2004.


