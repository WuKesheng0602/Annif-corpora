Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2012

Medical Image Segmentation and Classification Based on Sparse Representation and Dictionary Learnng Algorithms
Mohammadali Julazadeh
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Biomedical Engineering and Bioengineering Commons Recommended Citation
Julazadeh, Mohammadali, "Medical Image Segmentation and Classification Based on Sparse Representation and Dictionary Learnng Algorithms" (2012). Theses and dissertations. Paper 1400.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

MEDICAL IMAGE SEGMENTATION AND CLASSIFICATION BASED ON SPARSE REPRESENTATION AND DICTIONARY LEARNING ALGORITHMS by Mohammadali Julazadeh BASc, Tehran Azad University, Tehran, Iran, 2008

A thesis Presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Applied Science in the program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2012 Â© Mohammadali Julazadeh 2012

I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

Signature I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Signature

II

Ryerson University requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date.

III

MEDICAL IMAGE SEGMENTATION AND CLASSIFICATION BASED ON SPARSE REPRESENTATION AND DICTIONARY LEARNING ALGORITHMS
Master of Applied Science 2012

Mohammadali Julazadeh Electrical and Computer Engineering Ryerson University

Abstract

In this thesis a novel classification approach based on sparse representation framework is proposed. The method finds the minimum Euclidian distance between an input patch (pattern) and atoms (templates) of a learned-base dictionary for different classes to perform the classification task. A mathematical approach is developed to map the sparse representation vector to Euclidian distances. We show that the highest coefficient of the sparse vector is not necessarily a suitable indicator for classification. The proposed algorithm is compared with the conventional Sparse Representation Classification (SRC) framework as well as non-sparse based methods to evaluate its performance. Taking advantage of the introduced classification framework, we then propose a novel fully automated method for the purpose of segmenting different organs in medical images of the human body. Our results demonstrated an acceptable accuracy rate for both classification and the segmentation frameworks. To our knowledge, no other method utilizes sparse representation and dictionary learning techniques in order to segment medical images.

IV

Acknowledgements
I would like to extend my sincere appreciation to Professor Javad Alirezaie, my MASc. supervisor in Department of Electrical and Computer Engineering, Ryerson University for helping me to research in the field of my interest, image and signal processing, and for his advices through my research and studies. I would like to express my appreciation to my co-supervisor, Dr. Paul Babyn in the Department of

Medical imaging at university of Saskatchewan and Saskatoon Health region for his support and
comments on my papers and my research trend. I would like to thank my beloved parents, Farzaneh and AliAkbar as well as my two sisters, Maryam and Leila for their encouragement and supports during my research. I also thank my friends and coresearchers at Ryerson University to help me better achieve my purpose.

V

Table of Contents
Chapter One: Introduction....................................................................................................................... 1 1.1 1.2 Preface..................................................................................................................................... 1 Computer Aided Diagnosis (CAD) ............................................................................................. 3

1.3 Image Segmentation ...................................................................................................................... 4 1.4 Sparse Representation of Signals ................................................................................................... 6 1.5 Our contributions .......................................................................................................................... 7 Chapter Two: Sparse Representation ....................................................................................................... 9 2.1 Sparse Approximation.................................................................................................................. 10 2.1.1 Uniqueness of sparsest approximation: ................................................................................. 11 2.2 Importance of Sparse Approximation: .......................................................................................... 13 2.3 Numerical Methods and Algorithms for Sparse Coding................................................................. 14 2.3.1 Greedy Methods: .................................................................................................................. 15 2.3.1.1 Matching Pursuit ............................................................................................................ 16 2.3.1.2 Orthogonal Matching Pursuit ......................................................................................... 17 2.3.1.3 Batch Orthogonal Matching Pursuit: ............................................................................... 19 2.3.1.4 Stagewise Orthogonal Matching Pursuit: ........................................................................ 21 2.3.1.5 History of greedy methods for sparse approximation: .................................................... 23 2.3.2 Convex Relaxation Methods: ................................................................................................. 24 2.3.2.1 Basis Pursuit: .................................................................................................................. 24 VI

2.3.2.1.1 Primal Dual Interior Point Algorithm for linear programming: .................................. 25 2.3.2.1.2 Solving the Basis Pursuit optimization problem: ....................................................... 27 2.3.3 Stopping Criterion: ................................................................................................................ 28 Chapter Three: Dictionary Learning ....................................................................................................... 29 3.1 Quest for a proper dictionary ....................................................................................................... 29 3.2 Fixed or Learned Dictionaries? ..................................................................................................... 30 3.3 Fixed dictionaries ......................................................................................................................... 31 3.3.1 Time-Frequency Dictionaries ................................................................................................. 31 3.3.2 Discrete Cosine Transform .................................................................................................... 31 3.3.3 Wavelet and Wavelet Transform ........................................................................................... 32 3.4 Learning the Dictionary ................................................................................................................ 34 3.4.1 Method of Optimal Directions (MOD):................................................................................... 35 3.4.2 K-SVD Dictionary Learning ..................................................................................................... 37 3.5 Discriminative Dictionary Learning ............................................................................................... 42 3.5.1 Dictionary Learning with Structured Incoherence and Shared Features for Classification and Clustering ...................................................................................................................................... 42 3.5.2 Discriminative K-SVD ............................................................................................................. 45 3.5.3 Fisher Discriminative Dictionary Learning .............................................................................. 48 3.5.3.1 Discriminative fidelity term ............................................................................................ 49 3.5.3.2 Discriminative Coefficient Term ...................................................................................... 50 Chapter Four: Classification and Clustering based on Sparse Representation ......................................... 54 VII

4.1 Motivation: Texture Separation ................................................................................................... 57 4.2 Sparse Representation Classification (SRC) ................................................................................... 60 4.2.1 The Classification Scheme ..................................................................................................... 61 4.3 Reconstruction Error Base, Classification...................................................................................... 64 4.3.1 Segmenting  MRI images using the reconstruction error ................................................... 65 Chapter Five: Image Segmentation and Sparse Representation Classification Based on Euclidian Distances ............................................................................................................................................... 67 5.1 Methodology ............................................................................................................................... 68 5.1.1 OMP and K-SVD..................................................................................................................... 68 5.1.2 Sparse Euclidean Classification: ............................................................................................. 69 5.2 performance ................................................................................................................................ 72 5.3 Organ Boundary Segmentation .................................................................................................... 79 5.3.1 Methodology ........................................................................................................................ 79 5.3.2 Segmenting the human heart in MR images .......................................................................... 80 5.3.3 Segmentation of the kidney in CT images .............................................................................. 82 5.3.4 Segmentation of ventricular system in MR Images of the Brain ................................................. 84 Chapter Six: Conclusion and Discussion ................................................................................................. 88 6.1 Sparse Euclidian Classification ...................................................................................................... 89 6.2 Organ Segmentation Framework ................................................................................................. 89 6.3 Future Work ................................................................................................................................ 90 References: ....................................................................................................................................... 92 VIII

List of Accepted Publications: ............................................................................................................ 99

IX

List of figures
Figure 2-1 A visual demonstration of the sparse representation problem .............................................. 10 Figure 2-2 Block diagram presentation of St-OMP method. ................................................................... 23 Figure 3-1 A DCT dictionary with 8*8 size patches ................................................................................. 32 Figure 3-2 A Har base wavelet dictionary ............................................................................................... 34 Figure 3-3 The MOD dictionary learning behavior .................................................................................. 37 Figure 3-4 K-SVD dictionary learning behavior ....................................................................................... 40 Figure 3-5 K-SVD behavior VS MOD behavior ......................................................................................... 41 Figure 3-6, Original Barbara Image (Top), MOD Dictionary (bottom left), K-SVD Dictionary (bottom middle) and the representation error graph (bottom right). .................................................................. 41 Figure 3-7 Bike detection on the Graz dataset. ...................................................................................... 44 Figure 3-8 Brodatz dataset classification using the described method .................................................... 45 Figure 3-9 Segmentation results of the Brodatz dataset using DK-SVD method ...................................... 48 Figure 3-10 FDDL discriminative fidelity term......................................................................................... 50 Figure 4-1 Top: shows the original Barbara image (left) with the separated texture (middle) and the cartoon contents (right). Bottom: Both dictionary atoms in the left. ...................................................... 59 Figure 4-2 same experiment and results as Figure 4-1 on an MRI Image of the human knee. ................. 59 Figure 4-3 Face recognition results with partial features ........................................................................ 63 Figure 4-4 The image on the top demonstrates the block diagram of the method and the image in the bottom illustrates the segmentation procedure for an example image. ................................................. 66 Figure 5-1 a block diagram representation of the proposed method ...................................................... 72 Figure 5-2 (a). Demonstrates the original data with their mean in red. (b) Demonstrates the sparsely represented data. (c) Illustrates the original data labels on top which was assigned by hand, the first fifty

I

samples are from class1 and the rest are from class2, on the bottom are the labels that are resulted from the proposed method. One miss classified sample can be seen. .................................................... 73 Figure 5-3 (a) Number of miss classified samples based on the difference between the means of classes. (b) Average accuracy (percent) versus the difference between classes .................................................. 74 Figure 5-4 (a) Four randomly Gaussian distributed data with means of 40,50,60 and 70 displayed in four different colors, (b) Sparse representation and classification of the original data using SEC method. (c) Labels of the original data, first 50 samples are from class 1 and are displayed with a black label. Similarly dark gray label for class2, light gray label for class 3 and white label for the 50 samples of class 4. (d) Labels that are correctly assigned to the representation of the data using SEC method. ............... 74 Figure 5-5 Four randomly Gaussian distributed data with means of 48,51,53 and 54 displayed in four different colors, (b) Sparse representation and classification of the original data using SEC method. (c) Labels of the original data. (d) Labels of the represented data assigned by SEC method. 5 misclassified samples among 200 samples can be seen. ............................................................................................. 75 Figure 5-6 classification results comparison between SRC and SEC ........................................................ 75 Figure 5-7 classification results of the proposed method on the standard Brodatz database. ................. 76 Figure 5-8 Classification rates for three different sparse based methods ............................................... 76 Figure 5-9 Block diagram of the proposed organ segmentation method ................................................ 80 Figure 5-10 An example of segmenting the human heart in a MR image. ............................................... 81 Figure 5-11 (a) Original image, (b) The detected organ, (c) Boundary of the detected organ, (d) final segmented image .................................................................................................................................. 82 Figure 5-12 (a) Original CT image. (b) Segmented kidneys. (c) Boundaries of the segmented kidneys ..... 84 Figure 5-13 the block diagram of the ventricular system segmentation technique ................................. 86 Figure 5-14 ventricular system segmentation in MRI images. (a) Image. (b)  Image. (c) Final segmented image. ................................................................................................................................. 86

II

Figure 5-15 Manual segmentation of the lateral ventricle ...................................................................... 87

III

List of tables
Table 2.1 Matching Pursuit algorithm for sparse approximation ............................................................ 16 Table 2.2 Orthogonal Matching Pursuit pseudo algorithm for sparse approximation ............................. 19 Table 2.3 Batch Orthogonal Matching Pursuit algorithm for fast sparse approximation ......................... 21 Table 2.4 General Primal Dual Interior Point algorithm .......................................................................... 27 Table 2.5 Basis Pursuit optimization problem ........................................................................................ 28 Table 3.1 The MOD dictionary learning algorithm .................................................................................. 36 Table 3.2 K-SVD dictionary learning Pseudo algorithm ........................................................................... 40 Table 3.3 DK-SVD pseudo algorithm....................................................................................................... 48 Table 3.4. The Fisher's discriminative dictionary learning pseudo algorithm .......................................... 52 Table 3.5 recognitions rates for various methods on Extended Yale database ........................................ 53 Table 3.6 recognitions rates for various methods on AR database ......................................................... 53 Table 3.7 recognitions rates for various methods on Multi-Pie database ............................................... 53 Table 4.1 The SRC pseudo algorithm ...................................................................................................... 63 Table 5.1 the pseudo algorithm of the proposed SEC method ................................................................ 71 Table 5.2 Computational time (seconds) for the proposed method and the SRC method ....................... 77 Table 5.3 Accuracy measures for proposed method and the SRC method .............................................. 77 Table 5.4 Error rate measurements ....................................................................................................... 78 Table 5.5 Dice coefficient calculated for ten MRI images ....................................................................... 87

IV

List of Acronyms
BOMP: Batch Orthogonal Matching Pursuit BP: Basis Pursuit CAD: Computer Aided Diagnosis DCT: Discrete Cosine Transform DL: Dictionary Learning DK-SVD: Discriminative KSVD FDDL: Fisher Discriminative Dictionary Learning FOCUSS: Focal Under-determined System Solver K-SVD: Generalized K-Means, Sigular Value Decomposition MP: Matching Pursuit MOD: Method of Optimal Directions OMP: Orthogonal MAtching Pursuit St-OMP: Stagewise Orthogonal Matching Pursuit SRC: Sparse Representation Classification SEC: Sparse Euclidian Classification SSF: Separable Surrogate Functions PDIP: Primal Dual Interior Point algorithm

V

Chapter One: Introduction
1. 1

1.1 Preface

Recent advances in the field of medical imaging and the emergence of new techniques in acquiring images using Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) technologies have enabled these different imaging modalities to be widely used in the diagnosis and quantifying different diseases. As a matter of fact the expanding volume of CT and MRI studies and their image data has illuminated the need for Computer Aided Diagnosis (CAD) schemes to assist the radiologists. The first step in most CAD systems is to segment the acquired image. Image segmentation is basically subdividing an image into components such as lines or regions based on some local 1

similarities in the divisions. Segmenting an image is often a very essential part of computer aided surgery prior to the surgery itself. In order to plan a surgery or diagnose a disease it is often necessary for the medical experts to obtain a patient specific 3-D model of the organ. These 3-D models of the organs are generated by segmenting the desired organ from a set of medical images which are usually acquired from different modalities such as CT and MRI [1]. In various medical applications such as cardiology and radiography it is necessary to generate a four dimensional model of the organ of interest [2]. This four dimensional model may be used to describe the temporal change in the organ position and the shape of the organ which is very critical in follow up studies for the medical personnel. An example of the use of four dimensional models is in the radiotherapy treatment of the thorax and/or upper abdomen [2]. Organ volumetry is another significant application of segmenting organs in medical images [3]. Organ volumetry which is required in 3-D medical datasets requires an accurate segmentation of the desired organ. Organ segmentation is one of the most challenging steps prior to 3-D rendering of the organs. Although the need for medical image segmentation is inevitable, there are still many limitations and difficulties in this field. One of the most significant problems is that there is no generic method that can perform the task of segmentation for all medical imaging modalities. If an image is proven to be useful for a specific organ in a specific modality it is not guaranteed that the same method performs well for another organ or a different modality. This dilemma is mainly due to the large variations of medical images characteristics, human anatomy and pathology and different techniques that are being used to acquire such images [4]. Knowing the value of segmenting images in the biomedical field, one cannot deny the fact that for a segmentation technique to be suitable for the medical personnel it has to require minimum human interface and it should be automatic. Manual segmentation of organs is not desired for medical datasets because of the large volume of the data that is assessed for medical application. This need for an automatic segmentation algorithm is another significant challenge in CAD 2

related studies. Although many algorithms have been developed to automatically segment medical images, due to the complexity of these images most proposed methods require human interaction during segmentation [3]. In this thesis we are proposing a novel approach to segment different organs in the human body in an entirely automatic approach using the new emerging field of dictionary learning and sparse representation of signals. Unlike most methods that are limited to a specific modality and a specific organ our method is generic and can be utilized within different modalities for a variety of organs. Acceptable results have been gained both in CT and MRI modalities with minimum human interaction required. In recent years utilizing methods for sparsely representing a signal/image over a given dictionary has gained considerable attention by scholars around the world. Applications of signal sparse representation varies from compression [5] to denoising [6], restoration [7] and many more. In this thesis we have expanded this growing area of research into a new level by introducing a new approach for segmenting different organs in MRI and CT images utilizing sparse representation techniques.

1.2 Computer Aided Diagnosis (CAD)

While improvements in computer technology have had a remarkable impact on the medical imaging field, the interpretation of medical images is still best performed by medical personnel. During the past decade the use of computers in image interpretation and medical diagnostics have grown tremendously [8]. So far CAD has been described as "a second pair of eyes for the radiologists" [8] and there have been some cases that computers have outperformed human eye's observations. In CAD related studies scholars are mainly focusing on segmentation and feature extraction approaches and techniques in pattern recognition and classification frameworks. These applications can be related to any organ in the body from chest, to brain and in any modality from ultrasound to x-ray, CT 3

and MRI. The focus of this thesis is mainly on segmenting different human organs in images acquired from CT scans and MRI modalities.

1.3 Image Segmentation

The operation of subdividing an image into components such as points, lines or regions based on local similarities, pixel's intensities, frequency components and etc is called image segmentation. Imag e segmentation is primarily essential to processes such as CAD, quantitative analysis, visualization, registration and many more. Generally, image segmentation algorithms are based on two intensity value properties, either on discontinuity or similarity [9]. For segmentation based on discontinuity the approach is to obtain partitions of the image based on abrupt changes in intensity such as edges. In segmentation based on intensity similarity however, partitioning the image is based on some predefined similarities. Thresholding, region growing, region splitting and merging are a few examples of segmentation based on similarity. There are different classifications to image segmentation techniques in the literature; here we briefly present some of the important image segmentation techniques: Thresholding Methods [10]: These methods are considered to be the simplest methods in segmenting images. In these approaches usually an image is turned into a binary image utilizing a defined threshold. Using the edges in the acquired binary image the image can be segmented.

4

Clustering based methods [9]: In these approaches the segmentation procedure is carried out through a classification method such as K-means type of approaches. Having classified the image into different clusters, each pixel is assigned to a cluster centriod. By assigning the pixels to different clusters (usually in an iterative manner) the image will be segmented according to the desired cluster centroid. Histogram based methods [11]: These segmentation techniques are considered to be among the most efficient segmentation methods in the literature. In these methods the histogram of the image is computed for all pixels and based on the peaks and the valleys in the histogram diagram the image pixels will be categorized. Repeating the process of categorizing the pixels based on the peaks and the valleys will result in segmentation of the desired area in the image. Based on a survey study published by Srinivasan et. al [11]on segmentation techniques for target recognition purposes, image segmentation techniques are categorized as: 1. Edge Detection Methods 2. Tree/Graph Based Methods 3. Region Splitting Methods 4. Region Growing Methods 5. Model Based segmentation 6. Neural Network Based segmentation 7. Graph Partitioning Methods 8. Watershed Transformation 9. Multiscale segmentation 10. Probablistic and Bayesian approaches In this thesis we introduced a new automated segmentation framework which is based on the new concept of sparse representation algorithms. The proposed method can be categorized to be a 5

"Clustering based" method since it first classifies the image based on sparse techniques and following that the boundaries of the clustered area is segmented accurately.

1.4 Sparse Representation of Signals

The problem of sparse representation of signals is basically estimating a sparse multi-dimensional vector with respect to a linear system of equations; given a high dimensional observed data and a matrix known as the dictionary. If we consider y  D as the linear system of equations in which D is an m  n , (m

n ) matrix with y 

m

and  

n

, D is called the dictionary which is usually over complete and

the elements of it are called atoms, the problem is to estimate the vector (or signal) y subject to the constraint that it is sparse. It is important to note that the difference between a complete and an over complete dictionary lies in the fact that an over complete dictionary contains more amount of data since its atoms are not just orthogonal to each other but can be in any directions with respect to each other. Because of the nature of sparsity,  contains only a few non-zero elements which imply that y is decomposable as a linear combination of only a few numbers of atoms inside D . The sparse representation problem is characterized as:

min  0 such that y  D n


(1.1)

In which  0 is a pseudo-norm l0 indicating the number of on-zero coefficients of the vector  . However solving such a problem is NP-Hard [12], and with a change of l0 into l1 norm the solution to the problem (1.1) would be in the convex form of:

min  1 such that D  y 2   n


(1.2)

Where  determines the termination criterion. This problem and its derivations can be solved using different numerical algorithms which will be discussed in the following chapters.

6

1.5 Our contributions

In this thesis we are introducing a novel automated image segmentation approach to segment different organs in biomedical images. Our method is proposed based on a novel and original classification framework which we develop in conjunction with the new emerging concept of signals sparse representation and dictionary learning. We learn a particular dictionary for a specific organ, and based on the derived clustering algorithm, we first classify and later segment the desired organ. Furthermore we have proven that our method is a generic method and is not limited to a specific organ or even a specific modality. The method has been tested on standard databases such as Brodatz textures in order to evaluate its performance and for comparison with other techniques. Since the method is based on dictionary learning, one can basically learn a specific dictionary for a particular purpose and modality and takes advantage of our classification framework to cluster and later segment the image in applications other than medical images. To our best knowledge no other technique utilizes sparse based techniques in order to segment medical images. In this thesis we are aiming to introduce the advantages of sparse representation and dictionary learning algorithms in the field of medical image processing. This thesis is organized in six chapters as follow: First a general overview of sparse representation and approximation is presented in chapter 2. The original problem of sparse approximation and the mathematical approaches and algorithms to solve this problem are defined in this chapter. Chapter 3 is a comprehensive section on dictionaries and dictionary learning approaches. Since the main focus of this thesis is on classification, discriminative dictionary learning algorithms are presented in a separate sub-section in chapter 3. Chapter 4 focuses on the new emerging area of using sparse based techniques and dictionary learning for the applications 7

of pattern recognition and computer vision. The famous Sparse Representation Classification (SRC) framework is presented in this chapter. The chapter ends with a short section on the contribution that we had to the field of medical image processing using SRC based methods. Chapter 5 is a comprehended section on the contribution that we had to the field of sparse representation and classification. We present a novel classification framework with a methodology that has never been investigated before in this field. This methodology is based on defining a transform between sparse features and Euclidian distances for the purpose of clustering. Following this method we present our automated segmentation algorithm for CT and MRI images. To the knowledge of the author this is the first study focusing on segmenting medical images by means of sparse representation and dictionary learning topic. The thesis ends with a short discussion and overview of the future works in chapter 6.

8

Chapter Two: Sparse Representation
2. S

In this chapter, we first explain the concept of the sparse representation and following that numerical and mathematical algorithms to solve the sparse approximation problem are presented in details. In order to explain the concept of Sparse Representation, first the term "Sparse" should be clarified. In linear algebra the term sparse refers to a measurable property of vectors. Sparsity is not an indicator of the size of the vector, but it concerns the number of non-zero coefficients in the vector. One of the main advantages of sparsity is the simplicity of calculation that this property brings in vector calculations, as a matter of fact multiplication of a matrix by a sparse vector takes less computational time compared to a non-sparse vector, also sparse vectors can be saved on a computer by only having the position and value of its non-zero coefficients, hence utilizes less memory. 9

One indicator to measure the sparsity of a vector is the l0 norm. The l0 norm which is demonstrated as

|| . ||0 refers to the number of non-zero entries in a vector. For example l0 norm of the vector

[1,0,0,6,0,0,0] is 2 .
l1 And l2 norms indicated as || . ||1 and || . ||2 respectively are also indicators of sparsity. The l1 norm is the
sum of the absolute value of the coefficients in a vector and l2 is the Euclidian length of a vector. The l1 norm of a vector x can be indicated as:

| x |
i 1 i

n

(2.1)

And the l2 norm of this vector is defined as:

( | xi |2 )
i 1

n

(2.2)

2.1 Sparse Approximation

n nm Given a vector x  R and a matrix D  R the problem of sparse approximation can be

characterized as finding the vector   R such that x  D . In which D is usually an over complete (
n

m

n ) matrix known as the dictionary. This problem is visually demonstrated in Figure 2-1.

Figure 2-1 A visual demonstration of the sparse representation problem

10

The fact that D is an over complete matrix implies that this problem does not have a unique solution, in this problem if a sparse vector  can be found, it is called the sparse representation of x . This is due to the fact that  is the vector that can be used to reproduce or better to say, "represent" x . With the above mentioned explanations the problem of sparse approximation can now be characterized as:

|| D  x || 

(2.3)

With  (epsilon) is the reconstruction error. If a sparse vector  can be found as the solution to (2.3), it can be stated that  is a sparse approximation of x . In this problem  is no longer exactly reproducing

x , but it produces approximations of x (because of  ) and this brings more flexibility for the choice of x , also there will be a wider range of matrices for D . Below is an example of sparse representation and
sparse approximation using a randomly generated matrix and vector:

 0.9593 0.5472 1)  0.1386   0.1493

 0  0.2575 0.2435 0.2511 0.8308   0.2254 0.7537    0.7531 0.8407 0.9293 0.6160 0.5853     0.1285      0.2543 0.3500 0.4733 0.5497   0.2366    0    0.8143 0.1966 0.3517 0.9172  0.6390    0   0  0.2575 0.2435 0.2511 0.8308   0.2254 0.7537    0.7531 0.8407 0.9293 0.6160 0.5853     0  0.2543 0.3500 0.4733 0.5497    0.2366 0     0.8143 0.1966 0.3517 0.9172   0.6390    0 

 0.9593 0.5472 2)  0.1386   0.1493

Equation (1) is an example of sparse representation while equation (2) is an example of sparse approximation of a vector. It can be seen that compare to the length of the original signal which is 1.0403 the amount of error in approximation is relatively small (0.1338). 2.1.1 Uniqueness of sparsest approximation: The problem of sparse approximation shown in (1.1) can also be written in the form of:

11

min x 0 subject to x  D
x

(2.4)

The main goal of sparse approximation should be referred to this equation ((2.4)) hereafter. This equation has two major shortcomings: 1) The equality requirement of x  D is too strict and representing the vector x by a few elements of D can sometimes be unlikely. If some deviations are allowed it would have better results (approximation). 2) The sparsity measure is too sensitive to very small entries in  , a more appropriate measure would adopt a more forgiving approach towards such small entries. Based on the definition that was given in (2.4), generally sparse approximations are not unique. However there are some conditions that under which the sparse approximation can be unique. Some of these conditions are: Uniqueness via the Spark, Uniqueness via the Mutual-Coherence and Uniqueness via the Babel Function [12]. Below the Uniqueness via Spark condition is explained in detail, just to prove its uniqueness: Spark of a matrix which is a term that was first coined by Donoho and Elad [12] in 2003 is defined as the size of the smallest set of linearly dependent vectors of the matrix. This definition should not be considered the same with the Rank of a matrix which is the largest number of columns of the matrix which are linearly independent. Uniqueness via Spark condition states that a sparse representation  of x over the matrix (Dictionary) D is unique if:

 0
Proof:

Spark ( D) 2
assuming that 1 and  2 are sparse representations of x where

(2.5 )

1   2 and

1 0 ,  2 0 

Spark ( D) it can be stated that: 2

12

D1  D 2  x  D1  D 2  0  D(1   2 )  0  1   2  Spark ( D)
This contradicts the minimality of Spark (D).

2.2 Importance of Sparse Approximation:

Finding a sparse approximation is more than just an abstract mathematical problem. Sparse approximations have a wide range of practical applications. Vectors are often used to represent large amounts of data which can be difficult to store or transmit. By using a sparse approximation of the data the amount of space needed to store the vector would be reduced to a fraction of what was originally needed. Sparse approximations can also be used to analyze data by showing how column vectors in a given basis come together to produce the data. Many areas of science and technology have been greatly promoted by taking advantage of sparse approximation techniques. The very useful nature of sparse approximations have prompted much interest and research in recent years and there is no doubt that sparse approximations will continue to be of great interest in the coming years also. In recent years sparse approximation techniques have influenced the image and signal processing community in numerous ways such as denoising [6], image compression [5], feature extraction [13] and many more [12]. One of the fields that sparse approximation and dictionary learning technique have not yet deeply investigated is the case of image classification and segmentation. In our work we took one step further towards image classification and segmentation by taking advantage of sparse approximation and dictionary learning techniques.

13

In the following section, numerical algorithms and approaches for solving the problem of sparse approximation in (2.3) and/or (2.4) will be presented in details.

2.3 Numerical Methods and Algorithms for Sparse Coding

Sparse Coding or Atom Decomposition is the process of computing the representation coefficients

 based on the given signal y and the dictionary D . This process which in fact is no different than
solving the equation (2.3) or (2.4) is generally carried out through a "pursuit algorithm", which finds an approximate solution for the original problem in any of (2.3)or (2.4). The approximate solution is acceptable because the exact solution in determining the spars representation is proved to be a NP-hard problem [14]. In this section we briefly discuss several such pursuit algorithms, and their prospects for success. Matching Pursuit (MP) [15] and Orthogonal Matching Pursuit (OMP) [16], [17], [18] are among the very well known pursuit greedy algorithms that solve the sparse approximation problem. These methods are considered to be simple because in them the greedy algorithm selects the dictionary atoms sequentially and by computing the inner product between the signal and the dictionary atoms arranges some least squares solvers or projections. Another well-known pursuit approach in sparse coding is the Basis Pursuit (BP) [19] algorithm. By suggesting the l1 solution instead of l0 , BP introduces a convex method to solve the sparse approximation problem. The Focal Under-determined System Solver (FOCUSS) [20] is a very similar approach; instead, it uses the l p norm with p  1 as a replacement to the l0 norm. In FOCUSS for p  1 the similarity to the true sparsity measure is better, but the overall problem becomes non-convex, giving rise to local minima that may be misleading when searching for a solution. Lagrange multipliers are used

14

to convert the constraint into a penalty term, and an iterative method is derived based on the idea of iterated reweighed least-squares that handles the l p norm as a l2 weighted norm.

In all the algorithms there will be a stopping criteria which based on application can be the reconstruction error  (epsilon) or the desired number of non-zero coefficients for representation.

In this chapter greedy and convex sparse representation algorithms are presented. The chapter concludes with a short discussion about the stopping criterion. 2.3.1 Greedy Methods:

If the dictionary is orthogonal it is possible to solve the sparse representation problem in (2.3) or (2.4) by choosing the atoms with the largest inner product value between the dictionary atoms and the target signal. A conventional approach to perform such procedure is to find the atoms which have the maximum correlation with the signal, and then subtract the calculated contribution from the signal and repeat this procedure in an iterative process. In this section first Matching Pursuit (MP) [15] which is a straight forward representation method is presented. Following that Orthogonal Matching Pursuit (OMP) [16] which adds a least-square minimization process to MP in order to improve its performance is presented. OMP is considered to be the most admired leading sparse representation method in the literature. After OMP, batch-OMP and stage-wise OMP are also presented. The section is concluded with a brief discussion on the history of greedy methods.

15

2.3.1.1 Matching Pursuit Introduced in 1993 by Mallat et. al, [15] Matching Pursuit (MP) is a method capable of decomposing a signal into a liner expansion of waveforms that describe the time-frequency properties of the signal.MP is a greedy method which iteratively decomposes the signal into its representing waveforms (atoms). Considering a fixed dictionary D and a stopping criterion the Matching Pursuit algorithm [15] operates as follows: Purpose: solving the problem of min x 0 subject to x  D .
x

Inputs: original signal x , the dictionary D and the stopping criterion  Initialization: set k  0 and initialize:     Initial solution  0  0 Initial residual r 0  x  D 0  x Initial solution support S 0  Support{ 0 }  0
k 1 2 Sweep: Compute the errors  ( j )  min z j || a j z j  r ||2 for all j using the optimal choice
T k 1 z* / || a j ||2 j  aj r 2.

Main loop: k  k  1 and perform:



Update

Support:

find

a

minimizer,
k 1

j0 of  ( j ) : 1  j  m,  ( j0 )   ( j ), and
and update  k ( j0 )   k ( j0 )  z* j

update

S k  S k 1 { j0 } .
   Update Provisional Solution: set   
k

Update Residual: Compute r k  x  D k  r k 1  z* j0 a j0 . Stopping regulation: if || r k ||2   0 stop! Else: apply one more iteration.
k

Output: The approximated  vector after k iteration.
Table 2.1 Matching Pursuit algorithm for sparse approximation

The sweep step of the main loop indicates the greedy selection of the atoms of the dictionary that have the most correlation with the residual part of the signal. An important fact about MP is that in this algorithm, one index from the dictionary might be chosen several times when the dictionary is not orthogonal. This repetition occurs because the inner product between an atom and the residual does not account for the contributions of other atoms to the residual. In the update stage the current

16

coefficient vector gets updated to account for the effect of atom S k . Following that a new residual is computed by subtracting a component which is in the direction of the atom DS k . 2.3.1.2 Orthogonal Matching Pursuit Orthogonal Matching Pursuit or simply the OMP [16] [17] [18] is a greedy algorithm similar to the Matching Pursuit. It adds a least-squares minimization to the MP method to obtain the best

approximation over the dictionary atoms that have already been selected. This fact considerably improves the performance of OMP compare to MP. At each step OMP picks the dictionary atoms that have the maximal projection onto the residual signal; note that the dictionary elements should be normalized in this process. Following the selection of atoms the sparse representation coefficients are found by means of least-squares with respect to the atoms that are chosen so far. Given a signal x  R n and a dictionary D  Rn m with normalized atoms as its columns the algorithm starts by initializing k  0 and the residual r 0  x and performing the following procedure: In the sweep stage, the algorithm computes the reconstruction error values in the following form:

 ( j )  min z || a j z j  r
j

k 1

|| 
2 2

k 1 aT jr

2

|| a j ||

2 2

aj  r

k 1 2

(2.6)

Which indicates that finding the smallest error for representation is in fact no different than finding the largest absolute value of the inner product between the residual r dictionary D . In order to update the provisional solution stage, the algorithm minimizes the term D  x 2 with
2

k 1

and the normalized atoms of the

respect to  such that S is its support. Considering DS k as parts of the dictionary which contains atoms
k

of D that belong to this support the problem is to minimize the term DS k  S k  x where  S k is the non2

2

17

zero part of the vector  . The solution to this problem is obtained by zeroing the first derivative of the following quadratic equation:
T T k DS 0 k ( D k  k  x)   D k r S S S

(2.7 )

This relation implies that the columns (atoms) in D that are parts of the support S k are orthogonal to the residual r k and hence guarantees that in the following iterations, the already chosen atoms (columns) will not be chosen again for the support; this is the main reason for calling the method "Orthogonal" Matching Pursuit. The algorithm will iteratively run until convergence is reached, that is when it has selected a predefined number of atoms or when the reconstruction error (  ) is smaller than a given initial value. As a matter of fact the stopping criterion is based on the norm of the residual or/and the maximal inner product computed in the following atom selection stage. Table 2.2 demonstrates an overview of the OMP algorithm. Purpose: solving the problem of min x 0 subject to x  D .
x

Inputs: original signal x , the dictionary D and the stopping criterion  Initialization: set k  0 and initialize:     Initial solution   0
0

Initial residual r  x  D  x
0 0

Initial solution support S 0  Support{ 0 }  0
k 1 2 Sweep: Compute the errors  ( j )  min z j || a j z j  r ||2 for all j using the optimal choice
T k 1 z* / || a j ||2 j  aj r 2.

Main loop: k  k  1 and perform:



Update Support: find a minimizer, j0 of  ( j ) : j  S k 1 ,  ( j0 )   ( j ), and update

S k  S k 1 { j0 } .
 Update Provisional Solution: Compute x ,the minimizer of || D  x ||2 2 subject to
k

Support{x}  S k .
 Update Residual: Compute r  x  D .
k k

18



Stopping regulation: if || r k ||2   0 stop! Else: apply one more iteration.
Table 2.2 Orthogonal Matching Pursuit pseudo algorithm for sparse approximation

Output: The approximated  k vector after k iteration. A persuasive method, OMP, unlike many methods has the advantage of being able to sparsely represent a signal with an a-priori fixed number of non-zero elements, which makes it a desired approach in dictionary learning methods [21]. The fact that OMP is capable of representing a signal with an a-priori knowledge of non-zero elements is what we use later in our novel sparse classification approach. Since OMP is a successful method in signal sparse representation, different variations of it has been developed. Based on a categorization introduced by M.Aharon [22]these variants are:   skipping the least-squares and using the inner product itself as a coefficient; applying least-squares per every candidate atom, rather than just using inner-products at the selection stage;  projecting all non-selected atoms onto the space spanned by the selected atoms before each new atom selection;  doing a faster and less precise search where instead of searching for the maximal inner product, a nearly maximal one is selected, thereby speeding up the search; The following two sections introduce two of OMP variants known as Batch OMP [23], [24] and Stagewise OMP [25]. 2.3.1.3 Batch Orthogonal Matching Pursuit: One of the conventional algorithms that reduces the computational complexity compared to OMP is Batch Orthogonal Matching Pursuit (B-OMP) [23], [24]. This method reduces the computational time needed for calculating the inner product (correlation) between the residual vector and atoms of the dictionary. The algorithm assigns a predefined matrix G  DT D in the memory to eliminate the unnecessary computations. 19

Considering the linear equation x  D for the signal x , dictionary D and the sparse representation vector  , it can be stated that:

   DT x   DT r  DT ( x  DI ( DIT DI )1 DI T x)     GI (GI , I ) 1 I

(2.8)

Equation (2.8) indicates that the relation (correlation) between the residual vector and the dictionary atoms is calculated through the matrix G and is independent of having the residual vector. Just like OMP the stopping criterion can be imposed either based on the number of non-zero coefficients or the reconstruction error which basically is the norm of the residual. If the stopping criterion is chosen to be based on the norm of the residual, it can be calculated based on the value of error in the current and the previous iteration:

rk  x  D k  x  D k  D k 1  D k 1  rk 1  D  k 1   k 

(2.9)

The orthogonality property of the OMP implies that the residual is perpendicular to the signal
T approximation ( (rk ) D k  0 ) [24] ; Hence the residual norm can be obtained as:

rk

2 2

 (rk )T rk  (rk )T (rk 1  D   k 1   k )  rkT rk 1  rkT D k 1
2 2 2 2 2 2

 (rk 1  D   k 1   k )T rk 1  (rk )T D k 1  rk 1 2  (rk 1 )T D k  (rk )T D k 1  rk 1 2  ( x  D k 1 )T D k  ( x  D k )T D k 1  rk 1 2  x T D k  x T D k  rk 1 2  (rk  D k )T D k  (rk 1  D k 1 )T D k 1  rk 1 2  ( k )T DT D k  ( k 1 )T DT D k 1  rk 1 2  ( k )T G k  ( k 1 )T G k 1
(2.10)

The last equation of the above formulation ( rk 1

2 2

 ( k )T G k  ( k 1 )T G k 1 ) is nothing but the norm

of the residual or basically the error which is computed in each iteration and makes the error update stage faster. The pseudo algorithm of B-OMP is demonstrated in Table 2.3. 20

Purpose: solving the sparse approximation problem in an efficient order. Inputs: original signal x , the dictionary D and the stopping criterion  , x '  DT  , Initialization: set k  0 and initialize:   

G  DT D

x '  DT  I  () , L  [1] ,  0  x T x ,   0 ,     ,  0  0 ,
j : max  
If n  1 o o

Main loop:  

w : solve
L L T w 

Lw  GI , j
  1  wT w   0

       

Update I  I  j

y : solve

Ly   I

 : solve

LT  I  y

  GI  I
    

 n   IT  I
 n   n1   n   n1

n  n 1

If  n1   n then go to step 2 Output: sparse representation vector  k after k iteration
Table 2.3 Batch Orthogonal Matching Pursuit algorithm for fast sparse approximation

2.3.1.4 Stagewise Orthogonal Matching Pursuit: Stagewise Orthogonal Matching Pursuit (St-OMP) [25] is another efficient technique based on OMP which provides a sparse solution in the case of underdetermined sparse representation problems. Inspired by Orthogonal Matching Pursuit and the Least-Angle Regression (LARS), StOMP is especially tailored for the cases in which dictionary D is random (such as in compressed sensing problems).

21

The goal of StOMP is to reduce the reconstruction error in a stagewise trend by approximating the solution of x  D 0 in which  0 is the sparsest (best) solution. The algorithm extracts multiple atoms in each stage (stage-wise) whereas conventional OMP finds only one atom per iteration and hence the number of iteration and the computational time in StOMP is significantly reduced compared to OMP. Similar to OMP, the process of selecting atoms is performed through a Matching filter ( x  DT  ) and the reconstruction accuracy is computed by subtracting the reconstructed signal in each stage from the original signal ( z  x  x0 ). If we assume that the dictionary D  RnN is randomly chosen with its columns being independent from one another, then the elements of the vector z will approximately have a Gaussian distribution with a standard deviation of  as follows:



x0

2

(2.11)

n

Just like OMP the St-OMP initiates the residual with the input signal and in each stage the correlation between the residual rs and dictionary atoms are calculated (Match Filtering). A hard threshold is applied to the output of the Match filter to select the atoms with maximum correlation. The index of selected atoms will be saved and after wards the input signal is projected to these selected atoms:

( xs ) Is  ( DITs DIs )1 DITs y

(2.12)

The result of the algorithm is the sparse representation vector (  s ) as well as an updated residual ( rs  x  D s ) gained from the final stage, note that s indicates the number of the stages. Like other methods the algorithm runs for a predefined number of stages or until a desired reconstruction error is achieved. The threshold in each iteration is computed based on considering the

22

noise to be Gaussian with a standard deviation  . The block diagram of St-OMP is proposed by Donoho et al [25]as follows:

Figure 2-2 Block diagram presentation of St-OMP method proposed by Donoho et al, image is acquired from [25].

2.3.1.5 History of greedy methods for sparse approximation: Greedy algorithms for sparse approximation first came to existence in the statistics literature in the 1950s to solve subset selection methods. The first greedy technique is called Forward Selection which is an intricate version of MP. Details on preliminary greedy algorithms are provided in [26]. Greedy sparse representation methods as we know them today came to existence in the late 1980s and early 1990s. MP was originally invented in 1981 [27] but it was introduced to the signal processing community by Mallat and Zhang [15] in 1993. OMP was developed independently by many researchers around the globe; the earliest references cited goes back to 1989 in the paper of Chen, Billings and Luo [16]. It was first introduced to the signal processing community around 1993 by Pati, Zhang and Mallat [17] [28].

23

2.3.2 Convex Relaxation Methods:

The presence of l0 norm makes the sparse approximation problem a combinatorial problem. A conventional approach in solving combinatorial problems is to replace the original problem with a relaxed version of the original problem which can be solved more efficiently. In this section the Basis Pursuit algorithm [29] which is known as the most famous convex relaxation approach in sparse representation is presented. For other convex relaxation techniques such as "Error-Constrained Approximation" and "Subset Selection" the readers are advised to study [30] and references therein.

2.3.2.1 Basis Pursuit: Among the sparse representation techniques, Basis Pursuit [29] is considered more as an optimization technique rather than a direct algorithm. Basis Pursuit (BP) is a method to decompose signals by means of over-complete dictionaries; Note that an over-complete dictionary is basically a concatenation of multiple dictionaries, these dictionaries can be image based (constructed directly from image patches) or they can be composed of basic wave forms such as Wavelets, DCT, Curvelets, Ridgelet and many more. The purpose of creating such dictionaries is to combine the characteristics of different dictionaries. A detailed explanation about dictionaries will be presented in chapter 3. In order to explain how BP works consider an input signal (image) x and an over-complete dictionary D with d i as its atoms, the representation equation can be defined as:

x    i j di j  r ( m )
j 1

m

(2.13)

24

In which r ( m ) is the residual. Unlike many other sparse representation methods, BP uses a convex optimization problem which minimizes the l1 nom of the coefficients in the sparse representation vector [34]. Substituting l0 norm with l1 norm leads to a non-linear optimization problem which results in a higher sparsity. Another feature that emerges in the BP method is the ability to solve the optimization problem globally and therefore it sturdily finds the global optimum representation. This is a feature that a method like MP which is based on l0 norm cannot perform. The sparse representation problem in this case can be rewritten as:

min  1 subject to x  D

(2.14)

The solution to the optimization problem in (2.14) lies within an algorithm known as Primal Dual Interior Algorithm [31] introduced by Florine et. al, discussed briefly below.
2.3.2.1.1 Primal Dual Interior Point Algorithm for linear programming:

The Primal Dual Interior Point (PDIP) described in [31] is a linear programming method aiming to solve the following optimization problem:

x  arg min cT x subject to Ax  b x  0 ,
x

(2.15)

In which c, x 

n

, b

m

and A is a matrix with the size of m  n . There is a dual problem associated

with the primal problem:

arg max bT y subject to AT y  s  c, s  0
y,s

(2.16)

Where y  R m and s  R n is called the dual slack [31]. Introduced by Florian et. al [31] cT x  bT y is called the duality gap which acts as the termination criterion in the linear programming. The equation in (2.16) can be rewritten as:

25

AT y  s  c Ax  b XSe  0 ( x, s )  0

(2.17)

Where e  R n , [ei ]  1,..., n and X and S are diagonal matrices defined as:
 [ x]1 0  0 [ x]2 X    0  0 0   [ s]1 0  , 0 [ s]2 0  S    0   0 [ x]n   0

0

0

0   0  0   [ s]n 

(2.18)

In order to solve the system of equations in (2.18) the equations in (2.17) can be rewritten in the following format:

 AT  s  c    F ( x, y, s)   Ax  b   0, ( x, s)  0  XSe   

(2.19)

Aiming to solve equation (2.19) iteratively by considering ( x ,  y ,  s ) as the difference between the results in each iteration the Jacobean of F can be defined as:

 x   F ( x, y, s)   y    F ( x, y, s)   s  
Based on (2.20) and (2.19) the final formulation can be written as:

(2.20)

 0nm   Amn  Snn 

T An m 0mm 0nm

0nn   x   0n1      0mn    y    0m1  X nn   s      XSe   

(2.21)

th th The value of ( x, y, s) in the K  1 iteration which is the value of ( x, y, s) in K iteration plus

( x ,  y ,  s ) will be iteratively modified so that the amount of duality gap reaches a value less than the
error. The following table demonstrates the pseudo algorithm of (PDIP):

26

Purpose: Determining ( x0 , y 0 , s 0 ) strictly feasible by solving the primal dual interior point Inputs: x, y and s Initialize: set k  0 Repeat:  Set

 k [0,1] and  k 
Solve system of equation in (2.21) to obtain ( x k ,  y k ,  s k )  Set

xT s n

( xk 1 , y k 1 , s k 1 )  ( xk , y k , s k )   k ( xk ,  y k ,  s k )
Choosing  k so that ( x k 1 , s k 1 )  0.  K  K 1 Until: Convergence
Table 2.4 General Primal Dual Interior Point algorithm [31]

2.3.2.1.2 Solving the Basis Pursuit optimization problem:

Having discussed the Primal Dual Interior Point (PDIP) algorithm to solve the l1 norm problem, we can take a similar approach to solve the Basis Pursuit (BP) problem in (2.14). BP finds a sparse solution with n non-zero coefficient corresponding to n columns of D . In order to solve the problem in BP by taking advantage of PDIP the patches in the signal are split into two positive sub patches such that s  u  v and variables x  [u of the signal's patch can be represented as:

v]T and A  [ D  D] are defined. The l1 norm

s 1  u 1  v 1   ui   vi  cT x

(2.22)

The above formulation is not very different from the problem demonstrated in PDIP by Florien et. al [31], and a similar approach can be utilized to solve this problem. The following table demonstrates the pseudo algorithm of the Basis Pursuit method:

27

Purpose: solving the l1 optimization problem in BP Initialize: x, y and s   End
Table 2.5 Basis Pursuit optimization problem

Solve: Solving equation (3.16) to find  x

y s 

Update: x, y and s Check convergence criteria, if not converged go to step 2

2.3.3 Stopping Criterion:

So far a few greedy sparse representation algorithms have been introduced; in all of these methods there is one fact which is universal and that is the algorithms stopping criterion. In general stopping criterion can be classified into three major categories of EXACT, SPARSE And ERROR, each of which is useful for a specific sparse approximation problem which can be summarized as follows:  EXACT: This stops the algorithm after the norm of the residual r k equals zero. In cases in which recovering a sparse input signal is desired, this criterion is usually used.  ERROR: This may halt the algorithm when the norm of the residual r declines below a specific defined threshold. In dealing with error constrained problems this criterion is more appropriate.  SPARSE: This criterion will stop the algorithm whenever a predefined number of atoms have been selected from the dictionary. Using this criterion will result in a sparse vector with the exact length of the predefined number of atoms.
k

28

Chapter Three: Dictionary Learning
3. D

3.1 Quest for a proper dictionary

A fundamental question in the field of sparse representation of signals and its deployment to different applications is the selection of the proper dictionary to represent the data over. How can a proper dictionary be wisely chosen to perform well on the input signal so it can have the optimum efficiency for that specific purpose? This chapter of this thesis is dedicated to explain the nature of different dictionaries, and some of the most significant dictionary learning methods. First a brief discussion on the difference between fixed and learned dictionaries is presented following that some of the important fixed dictionaries and the mathematical approaches behind them is offered. After fixed

29

dictionaries the concept of dictionary learning in the form of two very important and famous dictionary learning methods is discussed, finally the chapter ends with a section on discriminative dictionaries.

3.2 Fixed or Learned Dictionaries?

The quest for a proper dictionary so that it can be used for a specific application can reach the point that one considers pre-constructed or fixed dictionaries. Such dictionaries are the discrete Cosine Transform (DCT), Wavelets, Contourlets, Curvelets and many more. Some of these proposed dictionaries (also known as transforms) are accompanied by a detailed theoretical analysis establishing the sparsity of the representation coefficients for such simplified content of signals. This is usually done in terms of representation of signal using the best non-zero coefficients from the transform. Whether in discrete or continuous domain, one can alternatively utilize a tunable selection of a fixed dictionary. As an example wavelets and wavelet packets can be mentioned which suggest an acceptable control over timefrequency subdivision, tuned to optimize performance over a specified instance of signals. While pre-constructed (fixed) dictionaries can typically lead to a fast transform, they are limited in sparsifying the signals which they are meant for. Furthermore in most of the cases fixed dictionaries are restricted only to a specific class of signals (or images) and cannot be used for an arbitrary class of new input signals. Although this family of dictionaries are suitable to address some of the sparse representation applications, their limitations lead to a new idea of learning the dictionaries based on the application that they are meant for. The above mentioned learning procedure works by building a training dataset of signals similar to the signals that are supposed to be used based on the desired application, and constructs an empirically learned dictionary in which its atoms (elements) are generated based on an underlying empirical data rather than a theoretical (or mathematical) model like fixed dictionaries. This learned dictionary can 30

then be used as a fixed dictionary itself in the specific application that it is meant for. Having mentioned the advantages of learning the dictionaries it should be mentioned that learning a dictionary itself is a computationally expensive procedure compared to using pre-constructed dictionaries.

3.3 Fixed dictionaries
In this section some of the famous fixed dictionaries are presented. 3.3.1 Time-Frequency Dictionaries Fourier transform is the most well-known method in extracting a signal's frequency characteristics. Fourier transform is capable of representing a signal as its frequency domain components. Since by definition Fourier transform represents the signal in the form of exponentials (sine and cosine) and knowing the fact that sinusoidal functions are pair wise orthogonal, all signals can be represented as a linear combination of these orthogonal coefficients. The representation coefficients themselves are obtained using the inner product of the given signal with the Fourier basis:


x(t ) 



 X ( f )e

i 2 ft

df

(3.1)

Where X ( f ) represents the frequency representation of the signal x .

3.3.2 Discrete Cosine Transform Discrete cosine transform (DCT), first introduced by Desai. et. al [32]and Watson. et. al [33] to the image processing community expresses a sequence of finite data points in terms of a summation of cosine functions oscillating at different frequencies. The use of Cosine instead of Sine function can be decisive and useful in specific applications such as compression in which fewer functions are needed to approximate a signal. A typical signal can be decomposed with DCT as:

31

M 1 1 (2m  1)k , m  1, 2,..., M  1 X (m)  Gx (0)   Gx (k ) cos 2M 2 k 1

(3.2)

In which Gx (k ) stands for the k th coefficient. By definition DCT is not much different from Fourier transform, as a matter of fact DCT is similar to Discrete Fourier Transform (DFT) but only with real parts and no imaginary element. Expanding equation (3.2) from one dimensional into a two dimensional space the DCT transform for 2D signals or images can be expressed as follows:

X (m, n)   Gxy (k , l ) cos
m 0 n 0

M 1 N 1

(2k  1)u (2 y  1)v , u  1,...., N  1, v  1, 2,..., M  1 cos 2n 2m

(3.3)

Figure 3-1 demonstrates a two dimensional DCT based dictionary with 8  8 size patches.

Figure 3-1 A DCT dictionary with 8*8 size patches

3.3.3 Wavelet and Wavelet Transform A wavelet is a wave-like oscillation with an amplitude that starts out at zero, increases, and then decreases back to zero. In wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands or similar subspaces. The Wavelet transform offers a resizable structure for atoms. This structure of different frequencies is based on each atom size so it can address the problem of having different structure sizes. As a matter of fact, the flexible time-frequency windows (wave forms) in the wavelet transform provides a non-uniform frequency bandwidth in which the 32

frequency resolution is higher at lower frequencies and vice versa. These variable size windows or the so called wavelets are generated by scaling and shifting the basic wavelet  (t ) [34] as follows:

 a ,b (t ) 

1 t b ( ) a a

(3.4)

Where a is a positive value that defines the scale, b is a real number that defines the shift and the pair

(a, b) defines a point in the right half-plane. The actual wavelet transform is defined by calculating the
inner product of the above basis function in the following integral:

1 t b F (a, b)  ( )  f (t ) * ( )dt a a 



(3.5)

It is proven [34] that if a  2 m and b  2 m k the original signal f (t ) can be recovered using the wavelet series as:

f (t ) 

m , kZ

  f ,

mk

(t )   mk (t )

(3.6)

In which  .,.  is indicator of the inner product operator and wavelet basis functions, {  mk } , are supposed to be orthonormal.

The wavelet transform is a perfect tool in representing 1D signals like audio. It is sensitive to highfrequency changes while it detects low-frequency terms in the signal. However in higher dimensions like images (2D) it fails to track directional information. As a matter of fact it performs very well in representing horizontal and vertical data while resulting in a non-convincing representation for other directions. Moreover, the wavelet transform is sensitive to discontinuities in the edge points. However it fails to accurately represent smoothness along the contours in images. The following figure is a demonstration of a wavelet transform based dictionary.

33

Figure 3-2 A Har base wavelet dictionary

3.4 Learning the Dictionary

This section describes the learning methodologies and justifications required in order to construct a proper dictionary so that the data can be represented over. Assuming that a training data set is given, if this dataset is generated by a fixed but unknown source or model, the question would be: can this training dataset allow us to be able to recognize the original generating model and specifically the dictionary? This is a problem that was first looked into by Field and Olshausen [35] in 1996 when they were trying to establish a similarity between atoms of a dictionary and the population of simple cells in the visual cortex. Working on simple cells they considered the possibility of learning a dictionary that can model the evolutionary process which led to the presented collection of simple cells; and to a good extent they were able to do so. Later works by other scholars around the world such as Elad [6], Aharon [22], Engan [12] and many more completed this work and resulted in novel dictionary learning methods, however this topic is still open and there is lot of room to grow in the field of dictionary learning. Here we present "Method of Optimal Directions" (MOD) and K-SVD algorithm as two of the most well known dictionary learning algorithms that are being used by researchers in recent years.

34

Having the problem of sparse representation in (1.1), dictionary learning can be addressed as follows: The aim is to find a proper estimation of the dictionary D by assuming that the stopping criterion  (model deviation) is known in the following optimization problem:

min  xi
D , xi i 1

M

0

subject to xi  Di

2

  ,1  i  M

(3.7)

Equation (3.7) describes any given signal xi as the spars representation xi over an unknown dictionary D and aims to find not only the representation but also the unknown dictionary. Clearly if a solution to this equation can be found that can meet the restrictions on the number of non-zero coefficients a candidate feasible model for the dictionary is also found. Just like the problem of sparse representation, dictionary learning can also be presented in the form of constraining the sparsity and obtain the best fit for it:

min  xi  D i 2 subject to xi
2 D, x i 1

M

0

 N0 , 1  i  M

(3.8)

In which N is the number of non-zero coefficients for representation. Similar to the problem of sparse representation the uniqueness of a gained dictionary can be derived by using the concept of Spark described in chapter 1. Aharon et al [22] showed that for the case of   0 , if there exists a dictionary

D0 and a sufficiently diverse database of samples that all of which are representable using at most
k  sparkD0 / 2 number of atoms, the derived dictionary is unique to achieve the desired sparsity ( N )
for all the elements in the training dataset. 3.4.1 Method of Optimal Directions (MOD): Method of Optimal Directions (MOD) is an algorithm which was first developed by Engan et. al [36] to learn dictionaries for sparse representation. The core idea in this method is to look at the problem in equation (3.7) or (3.8) as a nested minimization problem consisting of an inner minimization 35

of the number of non-zero coefficients in sparse representation vector  over a fixed dictionary D and an outer minimization problem over the dictionary D itself. The nested minimization approach in the problem can result in an alternating minimization approach in solving the problem; developing an iterative algorithm at the k th stage the algorithm uses the dictionary Dk 1 from the previous stage and
 solves M instances of the l0 norm, one for each dataset entry xi by using the dictionary Dk 1 . This will

result in the matrix (or vector)  k and then the algorithm can be solved for Dk by Least-Squares:

Dk  arg min X  D k
D

2 F

 X  ( k )
T k

T 1 k

(3.9)

 X  k
The purpose of the Frobenious norm ( . F ) is to evaluate the error. The columns of the gained dictionary can be re-scaled and k will increment until the convergence criterion is met. The block diagram of the proposed method is demonstrated in Table 3.1. Purpose: To train the dictionary D so it can sparsely well represent the data xi by means of approximating the solution to (3.7) or (3.8). Initialization: set k  0 and:   Initialize Dictionary D : build an initial D0  R nm , usually by using random entries. Normalization: normalize the columns of D0 .

Main loop: increment k by 1 and:  Sparse Coding Stage: use any of the pursuit algorithms to approximate the solution of:

^i  arg min xi  Dk 1 2 subject to  0  k0 
2



^ i to form the matrix  k Obtaining the sparse representations 
MOD dictionary update stage: updating formula:

Dk  arg min X  D k
D

2 F

T T 1  X k ( k k )
2 F



Stopping Criterion: if the amount of change in X  D k another iteration.

is small enough, stop. Else: apply

Output: Desired dictionary Dk
Table 3.1 The MOD dictionary learning algorithm

36

Figure 3-3 demonstrates the MOD behavior in learning a dictionary on a synthetic data in terms of representation error and number of recovered atoms.

100

0.22 0.2

Number of Recovered Atoms

Representation Error

80 60 40 20 0 0

0.18 0.16 0.14 0.12 0.1

10

20 30 Iteration

40

50

0.08 0

10

20 30 Iteration

40

50

Figure 3-3 The MOD dictionary learning behavior

3.4.2 K-SVD Dictionary Learning Inspired from K-means clustering technique by M. Aharon and M. Elad [21] K-SVD dictionary learning uses a different approach in updating the atoms (columns) of the dictionary by handling them sequentially. K-SVD update stage works as it keeps all of the atoms of the dictionary fixed except for the
' th one, d j0 . This one column will be updated along with the coefficients that multiply it in  . The j0

dependency on d j0 can be gained by rewriting equation (3.9):

X  D

2 F

 X   d j
j 1

m

2 T j F 2

(3.10)

T  ( X   d j T j )  d j0  j0 j  j0 F

The term in the parentheses is known as a pre-computed error matrix:

E j0  X   d j T j
j  j0

(3.11)

37

With X as the data matrix, optimization of equation (3.10) lies in the hands of d j0 and  T j0 , the optimal value of d j0 and  T j0 is called the "rank-1" approximation of the error matrix E j0 which can be gained via the SVD method. However an SVD based approach would usually yields in a dense representation vector

T j which introduces more non-zero entries in the sparse representation vector  . In order to maintain
0

the number of non-zero entries of the sparse vector to a minimum level (acceptable sparse value) a subset of columns of E j0 which are those elements that correspond to the original signal from the dataset should be taken. These components are usually the ones using the elements of the  T j0 row in the vector  . In this way only the non-zero coefficients in  T j0 vary and the sparsity is maintained. In order to remove the non-relevant columns a limitation operator Pj0 that multiplies E j0 from the right is defined. This Pj0 matrix has M (number of overall examples in the dataset) rows and M j0 columns
' th (number of elements that use the j0 atom). In order to choose the non-zero entries only, a restriction

R T T on the row  T j0 is defined as: ( j0 )   j0 Pj0 . For the intention of updating the atom d j0 and also the

corresponding coefficients x R j0 in the sparse vector, a rank-1 approximation for the sub-matrix E j0 Pj0 is applied via SVD algorithm which results in a simultaneous update. An alternate approach in updating the dictionary atoms and elements of the sparse vector can be proposed by fixing the atom d j0 first and
R updating  j0 by a plain Least-Squares problem:

min E j0 Pj0  d j0 ( ) R
 j0

2 R T j0 F

 
R j0

PjT ET j0  j0 0

j

2 2

(3.12)

0

Once the sparse coefficient is updated then the atom d j0 will be updated:

38

min E j0 Pj0  d j0 ( )
d j0

2 R T j0 F

 d j0 

E j0 Pj0  R j0

(3.13)

R j

2 2

0

Using the above formulas the desired updated dictionary can be achieved after a few iterations. If the above process is considered for the case of sparse representation with only one non-zero element (

N0  1 ) the problem is simplified into a K-means clustering task. For the cases of more than one nonzero coefficient the algorithm performs as an SVD operation for each of the K different subsets and thus the name K-SVD.

Table 3.2 demonstrates the pseudo algorithm of the K-SVD algorithm.

Purpose: To train the dictionary D so it can sparsely well represent the data xi by means of approximating the solution to (3.7) or (3.8). Initialization: set k  0 and:   Initialize Dictionary D : build an initial D0  R nm , usually by using random entries. Normalization: normalize the columns of D0 .

Main loop: increment k by 1 and:  Sparse Coding Stage: use any of the pursuit algorithms to approximate the solution of:

^i  arg min xi  Dk 1 2 subject to  0  k0 
2



^ i to form the matrix  k Obtaining the sparse representations 
K-SVD Dictionary-Update stage: Repeating for j0  1, 2,..., m the updating procedure is as follows:  Defining the data samples using the atom d j0

 j0  i 1  i  M ,  k  j0 , i   0 .
 Compute the residual matrix





E j0  X   d j T j ,
j  j0



Restrict E j0 (pre-computed error) by cho0sing only the columns corresponding to  j0 and obtain E j0 .
R R T Apply SVD decomposition E j0  U V , update the dictionary atom d j0  u1 , and R representations by  j0   1,1.v1 .



39

Stopping criterion: if the amount of change in X  Dk k apply another iteration. Output: learned dictionary D .

2 F

is small enough (defined by user) stop, else;

Table 3.2 K-SVD dictionary learning Pseudo algorithm

In order to be able to analyze the behavior of K-SVD more clearly, the method is implemented on synthetic data. The following figures illustrate K-SVD's performance in terms of number of recovered atoms and the representation error on the synthetic data.

Figure 3-4 K-SVD dictionary learning behavior

The fact that dictionary learning is a generalization of a clustering approach reveals that: 1. Just like K-means neither K-SVD nor MOD do not guarantee to reach a global or even a local minimum of the penalty function in (3.8) since both methods have the potential to get stuck in a steady state solution. 2. Regarding the convergence of the algorithms, since both algorithms use a pursuit sparse representation technique, having a monotonic non increasing penalty value based on the number of iterations is not guaranteed. In order to have a proper comparison between the above mentioned dictionary learning algorithms (K-SVD and MOD), both of them have been implemented on the same data and the following results have been gained: 40

100

0.22

Average Representation Error

Number of Recovered Atoms

0.2 0.18 0.16 0.14 0.12 0.1 0.08 0 10 20 30 Iteration 40

MOD K-SVD

80

60

40

20 MOD K-SVD 0 0 10 20 30 Iteration 40 50

50

Figure 3-5 K-SVD behavior VS MOD behavior

Figure 3-6 shows the famous Barbara Image, and the MOD and K-SVD learned dictionaries based on this image. The representation error of the image for both dictionaries is also displayed.

Figure 3-6, Original Barbara Image (Top), MOD Dictionary (bottom left), K-SVD Dictionary (bottom middle) and the representation error graph (bottom right).

41

3.5 Discriminative Dictionary Learning

In the previous section two famous dictionary learning methods have been demonstrated, however there are other dictionary learning algorithms ( [37], [38] and references therein). Most of the dictionary learning algorithms in image processing focus on applications such as image denoising [6], compress sensing [5]and image restoration [7], etc. Most of these dictionary learning techniques are not suitable for the purpose of image classification, this is due to the fact that in the process of learning a dictionary there are no constraints on separating the learned patches based on the specific class that they belong to. In fact dictionary learning creates a new space in which the sparse representation of the data can be performed with more accuracy. However if the learning algorithm does not consider any restrictions on separating the patches based on the class that they belong to, the resulting space is of no use for the purpose of classification. In this section three famous dictionary learning techniques that consider class specific limitations in the process of learning are presented. Using these methods result in a new space in which sparse representation of the signal can be performed based on the fact that each patch belongs to a specific class of the training dataset. This will result in the process of labeling the patches based on the dictionary that they belong to. 3.5.1 Dictionary Learning with Structured Incoherence and Shared Features for Classification and Clustering The work presented by Ramirez et al in [39] is among the well-known methods in learning a dictionary for classification and clustering (discriminative dictionary learning). In this framework unlike K-SVD type of approaches that finds a set of centroids which best fits the data, this method optimizes for a set of dictionaries, one dictionary for each class. As a matter of fact the method trains a dictionary consisting of various sub-dictionaries, one for each class. An incoherence promoting term is utilized to

42

force the class specific dictionaries (sub-dictionaries) to be as independent as possible. It is a robust method that is suited to handle large data sets in both supervised and unsupervised type of approaches. The algorithm uses l1 regularization in its sparse coding stage. Given K clusters, the method learns K dictionaries to represent the data, and then associates each signal to the dictionary for which the "best" sparse decomposition is obtained. In other words, the test data (signal or image) is represented "only" with the atoms of the sub-dictionary that it is assigned for that specific class. The first block to build such a clustering technique is based on the following equation:

min 
Di ,Ci

K

i 1 x j Ci

 R( x , D )
i i
n Ki

(3.14)

Where Di   d1 , d2 ,..., d Ki  

is the class Ci dictionary with K i atoms, and R as the function

measuring how well the sparse representation of signal xi over the dictionary Di is performed. In order to make the sub-dictionaries as independent as possible a new term " Q "is added to the above block:

min 
Di ,Ci

K

i 1 x j Ci

 R ( x , D )    Q( D , D )
i i i j i j

(3.15)

This new term will work as an energy formulation optimizing the dictionaries to properly represent the corresponding class strongly ( R ) while at the same time makes a weak representative for other classes (

Q ). The proposed Q function for this case is equal to:

Q( Di , D j )  DiT D j

2 F

(3.16)

Before describing the nature of the constraint R we recall that the original sparse approximation problem introduced at the beginning of this thesis as: min x  Da 2   a 1 with  acting as the
2 a

tradeoff parameter between reconstruction error and sparsity. On the other hand creating a reconstructive dictionary to represent the data that has been discussed in details in the previous section

43

results from:

{ai }i 1,2,..., m

min


i 1

m

xi  Dai 2 2  ai 1 (the term  sometimes gets neglected). For the purpose of

discriminative dictionary learning and to have a good within class representation and between class discrimination the proposed constraint for R is defined as:

R( x, D)  x  Da
or

2 2

(3.17)
a

^ ( x, D)  min x  Da 2   a R 2 1
Imposing the defined Q and R in the original discriminative dictionary learning problem in (3.15) will result in the following formulation to learn a dictionary with big between class and a small within class scatter:

  X i  Di Ai  { Di , Ai }i1,... K i 1 
K

min

mi    aij     DiT D j  2 1 j 1 i j  2

2 F

(3.18)

Experimental results using this approach on different datasets shows an accuracy of 85% to 95% for different datasets:

Figure 3-7 Bike detection on the Graz dataset using the described method. The detected object is displayed with hot pixels. Image is acquired from [39].

44

Figure 3-8 Brodatz dataset classification using the described method, classification results of the images on the top is shown in the bottom row. Image belongs to [39].

3.5.2 Discriminative K-SVD

A popular dictionary learning algorithm, K-SVD has no discriminative behavior; in fact it is a proper method to learn reconstructive dictionaries which can be used within image representation, compression and denoising applications. In order to utilize this popular method in a clustering and/or classification frame work it needs to be learned under certain conditions which can impose the discriminative behavior to the trained dictionary. Discriminative K-SVD (DK-SVD) dictionary learning algorithm [40], [41] adds the necessary discrimination criterion to the conventional K-SVD as well as maintaining its reconstructive features in order to perform local image discrimination tasks. This is done by proposing an energy formulation with both sparse reconstruction and class discrimination components getting optimized during the processes of learning and updating. DK-SVD tries to learn multiple class specific dictionaries which are simultaneously reconstructive for one class and discriminative for the other(s). It uses the reconstruction error of the dictionaries on image patches to achieve a pixelwise classification rate. This learning technique has the advantages of not only learning redundant non-parametric dictionaries, but also the sparse local representations are learned with an explicit discriminative purpose. As in K-SVD and sparse representation pursuit algorithms DK-SVD uses the residual vector as well. The residual vector r is defined as: 45

 * ( x, D)  arg min x  D

k

2 2

such that 

0

L
(3.19)

r ( x, D,  )  x  D

2 2 2 2

r * ( x, D)  x  D * ( x, D)

As mentioned above the algorithm takes advantage of the residual vector just like the K-SVD, but the method increases the discriminative power of r * ( x, Di ) with the goal of dictionary Di associated to a specific class Si should be "good" to reconstruct this class but at the same time "bad" for representing other classes. A discriminative term for different classes (i  1, 2,...N ) is defined as follows:

 N   ( y j  yi )  C ( y1 , y2 ,..., yN )  log   e   j 1 
 i

(3.20)

Which is close to zero when yi is the smallest value among y j , and provides an asymptotic linear penalty cost  ( yi  min j y j ) otherwise. A positive value for  results in a high penalty cost for each missed classified patch. Having N dictionaries for N distinct classes the question is to solve:

{ D j } j 1... N

min

i 1... N
lSi

 C  R ( x , D )
* i l j

N j 1

   r ( x , D )
* l i

(3.21)

In which   0 is the parameter controlling the trade-off between reconstruction and discrimination (The higher  is, the more reconstructive is the dictionary). Solving this problem makes the resulting dictionary Di good for its own class other than any other class. Like most of the dictionary learning methods DK-SVD consists of two major parts: sparse coding and dictionary update. In DK-SVD the sparse coding step is no different than K-SVD or MOD, that means keeping the dictionary atoms fixed and computing the sparse decomposition of the patches; Dictionary update stage on the other hand is a bit different than other methods. This is the part that makes the algorithm discriminative by updating the dictionary atoms sequentially while letting the corresponding

46

 coefficients associated with an atom in the sparse coding stage to change as well. Making the update
stage similar to that of MOD by converting the optimization problem of (3.21) in the following format:

{ D j } j 1... N

min

i 1... N
lSi

 C  R( x , D , )
i l j lj

N j 1

   r( x , D , )
l i li

(3.22)

With  li being fixed and computed during the previous sparse coding stage. If the patches are classified correctly their cost is rapidly close to zero. It can be shown that performing truncated Newton iteration to update the p th dictionary is equivalent to solve:

D

min l R( xl , D' ,  lp ) ' nk 
i 1 lSi

N

in which:

(3.23)

l 

Ci y p

R (x , D )    1 (i)
* N l j j 1 p

1p is 1 if i  p and 0 otherwise.
The DK-SVD dictionary learning algorithm is displayed in details in the following table.

Purpose: To train N distinct dictionaries to be able to sparsely represent as well as discriminate between the classes. Main loop: for i  1...N and j  1...k update d , the j th column of Di and:     Select the set of patches that uses d :

  l 1...M | li [ j ]  0.
For each patch l in  , compute the residual of the decomposition of xl : rl  xl  Dili . Compute the weights l (equation***) for p  1...N for all l . Compute a new atom d '  on the selected set  :
n

(3.24)

and associate coefficients  || that minimizes the residual error
2 2

min '
d
||

p 1... N
lS p 



l rl   li [ j ]d  l d '



Update Di and  using the new atom d ' , and replace the scalars li [ j ]  0 from equation

47

  l 1...M | li [ j ]  0.
End
Table 3.3 DK-SVD pseudo algorithm [40]

(3.24)

The results of segmenting the standard Brodatz dataset using this method are shown in Figure 3-9.

Figure 3-9 Segmentation results of the Brodatz dataset using DK-SVD method. Original images are displayed on the top and the result of the segmentation is displayed on the bottom. The table on the right displays the error rate for 10 of Brodatz mosaics segmentation.

3.5.3 Fisher Discriminative Dictionary Learning Last but not least among discriminative dictionary learning algorithms is Fisher discriminative dictionary learning (FDDL) introduced by Yang et.al [42]in 2011. In this algorithm based on Fisher Discrimination Criterion a dictionary with its atoms corresponding to a specific class labels is learned such that sparse coding reconstruction error (error between a reconstructed and original signal/image) can be used as an indicator for pattern classification. Using Fisher's criterion has the advantage of having a small within-class scatter but a big between-class scatter for the coding coefficients. The difference between this method and the two aforementioned algorithms is that DK-SVD and classification using l1 regularization use the reconstruction error (residual) as the discriminative criterion for classification and they don't impose discriminative information in sparse coding coefficients, but in fisher dictionary learning scheme both the reconstruction error and fisher's discrimination criterion are 48

used as classifiers for dictionary learning. In this method a structured dictionary D   D1 , D2 ,..., Dc  is learned with Di being the class-specific sub-dictionary associated with class i and c being the total number of classes. Denote by A   A1 , A2 ,..., Ac  the set of training samples and X   X1 , X 2 ,..., X c  the coding coefficient matrix of A over D ( A  DX ). The proposed FDDL model is as follows:

J ( D, X )  arg min r ( A, D, X )  1 X 1  2 f ( x)
( D, X )

(3.25)

With r ( A, D, X ) being the discriminative fidelity term, X 1 sparsity term, f ( X ) the coefficients discrimination constraint and 1 and 2 being scalar parameters. In order to be able to present the FDDL model in details, first we have to clarify the concept of Discriminative fidelity term ( r ( A, D, X ) ) and the discriminative coefficient term ( f ( X ) ). 3.5.3.1 Discriminative fidelity term As we already mentioned above, in this method the dictionary D is a structured dictionary with its sub-dictionaries being class specific. According to such a dictionary the matrix X i can be written as
1 j c j Xi    X i ,..., X i ,..., X i   in which X i is the coding coefficient of the signal Ai over the sub-dictionary

D j . We indicate the representation of Dk to Ai as Rk  Dk X ik . The main goal of learning a dictionary is
based on the fishers criterion so that it would be able to well represent the signal Ai ,

 A  DX
i

i

 D1 X i1  ...  Di X ii  ...Dc X ic  R1  ...  Ri  ...Rc  . And also it is desired that Ai should be
2 F

i represented by Di and not the D j ( j  i ). This implies that Ai  Di X i

is small, while X i j should have

j as minimum non-zero coefficients as possible such that D j Di

2 F

is small. Thus the discriminative fidelity

term is defined as:

49

r ( Ai , D, X i )  Ai  DX i

2 F

 Ai  Di X

i 2 i F

  D j X ij
j 1 j i

c

2 F

(3.26)

Figure 3-10 demonstrates the role and importance of each of the components in equation (3.26). Figure 3-10 (a) indicates that although D is ensured to represent Ai well, Ri may deviate much from Ai so that Di could not well represent Ai . Figure 3-9 (b) illustrates that if the constraint of Ai  Di X ii
2 F

being

small is added better discrimination results will be achieved but the problem is that Ai may also be well represented by other sub-dictionaries (not Di ).Figure 3-10 (c) shows how to overcome this problem by forcing the representation of D j , j  i to Ai to be small and the discrimination fidelity term is proposed completely.

Figure 3-10 FDDL discriminative fidelity term. (a) Only D is required to well represent Ai . (b) Both D and Di are required to well represent Ai .(C) The discriminative fidelity term in equation(3.26). Figure is acquired from [42].

3.5.3.2 Discriminative Coefficient Term In order to impose a more discriminative capability to the dictionary for the signal samples in A , we make the coding coefficient of A over D discriminative. This is a fact that can be achieved by

50

minimizing the within-class scatter and maximizing the between-class scatter of X . Within-class and between-class scatters are denoted as SW and S B respectively and are defined as:

SW ( X )  
c

c

i 1 xk X i

 (x

k

 mi )( xk  mi )T
(3.27)
T

S B ( X )   ni (mi  m)(mi  m)
i 1

Where mi and m are the mean vector of X i and X and ni being the number of samples in class Ai . The discriminative fidelity term is defined as:

f ( x)  tr (SW ( X ))  tr (S B ( X ))   X

2 F

(3.28)

With "tr" being the trace of the matrices and is a parameter that will be discussed in the following subsection. Having introduced the fidelity and the discriminative coefficient terms the actual FDDL model can be defined by substituting equations (3.26) and (3.27) in equation (3.28):

 c J ( D, X )  arg min  r ( Ai , D, X i )  1 X 1  2 tr ( SW ( X )  S B ( X ))   X ( D , X )  i 1



2 F

  

(3.29)

Just like any dictionary learning algorithm the process of learning a dictionary through FDDL is consist of two main fractions: updating X by fixing D and updating D by fixing X both in an iterative procedure. While X is fixed Di is updated class by class (it updates Di when all D j , j  i are fixed). In order to solve the FDDL dictionary learning problem in (3.29) this problem is first optimized to the following form:

J X i  arg min r ( Ai , D, X i )  1 X i 1  2 fi ( X i )
Xi

(3.30)

In which:

fi ( X i )  X i  M i

2 F

  Mk  M
k 1

c

2 F

 X i

2 F

(3.31)

51

With M k and M be the mean vector matrices of class k and all classes respectively. In order to update D class by class, the objective function would be:
c   J Di  arg min  A  Di X i   D j X j Di j 1, j  i   2

 Ai  Di X ii
F

2 F



j 1, j i



c

Di X ii

2

   F  

(3.32)

The FDDL algorithm is demonstrated in Table 3.4. Purpose: To train c distinct dictionaries to be able to sparsely represent as well as discriminate between the classes. Initialization: the pi atoms of each Di dictionary can be initialized randomly. (it has to be with a unit l2 norm) Main loop:  Updating the sparse coding coefficients X : Fix the dictionary D, solve for X i , i  1, 2,..., c one by one by solving equation (3.30)   Updating the dictionary D : Fix X and update each Di independently by solving equation (3.32) Stopping criterion: Return to step one (updating X ) until the maximum number of iterations are achieved or the value of J ( X , D ) in adjoining iterations are close enough. Output: the representation vector X and discriminative dictionary D consists of sub-dictionaries for c classes.
Table 3.4. The Fisher's discriminative dictionary learning pseudo algorithm

FDDL has been implemented on various datasets to evaluate its performance and comparison with other classification methods. The following tables acquired from [42] are represented in order to evaluate FDDL performance.

52

Table 3.5 recognitions rates for various methods on Extended Yale database

Table 3.6 recognitions rates for various methods on AR database

Table 3.7 recognitions rates for various methods on Multi-Pie database

53

Chapter Four: Classification and Clustering based on Sparse Representation
4. C

During the past few years, classification and clustering has gained a considerable amount of attention by image and signal processing researchers and scholars around the world. Successful applications of sparse representation in image restoration [7], denoising [6] and compressed sensing [5] as well as the fact that most natural signals can compactly be represented by only a few coefficients that carry the most important information in a certain basis or dictionary, proved that sparse coding algorithms can benefit the image processing community in numerous ways. Yet image classification, clustering and even segmentation using sparse based techniques are not yet vastly analyzed. The key factor in classification and clustering based on sparse representation is that different classes of an image

54

should be presented over different class-specific dictionaries so that their patches can be labeled accordingly. The reason for the emergence of this application (classification) through the field of sparse representation and dictionary learning is that despite the high dimensionality of natural signals, the signals of the same class usually lie within a low-dimensional subspace. Hence if the class specific dictionary is able to cover this subspace, it can result in a proper classification results. Therefore it is correct to say that for every typical signal (input patch) there exists a sparse representation with respect to a proper dictionary. Different works in the field of compress sensing ensure that a sparse signal can be recovered from its projections with a high probability [5], [43], [44]. This fact allows the recovery of the sparse representation vector(s) by decomposing the sample over a dictionary. Having the representation vector, one can extract the semantic information from the recovered sparse representation vector. Applications of sparse representation based classification techniques in the field of pattern recognition and computer vision can be found in: face recognition [45], iris recognition [13], image super-resolution [46] and other applications. In this thesis we have extended this new emerging field of research into the concept of medical image processing. As discussed before sparse representation techniques have a beneficial effect on classical signal processing problems (compression, representation, etc) with a compact high-fidelity representation. However in pattern recognition problems, the main interest is in the content or semantics of an image, not just the compact representation of it. This computer vision point of view is a new field in sparse representation studies that has grown due to the development of l1 minimization techniques in the past few years. In most approaches sparsity is usually used as a prior for clustering, in fact sparsity and proper dictionary learning have influenced the emergence of not only new algorithms but also new physical imaging systems [47] [48] [49].

55

The ability of sparse representations to uncover semantic information derives in part from a simple but important property of the data:  Although the images (or their features) are naturally very high dimensional, in many applications, images belonging to the same class lie on or near low-dimensional subspaces.  Such a sparse representation, if computed correctly, might naturally encode semantic information about the image. Among these newly emerging algorithms, "Robust Face Recognition via Sparse Representation" introduced by Wright et. al [45] has gained most of the attention by scholars working in the field of sparse representation based classification. This algorithm resulted in the famous classification scheme based on sparse representation known as: "Sparse Representation Classification" or simply SRC. SRC consists of two phases: Coding and Classification. First a query image is coded over a dictionary with some sparsity constraint; the classification is then performed based on the coding coefficients and the minimal reconstruction error. This use of the minimal reconstruction error for classifying signals/images in a sparse representation framework has opened a new and fast growing field in the theme of pattern recognition and computer vision based on sparse representation. This chapter starts by looking at the first sparse image separation study which is very significant as it motivated further studies in image classification based on sparse representation techniques. Following that the SRC frame work is presented. Some of these new emerging approaches which are based on reconstruction error are then introduced. An example of our contribution to the field of medical image processing using sparse based techniques is presented at the end of the chapter.

56

4.1 Motivation: Texture Separation

The first work that focused on representing different classes of an image using different dictionaries focused on decomposing an image into its texture and non-texture components or as it is said in the literature: Cartoon and Texture parts [12]. The core idea of separating texture images into cartoon and texture components is to try to represent these different components using separate dictionaries. As it is mentioned before, representing different parts or classes of an image over different dictionaries is the core idea behind classification and clustering using sparse methods. For image texture separation using sparse representation like most of the signal decomposition techniques it is assumed that the original image is a linear combination of its constructing sub signals:

I  I c  It  n with I c and I t being the non-texture (cartoon) and texture components respectively and

n as the white Gaussian noise with a known standard deviation  . In all sparse based separation
frameworks selecting the proper dictionaries is a key point. This is mainly due to the fact that the separation process is carried out through the representation of different classes over the dictionaries. Starck et. al [50]showed that since DCT and Gabor transforms result in oscillatory atoms; these dictionaries are suitable for representing the texture data while Bi-Orthogonal Wavelet Transform, Curvelet Transform, Local Ridgelet Transform are the proper candidate dictionaries for sparsely representing the cartoon components of an image. Considering the image as a linear combination of its components, solving the following equation will result in the corresponding sparse representations of the two classes:

^ ,X ^  arg min  X   X  1 I  D X  D X X c t c 1 t 1 c c t t Xc , Xt 2

2 2

(4.1)

57

In which  is the constant balancing between fitting the data completely and having the sparsest solution, Dc and Dt are the corresponding dictionaries for each class and X c and X t are the sparse vectors for representing each class. Finding the representation of each class, they can be estimated as:

^ D X ^ ^ ^ ^ ^ I c c c and I t  Dt X t with I c and I t as the estimation for each class. Imposing such formulation in
equation (4.1) will result in:

^ ,I ^  arg min  T I   T I  1 I  I  I I c t c c 1 t t 1 c t 2 Ic , It

2 2

(4.2)

Where T  D1 . Daubechies et. al [51] suggested Separable Surrogate Functions (SSF) algorithm to

^ : ^ and I solve the following optimization problem to find an iterative solution for determining I t c
^ ,X ^  arg min  X   X  1 I  D X  D X 2 subject to T D X  X and X c t c 1 t 1 c c t t 2 c c c c 2 Xc , Xt Tt Dt X t  X t
Using SSF will result in the followed iteration formulation to find 1 ,2 :

(4.3)

I cK 1  Dc S I
K 1 t

 Dt S

 

1 c 1 c

T ^K  I ^K  T I ^K Dc I I c t c c

D

T t

  I  I^

K c

^K I t

    T I^ 
K t t

(4.4)

Where S (r ) is an element-wise soft thresholding operation on r with a threshold  and the parameter
T T c should be chosen such that c  max ( Da Da )  max ( Dc Dc  Dt DtT ) . For further backgrounds and

mathematical justifications readers are advised to refer to [50]and [12]chapter15.

The aforementioned method has been implemented on the famous Barbara image and on a MRI image of the human knee using a DCT dictionary for texture and a Curvelets dictionary for cartoon contents of the images.

58





Figure 4-1 Top: shows the original Barbara image (left) with the separated texture (middle) and the cartoon contents (right). Bottom: Both dictionary atoms in the left. Image in the right is a mapping that shows the label of each dictionary atom. Patches with a white label belong to the DCT dictionary, and patches with a black label belong to the Curvelets dictionary.


59



Figure 4-2 same experiment and results as Figure 4-1 on an MRI Image of the human knee.

The discussed frame work for texture detection takes advantage of Separable Surrogate Functions in order to solve the problem in (4.3). It utilizes two class specific learned (learned with K-SVD) dictionaries and represents the data over them to separate the two label of the classes. It is among the first methods that perform an image separation task based on sparse representation and dictionary learning approach.

4.2 Sparse Representation Classification (SRC)

SRC is considered to be one of the leading methods in the field of classification based on sparse representation. One important fact which can be considered as a drawback of this method is that SRC does not use a generic dictionary (whether it is reconstructive or discriminative) to represent the data over, but instead it uses the actual training dataset as a space to represent the data over. If the training data set is large enough the SRC eliminates the critical need of features for the purpose of classification [45]. On the other hand using large training datasets and representing the data over them, not only makes the method slow but also dependent on large training data samples which are not always available. Some methods like the one in [42], have overcome this problem by introducing the use of SRC in parallel with specific discriminative dictionaries such as Fisher discriminative dictionaries. In SRC if the dictionary is an over-complete dictionary of the training samples (sufficient numbers of training samples are available in the dictionary for each class) the algorithm will sparsely represent the test samples of each class as a linear combination of the training samples of the same class. It will be shown that this sparse representation is actually the sparsest representation (in most cases) of the image over this dictionary and the original test image can be recovered by means of l1 minimization. As a result seeking the sparsest representation is discriminating between different classes. Basically each test sample is individually sparsely presented and adaptively selects the training sample that provides the most 60

compact representation (the use of the reconstruction error). This approach makes the general form of SRC to be somewhat close to a generalization of the Nearest Neighbor (NN) method [52]. NN classifies a test sample based on the best representation in terms of a single training sample whereas SRC uses a similar approach but it considers all possible supports and adaptively chooses the minimum number of atoms (atoms in this case are the training samples) needed to represent each sample. 4.2.1 The Classification Scheme In most object detection techniques a common approach is to use labeled training data from K different object classes, consequently after giving a new test sample it determines to which class this test data belongs. In this case the training data for a distinct class i is arranged as columns of a matrix

Ai   i ,1 , i ,2 ,..., i ,ni  

mni

for ni training samples. For the purpose of classification the algorithm will
m

classify object classes using the vector  

. Basically each vector in A is a training sample for the i th

class which lies on a linear subspace. Subspace models are flexible enough to capture much of the variation in real data sets. It is assumed that the training samples from a single class lie on a single subspace [45]. A new object sample y 
m

belonging to one the classes can and will be presented as a

linear combination of the training samples:

y  i ,1i ,1  i ,2i ,2  ...  i ,ni i ,ni

(4.5)

For some scalar i , j  , j  1, 2,..., ni . In order to generalize the problem so that any given object can be represented and later classified without knowing to which class it belongs, dictionary matrix D for k distinct classes is defined as:

D   A1 , A2 ,..., Ak    1,1 , 1,2 ,..., k ,nk  
So the linear representation of y can be written in the general form of:

(4.6)

y  D 0

(4.7)

61

In which  0   0,...,0, i ,1 , i ,2 ,..., i ,n ,0,...0  

T

n

is a coefficient vector with all of its entries being

zero except those associated with the i th class. The solution to equation (4.7) can be gained either by choosing the minimum l2 -norm solution of:

^1  arg min  2 subject to y  D 
Or by solving the convex optimization problem of:

(4.8)

^1  arg min  1 subject to D  y 2   
This representation is naturally sparse if the number of object classes k is reasonably large.

(4.9)

For the purpose of classification of a new test sample y one should compute the sparse

^1 over the dictionary D ( D   A1 , A2 ,..., Ak  ) via (4.8) or (4.9) and associate the nonrepresentation  ^1 with the columns of D from a single object class i , ideally the test sample zero entries of the estimate 
y can be assigned to that specific class. Unfortunately the presence of noise, modeling errors and the
similarity of information in specific patches will lead to some non-zero entries associated to multiple classes (miss classified patches). In order to overcome such inaccuracies y is instead classified based on how well the coefficients associated with all training samples of each object reproduce it ( y ). In order to overcome the aforementioned problems  i is defined for each class to choose the best coefficients associated with the class i . By using a class specific coefficient, a given test sample y can then be approximated as:

^1 ) ^i  D. i ( y

(4.10)

^ i by: y can then be classified based on the approximations that minimize the residual r between y and y

^1 ) min ri ( y)  y  D. i (
i

(4.11)
2

62

Equation (4.11) will result in a representation of y over D which is not only sparse but also labels y based on its minimum reconstruction error for its specific class. Table 4.1 demonstrates the pseudo algorithm for SRC method

Purpose: to sparsely represent a test sample y , and label it based on its minimum reconstruction error over the dictionary D . Input: A dictionary consists of as many training samples as possible, a test sample y  termination criterion. Normalization: Normalize columns of D to have a unit l2 -norm. Main loop:   Solve :
m

and  as the

^1  arg min  1 subject to D  y 2   

^1 ) Compute the residual ri ( y)  y  D i (
i

2

Output: y  arg min ri ( y)
Table 4.1 The SRC pseudo algorithm

Figure 4-3 exhibits a comparison between the performance of SRC and some of the well known conventional methods for face recognition applications.

Figure 4-3 Face recognition results with partial features. (a) example features, (b) results for different methods on Extended Yale B database [45].

63

Recently SRC has gained lots of attention by researchers in the field of classification and clustering based on sparse representation. Many variations of this method have been developed using the concept of reconstruction error for sparsely classifying images. In the following section we will shortly discuss the main concept of two of these methods.

4.3 Reconstruction Error Base, Classification

After proposing the sparse representation texture separation method by Starck et. al [50]the idea of utilizing dictionary learning and sparse representation for the purpose of classification flourished. After many works by scholars around the world, this idea turned into a practical method with the emergence of SRC framework by Wright et. al [45]in 2009. Since then many variations of using reconstruction error as an indicator for classification have been developed. Among these recently developed works the papers by Guha et. al [53] and Sivalingam et. al [54]are of more interest to us. Guha et. al [53] proposed a sparse classification method for image and video signals and took advantage of the reconstruction error in their method. They describe each training signal by an error vector consisting of the reconstruction errors that a test signal produces with respect to each class specific dictionary. Since there exists a dictionary for each class, it is expected to gain small reconstruction error over that class specific dictionary and simultaneously large error for reconstructing a signal over a dictionary which is learned for other classes. Utilizing the Mahalonobis distance as a measure for clustering based on errors; they classify a test signal according to their reconstruction error. Another recent method which performs the classification task based on the reconstruction error is presented by Sivalingam et. al [54] in 2010. The core idea of this method is based on the proposed method by Mairal et. al [40], which was discussed earlier in chapter 3, section 3.5.2 under the topic of Discriminative K-SVD dictionary learning algorithm. Learning the dictionaries through DK-SVD algorithm 64

and representing the data utilizing OMP method, they create the reconstruction error curves. These curves will be used as indicators for classifying different classes in an image. For a comprehended explanation on these two methods, the authors are referred to [53], [54]and references therein. 4.3.1 Segmenting  MRI images using the reconstruction error

Following the same path that the above mentioned scholars in [53] and [54]passed, by taking advantage of the reconstruction error, we developed a method to classify T2 MRI images of the brain and segment the lateral ventricle in these images. Utilizing K-SVD dictionary learning we trained two dictionaries for the lateral ventricle and cerebral cortex of the brain. By taking advantage of the introduced SRC frame work, we sparsely represent the data over these dictionaries individually and by calculating the minimum reconstruction error in representing the patches of the image; we label and classify them. We then extract and employ the center of the detected patches as a seed point for the Active Contour Without Edges segmentation framework [55] to fully segment this area. The following block diagram shows the steps taken in this method.

65

Figure 4-4 The image on the top demonstrates the block diagram of the method and the image in the bottom illustrates the segmentation procedure for an example image.

The implementation of the algorithm on 25 T2 weighted MRI images showed an average accuracy of 88.7% and an average computational time of 430 seconds for 400  400 size images. The results of this work is published in IEEE 11th international conference on information science, signal processing and their applications. More details on MRI images and the importance of segmenting the lateral ventricle will be presented in the final chapter when we present our novel segmentation framework.

66

Chapter Five: Image Segmentation and Sparse Representation Classification Based on Euclidian Distances
5. I

In this chapter of the thesis a novel framework for clustering and classification signals and images based on sparse representation and dictionary learning approach and our segmentation framework are presented. We are presenting a technique that can map the sparse representation vector to Euclidian distances. We basically transform the sparse domain into a new space which is based on Euclidian distances. Based on these distances we can perform the classification task with a higher accuracy and much faster than other existing sparse based classification techniques. Like most sparse based classification techniques we are using class specific learned dictionaries to increase the accuracy of the 67

algorithm. Unlike other sparse based methods however, we are not using the reconstruction error for the purpose of classification, instead, we are using the aforementioned transform. This method finds the minimum Euclidian distance between an input patch and the atoms of a learned-base dictionary to perform the classification task. Since in sparse based methods that perform the labeling task based on the minimum reconstruction error, the algorithm has to compute the residual vector for each and every patch of the test signal therefore making these methods computationally expensive. Besides the computational time, in SRC a very large training data set is needed; and as we have mentioned earlier this is the main drawback of the SRC. In our method the reconstruction error calculations are not required, making the method fast and also the problem of requiring a large dataset is solved by means of using learned over-complete dictionaries. In this chapter we present the methodology and justifications behind this novel framework in details. To evaluate our method we implemented the method on the famous Brodatz texture dataset on the same images that other methods such as SRC are tested on. We also compare our classification results with SVM and K-NN as two non-sparse based classification techniques. This framework and its results have been presented to IEEE 26th international conference on visual communication and image processing (VCIP) conference with a special session on sparse representation. It will be available on IEEE xplore digital library in the near future.

5.1 Methodology

5.1.1 OMP and K-SVD This method takes advantage of the Orthogonal Matching Pursuit (OMP) and K-SVD dictionary learning algorithms presented in sections 2.3.1.2 and 3.4.2 respectively. Having the sparse representation problem of:

arg min y  D subject to 


0


68

(5.1)

For a signal/image y , a dictionary D and a representation vector  with a termination criteria  the OMP finds a sparse solution for (5.1) in a recursive manner. It maintains a converging solution for nonorthogonal dictionaries. The OMP attempts to recursively minimize the residual of the reconstructed patch by finding the best matching direction to the residual among all dictionary atoms, per iteration. The main idea behind the OMP algorithm is to solve the following equation in each iteration:

 (iter )  D y

(5.2)

In which D  ( DT D)1 DT is the pseudo-inverse of the dictionary matrix D . In order to learn class specific dictionaries for k distinct classes ( D  D1 | ... | Dk  ), K-SVD algorithm is used. Here we just use OMP and K-SVD algorithms as parts of our classification framework, for more details about these algorithms readers are referred to chapter 2 and chapter 3 of this thesis. 5.1.2 Sparse Euclidean Classification: The clustering frame work that we call it Sparse Euclidian Classification (SEC) is based on sparsely representing a signal with two non-zero coefficients (   2 ) the method is as follows: Suppose there are two classes of images (can be two classes in one image) and OMP is used to represent the image patches with two non-zero coefficients 1 and  2 (sparsity level   2 ), the vector  corresponding to two atoms (templates) can be defined as:

  1  2   ( D y)T
T

(5.3)

In which D   di1 , di 2  is the dictionary with two sub-dictionaries learned to have minimum distances to the input pattern for each class. By minimum distance we basically indicate that d1 and d 2 are learned through K-SVD algorithm specifically for class 1 and class 2 respectively. The pseudo inverse problem of

  D y can be expanded as:

69

  diT   diT    i1   1 1  D y  [ d d ]     T  i1 i 2   T  y    i 2     di 2    di 2 
And:

1

(5.4)

 (diT2  di 2 ) (diT   diT  1  di 2 ) 1y .  T   T  (di 2  di1 ) (diT  i1  1  d i1 )   d i 2  y      (d T  d )(d T  d )  (d T  d )(d T  d )  i2  i1 i1 i1 i1 i2 i1 i1 i2   M    i1    i 2  c  diT  y   T1   di 2  y 

(5.5)

The Euclidean distance between a pattern (patch) and two templates (atoms) can be determined as:

 r1   ( y  di1 )T  ( y  di1 )  r    T  r2  ( y  di 2 )  ( y  di 2 )   yT y   diT1  di1   diT1  y   T  T   2  T   C1  C2  2r  y y   di 2  di 2   di 2  y 
Replacing equation (5.6) in (5.5) will result in a relation between  and r :

(5.6)



M 1 1    C1  C2   r  c 2 2 

(5.7)

And also:

r  C1  C2  2cM 1  yT y   diT1  di1   (diT1  di1 ) (diT1  di 2 )   T  T  2 T  T  y y   di 2  di 2  (di 2  di1 ) (di 2  di 2 ) 
(5.8)

Equation (5.8) demonstrates the minimum distance as a transform of  ; this equation also proves that  alone is not a proper indicator to classify different input pattern classes. The minimum distance to the pattern for each class is attained by performing the OMP for the corresponding sub-dictionary as:

70

r

p min

 diT   (diT  (diT 1 .di1 1 .di1 ) 1 .d i 2 )  min  T  2 T  T  di 2 .di 2  (di 2 .di1 ) (di 2 .di 2 ) 

(5.9)

In which p refers to the class label and

p rmin is the minimum distance of the corresponding class. The

input pattern class is specified by finding the smallest minimum distance of all classes. This

p rmin

(minimum distance of each input patch with dictionary atoms) acts as the classifier for the method. The pseudo algorithm of the proposed method is demonstrated in Table 5.1.

Purpose: To classify different classes in a given test image/signal using Sparse Euclidian Classification technique. Input: Test image/signal and the dictionary D consist of k sub-dictionaries separately learned for k distinct classes. Main loop:     End. Output: The labeled image/signal patch according to the label of the class specific dictionary.
Table 5.1 the pseudo algorithm of the proposed SEC method

For k  1, 2,..., n

i  OMP( Dicti , y )
ri  2  M .i

Class( yk )  min(min(rk )), k  1, 2,..., n

This method is not limited to two classes only, it can cluster up to k  1, 2,..., n classes, the algorithm will simply iterate for as many classes in the image. For a better demonstration of the algorithm the following block diagram is presented:

71

Figure 5-1 a block diagram representation of the proposed method

5.2 performance

The proposed classification technique has been implemented to evaluate its performance. The following figure demonstrates classification results for two randomly projected Gaussian data, one with a mean of 95 and the other with a mean of 100 with a standard deviation of 12. Although the means of both classes are close to each other the result shows only one miss classified sample.

72

Original Data 130

Classified Data 130

120

120

110

110

100

100

90

90

80

80

70

70

60 70

75

80

85

90

95

100

105

110

115

120

60 70

75

80

85

90

95

100

105

110

115

120

(a)

(b)

(c) Figure 5-2 (a). Demonstrates the original data with their mean in red. (b) Demonstrates the sparsely represented data. (c) Illustrates the original data labels on top which was assigned by hand, the first fifty samples are from class1 and the rest are from class2, on the bottom are the labels that are resulted from the proposed method. One miss classified sample can be seen.

For randomly generated Gaussian distributions with different means and standard deviations the proposed framework has been tested in different scenarios (different means and variances). For a total number of ten repetitions per each test (since the data is randomly generated) the average error rate for labeling the classified data and accuracy using the proposed framework can be demonstrated as follows:

73

12

100 90

10

Average number of miss classified smaples

80 70
8

60
Accuracy

6

50 40

4

30 20
2

10
0 20

15

10

5 4 3 2 1.5 Difference in means of classes

1.2

1

0.5

0 20

15

10

8

7

5 2 1 Class Difference

0.75

0.5

0.25

(a) (b) Figure 5-3 (a) Number of miss classified samples based on the difference between the means of classes. (b) Average accuracy (percent) versus the difference between classes

It can be seen that even in the case of having classes with a small difference in their means; the classification rate is still acceptable (around 5% when the difference of means is about 2 to1).

(a)

(b)

(d) (c) Figure 5-4 (a) Four randomly Gaussian distributed data with means of 40,50,60 and 70 displayed in four different colors, (b) Sparse representation and classification of the original data using SEC method. (c) Labels of the original data, first 50 samples are from class 1 and are displayed with a black label. Similarly dark gray label for class2, light gray label for class 3 and white label for the 50 samples of class 4. (d) Labels that are correctly assigned to the representation of the data using SEC method.

74

(a)

(b)

(c) (d) Figure 5-5 Four randomly Gaussian distributed data with means of 48,51,53 and 54 displayed in four different colors, (b) Sparse representation and classification of the original data using SEC method. (c) Labels of the original data. (d) Labels of the represented data assigned by SEC method. 5 misclassified samples among 200 samples can be seen.

In order to make a more comprehended evaluation of the error rate of the method, the method has been compared with the famous Sparse Representation Classification (SRC) method under the same conditions. The comparison result is as follows:

20 18 16 14 12 10 8 6 4 2 0 20 SEC SRC

Average number of miss classified samples

15

10

5 4 3 2 1.5 Difference in means of classes

1.2

1

0.5

Figure 5-6 classification results comparison between SRC and SEC based on the distance of the means of different classes

The Brodatz dataset is used to compare the accuracy and efficiency of this method with SRC as the best kwon sparse based classification method as well as SVM and K-NN. The class specific sub-

75

dictionaries are learned using K-SVD with 20 iterations, creating a dictionary with the size of m  100 and
n  400 . Figure 5-7 demonstrates the classification results of the proposed method on the Brodatz

textures.

Figure 5-7 classification results of the proposed method on the standard Brodatz database texture images. The top row shows the original image while the bottom row shows the classification results. Class 1 is shown in black, while class 2 is shown in white and Class 3 in gray.

Figure 5-8 demonstrates the evaluated accuracy based on different patch sizes for the proposed method, SRC and classification based on the sparse vector (  ). It is important to note that minimum accuracy is obtained in the results when the sparse vector alone is used as an indicator for classification.
100 Alpha SRC Proposed Method

90

80

Accuracy%

70

60

50

40 10

12

14

16 18 Patch size

20

22

24

Figure 5-8 Classification rates for three different sparse based methods

76

In this method we are utilizing learned class specific dictionaries to represent the data over, while in SRC the actual training samples (training images) are being used as the dictionaries to represent the data over and perform the classification. Hence the computational time is reduced in our method compare to SRC. In this method unlike SRC not only we are not using all the training samples as the space to represent the data over (we utilize class specific dictionaries which have a very smaller size but an acceptable accuracy), but also we do not compute any reconstruction error (or any error vector) for the samples. When the sparse representation of each patch is computed, using the transform in (5.9) we calculate its Euclidian distance from the dictionary atoms in the same iteration (Table 5.1). This will result in a noticeable increase in the accuracy as well as reducing the computational time significantly. Table 5.2 demonstrates a comparison between SRC and the proposed method's computational time and Table 5.3 illustrates the measured accuracy for both methods on the same data. Patch Size SRC Proposed Method Patch Size SRC Proposed Method

10x10 16x16 20x20 24x24

500 290 230 200

3.1 2.96 2.27 2.1

10x10 16x16 20x20 24x24

50% 72% 85% 90%

55% 80% 97% 98.5%

Table 5.2 Computational time (seconds) for the proposed method and the SRC method

Table 5.3 Accuracy measures for proposed method and the SRC method

In order to compare the accuracy of the proposed framework with non-sparse based classification techniques, as well as SRC the error rate for classifying Brodatz dataset textures is compared with K-NN and SVM methods. These results are demonstrated in Table 5.4.

77

Method

Error Rate (percent)

K-NN SVM SRC Proposed Method

5.2 1.5 10 1.5

Table 5.4 Error rate measurements

In this chapter we proposed a sparse representation-base image classification method using a transformation which maps sparse vectors to Euclidian distances. It is a generic method that can be utilized within a variety of image and signal classification as well as other sparse representation applications. The uniqueness of this method lies in the fact that unlike other conventional sparse-base classification methods, we are not using the representation vector or the reconstruction error as an indicator for classification. Conversely we employed a transform domain in which each class sparse features appear to be more distinct. Using the new proposed measure introduces a strong classification approach that eliminates the need for large dictionaries; hence the computational time is significantly reduced compared to other prevailing methods. The proposed framework is suitable to be used as a platform for different image sparse classification algorithms. It can be used in conjunction with other dictionary learning methods such as FDDL or MOD and instead of OMP other sparse representation frameworks can be utilized. A variety of sparse pursuit algorithms such as FOCUSS, Batch OMP, and other methods that utilize the pseudo-inverse operation in order to update the sparse vector can be substituted with OMP. OMP was employed here because it is the leading sparse representation method and is the building block for most other pursuit representation methods. As a result our method can be expanded by most of these methods.

78

5.3 Organ Boundary Segmentation

After proposing the novel sparse Euclidian classification framework and evaluating its performance on texture images for image classification, in this section by taking advantage of the proposed sparse based classification method, we present algorithms capable of segmenting different body organs in medical images. These algorithms are compatible with both MRI and CT scan imaging modalities. The main advantage of these methods is that they are not limited to one specific organ and a single imaging modality. In fact in the aim is to present a new image classification and segmentation technique to the medical image processing community based on sparse representation. A method that unlike many other conventional techniques is not limited to a specific organ or modality. The main goal in the near future is to be able to learn class specific dictionaries for any organ in any modality and by means of the introduced SEC method, classify and label different organs in the image accurately. There are still lots of room for research and investigation in this field. However our results were quite convincing and have been evaluated and approved by medical experts. 5.3.1 Methodology

The main idea behind this segmentation approach is to use class specific labeled dictionaries and try to sparsely represent the image over the labeled and learned dictionaries; the dictionaries are trained for different classes using either K-SVD or FDDL algorithms introduced in chapter 3. Using the SEC classification framework we label each patch of the test image with respect to its minimum Euclidian distance from the dictionaries. Having labeled every patch in the image, the desired organ is detected according to the labels. With the desired organ detected, its boundaries can be segmented

79

effortlessly utilizing an image processing tool such as edge detection or active contours [55]. Figure 5-9 demonstrates a block diagram of the proposed method.

Figure 5-9 Block diagram of the proposed organ segmentation method

5.3.2 Segmenting the human heart in MR images

Compared to other imaging modalities (such as ultrasound and magnetic resonance imaging), cardiac magnetic resonance (MR) can provide detailed anatomic information about the heart chambers, large vessels, and coronary arteries [56]. Therefore, MRI is an important imaging modality for diagnosing cardiovascular diseases. Complete segmentation of the heart, is a prerequisite for clinical investigations, providing critical information for quantitative functional analysis for the whole heart [56]. We propose our sparse classification and segmentation technique for segmenting the human heart in MR images (it can be used in CT modality as well). In this method we train two different dictionaries, either through KSVD or FDDL algorithms. We learn one dictionary based on the texture, intensity, and frequency information (when we utilize DCT dictionaries) of the desired organ (heart in this case), and we train another dictionary for the rest of the image. The purpose for training two dictionaries in such a way is that we want to have a good representation of the heart itself, but the representation of rest of the 80

image is of no interest to us. As a result we dedicate one dictionary to the heart and one dictionary for all other classes in the image. Based on experience we usually utilize one DCT and one image patch based dictionary. If both dictionaries are DCT or image patch based, the algorithm will still classify the image but with less accuracy (in some cases only). Both learned and labeled dictionaries along with the test image are fed into the SEC classification framework, the image patches that are representing the heart will be labeled according to the dictionary that is learned for the heart. Having detected the heart patches, the rest of the patches will be eliminated resulting in a binary image of the heart. The boundaries in this image can easily get detected by means of an active contour model automatically. The steps in this procedure are demonstrated in the following figure:

Figure 5-10 An example of segmenting the human heart in a MR image. Dictionary1 is a KSVD learned dictionary with a DCT basis, while dictionary 2 is based on image patches.

81

Figure 5-11 shows the segmentation results of this method on few of our images.

(a) (c) (d) (b) Figure 5-11 (a) Original image, (b) The detected organ, (c) Boundary of the detected organ, (d) final segmented image

We evaluated this method by comparing the results of our work for 30 images with the manual segmentation of the images performed by an expert. For all the Images, Dice coefficient was computed in order to measure the accuracy. An average accuracy of 92.5% for all the images was gained using the proposed method. 5.3.3 Segmentation of the kidney in CT images

Isolating the kidney from its surrounding anatomical structures and organs is a crucial step in many medical diagnosis frameworks that assess the renal functions. Frameworks that are proposed for

82

automatic classification of normal kidneys and acute rejection transplants [57] are among the applications of kidney segmentation. Acute rejection is the immunological response of the human immune system to the foreign transplanted kidney after surgery. It is the most important cause of graft failure after renal transplantation [58]. Unlike invasive monitoring and post renal transplant follow-ups such as biopsy, noninvasive follow-up studies are based on acquiring images from the transplanted kidney. These imaging modalities are usually in CT or sometimes MRI (Dynamic Contrast Enhance-MRI) modalities. In order to segment the kidney in acquired images for pre or post-surgery studies we propose a similar approach to what we introduced for segmenting the heart in MRI images. In this approach we train one over-complete dictionary based on the patches acquired from the kidneys pixels in CT images (a similar approach can be utilized within MRI); another dictionary is learned based on the rest of the image. Feeding both dictionaries as well as a new test image to the proposed classification framework we can label and detect the patches that are representing the kidney. Since the number of other organs in these images are relatively larger compared to the images acquired from heart MRIs or CT scans, the chance of misclassifying the patches are higher. In order to overcome this problem usually a threshold is utilized to neglect the misclassified patches based on the density of the detected patches in an area. The concentration of the patches is relatively higher in the kidneys areas. The following figure shows the segmentation of the kidneys in acquired CT images.

83

(a)

(b)

(c)

Figure 5-12 (a) Original CT image. (b) Segmented kidneys. (c) Boundaries of the segmented kidneys

Since neither the images nor the algorithm of other kidney segmentation frameworks were available, in order to evaluate the accuracy of the method, all segmentation results were compared with manual segmentation of the images. The acquired dice coefficient calculation for 12 CT images showed an average accuracy of 87%.

5.3.4 Segmentation of ventricular system in MR Images of the Brain

Segmenting the ventricular system of the brain in medical images plays an important role in medical diagnosis. The volume of lateral ventricle increases with age and it is an important indicator of Alzheimer's, schizophrenia, and depressive disorders. For medical diagnosis both CT and MRI imaging modalities are used in brain studies. Within the MRI itself there are T1 , T2 , Proton Density and Diffusion

84

model imaging modalities. In most diagnostics, especially in the ventricular system studies the T1 and T2 modalities are of more concern. T1 and T2 are two basic yet famous imaging modalities in MRI. T1 weighted scans refer to a set of standard scans that represent differences in the spin-lattice (or T1 ) relaxation time of various tissues within the body. T1 -weighted sequences have the ability to be acquired rapidly because of the fact that they use short inter-pulse repetition times ( TR ). These sequences are often collected before and after infusion of T1 -shortening MRI contrast agents. In the brain T1 -weighted scans provide appreciable contrast between gray and white matter. In the body, T1 weighted scans work well for differentiating fat from water, with water appearing darker and fat brighter. T2 -weighted scans are another basic MRI modality. Like the T1 scan, fat is differentiated from water, but in this case fat shows darker, and water lighter. In order to segment the ventricular system using the SEC framework we follow the same path that we did before, which is learning class specific dictionaries. The difference is that in MRI images we implement the method on both modalities simultaneously. In each modality we train a dictionary for the lateral ventricle and the cerebral cortex classes in the image (four dictionaries in total). For each modality the image and the dictionaries are fed to the SEC frame work and the intersect of the detected area from each modality would be the final segmentation of the ventricular system. The block diagram of this method is presented below:

85

Figure 5-13 the block diagram of the ventricular system segmentation technique

In order to evaluate the performance of the method, a comparison between the results of this method and manual segmentation of the ventricular system in the images has been carried out. Figure 5-14 shows an MRI image in both T1 and T2 modalities with their ventricular system segmentation results. And Figure 5-15 demonstrates the manual segmentation process of the brain ventricles in another MRI image.

(a)

(b)

(c)

Figure 5-14 ventricular system segmentation in MRI images. (a) Image. (b)  Image. (c) Final segmented image.

86

(a)

(b)

(c)

Figure 5-15 Manual segmentation of the lateral ventricle. (a) Original Image. (b) manual selection of boundary points for segmentation. (c) Manually segmented image.

For all of the Images, Dice coefficient was computed in order to measure the accuracy of the method. Table 5.5 demonstrates the Dice coefficients comparisons gained from 10 of the images. An average accuracy of 94.3% for all the images was gained using the proposed method.

Image Dice Coeff

1 87.5

2 93.75

3 91.2

4 89.3

5 92.5

6 86

7 87.3

8 95

9 90

10 88

Table 5.5 Dice coefficient calculated for ten MRI images

87

Chapter Six: Conclusion and Discussion
6. C

Sparse representations of signals and learning a dictionary to represent the signal over, has gained considerable amount of attention by many scholars focusing on different applications around the world. In this thesis we took a step further exploring the effects of sparse representation and dictionary learning in medical image processing. Sparse representation techniques are considered to be a powerful method in image processing, especially in applications such as denoising and compression. However these techniques have not yet vastly been explored for other application purposes such as classification and segmentation. We presented a novel approach in classification images using sparse based techniques that is capable of clustering different classes in an image with an acceptable accuracy. We then utilized this classification technique and extract a novel fully automated method to segment 88

medical images. To our knowledge we are the first to utilize sparse classification techniques as well as dictionary learning for image segmentation purposes.

6.1 Sparse Euclidian Classification

In this thesis we proposed a sparse representation-base image classification method that we call Sparse Euclidian Classification (SEC). We utilize a transformation which maps sparse vectors to Euclidian distances. Unlike the few conventional sparse based classification techniques we are not using the representation vector or the reconstruction error as an indicator for classification. Conversely we introduced a transform that maps the sparse representation vectors for each patch to Euclidian distances. By using these Euclidian distances each class sparse features appear to be more distinct therefore resulting in strong classification results. Since these Euclidian classification features have strong clustering characteristics the proposed method is independent of very large datasets other methods need for the purpose of classification, hence making the method considerably faster. Comparisons between the results of SEC and the famous SRC method demonstrated higher accuracy and noticeably less computational time for our method. We developed the Sparse Euclidian Classification algorithm based on Orthogonal Matching Pursuit method for sparse representation; however it is possible to utilize other sparse representation techniques within the SEC framework and evaluate its performance.

6.2 Organ Segmentation Framework

In this thesis we proposed a novel fully automated segmentation algorithm to segment different organs in different images using our Sparse Euclidian Classification (SEC) framework. After gaining

89

acceptable classification results utilizing SEC and implementing it on different texture images for evaluation, we utilized this classification framework as the main element in our segmentation technique. In order to segment an organ in an image first a class-specific dictionary is learned through KSVD algorithm. Having gained the specific dictionary, a test image is sparsely represented and classified over it within the SEC framework. Utilizing SEC, the patches that belong to the specific organ are labeled and classified. Since the information on the edges of the detected organs is of great importance to us, we then detect the edges of the classified organ/patches using active contours or edge detection methods. As an advantage, this approach requires minimum human interaction and is fully automated. The other significant feature that this method has is that since it is based on learning a class-specific dictionary, we can utilize this method for different organs in different modalities. As a matter of fact this method can be used as a platform for other segmentation applications (both medical and non-medical). So far we were able to use this method in MRI and CT images for different organs; the main goal is to be able to introduce a novel method that by learning its specific dictionaries is capable of segmenting any organ within any modality.

6.3 Future Work

Future works of this thesis can be categorized in two main categories: 1. 2. Sparse classification Image segmentation 1. In sparse classification, the goal is to continue the idea of using distances rather than the sparse vector or the reconstruction error as indicators for classification. Use of other distances such as Mahalanobis distance is currently being investigated. Using distances other than the Euclidian, has the

90

potential to introduce stronger classification results. The mathematical approach in the original SEC might need some minor changes in order to utilize other distances. 2. For the purpose of segmentation we are investigating the use of the proposed method for other organs and other image modalities. Our studies illustrated that promising results in CT and MRI modalities are expected, however in other modalities such as X-ray and ultrasound imaging the method might need more discriminative properties. In order to add more discriminative characteristics to the method the use of a new stronger discriminative dictionary learning algorithms might be highly beneficial. The ultimate goal is to introduce a method which is capable of segmenting any area with specific characteristics (texture, intensity, frequency info, etc...) in any imaging modality. This can be achieved by sparsely representing images over class-specific learned dictionaries followed by labeling the desired organ patches accordingly.

91

References:

[1] H. Badakhshannoory and P. Saeedi, "A Model-Based Validation Scheme for Organ Segmentation in CT Scans Volumes," IEEE Transactions on Biomedical Engineering, vol. 58, no. 9, pp. 2681-2693, 2011.

[2] J. Ehrhardt, A. Schmidt-Richberg and H. Handels, "A Variational Approach for Combined Segmentation and Estimation of Respiratory Motion in Temporal Image Sequences," in IEEE 11'th International Conference on Computer vision (ICCV), 2007.

[3] O. Gloger, K. D. Tonnies, V. Liebscher, Brend. Kugelmann, R. Laqua and H. Volzke, "Prior Shape Level Set Segmentation on Multistep Generated Probability Maps of MR Datasets for Fully Automatic Kidney Parenchyma Volumetry," IEEE Transactions on Medical Imaging, vol. 31, no. 2, pp. 312-325, 2012.

[4] F. Fischer, M. A. Selver, W. Hillen and G. Guzelis, "Integrating Segmentation Methods From Different Tools Into a Visualization Program Using an Object-Based Plug-In Interface," IEEE Transactions on Information Technology in Biomedicine, vol. 14, no. 4, pp. 923-934, 2010.

[5] Donoho, D.L., "Compressed sensing," IEEE Transactions on Information Theory, vol. 52, pp. 12891306, 2006.

[6] Elad, M., "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries," IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736-3745, 2006.

92

[7] J. Mairal, M. Elad and G. Sapiro, "Sparse representation for color image restoration," IEEE Transactions on Image Processing, vol. 17, pp. 53-69, 2008.

[8] S. Armato, "CAD dissects growing volume of data from lung CT exams," Diagnostic Imaging, pp. 612, 2003.

[9] R.C. Gonzalez and R.E. Woods, Digital Image Processing, New Jeresy: Prentice Hall Publishing company, 2002.

[10] K.J. Batenburg and J. Sijbers, "Adaptive thresholding of tomograms by projection distance minimization," Elsevier Journal on Pattern Recognition, vol. 42, no. 10, pp. 2297-2305, 2009.

[11] G.N. Srinivasan and G. Shobha, "Segmentation Techniques for Target Recognition," International Journal of Computers and Communications, vol. 1, no. 3, pp. 313-333, 2007.

[12] M. Elad, Sparse and Redundant Representation, Springer New York Dordrecht Heidelberg London, 2010.

[13] J. K. Pillai, V. M. Patel and R. Chellappa, "Sparsity inspired selection and recognition of iris images," in IEEE International Conference on Biometrics, 2009.

[14] G. Davis, S. Mallat and M. Avellaneda, "Adaptive greedy approximations," Journal of Constructive Approximation, pp. 57-98, 1997.

[15] Zhang, S. Mallat and Z., "Matching pursuits with time-frequency dictionaries," IEEE Transactions on Signal processing, pp. 3397-3415, 1993.

[16] S. Chen, S.A. Billings and W. Luo, "Orthogonal least squares methods and their application to non-

93

linear system identification," International Journal of Control, vol. 50, no. 5, pp. 1873-1896, 1989.

[17] Y.C. Pati, R. Rezaiifar and P.S. Krishnaprasad, "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition," Conference Record of The Twenty Seventh Asilomar Conference on Signals, Systems and Computers, 1993.

[18] Tropp, J.A., "Greed is good: Algorithmic results for sparse approximation," IEEE Transactions on Information Theory, pp. 2231-2242, 2004.

[19] S.S. Chen, D.L. Donoho and M.A. Saunders, "Atomic decomposition by basis pursuit," SIAM Review, pp. 129-159, 2001.

[20] Rao, I.F. Gorodnitsky and B.D., "sparse signal reconstruction from limited data using, FOCUSS: A reweighted norm minimization algorithm," IEEE Transactions on Signal Processing, vol. 45, no. 3, pp. 600-616, 1997.

[21] M. Aharon, M. Elad and A. Bruckstein, "KSVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation," IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311-4322, 2006.

[22] M. Aharon, "Overcomlepet Dictionaries for sparse representation of Signals," PhD Thesis, 2006.

[23] R. Rubinstein, M. Zibulesky, M. Elad, "Efficient Implementation of the KSVD algorithm Using Batch Orthogonal Matching Pursuit," Technical Report- CS Technion, 2008.

[24] H. Wang, J. Vieira, P. Ferreira, B. jesus and I. Duarte, "Batch Algorithms of matching pursuit and orthogonal matching pursuit with applications to compressed sensing," in International Conference

94

on Information and Automation (ICIA), Aveiro Prtugal, 2009.

[25] D.L. Donoho, Y. Tsaig, I. Drori; Starck, J.L., "Sparse Solution of Underdetermined Systems of Linear Equations by Stagewise Orthogonal Matching Pursuit," IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1094-1121, February 2012.

[26] A. Miller, Subset selection in regression, 2nd Edition ed., London: Chapman and Hall, 2002.

[27] Stuetzle, J.H. Friedman and W., "Projection Pursuit Regression," Journal of the American Statistical Association, vol. 76, pp. 817-823, 1981.

[28] Zhang, S. Mallat and Z., "Adaptive time-frequency decompositions," Optical Engineering, vol. 33, no. 7, pp. 2183-2191, 1994.

[29] Donoho, S. Chen and D., "Basis Pursuit," in Twenty-Eighth Asilomar Conference on Signal, Systems and Computers, 1994.

[30] J. Tropp, "Topics in Sparse Approximation," PhD Thesis, University of Texas at Austin , 2004.

[31] A. Florian, A. Potra and S.J Wright, "Interior-point methods," ELSEVIER, vol. 124, no. 1, pp. 281-302, 2000.

[32] U. Y. Desai, "DCT and wavelet based representations of arbitrarily shaped image segments," in IEEE International Coneference on Image Processing , 1995.

[33] A. B. Watson, "Image Compression Using the Discrete Cosine Transform," Mathematica Journal, vol. 4, no. 1, pp. 81-88, 1994.

95

[34] A. Averbuch, D. Lazar and M. Israeli, "Image compression using wavelet transform and multiresolution decomposition," IEEE Transactions on Image Processing, vol. 5, no. 1, pp. 4-15, 1996.

[35] Field, B.A. Olshausen and D.DJ., "Natural image statistics and efficient coding," NetworkComputation in Neural Systems, vol. 7, no. 2, pp. 333-339, 1996.

[36] K. Engan, S.O. Aase and J.H. Husoy, "Multi-frame compression: Theory and design," EURASIP Signal Processing, vol. 80, no. 10, pp. 2121-2140, 2000.

[37] J. Mairal, F. Bach, J. Ponce and G. Sapiro, "Online Dictionary Learning For Sparse Coding," in IEEE 26th International conference on Machine Learning , Montreal, Canada, 2009.

[38] H. Lee, A. Battle, R. Raina and A.Y. Ng, "Efficient sparse coding algorithms," in Advances in Neural Information Processing Systems, 2006.

[39] I. Ramirez, P. Sprechmann and G. Sapiro, "Classification and clustering via dictionary learning with structured incoherence and shared features," in Computer Vision and Pattern Recognition , 2010.

[40] J. MAiral, F. Bach, J. Ponce, G. Sapiro and A. Zisserman, "Discriminative learned dictionaries for local image analysis," in Computer Vision and Pattern Recognition, 2008.

[41] Li, Q. Zhang and B., "Discriminative K-SVD for dictionary learning in face recognition," in Computer Vision and Pattern Recognition (CVPR), 2010.

[42] M. Yang, L. Zhang, X. Feng and D. Zhang, "Fisher Discrimination Dictionary Learning for Sparse Representation," in IEEE International Conference on Computer Vision (ICCV) , 2011.

96

[43] J. Wright, Y. Ma, J. Mairal, G. Sapiro and S. Yan, "Sparse representation for computer vision and pattern recognition," IEEE Proceedings, vol. 98, no. 6, pp. 1031-1044, 2010.

[44] E. Candes, J. Romberg and T. Tao, "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information," IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489-509, 2006.

[45] J. Wright, A. Y. Yang, A. Ganesh, S. Sastry and Y. Ma, "Robust face recognition via sparse representation," IEEE Transactions on Patteren analysis and Machine intelligence , vol. 31, no. 2, pp. 210-227, 2009.

[46] J. Yang, J. Wright, T. Huang and Y. Ma, "Image super-resolution as sparse representation of raw image patches," in IEEE conference on Computer Vision and Pattern Recognition, 2008.

[47] P. K. Baheti and M. A. Neifield, "Feature-specific structured imaging," Applied Optics, vol. 45, no. 28, pp. 7382-7391, 2006.

[48] P. K. Baheti and M. A. Neifeld, "Random projections based feature-specific structured imaging," Optics Express, vol. 16, no. 3, pp. 1764-1776, 2008.

[49] J. Romberg, "Imaging via Compressive Sampling," IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 14-20, 2008.

[50] J.-L Starck, M. Elad and D. L. Donoho, "Image decomposition via the combination of sparse representations and a variational approach," IEEE Transactions on Image Processing, vol. 14, no. 10, pp. 1570-1582, 2005.

97

[51] I. Daubechies, M. Defrise and C. De Mol, "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint," Pure and Applied Mathematics , vol. 57, no. 11, pp. 14131457, 2004.

[52] R. Duda, P. Hart, D. Stork, Pattern Classification, John Wiley & Sons, 2001.

[53] T. Guha and R. Ward, "A SPARSE RECONSTRUCTION BASED ALGORITHM FOR IMAGE AND VIDEO CLASSIFICATION," in IEEE International Conference on Acoustics, Speech and Signal Processing , 2012.

[54] R. Sivalingam, G. Somasundaram, V. Morellas, N. Papanikolopouls, O. Lotfallah and Y. Park, "Dictionary learning based object detection and counting in traffic scenes," in IEEE International Conference on Distributed Smart Cameras, 2010.

[55] T.F. Chan and L.A. Vese, "Active Contours Without Edges," IEEE Transactions on Image Processing, vol. 10, pp. 266-277, 2001.

[56] Y. Zheng, A. Barbu, B. Georgescu, M. Scheuering and D. Comaniciu, "Four-Chamber Heart Modeling and Automatic Segmentation for 3-D Cardiac CT Volumes Using Marginal Space Learning and Steerable Features," IEEE Transactions on Medical Imaging, vol. 27, no. 11, pp. 1668-1681, 2008.

[57] A. M. Ali, A. A. Farang and A. S. El-Baz, "Graph Cuts Framework for Kidney Segmentation with Prior Shape Constraints," in Medical Image Computing and Computer Assisted Intervention (MICCAI), 2007.

[58] S. E. Yuksel, A.El-Baz, M. El-ghadr, T. Eldiasty and M. A. Ghoneim, "A Kidney Segmentation Framework for Dynamic Contrast Enhanced Magnetic Resonance Imaging," Journal of Vibration and Control, vol. 13, no. 9-10, pp. 1505-1516, 2007.

98

List of Accepted Publications:
 A. Julazadeh, J. Alirezaie and Paul Babyn, "A novel automated approach for segmenting lateral ventricle in MR images of the brain using sparse representation classification and dictionary learning ", IEEE 11th international conference on information science, signal processing and their applications (ISSPA), Montreal, Quebec, Canada. 2012  A. Julazadeh, M. Marsousi and Javad Alirezaie, "classification based on sparse representation and Euclidian distance", IEEE 26th international conference on visual communication and image processing (VCIP), San Diego, California, USA. 2012

99

