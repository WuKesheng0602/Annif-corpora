Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2008

Data Denoising By Noise Invalidation
Nima Nikvand
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Electrical and Computer Engineering Commons Recommended Citation
Nikvand, Nima, "Data Denoising By Noise Invalidation" (2008). Theses and dissertations. Paper 1172.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

DATA DENOISING BY NOISE INVALIDATION

by

Nima Nikvand Bachelor of Science, Isfahan University of Technology, 2005

A thesis presented to Ryerson University in partial fulfillment of the requirement for the degree of Master of Applied Science in the Program of Electrical and Computer Engineering Toronto, Ontario, Canada, 2008

©

Nima Nikvand 2008

PROPERTY OF RYERSON UNJVERSITY LIBRARY

Author's Declaration
I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend this thesis to other institutions or individuals
for

the;lc]d:m.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

' ii

Abstract
Data Denoising by Noise Invalidation
©
Nima Nikvand, 2008

Master of Applied Science (MASc) Department of Electrical and Computer Engineering Ryerson University

In this thesis, the problem of data denoising is studied, and two new denoising approaches are proposed. Using statistical properties of the additive noise, the methods provide adaptive data-dependent soft thresholding techniques to remove the additive noise. The proposed methods, Point-wise Noise Invalidating Soft Thresholding (PNIST) and Accumulative Noise Invalidation Soft Thresholding (ANIST), are based on Noise Invalidation. The invalidation exploits basic properties of the additive noise in order to remove the noise effects as much as possible. There are similarities and differences between ANIST and PNIST. While PNIST performs better in the case of additive white Gaussian noise, ANIST can be used with both Gaussian and non Gaussian additive noise. As part of a data denoising technique, a new noise variance estimation is also proposed. The thresholds proposed by NIST approaches are comparable to the shrinkage methods, and our simulation results promise that the new methods can outperform the existing approaches in various applications. We also explore the area of image denoising as one of the main applications of data denoising and extend the proposed approaches to two dimensional applications. Simulations show that the proposed methods outperform common shrinkage methods and are comparable to the famous BayesShrink method in terms of Mean Square Error and visual quality.

iii

Dedication
Not a single page of this thesis would have been possible without the help, support and mentorship of Dr Soosan Beheshti. She helped me when I needed help the most, and her humane and friendly behavior with her students will always be my model. I chose this topic under her supervision; her knowledge and creative mind was an invaluable source of inspiration in solving various problems I faced during the research. I will always be proud to have worked under her supervision, and this thesis is dedicated to her.

iv

Acknowledgments
This thesis would not have been possible without the help of my parents. My father,

J ahansha, has always been the main source of support and confidence in my life
and I have relied on his constant encouragement and love throughout my academic career. His love and sacrifice for me will always be a great source of inspiration in my life. I thank My mother, Forough, whose unconditional love, support and courage helped me withstand hardship and make the best decisions in my life. I will always be thankful to both of them, and I don't believe I will ever be able to love anyone as much as they love me.

v

Table of Contents
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Acknowledgments 1 Introduction iii
v

1

2 Background 2.1 Wavelet Transform . . . . . . . . . . . . . . . 2.1.1 Continuous Wavelet Transform (CWT) 2.1.2 Discrete Wavelet Transform (DWT) . 2.2 Wavelet Shrinkage 2.3 Data Denoising . . 2.3.1 VisuShrink 2.3.2 SureShrink. 2.3.3 BayesShrink . 2.3.4 Minimum Noiseless Description Length (MNDL) . 2.4 Concluding Remarks . . . . . . . . . . . . . . . . . . . . 3 Thresholding Method 3.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Quality of the Thresholding Approaches . . . . . . . . . . . 3.2 Accumulative Noise Invalidating Soft Thresholding (ANIST) Method 3.2.1 Expected Value and Variance of Noise Effects in the Absence of Data . . . . . . . . . . . . 3.2.2 Noise Statistics in ANIST .. 3.2.3 Noise Invalidation in ANIST . 3.2.4 Selection of a Proper A . . . . 3.2.5 Simulation Results for ANIST 3.3 Point-wise Noise Invalidating Soft Thresholding (PNIST) Method 3.3.1 Expected Value and Variance of Noise Effects in the Absence of Data . . . . . . . . . . . 3.3.2 Noise Statistics in PNIST .. 3.3.3 Noise Invalidation in PNIST . 3.3.4 Simulation Results for PNIST 3.4 Concluding Remarks . . . . 4 Additive Noise in Denoising 4.1 Noise Estimation in MNDL Approach . 4.2 Proposed Noise Variance Estimation Method . 4.3 Noiseless Data with Gaussian Properties . . . 4.3.1 Conditional Distributions and Data Denoising

5 6
6 7

10 12 12 12 13 15 17

18 18 19 20
20 21 22 25 27 31 31 32 36 38 40
43

I.

44

46
49 51

Vl

4.4

Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . 55
56 61
63

5 Image Denoising 6 Conclusion

Appendices A Distribution of Order Statistics of a Random Variable . B Gaussian Data with Additive Noise . . . . . . . . . . .

63
65

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

vii

List of Tables
3.1 3.2 3.3 3.4 3.5 4.1 5.1 MSE by different A values. . . . . . . . . . . . . . . . . . . . . . . . 26 Proposed Thresholds by each method for Blocks test signal. Optimum threshold is the best threshold possible. . . . . . . . . 29 MSE for different Methods compared. . . . . . . . . . . . . . . . . 31 MSE for different Methods compared.(l)ANIST (2) PNIST. . . . 38 MSE by Method for Blocks signal contaminated by uniform noise. 42 Estimated variance by the proposed method. . MSE for different Methods compared . . . . . . 51
57

viii

List of Figures
2.1 2.2 2.3 2.4 2.5 3.1 3.2 3.3 Short Time Fourier Transform windows, narrow in time domain means wide in frequency domain and the opposite. Image source: [27]. 6 Block diagram of filters equivalent to wavelet decomposition. Image source: [42]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Block diagram of filters banks for multilevel wavelet decomposition. Image source:[42]. . . . . . . . . . . . . . . . . . . 9 Four level Decomposition of Einstein. . . . . . . . 10 Left: Hard thresholding, Right: Soft thresholding. 11 Histogram of noise for a 2 = 1 at Top left: m = 1, Top right: m = 2, Bottom left m = 3, Bottom right m = 4. . . . . . . . . . . . . . . . Histogram of noise for a 2 = 1 at Top left: m = 10, Top right: m = 20, Bottom left: m = 100, Bottom right: m = 1000. . . . . . . Noise behavior for a 2 = 1 Top left: Histogram at m = 10, Bottom left: Norm plot at 'm = 10, Top right: Histogram at rn = 20, Bottom right: Norm plot at m = 20. . . . . . . . . . . . . . . . . . . . . . . Noise behavior for a 2 = 1 Top left: Histogram at m = 100, Bottom left: Norm plot at m = 100, Top right: Histogram at m = 1000, Bottom right: Norm plot at m = 1000. . . . . . . . . . . . . . . . . E(w~) and upper and lower bounds for,\= 5, a 2 = 1. . . . . . . . '1/Jm of contaminated Blocks signal crossing upper bound of Invalidation region@ m* = 956 in decimal plot. Shaded area is Gaussian noise region with a 2 = 4. . . . . . . . . . . . . . . . . . . . . . . . . Test signals and their noisy forms. Top left: Blocks signal, Top right: Mishmash signal, Bottom left: Noisy Blocks with a 2 = 1, Bottom right: Noisy Mishmash with a 2 = 1. . . . . . . . . . . . . . . . . . Test signals and their wavelet coefficients. Top left: Blocks signal, Top right: Mishmash signal, Bottom right: Blocks coefficients, Bottom left: Mishmash coefficients. . . . . . . . . . . . . . . . . . . . '1/Jm and E('l/J'!/n) for Blocks, a 2 = 1. The cross point is at m* = 12. '1/Jm and E('l/J'!/n,) for Blocks, a 2 = 4. The cross point is at m* = 986. Top: E('l/J'!/n) and '1/Jsm for Mishmash, a 2 = 1, Bottom: zoomed version of top figure. The cross point is at m* = 22. . . . . . . . . . . . . . Histogram of noise for a 2 = 1 at Top left: m = 1, Top right: m = 2, Bottom left: m = 3, Bottom right: m = 4 for PNIST. . . . . . . . . Histogram of noise for a 2 = 1 at Top left: m = 10, Top right: m = 20, Bottom left: m = 100, Bottom right: m = 1000, for PNIST. Histogram of noise for a 2 = 1 at Top left: m = 10, Bottom left: Norm plot at m = 10, Top right: m = 20, Bottom left: Norm plot m = 20 for PNIST. . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix

22 23

23

3.4

3.5 3.6

24 25

26

3. 7

28

3.8

3.9 3.10 3.11 3.12 3.13 3.14

28 29 30 30 33 33

34

3.15 Histogram of noise for r7 2 = 1 at Top left: m = 100, Bottom left: Norm plot at m = 100, Top right: m = 1000, Bottom right: Norm plot m = 1000 for PNIST. . . . . . . . . . . . . . . . . . . . . . . . 3.16 Probability distribution function of at m = 20 compared with Gaussian probability distribution function of the same mean and variance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.17 Probability distribution function of at m = 100 compared with Gaussian probability distribution function of the same mean and variance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.18 E(r:n) and its upper and lower bounds for A= 5. . . . . . . . . . . 3.19 "Ym of contaminated Blocks signal crossing upper bound of Invalidation region @ m* = 700 in decimal plot. Shaded area is Gaussian noise region with r7 2 = 4. . . . . . . . . . . . . . . . . . . . . . . . 3.20 PNIST: "Ym and upper bound for noise average in case of Blocks, m* = 862, r7 2 = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.21 PNIST: "Ym and upper bound for noise average in case of Mishmash, m,* = 8. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.22 Uniform noise with r7 2 = 4. . . . . . . . . . . . . . . . . . . . . . . . 3.23 '1/J":n histogram and normplot for r7 2 = 4 at m = 100 and m = 1000. Note that sample data falls exactly on the straight line. . . . . . . .

r:n

34

r:n

35

36 38

39 39 40 41 41 45 47 47

4.1 4.2 4.3 4.4
li'·'

4.5 4.6 4. 7 4.8 4.9 4.10 5.1 5.2 5.3 5.4

Data description length versus a in MNDL method when the true variance is CJ 2 = 2. . . . . . . . Autocovariance of noise. . . . . . . . . . . . . . . . . . . . . . . . . Au tocovariance of noisy signal. . . . . . . . . . . . . . . . . . . . . Autocovariance graph for different test variances, from left to right: (a) rJ 2 = 1 (b) r7 2 = 4 (c) r7 2 = 9 (d) r7 2 = 16. True noise variance is 2 ' r7 = 9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Area under autocovariance of discarded coefficients for true variance of f7 2 = 4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Area under autocovariance of discarded coefficients for true variance of f7 2 = 8. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Histogram of Mishmash coefficients. . . . . . . . . . . . . . . . . . . Noisy Gaussian signal, noisy Mishmash signal and Gaussian noise compared with 210 samples. . . . . . . . . . . . . . . . . . . . . . . Noisy Gaussian signal, noisy Mishmash signal and Gaussian noise compared with 220 samples. . . . . . . . Distribution of f(niy =a) & f(xly =a). . . . . . . Barbara denoised by ANIST and PNIST, Cameraman denoised, rJ; = 100. Lena denoised, rJ; = 400 .. . Einstein denoised, rJ; = 900 .. .

48 50 50 52 52 53 54 57 58 59 60

rJ; = 225.

X

Chapter 1 Introduction
Data denoising is a well-known problem that is the concern of diverse application areas. For example, for generations, communication system designers have been trying to implement techniques to get rid of the natural nuisance and interference that corrupts the message signal; Thermal noise exists in all electronic elements, furthermore, channel introduces degradation to message signal, and defective instruments and data acquisition systems affect the quality of the observed data as welL Thus denoising is a necessary process in the receiver of all modern communication systems. Linear filters like Wiener filters are among the most simply implemented denoising methods [21]. More complicated methods may introduce nonlinearity to Wiener filters [44] or use other nonlinear techniques such as Median filters [44, 24]. A relatively new approach is called Wavelet Shrinkage and is based on rejecting those wavelet coefficients of the observed data that are smaller than a certain threshold value [10]. The principle behind this method is that wavelet transforms enable us to represent the signal with a high degree of sparsity [10]. Sparsity of wavelet coefficients, meaning that most of the signal information in wavelet expansion is conveyed by a relatively small number of large coefficients, helps us find an estimate of the signal after rejecting some of the coefficients considered as noise and yet keeping most of the information carried by the signal in the remaining coefficients [10, 5, 12]. Using wavelet transforms as a tool for denoising of signals has drawn a lot of research attention in the past decade leading to the famous shrinkage algorithms[10, 8, 3, 36, 2]. The first method proposed based on Wavelet Shrinkage was Visu Shrink[10]. VisuShrink proposed by

1

Donoho and Johnstone applies a universal threshold that depends only on noise variance and data length. SureShrink, proposed by the same authors, minimizes
Stein's Unbiased Risk Estimator (SURE) to find a threshold [8]. In BayesShrink,

an important image denoising approach, a threshold that minimizes Bayesian Risk is found based on the assumption that the image has the properties of a Generalized Gaussian signal [3]. Another recently introduced thresholding technique is

called Minimum Description Length (MDL) [36]. MDL chooses a threshold that minimizes Normalized Maximum Likelihood (NML) of the noisy data in each subspace, and Minimum Noiseless Description Length (MNDL) suggests choosing a subspace which minimizes description length of the noiseless data [2]. In this thesis we propose two new thresholding methods based on Noise Invalidation. It is shown that a proper threshold can be found based on Gaussian invalida-

f,

tion of coefficients or a function of coefficients out of the noise region. In chapter two a brief review on one dimensional and two dimensional wavelet transforms and a survey of related shrinkage methods are provided. The Shrinkage methods,
1
~, .,,

originally proposed by Donoho and Johnson [10, 8], play a key role in data denoising. Some of these methods are designed to apply a threshold based on limit theorem and some others assume a prior for the noiseless data and minimize the error using the prior such as bayeshrink[3]. BayesShrink, a thresholding method originally proposed for Image denoising applications, is applied to one dimensional signals in this research and it shows acceptable results for those signals that follow a Gaussian distribution. Working with noisy part of the signal in MNDL inspired us to examine the statistical properties of noise and noisy coefficients of a sorted signal, which led to the introduction of two novel soft thresholding methods in chapter three.

2

Chapter three introduces the two novel thresholding techiques and compares the results to other shrinkage methods. Based on statistical properties of noise and noisy parts of the signal, it is shown that a Gaussian approximation can be used to invalidate signal coefficients or a function of sum of square added coefficients of the observed signal at each point. Thanks to the fact that we are using a summation of weakly independent random variables in one of our methods, we can benefit from results of Central Limit Theorem for weakly dependent variables and apply the method for non Gaussian noise situations as well. Chapter four proposes a novel noise variance estimation method. Estimation of noise variance is crucial to any denoising algorithm and the literature on this subject is limited. While most methods pick the fine scale coefficients through an ad-hoc threshold to estimate noise, the novel method proposed in this research uses autocovariance properties of Gaussian noise combined with sparsity of wavelet coefficients to propose estiamte of the noise variance. In chapter four, properties of Gaussian signals are also studied and it is shown that a large class of signals, like Mishmash signal follow Guassian distribution and have similar behavior in the eye of the two proposed methods in chapter three. Using this fact, we propose a data and subband rejection criterion that ensures the share of noiseless signal is larger than the share of noise in the observed set of data with at least fifty percent probability. This gives us a data rejection criterion which is more conservative than that of BayesShrink. The proposed criterion ensures that a set of observed data or a subband of the data is worth going through the rest of the denoising procedure and can be used as a pre-processing condition. Chapter five applies the proposed thresholding techniques to images and compares the results with the mentioned methods. In this chapter, wavelet transform of noisy images are found and after pre-processing with the novel data rejection criterion proposed in chapter four, a global threshold is

3

found through either of the proposed thresholding methods. This threshold is then applied to soft thresholding function and the smoothed data is then taken for inverse wavelet transform and the denoised Image is reconstructed. The rest of the chapter compares other shrinkage methods in terms of mean square error and visual quality to the proposed methods and performance of the methods are compared for standard Images such as Barbara, Cameraman, Einstein and Lena.

4

Chapter 2 Background
The main idea of Data Denoising is to find a basis which represents the data better than the noisy data itself. One of the useful orthonormal bases is Fourier Transform {FT). Fourier transform provides us with frequency components of a signal which prove to be invaluable in many applications including, linear time invariant analysis. However, Fourier transform is unable to provide information in time and frequency domain simultaneously. This means that Fourier transform provides us with the frequency components, but it does not tell us when they happen in time domain. This is not an issue in many applications where only frequency components are needed such as speech processing. However, in some applications such as Image processing, we need to have time and frequency information of the signal simultaneously. To solve this problem, a general idea would be to divide the signal into different parts and analyze frequency components of these parts separately. One of the early solutions to this problem was Short Term Fourier Transform {STFT) [15]. STFT proposes moving a window across the signal to analyze the frequency domain of the signal for each part. The problem still remains since STFT has a fixed resolution. This property is due to Heisenberg's uncertainty principle, which states the product of the standard deviation in time and frequency is limited. Thus measuring frequency and time can not be done simultaneously at a desired resolution. A wide window gives better frequency resolution but poor time resolution. A narrower window, on the other hand, gives good time resolution but poor frequency resolution.

5

~eq

time

time

FIGURE 2.1. Short Time Fourier Transform windows, narrow in time domain means wide in frequency domain and the opposite. Image source: [27).

2.1

Wavelet Transform

Wavelet Transform (WT) was proposed as a solution to this problem. Wavelet
transform uses a scalable modulated window to move across the signal. Using a small scale for high frequency parts and a big scale for low frequency parts enables wavelet transform to provide a good time and frequency resolution. This cycle is repeated and each time a slightly smaller or bigger window is selected. The result of the process will be a set of time frequency representations of the signal with different resolutions. These representations are called scale-time representations as opposed to frequency representations in Fourier transform.

2.1.1

Continuous Wavelet Transform (CWT)

Continuous wavelet transform is defined as:

-y(s, r)

=

1:

j(t)if!;,r(t)dt

(2.1)

Where \ll s,T is a dilated and translated wavelet function made of the mother wavelet function as the following:
\lf S,T

= ~"''(tT) r;_ 'f/
yS
S

(2.2)

where

~(t)

is the mother wavelet function. Finally the Inverse CWT is defined as:

(2.3)

6

The two main properties of wavelet functions are admissibility and regularity. It can be shown that a square integrable function such as w(t), can be used to decompose and then reconstruct a signal when w(t) meets the admissibility condition stated as follows[38]:

1
admissibility condition implies:

oo

-oo

jw(w)l2 lwl dw <

oo

(2.4)

(2.5)
This also requires the mother wavelet signal to have a zero average across time. In other words 1 :

1:

W(t)dt = 0

(2.6)

Regularity means the wavelet function should have smoothness and concentration in both frequency and time domains [27, 28].

2.1.2

Discrete Wavelet Transform (DWT)

To implement wavelet transform, we have to overcome some barriers. The first and the most important of these barriers is redundancy of CWT. Wavelet transform is calculated by constantly shifting a continuously scalable function over the signal and finding the correlation with different parts of the signal. It is shown that these scalable functions are not orthogonal [17, 11]. Thus the resulting coefficients are redundant. In practical applications this redundancy must be removed. Another problem is that for many signals wavelet transform has no analytical solution and must be calculated numerically. To overcome these problems, Discrete Wavelet
1 This is where the name wavelet is coming from as zero average across time domain means being oscillatory or a wave

7

Transform {DWT) was proposed. DWT is achieved by changing the wavelet of

CWT in equation 2.2 to:

\I!j,k(t) = Vs£1 'J!(tSJ

~Tos&)
So

(2.7)

0

DWT reduces the number of wavelets and ensures that under certain conditions, the resulting coefficients can be used to reconstruct the original data[6]. To remove redundancy, wavelets can be selected as orthogonal to their own scales and shifted versions. Thus we choose:

1

oo

\I! j,k( t) w:n,n (t)dt =

{

1 if j
0 if j

= m & k = n,

(2.8)

-oo

f.

m, or k

f.

n.

Discrete wavelet coefficients are found:

"((j, k) =

1:

f(t)IJ!j,k(t)dt

(2.9)

and the original signal can be reconstructed:
It"' ·

J(t) =

L -y(j, k)wj,k(t)
j,k

(2.10)

Wavelets have band pass spectrum in frequency domain, and from Fourier transform we know that dilation in time domain will act as compression in frequency domain. This gives us the idea to implement DWT as a bank of filters. DWT of a digital signal x can be found as the following:
Ylow[n] Yhigh[n]

=
=

L
k=-00

00

x[k]g[2n- k] x[k]h[2n - k]

(2.11)

L
k=-oo

00

(2.12)

Where h is impulse response of a high-pass filter and g is impulse response of a low-pass filter. These filters are quadrature mirror filters of the original bandpass 8

~
x[nJ

l\pprollimation coefficiffits

-L~ ~taileo~icients

FIGURE 2.2. Block diagram of filters equivalent to wavelet decomposition. Image source: [42).

filter of the wavelet [27, 42]. Figure 2.2 shows the implementation of filters for one level decomposition. For a multi level decomposition we can implement a filter bank shown in Figure 2.3. Signal is passed through each stage of low pass and high pass filters and the result is subsampled by two at the end of the stage. The high pass filter output, which is called detail subband, is the result of decomposition at each stage. Output of the low pass filter which is called approximate sub band is then taken for further filtering stages.

x[n]

FIGURE 2.3. Block diagram of filters banks for multilevel wavelet decomposition. Image source: [42).

A similar process is used for images as for two dimensional data. It consists of alternating one decomposition by rows and another one by columns, repeating the procedure on the low-pass sub-image[27, 28, 30, 42]. Thus an image produces four subbands at every level. Figure 2.4 shows a four level decomposition of Einstein using "Haar" wavelet.
9

FIGURE 2.4. Four level Decomposition of Einstein.

In this thesis, we concentrate on using the wavelet transform as an orthonormal basis, however, the study can be generalized to any other orthonormal basis such

as Fourier transform.

2.2

Wavelet Shrinkage

Wavelet Shrinkage (WS) is a nonlinear and nonparametric signal denoising technique[lO]. Based on the sparsity of wavelet coefficients, we know that most of the information in a signal is carried by a small number of large coefficients, and the rest of the coefficients carry little or no information[lO, 30]. Thus by selecting orthogonal wavelets, this method attempts to discard the small coefficients in the orthogonal transformation of data that tend to be attributed to noise. There are two major thresholding techniques, namely Soft and Hard thresholding. Hard thresholding is simply setting to zero all the coefficients that are smaller than a certain value (threshold) . Soft thresholding is an extension of hard thresholding which sets the coefficients smaller than the threshold to zero and reduces

10

Hard Thresholding

80

100

FIGURE 2.5. Left: Hard thresholding, Right: Soft thresholding.

the remaining coefficients by the threshold value. Figure 2.5 depicts output of hard and soft thresholding techniques. Hard thresholding is defined as:

(2.13)

and soft thresholding is:

I

(2.14)

Some denoising methods use hard thresholding, and some are proper for soft thresholding. Hard thresholding, however, is not used for applications such as image denoising since it introduces artifacts to the result and is not appealing in terms of visual quality.
11

2.3

Data Denoising

Success of the thresholding technique in different applications depends on the choice of threshold. Thus choosing the proper threshold is crucial to a successful denoising. In the following sections some of the most famous threshold selection methods are reviewed.

2.3.1

VisuShrink

VisuShrink also known as Universal Threshold is the best known approach suggested by Donoho and Johnstone [10]. VisuShrink proposes a threshold that is dependent on the length of the data:
(2.15)

Where

a; is noise variance and N is the Data length.

However, VisuShrink is known to produce over smoothed. results due to the fact that the threshold is dependent on data length and increases with number of samples under the same noise variance[18].

2.3.2

SureShrink

Another well known method proposed by Donoho is based on minimizing Stein's
Unbiased Risk Estimator {SURE) and is called SureShrink[8]. SURE is an esti-

mator that is used to find an unbiased estimate of 1, · · · , N) is a vector of length Nand Y

llt1- p,jj 2

where p, = (Mi : i =

= {yi} (Yi

rv

N(O, 1)) is a vector of multi-

variate normal observations with mean vector p,. In our case J1 The risk for E(JJY- Yarigll 2 ) is found to be:
SU RE(T, y)

= Y and

p,

= Yarig·

= N- 2.#{i: IYil:::; T} +

L min(JyiJ, T)
i=l

N

2

(2.16)

and the threshold is selected as:
Tsure

= arg

min
O<T<yhlog(N)

SU RE(T, y)

(2.17)

12

SureShrink is a subband dependent thresholding technique, which means it proposes a different threshold for each subband of data. Thus N in equation (2.17) is the number of coefficients in the corresponding subband. However, the threshold proposed by SURE has serious drawbacks in sparse subbands, thus a hybrid threshold of SURE and Visu is applied:
{J

={

TJTsure (B)
rJTvisu (B)

if (Jv 2:: if

"fN "fN

(2.18)

(Fv

<
.

where

(N

2

=

LN-1 ((P-1) 3
J-

N

and

rN

=

log 2 (N)2/3

v'N

. Hybnd thresholding solves one prob-

.

lem to add another as SureShrink risks the over smoothing problem in VisuShrink due to hybrid thresholding.

2.3.3

BayesShrink

Different denoising methods assume different distributions for the noiseless data or its subbands[29, 30, 4, 3, 27] . The choice of a prior usually helps solving the optimum threshold problem through a Maximum A Posteriori (MAP) strategy or minimizing the risk function[19, 22, 20, 30, 4, 3]. In BayesShrink, proposed by Chang, we assumes a Generalized Gaussian Distribution {GGD) prior for the noiseless data and find a threshold for each subband[3]. The GGD is given by:

(2.19) where -oo <
.T

< oo and {3 > 0 with:
(2.20)

C(
ax,

{3) = {J.a(ax, {3) 2f(~)

(2.21)

13

and

(2.22) Assuming f3 (which is called Shape Parameter) has a range of 0.5 to 1, it has been observed that most noiseless images follow this distribution[3, 39, 40, 34]. Using a GGD prior, we can now minimize Bayesian Risk which is defined as the expected value of the mean square error[37, 43, 3]: (2.23) where

X is

estimate of noiseless data

(X = TJr(Y))

and Y is the observed data

(YIX"' N(x, a 2 )) with X"' GGDaxJ3· The optimal threshold is found as:

TBayes = arg min r(T)
T

(2.24)

However, there is no close form solution for equation (2.24) thus; numerical solutions are used, and it is observed that:
T* =an
2

ao

(2.25)

is closest to the optimum [3]. We need to have an and a 0 to find T*. Noise variance a; is usually estimated using Robust Median Estimator: (B)= Medianl8nl 0.675 l8nl E HH1 (2.26)

an

Variance of the observed data is estimated as:
1 N

a~= N

L B(i,j)2

(2.27)

i,j=l

14

with N being data length or number of pixels in the considered subband. Since the problem assumes noiseless data, X, to be independent of noise we have: (2.28) thus: (2.29) and by replacing the results from equations (2.29) and (2.26) in equation (2.25), we can find the closest proposed threshold to the optimum.

2.3.4

Minimum Noiseless Description Length (MNDL)

Minimum Noiseless Description Length (MNDL) is a new approach to subspace selection proposed by Beheshti and Dahleh[2]. MNDL's idea was inspired by Minimum Description Length(MDL)[36] However, in MNDL, length of noiseless data is estimated in every subspace, and the subspace with minimum noiseless data description length is selected as the optimum subspace. When Gaussian noise is added to the noiseless data vector yN the joint probability distribution function of noisy data yN can be found as follows:

= [y(1), y(2), · · · , y(N)]T,

= [y(1), y(2), · · · , y(n)]T

(2.30) and data description length is defined as:

However, equation (2.31) is not available in practical situations as noiseless data vector yN is unavailable, and we can attempt only to estimate noiseless data description length for different data representations. 15

(2.32) Thus it is zsm = ~llYN- yfm II~ that we need to estimate to find the minimum description length and the optimum subspace. Note that zsm is a sample of random variable Z sm. It is clear that if we find boundaries for Zsm we can choose the subspace which minimizes the upper bound of data description length as the optimum subspace. It is shown that Zsm follows Chi-square distribution[2] and: (2.33) obviously, Dsm is a function of p 1 , and we can find an upper and lower bound on

Zsm assuming E(Zsm) or its bounds are available.
(2.34) However, it is shown that E(Zsm) is unavailable and is dependent on another variable estimated by Xsm

=

~llYN

-

yfm I where xsm is a sample of random

variable X sm. Through validation, we can find upper and lower bounds of the this random variable: (2.35) and finally the bounds for Zsm are found with relationships that are dependent on PI and P2 probabilities:

(2.36) and the optimum subspace is selected as: (2.37)

16

The optimum threshold proposed by MNDL is found by comparing the nested subspaces of different orders. The absolute value of noisy coefficients are sorted in decreasing order. This provides a nested set of subspaces, and the optimum subspace provides the threshold:
T*

=

m~n
t

l8(i)l, i

E

Jisi

E

s:n.

(2.38)

However, MNDL-SS assumptions for the additive noise are not working in thresholding because sorting the coefficients provides subspaces that are no longer independent and the noise vectors of the nested subspaces are correlated. MNDL-SS assumes random choice of subspaces which leads to independent additive noise veetors. It is later shown in MNDL thresholding [13] that by estimating the noisy part of Zsm through square adding sorted coefficients of a certain number of generated noise vectors in ascending order, we can provide a better estimate of E (Z sm) in thresholding applications.

2.4

Concluding Remarks

In this chapter, wavelet transform, wavelet shrinkage and popular wavelet shrinkage methods were reviewed. Wavelet shrinkage is a non linear, non parametric, approach to data denoising. VisuShrink and MNDL are usually used for one dimensional data but can be extended for two dimensional data as well. SureShrink and BayesShrink are commonly applied to images. BayesShrink outperforms SureShrink and other methods in image denoising and is a suitable solution for daily image denoising applications.

17

Chapter 3 Thresholding Method
3.1
noise:

Problem Statement

Noiseless data f)= [fJ1 , y2 , · · · , iJN]T of length N has been corrupted by an additive

y[n] = y[n] + w[n]
where wN

(3.1)

= [w[1], w[2], · · · , w[N]]T is an independent and identically distributed

(i.i.d) Gaussian random process with zero mean and variance a 2 . In the denoising process, we first project the noisy data into an orthogonal basis. The considered orthogonal basis
sis

are orthonormal and we have:

<
and < is:
si, Sj

Si, Sj

>-

- { 1 if 'l
Q

=

j,

(3.2)
The ith coefficient of yN

if i =I= j.
si

> is the inner product of vectors,

and

Sj·

(3.3)
and we have:

O[i] = O*[i] + Ow[i]
where 0* is the desired coefficient of the noiseless data:

(3.4)

f)

=

I: 0* (
i=l

N

i)si,

y

=

I: 0(
i=l

N

i)si

(3.5)

and noise coefficients vector,

o;: = [Ow[1], Ow[2], · · · , Ow[N]], is a white Gaussian
18

random process with the same mean and variance of wN due to the orthonormality

of the bases :

E(Bw) = 0, var(Bw) =a~

(3.6)

In a denoising approach, the available coefficients of noisy data are thresholded with the purpose of providing the best estimate of the noiseless data coefficients. As was mentioned in chapter two, there are two main types of thresholding, hard thresholding and soft thresholding. Although we briefly studied the hard thresholding approach, our focus in this thesis is on soft thresholding approaches. We denote the thresholded coefficients with soft threshold T 8 , as B(:

f)((i) =

,. .

{ sgn(B(i))(l B(i) I -Ts) if j8(i)j 2:: Ts,
0 otherwise

(3.7)

where ( is the number of non-zero coefficients resulted from thresholding with

( = #{ B[i], 1 ~ i

~

N : j8[i] I 2:: Ts}

(3.8)

The estimate of the noiseless data with this threshold is denoted by:
N

[}( =

2::: Bdi]si
i=l

(3.9)

Based on selected coefficients by thresholding function , equation (3.9) provides an estimation of the noiseless data.

3.1.1

Quality of the Thresholding Approaches

For comparison and evaluation of the thresholding approaches we consider the measure of quality as mean squared error (MSE):
(3.10)

19

(the equality of errors in data and bases spaces is a direct result of Parseval's theorem.) An estimator with smaller MSE is considered as a better estimator.

3.2

Accumulative Noise Invalidating Soft Thresholding (ANIST) Method

After finding the coefficients for the observed data and sorting them in ascending order, we define a new variable, 7/Jm such that it is the sum of squared version of the first "m" values:

7/Jm =

L
n=l

m

IBsort[n]l =

2

L
n=l

m

IB;ort[n] + v[n]l 2

(3.11)

where Bsort is the set of ascending sorted version of coefficients based on the absolute value of the coefficients, and 0* and v are the corresponding noiseless and noise parts of Bsort· Here, v[n] is a sample of random variable V[n] and 7/Jm is a sample of random variable Wm· In the following sections we are looking for the proper values of m for soft thresholding and present a new approach by noise invalidation. Note that since the coefficients are sorted in an ascending order, the value of m, in our method provides the ( in equation (3.8) with the following relation:

(=N-m

(3.12)

3.2.1

Expected Value and Variance of Noise Effects in the Absence of Data

Here we study the properties of the additive noise in 7/Jm as a function of m. If the discarded coefficients in the thresholding process with T 8 are only noise, then we denote 7/Jm in (3.11) with 7/J~ . In this case, 7/J~ is a sample of random variable w~ with the following expected value and variance:
m

E(w~) = E(L V 2 [n])
n=l

(3.13)

20

m

(3.14)
n=l

The expected value of noisy part of 'l/Jm and its variance play key roles in our analysis, and exact calculation of these variables can be cumbersome. However, in this scenario with B* [n]

= 0; 1

~

n

~

m, the ascending coefficients of noisy data

are exactly the ascending coefficients of the noise. This will allow us to estimate the expected value and variance by generating L samples of Gaussian random vectors of length m and averaging them, assuming vf[n] denotes the sorted version of the 'ith set of the Gaussian noise:
(3.15)

(3.16)

3.2.2

Noise Statistics in ANIST

As we sort and add squared coefficients of generated noise samples ('l/J"/n), we observe that as m increases, the random variable begins to behave similar to Gaussian distribution and can be well approximated by that. This can be seen by generating and plotting the histogram for a large number of samples of this random variable. Figure 3.1 and Figure 3.2 show that the histogram starts with a shape that is similar to chi-square distribution. However, as m grows, the distributions are very similar to that of Gaussian. Thus, the distribution of the sum of sorted squared noise tends to normality as degree of freedom approaches infinity or m increases. This property can be used to probabilistically invalidate the signal from noise and find the proper threshold. To elaborate this property more precisely, Figures 3.3 and 3.4 display normal probability plot of the data. Normal probability plot determines if the sample data 21

400 300 200 100 0 -5 200 150 100 50 0 -5 0 m=3 5 10
..
.
·'·

300 200
. . .. .. . .

100 0 -5 200 150 100 50 0 -5
0

0 m=1

5

10

0

5

10

5

10

FIGURE 3.1. Histogram of noise for a 2 Bottom left m = 3, Bottom right m = 4.

= 1 at Top left: m = 1, Top right: m = 2,

follows Gaussian distribution. When the data is Gaussian, sample data displayed with (+) symbol follows the line that is connecting first and third quartiles of the data. Other distributions will cause curvature in the plot.
tll! l 11 1

3.2.3

Noise Invalidation in ANIST

The observation of the behavior of 7/J":n indicates that there is a weak dependence among the v[n]s and therefore, for the summation of

2:::;:

1

v2 [n] the use of the

Central Limit Theorem(CLT) for large enough values of m is valid [14, 7]:

(3.17)

where error function is:

er f(x) =

fi Jo
22

1

{X

2

e-t dt

(3.18)

-1

0 m-1000

FIGURE 3.2. Histogram of noise for a 2 = 1 at Top left: m = 10, Top right: m = 20, Bottom left: m = 100, Bottom right: m = 1000.

m-10

X

10-.'1

6 m·20
Normal Probability Plot

10

12

14

x to-·

Normal Probability Plot

0.999 0.997

8:aa
0.95 0.90
~ 0.75

~

... ~++

0.95 0.90
~ 0.75

a.. 0.25
0.10 0.05

~

0.50

J
0.5 1.5 2 m·10 2.5 3.5 X 10-.'1

0.50 0.25 0.10 0.05

8:8~

8:8~
8 m·20 10 12 14
X

10-.'1

FIGURE 3.3. Noise behavior for a 2 = 1 Top left: Histogram at m = 10, Bottom left: Norm plot at m = 10, Top right: Histogram at m = 20, Bottom right: Norm plot at m=20.

23

0 0.1

0.2

0.3

0.4

0.5 0.6 m-100

0.7

0.8

0.9

800

850 900 m-1000

950

1000

1050

Normal Probability Plot 0.999 0.997 0.999 0.997

Normal Probability Plot

8 :BB
0.95 0.90

8 :RB
0.95 0.90
~

I
0..

.~

0.75 0.50 0.25 0.10 0.05 0.003 ~t 0.001 + 0.3

0.75 0.50 0.25 0.10 0.05

1
0..

~

8:8~

8:8~
0.003 0.001 +·' 0.4 0.5 0.7 0.6 m- 100 0.8 0.9

; . .,t--'
750 800 850 m- 1000 900 950 1000

FIGURE 3.4. Noise behavior for a 2 = 1 Top left: Histogram at m = 100, Bottom left: Norm plot at m = 100, Top right: Histogram at m = 1000, Bottom right: Norm plot at m = 1000.

Figure 3.5 Shows
II

E(w~)

and its upper and lower bounds provided by standard

I

.:~~ ~ll

J ;:

deviation for Gaussian noise with unit variance. If A is selected large enough, all noise cases fall between these two bounds with a close to one probability. On the other hand, for a given data, the '1/Jm in equation (3.11) is available. We can use the available property of '1/J":n in Figure 3.5 to check for which m, the available

'1/Jm is in the range of '1/J":n with probability er f(.X/ .;2). As Figure 3.6 shows, the '1/Jm will leave the boundaries of 't/J":n once the presence of the noiseless coefficients
is effective. Therefore, the invalidation criterion in this case is

(3.19)

and the desired m* is found by checking the following condition:

m*

= argmin({3[m] 2: 1)
m

(3.20)

24

Thus m* is the index where the 'l/Jm crosses the upper bound of 'l/J":n and the corresponding threshold with invalidation probability er f(,\/ J2) is:

TANIST

= IBsort[m*] I = N- m* in equation (3.8).

(3.21)

which provides the number of coefficients (

FIGURE 3.5. E(w~) and upper and lower bounds for ,\ = 5,

CJ

2

= 1.

We denote this method as Accumulative Noise Invalidating Soft Thresholding and simply refer to it as ANIST [1].

3.2.4

Selection of a Proper A

The value of,\ is responsible for our measure of certainty in the invalidation process, and it should be selected carefully as it is a sensitivity factor in our method. From the normal curve table we know that usually,\

= 3.9 is translated into probability

0.9. Thus, we expect that by choosing,\ to be in a range of 3.5 to 7, we are ensuring

that with a close to one probability, the condition of equation (3.20) is met out of

25

14000
-'I'm
- - E('f'~)+).STD('¥~ 12000 '" ' "" E('¥~)-).STD(%,)

-·-· · E('¥~)

10000

~ Noise Region

·

Crossing point

8000

6000

4000

2000

....
i
100 200 300
i

.... ~ -:.:::::

.../.... . ·'' .... ...

d!·
·1000

.
i
800
900

400

500
m

600

700

FIGURE 3.6. 'l/Jm of contaminated Blocks signal crossing upper bound of Invalidation region @ m* = 956 in decimal plot. Shaded area is Gaussian noise region with a 2 = 4.

noise invalidation process. For example, we have:

er f(3/v'2)

= 0.997300203937, er f(5/v'2) = 0.999999426697

(3.22)

On the other hand, if the selected value for A is too large, the invalidation region is larger and the sensitivity of the method is lost. To find the optimum value of A, we compare the MSE results for a range of A values in different noise variances. For this study we consider a Blocks signal seen in Figure 3. 7 with five level Haar wavelet decomposition.
3.5 a = 1 0.49 a 2 = 4 1.28 a 2 = 9 1.78
2

4 0.37 0.92 1.29

4.5 0.26 0.81 1.12

5 0.20 0.73 0.91

5.5 0.26 0.89 0.97

6
0.32 1.05 1.31

TABLE 3.1. MSE by different ..\ values.

26

Table 3.1 shows that in most cases A = 5 provides optimum results 1 . A similar procedure can be taken to show that A= 5 is proper for Mishmash signals.

3.2.5

Simulation Results for ANIST

We continue with the simulation for standard Blocks signal and mishmash signal, which are provided in MATLAB R2007b and were used as test signals in (10]. Wavelet transform is selected as the orthogonal basis for analysis and Blocks and Mishmash are selected as test signals. The two signals can represent two extreme cases. The Blocks signal has very small non-zero coefficients and Mishmash has no zero coefficients, thus they can present a very good test package between two

1n Noise (AWGN) was added to original signals with data

#300 03-04-2010 12:50PM Item(s) checked out to Abdullah, Alaa. TITLE: Data deno is ing by noise invalidat CALL#: TK7867.5 .N55 2008 BARCODE: 0010149751934 DUE DATE: 06-12-1 0 C heck You r Fi le: http://www .library.ryerson.ca/
the first 1012 (( order.

~ variances of a 2 = 1, 4, 9 were tested. Figure 3. 7 shows

ir noisy versions. Five level decomposition with "Haar" .his experiment. Figure 3.8 shows test signals with their

esult of one set of observed data. For this observation, the :.20) is met in m*

= 12 and the desired subspace will be

= N- m*) thresholded coefficients of noisy signal in descending

Figure 3.10 shows noisy Blocks signal crossing the upper bound in m* = 986. Proper threshold is selected at m * = 986 and the value of this threshold is:

TANIST
1 The

= 1Bsort(986) I
= 1000 generations. 27

results are based on statistical average for L

200

400

600

800

1000

200

400

600

800

1000

10

.' ~
-4

0

200

400

600

800

1000

200

400

600

800

1000

FIGURE 3.7. Test signals and their noisy forms. Top left: Blocks signal, Top right: Mishmash signal, Bottom left: Noisy Blocks with 0" 2 = 1, Bottom right: Noisy Mishmash with 0" 2 = 1.

600

m

40,...-----.-----.---..------.,.----,-,

30 20 10

-10'--------'----'----'-------'--_.... 1000 400 600 800 0 200

~~-~--~--~-~--~

0

200

400

600

800

1000

FIGURE 3.8. Test signals and their wavelet coefficients. Top left: Blocks signal, Top right: Mishmash signal, Bottom right: Blocks coefficients, Bottom left: Mishmash coefficients.

28

FIGURE 3.9.

1/Jm

and E( '1/J'!/n) for Blocks, a 2

= 1. The

cross point is at m*

= 12.

thus the desired subspace will be the first 38 thresholded coefficients in descending order. Figure 3.11 shows noisy Mishmash signal crossing the upper bound in (a) and a zoom in of crossing point in (b). We compare the performance of the proposed method with some of the existing methods, SureShrink[8], MNDL thresholding[2], BayesShrink[3] and VisuShrink[10]. BayesShrink, originally proposed as a soft thresholding Image denoising approach, is applied to one dimensional data in this simulation.

I OPTIMUM I SURE I MNDL I BAYES I VISU I ANIST
a a2 a2
2

=1 =4 =9

2.69 6.57 9.59

2.91 7.08 10.12

2.7112 6.02 10.04

0.58 2.27 5.12

3.10 6.78 10.21

2.13 2.31 3.74

TABLE 3.2. Proposed Thresholds by each method for Blocks test signal. Optimum threshold is the best threshold possible.

29

FIGURE 3.10. 'l/Jm and E('l/;'!/n) for Blocks, a 2 = 4. The cross point is at m* = 986.

-'I'm
- - - E(V~)+ASTD(V~)

·
100 200 300 400 500
m

600

700

800

900

1000

FIGURE 3.11. Top: E('lf;'!/n) and 'l/Jsm for Mishmash, a 2 = 1, Bottom: zoomed version of top figure. The cross point is at m * = 22.

Table 3.3 provides the results with soft thresholding and compares ANIST with existing methods. As the table shows, ANIST outperforms other shrinkage methods and provides acceptable results in comparison with MNDL and BayesShrink. 30

Blocks (j'l. = 1 (j2 = 4 (j2 = 9
(j2 (j2 (j2

I SURE! MNDL I BAYES I VISU I ANIST I
0.48 0.91 1.19 1.48 4.02 6.11 0.45 0.71 1.02 1.28 3.99 5.63 0.42 0.86 1.26
1.12 3.54 5.32

0.35 0.96 1.32 1.59 4.31 6.78

0.20 0.73 0.94

I Mishmash I
=1 =4 =9
1.32 3.34 5.21

TABLE 3.3. MSE for different Methods compared.

3.3

Point-wise Noise Invalidating Soft Thresholding (PNIST) Method

A similar approach can be used to invalidate sorted coefficients directly instead of working with a summation of squared, sorted coefficients. To work with sorted coefficients directly, we define a new random variable, 'Ym such that:

"!m

= IBsort[m]l = je;ort[m] + z[m)l

(3.23) and z are

Where

esort is the set of ascending sorted version of coefficients and ()*

noiseless and noise parts of e sort.

3.3.1

Expected Value and Variance of Noise Effects in the Absence of Data

If discarded coefficients in the thresholding process with Ts are only noise, we

denote 'Ym in equation (3.23) with "f:n_. In this case variable
r~

,:n, is a sample of the random

with the following expected value:

E(r~)

= E(Z)

(3.24)

and its variance will be:

var(r~)

= E(r~- E(r~)) 2 = E(Z- E(f~))
31

2

(3.25)

Similar to the situation in ANIST, the expected value of this noise, and its variance are crucial to our method and an exact calculation of these values can be quite cumbersome. However, the fact that ascending coefficients of noisy data are exactly the ascending coefficients of a Gaussian noise in the absence of noiseless data coefficients can be used to estimate the expected value and variance by generating
J samples of sorted Gaussian random variable and averaging them. Consider
z~,

the ith sample of a sorted Gaussian noise, then we have:

E(r~) ~

-

J I: z7[m]
J
i=l

1

(3.26)

J

1 var(r~) ~ J_

1

2 l:(z;[m]- E(r~))

(3.27)

i=l

where Z represents the random variable with sample z[n].
II

·''
;;r:

'" !!:

3.3.2

Noise Statistics in PNIST
(r~)

''"
' Ill
jill

To find the distribution of sorted noise coefficients

at each m we simulate J

samples of Gaussian random vectors and look at the histograms at each m,. As it can be seen in Figures 3.12, 3.13, 3.14 and 3.15, the distribution behaves like Gaussian distribution as m increases. In this case, order statistics can be used to analyze the probability distribution function of the
r~.

Since the additive noise of coefficients (Bws in equation

(3.4)) are independent, identically distributed (i.i.d) random variables which follow Gaussian probability distribution ~-t(O, a- 2 ) and are sorted in ascending order, thus the probability distribution function for Appendix A):
r~

can be found as follows (Proof in

fzm(x)

= nf(x)

n ( m-1

-1)
32

F(x)m- 1 (1- F(x))n-m

(3.28)

250,.-----.----r----.~-~---.-----.-----,

-4

-2

10
X

12 10-.1

120 ,------r---r--,---.----.-----.----.---.----, 100 80

100 80 60

60 40 40 20 20 0 -5
X

12
X

14 10-.1

20 10-.1

FIGURE 3.12. Histogram of noise for a 2 = 1 at Top left: m Bottom left: m = 3, Bottom right: m = 4 for PNIST.

1, Top right: m = 2,

2.1

2.2

2.3 2.4 m-1000

2.5

2.6

2.7

FIGURE 3.13. Histogram of noise for a 2 = 1 at Top left: m Bottom left: m = 100, Bottom right: m = 1000, for PNIST.

= 10, Top right: m = 20,

33

0.005

0.01

O.D15 m· 10

0.02

0.025

0.03

Normal Probability Plot

Normal Probability Plot

0.999 0.997

1
!
·I ·II

0.90 -~ 0.75 0.50 0.25 0.10 0.05

8 :HB 0.95

..r :+

++

0.999 0.997 0.95 0.90 ~ 0.75 ) 0.50 0.25 0.10 0.05 0.003 0.001

8 :HB

£

0.003 0.001

8:8~

8:8~

~i-.

.
0.01 O.D15 m- 10 0.02 0.025

0.005

~----~--~-----L----~---L----~

O.D1

O.D15

0.02

0.025 m- 20

0.03

0.035

0.04

,,

FIGURE 3.14. Histogram of noise for a 2 = 1 at Top left: m = 10, Bottom left: Norm plot at m = 10, Top right: m = 20, Bottom left: Norm plot m = 20 for PNIST.

.11 !
II II

.!!::
l l!it

IH:: lil t·

2.1

2.2

2.3 2.4 m· 1000

2.5

2.6

2.7

Normal Probability Plot

Normal Probability Plot

0.999 0.997 0.95 0.90 ~ 0.75

0.999 0.997 0.95 0.90 ~ 0.75

8:~

8:~

i

~

0.50 0.25 0.10 0.05

j

0.50 0.10 0.05 0.003 :f ,. 0.001 w + _ · _._ · ___._____.___.___~__.____._____.___.___...___, 2.05 2.1 2.15 2.2 2.25 2.3 2.35 2.4 2.45 2.5 m- 1000

d: 0.25

8:8~ .. 8:8~ ~-+ · ·
0.09 0.1 0.11 0.12 0.13 m· 100 0.14 0.15 0.16

8:8~

FIGURE 3.15. Histogram of noise for a 2 = 1 at Top left: m = 100, Bottom left: Norm plot at m = 100, Top right: m = 1000, Bottom right: Norm plot m = 1000 for PNIST.

34

Where f(x) is probability distribution function (pdf) of Gaussian distribution

J.L(O, o- 2 ) and F(x) is the cumulative distribution function (CDF).
Using equation (3.28), we can compare probability distribution function of

r:n

with Gaussian probability distribution function at different m's and ensure that the observed results in histograms are accurate.

FIGURE 3.16. Probability distribution function of r~ at m = 20 compared with Gaussian probability distribution function of the same mean and variance.

Figure 3.16 shows probability distribution function of

r:n at m, = 20 for o-

2

= 1

compared to a Gaussian distribution function of the same mean and variance. The histogram of this distribution can be seen in Figure 3.12. The small deviation from the Gaussian distribution form= 20 is evident in the histogram and pdf shown in Figure 3.14. However, Figure 3.17 shows the probability distribution function acts closely like Gaussian as m increases to 100. Thus, as expected and seen in the histograms and probability distribution function, by increasing m, the distribution of

r:n tends to behave similar to a Gaussian distri35

35~------~--------~--------~--------~--------~------~

.. I' .. I'
.. t'

1\

·

Gaussian PDF,!'. 0.0012, o. 0.0117
·

- - -PDF of~ for m· 100 , Etr, 00 )

0.0012, STD(r, 00 )

·

0.0117

}c

-

I

·

I

"
I

f

A

·

" '
I

Jc

J
0 0 0.05

'

'' · ·

\

· ·

·
\
0.15 0.2 0.25 0.3

0.1

!!
;;;
::!!

"''

FIGURE 3.17. Probability distribution function of r~ at m = 100 compared with Gaussian probability distribution function of the same mean and variance.

bution. This behavior can be used to probabilistically invalidate signal coefficients
jl""'
~ II' ·· . .!!!!

out of noise region and find the proper threshold similar to the case in ANIST.

' ,.

l
'I'

:i:l·

3.3.3

Noise Invalidation in PNIST
E(r~)

Figure 3.18 shows

and its upper and lower bounds provided by standard

deviation for Gaussian noise with unit variance. Since
r~

approximately follows Gaussian distribution, the probability of a

1:n

being bounded in a certain confidence interval such as Figure 3.18 can be approximated using Gaussian error function:

Pr{ l!:n- E(r~)l < 1} ~ er f( ~) -\vfvar(r~) V2
where the error function is:

(3.29)

er f(x)

=

fi Jo
36

1

{X

2

e-t dt

(3.30)

If A is selected large enough, with close to one probability all noise cases fall

between the two bounds. On the other hand, for a given data, the "Ym in the equation (3.23) is available. We can use the available property of

ry:n

shown in

Figure 3.18 to check for which m the available "Ym is in the range of

1:-n

with

probability er j(A/V2). As Figure 3.19 shows, the '"Ym will leave the boundaries of

ry:n

once the presence of the noiseless coefficients is effective. Therefore, the

invalidation criterion in this case is:

(3.31)

To find the proper threshold using noise invalidating the proportion proposed in 3.31 is calculated at each m. Similar to ANIST, the desired m* is found by checking the following condition:

m*

= arg min( a[m] m

~

1)

(3.32)

Thus m* is the index where "Ym crosses the upper bound of the noise mean for Gaussian invalidation, and the corresponding sorted coefficient at this point is the proper threshold.

TPNIST

= IBsort[m,*]l

(3.33)

A table similar to table 3.1 can be produced to find the proper A in this case. Simulation results show that A = 5 provides acceptable results for this approach as well. Thus, we keep our selection of A= 5. We denote this approach as Point-wise Noise Invalidating Soft Thresholding (PNIST). 37

FIGURE 3.18.

E(r~)

and its upper and lower bounds for A = 5.

3.3.4

Simulation Results for PNIST

We use the same test signals and add AWGN to original signals with a data length of 1024 and ,\ = 5 similar to the simulation for ANIST. Figure 3.20 shows !m crossing the upper bound of noise average for a 2 = 1 at
m*

= 862. Figure 3.21 shows !m crossing the upper bound for
Blocks a'2 = 1 a2 = 4 a2 = 9 I Mishmash a'2 = 1 a2 = 4 a2 = 9

a2

= 1.

I SURE I MNDL I BAYES I VISU I
0.48 0.91 1.19 0.45 0.71 1.02 1.28 3.99 5.63 0.42 0.86 1.26
1.12 3.54 5.32

0.35 0.96 1.32 1.59 4.31 6.78

(1) 0.20 0.73 0.94 1.32 3.34 5.21

I

(2) 0.18 0.76 0.90 1.24 3.29 5.27

I

I
1.48 4.02 6.11

TABLE 3.4. MSE for different Methods compared.(l)ANIST (2) PNIST.

Table 3.4 provides the results with soft thresholding and compares PNIST with ANIST and other existing methods. As the table shows, PNIST outperforms 38

35

I

--rm
30

-

- E(£!)-ASTD(r~)

........ E(r!,l - · -· E(£!)-ASTD(r~)
5

...... -Gaussian Noise Region

·

Crossing point

mm ·=700

20

15

:

10

~
·-

-5

0

100

200

300

J 400

1

500 m

600

700

800

900

1000

FIGURE 3.19. 'Ym of contaminated Blocks signal crossing upper bound of Invalidation region @ m* = 700 in decimal plot. Shaded area is Gaussian noise region with a 2 = 4.

a2

FIGURE 3.20. PNIST: 'Ym and upper bound for noise average in case of Blocks, m* = 1.

= 862,

39

FIGURE 3.21. PNIST: 'Ym and upper bound for noise average in case of Mishmash, m* = 8.

ANIST and provides acceptable results in comparison with other shrinkage meth' : :. i il'"' :::i·,
'I·.
!!~I
I

ods.

r ~: .

I

~

3.4

Concluding Remarks

As mentioned before, ANIST uses a function of square added sorted coefficients and its noise statistics to invalidate signal out of noise region. As a result of using accumulation in noise estimation, Central Limit Theorem (CLT) allows us to implement ANIST for additive noises other than Gaussian. Our simulations also demonstrate this important property. Figure 3.22 shows zero mean uniform noise with a 2 = 4. Figure 3.23 shows '1/J":n for this noise with

M = 10000 generations. It is evident that due to CLT, distribution of
approximated by Gaussian distribution.

\II~

can be

In another example with additive Laplacian noise, we compared the resulting MSE of BayesShrink and MNDL with that of ANIST for a Blocks signal con40

50,----------.----------,----------.----------.---------~--------~

FIGURE 3.22. Uniform noise with cr 2 = 4.

- Histogram of '1':@ m· 1000 Nonnal distribution

1000

500

0.02

0.04

0.06

0.08 0.1 @m· 100

0.12

0.14

0.16

0.18

2

2.5

@m-1000

Nonnal Probability Plot

Nonnal Probability Plot

0.75 0.75 0.50 0.25

1 a:
0.02 0.04 0.06 0.08 0.1 @m·100 0.12 0.14 0.16

~

0.50

8 :881

8:8f

8 :6R

0.25 0.10

0.05

FIGURE 3.23. 'l/J"/n histogram and normplot for cr 2 that sample data falls exactly on the straight line.

= 4 at

m

= 100 and

m

= 1000.

Note

taminated with Laplacian noise with variance of a 2 threshold results in M SE

= 4. The ANIST proposed

= 0.27 while the MSE is 0.65 in BAYES and 0.39 in
41

MNDL. This shows the method's robustness towards various noise behavior. Table 3.5 shows MSE results for Blocks signal contaminated by uniform noise.

I Uniform
a a2 a2
2

= = =

I MNDL 1.19 1 2.89 4 3.90 9

I BAYESI 2.17 3.87 3.98

VISU I ANIST 0.26 3.21 5.12 0.89 0.97 5.98

I PNIST 0.48 1.29 1.72

I

TABLE 3.5. MSE by Method for Blocks signal contaminated by uniform noise.

While this property does not hold true for PNIST method, PNIST provides better results in most cases when the additive noise is Gaussian, and the rate of convergence of noise distributions to Gaussian is faster in PNIST compared to ANIST. This can be seen graphically by comparing Figure 3.14 to Figure 3.3 where
~~

performs better in normal plots compared to 1/J":n at m

= 10 and m = 100.

It

can also be seen in Table 3.4 that Mean Square Error(MSE) results of PNIST outperform those of ANIST in most cases, when additive noise follows Gaussian distribution.

42

Chapter 4 Additive Noise in Denoising
The main goal of denoising is to recover the original noiseless data from the observed data. Many algorithms have been proposed for this purpose, and all of them require the value of noise variance. Thus, estimation of the noise variance is crucial to the denoising methods. Here, we propose a new noise variance estimation algorithm based on properties of the observed data. The literature on noise variance estimation is limited, and the existing algorithms use many assumptions on the data which makes their implementation limited to particular applications. For example, one of the first methods proposed to estimate variance of noise in speech processing assumes the uncontaminated signal to be Auto Regressive (AR) [33]. The method uses AR parameters to estimate noise variance. The drawback here is that not all the signals can be considered as AR, and thus the method is limited only to special applications. In another speech enhancement method, the white noise variance is estimated from preceding silent portions of speech [25]. However, this method has two major drawbacks. First, detection of silent portions is very difficult in low SNR situations. Second, the estimated variance can be used in other segments only if the noise is of stationary nature. Another method that can be applied to both lD and 2D data is based on estimation of the scatter of normally distributed data with high level of outliers [26]. The method is based on the shortest half sample method. In this method, the length of the shortest half sample is used as an estimation of standard deviation of the uncontaminated data. One disadvantage of this method is that the method

43

is applicable only to data with the majority of the data points having no signal present. Another widely used method suggests estimating the variance using the median of absolute value of normalized, fine-scale wavelet coefficients [9, 8, 3]. Maximum Likelihood (ML) estimators provide another class of methods [23]. In other variance estimation methods, a primary threshold or filter is first chosen to pick the finescale wavelet coefficients and suppress the effect of unknown noiseless data [35, 32]. The noise variance is then estimated using the remaining coefficients. The sensitivity of these approaches to the choice of the first threshold or filter, which estimates the noise variance, is not known. These methods are successful only if enough of the coefficients can be assumed to be entirely noise dependent.

4.1

Noise Estimation in MNDL Approach

Minimum Noiseless Description Length (MNDL) proposes another variance estimation method that estimates the noise variance during the denoising process [2]. In MNDL, the noiseless data "Description Length" (DL) is a function of noise variance. To estimate noise variance, MNDL finds the optimum subspace as a function of the noise variances for a considered range of variances. For any noise variance, the bounds on the reconstruction error provide bounds on the description length. To estimate the noise variance, MND L suggests choosing the noise variance that minimizes the noiseless data DL. From equation (2.34) we know that bounds for reconstruction error provide bounds for data description length:
(4.1)

where the DL is defined in equation (2.32). Thus we have: (4.2)

44

and finally noise variance is estimated as the variance which minimizes MNDL in the optimum subspace:

a~

= arg m~n M N D L(yN; a~)
Uw

(4.3)

However, it can be shown that in many cases, equation (4.3) fails to provide a global minimum. In most situations, it leads to a graph of several local minimums that do not provide us with the necessary information to estimate the variance. For example, Blocks signal contaminated with Gaussian noise of a 2

= 2 was tested

on MNDL noise estimation. As Figure 4.1 shows, minimum description length happens at a 2

= 5.5, thus; the estimate of MNDL is 5.5. However, the true noise

is much smaller, and the description length graph fails to give a global minimum.

500

II
450

I

-

400

·~ ·~
I

II

I
-

350 f-

·~

300 f-

·~

·~
II

f-

200

150

-

100

50 f-

0

0

3 Test Sigma

FIGURE 4.1. Data description length versus 8- in MNDL method when the true variance is a 2 = 2.

45

4.2

Proposed Noise Variance Estimation Method

To estimate the variance of noise, we propose a new approach that is based on the fact that autocovariance of a zero mean Gaussian noise vector is zero at any point other than the origin, whereas the autocovariance of the noiseless signal is impulse at the origin and non-zero at other points. This property is one of the main differences between the noiseless signal and noise part that has been considered in all denoising approaches. Thus, we expect the autocovariance of noise vector to have a spike at the origin and to be negligible at other points. Autocovariance of a zero mean Wide Sense Stationary (W.S.S) random variable

X is defined as

c(m,) = E(X(n + m)X(n))

(4.4)

This autocovariance can be estimated by a sample data of the random variable, x with length of N, as follows:

C(m) =
{

_1_

N-m L...m-0

~N_=-Iml-1 X(n + m)X(n)

m~O

(4.5)

C*(-m)

m<O

Figure 4.2 shows the autocovariance of a Gaussian noise vector. As signal is added to the noise, we expect the random behavior of the autocovariance graph to change and the area under its curve to increase. Autocovariance of a noisy Blocks signal can be seen in Figure 4.3. Let us demonstrate the new approach with an example. Consider the case that the true variance is a 2

= 9. We want to estimate the variance using the mentioned

autocovariance property. To do this, we start by selecting a range of values for the variance, hoping the true variance falls within this range. In this case we select our range to be a test

= [1 : 0.5 : 16], which means that starting from 1 we increase our

test variance by a step of 0.5 to 16.
46

1200rr----r----,r---.------.---,---..------r------r---.-------rl

1000

800

400

FIGURE 4.2. Autocovariance of noise.

FIGURE 4.3. Autocovariance of noisy signal.

For each test variance, a proper threshold is calculated using the PNIST or ANIST (even MNDL) approaches, and the threshold is applied to acquire an estimate of the noiseless data. This estimate can be used to find an estimate of the noise vector:
Wutest

[n] = y[n] -

Yutest

[n]

(4.6)

47

5000...------.----,.------.-----,-, 4000 3000

500

1000

500

1000

FIGURE 4.4. Autocovariance graph for different test variances, from left to right: (a) a 2 = 1 (b) a 2 = 4 (c) a 2 = 9 (d) a 2 = 16. True noise variance is a 2 = 9.

Where y is the vector of observed data and f) is an estimate of noiseless data. The estimate of the autocovariance of this noise is

C

( )= CTt e st m

N -m wn=O

1

""N -lml-1
-m)

(
WeTt es t

n+m

)
WeTt es t

( )

n

(4.7)
m<O

{

c ;test (

Autocovariance of the estimated noise for some of the test variances is shown in Figure 4.4. The area under the autocovariance curve can be estimated using the following relation for different values of O"test
A CTt e st
2
:

=

1
00

2 CCTte dt st

(4.8)

-oo

In discrete cases such as our example, l2 -norm of the estimated noise vector can be used as a measure of the area under the curve:
N

ACTtest

2

,_ """ ,_
k=1

L......t CCTtest [k]

2

(4.9)

As test variance increases, we observe an increase in the area under the curve of autocovariance of the
weTt e st,

which is due to the presence of noiseless signal

48

coefficients. To find the best estimate of the noise vector, we need to eliminate the signal coefficients as much as possible. Therefore, the proper estimate of the noise vector, and the proper estimate of the noise variance happens where the area under the curve of the autocovariance of the noise estimate sees a jump in value due to presence of signal coefficients. This sudden increase in the area under the curve of the autocovariance can be detected with an experimental sensitivity condition H[ ] _ A(atest + 0.5) atest A( a test ) (4.10)

For example in our observations we noticed that the jumping ratio of 10 would provide a reasonable estimate of the noise variance and variance is estimated as:

aw = arg min{ atest!
Utest

H[atest] ~ 10}

(4.11)

Figure 4.5 shows a variance estimation scenario with Mishmash signal, where the true noise is a 2

= 4, and the method of denoising is PNIST. The estimated variance

Figure 4.6 shows an estimation case where a Blocks signal was contaminated by Gaussian noise with a variance of a 2

= 8, and the denoising method is PNIST. = 7, and

It can be seen that the l2 -norm value leaves zero axis at around a 2

the estimate in this case is a 2

= 7.5. Table 4.1 shows the result of our variance

estimation method in PNIST for a Blocks signal compared to MNDL variance estimation for different noise powers.

4.3
I

Noiseless Data with Gaussian Properties

In image denoising applications, usually a threshold is found for each subband assuming a Generalized Gaussian Distribution (GGD), and in general, a large class of noiseless signals show Gaussian behavior [3]. One example of this class

49

PROPERTY OF RYE RSO NUNIVERSITY UBIWIY

0~--------~----------~--------~~--------~---------L--------~ 0 3
Test Variance

(]'2

FIGURE 4.5. Area under autocovariance of discarded coefficients for true variance of = 4.

7000 .. .... 6000

0

0

0

z

0

§ sooo
4000 3000 2000 1000
0~----~------~------._--'-~-------L-------L~

I ~

0

2

4

6
Test Variance

8

10

12

FIGURE 4.6. Area under autocovariance of discarded coefficients for true variance of a 2 = 8.

50

I True Variance I Proposed I MND L
a'2 a2 a2 a2 a2 a2

=1 =3 =4 =5 =6 =9

2.5 2 3.5 7.5 9 11

5.5 8 12 10 13.5 14

TABLE 4.1. Estimated variance by the proposed method.

is the Mishmash signal, which is zero mean and has a large number of non-zero coefficients that follow the behavior of the Gaussian distribution as shown in Figure 4. 7. The figure shows the histogram of Mishmash signal coefficients. As the figure depicts, the distribution of the coefficients is very similar of that of a Gassuian signal of the same length and same power(variance). Figures 4.8 and 4.9 show sorted coefficients of a noisy Mishmash signal and sorted coefficients of a signal with Gaussian distribution with the same energy. As can be seen, the two sorted signals nearly overlap. The overlap becomes more evident when more data are used as sample data length is increased from 210 in Figure 4.8 to 220 in Figure 4.9. This property inspires us to study in depth the behavior of a random Gaussian signal contaminated by a Gaussian noise; this can be a general problem of any signal of Gaussian class contaminated by noise. Note that the analysis is point wise as we know that the signal of image correlation makes it a well colored signal compared to the white additive noise.

4.3.1

Conditional Distributions and Data Denoising

Consider the following observed noisy signal
y=x+n

(4.12)

Where both the noiseless signal x and the noisy part n are zero mean Gaussian signals with variances

a; and a; respectively. Therefore, the noisy signal y itself is
51

FIGURE 4.7. Histogram of Mishmash coefficients.

10

1

FIGURE 4.8. Noisy Gaussian signal, noisy Mishmash signal and Gaussian noise compared with 2 10 samples.

a Gaussian signal with variance

a;.
(4.13)

52

FIGURE 4.9. Noisy Gaussian signal, noisy Mishmash signal and Gaussian noise compared with 220 samples.

In this case, given the observed y =a, we calculate the conditional density of the noiseless data, f(xly =a)

f(xly

=a) =

f(x,y =a) f(x)f(n =a-x) f(y =a) = f(y =a)

(4.14)

(4.15)

The conditional distribution is

(4.16)
2

which is a Gaussian distribution with the mean of J.lxJy deviation of O"xJy =
u;~x.

=

a~
Uy

and the standard

A Complete proof may be found at Appendix B. Similarly

53

a 2

Jlxjy

FIGURE 4.10. Distribution of f(nly =a) & f(xly =a).

the conditional distribution of the noise part is:

(4.17)

which is a Gaussian distribution with mean of f.-Lnly

= a~ and std
ay

2

a-nly
~

=

~. ay

Therefore, we can check for which cases the probability that x observed data y =a is larger than 0.5.

n given an

P(x > njy =a) 2: 50% P(x > n) =p(x >a-x) =p(2x >a) =p(x > 2) 2:2
and we have
a

(4.18)

1

(4.19)

p(x >

2) = p(x- 2 > 0) 2: 2 => 2 -

a

a

1

a

1-lxiy ~ 0

(4.20)

and the criterion is found as:
(4.21)

Note that the condition is independent of the observation value, and it means that if we want to be at least 50% sure that the original signal coefficient is larger
54

than the noise coefficient, we should check the condition proposed in equation (4.21). Therefore, this condition can be used to reject the data that is too noisy to be denoised.

4.4

Concluding Remarks

In this chapter a new variance estimation method was proposed. The method is applicable for the proposed NIST methods as well as for MNDL denoising. Also, the Gaussian noiseless signals were studied and based on an analytical approach, an important data rejection condition in data denoising was provided. The method can provide not only the condition to detect whether the data can be denoised but also provides the condition of subband rejection in wavelet denoising. This condition provides a more conservative criterion for subband rejection of noisy data than that of BayesShrink.

55

Chapter 5 Image Denoising
In this chapter we compare the performance of the proposed algorithms, PNIST and ANIST, with the existing algorithms for an example of two dimensional data in image denoising. Soft thresholding was used for all algorithms as soft thresholding has better results visually, and it is proved that soft thresholding achieves nearly Minimax error rate in most cases [9]. Similar to the simulation in previous chapters,

Haar wavelet with five level of decomposition is used.
Some methods propose a different threshold for every sub band. This is called subband dependent thresholding and is commonly used in BayesShrink and SureShrink. However, as we are dealing with noise invalidation in PNIST and ANIST and the properties of noise do not vary from one subband to another, these approaches provide a global threshold for the image and pre-process every subband of the denoised image using equation (4.21). The pre-processing rejects those subbands which are too noisy and contain little information about original data. Figure 5.1 shows an image of Barbara, a noisy version of Barbara and Barbara denoised by ANIST and PNIST. Figure 5.2 shows the famous Cameraman image denoised by BayesShrink, SureShrink, PNIST and ANIST. The visual quality of the denoised images by PNIST and ANIST clearly outperform the SureShrink image and are comparable with those of BayesShrink and MNDL. MSE comparison of the proposed methods for Cameraman and Barbara can be found in Table 5.1. As the table shows both PNIST and ANIST provide acceptable results compared to other shrinkage methods.

56

Original

PNIST

ANIST

FIGURE 5.1. Barbara denoised by ANIST and PNIST, a~= 225.

I Cameraman a'2 n = 25 2 a n = 100 2 an = 225
Barbara 2 an = 25 2 a n = 100 2 an = 225

I SUREI MNDL 15.9 18.3 52.9 49.2 93.8 95.1 19.1 59.9 118.1 18.9 51.6 89.2

I BAYES I PNIST I ANIST I 17.1 16.2 17.9 48.2 46.9 47.8 90.2 107.3 101.9
16.7 48.3 86.7

17.3 49.5 91.2

17.7 47.6 101.8

TABLE 5.1. MSE for different Methods compared.

Einstein and Lena denoised by common methods can be found in Figures 5.3 and 5.4.

57

Original

BayesShrink

MNDL

PNIST

ANIST

FIGURE 5.2. Cameraman denoised,

CT; =

100.

While PNIST outperforms ANIST in terms of MSE in most cases, it can be seen in Figures 5.1 and 5.3 that ANIST seems to have a more pleasing visual quality.

58

Original

BayesShrink

MNDL

PNIST

ANIST

FIGURE 5.3. Lena denoised, a~ = 400.

59

Original

Noisy, an =30

BayesShrink

MNDL

PNIST

ANIST

FIGURE 5.4. Einstein denoised,

a;_ =

900.

60

Chapter 6 Conclusion
Two new thresholding techniques based on noise invalidation were proposed. The new methods, ANIST and PNIST, are based on Gaussian invalidation of coefficients of the observed data out of the noise region, where the coefficients represent the transformation of the data with a set of orthonormal bases. It was shown that the behavior of sorted absolute coefficients in PNIST and sorted square added coefficients in ANIST can be approximated with Gaussian distribution. This property was used to select a threshold as the first coefficient that is out of the noise region with high certainty. We studied ANIST and PNIST in terms of their performance for two classes of signals: Blocks signal as the representative of signals with a high number of zero coefficients and Mishmash signal as the representative of signals with a high number of non-zero coefficients. Our study shows that ANIST and PNIST can perform acceptably regardless of the original signal type in comparison with other shrinkage methods. We proposed a new noise variance estimation technique and showed that the method provides acceptable results and outperforms the recently proposed MNDL noise variance estimation method. We extended the methods to two dimensional data, and the proposed image denoising methods were compared with other shrinkage methods. It was shown that ANIST and PNIST have comparable Mean Error Square (MSE) with BayesShrink and outperform other methods with a pleasing visual quality. Future research could focus on extending ANIST for Magneto Resonance Images (MRI). It is a very well documented fact that MR images are corrupted by Rician noise due to defects in imaging instruments [31, 16]. As was shown ANIST can

61

be a solution to non Gaussian noise problems; thus ANIST can be extended as a general solution for this type of problems as well as any other applications where Gaussian noise solutions can not be applied.

62

Appendix
A Distribution of Order Statistics of a Random Variable
, Xn

Let X 1 , X 2 , · · · X( 1), X( 2 ) , · · ·

be independent identically distributed random variables, and be the corresponding order statistics. Assuming f(x) is proba-

, X(n)

bility distribution function of random variable Xi and F(X) is the corresponding cumulative distribution function, then probability distribution function of
x(k)

is:

d d d fx(k) = -Fx(k) = -d P(X(k) ~ x) = -P(at least k of the n Xs are ~ x) dx x dx (A-1)
=

d~ P("?. k

success inn trials)

(A-2) (A-3)

=

d~ t ( ~) P(X ~ x)i(1- P(Xi ~ x))(n-i) j=k
1

J

=

d~ t (~ )F(x)i(1- F(x))n-j j=k
J
(

(A-4)

n = L(n

n. 1 ) F(x)i- 1 (1 - F(x))n-j
J- 1

j=k
- n

n j (

-1)
(

F(x)'(l- F(x))n-H)j(x)

(A-5)

n-1 = nf(x)( L j=k-1

n ~ 1 ) F(x)i(1- F(x))(n-l)-j
J

n-1 - L j=k

(

n ~ 1 ) F(x)i(1- F(x))(n-l)-i)
J

(A-6)

63

After simplifying the results, all terms cancel out each other except the first and the last, thus we have:

= nf(x)(

-

(n-1) (n-1)
k-1
n

F(x)k- 1 (1- F(x))(n-l)-(k-l)

F(x)n(l- F(x))(n-1)-n)

(A-7)

and the term over underbrace is zero thus:

fx(k)(.rr) = nf(.rr)

n-1) (
k-1

F(.rr)k- 1 (1- F(x))n-k

(A-8)

64

B

Gaussian Data with Additive Noise

f(xly =a)= J(x, y =a) = f(x)f(n =a-x) · J(y =a) J(y =a)

(B-1)

(B-2)

(B-3)
(B-4) We have:

a2 y

= a2 + a2 x n

(B-5)

thus:
(a2a2 - a2a2) X n X y (a2a2 n y

=

-a4
X

(B-6) (B-7)

+ a2a2) = y x

a4 y

and by replacing equation (B-6) and equation (B-7) in equation (B-4) we have:

(B-8)

=
and J(xly

1
27r~
Uy

e

(B-9)

= a) follows Gaussian distribution with

J-lxiy=a

=

a~ and y

2

axly=a

=

65

References
[1] S. Beheshti, N. Nikvand, X. N. Fernando, "Soft thresholding by noise invalidation", Proceedings of the 24th Biennial Symposium on Communications, pp. 235 - 238, June 2008. [2] S. Beheshti and M.A. Dahleh, "A new information theoretic approach to signal denoising and best basis selection", IEEE Transactions on Signal Processing, vol. 53, No. 10, pp. 3613 - 3624, Oct. 2005. ' [3] S. G. Chang, Bin Yu, M. Vetterli, "Adaptive Wavelet Thresholding for Image Denoising and Compression", IEEE Transactions on Image Processing. vol. 9, Issue 9, pp. 1532 - 1546, Sep. 2000. [4] S. G. Chang, B.Yu, and M. Vetterli, "Image denoising via lossy compression and wavelet thresholding", IEEE Transactions on Image Processing, vol. 9, pp. 1532 - 1546, Sep. 2000. [5] R. R. Coifman and D. L. Donoho, "Translation-invariant de-noising,". In Wavelets and Statistics, A. Antoniadis and G. Oppenheim, Eds. Berlin, Germany: Springer-Verlag, 1995. [6] I. Daubechies, "Ten Lectures On Wavelets", Vol. 61 of Proc. CBMS-NSF Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM, 1992. [7] J. Dedecker, P. Doukhan, G. Lang, R. Leon, J.R., S. Louhichi, C. Prieur, "Weak Dependence: With Examples and Applications", Lecture Notes in Statistics, Springer, vol. 190, 2007. [8] D. L. Donoho and I. M. Johnstone, "Adapting to unknown smoothness via wavelet shrinkage", Journal of the American Statistical Association vol. 90, pp.1200 - 1224, 1995. [9] D. L. Donoho, "De-noising by soft-thresholding", IEEE Transactions on Information Theory, vol. 41, Issue 3, pp. 613- 627, May 1995. [10] D. L. Donoho and I. M. Johnstone, "Ideal spatial adaptation by wavelet shrinkage", Biometrika, vol. 81, pp. 425 - 455, 1994. [11] D. L. Donoho and I. M. Johnstone, "Ideal denoising in an orthonormal basis chosen from a library of bases", Compt. Rend. Acad. Sci. Paris Ser. A, 319, pp. 13171322, 1994. [12] M. Elad and M. Aharon, "Image denoising via sparse and redundant representation over learned dictionaries", IEEE Transactions on Image Processing, vol. 15, No. 12, pp. 3736- 3745, Dec. 2006. 66

[13] A. Fakhrzadeh and S.Beheshti, "Minimum noiseless description length (MNDL) thresholding", Proceedings of the 2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing, 2007. [14] W. Feller, "The fundamental limit theorems in probability", Bulletin of the American Mathematical Society, vol. 51, pp. 800 - 832, 1945. [15] D. Gabor, "Theory of Communication", In Journal of lEE, vol. 93, pp. 429441, 1946. [16] R. L. Gregg, R.D. Nowak, "Noise removal methods for high resolution MRI", Proceedings of the 1997 IEEE Medical Imaging Conference, vol. 2, pp. 11171121, Nov. 1997. [17] Hubbard, B. Burke, "The world according to wavelets", Wellesey, MA (USA), 1996. [18] I. Hussain, Hujun Yin, "A novel wavelet thresholding method for adaptive image denoising", Proceedings of the 3rd International Symposium on Communications, Control and Signal Processing, ISCCSP 2008, pp. 1252 - 1256, Mar. 2008. [19] M. Jansen and A. Bultheel, "Empirical hayes approach to improve wavelet thresholding for image noise reduction", Journal of American Staistical Association vol. 96, Issue 454, pp. 629639, Jun. 2001. [20] M. Jansen, M. Malfait, and A. Bultheel,"Generalized Cross Validation for Wavelet Thresholding", In Signal Processing, vol. 56, pp. 33- 44, Jan. 1997. [21] Jingdong Chen, J. Benesty, Yiteng Huang, S. Doclo, "New insights into the noise reduction Wiener filter", IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, Issue 4, pp. 1218 - 1234, Jul. 2006. [22] I. M. Johnstone and B. W. Silverman, "Wavelet threshold estimators for data with correlated noise", Journal of the Royal Statistical Society. Series B {Methodological) 59, pp. 319351, 1997. [23] A. Kosir, A. Mujcic, N. Suljanovic, J.F. Tasic, "Noise variance estimation based on measured maximums of sampled subsets" Mathematics and Computers in Simulation, vol. 65, issue 6, pp. 629-639, May 2004. [24] J. S. Lee, "Digital image enhancement and noise filtering by use of local statistics", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-2, pp. 165- 168, Mar. 1980. [25] J. S. Lim, A.V. Oppenheim, "Enhancement and bandwidth compression of noisy speech", Proceedings of the IEEE vol. 67, Issue 12, pp. 1586- 1604, Dec. 1979. 67

[26] D. Makovoz, "Noise Variance Estimation In Signal Processing", Proceedings of the 2006 International Symposium on Signal Processing and Information Technology, pp. 364 - 369, Aug. 2006. [27] S. G. Mallat, "A Wavelet Tour Of Signal Processing", Elsevier Science & Technology Books, Sep. 1999. [28] S. Mallat, "A theory for multiresolution signal decomposition: The wavelet representation", IEEE Transactions on Pattern Analysis. Machine Intelligence, vol. 11, pp. 674- 693, Jul. 1989. [29] M. K. Mihcak, I. Kozintsev, K. Ramchandran, P. Moulin, "Low-Complexity Image Denoising Based on Statistical Modeling of Wavelet Coeficients", IEEE Signal Processing Letters, vol. 6, no. 12, Dec. 1999. [30] P. Moulin, J. Liu, "Analysis of multiresolution image denoising schemes using a generalized Gaussian and complexity priors", IEEE Transactions on Information Theory, vol. 45, pp. 909 - 919, 1999. [31] R. D. Nowak, "Wavelet-based Rician noise removal for magnetic resonance imaging", IEEE Transactions on Image Processing, vol. 8, Issue 10, pp. 14081419, Oct. 1999. [32] S.I. Olsen, "Estimation of Noise in Images: An Evaluation", CVGIP: Graphical models and Image Processing, vol. 55, No. 4, pp. 391-323, Jul. 1993. [33] K. K. Paliwal, "Estimation of noise variance from the noisy AR signal and its application in speech enhancement", IEEE Transactions on Acoustics, Speech, and Signal Processing vol. 36, Issue 2, pp. 292 - 294, Feb. 1988. [34] J. Portilla, Vasily Strela, Martin J. Wainwright, Eero P. Simoncelli, "Image Denoising using scale Mixtures of Gaussians in the Wavelet Domain", IEEE Transactions on Image Processing, vol. 12, No. 11, pp. 1338-1351, Nov. 2003. [35] K. Rank, M. Lendl, and R. Unbehauen, "Estimation of image noise variance", Proceedings of the lEE Vision, Image, and Signal Processing, vol. 146, Issue 2, pp. 80 - 84, Aug. 1999. [36] J. Rissanen, "Minimum description length denoising", IEEE Transactions on Information Theory, vol. 46, pp. 2537- 2543, 2000. [37] F. Ruggeri and B. Vidakovic, "A Bayesian Decision Theoretic Approach to Wavelet Thresholding". Journal of American Statistical Association, vol. 93, pp. 183 - 197, 1998. [38] Y. Sheng, "Wavelet Transform", In: The transforms and applications handbook, Ed. by A. D. Poularikas, CRC Press, The Electrical Engineering Handbook Series, 1996, pp. 747- 827.

68

(39] E. P. Simoncelli, "Bayesian Denoising of Visual Images in the Wavelet Domain", In: Bayesian Inference in Wavelet Based Models, P. Muller and B. Vidakovic, Chapter 18, Lecture Notes in Statistics vol. 141, Sep. 1999, pp. 291-308. (40] E. P. Simoncelli, "Statistical Models for Images: Compression, Restoration and Synthesis", Proceedings of the 31st Asilomar Conference on Signals, Systems and Computers, Nov. 1997, pp. 673- 678. (41] H. F. Trotter, "An Elementary Proof of the Central Limit Theorem", Archiv der Mathematik, vol. 10, pp. 226- 334, 1959. (42] M. Vetterli and J. Kovacevic, "Wavelets and Subband Coding", in Englewood Cliffs, NJ Prentice-Hall, 1995. (43] B. Vidakovic, "Nonlinear Wavelet Shrinkage with Bayes Rules and Bayes Factors", Journal of American Statistical Association, vol. 93, no. 441, pp. 173- 179, 1998. [44] Y. Washizawa, Y. Yamashita, "Non-linear Wiener filter in reproducing kernel Hilbert space", Proceedings of the 18th International Conference on Pattern Recognition, ICPR, vol. 1, pp. 967- 970, 2006.

69

