IMPLEMENTATION OF PREISACH-KRASNOSELSKn HYSTERESIS MODEL WITH THE USE OF ARTIFICIAL NEURAL NETWORKS

By Zhao Wang

A project Presented to Ryerson University In partial fulfillment of the Requirements for the degree of

Master of Engineering In the program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2004

Â© Zhao Wang 2004

p r o p e r t y OF RYER80W WWVmsiTY LfBRARV

U M I Num ber: E C 5 2 9 8 8

All rights reserved INFO RM ATIO N TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

UMI*
UMI Microform EC52988 Copyright 2008 by ProQuest LLC All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code.

ProQuest LLC 789 East Eisenhower Parkway P.O. Box 1346 Ann Arbor, Ml 48106-1346

AUTHOR'S DECLARATION

I hereby declare that I am the sole author of this project.

I authorize Ryerson University to lend this project to other institutions or individuals for the purpose of scholarly research.

Signature

I further authorize Ryerson University to reproduce this project by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Signature

BORROWER'S PAGE

Ryerson University requires the signatures of all persons using or photocopying this project. Please sign below, and give address and date.

Ill

IMPLEMENTATION OF PREISACH-KRASNOSELSKII HYSTERESIS MODEL WITH THE USE OF ARTIFICIAL NEURAL NETWORKS

ABSTRACT

Accurate modeling of hysteresis is essential for both the design and performance evaluation of electromagnetic devices. This project proposes the use of feedforward neural networks to implement an accurate magnetic hysteresis model based on the mathematical definition provided by the Preisach-Krasnoselskii (P-K) model. Feedforward neural networks are a linear association networks that relate the output patterns to input patterns. By introducing the multi-layer feedforward neural networks to Hysteresis modeling, the proposed multi-layer perceptron neural network and radial basis function neural network both are capable of modeling hysteresis accurately. The feedforward neural networks make the hysteresis modeling accurate without estimation of double integrals. Simulation results provide the detailed illustrations. The comparisons with the experiments show that the proposed approach is able to satisfactorily reproduce many features of observed hysteresis phenomena and in turn can be used for many applications of interest.

ACKNOWLEDGMENTS

The author would like to give my academic supervisor Dr. Alireza Sadeghian sincere gratitude and appreciation for his guidance towards my academic success. The project would also not be successful without other members of committee. Specially thanks to my husband and daughter for their full support.

Table of Contents

1. Introduction.................................................................................................... 1 1.1. Motivation.............................................................................................1 1.2. Feasibility............................................................................................ 2 1.3. Summary...............................................................................................3 2. Hysteresis & Preisach-Krasnoselskii Model...............................................4 2.1. Hysteresis............................................................................................. 4 2.2. The Preisach Approach........................................................................ 5 2.2.1. Preisach Units.............................................................................5 2.3. The Preisach-Krasnoselskii Model....................................................11 2.4. Summary............................................................................................. 19 3. Neural Networks...........................................................................................21 3.1. Neural Networks Structure................................................................21 3.2. Error-correction Learning Law..........................................................23 3.3. Error Back-propagation Training......................................................29 3.4. Multi-Layer Perceptron Neural Networks........................................ 33 3.5. Radial Basis Function Neural networks........................................... 34 3.6. Summary.............................................................................................36 4. Reconstruction of P-K Hysteresis Model..................................................38 4.1 Simulation and Experimental Results of MLP Neural Networks 38 4.1.1. Simulation set u p .....................................................................38 4.1.2. Simulation Experiments of MLPnetworks................... 40 4.2. Simulation and Experimental Results of RBF Neural Networks 47 4.2.1. Simulation Set u p ....................................................................47 4.2.2. Simulation Experiments of RBFnetworks.................... 48 4.3. Summary............................................................................................. 54 5. Conclusion........................................................................................................56 REFERENCES................................................................................................... 58

V I

List of Tables

4.1 Configuration Parameters for the MLP Experiments................................. 40

4.2 MLP Network Results Using Square Permalloy 80 at Frequency of 3 kHz........................................................................................................... 41

4.3 MLP Network Results Using Square Permalloy 80 at Different Frequency (DC, 1 kHz, 3 kHz, 6 kHz).....................................................45

4.4 Configuration Parameters for the RBF Experiments.................................. 49

4.5 RBF Network Results Using Square Permalloy 80 at Frequency of 3 kHz.......................................................................................................... 49

4.6 RBF Network Results Using Square Permalloy 80 at Different Frequency (DC, 1 kHz, 3 kHz, 6 kHz)....................................................52

vu

List of Figures

2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 2.10 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 4.1 4.2(a) 4.2(b) 4.3(a) 4.3(b) 4.4(a) 4.4(b)

Representation of A Baukhausen Instability........................................ 6 Preisach Plane....................................................................................g The Square Hysteresis Loop............................................................ 10 A Hysteresis Multi-branch Non- linearity......................................12 An Elementary Hysteresis Operator ........................................ 13

A geometric Interpretation of P-K model..................................... 15 First Order Reversal Curves............................................................ 15 Graphical Representation of expression (2.8)............................... 17 Block Representation of the Classical Preisach- Krasnoselskii Model....................................................... 18 Operator ANN Realization of the P- K Model..................................19 Neuron Model............................................................................ 22 Multi-Layer Perceptron With One Hidden Layer........................... 23 Three-Layer Back-propagation Neural Networks....................... 24 Weight Adjustments Using Steepest Descent Method..................26 A Sample Neuron Relation Between Hidden Layer and Output Layer............................................................................................ 27 Basic Flowchart Presentation of Back-propagation Training Algorithm......................................................................................32 Block Representation of the MLP Network.................................34 Block Representation of the RBF network...................................36 Hysteresis Loop Sample for simulation Experiments.....................39 Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at frequency of 3 KHz.................................. 43 Results from MLP neural network-based model built based on P-K Model........................................................................................... 44 Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at different frequency.................................. 46 Results from MLP neural network-based model built based on P-K Model......................................................................................... 47 Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at frequency of 3 KHz................................50 Results from RBF neural network-based model built based on P-K Model......................................................................................... 51

4.5(a) 4.5(b)

Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at different frequency..................................... 53 Results from RBF neural network-based model built based on P-K Model............................................................................................. 54

Chapter 1

INTRODUCTION
1. Introduction

1.1. M otivation

The accurate and effective modeling o f hysteresis is crucial to the prediction o f the perform ance o f electromagnetic devices. Ferro-magnets, such as iron, nickel, and cobalt etc, will tend to stay magnetized to some extent after being subjected to an external magnetic field; this tendency to "rem em ber their m agnetic history" is called hysteresis.

M agnetic materials are usually described in terms o f a single valued B-H curve (magnetic flux density - magnetic field) in finite element analysis.

The industrial applications range from modeling o f recording heads, to m odeling the behavior o f magnetic cores, a single-valued B-H curve m ay adequately characterize the magnetic material. In many instances, it is im portant to have a good model o f hysteresis in order for the core loss to be accurately predicted since hysteresis loss can be a significant component o f the total loss.

In m any electronic devices, such as motors and generators, the behavior o f the device is strongly influenced by the behavior o f the magnetic com ponents (e.g. inductors, transformers). Hysteresis then can have a powerful influence on both

devices perform ance and losses. The use o f phenomenological mathematical m odels of hysteresis is a well-established approach for describing

m agnetization phenomena. [1] A typical example o f such models is the classical Preisach model that is a scalar model o f magnetic hysteresis based on the physical mechanisms o f magnetization. [2] Attempts to separate the physical meaning o f magnetism from the Preisach model have resulted in the Preisach-Krasnoselskii model. [3] The P-K model is a mathematical tool that can be used for the mathematical description o f hysteresis o f any physical nature. This purely mathematical model takes a macroscopic point o f view to hysteresis phenomena and provides no insights into the physical causes o f hysteresis. Owing to this characteristic, the P-K model becomes a very pow erful and effective tool for designing devices with hysteresis features.

Recently, artificial neural networks have been widely used to model a wide range o f systems for which mathematical models either cannot be defined, or are ill defined. It has been successfully used in those applications involving pattern matching [4] and systems modeling [5]. This success was the main m otivation behind some attempts to utilize ANNs in the area o f magnetic hysteresis. Innovative techniques for the accurate modeling o f hysteresis phenom ena will have very important practical significance.

1.2. Feasibility

This project proposes the use o f artificial neural networks as functional approximation tools with local memory capabilities that will be suitable for the construction o f a P-K model. The exploration o f the working hypothesis in this po q ect is that both P-K model and the ANN model have local mem ory, and

both

exhibit

fault

tolerance

capabilities.

Furthermore,

they

both

are

m athem atical tools, which take a macroscopic approach to the problem.

Specifically, this paper considers the applications o f multi-layer perceptron (M LP) and radial basis function (RBF) networks that are already proven to be universal approximators, and will show that there is a natural correlation between; (i) the coefficients o f the P-K model and the connection weights in A N N m odel and, (ii) the Preisach operators and A N N 's activation functions. In this respect, this project treats the magnetic hysteresis system as a black box and the mathematical tools as means for the identification o f the unknown input/output relation governing the system.

1.3. Summary

In summary, P-K model research can bring up alternative mathematical tools to ensure the accurate and effective modeling of hysteresis.

In this research paper, a review o f P-K model and neural networks in Chapter 2 and Chapter 3 will uncover that multi-layer perceptron and radial basis function netw orks as universal approximators can be used to reconstruct the P-K model. N ext, m y new MLP and RBF neural network implementations using Square Perm alloy 80 at different frequencies including DC, 1 kHz, 3 kHz, and 6 kHz w ill be described in detail followed by modeling and analysis o f the im plem entations in Chapter 4. The simulation experiments are also provided for illustration purposes. Finally, the conclusion section will show the research contributions o f this work by presenting a new m ethodology for accurate and effective modeling o f hysteresis.

Chapter 2

HYSTERESIS & PREISACH-KRASNOSELSKII MODEL
2 Hysteresis & Preisach-Krasnoselskii Model

2.1. Hysteresis

The process o f magnetic hysteresis is basically a sequence o f Barkhausen jum p. The Barkhausen effect consists o f discontinuous changes in flux density during sm oothly changing magnetic field. They are known as Barkhausen jum ps and are caused by the sudden irreversible motion o f magnetic domain walls as they break away from pinning sites.

W hen we detect a jum p, the system is leaving a metastable state in favor o f some other state o f lower energy. Two energy terms are involved in the event, the free energy change AF from the initial state to the final state and the amount o f energy AE dissipated as heat during the jump. If we can keep track o f the energy changes while the jump sequence proceeds, we would have at hand quite a general description o f hysteresis. The main difficulty behind this picture is that we do not know the general principles controlling the statistics of Barkhausen jum p and those determining in what proportion the work performed by external sources is subdivided into stored and dissipated energy in the jum p sequence. This brought to us F. Preisach; the German physicist gave the idea o f describing a generic hysteresis system as the superposition o f m any bistable units and gave this idea an elegant and illuminating graphical representation.

In the 1970 s, the Russian mathematician Krasnoselskii came across Preisach*s m odel and understood that it contained a new general mathematical idea. As a result, a new mathematical model was revealed which can be used for the m athem atical description o f hysteresis o f any physical nature.

2.2.

The Preisach Approach

W e shall first discuss how the Preisach approach fits in with the general interpretation o f the magnetization process as a sequence o f Barkhausen instabilities, and on this basis we shall introduce the Preisach-Krasnoselskii M odel. 2.2.1. Preisach Units The simplest energy profile giving a clear representation o f a Barkhausen instability is shown in Fig 2.1. The term preisach unit is. refers to a system characterized by an energy profile of this kind. The state variable m represents some characteristic magnetic moment involved in the problem, to be identified case by case. The system is confined to the interval - Am < m < -I-Am (Am > 0 ). The free energy o f the unit, F^(m) is determined by its values at the points - Am,0,4-Am . It is convenient to express these values in the form -//oA^Am,//oA^Am,/^oA^Am, where and are appropriate fields. The three

points are connected by straight segments, which give the two characteristic energy gradients, A ^ --A ^ for m > 0, and A ^ -i- A ^ for m < 0 . At last, we assume that the unit is coupled linearly to the magnetic field H , in the sense that the stability o f the unit is controlled by the Gibbs energy Gp(m;H) = F' p (w)-/^oH^Â« (2-1)

Â·TT- > 'i ' - f -?-Â»,. Â· <T.I ,Â· , i i Â» '.: , Â·Â·>-Â»>.'r ^

^ *'kt
H= 0

fiohAm

Moh^Ã m maw* Â»

-jUaKÃ m

H>0 - /jA i A m

/U q H A th

tr fi'-'ti '?0i

Fig 2.1. Representation of A Barkhausen Instability Top: Energy Profile (m) Characterizing a Preisach Unit. W ;H ), as defined by Equation

Bottom: Corresponding Gibbs Energy
(2 .1)

The presence o f the field gives rise to a Gp profile o f the same form as the Fp one, w ith --H ) in the place o f (see Fig 2.1). Other more realistic energy

profiles m ight be considered, the distinctive features o f the description are provided as: i. The position, Â± Atn , o f the two energy minima is independent of the field H ; ii.The height o f the energy barriers separating the two minima depends linearly on H. Depend on the relative values of A,,,A^,and H, G p{m-,H) may exhibit one or two local minima. When H >h^ + , only one minima exists, at m = +Am . is a minima. In the

Analogously, when H < h ^ - h ^ , only m = - A m interm ediate case, - h ^ < H <h^ +

, two minima, one stable and one

m etastable, exist if A ^ > 0. We will not consider the case A ^ < 0 , because the potential then has one minimum under any arbitrary field, and the unit exhibits a reversible response to the field, which can be separately treated with no difficulty. The stability condition just listed can be expressed in graphical form w ith axes A ^ and A,,. A given Preisach unit is identified by the representative point o f coordinates (A^,AJ in this plane; Let us subdivide the half-plane A ^ > 0 into the three regions show in Fig 2.2. The boundary o f region II is the bifurcation set where Barkhausen jum ps may occur.

Â« w a sfsa f.^ !*

H
t , , / lr* i t, . f

^ t Â«**-

' ,Ã¿ "Ã¿ ' C ., , . y

Fig 2.2. Preisach Plane with indication o f regions where the energy Gp (m; H ) has one or two minima.

Region \\

> H + h^. I f the representative point lies in this region, the unit is

certainly in the state m = -A m , which we call the ( - ) state.

Region II: H - h ^ <h^ < H + h^. If the representative point lies in this region, both the states m = -Am and m = +Am are locally stable. W hich state the unit occupies will depend on the past history.

R egion III:

< H - h ^ . l f the representative point lies in this region, the unit is

certainly in the state m = +A m , which we call the (+) state.

The m etastability region, region II, has the form o f a core whose vertex is at the point = 0, = H . W hen the field changes in time, this m etastability cone

m oves up and down in the plane and a given representative point m ay pass from one region to another. The boundary of cone represents the set o f points w here Barkhausen jum ps may take place.

Let us consider a certain point o f the Preisach plane, o f coordinates The point w ill certainly belong to region I when H - o o . With increasing H,

the point will first enter region II and then it will pass from region II to region III at the mom ent w h e n // = . At this point, a jum p occurs and the unit

suddenly passes from m - - A m to m = +Am . A decreasing field will cause the same thing except that the jum p from m = +Am to m = - A m takes place aXH = 2.3. - h^. The unit describes the square hysteresis loop is shown in Fig

U nits associated with different points o f the Preisach Plane will describe loops differing in their width as well as in their shift along the H axis. Also, the loop height, A m ( h ^ , h j , may be, in principle, different from unit to unit.

R Y E R G O fiimÃ«vmmu d R A frr

PRQPERT'CF

i;; ' S '?#! o L +A /M m
C ' y S a j . r t " ''' - ' " : '' i Â» j * ' Â· ;% Â» :.' Â« > ? " Ã¿ i i

K
^

W w K

P
% - A /M
.

a
H

i

N .

Fig 2.3 the Square Hysteresis Loop

The energy representation given in Fig 2.1 shows that, when a Barkhausen ju m p occurs, the energy G^(m ;H) suddenly decreases by If^Jn^b^n . This is the amount o f energy dissipated as heat in the jump. At the same time the free energy F^{m) changes by the amount and at the jump. We see that

are appropriate coordinates to keep track o f dissipated and stored

energy. In subsequent sections, we sometimes use the square loop switching fields,

a =K+K P =K ~K

(2 .2 )

10

instead o f

and

, as alternative coordinates in the Preisach plane.

In the next section, the starting point o f the Preisach model is usually the idea that a system should be described by a collection o f elementary square loops o f the kind shown in Fig 2.3. This is mathematically appropriate because the

elem entary loops are actually a pictorial representation o f elementary hysteresis operators that can be superposed, to build up hysteresis nonlinearities w ith a m ore complex structure. The basic physical aspect o f the approach is to introduce the two variables, dissipated energy. and , describing stored energy versus

2.3.

The Preisach-Krasnoselskii Model

The phenom enon o f hysteresis is encountered in many areas o f science. But the interpretation o f hysteresis differs from one area to another and from paper to paper. This led us to rigorous mathematical definitions o f hysteresis in order to avoid confusion and ambiguity. The definition provided in [3] is o f particular interest. According to this definition, a function is called a static hysteresis function if its input-output relationship is a multi-branch non-linearity for w hich a branch-to-branch transition occurs after each input reaches a minimum or m axim um (Fig 2.4). The term "static" implies that branches o f hysteresis are determ ined by the past extremum values o f the inputs, while the speed o f input variation between extremum values have no influence on branching. This definition highlights two key attributes of the hysteresis phenomenon. One is that hysteresis behaves sequentially and can be specified by following a predeterm ined sequence o f states which the output is a function o f input and internal states. Another is that hysteresis is inevitably associated with memory.

II

 H

Fig 2.4. A hysteresis multi-branch non-linearity

The P-K model can be used as a mathematical tool to represent description o f m agnetic hysteresis [1]. This model can be represented by using a finite set o f hysteresis binary operators, f^p (with a and (3 corresponding to up and down sw itching values o f the inputs), serves as a local memory (Fig 2.5).

12

k '+1 p ' ^ r ik
--I

-

   'w  k^

a -

1

Fig 2.5. An elementary hysteresis operator y .

T he P-K model (2.3) expresses that given the output F{t ) and the input , then the output F(t) can be uniquely determined for all / < .

This captures the essence o f the hysteresis phenomenon that is the past history o f the input/output influences the present instantaneous output;

F ( / ) = \\n {a ,fi)ra p H {t)d a d p

(2.3)

W here

is a set o f an arbitrary weight function with respect to , and the hysteresis operator can only assume the values

hysteresis operator

13

+1 or -1. In (2.3), function

represents the only model unknown and has

to be determ ined from some experimental data. The geometric interpretation o f model (2.3) is based on the fact that there is one-to-one relationship between operator y^p and points (cr,/ 7) o f the h alf plane a > P . Using this fact, it can be concluded that at any time the triangle T is subdivided into two sets (Figure 2.6). w hich y^pH(t) = \ and S~{t) consists {t) consists o f points (a ,/7 )fo r of points {a,p) for which

y^pH(t) = - \ . It can be shown that the interface L{t) between S* {t) and S~{t) is a staircase line whose vertices have a and p coordinates coinciding w ith local m axim a and minima of input at a previous time. The final link o f L{t) is attached to the line a = P and moves when the input changes. W hen the inputs increase, this link is a horizontal one and moves up, and it is a vertical one, and moves from right to left when the input decreases.

U sing this interpretation, the model (2.3) can be presented as:

^ (0 =

P)H{t)dadp -

P)H {t)dadp,

(2.4)

From the expression (2.4), it follows that an instantaneous value o f output depends on the shapes o f the interface T (t), which is determined by the extreme values o f input at previous instants of time. As a result, the past extremum value o f inputs shape the interface Z,(t), and with which they leave their mark upon the future.

14

m m *

Fig 2.6 A geometric Interpretation o f P-K model

Â· Z ' / I . Â·

Â«

.

'4 ~ fX fw 'n T

Fi
,

'
J ÃÂ» Ã;` Â· ^  :\ .

Â«r,

S '

.tiyÃ®fl

Fig 2.7 First Order Reversal Curves

15

The unknowns o f this model may be systematically found using f-o reversal curves (Fig 2.7). This can only lead to good prediction results. This can be done by first bringing the input value to states such that the outputs o f all operators 7ap equal to -1. If we now gradually increase the input value, it will follow will be used for

along a lim iting ascending branch (Fig 2.7). The notation

the output value on this branch corresponding to the input H = a . The firstorder reversal curves are attached to the limiting ascending branch. Each o f these curves is formed when the above monotonie increase o f the input is follow ed by a subsequent monotonie decrease. The notation F^p will be used for the output value on the transition curve attached to the limiting ascending branch at the point F^ . This output value corresponds to the in p u t// = (3 . Now w e can define the functionF ( a ,/ Ã® ) :

F(a,P ) = (F ^-F ^,, )ll

(2.5)

U sing the geometric interpretation o f the model (2.3), it is easy to prove that:

F {a, P) =

H{x, y)dxdy =

(2.6)

w here T { a , p ) is the triangle formed by the intersection o f the lines X = P , y = a and y = x . (Fig 2.8)

16

Fig 2.8 Graphical Representation o f expression (2.8)

From (2.6), we find:

fu{a,/3) = -

d ^ F { a ,P )

dpda

(2 7)

To avoid numerical evaluation o f double integrals, It is shown in [4] that the determ ination o f the coefficients o f the hysteresis operator can be simplified by approxim ating ( 2 .3 ) using a finite superposition o f hysteresis operators

N

N

(2 .8)
(=1 j= \

(z -- 1)
a,- = A = Â« i

(2.9)

17

W here N

is the total number o f hysteresis operators involved while < x^

represents the input at which the positive saturation o f the actual magnetization curve is achieved.

The Preisach- Krasnoselskii model, as given by (2.8), can be illustrated as shown in Fig 2.9. According to this figure, the model identification can be sim plified to finding out the different branch of weights. It then becomes a typical ANN problem which is to find different weights using arbitrary input/output training data. This means if the P-K model can be explicitly realized b y an ANN, then the model unknowns may be determined using any arbitrary experimental data.

KcCi^Pj)

Fig 2.9 Block Representation o f the Classical Preisach-Krasnoselskii Model

18

It turns out that model (2.9) m aybe able to be realized by the A N N 's shown in Fig 2.10 if the hysteresis operators are represented by elementary rectangular loops.

Hysteresis Operator

Hysteresis

H {t)

O perator

F(t)

Ã Â« 1 .J

m m #

Hysteresis Operator

Fig 2.10 Operator ANN Realization of the P-K model

2.4.

Summary

The P-K model above is shown to satisfy the formal definition o f hysteresis provided. First, the P-K model uses the binary units to form the local memory capability. Secondly, it is shown, that this model stores information not in particular separate units, but some groups o f the units which together keep each piece o f information related to hysteresis. As a result, if some o f the units are destroyed, the stored information still might be preserved [1]. This, so called,

19

fault tolerance property is similar to that o f the memories in biological systems, and to the memory capability o f an ANN,

20

Chapter 3

NEURAL NETWORKS
3 Neural Networks

3.1.

Neural Networks Structure

Artificial Neural networks, commonly known as "neural networks", can be defined as a model o f reasoning inspired by biological neural interaction in the natural world. The human brain consists of a densely interconnected set o f nerve cells, neurons. Similarly, a neural network is a computer system made up o f a num ber o f very simple and highly interconnected processors, also called neurons, that process information in parallel by that dynamic state response to external inputs [5].

A neuron is the basic information-processing unit o f a neural network. A neuron k , from input to output can be described using the following two equations:

r

1= 1

and

yk = q>{v,)

(3-2)

21

where x^,X ,...,x^ axe the inputs; w^,,w^2 > --
2

are the synaptic weights o f Ã§j(*) is the activation

neuron k ; function and

is the neuron internal activity index; is the output o f the neuron k .

A neuron model can be illustrated as Fig 3.1. The threshold 9^ o f neuron k here is represented by a synaptic link with an input. This signifies the scenario where a neuron generates an output if its input is beyond a threshold.

*
X
2

w kO f `  'f

Â«
Fig 3.1 Neuron Model

22

This model is a simple, yet useful, approximation o f a biological neuron and can be used to develop different neural structures including feedforward networks. A typical network consists o f an input layer, one or more hidden layer and an output layer o f neurons. A multilayer perceptron with one hidden layer is shown in Fig 3.2.

Input Layer

Hidden Layer

Output Layer

Fig 3.2 Multi-Layer Perceptron with One Hidden Layer

The neurons o f one or more hidden layers o f MLPs are not part o f the input/output o f the networks. These hidden neurons provide the learning capability for the networks. It enables the network to learn complex tasks by extracting m ore meaningful features from the input vectors. The procedure o f this m ethod is referred as the error-correction learning law.

23

3.2.

Error-correction Learning Law

To derive the error-correction learning law, let us consider the three-layer netw ork shown in Fig 3.3. The indices i, j and k refer to neurons in the input, hidden and output layers, respectively. Input signals, are propagated through the network from the left to

right, and error signals, e ,,e 2 ,...,e,,from right to left. The symbol Wy denotes the weight for the connection between neuron i in the input layer and neuron j in the hidden layer, and the symbol W j^. the weight between neuron j in the hidden layer and neuron k in the output layer.

Input siginals

1

T2 T*
X;

T/ INPUT LAYER HIDDEN LAYER OUTPUT LAYER

Fig 3.3

T h re e -la y e r

back-propagation neural

n e tw o rk

24

Suppose the learning procedure o f the neural network selects (3.2.1) as the cost function to minimize.

2

,

( 3 . 2 . 1)

To propagate error signals, we start at the output layer and work backward to the hidden layer. The error signal at the output o f neuron k is defined as:

^ k = d k - y k

( 3 . 2 .2 )

where

denotes desired output for neuron k ,

denotes the actual output o f

neuron k when the input vector x is presented. The purpose o f error-correction learning is to minimize the cost function with respect to the synaptic weights W of the network so as to make the network closer to the desired output.

This can be achieved by employing the method o f gradient descent, also known as the steepest descent method. We can adjust the weight values as following :

= W Â° ``` + r / D

( 3 .2 . 3 )

where 77 is a positive constant known as rate o f learning, which determines the length o f step; D is a search guiding direction, determined by the gradient o f cost function ^ evaluated at the weight value WÂ° :

25

Â£> = -- dJV old

(3.2.4)

A ccording to the steepest descent method, the adjustment to the weight Aw should be along the negative gradient so we have:

Aw = -77 Ã I dw The relationship is shown in Fig 3.4.

(3.2.5)

Aw =

- 7]

W o

w(Â« + l)
'V, : Â»r)Â» ;-

Fig 3.4 W eight Adjustments Using Steepest Descent Method

26

To calculate ^

for (3.2.5), let us consider a sample neuron relation between

hidden layer and output layer as Fig 3.5.

X;

Fig 3.5 A Sample Neuron's Relation Between Hidden Layer And Output Layer B ased on (3.2.5), we set (3.2.6)

The chain rule can be used to calculate the partial derivative and the above partial derivative can be written in the equivalent form: ^ dvj^ ^ dyi, dv^

(3.2.7)

dw^j

dv^ dw^j

dv^ dy^ dw^j

27

According to (3.1), we have

aw,, A ccording to (3.2), we have

P . 2 .8 )

(P\^k) = ^

(3.2.9)

w here (p\v^ ) is the derivative o f the activation function ^ ( # ) .

To calculate -- ^ , two cases have to be considered.

1.

A : is an output neuron in the output layer o f the network:

- ^

= -(^k - yk)

( 3 . 2 . 10)

then (3.2.6) can be solved as: = -rj(p\v^){d^ - y k ) ^ j
2.

(3.2.II)

A : is not an output neuron in the output layer o f the network: in calculating

This is the case where the output errorcannot be used directly the error signal. The total output error is:

=

=

=^*(3,2.12)

dy^

I `

, dv, dy^

i dv, dy,^

,

i dv, k is not in.

w here 1 is the corresponding output layer that neuron then (3.2.6) will be

28

Aw ^j = -7 ](p '(y ,

( 3 .2 .1 3 )

and (3.2.7) can be expressed as:

A w,j =-710,X j

(3.2.14)

A ccording to (3.2.11), when the output layer is /th layer, we can calculate Ã, as:

=-<P'(y,)(d,-y,)

(3.2.15)

otherwise, calculating the (5^ recursively as (3.2.12). The back-propagation learning algorithm for error-correction learning

explained above is applicable to any network with a differentiable activation function <p(Â»). According to (3.2.12) and (3.2.15), we can back-propagate the errors layer by layer; according to (3.2.14), we can update the weight values.

N ow w e can derive the back-propagation training algorithm.

3.3.

Error Back-propagation Training

The best known supervised learning algorithm. The learning algorithm was first developed by Werbos [5] and latter rediscovered by Rumelhart et al. [ 8 ]. The learning is done on the basis o f direct comparison o f the output o f the network w ith know n correct answers. The back propagation algorithm is an efficient

29

m ethod o f computing the change in each connection weight in a multi-layer netw ork so as to reduce the error in the outputs. The training algorithm consists o f the following steps:

Step 1 : Initialization Set all the weights and thereshold levels o f the network to random numbers uniformly distributed inside a small range [5 ]: ^ 2.4 2.4^ Iy where is the total number o f inputs o f neuron i in the network. The

w eight initialization is done on a neuron-by-neuron basis.

Step 2: Activation Activate the back-propagation neural network by applying inputs Xj, X ,...,x,,and desired outputs
2

.

a)

Calculate the actual outputs o f the neurons in the hidden layer: y j = sigmoid /= ] where n is the number o f inputs o f neuron j in the hidden layer, and sigmoid is the sigmoid activation function.

b)

Calculate the actual outputs o f the neurons in the output layer: = sigmoid y=i -9,

30

where m is the number o f inputs o f neuron k in the output layer.

Step 3; W eight Training U pdate the weights in the back-propagation network propagating backward the errors associated with output neurons. a) Calculate the error gradient layer using (3.2.11) or (3.2.12): for the neurons in the output

Calculate the weight corrections

using (3.2.14):

Update the weights at the output neurons: ^kj = ^kj + ^^kj b) Calculate the error gradient for the neurons in the hidden layer: I Sj = y j x ( l - y ; ) x ^ < 9 * Xw,j Calculate the weight corrections:
A W ;j = -7 / X Xi X S j

Update the weights at the hidden neurons:
W y = W y + AW y

Step 4: Iteration

Increase the iteration by one, go back to Step 2 and repeat the process until the selected error criterion is satisfied (Fig 3.6)

31

2:
Define network structure Define com ecticn pattern Define activation functions Define performanoe Prepare training data R-epare validation data

Provide stimlus from training s d to t f ie network

feedforward flew of inforrration - generate output and performance rreasure

Provide stimlus from validation to ttie network

error t)ackptopagated tfiroÃ¼Ã¿i tfie network, changes proportional to tfie derivative of error wrt waicht tyo tDe made to synaptio

^

/

perfomiaiice measure satisfactory?

feedforward flew of information - generate output and performance rreasure

w e i^
performanoe measme

satisfectay?
I# !* #
e n d of training

Fig 3.6 Basic Flowchart Presentation of Back-propagation Training Algorithm

32

3.4. Multi-Layer Perceptron Neural Networks

A m ulti-layer perceptron network (ML?) represents a class o f feedforword neural networks that contain one input layer with one output layer, together representing the system inputs and outputs, respectively, and one or more hidden layers providing the learning capability for the network [7](Fig 3.7). The basic element o f a M L? network is an artificial neuron whose activation function, for the hidden layer, is a smooth, differentiable function (usually sigmoid). The neurons in the output layer have a linear activation function. The back-propagation algorithms based on error-correction learning rule can be considered as an extension o f the "mean least square" algorithms [10]. This learning algorithm works by continuously presenting batches o f input vectors and their associated desired outputs and then iteratively changing the values of the network weights and biases in the direction of the steepest decent with respect to the relative error. One way of determining when to stop is the average error over the entire training set reaches a defined error or any other convergence criterion. A MLP network function can be formulated as:

f n

^ (3.2.16)

f{Xi,...,x,,) = Y,cÃ¹--g J^yvyXj-Oi
(=1

\y=i

(3.2.17) l + e'

33

W here / ( Â· ) is the network output; g(Â») is the activation function; Wy and 0^ represent weights and biases in the hidden layer; m denotes the number o f hidden neurons; and n). represents the output layer connection weights which, in effect, serve as coefficients to the linear output function.

1 ,1

X.n

w. -ip>U T LAYER HIDDEN LAYER

CO

m OUTPUT LAYER

Fig 3.7 Block Representation of the Multi-Layer Perceptron Network

3.5. Radial Basis Function Neural networks

In general, Radial basis function (RBF) network exhibits similar properties as M LP networks, such as generalization ability, fault tolerance and robustness. It therefore can be treated as an alternative tool for learning in neural networks.

34

RBF networks also have a fast learning capability comparing with other feedforward neural networks [9], The architecture o f the RBF network is sim ilar to a MLP network and consists of a fully connected two-layer network; the hidden layer and the output layer. The hidden layer is a collection o f neurons with radial basis activation functions, typically one of "Thin Plate Spline", Gaussian, Multi-Quadratic or Inverse Multi-Quadratic functions [9]. There are two parameters, the center c and the width, r , work with RBF neurons. They are similar to the weights and biases o f the hidden layer in a M LP network. The output layer is just an "adder" that sums a weighted inputs using the output connection weight, w , that represent the strength o f these connections (Fig 3.8) A RBF network function can be formulated as:

J
1 = 1

(3.2.18)

{X -C Y

^ ( x --c) = e^

''

(3.2.19)

where ^(Â») is the activation function; w, represents the weights in the output layer.

35

 > (Â·)

INPUT LAYER

HIDDEN LAYER

OUTPUT LAYER

Fig 3.8 Block Representation of the Radial Basis Function Network

3.6.

Summary

A ccording to the review, a neural network is massively parallel & distributed processor that works in a fashion similar to the human brain. It resembles a biological brain in two respects; first, the knowledge is acquired through a learning process, & second, inter-neuron connection strengths known as w eights are used to store the knowledge. The learning process involves m odification o f the connection weights to obtain a desired objective. Two kinds

36

o f neural networks that we specifically mentioned are MLP and RBF networks. They are all feedforward neural networks. Feedforward networks can be considered as linear association networks that relate output patterns to input patterns. Among the advantage of feedforward neural networks, the following features are especially important to the proposed implementation of Hysteresis phenom enon: 1) function approximation (I/O mapping): nonlinear function to the desired degree 2) Learning and generalization: ability to learn I/O patterns, extract the hidden relationship among presented data, & provide acceptable response to new data that the network has not yet experienced. 3) Fault tolerance: due to their highly parallel/distributed structure, failure o f a num ber o f neurons to generate the correct response does not necessarily lead to failure o f the overall performance of the system. ability to approximate any

37

Chapter 4

RECONSTRUCTION OF P-K HYSTERESIS MODEL
4 Reconstruction o f P-K Hysteresis Model

In this paper, two major kinds o f neural network architectures, namely multiLayer Perceptron (MLP) and the Radial Basis Function (RBF) networks, are proposed to model the Electric Arc Furnace (EAF). The reconstruction o f the PK M odel is achieved using the normalized data representing the hysteresis loops for Square Permalloy 80 at different frequency including DC, 1 kHz, 3 kHz, and 6 kHz. Square Permalloy 80 is materials consists of 80/20 alloy o f nickel and iron that can be easily magnetized and demagnetized. The perform ance criteria by which neural network-based models can usually be m easured including the training time, number of epochs, size of the network and accuracy. The first three criteria measurements are straightforward except the accuracy. In order to investigate the accuracy of implemented neural networks, comparisons between experimental data and the output o f the netw orks are made. The error index used for the comparison is non-dimensional index error (NDIE) that is the mean square error (MSB) o f hysteresis modeling norm alized by the standard deviation of the experimental data.

4.1.

Simulation and Experimental Results of MLP Neural Networks

4.1.1. Simulation set up

38

The EAF sample used for simulation experiments is illustrated as Fig 4.1.

Inputs Outputs
"1

Fig 4.1 Hysteresis Loop Sample for Simulation Experiments

The dim ension o f the input vector and output vector is 1. The numbers of hidden layer neurons will vary. The corresponding simulation software program is coded in Matlab 6.5. The computer system used for the implementation is Pentium Â® 4 CPU 2.4 GHz, 512 MB of RAM.

The configuration o f the hysteresis MLP is controlled by a script file. The configuration script file controls the parameters such as the dimension o f the input and output vectors, the dimension of the hidden layer, the learning stop condition and learning rate etc. The activation function is a sigmoid function. The training algorithm is the Levenberg-Marquardt algorithm that was designed to approach second-order training speed without having to compute the Hessian m atrix [17]. The weight and bias are all set to an identical initial value before

39

training. A t the same time the inputs are normalized to between 0 and 1 so that the neural network will be sensitive to the input pattern.

The M LP network implementation is the matter o f finding the following m atching factors that attributed to failure in learning: 1) adequate numbers o f training cases; 2) sufficient numbers o f hidden neurons; 3) non-deterministic relationships between inputs and outputs. The second one is o f particular interest in the simulation.

4.1.2. Simulation Experiments o f MLP networks

Sim ulation experiments are carried out to show the effect o f different numbers o f hidden neurons to the speed of learning and accuracy. The configuration parameters for the rest o f the experiments in this section are listed as Table 4.1.

Table 4.1 Configuration Parameters for the MLP Experiments

W eight Initial Value 1

Dimension o f Input Vector 1

Dimension o f Output Vector 1

Learning Rate

Learning Stop Condition

M aximum No. o f Epochs 300

0.05

.0001

The first experiment is to try to use a set of data in single frequency with 51 training cases and 51 validation data at the fi"equency o f 3 kHz. Numbers o f M LP networks with different numbers o f hidden neurons have been

40

im plem ented and their NDBE, MSB, training time, and No o f Epochs have been recorded m the table 4.2. The experiment shows that the error indices (NDIE) vary from as low as 0.33% to as high as 0.64% with an average NDIE o f 0.2327% , and the computational results compared well with existing

m easurements. Based on the simulation results, we can see that the optimum num ber o f neurons in this experiment is 20. Fig 4.2(b) depicts the hysteresis loops at frequency o f 3 kHz generated by the modified MLP networks.

Table 4.2. MLP Network Results Using Square Permalloy 80 at Frequency of 3 kHz

Size o f Networks

Training Time (Second)

No of Epochs

NDIE (%)

MSB (10"*)

10 15 20 21 22 23 24 25 26 30 35 40

2.1560 0.8750 0.4380 0.5780 0.5470 0.4690 0.5000 0.5630 0.7350 0.7030 0.5620 0.7500

75 66 14 27 23 16 17 21 23 17 12 12

0.34 0.33 0.32 0.28 0.34 0.25 0.31 0.31 0.33 0.28 0.19 0.32

0.998305 0.92339 0.86197 0.669531 0.999437 0.563349 0.815577 0.860202 0.930848 0.788511 0.322733 0.693552

41

45 50 55 60 65 70 75 80 85 90 95 100 105 110 115

0.7180 0.9370 0.9840 1.0150 1.0470 1.0320 0.8120 1.2500 0.9220 1.4060 0.9850 0.9220 1.0000 1.0930 1.3440

10 11 11 8 7 7 6 7 5 6 5 4 5 6 5

0.24 0.24 0.55 0.33 0.16 0.27 0.16 0.29 0.092153 0.065053 0.074806 0.095903 0.018202 0.057863 0.037592

0.480072 0.469641 0.297932 0.97101 0.23044 0.647826 0.231463 0.746818 0.0736879 0.0367206 0.048557 0.0098066 0.00287469 0.0290517 0.0122623

42

Hysteresis loop for 2 mil Square Permaloy 60 Â· 3 KHz

H(/Vm)

Fig 4.2(a) Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at frequency o f 3 kHz

43

Hysteresis Modeling Based on Experimantal data - 3 KHz

m

H(AAn)

Fig 4.2(b) Results from MLP Neural network-based hysteresis model built based on P-K Model

The Second experiment is to try to use the data of different frequency with 204 training cases and 204 validation data at different frequency specifically DC, 1 kHz, 3 kHz, and 6 kHz. Numbers of MLP networks with different numbers o f hidden neurons have been implemented and their NDIE, MSB, training time, and No o f Epochs have been recorded in the Table 4.3. The experiment results shows that the error indices (NDIE) vary from as low as 0.29% to as high as 0.34% w ith an average NDIE of 0.3615%, and the computational results com pared well with existing measurements. Based on the simulation results, we can see that the optimum number of neurons in this experiment is 49. Fig 4.3(b)

44

depicts the hysteresis loops at different frequency (DC, 1 kHz, 3 kHz, 6 kHz) generated by the modified MLP networks.

Table 4.3. MLP Network Results Using Square Permalloy 80 at Different Frequency (DC, 1 kHz, 3 kHz, 6 kHz)

No. o f Hidden Neurons 40 45 46 47 48 49 50 55 60 70 80 90 100

Training Time (s) 15.8280 19.3130 12.4680 11.125 9.5310 9.0310 12.9840 16.7970 20.8750 35.0470 38.7810 34.9370 61.9220

No. of Epochs

NDEI (%)

MSE (10-^)

300 300 184 158 132 122 167 166 172 211 180 127 180

0.64 0.35 0.33 0.34 0.33 0.34 0.34 0.34 0.34 0.34 0.33 0.34 0.34

3.50616 1.00509 0.926991 0.988855 0.944316 0.962414 0.988385 0.872004 0.990783 0.966097 0.930218 0.956249 0.998783

45

Hysteresis loop for 2 mil Square Permalloy 80 - DC. I KHz, 3 KHz, 6 KHz
" I I I ------------------ r ----------------- 1------------------- ,------------------- , --

3 -

2

-

1

-

0

-

-1

-2

-

-3 -

Is

-3

-2

-1

0 H(AAn)

1

Fig 4.3(a) Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at different frequency

46

Hysteresis Modeing Based on Experimantal data - DC. 1 KHz. 3 KHz, 6 KHz

3 -

2

-

1

-

-1 
-2

-3

-4

-1

0

1

H(Artn)

Fig 4.3(b) Results from MLP Neural network-based hysteresis model built based on P-K Model

4.2.Simulation and Experimental Results of RBF Neural Networks

4.2.1. Simulation Set up

The EAF sample used for simulation experiments is illustrated as Figure 4.1.

The dimension o f the input vector and output vector is 1. The configuration o f the hysteresis RBF is controlled by a script file. The configuration script file controls the parameters such as the dimension of the input and output vectors.

47

m axim um number o f neurons, the learning stop condition and spread constant etc. The weight and bias are all set to an identical initial value before training. A t the same time the inputs are normalized to between 0 and 1 so that the neural network will be sensitive to the input pattern.

The RBF network implementation is a matter of finding the proper radial basis function and the appropriate learning algorithm. It has been shown theoretically and practically, that the performance of the RBF network does not depend on the choice o f the function [8] and hence, Gaussian functions are usually chosen. Therefore, the learning problem is to select adequate number o f RBF neurons, funetion centres and widths. In this paper, the learning was implemented using the orthogonal least squares algorithm that provides a systematic approach to the selection o f RBF centres while using forward regression procedure [10].

4.2.2. Simulation Experiments of RBF networks

Sim ulation experiments are carried out to show the effect o f different spread constants to the speed o f learning and accuracy. The configuration parameters for the rest o f experiments in this section are listed as Table 4.4.

48

Table 4.4 Configuration Parameters for the RBF experiments

W eight Initial Value 0.5

Dimension ofInput Vector 1

Dimension of Output Vector 1

Learning Stop Condition .0001

M aximum No. of Neurons 300

The first experiment is to try to use a set of data in single frequency with 51 training cases and 51 validation data at the frequency o f 3 kHz. A Num ber of RBF networks have been implemented and their NDEI have been recorded in Table 4.5. The error indices vary from as low as 1.9048x10''*' to as high as 4 .4 7 1 4 XlO'"* with an average NDIE of 1.5592x 10"'* and the computational results compared well with existing measurements. Based on the simulation results, we can see that the optimum number o f neurons in this experiment is 49. Fig 4.4(b) depicts the hysteresis loops at frequency of 3 kHz generated by the modified RBF networks.

Table 4.5. RBF Network Results Using Square Permalloy 80 at Frequency o f 3 kHz Spread Constant 0.1 0.5 0.8 1 No of Neurons 49 50 49 48 Training Time (s) 1.2810 1.0940 1.0780 0.9370 NDIE (10-^) 2.6724x10"^ 1.9048x10"': 0.5070 3.9509 MSE (10-'*) 3.1446x10"': 1.59758x10"" 0.0113751 0.69077

49

1.2 1.5 2 2.5 3

48 48 49 50 50

1.0620 1.0150 1.0780 1.0150 1

2.8253 4.4714 2.2781 5.3465x10"Â® 3.4471x10"'

0.35325 0.884753 0.229666 1.265x10"'^ 5.25825x10"'*

Hysteresis Modeling Based on Experimantal data - 3 KHz

-1

0 H(AAn)

1

Fig 4.4(a) Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at frequency o f 3 kHz

50

Hysteresis loop for 2 mil Square Permalloy 80 - 3 KHz

-1

0

1

H(AAn)

Fig 4.4(b) Results from RBF Neural network-based hysteresis model built based on P-K Model

The Second experiment is try to use the data in different frequency with 204 training cases and 204 validation data at different frequency specifically DC, 1 kHz, 3 kHz, and 6 kHz. Numbers of RBF networks have been implemented and their NDIE, MSE, training time, and No of Epochs have been recorded in the table 4.6, The experiment shows that the error indices (NDIE) vary from as low as 0.00019686 to as high as 0.00022638 with an average NDIE o f 0.00020456, and the computational results compared well with existing

51

m easurements. Based on the simulation results, we can see that the optimum num ber o f neurons m this experiment is 154. Fig 4.5(b) depicts the hysteresis loops at different frequency (DC, 1 kHz, 3 kHz, 6 kHz) generated by the m odified RBF networks.

Table 4.6. RBF Network Results Using Square Permalloy 80 at Different Frequency (DC, 1 kHz, 3 kHz, 6 kHz)

Spread Constant 0.05 0.1 0.15 0.2 0.3 0.4 0.5 1 1.5 2 2.5 3

No of Neurons 154 154 154 157 159 161 160 161 161 163 162 162

Training Time (s) 9.1720 9.4380 9.7810 9.3120 9.0310 10.3910 10.3290 10.5470 10.8130 10.6720 10.6570 10.9060

NDIE (lO-^*) 1.9686 1.9686 2.1164 2.1458 2.2595 2.2638 1.9820 1.9686 1.9686 1.9686 1.9686 1.9686

MSE ( io - ") 0.666667 0.666667 0.770521 0.79207 0.878263 0.881554 0.675763 0.666667 0.666667 0.666667 0.666667 0.666835

52

Hysteresis loop for 2 mil Square Peimaloy 60 - DC, 1 KHz. 3 KHz, GKHz

-1

0
H { N m )

1

Fig 4.5(a). Experimental data illustrating normalized hysteresis loops for Square Permalloy 80 at different frequency

53

Hysteresis Modeing Based on Experimantal data - DC, 1 KHz, 3 KHz, 6 KHz

E m

H(AAn)

Fig 4.5(b) Results from RBF Neural network-based hysteresis model built based on P-K Model

4.3. Summary The simulation experiments provide illustration to verify the potential possibility to apply neural networks for reconstruction o f P-K based hysteresis phenomena. The project contributes to the reconstruction o f P-K models based on both MLP networks and RBF networks. The results of simulations show that both M LP and RBF neural networks capable of modeling P-K based hysteresis phenomena. Based on a qualitative comparisons o f the RBF-based and MLP

54

based results and experimental data. It was observed that the computational results compared well with existing measurements. The numbers o f neurons in hidden layers helps to guarantee the training time and accuracy o f modeling procedure so that design o f devices with hysteresis features can be obtained accurately without numerical evaluations.

55

Chapter 5

CONCLUSION

In the project, the source o f the problem is to consider the potential possibility o f applying neural networks for the reconstruction o f P-K based hysteresis phenomena. The review o f both P-K based hysteresis and neural networks demonstrates the feasibility o f modeling P-K based hysteresis using neural networks. T h e contribution of this project is to provide an interpretation of the Preisch-Krasnoselskii model that can be readily implemented by means of neural network methodology. The analysis of the simulation shows that the num ber o f hidden layer neurons plays an important role in the design. It directly effects the training time and accuracy of the modeling. The results o f the simulation show that MLP and RBF neural networks are both capable o f m odeling P-K based hysteresis phenomena.

In summary, based on the formal definition of the P-K based hysteresis provided, the P-K based hysteresis model has several similarities shared with artificial neural network characteristics for reconstruction of P-K model. First o f all, the proposed MLP and RBF neural networks both have ability to approximate any nonlinear function to the desired degree, to learn I/O patterns, extract the hidden relationship among presented data, and provide acceptable response to new data that the network has not yet experienced. This enables neural networks to provide models based on imprecise information. Secondly, they both have local memory, and due to their highly parallel/distributed

56

structure, failure o f a number o f neurons to generate the correct response does not lead to failure o f the overall performance o f the system. In fact there are m any magnetic field applications that will consider the proposed neural netw ork practical. Every time the design of electronic devices is applied, the proposed neural networks could be helpful. Further research work about applying different type of artificial neural networks in the magnetic related fields will provide more valuable information for the design o f electronic devices with hysteresis features. Furthermore, we can consider the following approach; 1) 2) Add noise to the validation data set as the training set remains unchanged. Use Leave-one-out (Jack-Knife) method in the training and validation.

57

REFERENCES

[1]

I.D. Mayergoyez, Mathematical M odel o f Hysteresis, Springer-Verlag, N ew York, 1991

[2]

G. Bertotti, Hysteresis in Magnetism, Academic Press, San Diego, 1998

[3]

I.D. Mayergoyez, "Mathematical model o f hysteresis," IEEE Trans. On M agnetics, vol. 22, no. 5, pp. 603-608, Sept. 1986

[4]

A.A. Adly and S.K. Abd-El-Hafiz, "Using Neural Networks in the Identification o f Preisach-Type Hysteresis Models," Ã EE Trans. On Magnetics, vol. 34, no. 3, pp. 629-635, 1998

[5]

Simon Haykin, Neural Networks, A Comprehensive Foundation. M acmillan College Publishing Company, 1998

[6]

A.R. Sadeghian, "An Interpretation of Preisach-Krasnoselskii Hysteresis M odel W ith the Use o f Artificial Neural Networks," IEEE International, Magnetic Conf. INTERMAG Europe 2002, 28 Apr.-2 M ay 2002

[7]

K. Homik, M. Stinchcombe, and H. White, "Multi-layer Feedforward Networks Are Universal Approximator," Neural Networks, Vol. 2, pp. 359-366, 1989

58

[8]

D.E. Rumelhart, G.E. Hinton, and R. J. Williams, Learning Internal representations by Error Backpropagation, Parallel Data Processings Vol. 1, Cambridge, MA: MIT Press, 1986

[9]

M.J.D. Powell, "Radial Basis Function Approximations for Polynomials, Proc. 12'^' Biennial Numerical Analysis Conf., pp. 223241, Dundee, 1987

[10]

S Chen, C. F.N. Cowan, and P.M. Gamt, "Orthogonal Least Squares Learning Algorithm for Radial Basis Function Networks," IEEE Trans. On Neural Networks, Vol. 2, no. 2, pp.302-309, M arch 1991

[11]

H.H. Saliah, D.A. Lowther, and B. Forghani, "Modeling Magnetic Materials Using Artificial Neural Networks," IEEE Tran. On Magnetics, Vol. 34, no. 5, pp. 3056-3059, September 1998

[12]

Silvano Cincotti, Michele Marchesi, and Antonino Serri, "NN Models o f Hysteretic Non-linear Electromagnetic Devices Under Voltage and Current Supply," IEEE Trans, on Magnetics, Vol. 35, no. 3, pp. 12351238,M ayl999

[13]

D. Lederer, H., Igarashi, A. Kost, and T. Honma, " On the Parameter Identification and Application of the Jiles-Atherton Hysteresis Model for Numerical Modeling o f Measured Characteristics," /E E f Trans. On Magnetics, Vol. 35, no.3, pp. 1211-1214, May 1999

59

[14]

Jun Ho Lee, Dong Deok Hyun, "Hysteresis Analysis for the Permanent M agnet Assisted Synchronous Reluctance Motor By Coupled FEM & Preisach Modeling," IEEE Trans. On Magnetics, Vol. 35, no. 3, pp. 1203-1206, May 1999

[15]

Silvano Cincotti, Ivano Daneri, " A Non-linear Circuit Model of Hysteresis," ÃÃE Trans. On Magnetics, Vol 35, no. 3, pp. 1247-1250, M ay 1999

[16]

D.A. L ow ther, E.M. Freeman, C.R.I, Emson, and C.M. Saldanha, "Knowledge Based Systems Applied to the Design o f Electrical Devices," IEEE Trans. On Magnetics, Vol 24, no. 6, pp. 2576-2578, November, 1988

[ 17]

M.T. Hagan, H.B. Demuth, and M.H. Beale, Neural Network Design, PW S Publishing Company, Boston, MA 1996

60


