S E L E C T IN G A M IX O F D IS P A T C H IN G R U L E S F O R A JO B S H O P B Y U S IN G A R T IF IC IA L NEURAL NETW ORKS

D E C - 9 2004

By Pramit Shah Bachelor of Production Engineering, Mumbai University, June 1997

A thesis presented to the Ryerson University in fulfillment of the thesis requirement for the degree of Master of Applied Science in Mechanical and Industrial Engineering

PROPERTY OF

R yersonU niversity library

Toro nto ,Ont ario .Canada, 2004 @ (Publisher Pramit Shah) 2004

UMI Number: EC52985

All rights reserved INFORM ATION TO USERS

The quality of this reproduction Is dependent upon the quality of the copy submitted. Broken or Indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and Improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, If unauthorized copyright material had to be removed, a note will Indicate the deletion.

UMI
UMI Microform EC52985 Copyright 2008 by ProQuest LUC All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code.

ProQuest LLC 789 East Eisenhower Parkway P.O. Box 1346 Ann Arbor. Ml 48106-1346

AUTHOR^S DECLARATION
I hereby declare that I am the sole author of this thesis.

I authorize the Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

Pramit Shah

I further authorize the Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose^of scholarly research.

Pramit Shah

(Ü )

pro per ty o f

B orrow er's page

R yersonU niversity library

Ryerson University requires the signature of all persons using or photocopying this thesis. Please sign below, and give address and date.

Nam e

Address

Date

(iii)

ABSTRACT
Selecting a Mix of Dispatching Rules For a Job Shop by Using
Artificial Neural Networks @ Publisher Pramit Shah, 2004 Master of Applied Science In the program of Mechanical and Industrial Engineering Ryerson University Dispatching rules are a popular and commonly researched technique for scheduling tasks in job shops. Much of the past research has looked at the performance of various dispatching rules when a single rule is applied in common on aU machines However, better schedules can frequently be obtained if the machines are allowed to use different rules from one another. This research investigates an intelligent system that selects dispatching rules to use on each machine in the shop, based on a statistical description of the routings, processing times and mix of the jobs to be processed. Randomly generated problems are scheduled using permutations of three different dispatching rules on five machines. A neural network is then trained by using a commercial package to associate the statistical description of each problem with its best solution. Once trained, a network is able to recommend for new problems a dispatching rule to use on each machine. Two networks were trained separately for minimizing makespan and the total flowtime in the job shop. Test results showed that the combination of dispatching rules suggested by the trained networks produced better results for both objectives than the alternative of using the one identical rule on all machines.

(iv)

ACKNOW LEDGEM ENTS

I would like to express my gratitude to all those who contributed to this work directly as well as indirectly.

To work with Dr. Ahmed El-bouri was a lifetime opportunity. His trust in his students, continuous encouragement, strong support, insightful guidance makes a student perform far better than he is. I sincerely wish to thank him for all the help provided to me. I really enjoyed the exchange of ideas that helped me to formulate this work. Thank you. Sir!

The financial support through Professor Ahmed El-bouri of Department of Mechanical Engineering- Ryerson University is greatly appreciated.

My family had an indirect contribution to this research. Especially I would like to thank my wife, Miloni for her patience, understanding and emotional support. Thank you from the bottom o f my heart!

I am also thankful to all my fellow graduate students specially Akram and Jayesh, for keeping my spirit up.

And above all, I thank God for giving me the strength and determination.

(v)

To my parents and Miloni

(vi)

propeftty o f

TABLE OF CONTENTS

K yorson U nîvG rsity(ibraiy

A UTHOR 'S DECLARA TION- ----------------------------------------------------------------------------Ü ABSTRACT ---------------------------------------------------------------------------------------------------- iv ACKNOW LEDGEM ENTS --------------------------------------------------------------------------------- v TABLE OF CONTENTS ---------------------------------------------------------------------------------- vu LIST OF FIGURES ----------------------------------------------------------------------xU

LIST OF TABLES ----------------------------------------------------------------------------------------- -xiv NOTATIONS -------------------------------------------------------------------------------------------------xvi GLOSSARY ----------------------------------------------------------------------------------------------------------------------------------- -XV Ü

Chapter 1 : Introduction and Background 1.1 Introduction--------------------------------------------------------------------------------------- 1 1.2 Problem Definition ----------------------------------------------------------------------- 5

1.3 Dispatching Rules-------------------------------------------------------------------------------7 1.4 Thesis Outline------------------------------------------------------------------------------------ 9

Chapter 2: Literature Review 2.1 Literature Review------------------------------------------------------------------------------ 10

Chapter 3: Artificial Neural Networks.................................................................................... 22 3.1 Introduction to Artificial Neural Networks (ANN)-----------------------------------22 3.1.1 Basic Concepts------------------------------------------------------------------- 22

(V Ü )

3.1.2 Architecture of ANN----------------------------------------------------------- 23 3.1.3 Classification of Neural Network Architectures------------------------25 3.1.4 Feed Forward Neural Network----------------------------------------------- 25 3.1.5 Learning rule---------------------------------------------------------------- --- 26 3.2 Neural Network Training by Back Propagation (BPNN)------------------------ 27 3.2.1 Scaling Function---------------------------------------------------------------- 28 3.2.2 Transfer Function--------------------------------------------------------------- 28 3.2.3 Delta Rule-------------------------------------------------------------------------29 3.2.4 Type of Datasets---------------------------------------------------------------- 29 3.2.5 Learning Rate and Momentum----------------------------------------------- 30 3.2.6 Initial Weights--------------------------------------------------------------------31 3.2.7 Training BPNN------------------------------------------------------------------ 31

C hapter 4: M ethodology and Design of BPNN................................................................... 33 4.1 Design of a BPNN for Job Shop Scheduling----------------------------------------- 33 4.2 Problem Analysis---------------------------------------------------------------------------- 33 4.3 Data Acquisition and Preparation-------------------------------------------------------35 4.3.1 Input Variable Selection-------------------------------------------------------35 4.3.2 Construction of Input Vector----------------------------- ----------------- 37 4.3.3 Output Layer--------------------------------------------------------------------- 41 4.3.4 Data Collection------------------------------------------------------------------ 41 4.3.5 Validation of Data Set Output------------------------------------------------ 43 4.3.6 Data Sorting---------------------------------------------------------------------- 46

(viii)

4.4 Design o f Neural Network-------------------------------------------------------------------49 4.4.1 Hidden Layer----------------------------------------------------------------------- 49 4.4.2 Hidden Units (Neurons)----------------------------------------------------------50 4.4.3 Training Stopping Criteria------------------------------------------------------ 51 4.4.4 Training Procedure----------------------------------------------------------------53 4.4.5 To Find Optimal Number of Hidden Neurons (Units)------------------53 4.5 Training BPNN--------------------------------------------------------------------------------- 55 4.6 Implementation--------------------------------------------------------------------------------- 56 4.7 BPNN Model for Minimizing Mean Flowtime Criteria----------------------------- 59

Chapter 5: Analysis and Discussion.........................................................................................62 5.1 Analysis o f Trained Network--------------------------------------------------------------- 62 5.1.1 Neural Network Weights-------------------------------------------------------- 62 5.2 Test Problem s---------------------------------------------------------------------------------- 62 5.3 Test results for Minimizing M akespan--------------------------------------------------63 5.4 Test results for Minimizing Mean Flowtime-------------------------------------------64 5.5 Further Discussion on R esults------------------------------------------------------------- 6 8 5.6 Analysis of Variance (A N O V A )----------------------------------------------------------69 5.6.1 Assumptions------------------------------------------------------------------------ 72 5.6.2 Data Representation-------------------------------------------------------------- 72 5.6.3 General Logic - Analysis of Variance--------------------------------------- 73 5.6.4 ANOVA Calculations-----------------------------------------------------------74 5.6.5 Hypothesis-------------------------------------------------------------------------- 75

(ix)

5.7 ANOVA Results for Makespan----------------------------------------------------------75 5.8 ANOVA Results for Mean Flowtime--------------------------------------------------- 78

Chapter 6: Conclusion and Future research...................................................................... 80
6

.1 Summary-------------------------------------------------------------------------------------- 80

6.2 Conclusions-----------------------------------------------------------------------------------81 6.3 Future Research------------------------------------------------------------82

References-------------------------------------------------------------------------------------------------- 84

Appendix A: C++ Programs Appendix A l; Problem Generation---------------------------------------------------------A1 Appendix A2: Training Set Generation-------------------------------------------------- A3 Appendix A3: Comparison of BPPNN Results with Other Dispatching Rule Results------------------------------------------------------------------------------------------ A13 Appendix A4: Test Problem Generation------------------------------------------------ A 17

Appendix B: Arena Simulation Model------------------------------------------------------------B1

Appendix C: Experimental Setup Appendix C l: Determining Number of Hidden Neurons for Makespan------------C l Appendix C2: Determining Number of Hidden Neurons for Mean Flowtime --C2

(x)

Appendix D: Neural Weights Appendix D l: Makespan Network---------------------------------------------------------- D1 Appendix D2: Mean Flowtime Network---------------------------------------------------- D9

Appendix E: Comparison Results Appendix E l; Comparison o f BPNN with makespans from optimal rule combinations for n = 10 to 100 ---------------------------------------------Appendix E2: Comparison of BPNN with mean flowtimes from optimal rule combinations for n = 10 to 1 0 0 --------------------------------------------------------------- E8 El

(xi)

LIST OF FIGURES
Figure 1.1 Schematic layout of a job shop, illustrating the routes followed by two different jo b s------------------------------------------------------------------------------------------------- 2 Figure 2.1 Degree of artificial intelligence required by different scheduling approaches-------------------------------------------------------------------------------------------------- 15 Figure 3.1 Basic features of a single biological neuron----------------------------------------- 23 Figure 3.2 Processing elements-----------------------------------------------------------------------24 Figure 3.3 Classification of common neural network architectures-------------------------- 25 Figure 3.4 Multi-layer feed forward neural netw ork----------------------------------------------26 Figure 4.1 Design methodology for building ANN------------------------------------------------ 34 Figure 4.2 Structure of ANN proposed for the 5-machine job shop -------------------------- 44 Figure 4.3 Details of a hidden n o d e.................... Figure 4.4 Effect of number of hidden neurons on the minimumaverage error for makespan--------------------------------------------------------------------------------------------------- 54 Figure 4.5 BPNN generalization vs. minimum average error for makespan---------------- 57 Figure 4.6 Effect of number of hidden neurons on the minimum average error for mean flowtime-----------------------------------------------------------------------------------------------------60 Figure 5.1 BPNN makespans compared to makespans from optimal rule combinations for n=50----------------------------------------------------------------------------------------------------------65 Figure 5.2 BPNN mean flowtime compared to mean flowtime from optimal rule combinations for n=50------------------------------------------------------------------------------------ 67 Figure 5.3 Comparison of BPNN, SPT, LPT and MWKR with respect to makespan of optimal rule combinations-------------------------------------------------------------------------------70 50

(X Ü)

Figure 5.4 Comparison of BPNN, SPT, WINQ+PT and LWKR with respect to mean flowtime of optimal rule com binations-------------------------------------------------------------71

(xiii)

LIST OF TABLES
Table 1.1 Characteristics of the job shop problem considered------------------------------------ 9 Table 4.1 Raw data of example problem N10M265-----------------------------------------------38 Table 4.2 Determining inputs 1 to 5 for example problem NM 10265-------------------------38 Table 4.3 Inputs
6

to 10 for example problem N10M265---------------------------------------- 39

Table 4.4 Example of normalized value for inputs 11 through 15 for problem N10M265----------------------------------------------------------------------------------------------------40 Table 4.5 15-unit input vector representing Job shop problem of Table 4.1---------------- 40 Table 4.6 Optimal rule combinations for example problem N10M265-----------------------42 Table 4.7 Numerical representation for optimal rule combinations of Table 4.6-----------43 Table 4.8 15-unit vector representing desired output for example problem N10M265-----------------------------------------------43

Table 4.9 Sample of input and output pattern pairs-----------------------------------------------45 Table 4.10 Attributes used in job shop Arena model--------------------------------------------- 46 Table 4.11 Input and output patterns of Table 4.9 after sorting------------------------------- 48 Table 4.12 Total makespans using BPNN with various number of hidden neurons 54

Table 4.13 Training and generalization results for mdnirnizing makespan------------------ 56 Table 4.14 A 20-job example problem--------------------------------------------------------------- 57 Table 4,15 Input vector for example problem of Table 4 .1 4 ---------------------------------; --58 Table 4.16 BPNN output for example problem of Table 4 .1 4 ----------------------------------58 Table 4.17 Total mean flowtimes for BPNN with various numbers of hidden neurons------------------------------------------------------------------------------------------------------ 5 9

(xiv)

Table 4.18 Mean flowtime BPNN output for example problem of Table 4.14-------------- 60 Table 5.1 Summary o f total makespan in sets of 50 test problems for various n -----------64 Table 5.2 Results for individual problems for n=50 (set no. 9 in Table 5 .1 )-----------------65 Table 5.3 Summary o f total mean flowtime results for test sets consisting of 50 problems ea c h ------------------------------------------------------------------------------------------------- 6 6 Table 5.4 Mean flowtime for individual test problems for n=50 (set no. 9 in Table 5 .3 )---------------------------------------------------------------------------------------67 Table 5.5 Experimental data for ANOVA (average makespan)---------------------------------76 Table 5.6 ANOVA results for the performance objective of minimizing m akespan 76

Table 5.7 LSD calculations for the minimization of makespan experim ent---------------- 77 Table 5.8 ANOVA experimental data (average mean flow tim e)------------------------------- 78 Table 5.9 ANOVA results for the performance objective of rrimimizing mean flow tim e------------------------------------------------------------------------------------------------ 78 Table 5.10 LSD calculations for the meanflowtime experim ent-------------------------------- 79

(xv)

NOTATIONS

m I j k A j-

Number of machines Subscript for jobs, 1= 1,2,3, n.

Subscript for operation, j = 1,2,3,.......m. Subscript for machines, k = l,2,3....m . Time of arrival of job on the shop floor. Processing time of operation j of job i on machine k.

Qll^ ni
tIq

Number of operations j required to complete job i Number of input neurons. Number of output neurons. Mean routing order on machine k. Completion time of job i. Maximum completion time or makespan. Flow time of job i. Standard deviation. Variance Numberof treatments for ANOVA experiments. Grand mean. Group mean.

Rj^ C^max Fj< T Var (X) K X Xj

(xvi)

G LO SSARY

D ispatching rules

Rules based on the smallest processing time or least work remaining LWKR: - Select the job with the least total work remaining work S P T S e le c t the job with the least intermediate processing time TFT : - Select the job with least total processing time Rules based on the largest processing time or most work content LPT:- Select the job with the most intermediate processing time LOPNR: - Select the job whose least number of operations remaining MOPNR: - Select the job whose most number of operations remaining. MWKR:- Select the job with the most total work remaining work MWKR-P;- Jobs ranked by MWKR after the present scheduling operation MWKR/P: - Jobs ranked by the greatest ratio of total remaining work to processing time of schedulable operation Rules based on due date DS: - Select the job with the smallest slack, where slack is the due date less the total work remaining. This is also called as the dynamic slack. DDT / EDD: - Jobs ranked according to their earliest due date SLK/RO: - Select the job wit the smallest ratio of its dynamic slack to the number of remaining operations Rule based on arrival AT: - Select the job based on earliest arrival time

(xvii)

FIFO: - First in first out LIFO: - Last in first out RAN: - Select the job randomly Rule based on queue status WINQ: - Select the job based whose workload in the nextqueue is least NINQ: - Select the job based on the number of jobs waitingin the queue of the next machine.

(xviii)

CHAPTER 1 Introduction and B ackground

1.1 Introduction
In real world applications, one of the most important aspects of competitiveness and success o f an organization is efficient production management and timely decisions. Such decisions are often constrained by specific objectives and requirements. These constraints are frequently incompatible in nature, and as the number of such constraints increases, the decision process becomes more complex. Scheduling decisions are an important component of overall operational control in any manufacturing system. The decision-making requirements for scheduling production systems have been a fascinating research topic for more than three decades. The goal of scheduling is to ensure that production objectives are met optimally. Researchers have developed numerous models and methodologies for the planning and scheduling control of production activities for different manufacturing layouts. Two of the most researched models are flow shop and job shop. A flow shop is a model in which machines are arranged in series. In this system, jobs flow from an initial machine (resource), through several intermediate machines, and ultimately to a final machine. A job may skip a particular machine, but all jobs must be processed according to a fixed route. The job shop differs from the flow shop in one important aspect: the flow of work is not unidirectional, meaning that jobs need not visit the machines in the same order. The problem in the job shop is to schedule n jobs on m

machines such that one or more performance objectives is optimized. Each job requires 7 number of operations (tasks) that are to be processed on these machines. Generally, a job may not require all the machines and it may visit the same machine more than once. A scheduling problem in a job shop is defined by several attributes, namely, the number of jobs; the number of machines; the processing order of the jobs on the machines (or routing); processing time of each job on different machines; job arrival times, and last but not the least, the performance objective to be achieved. Figure 1.1 displays a general job shop model. It illustrates the routes taken by two different jobs through the shop. In this instance both the jobs visit machine number three after completion of two operations as per their respective job routes. Supposing Job 1 completes both its operations prior to job 2 , then job
2

may have to wait in the machine

buffer (or queue) if the machine is still busy with job 1 .
A Job shop L ay o u t

Mac hine 3 Machine 1
B u ffe r 3

B u ffe r 1

Machine 5

B u ffe r 5

^ Mat e r i al Ha n d l i n g s y s t e m

Machine 2

Machine 4

B u ffe r 2

B u ffe r 4

J ob 1 route is 1 - 2 - 3 - 5 - 4 . a n d job 2 is 5 - 4 - 3 - 1 - 2

F ig 1.1 Schem atic layout o f a job shop, illustrating the routes follow ed by tw o different jobs.

For scheduling n jobs on a single machine there are n! possible schedules, but in the case of a job shop the number o f possible combinations is (n!)^, when each job has one and only one operation on each machine (Conway, ] 967). For example with 10 jobs on 5 machines there are 6.92x10^^ possible number of schedules. Thus, job shop scheduling is a NP-hard problem (Pinedo, 1995) and, consequently, it is not practical to search for a global optimal solution other than for very small problems. A general job shop problem can be either static or dynamic. The static job shop assumes that all jobs are available at the beginning of the planning phase. On the other hand, the dynamic job shop problem assumes that the arrival times o f the jobs are unknown, and that jobs arrive at random intervals throughout the production cycle. A schedule can be evaluated by its ability to meet production objectives like minimizing the make span (or completion time), the mean flow time and mean tardiness. A schedule that results in minimum completion time for a particular problem does not necessarily minimize the mean flowtime in that problem. Hence, the aim of the scheduling is to find a schedule for processing all jobs, such that one or more specified performance measures are optimized. A number o f job shop scheduling techniques have been developed for solving static and dynamic problems. Among these are the dispatching rules, which are a popular and commonly researched technique for scheduling tasks in a job shop. A dispatching rule (DR) is used to select the next job to be processed from a set of jobs waiting in the queue (or buffer) when a machine becomes free. The simplicity and ease in application o f dispatching rules (DRs) have made them a practical tool for scheduling in the real world. However, there are some shortcomings in the use of dispatching rules. First, none o f the dispatching rules dominates the others for the important performance criteria like mean

flow time, mean tardiness, etc. This implies that DRs are problem dependent, and one DR that gives a good result for one problem may not give an equally good result in another. Due to the dynamic and changing characteristics of the jobs as they are processed, a job shop may be able to meet the performance objectives better by judiciously changing the DR on individual machines over time, or by using a combination of different rules for the machines. As a result of the scheduling complexity in modem manufacturing systems, Artificial Intelligence (Al) techniques have been considered as a scheduling decision tool; one technique that has shown promise is the Artificial Neural Network (ANN). In simple terms. Artificial Neural Networks apply knowledge gained from past experience to new problems or situations. An ANN looks for pattems in "training" sets of data, learns these pattems and develops the ability to correctly classify new pattems. This approach in a job shop that uses dispatching ml es requires the neural network to be prepared by selecting a set of training examples for different performance measures, and finding from simulation (or other) studies the optimal mles to use in these examples. Information from these optimal solutions is then used to "teach" the network to select the most appropriate dispatching rule from several candidate / mles. Once this procedure is completed, the trained neural network is capable of providing faster solutions to new problems which were previously unseen by the net'vork. This research addresses the problem of an appropriate selection of dispatching mles (in this case from three altematives) to use for each of the machines in a job shop by using artificial neural networks. Two separate artificial neural networks are developed for the performance criteria of minimizing makespan and minimizing mean flowtime. Each o f these criteria have a different set of competing DRs to choose from.

1.2 Problem Definition
A job shop problem of interest is a 5-machine job shop. In actual manufacturing systems, particularly those that follow a group technology concept, the number of machines used can be reasonably expected to be between 3 to 10. Also, it seems to be a consensus among researchers that a four-machine job shop is adequate to represent the complexity involved in a large job shop (Kiran et al. 1984). Thus, the number of machines chosen for this study is 5 machines, with number of jobs ranging from 10 jobs to a maximum o f
100.

In this study, a static job shop is considered with arrival time

=

0

for all the jobs.

Thus, all the jobs that are to be processed are available at the start o f the scheduling period. It has been assumed that all the jobs have two sets of attributes. The first one is the work flow Q-j^ pattern (or route), that is machine k's order in job f s route. At the beginning o f the schedule, each jo b 's flow pattern is defined with the condition that each job visits all machines once. In other words, each job has a specific precedence order of operations, which has to be followed before exiting from the system. For example, a specific job i may have the precedence order 4 indicating that job f s first operation must be done on machine 4, its second operation on machine 5, and so on. The second characteristic is the processing time (Pjjjç) representing t h e o p e r a t i o n o f job i on machine k. The time needed by each operation j o f job i on machine k is known in advance. In this study, deterministic processing times for these operations are generated from the discrete uniform distribution U (10, 99) integer time units. For ^ 5 ^ 3 ^2 ^ 1,

example, a specific job on 5 machines may have processing times of 45-95-61-20-35 corresponding to the machines on the job's route. Once operation j o f job ; has been completed, it will be transferred with the help of a material handling system to the next machine, as per the job's route for the next operation 7 +1 if that machine is free, or to a buffer for that machine otherwise. It has been assumed that the transfer time between machines is negligible and that the material handling system is always available whenever required. Hence, jobs are either in process on a machine, or waiting in a buffer for processing. "Schedules are generally evaluated by aggregate quantities that involve information about all jobs, resulting in one-dimensional performance measures" (Baker, 2002). The following two perfonnance measures or criteria or objectives are considered in this study. a) Makespan: The makespan measures the total time taken by a given schedule to complete the set of available jobs. In other words, it is the time at which the last job exits from the shop. This measure is defined, for a sequence of n jobs, by
^ m ax ~ maxj{Cj} -- (2.1)

Where

= Completion time of job i = 1,2,3.... n

The objective of scheduling the jobs in a way that minimizes the makespan is an important one, because it reduces throughput time for processing a batch of different jobs. b) Mean Flowtime: Flowtime represents the total time spent in a job shop. This includes actual machine processing time plus time spent waiting in buffers. The mean flowtime is given by:

F =-ÎFi
n ;= 1

....

(2 .2 )

Where Fj - Flow time or the time spent by job / in the system The objective o f minimizing the mean flowtime is a common objective in scheduling, because it acts to reduce work-in process and inventory levels. The job shop model considered in this study is based on the following explicit assumptions: · Jobs are independent and consist o f strictly ordered operational sequences (or job routes). Furthermore, all jobs have equal weights (importance). · Job pre-emption or cancellation is not allowed. Once the processing of any operation has started on a machine, it cannot be interrupted before its completion, and then resumed at a later time. · There is only one machine o f each type in the shop, and operations for two different jobs cannot be processed simultaneously on the same machine. · · · Set up time is negligible. An operation may not begin until its predecessor is complete. Each machine is continuously available for production; machine breakdown or downtime is not considered. · There are no alternate routes for any job. · The buffer capacity is unlimited and machine blocking does not occur.

1.3 Dispatching Rules (DRs)
Dispatching rules or (priority rules) refer to the procedure used to prioritize the jobs that are waiting in queue for a machine. The dispatching rule therefore picks the next job

to load on the resources or machine. There are more than 100 dispatching rules surveyed by Panwalkar and Iskander. (1977). Dispatching rules use job specific information snch as processing time, due date, remaining number of operations, etc. The rules considered in this study are;

a) R u les b a se d on s h o r te st p r o c e ssin g tim e · Shortest processing time (SPT): - Priority is given to the job with the shortest immediate processing time (i.e., smallest processing time on the current resource). · Least work remaining (LWKR): - Priority is given to the job with the least sum of remaining operation processing times. b) R u le s b a sed on L o n g e st p r o c e ssin g tim e · Longest processing time (LPT): - Priority is given to the job with the longest immediate processing time (i.e., largest processing time on the current resource). · Most work remaining (MWKR) - Priority is given to the job with the total work remaining processing times.
c) R u le b a sed on Q u e u e sta tu s

· Work in the next queue (WfNQ + PT): - Priority is given to the job with the least workload in the next queue it will visit, plus the processing time resource. The Following Tablel.l summarizes the job shop problem parameters considered in this research. on the current

Number o f jobs Number of machines Job arrival times Flow pattern Processing time

From 10 to 100 5 All jobs are available at the start of schedule Process routes Deterministic integers drawn from U (10,99) 1) Minimizing makespan

Performance measures 2) Minimizing mean flowtime Dispatching rules SPT. LPT, MWKR, LWKR, WINQ+PT Selection of dispatching rules by a trained neural Job shop scheduling technique network
T a b le 1.1 C h aracteristic o f th e jo b sh op p rob lem und er con sid eration .

1.4 T hesis outline
The rest of the thesis is organized as follows; Chapter 2 provides a detailed literature review on the dispatching rules, neural networks for scheduling, and past research in selecting combinations o f dispatching rules by different techniques. Chapter 3 presents a background introduction to neural networks. Chapter 4 provides a step-by-step guideline on building a feed fon\'ard back-propagated supervised neural network for selecting dispatching rule combinations. Chapter 5 analyses the output generated from the trained network and examines the generalization capability of the trained network. Chapter
6

provides a summary and a conclusion, and discusses further research scope.

CHAPTER 2 Literature Review

2.1 Literature Review
Scheduling jobs is an important aspect of a job shop manufacturing system environment, for it can have a deep impact over the system's performance efficiency. Job shop scheduling has been studied extensively over the last three decades. Many approaches have been developed to solve the static job shop scheduling problem, and some of the well-known approaches can be found in French, (1982) and Pinedo, (1995). However, for a practical application, scheduling decisions are usually taken in real time considering existing constraints. Some of such constraints are the state of the shop floor (e.g., availability of resources), characteristics of the production program (e.g., part routing, due date of jobs) and production objectives (e.g., minimizing makespan) to be achieved. Nevertheless, some of the uncertain variables such as breakdown / failures of resources, new jobs prompted during a production cycle, etc. also need to be considered. For these reasons many researchers tend to approach the job shop scheduling issue through the acceptance of heuristic dispatching rules rather than seeking a deterministic optimal solution to the problem. This literature review reports on various job shop scheduling approaches. Both Jones and Rabelo, (1996) and Blazewicz et al, (1996) have done detailed surveys on various job shop scheduling techniques, which can be categorized mainly as either exact or approximation methods.

10

The category o f exact methods includes all the mathematical models. These guarantee optimal solutions for a job shop problem. However, their application is limited to a smaller numbers o f jobs and resources. This is due to some of the limitations like computational requirements for obtaining an optimal solution, difficulty in formulation of material flow constraints, etc. Also, the development in computational power o f the computer has sharply improved the use of such approaches, but nevertheless its utilization remains limited. Branch and bound is one such mathematical technique that deals with NP- hardness o f scheduling problems by decomposition into smaller sub problems that may be solved for optimality. Blazewicz et al, (1996) presented a detailed discussion on the success and the limitations o f this method in job shop problems. The category o f approximation methods includes numerous algorithms and techniques that are developed for producing good solutions, which can be reasonably close to optimal results. These techniques can be further categorized as dispatching / priority rules. Artificial Intelligence (Al) techniques and other heuristie methods. They are used either to obtain a best sequence of jobs for the desired perfonnanee objective, or to select from various dispatching rules ones to apply on the maehines, based on the current or prevailing conditions. One o f the most common approaches to dynamically schedule jobs is to use dispatching rules (DRs). These rules are sometimes ealled scheduling rules, or priority rules. They are defined by Blackstone et al, (1982) as a "Rule used to select the next job to process from jobs awaiting service." Dispatching rules are widely used in practice and a considerable body o f research exists because o f their ease of implementation and their substantially redueed computational requirements. 11

The most well known and comprehensive survey of scheduling heuristics is by Panwalker and Iskander (1977) where more than 100 dispatching rules were presented, reviewed and classified based on their processing time, arrival time, queue status, etc. A survey of 34 dispatching rules could also be found in Blackstone et al, (1982). A common conclusion found in both surveys is that no single priority rule dominates and provides consistently best results for different job shop situations. There have also been many instances where combinations of rules have been successfully used in job shop scheduling (Blackstone et al, 1982). This approach has two or more dispatching rules dynamically selected for each of the machines based on the shop floor's prevailing conditions. Review of literature related to dispatching rules in job shop schedules reveals a focus either to introduce new dispatching rules to optimize the shop floor performance or to review and test the existing ones, both for different shop configurations and performance objectives. Advanced simulation tools have been widely adopted for this purpose. For instance, in the simulation study done by Waikar et al, (1995), ten different dispatehing rules (FIFO, SPT, DDT, LWK_R, MWKR, MWKR-P, MWKR/P, MOPNR, SLK/RO and RAN) were tested for different shop loads ranging from 70 to 85 %, with job arrival and processing times following exponential and normal distributions respectively. Waikar et al, (1995) considered two different sets of performance criteria based on the flow-time and tardiness. The results of this study showed that both SPT and LWKR perform well under different shop loads not only for mean flowtime, which resulted in lowering in proeess inventories, but also for total queue time and time spent in the system.

12

On the other hand, a comparative study done by Chang et al, (1996) evaluated the performance o f 42 different dispatching rules using a linear programming model. These rules were broadly categorized in six different categories based on the smallest processing time, the largest processing time, due date, number of operations, random rule and lastly on the queue status. In order to evaluate all the dispatching rules, seven different performance objectives were considered, which were further divided into two sets, based on completion time and tardiness. Their analysis indicated that the shortest processing time (SPT) related rules consistently performed well, while the longest processing time based rules consistently performed badly. Similarly, Rajender and Holthaus, (1999) carried out two different types of comparative studies o f dispatching rules in both flow shops and job shops. In the first study, operations were performed on all the machines, but missing operations were allowed for both the shops in the second study. The job shop studied had len machines, number o f operations ranging from
(2

. . . 1 0 ), processing times uniformly distributed

between (1,49), shop loads ranging from 80 to 95% and exponentially distributed interarrival time. In their study, thirteen different dispatching rules such as (FIFO, AT, FDD, SPT, PT+WINQ, etc) consisting of both existing and new rules proposed by Rajender and Hohhaus, (1999) were evaluated for seven different performance objectives. The objectives were based either on flow time or on tardiness. The results showed that for the mean flowtime critenon SPT, PT+WfNQ and RR (a rule by Raghu and Rajendran, 1993) perfonTied consistently well under different shop loads. For the same criteria with missing operations. PT+WfNQ rules emerged on average to be the best. The study also showed that for a higher shop load PT+WINQ performs significantly better than the SPT rule. 13

Almost all the papers cited above either introduced new dispatching rules or tested existing rules based on a validated simulation model developed to test the rule itself. Thus, simulation is one of the most common tools widely used by many researchers for testing new or existing dispatching rules. Although for many years the only practical approximation methods were priority/ dispatching rules, the introduction of more powerful computers, as well as an emphasis on carefully designed, analyzed and implemented algorithms has allowed more novel approaches to be developed for solving job shop problems. One example of such an approach is the use of artificial intelligence (Al). Al is a sub field of computer science that is concerned with integrating biological and computer intelligence. It has fundamental origins from biological understanding and uses principles in nature to find solutions for various complex problems. There are a number of classes of Al techniques, some of which are expert/knowledge-based system, neural networks (training and learning), fuzzy logic, genetic algorithm search, etc. As the development in solving job shop scheduling problems increased, the degree of intelligence and the knowledge required for solving such problems also increased. Figure 2.1 presents the degree of artificial intelligence required by different scheduling approaches (Sim et al, 1994). Expert / knowledge based systems mainly consist of knowledge and an inference engine to operate on that knowledge base. The application of such systems can be seen in

14

T3

·g
> o

(U

S
\

Neura! Network Expert systems

-- o
03

B E c o
(D

\
Dispatching rules
Com posite rule

R ule induction

P

O 0)
2

0) Q

en

Analytical Approaches

Scheduling methods
F ig . 2.1 D eg ree o f a rtificial in tellig en ce req u ired b y different sch ed u lin g a p p ro a ch es (S im et al, 1994).

Pierreval, (1992), who demonstrated its use for selecting priority rules in a two machine flow shop model o f a flexible manufacturing system, with the shop load o f 83%. Two different dispatching rules (SPT and HDD) for five different criteria (based on flow time and tardiness) were examined. The results showed that expert system (ES) provided on average good results for the perfonnance criteria of mean flowtime and average waiting time as compared to SPT on both machines. Also, ES achieved the best performance among SPT-EDD and SPT-SPT combinations on the two machines for mean tardiness criteria. Sim et al, (1994) developed an expert neural system in order to overcome some of the limitations with the expert system and for solving job shop problems for objectives related to tardiness. The artificial neural network was based on the back propagation feed forward neural network (BPNN) with the generalized delta rule as a learning algorithm and the sigmoid curve as the activation function. This model consisted o f an input array 15

of 14 neurons, sixteen sub-networks embedded in expert system and a single output node, which determined jobs to be processed first, based on the lowest output value. The job shop had nine resources, job arrivals following a Poisson distribution with an average rate of 0.6 to 0.775. Each job had 3-6 operations and processing time of 1-4 time units per operation and the performance objectives of minimizing lateness and tardiness. The first 10 input neurons represented different dispatching rules (such as SPT, LPT. EDD, etc) and the remaining neurons represented the arrival rate of jobs in the neural network. The results showed that the performance of this system was able to match the performance of the best dispatching rule used in training for both the performance objectives. Although the BPNN required a lengthy training process, once trained, the network only requires a single forward pass of computation (from the input nodes to the output node) to schedule. Production demands are often cyclic in nature, and if the pattern of such demands can be recognized, then systems can respond to seasonal and sudden ehanges. Thus, a system that is able to recognize such patterns is in a position to update scheduling deeisions effectively. Artificial neural networks, fuzzy logic and genetic algorithm are some o f the methods that can be used in order to develop such systems. Fuzzy logic application in dynamic selection of dispatching rules was examined by Subramaniam et al, (2000) for the performance objective of minimizing makespan in lenjob, /en-machine job shop problems. The proposed approach carefully selects ihree normalized input units, based on the conditions prevailing in the job shop, and a fuzzy seheduler seleets an appropriate dispatehing rule from the available eandidates SPT, WFNQ, MWKR to use for the individual machines. The results showed that the best makespan was obtained with the use of a eombination of dispatehing rules as compared to using a single dispatehing rule such as SPT, LPT, MWKR, LWKR, FIFO, LIFO, 16

MOPNR, LOPNR, NINQ, WINQ. The fuzzy scheduler is a one-pass approach and requires the same order of computation time as the simple dispatching rule. On the other hand, both Kumar & Srinivsan, (1996) and Chryssoluris & Subramaniam, (2001) used genetic algorithms (GA) for dynamic selection of dispatching rules in job shop scheduling problems. In the former, a genetic search procedure for a real world combinatorial optimization problem was considered. The job shop had eighty jobs and fifty-nine machines, with the number of operations ranging from (2, 37). Some jobs could visit a machine more than once. Genetic algorithms use the idea of survival o f the fittest by progressively accepting better solutions to the problems. In this case randomly generated strings o f dispatching rules had a length o f ten. The GA method generated a better makespan as compared to using any one of the seven different dispatching rules (SPT, LPT, TPT, RPT, DS, EDD, RS, FIFO) on all machines. The proposed algorithm yielded an improvement o f about 3% in makespan over the best (SPT) among the seven rules tested, but the computation time required by the genetic algorithm (998 sec) was very large as compared to generating makespan by using a single dispatching rule (3.32 sec). In the GA study by Chryssoluris & Subramaniam, (2001), the dynamic job shop had six machines, fixed job arrival times, from three to six operations and processing time ranging uniformly from U [1,100] and unifonuly distributed due date of L [-100,1500] (loose) and U [-100,500] (tight). The study also considered machine breakdowns and repair, and jobs could visit the same resource more than once. The performance objectives were minimizing cost and tardiness. The proposed GA outperformed the other dispatching rules. Varying due dates did not seem to have any effect on the relative

17

variation of the results. The computation time for this GA approach was about 2 orders of magnitude larger than for a simple dispatching rule. An artificial neural network (ANN) has the capability to recognize and learn new patterns to generalize for any measurable function. ANN has been employed in a number of real world applications in manufacturing, finance, stock market, medical field, national security, etc. Zhang and Hung, (1995) provided a detailed survey on the neural networks in manufacturing with the applications in the areas of process planning, quality assurance, engineering design, scheduling, process control, etc. There are various types of neural networks proposed and developed for solving scheduling problems. It has been observed in the literature that job shop scheduling problems were solved by neural networks either to obtain optimal sequence of jobs or to make a dynamic selection of dispatching rules based on the prevailing conditions of shop floor; so that the desired performance objective could be satisfied. For the purpose o f research review, the various types of neural network that are of interest, viz; 1) 2) Hopfield network and other optimizing networks Multi-layer preceptrons (Back propagation networks)

Sabunguoglu, (1998) presented a detailed survey on using neural networks exclusively for scheduling applications. He proposed two different classifications, based on the types o f neural networks used and the application area. Hopfield networks were the first type of artificial neural network used for solving job shop scheduling problems. Too and Takeji, (1988a, 1988b) proposed a Hopfield network with only input and output neurons. They mapped the problem on a two dimensional matrix of neurons with (nm + 1 ) rows and (mn) columns, where m is the number of machines and n is the number of jobs. A Simulated annealing process was then 18

applied to the model in order to force the network out of local minima. The method was successful for a 4-job, 3-machine problem, but it had several limitations, such as the number o f jobs must be greater than the number of machines. The proposed network in this method is not practical for large size problems and there is no guarantee of an optimal solution. Furthermore, computation time, even for a small problem is excessive. Satake et al, (1994) used a Hopfield network for minimizing the makespan in a job shop by obtaining an optimal sequence of jobs. In this case, a Boltzmann mechanism was used in order to avoid local minima. Various problems were considered ranging from
4-

jobs/3-m achines to 10-jobs/l 0-machines. The proposed network produced optimal or near optimal schedules within a reasonable amount of time. Further development in the application o f Hopfield networks for solving a job shop problem can be obtained from (Sabunguoglu, 1998) and (Jain and Meeran, 1998). The conclusions drawn from the review of Hopfield networks are that they require an excessive number o f neurons and interconnections and can get easily trapped in local minima. For these reasons, they are suitable only for small size problems. It has been obserx'ed in the literature that back propagation (BP) networks have drawn the attention o f many researchers. One o f the reasons for this interest is that BP network provides an increased speed for the selection process that may be needed in real world applications. Jones and Rableo, (1990) were the first to use the back propagated neural network in a proposed integrated (expert and AFTN) scheduling system for ranking a set o f dispatching rules based on the current shop status and job characteristics such as job types, arrival patterns, process plans. The output of the neural network was evaluated by an expert system, which then generated a schedule for the performance objective o f

19

minimizing tardiness. In their results, the BP network was able to predict correct results 90% of the time. Pierreval et al, (1992) used both neural networks and simulation for selecting dispatching rules dynamically in a m-' o-machine flow shop. In this study,//ve different dispatching rules and five different performance criteria were considered based on DR were considered. In this proposed hack propagated neural network, there were 4 input units representing mean arrival rate of jobs, mean expected processing time on each machine and processing time variance, and a total of 19 output units representing a combination of five dispatching rules on both the machines. A trial and error approach determined 16 hidden neurons for a single hidden layer. A total of 500 training sets were used for training the network. The trained network was capable of selecting the dispatching rules based on the performance criteria. In a comparison of results between neural network (NN) and simulation, the authors highlighted that NN had the advantage of computational time over simulation for decision making in real time scheduling and, also, memory required by NN was less as compared to the simulations using SIMAN IV and GPSS packages. In a comparison between NN and expert system, the expert system required expertise to develop larger knowledge bases, and this was difficult to obtain in the case of selection of scheduling heuristics. The learning capabilities of NN avoid these problems. During the literature review it has been observed that although neural networks were used in selection of dispatching rules in flow shops, there have been no investigations of their use in dynamic selection of combinations of dispatching rules in job shops.

20

Thus, in the current research an artificial neural network to select combinations of dispatching rules is investigated. The task o f the neural network is to pick an appropriate DR from a number o f alternatives, given an instantaneous environment in the job shop. (Pierreval, 1992) Also, it is desired to test the suggestion o f and Sabunguoglo, (1998) regarding the generalization property of back propagated networks for solving large size problems, having learned to solve small size problems.

21

CH APTER 3 Artificial Neural Networks

The objective of this chapter is to provide background information on artificial neural network, types of neural networks and a detailed description of a back propagated neural network (BPNN).

3.1 Introduction to Artificial Neural Networks (A N N )
A job shop problem can be solved by several methodologies including mathematical programming, simulation, priority/dispatching rules, expert system, artificial intelligence (Al) etc. An Artificial Neural Network (ANN), which is one of the Al techniques, is an information-processing paradigm. The inspiration of using neural networks lies in its ability to extract information from complex data, similar to the biological nervous systems, and how the brain processes information. The key element of this paradigm is the novel structure of the information processing system. It is composed of a large number of highly interconnected processing elements (neurons) working in harmony to solve specific problems. ANN systems, like people, learn by examples and dynamically modify themselves to fit the data presented.

3.1.1 Basic Concepts
The basic building block of any ANN is the neuron, which is the fundamental cell o f the brain or simply the processing unit of our brain. These neurons have three principal 22

components; dendrites, the cell body and an axon, which are presented in Fig 3.1. The dendrites are tree like receptive networks of nerve fibers that carry electrical signals into the cell body. The cell body effectively sums and thresholds these incoming signals. The axon is a single long fiber that carries the signal from the cell body out o f other neurons. The point o f contact between axon of one cell and a dendrite of another cell is called a synapse. Learning process in the brain occurs due to the strengthening and weakening of synapses (Reinhardt, 1990).

O

o

P ig 3.1 B asic fea tu res o f a sin gle b io lo g ica l n eu ron (b ased on G arson , 1998).

3 .1 .2 Architecture o f A N N
ANNs are composed o f basic units called processing elements (PE), which are also called as nodes or neurons. For example, in the back propagation (feed forward) network the PE are arranged in layers referred to as input, hidden and output layers. Each PE 23

receives an input

from every other neuron a, which is multiplied by corresponding

weights. The aggregate input signal to the PE. Net, is calculated as the sum of these weighted inputs. (Smith, 1999). The resulting signal is then passed through an activation function, which could be linear, logistic, step, ramp, sigmoidal or hyperbolic tangent depending on the problem to be solved. The output of the PE neuron is therefore 0^ - f (Net).

In simple terms, neurons multiply an input by a set of weights, then combine these weighted inputs into an internal activity level by adding them together. The resulting signal is then modified by the transfer function of the PE to produce the output. Figure 3.2 shows the structure of a PE. The power of neural computation comes from the immense number of interconnections between the PEs, which share the load of the overall processing task, and also from the adaptive nature of the parameters (weights) that interconnect the PEs. (Garson, 1998).

Summation

A ctivation

-EuQction

Output net

Weights bias Inputs

F ig 3.2 P rocessing elem ents (G arson, 1998).

24

3 .1 .3 C la ssifica tio n o f Neural N etw ork Architectures There are several neural network architectures, which are used not only for solving job shop scheduling, but also for several other applications. Figure.3.3 provides classifications within the field o f neural computing. Jain and Meeran (1998) have presented details about each o f the different tN^Des o f neural networks. There are several different neural architectural models, but most of these can be divided into two main categories viz. feed forward and feedback.

Neural Network Architectures

Supervised Training

Error Correction networks e.g Backpropagation

Multi-layer Perceptron Supervised Training Fixed Waght-Recurrent Networks Auto-Assodalive Memories

Protoabilistic networks e.g Simulated Annealing

Qassification and clustering Models Unsupervised Training

Searching networks e.g Hopfield

.

.

Unsupervised Training

Self-Organizing or oorrpeting networks

F ig .3 .3 C la ssifica tio n o f com m on n eural nerw'ork a rch itectu res (Jain an d M eera n , 1998).

3 .1 .4 Feed Forward Neural Network
Processing elements are usually organized in layers. These layers can be connected in a number o f different ways. A neural network with more than one layer is called a m ulti-layer neural network. Figure 3.4 shows the architecture o f a multi-layer neural network. As can be seen in this network, the output from each layer feeds the next layer 25

of units in a forward direction and there is no feedback connection between the layers, that is, the information is processed from left to right only. Garson (1998) describes this network as one that "has one or more inputs which are propagated through one or more number of hidden layers. Each layer contains a variable number of nodes, which finally reaches the output layer containing one or more output nodes".

Weights

Ouput Layer

Hidden Layer

input Layer

Fig 3.4 M ulti-layer feed forw ard neural netw ork (based on G arson , 1998).

3.1.5 Learning Rule
There are several learning rules, such as Hebb's rule, the Delta rule, Gradient Descent rule, etc. The choice of learning rule depends to some extent on the chosen architecture (Smith, 1999). For instance, in case of back propagation, the Delta rule is mostly used for generalization of errors. Learning rules can be divided into two main types: supervised and unsupervised learning (Smith, 1999).
26

a) Supe)-\>ised learning in simple tenns is defined as `learning with teacher' (Smith, 1999). In this case, the network is trained with the correct/desired responses for given inputs. During the learning process global infonnation may be required. Usually, superx'ised learning is done off-line i.e. the network can be trained separately and later on the trained network is used for obtaining solutions to new problems (Garson. 1998). b) Unsupervised learning. In this situation there is no external teacher used by the neural network for training. In other words, the desired output for the input is either unknown or does not exist, and learning is based on local information. Usually unsupervised learning is performed on-line (Smith, 1999).

3 .2 Neural N etw ork training by Back Propagation (B P N N )
Back propagation Neural Networks are a class of feed forward neural networks with supervised learning rules. In order to train a neural network to perform a certain task the weights o f each unit must be adjusted in such a way that the error between the desired output and the actual output is reduced. The actual response o f the network is subtracted from a target response to produce an error signal. The dérivâtes of the output error are passed back to the hidden layer using the original weighted connections. Each hidden node then calculates the weighted sum of the back-propagated errors to find its indirect contribution to the known output errors. After each output and hidden node finds its error values, the weights are adjusted to reduce the errors. This process requires that the neural network compute the error derivate o f the weights. The back propagation algorithm is the most widely used method for determining error dérivâtes o f the weights. For the present research different BPNN networks are built for different perform ance objectives. There are a few important parameters and 27

guidelines required while building the BPNN network, the details of which are provided below;

3.2.1 Scaling Function
Scaling function is used in the input layer to scale the data from their numeric range into a range that the network deals with efficiently. In case of supervised feed forward back propagation neural network the output patterns also require normalized or scaled value in the same range as of input. Common ranges for scaling are either 0 toi or -1 to 1. In this study both the input and output patterns for neural network training are scaled between 0 and 1(Neuroshell 2 manual; Swing!er, 1999).

3.2.2 Transfer Function
In simple terms, the transfer function, which is also called as transformation, squashing, activation or threshold is a mathematical formula that determines the output of a processing neuron. In most areas of research the Sigmoid or S-shaped function is more popular as compared to other functions such as hyperbolic, tangent, step, ramping, arc tan or linear. For sigmoid function, the output varies continuously but not linearly as the input changes. According to Swing]er (1999), the activation ifrnctions are "necessary to introduce non-linearity in the network. This non-linearity makes it possible to learn non linear functions". Sigmoid functions are more inclined to vary an output than threshold functions, because threshold functions usually are not sensitive to small changes in weights. For this reason, sigmoid functions are used in neuron activations for the proposed BPNN for job shop dispatching rules selection model (Garson, 1998).

28

3.2.3 D elta R ule
For a given input vector, the output vector is compared to the correct answer. If the difference is zero no learning takes place; otherwise, the weights are adjusted to reduce this difference (Garson, 1998). The change in weight w for output layer neuron a with respect to input layer neuron b is the learning rate r times the activation function (f) for neuron a times c, the correct desired output of neuron a: ^ ^ 'a b ^^fa b ^a (3.1)

is the difference between the expected and actual output. During this learning the delta weight as shown in the equation is added to the existing weight details about the rule can be obtained from (Garson, 1998). More

3 .2 .4 T yp e o f D atasets
It is important to define and discuss the various types and sets o f data. Data for neural networks are an input-output model. "Inputs are the presumed predictor variables, while outputs are the neural model's estimates of the dependent variable or variables" (Garson, 1998). In simple terms the data set is a set of examples for learning, which is nothing, but a "training set". The neural modeling software uses this set to compute model weights. Garson, (1998) defined the test data set as "the set of data to which the final neural model, and its associated weights, is applied for purposes of generalization." There is a distinction in types of data sets between test sets and validation sets. According to Garson (1998), a test set is a collection of data that is used for testing the performance measure without changing any of the parameters, while a validation data set is used to tune the parameters. However, Garson, (1998) also mentioned that the test data 29

sets are used for validation in the final stage of development of a neural model and thus, test data sets are sometime referred to as validation data sets. In this study only training and testing data sets are considered.

3.2.5 Learning Rate and Momentum
The learning rate (p) controls the magnitude of the adjustments to the network's weights in response to the error. Each time a pattern is presented to the network, the weights leading to an output node are modified so as to produce a smaller error the next time the same pattern is presented. The magnitude of the weight adjustments is

determined by the relationship: learning rate times the error. In most neural packages the learning rate is between 0.1 and 1. In Neuroshell 2 the default value is 0.1. A high learning rate might lead the local minimum to be overstepped constantly, causing oscillation from side to side, and never reaching convergence to the lower error state (Garson, 1998). One way to allow faster learning without oscillation is to make the weight change a function of the previous weight change in order to provide a smoothing effect. The momentum (a) factor determines the proportion of the last weight change that is added into the new weight change (Garson, 1998). The Momentum, which causes the weight changes to be affected by the size of previous weight changes, is used to avoid local minima. Typically a is selected in the range of 0 < « < 1 " (Swingler, 1999). The default momentum value in Neuroshell 2 is 0.1.

30

3 .2 .6 Initial W eights
Al] network weights must be set to initial values before training starts. If weights are too large then the network might become unstable and the nodes saturated, and if weights are too small then weight changes will be slow. The choice o f initial weights is dependent upon the problem and nonnalization o f variables (Swingler, 1999). Generally, weights are chosen randomly. The default initial weight value in Neuroshell 2 is 0.3.

3.2 .7 N eural N etw ork Training
Garson, (1998) defines training as, "the process of refining the weights in a neural model through a process in which training data set are fed into the model, analyzed and reprocessed through a number o f iterations." The objective o f training a neural network is to adjust the weights so that application o f a set of inputs produces the desired set of outputs. Training assumes that each input vector is associated with an output vector. In brief, training a feed-forward neural network with supervised learning consists o f the following procedure, adapted from Garson (1998). · Select a training pair from the training set and apply the input vector to the network input. · Calculate network output using a forward pass. Calculation of network output is accomplished by using a feed-forward process and application of an activation function for each layer in the network. · Compute the difference between network output and the desired target value from the training pair output value. · Change the network weights in a way that minimizes the error. 31

This chapter has provided a detailed description of BPKN along with different parameters to be considered while designing BPNN for solving the job shop problem for two different performance objectives. The next chapter describes building a BPNN for selecting appropriate dispatching rules in a 5-machine job shop.

32

CHAPTER 4 M ethodology and Design o f BPNN

This chapter introduces the methodology for constructing a back propagation neural network (BPNhJ) that selects a combination of dispatching rules to use in a job shop problem.

4.1 D esig n in g a B P N N for job shop scheduling
In order to build a BPNN, there are some common steps that should be followed for achieving desired objectives. Baily and Thompson, (1993) provided detailed guidance on how to build neural networks along with the design decisions to be considered for commonly used neural paradigms like BPNN, Boltzman machines, Hopfield network, etc. Likewise, Kaastra and Boyd, (1996) provided a step-by-step guide for building a BPNN forecasting model. Based upon both these references, Figure. 4.1 is constructed to highlight the main steps required in designing a BPNN. The procedure is not usually a single pass process and some of the steps like training and testing may need to be carried out several times. The material in this section is organized based on the design steps of Figure. 4.1. The detailed explanation is presented for a BPNN with regards to the performance criterion o f minimizing makespan. This same procedure can be used for other performance criteria, as will be illustrated for minimizing the mean flowtime in section 4.7 o f this chapter.

33

Analyze Problem

Acquire and Prepare Training Data

Design Architecture and Choose Learning Algorithm

Train Network

NO Trained Successfully? YES Test Network

NO Tested Successfully?

YES Implementation

F ig 4.1 D esign m ethodology for b uilding ANN .

34

4 .2 Problem A n alysis
The definition, nature and scope of the job shop problem along with the type o f neural network recommended for solving it have been discussed in chapter one. The case o f a 5-machine job shop is considered, although a similar approach can be used for job shops having different numbers of machines. The objective is to select for each machine, one o f three alternative dispatching rules so that a given performance criterion is optimized for the problem on hand.

4.3 Data A cqu isition and Preparation
In building an effective ANN, the researcher has to decide what kind o f data are to be used, nam ely either historical or constructed data. If historical data are being used, then they must be sorted or filtered from noisy data, if any. A format and range for each input and output are then selected and the data are expressed in a manner that can be presented to the BPNN. This usually takes form o f a data vector as follows: <lnput 1, input 2, input 3 , input Nj, output 1, output 2, output 3 output N q>

Where Nj and N q represents number of input and output nodes respectively. Before the data are collected, it is necessary to decide which of the job shop attributes will be used in the input layer and what output information is desired of the network.

4.3.1 Input V ariable Selection
Input variables must be selected very carefully, taking into account the given performance objective to be achieved. Here, the objective considered is to minimize 35

makespan, which mainly depends on the job's processing times {Pijf) and the routes ^Qik )· However, in a case where every job visits each machine once, the subscript j can be dropped and the processing time represented as (Pjjr.)- The number of units in the input vector is dictated by the specific representation adopted for the schedule of the job shop problem. In the proposed representation, it is desired to characterize the job shop in terms of machine loads, dispersion of machine processing times and mean routing order on each machine. This is achieved by a total of 15 input units for a 5-machine job shop. The information carried in each of these units is organized as follows: Input 1 Total processing time on machine =
1=1

1

....

(4.1)

n

Input 2; - Total processing time on machine 2

= %]Ti2
1=1

....

(4.2)

Input 3: - Total processing time on machine 3
1=1

....

(4.3)

Input 4: - Total processing time on machine 4

= /= i

....

(4.4)

Input 5: - Total processing time on machine 5

~
i= l

....

(4.5)

Input 6 : - Variance of processing time for machine

1

= -- ----------- ------- .... n(n - 1)

"Sfn-(Sfn)'

(4 .6 )

Input 7: - Variance of processing time for machine 2 = ---n(n - 1)

....

(4 .7 )

Input 8 : - Variance of processing time for m achines = -- ^ --------- id n (n -l)
36

"XPi3-(XPi3)'
(4

g'v

H

I) . 3

Input 9: - Variance o f processing time for machine 4

=

---------- .... n(n-])

(4.9't

Input 1 0 ; - Variance o f processing time for machine 5 = --^

^ ------« ( « - 1)     » i
....

(4

] 0" )

 ^

1

Input 11; - Mean routing order o f machine 1 =

= -^ Q .,

(4.11)

n
1

,= i

"
....
i= l

Input 12; - M ean routing order o f machine 2 = R ,^^2

n

(4.12)

Input 13: - Mean routing order o f machine 3 = R ,^=3

1 " = -^ Q jj
^
1=1

....

(4.13)

Input 14: - Mean routing order of machine 4 = R^ ^ 4

1

"
1=1

....

(4.14)

n

Input 15; - Mean routing order o f machine 5 = R ^=5

1 " = -^ Q j5 n i= l

....

(4.15)

4 .3 .2 C onstruction o f an Input V ector
A single input vector represents a job shop problem. This section gives detailed illustration o f how an input vector is computed for a randomly selected 10-job, 5-machine example. Table 4.1 depicts the raw data for this problem, identified as N10M265 - 10 denotes a 10-job problem; M represents the makespan criterion and 265 the problem number. (Mote this example problem is taken from a set o f 2494 random problems generated for training the network). The details of how each o f the input units is constructed is explained hereafter.

37

Job No 1 2 3 4 5 6 7 8 9 10 3 3 4 3 1 1 1 4 3 3 1 4 1 4 4 4 4 1 5 4

Routes 4 5 2 5 5 5 5 2 1 5 5 2 5 2 3 3 3 5 2 2 2 1 3 1 2 2 2 3 4 1

Processing time (f^^.) time units 47 47 46 43 77 95 64 41 52 46 67 44 85 31 24 17 48 66 64 52 43 72 84 55 55 78 53 66 95 64 74 74 69 51 40 52 33 53 60 53 57 60 30 85 88 63 49 37 21 85

T abic 4.1 Raw data o f sam ple problem N10M 265.

Input units 1 through 5 (Total processing times); The total processing time on a given machine is the summation of proeessing time requirements on that machine. This measure helps to identify whieh maehine is a bottleneek and to what degree. Total load on each of the machines is obtained by adding the proeessing time of all the jobs visiting that machine. The bottleneck machine is that which has the highest load. Applying equations 4.1 through 4.5 for the data o f Table 4.1 results in total machine loadings given in the third column in Table 4.2. Here, maehine 1 with a total load o f 780 (time) units, is the bottleneck machine.
Input unit 1 2 3 4 5 M achine 1 2 3 4 5 Total load 7 80 645 427 367 637 780 N o rm a lized value 1 0 .8 2 7 0.5 4 7 0.471 0 .8 17

Max V a lu e (H ) =

T able 4.2 D eterm ining inputs 1 to 5 for exam ple no. N M 10265.

The next step is to eonvert the machine loads in the range (0, 1). This is necessary beeause the input layer must have continuous values between
0

and

1

whenever the

sigmoid aetivation funetion is used. This ean be aehieved by ealeulating a relative pereentage for eaeh load with respect to the highest load. Thus, for a given range of data with maximum values= H, a speeific normalized value V is ealculated by V / H. The

38

normalized machine loads for the example job shop of Table column o f Table 4.2 Input units
6

4 .1

are given in the last

through 10 (Variance of processing time);

As different jobs have different processing times on a given machine, it is useful to have an indication o f the spread (dispersion) of these times about the mean. This is achieved by calculating variance^ which is a measure of this spread for each machine. The variances for the sample problem in Table 4.1 are computed by using equations 4.6 to 4.10. The data are formatted in the range 0 to 1 by dividing them by the maximum varianee am ong the five machines. Table 4.3 shows the variance for the five machines of the problem given in Table 4.1 and the normalized values used for input units 10 .
Input unit 6 7 8 9 10 M a ch in e 1 2 3 4 5 V ariance 16 9 .5 6 18 4 .2 8 5 7 .3 4 1 54 .23 8 7 .5 6 18 4 .2 8 N o r m a liz e d value 0 .9 2 1.00 0.311 0 .8 3 7 0 .4 7 5 6

through

M a x V alue =

T a b le 4.3 In p u ts 6 to 10 for exam p le p rob lem N 10M 265.

Input Units 11 through 15 (Mean order of routing (R^): The purpose o f this measure is to characterize the prevailing flow pattern, if any, due to the combination o f job routes for the problem to be scheduled. The mean routing order (Rj^) for each machine k is calculated as follows: -

(4.16)

Where,

= machine k's, order in job f's route.

A'=1,2,..,5

39

The minimum value for Qn^ is 1 and the maximum is 5 (because 5 machines are visited by each job). A lower value for g,* indicates that machine k is visited predominantly by jobs during the earlier stages in their routes, while higher values suggest that the machine is visited more towards the later operations o f the job's processing orders. Tlie mean routing order for five machines based on the problem of Table 4.1 is given in column 3 of Table 4.4
Input unit Machine 1 2 3 4 5 Mean routing order 2.7 4.2 2.7 2.2 3.2 4.2 N orm alized Value 0.54 0.84 0.54 0.4 4 0.64

]]
12 13 14 15

Max Value =

T a b l e 4 /4 E x a m p l e o f n o r m a l i z e d v a l u e f o r i n p u t s 11 t h r o u g h 1 5 f o r e x a m p l e p r o

From Table 4.4 it is seen, for example, that the mean routing order for machine 2 is 4.2. This indicates that machine 2 is predominately visited towards the end of the routes in most of the jobs. The mean routing orders are normalized between 0 and 1 by dividing Q jf^ for each machine by the number of machines (5 in this case). Once all the input data computations are done, the input vector can be prepared. The 15 input units representing the 10-job problem in Table 4.1 is constructed by using the normalized values from the last columns of Table 4.2, 4.3 and 4.4. This result is shown in Table 4.5.
Data Set N o. Input units value 1 1.0 Total Processing time 2 0.827 3 0 .5 47 4 0.471 5 0.8 17 6 0.92 Variance o f Processing time 7 1.0 8 0.311 9 0.837 10 0.47 5 11 0.5 4 Routing C om p lex ity V 12 0.8 4 13 0.54 14 0 .4 4 15 0.64

T a b l e 4 . 5 15 U n i t i n p u t v e c t o r r e p r e s e n t i n g j o b s h o p p r o b l e m o f T a b l e 4 . 1 .

The input vector in Table 4.5 can be interpreted to describe the job shop problem, for instance, as one where the machine
1

is a strong bottleneck. Machines 1 , 2 and 40

4

have

a wider range o f processing (imes as compared lo the other two machines; and finally machine
2

is visited last or near last by most o f the jobs.

4 .3 .3 O utput Layer
For solving the makespan minimization problem the three dispatching rules SPT, LPT and MWKR are considered. Past research by Kumar & Srinivsan, (1996) and by Subramaniam et al, (2000) has showed rules SPT, LPT and MWKR are among a few rules effective in minimizing makespan. The neural network's task is to select one o f these dispatching rules for each o f the five machines. Therefore, the output layer has been designed with 15 units (3 rules x 5 machines). Each unit represents a dispatching rule for a machine. The higher the value o f an output unit, the higher is the desirability o f using the dispatching rule associated with that unit on the corresponding machine.

4 .3 .4 D ata C ollection
Now that the input units and the desired output have been defined, the next step is to collect data for training and testing the network. A total o f 7500 job shop problems were generated randomly using n=10, 15 and 20 jobs in equal quantities. A C++ program (see Appendix A l) is used for this purpose. The processing times u%ed for the problems are uniform ly distributed between U (10,99) integer time units. The number o f different job routes in each problem is between 5 and 11, selected randomly from the 120 (or 5!) possible routes in a 5-machine job shop. This range for the routes is used to ensure that several jobs have identical routes in order to simulate more realistic situations. It is

41

unlikely in a real world application, particularly within group technology settings, for most if not all jobs to have different routes from one another. The optimal combination of dispatching rules that minimizes makespan for a given
5 -machine job

shop problem is found by enumerating all possible combinations, a total of

243 possibilities of dispatching rules (3^ combinations) for the three dispatching rules SPT, LPT and MWKR. The neural output assumes values between 0 and 1.0; the maximum value " 1 .0 " suggests undisputed preference for the specific dispatching rule represented by the unit. As the output value reduces, the preference for the corresponding dispatching rule diminishes proportionately. In supervised learning, desired (or target) outputs are needed in training the network. The target outputs for the proposed network are extracted from the optimal rule combinations in the following fashion, using data from Table 4.1 to illustrate the procedure. The optimal selection of dispatching rules for the problem N10M265 gives a makespan of 826. Two different optimal combinations of the three rules exist for this problem as shown in Table 4.6
M/C 1 MWKR MWKR M/C 2 SPT M W KR M /C 3 MWKR MWKR M/C 4 LPT LPT M/C 5 LPT LPT

Tab le 4.6 O ptim al rule com binations for exam ple problem N 10M 265,

In order to represent DR combination, the data of Table 4.6 are presented in a numerical form suitable for the output vector. The number of times a rule appears in an optimal combination is computed in each machine. For example, in machine 2 (Table 4.6) both SPT and MWKR (but not LPT) appear once in the optimal result. A similar calculation for the other machines is done and the final counts are given in Table 4.7.

42

DR SPT LPT MWKR

M .C 1 0 0 2

M /C 2 1 0 1

M /C 3 0 0 2

M /C 4 0 2 0

M /C 5 0 2 0

T a b le 4 .7 N u m erica rep resen tation for o p tim a l rule co m b in a tio n o f T ab le 4.6.

In order lo normalize the outpui results of Table 4.7 between 0 and 1, the entries in Table 4.7 are divided by the total number of optimal results (2) for the problem. The resulting output (o/p) vector is shown in Table 4.8, which represents the target output pattern that would be associated for training purposes with the input vector of Table 4.5.
M a ch in e] SPT 0 .000 LPT 0 .0 0 0 MWKR 1.000 SPT 0 .5 0 M a ch in e2 LPT 0 .0 0 MWKR 0 .5 0 SPT 0 .0 0 M a ch in e] LPT 0 .0 0 MWKR 1.00 SPT 0 .000 M a c h in e 4 LPT 1.000 MWKR 0 .0 0 0 SPT 0.0 0 M ach in es LPT 1 .0 0 0 MWKR 0.0 0

As can be seen from the output vector above, a minimum makespan can be obtained by using MWKR on machines 1 and 3; LPT on machine 4 and 5; and either SPT or M W KR with equal favor on machine 2. Table 4.9 shows a sample o f several pairs o f training patterns generated from the training problems where num ber of jobs (n) is
10.

The optimal combinations o f dispatching rule in each problem are used for training. In order to com pute the neural input and output (optimal combinations o f dispatching rule for each problem) vectors are computed by using a program in C++ (see Appendix A2). The back propagation neural network model proposed for selecting one of the three DRs for each o f the m achines in a 5 -machine job shop appears as shown in Figure 4.2.

4 .3 .5 V alidation o f Training data set output
In order to validate the makespan results obtained from the C++ program o f Appendix A2, a simulation model o f a 5 -machine job shop was developed with a student

43

Error Back propagation

Output Comparision

Information Propagation
Input 1 Hidden Layer

/

Input 2
S P T on M /C 1

O u tp u t 1

M W K R on M /C 5

Output 15 Input 15 Input Layer
W e ig h ts

Ouput Layer

F ig. 4.2 S tru ctu re o f A N N p rop osed fo r the 5-m ach in e job shop.

44

Data Set Numbers N10M 30I N10M 302 N10M 303 N10M 304 N10M 305 N10M 306 N10M 307 N10M 308 N10M 309 N10M 310 N10M 311 N10M 312 N10M 313 N 10M 3I4 Input 1 0 .5 0 0 .9 2 0 .7 5 0.71 1.00 0 .5 3 0 .4 8 0 .8 7 OTW 1.00

Total P roce ssin g time Input 2 Input 3 0.71 1.00 0 .3 5 0 .5 4 0 .8 0 Input 4 Input 5 1.00 Input 6 0 .7 5 0 .3 2 0 .6 4 0 .7 0 0 .7 4 0 .4 3 0 .7 6 0 .4 8

V ariance o f P ro ce ssin g time Input 7 0 .98 1.00 1.00 0.7 9 Input 8 0.51 0 .6 7 Input 9 1.00 Input 10 0 .4 6

M e a n R o u tin g order Input 11 Input 1 I 0 .4 6 0.4 8 0 .6 0 0 .7 2 Input 13 0 .7 6 0 .4 6 0 .5 0 0 .5 0 0 .6 0 0.6 2 0 .5 2 Input 14 0 .6 0 0 .8 0 0 .5 8 0 .7 6 0 .7 6 0 .6 6 Input 15 0 .5 8 0 .5 4 0 .8 8 0 .7 2 0 .7 2 0 .5 6 0 .8 0 0.7 2 0 .5 6 0 .5 4 0 .6 4 0 .5 6 0 .6 6 0.58

&87
1.00 0 .5 7

0.93 0.90
0 .6 7 0 .9 4 1.00 0.84 0.9 8 0 .5 5

&88
0 .7 2 1.00 0 .8 2 0 .4 8 0 .5 3 0 .4 0 0 .6 3 0.91

0.55
0 .5 4

0.59
0 .4 2

0.58
1.00 1.00 0.2 2

&68
0 .3 6

0.89
0 .6 0 1.00 0 .4 4 0 .5 8

&88
0 .7 7 0 .3 7 1.00 1.00 1.00 0 .5 2 0.71 0 .9 6 OTW 0 .7 7

&88 0J8
1.00 0 .8 7 0 .7 4

0.63
0 .9 7 OTW 1.00 1.00 0.81

0.97
0 .4 7 0.7 7

0J2
0.5 2

0J6 0.66 0.60
0 .6 4

0.69
0 .7 7 0.61 0 .6 8

&88
0 .7 6 0 .2 0 1.00

0J9 0 63 &84
1.00 0 .7 0

0 48
0 .4 6

0.88 0.60
0.68 0.7 2 0 .6 0 0 .5 6 0.5 4 0 .7 6

0J2
0 .4 4 0 .5 4 0 .7 4 0 .4 4 0 .6 0 0 .5 4 0 .6 8

0J8
0 .5 0 0 .3 8 0 .6 4 0 .7 6 0 .6 0 0 .6 8

&84
0 .4 7 1.00 0 .5 5 1.00 OTW

&72
0.6 2 0.6 8

0.52
OT^ 0 .8 0 0.7 5

0.66 &78
0 .7 5

0 63
1.00 0.9 4 0 .5 7 0 .9 6

0.95
1.00

0.63 0.93
1.00 1.00

0.48
0.1 5 0.85

&69
0.51 0 .7 2

0.95 &66
M a c h in e 1

0.79
0.61

&88
1.00

0 52 &66
0 .3 0

T a r g e t O u t p u t P a tte r n s
Data Set Number N10M 301 N10M 302 N10M 303 N10M 304 N10M 305 N10M 306 N10M 307 N10M 308 N10M 309 N 10M 3I0 N10M 311 N10M 312 N10M 313 N10M 314 o/p 1 0 .0 0 0 0 .0 0 0 0 .3 30 1.000 0 .0 0 0 0.500 0.310 1.000 0.370 0.330 0 .3 3 0 0 .48 0 0.500 M a ch in c2 o/p 3 1.000 0 .0 0 0 0 .3 3 0 0 .0 0 0 1.000 0 .4 2 0 0 .3 1 0 0 .0 0 0 o/p 4 0 .0 0 0 0.000 0 .0 0 0 0 .3 3 0 0.350 0 .3 3 0 0 .0 8 0 0.500 o/p 5 1.000 1.0 0 0 0 .8 6 0 0 .3 3 0 0 .3 0 0 0 .3 3 0 0 .4 6 0 0 .0 0 0 0 .3 2 0 0 .3 3 0 0 .4 3 0 0 .5 5 0 0 .0 0 0 o/p 6 0 .0 0 0 0 .0 0 0 0 .1 4 0 o/p 7 0 .0 0 0 0 .0 0 0 1.000 0 .4 0 0 0 .3 0 0 0.420 0 .4 6 0 0 .0 0 0 0 .0 0 0 0 .0 0 0 0 .4 3 0 M ach in e3 o/p 8 0 .0 0 0 1.000 0.000 0 .2 0 0 0 .2 5 0 0 .0 0 0 0.000 0 .5 0 0 0 .0 0 0 0 .3 3 0 0 .2 9 0 0 .1 0 0 0 .1 7 0 0 .0 0 0 o/p 9 1.000 0 .0 0 0 0 .0 0 0 0 .4 0 0 0 .4 5 0 0 .5 8 0 0 .5 4 0 0 .5 0 0 1.000 o/p 10 1.000 0 .0 0 0 0 .4 3 0 0 .0 0 0 0 .0 0 0 0 .0 0 0 0 .0 0 0 1.000 0 .5 5 0 0 .3 3 0 0 .0 0 0 0 .5 2 0 0 .4 2 0 0 .0 0 0 M a ch in c4 o/p 11 0 .0 0 0 0 .5 0 0 0 .2 9 0 0 .0 0 0 0 .0 0 0 0 .4 2 0 0.000 0 .0 0 0 0 .1 8 0 0 .3 3 0 0 .0 0 0 0 .1 3 0 0 .1 7 0 o/p 12 0 .0 0 0 0 .5 0 0 0 .2 9 0 1.000 1.000 0 .5 8 0 1.000 0 .0 0 0 0 .2 6 0 0 .3 3 0 1.0 0 0 0 .3 5 0 0 .4 2 0 1.000 o/p 13 0.000 0 .00 0 0 .0 0 0 0.000 0 .1 5 0 0.330 0.330 0 .3 3 0 0 .5 0 0 0.000 0 .3 3 0 0.190 0.000 0 .0 0 0 M acliin c5 o/p 14 0 .00 0 1.000 o/p 15 1.000 0 .0 0 0 0 .5 7 0 0 .4 0 0 0.400 0 .3 3 0 0 .33 0 0 .3 3 0 0 .3 4 0 1.000 0.330 o/p 2 0 .0 0 0 1.000 0 .3 3 0 0 .0 0 0 0 .0 0 0 0 .0 8 0 0 .3 8 0 0 .0 0 0 0 .3 7 0 0 .0 0 0 0 .3 3 0

0.430
0 .60 0 0.4 50 0 .3 3 0 0 .3 3 0 0 .3 30 0 .16 0 0 .0 0 0 0 .3 30

&330
0 .3 5 0 0 .3 3 0 0 .4 6 0 0 .5 0 0 0 .4 2 0 0 .3 3 0 0 .4 3 0 0 .1 3 0 0 .0 0 0

&260
0.670 0 .3 3 0 0.260 0.250

&260
0 .3 3 0 0.140 0 .3 2 0 1.000

0.670
0 .2 9 0 0 .4 8 0 0 .6 7 0 1.000

0.260
0 .2 5 0

0.420
0 .1 7 0 0 .0 0 0

0 .3 3 0 0 .3 3 0 0 .0 0 0 0.330 1.000 0 .0 0 0 T ab e 4.9 S am p le o f inp u t and output pattern pairs.

0.000

0.000 0.000 0.000

0.810 1.000 1.000

45

version o f the simulation software package Arena (Kelton et al, 2002). This model is given in Appendix B. Random ly selected problems from the training (data) set were tested using this Arena job shop model for all the combinations of the three dispatching rules on the 5 machines. It was seen that the simulation output from the Arena simulations matched the results from the C++ program. Table 4.10 presents the different dispatching rules tested by the arena model for the performance objective o f minimizing the makespan. Arena 5-machine Job shop simulation Model
D i s p a t c h i n g rules SPT LPT Q u eu e R an k in g R ule Least V a lu e First H ig h V a lu e First E xp ression /A ttrib ute P r o c e ssin g lim e (Pjjj^) P r o c e ssin g lim e (Pjjj^) R e m a in in g tim e MWKR H ig h V a lu e First

II f y.r-s
1=1

\ /

\j.k=\

^ ( ^ i . i k ) ~ (^Ki-i)k)

Table 4.10 A ttrib u te s used in jo b shop A rena m odel.

4 .3 .6 D ata sorting
As seen in Table 4.9, several of the output patterns show that more than one dispatching rule is favored on one or more o f the machines. This arises when there exists more than one combination o f dispatching rules that produce the lowest makespan. For example, in data set N10M 306, in Table 4.9, the three output units for machine 2 show equal values of 0.330. This indicates that all three dispatching rules appear with equal frequencies in the multiple combinations that yield minimum makespan for this problem. Using data from a problem such as this to train a network is not helpful because it introduces conflicting information and prevents the network from detecting the useful relationships between input data and optimal outputs. Therefore, if the network is trained without sorting and screening the input data, then chances are that the

46

network will not learn properly and may not select the best dispatching rules. The sorting is done conveniently, using Structure Query Language (SQL).

SQL has the capabilities to sort the data considering multiple criteria at a time. In this case there are 3 outputs for each machine. There are two different sorting criteria considered; A sorting index of 5 identifies the problem whose output units for all the five machines is either (1,0,0), (0,1,0,) or (0,0,1). For example referring to the problem number N10M301 of Table 4.9 the output value of each machine satisfies the sorting criteria for a sorting index of 5. A sorting index of 4 identifies the problem whose output units for any four out of the five machines is either (1,0,0), (0,1,0,) or (0,0,1). For the remaining machine the value of the output unit is treated as zero while sorting; resulting in the sum of sorting index as 4. For example, for the problem number N101V1302 of Table 4.9 the sorting index is 4 because four out of the five machines satisfy the sorting criteria. Likewise the patterns can be sorted for sorting indices of 3, 2 ,
1

and 0 . The data is sorted in the descending order having the maximum sorting index (5 in this

case). The sorting procedure is illustrated in Table 4.1 1 , after the same patterns of Table 4.9 are sorted. The first column in the target output patterns of Table 4.1 1 lists the sorting index for each problem. The number of outputs (equivalent to the sorting index) satisfied by the respective problems is highlighted in Table 4.1 1 . For all of the 7500 training problems (as explained in section 4.3.4) data are sorted simultaneously. For training the network it is not advisable to use only data whose sorting index is 5. This is because many problems exhibit multiple optimal combinations, and to help the network deal with such cases it is a good idea to incorporate some data having characteristics where more

47

Data Set Numbers N 1 0M 30 1 N 10M 302 N10M 314 N10M 304 N 10M 305 N10M 308 N10M 313 N10M 303 N10M 307 N10M 309 N10M 310 N10M 311 N10M 306 N10M 312 Input 1 0,50

Total Processing time Input 2 0.71 1.00 0.35 0.5 4 Input 3 0.80 OTD 1.00 0.57 Input 4 0.93 Input 5 1.00 Input 6 0.75

Variance o f P rocessing time Input 7 0.98 1.00 1.00 Input 8 0.51 Input 9 1.00 Input 10 0.46 Input 11 0.4 6 0.48 0.6 8 0 .3 6 0.32 0.52

Mean Routing order Input 12 0.6 0 0.72 0 .3 6 0.6 6 Input 13 0.7 6 0.4 6 0.50 0 .50 0 .6 0 Input 14 0 .6 0 Input 15 0.5 8 0.54

0.92 0 75
0.71 1.00

0.90
0.6 7 0.8 6

0.88
0.7 2 1.00

032 044
0.7 0 0.74 0.43

047
0.58 1.00 1.00

045
0.5 4

0.59
0.42

040 04 8
0.7 6 0.7 6

039
0.97 047 0.7 7

0.53 0.48
OTD 0.8 4 1.00 0.52

0.88 0J8
1.00 0.8 7 0.74 0.6 6 0.78 0.75

0.63
0.9 7 OTW 1.00 1.00 0.81

0.94
1.00 OTW

0.82 048
0.53 0.4 0

049 040
1.00 0.44

048
0.77 0.3 7 1.00 1.00 1.00

048 0.72
0.72 0.5 6

0.60
0.64

0,22
0.39

0 98
0.55

043
0.91

0J6 048 048
0.76

0.48
0.4 6 0.7 2

048
0.6 0

0 42 04 2
0.78 0 .5 0

046
0.3 2 0.44

0.80
0.72 0.5 6 0.5 4 0.64 0.56

049
0.77 0.61 0.68 0.48 0.15 0.85

0.63 0.84
1.00 0.70

048 044
0.4 7 1.00

048
0.72 0 .60 0.5 6 0.54 0.76

044
0.74 0.44

0.63
1.00

042
0.71

04 2
0.68 0.52 0.66 1 0.30

048
0.64 0.76 0.6 0 0.68

0.95
1.00

0 66 &80
0.75

0 94
0.57

043 043
1.00 1.00

030
1.00

04 9
0.51 0.72

045
1.00 0.84

046
0.64 0.77

040
0.5 4

0.95
0.66

0J9 0.61
Machine!

048
1.00

0.96

0.68

04 6 048
Machine5

Target O utput Patterns
Sorting Index Data Set Number N10M 30I N I0M 302 N 10M 3I4 N10M 304 N I0M 305 N I0M 308 N10M 3I3 N10M 303 N I0M 307 N10M 309 N10M 310 N10M 31I N 10M 306 N10M 312 0.500 0.330 0.310 0.370 0.330 0.330 0.500 0.480 0.250 o/p 1 M achine] o/p 3 o/p 4 o/p 5 o/p 6 M achine] Machinc4 o/p II o/p li o/p 13 o/p 2 o/p 14 p/p I

5
4 4

1 .006;
0.000 0.000 0.000

0.000 0.000
0 .3 3 0 0 .3 3 0 0 .3 5 0 0 .5 0 0 0 .33 0 0 .3 3 0 0.300 0 .00 0

0.000 0.000
0.330 0.330 0.350 0.500

0.000 0.000
0.000
0.400 0.300

^4
0.000
0.5 00

0.000
0.5 00

0.000 0.000

0.000
0.000

0.000 0.000 0.000
0 .1 70 0.420

0.000
0.000
0.1 50 0.3 30

n.noo
0 .6 00 0 .4 5 0 0J30

r'-T.OOO
0,40(1 0.400 0.3 30 0 .5 70 0 .3 30 0.3 40 0.3 3 0 0 .3 3 0 0 .8 1 0

2 2

2
2
1

0.000
0.170

###(
0 .0 0 0 0 .0 8 0 0 .2 6 0 0.3 3 0 0 .1 4 0 0 .3 3 0 0 .3 2 0

0.000
0.860 0 .4 60 0.320 0 .3 30 0.430 0 .33 0 0.55 0

0.000
0.140 0.460 0.420 0.330 0.430 0.330 0.130

# #
0.460

#

1 1 1 1

0.000 0.000
0.000
0.330 0.2 90 0 .6 70 0 .2 9 0 0 .5 80 0.4 80

0 .2 9 0

0.000

BET

0 .2 90

0.000 0.000
0.3 30 0.500

0.000
0.430 0 .3 30 0.1 60

0.000

0 .2 60 0.3 30 0.4 20 0.580 0 .3 50

0.000
0.430 0.420 0.420

0.000
0.3 30 0.3 30 0.1 90

0.000
0J30 0.330

0
0

0.000
0.100

0.000

Table 4.11 Input and output patterns o f Table 4.9 after sorting.

48

than one dispatching rules is favored by one of the machines. This will help the network to leam the stronger relationships between input and output patterns. On the other hand, using patterns with lower sorting indices could hamper the learning. Therefore, it was decided to incorporate only those patterns with sorting indices 4 and above for the training. Thus, ignoring all the data having a sorting index of less than 4, the result is a 2494 training data set extracted from the 7500 original problems for training the neural network.

4.4 D esign o f Neural Network
Once the pre-processing (sorting) of the data is done, the next step is to train the neural network. However, the final number of units for the hidden layer still needs to be decided.

4.4.1 Hidden Layer
The hidden layer is mainly required to overcome the non-linear learning problem. It is also where the network learns interdependencies in the model. Figure 4.3 provides some detail into what goes on inside a hidden node. Fleurons of the hidden layer receive inputs from neurons of the input layer, depending on the sum of the input weights that produce an output. The output is then further transferred to units of the output layer. Garson, (1998) states that "if the problem has a linear solution then it may not be appropriate to use hidden layers. In theory, a neural network with at least one layer and adequate number o f hidden units is capable of solving most problems. However, in practice one or more layers may be used depending on the complexity o f a problem (such
49

F (I) = X T W 1 + X 2 * W 2 - *

-tX n 'W n

F '( l ) = n o n l i n e a r ( u n c t i o n o t F ( l )

W n

W1

W2 W3

W4

Xn

X2

X4

Fig. 4.3 Details of a hidden node

as complicated function fitting problems) and cost and resources. As the number of hidden layers increase, the meaningfulness of the back propagated error term decreases. M oreover, the training time increases by an order of the magnitude for each additional hidden layer." Thus, considering all the above-mentioned factors, only one hidden layer is considered for building the neural network.

4 .4 .2 H idden U nits (neurons)
In most situations, there is no magic fonnula that finds the best number of hidden units without training several networks and estimating the generalization error o f each. If only a few hidden units are considered, then the network may not train well because the model lacks sufficient complexity to reflect input-output patterns in the training set. On the other hand, if too many hidden units are used, then the network may overtrain and generalization will suffer as the network simply memorizes the input-output patterns in the training set. Some researchers have developed a heuristic approach to determine the best num ber o f hidden neurons, (Smith, 1999). Two examples of these are the following. 50

If the input patterns are o f dimension N and the output o f K neurons, then the number

of hidden layers would be J = JN xK neurons.

·

Another approximation \sJ = ^ ( N + K) + -Ip . where P is the number of patterns in the training set. Tins is the default formula used in the commercial neural network package Neuroshell 2.

·

On the other hand (Baily and Thompson, (1990)) suggest that the number of hidden neurons in a three layer neural network, which include input, output and hidden layers should be 75% of the number of neurons in the input layer. Most of the above heuristics are based on the assumptions that the training set is at

least twice as large as the number of weights and preferably at least four or more times larger than the number of weights. If the same is not the case, then the number of hidden neurons will be affected by the number of training sets. Moreover, there are some other factors like the amount of noise in the test patterns, the complexity of the function or classification to be learned, the architecture, etc. that will also have impact on the number of hidden neurons. A trial and error approach is often taken starting with a modest number of hidden neurons and gradually increasing this number until the network fails to reduce its error Kaastra and Boyd, (1996) and Garson, (1999). This last approach is the one adopted in this study for determining the number of units.

4.4.3 Training Stopping Criteria
A neural network is trained in epochs, where each epoch represents a complete pass o f the training set through the network. When each training pattern is presented to the
51

network, the error between the actual outputs in the training pattern and the network's predictions for eacii o f the network's outputs is computed. The total error for each pattern is the sum o f the squares of the differences. At the end of each epoch the average error over all training patterns is computed. As the epochs progress and the training continues, the network learning improves, i.e.. the enor on the training and test sets decreases. For the test set, however, there comes a point where the error in the test examples starts to increase with additional training. While training a neural network, one expects to obtain a network with optima] generalization performance. Thus, the question of when to end the training is a critical one. There are different choices for stopping criteria for both training and testing sets adopted from Neuroshell 2, which are based on:
·

The average error in the training set. End training when the lowestvalue for the
average error in the training set is reached.

·

The largest average error in the training set. This is the network's latest computation
for the difference between the network's predictions and the actual predictions for data in the training set.

·

The events since the minimum average error in the testing set. An event is the
presentation o f a single training pattern to the neural network.

Additionally, Neuroshell 2 offers two options about automatic saving o f the training based on:
·

Best training set saves the network every time it reaches a new minimum average
error for the training set.

·

Best testing set saves the network every time it reaches a new minimum average
error for the test set. 52

4.4.4 Training Procedure
The following steps are used in order to achieve a generalization capability in the performance of a trained neural network. · Step J: Train the network with the training patterns and monitor the minimum average error. Once a new minimum average error is observed, then interrupt the training. Save the neural weights and go to step 2. · Step 2: Apply the partially trained network to the test set and compute the performance measure (using the C++ program of Appendix A3). If the performance measure (either total makespan or total mean flowtime) is improved upon from the previous trial, then return to step 1. Otherwise proceed to step 3. · Step 3: Save the current weights as a final trained network.

4.4.5 To Find optimal number o f Hidden neurons (units)
In order to find the best number of hidden neurons, a trial and error approach is carried out involving 6 to 18 hidden neurons using the above-mentioned training stopping criteria. The smallest number represents too few and the highest number too many hidden neurons. The training trials were done using the commercial software "Neuroshell 2", and with the parameters listed in Appendix C l. A new set of 1200 randomly generated test problems (having random seeds different than the training data set) were used. The original 2494 training patterns (as explained in the section 4.3.6) are considered for training. The objective is to find the size of the hidden layer that minimizes the obtained makespan for the test set (refer step the training procedure). 53
2

of

Once the training is completed the test job shop problems are run (using the C++ program o f Appendix A3) to see how accurately the neural network has assigned the priority rules. The neural network's final performance for each trial with a different number o f hidden neurons is tabulated in Table 4.12, which expresses the BPNN results in terms o f the total makespans of the test set problems. This is further illustrated graphically in Figure 4.4.
E xp erim en t N um ber 1 2 3 4 5 6 N um ber o f H id d en N e u r o n s 6 8 9 10 15 18 Total M a k esp a n 1398179 1394552 1393378 1395058 1397199 1397533 M in im u m A v e r a g e Error 1.4 47 1.4 56 1 .4 3 7 1.439 1.457 1.482

T a b ic 4 .1 2 T otal m ak esp an u sin g B PN N w ith v ariou s n u m b er o f h idden n eu ron s.
1.401
I  Optimal NN Results  Mnimum A v e ra g e error ;

r

1.485 1.48 1.475 2 W 1.47 0) O) 1.465 2 1 < 1.455 E 3 1.45 E 1.446 c ii 1.44
1 .435

1.398 -

o

c
1.395 - I

Q.
ns

1.46

S
No. of hidden N eu ron s

1.392




10
12
14

16

18

20

F ig .4 .4 E ffect o f n u m b er o f h id d en n eu ron on the m in im um average error for m ak esp a n .

When the number o f hidden neurons is small, the corresponding minimum average error (see section 4.4.3) is also low. However, the BPNN results are not the best. Figure 4.3 shows that the BPNN performs best, and the minimum average error is lowest when the number o f hidden neurons is 9. With a higher number of hidden neurons, the network appears to memorize rather than leam the patterns. So, the chosen network has a 15-9-15 structure. The next section explains in greater detail the training of the 15-9-15 network. 54

It is noteworthy that the same training process, training data and parameters described in the next section were also used in the trials to determine the number o f hidden neurons, presented in Table 4.12.

4.5 Training the BPNN
A total of 2494 training patterns extracted from problem instances combining n=10, 15, 20 for the five machines is considered for training the network. Similarly, a test data set of
1200

problems with the same combinations of n as the training set is generated

separately. As explained in the preceding chapter, the default values of Neuroshell 2 for training are a learning rate = 0.1, momentum = 0.1 and initial weights = 0.3. The training was performed on AMD 900MHz Presario personal computer. The training is carried out and the total of the makespans for the test set is recorded for every epoch that produces further reduction in the minimum average error (see section 4.4.3). The training results are tabulated as shown in Table 4.13. The training procedure is extended from generalization test number 9 to 16 (refer to the first column of Table 4.13) in order to observe the behavior of the neural network on the test set, if the training is continued beyond a certain point. In other words, to observe what happens when the neural network is over trained. Figure 4.5 presents the behavior of the minimum average error on the training set and the corresponding neural network results on the test set. It can be seen that as the minimum average error is reduced, the BPNN results improve correspondingly up to a certain point (generalization test number
8

in this case), beyond which the results start

55

deteriorating. It is at this point that the network generalizes best, and no further improvement is possible with continued training.
G e n eralization M in im u m Test A v e r a g e Error ] 2 3 4 5 6 7 8 9 10 ]] 12 13 14 15 16 1 .5 5 1 7 1 .5 2 9 5 1 .4 9 3 8 1 .4 9 0 6 1 .4 7 6 0 1.4711 1 .4 5 5 8 1 .4 3 7 2 1.4358 1.4336 1 .4 2 3 8 1 .4 1 9 6 1.4051 1 .3 8 9 0 1 .3 8 5 7 1.3776 Learning Event 4988 1 24 70 29928 34916 44892 52374 57362 92278 1 14 72 4 124700 187050 199520 369112 533716 1015058 1601148 T im e (S e c ) 3 K 17 19 25 30 33 52 65 70 105 113 2 13 315 58 3 1 00 2 BPNN S u g g e ste d DR 1449074 1439644 1414140 1410225 1409133 14 00621 1403802 1393378 1398482 1399688 1398570 1400944 1401905 1405274 1400672 1402333

Epoch 2 5 12 14 18 21 23 38 46 50 75 80 148 214 407 642

1

T a b le 4.13 T ra in in g and gen era liza tio n results for m in im izin g m a k esp a n .

4 .6 Im plem entation
Once the training o f the proposed network is completed, the next step is to see how well the neural network performs in selecting dispatching rules. In order to explain how the neural network assigns a dispatching rule to each machine, an example o f 20 jobs on 5 machines is given in Table 4.14. The above example data are converted to a neural network input representation by following the procedure explained in section 4.3.2. The resulting 1f-unit input vector is given in Table 4.15.

56

1 .4 6 -,
 "BPNN results"  "Min, Avg. error'

T

1.60j

1 .4 5 -

- 1.55:
1 .4 4 -

Î S 1 .4 3 -

O )

1.41
- 1.40' !

Generalizition test
1.39

Fig. 4.5 BPNN generalization V/S m inimum average error for m akespan.

Job no 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2 2 5 5 5 5 5 5 1 5 2 5 3 2 5 2 3 1 1 2 1 1 4 2 4 2 2 4 4 2 1 2 1 1 2 1 1 4 4 1

Routes 4 5 3 4 3 4 4 3 3 4 4 4 4 4 4 5 4 3 3 4 3 3 1 3 1 3 3 1 5 3 3 3 2 3 3 3 2 5 5 3 5 4 2 1 2 1 1 2 2 1 5 1 5 5 1 4 5 2 2 5 52 58 51 46 46 37 53 28 68 22 61 26 41 36 40 61 55 65 42 51 60 41 77 34 92 66 68 84 87 53 67 57 68 37 66 40 61 84 82 45

Processing time 94 20 33 92 58 61 90 45 42 92 64 63 65 95 67 15 91 52 26 80 33 34 74 62 63 29 34 70 49 46 24 32 67 47 30 58 59 38 16 45 23 57 37 53 35 55 63 46 32 50 53 76 38 45 71 69 14 67 32 51

T able 4.14 A 20-job exam ple problem .

57

Set N o . Input N20E1 1 0 .7 4

'] otal P roc essin g tim e 2 0 .6 5 3 0 .5 2 4 1.0 5 0 .45 6

Variance o f P rocessing time 7 0.91 8 0 .6 9 9 0 .8 6 10 1 11 0 .6 1 0 .7 9 2

M e an routing order 12 0 .5 6 13 0 .6 8 14 0 .5 8 15 0 .5 7

T a b ic 4 .1 5 In p u t vector for exam p le p rob lem o f T able 4.14.

A trained network generates output in response to the data vector presented to it at the input layer. Wlien the input vector of Table 4.15 is introduced to the BPNN, which has been ju st been trained for the minimization of makespan. a feed forward set of com putations produces the output vector given in Table 4.16.

M a ch in e 1 Dutput 1 SPT BPNN 2 LPT 3 MWKR 0.449 4 SPT 0 .4 6 2

M a c h in e 2 5 LPT 0 .1 1 7
6

M acliineS 7 SP T 0J96
8

M a c li in e 4 10 SPT 11 LPT 12 MWKR 0 .6 9 4 13 SPT 0 .5 0 7

M a c h in e s 14 L PT 0 .2 2 8 15 MWKR 0 .2 2 9

9 MWKR

MWKR 0.411

LPT

0 .332 0 .2 0 6

0 . 3 9 4 I 0 .2 6 6

0 259 0 .0 6 2

The maximum value in each set of 3 units associated with each machine is identified. The dispatching rule corresponding to those units (highlighted in the Table 4.16) is then assigned to the machines. In the above example, the neural network's choice for the first machine is MWKR; the second machine is SPT; the third machine is LPT; the fourth m achine is MWKR; and the last machine is SPT. Applying this allocation o f dispatching rules, the makespan for the problem is found to be 1652. This compares to a minimum makespan o f 1628 for the optimal rule combination o f SPT-SPT-LPT-MWKRSPT applied on machines 1 to 5 respectively. The neural network's suggested

combination o f dispatching rules deviates from the optimal combination only for the first machine, where instead o f SPT, the BPNN applies MWKR. Moreover, if all the machines use only a single rule for all machines (either SPT, LPT or MWKR), then the resulting makespans are 1720, 1699, and 1651 respectively.

58

4.7 BPNN model for minimizing mean flowtime criteria
For (he performance criterion of minimizing (he mean flowtime, a BPKN model is constructed by the same methodology as used for minimizing makespan. The only difference however, lies in the 3 dispatching rules, where SPT, PT+WINQ and LWKR are considered for the flowtime objective. This is because past research shows that all these rules are effective in minimizing mean flowtime (Waikar et al, 1995 and Rajender and Holthaus, 1999). In order to find an optimal number of hidden neurons, some modification was done in the step used for the makespan BPNN. A total of 2636 training patterns were extracted from problems of n=10, 15 and 20. The same 1200 test data set used for makespan was also used in the flowtime case. The NN parameters used for finding optimal number of hidden neurons is given in Appendix C2. Table 4.17 represents neural network results for seven different numbers of hidden neurons and Figure 4.6 depicts the effect of the number of different hidden neurons on the neural network and the minimum average error. Hence, the network used for minimizing the mean flowtime has a 15-20-15 structure.

Experiment Number 1 3 4 5 6 7

N um ber o f Hidden Neurons 10 15 17 18 19 20 25

Total Mean F low tim e 8 7 3 1 8 1 .6 2 8 7 2 5 2 9 .5 6 8 72 25 4.3 1 87 1 9 5 5 87 16 87 .81 87166931 8 7 1 9 6 7 .4 4

M inimum A verage Error 1.700 1.671 1.668 1.641 1.659 1 .5 9 4 " ' 1.655

Table 4.17 Total mean flow tim es for B PNN with various num bers o f hidden neurons.

59

0.874
  Optimal BFMN Results  "Mnimum A v erag e error" !

r 1.72
i

1,7

1.68

2
X
0)

1 0.872

d) > n
 

1.66

1

-

1.64

%

o

i 1.62

No. of hidden N eurons
0.870 ^
8

1.6

1.58 13 18 23 28

F ig . 4 .6 E ffect o f n u m b er o f b id d en n eurons on the m in im um average error for m ean flow tim e.

In order illustrate how the trained network selects dispatching rules to minimize flowtime, the same example problem of Table 4.14 is considered. The input vector is the same; however, the output vector represents different rules. Output resulting from the feed forward processing induced by the application of input vector of table 3.16 to the BPKN (15-20-15) trained for flowtime minimization is given in Table 4.18.
Output DR SPT Output 0 . 1 6 7 M a c h in e ! w rN O +PT 0 .07 0 M a c h in e ] w rNQ LWKR 0 .7 3 7 SPT 0 .0 1 4 +PT 0 .1 5 0 LWKR 0 .7 7 6 SP T 0 .2 1 6 M a c h in e s W IN Q +P T 0 .2 3 9 M a ch in e4 W ID O LWKR 0 .5 5 6 SPT 0 .95 6 +PT 0.0 LWKR 0.0 41 SPT 0 .0 7 ^ MacVlin es w rN O +PT 0.228

LWKR 0.708

The above neural network's recommendations (highlighted in the Table 4.18) are LWKR for the first, second and third machines, SPT for the fourth machine and LWKR for the last machine. This combination results in a mean flowtime of 936.65, which compares to a possible minimum of 921.45. The neural network's suggested combination o f dispatching rule deviates from the optimal combination only for the last machine, where instead o f WITJQ+PT, the BPNN suggests LWKR. On the other hand, if all the machines applied only one o f SPT, WINQ+PT or LWKR dispatching rules on all the machines, then the mean flowtimes are 989.95, 1031.55, and 952.85 respectively. 60

In this chapter, a detailed description of the design of BPNN for two different performance criteria is described. For the makespan, a BPNN model of 15-9-15 was designed and for the mean flowtime a BPNN model of 15-20-15. For both the performance criteria, the trained network suggested a combination of dispatching rules that were reasonably close to the optimal combinations. The next chapter will test the generalization capabilities of both trained networks for problems with the number of jobs ranging between a minimum of
10

to a maximum of

100

jobs.

61

CHAPTER 5 Analysis and Discussion

In this chapter, the generalization capability of the BPNN and its effectiveness in providing results to new problems (unseen during the neural network's training) will be examined for the performance objectives of minimizing makespan and mean flowtime. This will b e followed by a discussion on the results and an analysis of variance.

5.1 A n a lysis o f Trained Neural Network
Once the training o f the neural network as explained in the previous chapter is completed, it is necessary to test how well the neural network's suggested combination of dispatching rules perform in new problems.

5.1.1 N eural N etw ork W eights
The trained neural network holds its knowledge in the weights between the nodes. The final weights from the training process using Neuroshell 2 (as explained in the section 4.4.4) are provided in Appendix D1 and D2 for the makespan and mean fiowtime networks respectively.

5.2 Test Problem s
A total o f fourteen different sets o f new problems with the job numbers (n) ranging from
10

to

100

jobs is created using the same parameters used for the testing sets in the 62

previous chapier. hach set contains fifty test problems generated randomly using different

random seeds for each different category of n. Altogether, 700 problems are generated using the C program of Appendix A4.

5.3 Test Results for Minimizing Makespan
Once the generation of the various problems is completed as mentioned above, the next step is to obtain and compare the total makespan in each set. Each set of fifty problems is tested 4 times. In the first trial, the SPT rule is applied on all five machines and the total makespan is recorded. In the second and third trials, the LPT and MWKR rules are tested respectively, in a similar fashion. Finally, the BPNN is used in the fourth trial to select dispatching rule combinations for the test problems. The BPNN generates the preferred dispatching rule for each machine by feed forward processing identical to that described for the example problem in section 4.7. Table 5.1 presents the total of the makespans in each of the trials for all 14 sets of test problems. In order to evaluate how well the neural network's (BPNN) suggested combination of dispatching rules perfonns, a comparison with the makespans produced by optimal combinations of the dispatching rules is also presented in Table 5.1. As can be seen from the results of Table 5.1, the trained neural network generates better total makespan results as compared to using the same dispatching rule on all the machines. Furthermore, the percentage deviation of the neural network (BPNN) results from the optimal is close. Deviation is calculated by using equation 5.1. The deviation of BPNN from optimal results ranged from a minimum of 0 .3 % to a maximum of 3 .0 %.

63

Test S et n o. 1 2 3 4 5 6 1 7 8 9 10 11 12 13 14 n 10 15 20 25 30 35 40 45 50 55 60 75 85 100 SP T 41317 60826 7 69 71 94894 112414 131193 143277 165143 180936 204696 220154 274736 307177 356962

D is p a t c h in g rule (tim e units) LPT 42962 62 95 1 80075 98384 116351 133983 146885 169518 186490 209076 223899 277632 313202 361673 MWKR 38565 58414 74785 92800 110828 128464 141227 163726 17 9 3 8 4 203679 217278 275343 307296 355889 BPNN 38460 5 74 05 73 9 3 9 91 0 82 109676 12 75 39 139827 162822 177479 201029 215592 271626 304342 3 53841 Optimal Results 37337 55987 72498 90547 107831 12 6 3 0 7 138829 160345 176721 199326 214898 268818 303392 352714

D e v ia tio n (%) (E q. N o . 5 .1 ) 3 .0 0 8 2 .5 3 3 1.988 0.591 1.711 0 ,9 7 5 0 .7 1 9 1 .5 4 5 0 .4 2 9 0.854 0.323 1 .0 45 0 .3 1 3 0 .320

T a b le 5.1 S u m m a ry o f total m ak esp an in sets o f 50 test p rob lem for variou s n. {b p n n o p t im a l )

Deviation (%)=

*100

OPTIM AL

(5.1)

As a sample, results for all the 50 problems of test set no. 9 (n=50) are presented in Table 5.2. These results show that there is not much difference between the neural network's perform ance and the optimal combination of dispatching rules, as further illustrated in Figure 5.1. The neural network achieved optimal results in 80% of the test problems and deviated by an average of 2.1 % from the optimal in the rest. For comparison SPT, LPT and MWKR matched the optimal result in 14%, 35% and 4% respectively o f the test problems. Similar comparisons between optimal and BPNN results for individual problems in the other thirteen sets are attached in the Appendix E l .

5.4 Test Results for M inim izing Mean Flowtime
The same sets o f test problems used for testing the makespan are also used for testing the generalization capability and the effectiveness o f the neural network that has been trained to m inim ize mean flowtime. This neural network is tested in the same manner as the previous network used for testing makespan. In the first three trials each 64

Data set No. 50N1 50N 2 50 N 3 50N 4 50 N 5 50N6 5 0N 7 5 0N 8 50N9 SON 10 S O N ll SON 12 SON 13 SON 14 SON IS SON 16 SON 17 SON 18 SON 19 S0N20 50N21 50N22 50N23 50N24 50N2S

Dispatching rule (time units) SPT 2087 3358 2S17 3S 2 0 3 71 9 3 80S 3427 3 74 8 4349 39S4 37S9 3144 3138 39 93 3814 3952 3546 4029 3612 3248 3713 3534 3451 3751 4037 LPT 2181 3379 26 69 3571 3774 3754 3569 3769 4743 3957 3887 33 60 3321 4043 4025 4152 3919 4214 3826 33 96 3705 3626 3558 4017 4080 MWKR 20 87 3355 2449 3508 3677 3754 3406 3 62 0 4134 3 85 6 3739 3143 3104 4001 41 15 4002 36 35 3835 35 72 3163 3 56 6 3409 3451 3743 39 78 BPNN 2 08 7 3239 24 49 35 08 3677 3754 3406 3 62 0 4134 3856 3584 3143 3104 4001 3865 3952 3546 3835 3572 3163 3566 3409 3451 3743 3 97 8 Optimal 2087 3234 2449 3508 3677 3754 3406 3620 4134 3856 3568 3055 3104 3993 3645 3952 35 46 3835 3572 3163 35 66 3409 3451 3743 39 78

Data set No. 50N 26 50N 27 5 0N 2 8 50N 29 50 N 30 50N31 50N 32 50N 33 50N 34 50N35 50 N 3 6 50N 37 50N 38 5 0N 3 9 5 0N 4 0 50N41 50 N 4 2 50N 43 50N 44 5 0N 4 5 50 N 4 6 50 N 4 7 5 0N 4 8 50N49 50N50 T otal

Dispatching rule (time units) SPT 3017 3971 3406 3654 4 11 9 3548 3911 3632 3821 38 87 3324 39 46 3317 4049 3849 3 43 3 3 68 8 37 40 3720 3940 35 16 3331 3862 33 13 3 73 7 LPT 3224 3 99 9 3322 3614 4 15 4 3 64 8 4014 3 62 9 4106 39 94 3351 3982 3551 4159 3 99 4 3562 3799 38 28 3814 3917 3408 33 79 38 76 34 19 4 25 2 MWKR 3190 3881 3189 3540 3986 3 50 3 3874 34 82 4035 3885 3221 3774 33 25 3946 3 84 5 35 15 3683 3590 3960 3798 3408 32 63 37 19 3156 43 14 179384 BPNN 2993 3881 31 89 35 40 3986 3503 3 87 4 3 48 2 3920 3885 3 18 3 37 74 3287 3946 3845 3462 3683 3590 3699 37 98 3408 3263 3719 3156 3771 177479 Optimal 29 93 3881 3189 3540 39 86 3503 3874 3482 3689 38 56 3183 3774 3287 3946 3845 3433 3683 3590 3601 3798 3408 3263 3719 3156 3737 176721

18 09 36 18 64 90

problem s for n=50 (set no. 9 in T able 5 .]).

3900 -

c 3 4 0 0 -I

I

t'y

W VAî\
- o p tim a l - N e u r a l N e tw o rk
11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49

g 2900

2400 -

1900

Fig 5.1 BPNN m akespan com pared to m akespans from optim al rule com binations for n=50.

65

set IS tested using SPT, WINQ+PT and LWKR respectively on all machines. BPNN is tested in the fourth trial. Tlie results (expressed as total of mean flowtime) are given in Table 5.3.
T e st S e t no. ] 2 3 4 5 6 7 8 9 10 11 12 13 14 n 10 15 20 25 30 35 40 45 50 55 60 75 85 10 0 SPT 2 7 446.3 3 7 6 7 7 .4 7 45734 .4 6 55211.6 6 4 1 0 6 .9 9 7 4 3 2 2 .5 6 8 0 2 6 4 .6 6 9 3 7 5 4 .3 3 1 01784.1 1 1 3 8 0 0 .9 1 2 0 9 1 3 .9 1 4 9 9 7 7 .6 1 7 0 5 4 5 .7 1 9 3 6 5 8 .7 D isp a t c h in g rule (tim e units) W lN Q + P T 27596 3 7 7 0 9 .0 7 4 5 6 7 6 .2 1 5 5 0 2 5 .8 6 3 3 4 5 .3 9 7 2 8 0 3 .3 4 7 8 441.74 9 0887.78 9 8 259.69 1 0 9 8 9 0 .9 8 1 1 7 9 1 6 .7 4 1 4 6 4 1 0 .0 8 1 6 2 4 6 7 .9 8 1 8 8 0 7 0 .2 LWKR 2 6 831.5 3 6 2 8 0 .7 3 43 935.94 5 2 7 7 2 .5 5 6 0 8 9 7 .4 4 7 0 2 1 4 .7 3 7 5 3 7 3 .9 8 6 9 7 7 .5 2 9 4 9 2 2 .1 5 10 6 1 5 8 .4 1 1 3 1 1 7 .8 1 3 9 6 5 5 .6 1 5 6 7 2 6 .2 1 7 9 3 1 9 .3 BPNN 2 6 8 8 9 .2 3 6 1 9 1 ,9 3 4 3 8 4 2 .5 1 5 2 3 1 6 .5 1 60 255.63 69 623.42 7 49 7 5 .3 8 8 6 073.49 94 021.89 1 0 4 8 4 5 .4 11 1846.1 1 3 7 7 5 9 .3 1 5 4 9 0 7 .9 1 7 6 4 3 7 .4 Optim al 2 6 0 7 5 .1 3 4 5 5 8 .2 4 3 0 5 3 .4 8 5 1 5 1 6 .5 6 5 9 3 8 6 .8 2 68 4 6 9 .2 3 7 3 5 7 3.3 3 8 4 8 9 7 .6 5 9 2 624.78 1 0 37 8 5.0 8 1 1 0 3 6 4 .7 5 1 36 2 8 6.2 6 15293 8.7 6 17469 5.0 8 D e v ia tio n (%) (Eq. N o . 5.1) 3 .1 2 2 4 .7 2 7 1,833 1.553 1.463 1 .6 8 6 1.9 06 1.3 85 1.5 08 1.022 1.342 1.081 1.2 88 0.997

T a b le 5.3 S u m m a rj' o f total m ean fiow tim e results o f test sets con sistin g o f 50 p rob lem s each.

Once again, the neural network results are better as compared to using either SPT, W INQ+PT or LW KR common on all five machines. The network's results deviate from the optimal by an average of 1.74% (having a minimum of 1.0 to a maximum of 4.7%). SPT, PT+W INQ and LWKR results deviate on average by 9.9%, 6.25% and 2.52 % respectively from optimal results. Similarly, results for all the 50 problems in test set no. 9 (n-50) are presented in Table 5.4. Also, the comparison between BPNN and optimal mean fiowtime is shown graphically in Figure 5.2. It can be observed that BPNN achieves optimal results in 20% o f the problems, but deviates by an average of 2.3% from optimum in the remainder. Similar comparisons between optimal flowtimes and BPNN results for individual problems in the other thirteen sets are attached in the Appendix E2.

66

Data
SCI

Dispatching rule d im e unitsj .SP'J 1258.62 1822.46 1442.54 20 51 .6 2 03 2 .9 2205 1779.68 W l.N O ' P'l 1126.34 1783.34 1378.4 1902.76 2030.58 2092.04 1827.26 1949.8 2 30 1 .1 6 1987.08 2004.24 1779.46 1656.96 2 20 1 .1 6 2055.64 2147.8 1922.46 2 23 6 .5 6 1973.62 1929.04 1979.3 1902.14 1930.78 2 10 2.06 2 212.34 LWKR 1111.6 1756.04 1347.78 1826.3 1962.68 2011.22 1749.02 1941.28 2388.74 1960.96 1970.34 1637.22 1638.82 21 60 .08 1940.16 2092.14 1878.52 20 66 .92 1962.84 1695.08 1884.82 1837.14 1873.16 20 88 .08 2 1 2 5 .2 6 BPNN 1111.6 1700.62 1347.78 1826.3 1985.78 1928.54 1673.34 1941.28 2 388.74 1878.02 1975.08 1637.22 1638.82 20 71 .16 2 13 4.22 1935.8 20 6 7 .6 8 1962.84 1651.9 1857.3 1730.46 1727.46 2 08 8 .0 8 2 055.04 Optimal 1092.82 1680.32 1272.62 1810.28 1930.58 1920.02 1673.34 1914.10 2 276.36 1877.92 1952.14 1579.18 1630.08 207 1.1 6 2062.82 1871,16 20 66 .92 1926.58 1651.90 1857.30 1729.62 1727.46 20 60 .26 2 055.04

Data set No. 50F26 50F27 50F28 50F29 50F30 50F31 50F32 50F33 50F34 50F35 50F36 50F37 50F38 50F39 50F40 50F41 50F42 50F43 50F44 50F45 50F46 50F47 50F48 50F49 50F 50 T otal SPT

Dispatching rule (time units) W IN O + P T 1679.06 22 50 .56 1728.36 1977.44 2 139.38 1962.5 2073.72 1900.9 2027.34 2254.2 1854.4 2091.44 1853.54 2139.1 2 13 4 .4 8 1939.46 2 01 1.5 2020.3 2 04 1.98 2 1 5 1 .1 6 1867.52 1884.4 2 0 5 1 .3 6 1677.2 2 13 6 .0 8 9 8 2 5 9 .6 9 L W KR 1540.98 2 129.72 1666.48 1876.34 2073.84 1810.08 2 0 8 6 .3 6 1897.68 1994.06 2 054.12 1727.32 2040.54 1772.26 2 153.04 2 08 9 .6 4 1863.86 1990.16 . 1956.88 1982.9 2042 1812.36 1745.54 1984.04 1708.88 2 0 1 6 .8 8 9 4 9 2 2 .1 5 BPNN 15 40 .98 2 1 2 9 .7 2 1715.28 1814.5 2 0 0 6 .3 4 1775.84 2 2 6 5 .3 8 18 21 .3 6 1 99 4 .0 6 2 0 5 4 .1 2 1727.32 19 5 8 .4 8 1 77 2 .2 6 2 0 3 8 .5 6 2 0 8 9 .6 4 1 86 3 .8 6 1966.62 1 98 1.08 19 82 .9 2 0 4 5 .0 8 1 83 0 .4 6 1 68 8 .3 6 2011 16 69 .98 2 0 2 3 .5 9 4 0 2 1 .8 9 Optimal 1540.98 2 105.04 1662.92 1804.70 2004.38 1775.84 2070.38 1810.70 1942.26 2052.00 1663.20 1958.48 1720.42 2033.08 2080.86 1856.58 1966.62 1946.96 1982.90 2016.58 1798.40 1681.46 1972.10 1642.24 1959.56 92624.78

No. 30F ) 50F2 50F3 50F4 50F5 50F6 50F7

1774.12 2363.84 1851.14 1947.46 2052.44 1917.3 2280.92 1852.84 2 115.08 2 28 1 .5 8 1789.68 20 8 2 .5 8 1961.8 2150.1 23 29 .0 6 2040.58 21 62 .2 8 2 2 0 8 .6 2 13 8.88 2351.62 1921.72 1858.82 222 9.1 8 1799.56 2 125.22 101784.1

50F8 2 19 2 .3 8 50F 9 2 4 9 1 .3 6 50 F 1 0 2022.3 50F11 2 11 1.84 50F12 1701.34 50F 13 1801.34 50F14 22 54 .12 5 0F 15 2 0 4 2 .2 8 5 0 F 1 6 23 27 .5 50F 17 2 0 7 3 .2 6 50F18 2 3 2 6 .9 8 50F 19 21 2 1 .5 8 5 0F 2 0 1847.28 50F21 2 06 2 .2 6 50F 22 1796.18 50F23 1916.54 50F 24 2 30 2 50F25 2 21 4 .3 8

1940.16 ' 1886.16

Table 5.4 M ean flow tim e for individual test problem s for n=50 (set no. 9 in T able 5.3).

g

1550 -

Hf«V
- o p tim a l
·BPN N I 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49

I___________ 1

Fig. 5.2 BPNN mean flow tim e com pared to mean flow tim e from optim al rule com binations fo r n=50.

67

5.5 Funher D iscussion on Results
Computational results from Table 5.1 and Appendix El indicate that the trained neural network has the capability of seleeting an appropriate combination of dispatching rules in m ore than 75% o f all the 700 problem instances for the performance criterion of minim izing makespan. Moreover, the neural network provides the required result more quickly as compared to enumerating all possible combinations. Further, the efficacy of the proposed neural network approach to find the best combination of dispatching rules does not significantly decrease as the number of jobs increase. Figure 5.3 compares BPKN results with the total makespan of the optimal combinations carried out for the various num ber o f jobs (n) (ranging from a minimum of 10 to a maximum 100) for a 5machine job shop. It can be seen that BPNN performed consistently better than the three other dispatching rules for all n. Figure. 5.3 portrays that, as the number of jobs increases, the difference between BPNN and other dispatching rules decreases (particularly in the case with SPT). This explains SPT's improved performance under higher shop congestion levels; however, this does not imply that the trained network is weakening. In case of SPT, the job with the smallest processing time is given preference over other jobs so that the amount o f time and the number o f jobs waiting in the queue are reduced, and thus the desired objective o f minimizing makespan is achieved well. Further observations from Figure 5.3 show that, as the number o f jobs increases, SPT also improves in comparison to MWKR. This is because jobs with a larger total remaining processing time may be waiting in queue for a longer time as compared to the jobs with the smallest processing time when SPT is used. Also, the neural network can be

6«

seen as gradually losing ils advantage for the larger number of jobs due to the improved performance of SPT for larger n. For the performance objective of minimizing the mean fiowtime, a study is carried out using the results from Table 5.3. The difference between optimal and BPFTN mean fiowtime for the number of jobs (n) ranging from a minimum of of 100 is presented in Figure 5.4 (on the next page). It is evident from Figure 5.4 that the neural network's effectiveness is still maintained for large size problems. It is also observed that LWKR works well for both small and large size problems. However, the trained neural network proves that a combination of three competing dispatching rules in the job shop generate better mean fiowtime for both small and large size problems. It is also evident from this study that the neural network is fairly consistent while others tend to improve only when n gets bigger. From Figures 5.3 and 5.4 there is some indication that MWKR (for makespan minimization) and LWKR (for fiowtime minimization) become more competitive with respect to the BPNN as n grows large. Therefore, it is of interest to determine whether the difference between these dispatching rules and BPNN is significantly different in job shops processing a large number of jobs. This is done by means of ANOVA for the results from the largest problems tested, the
1 0 0 -job 10

to a maximum

problems.

5.6 Analysis O f Variance (ANOVA)
The trained neural network is tested with different sets of jobs. It is desirable to test the significance in the difference between the observ ed results. The technique used on the post-trained neural network is a one factor ANOVA where a comparison of all means is

69

0.175
-- SPT --  -L P T . -A- - M W K R -- H-- B P N N

0.150 -

0.125 - _

0.100

0.075

0.050 -

0.025 "t -4<
~r

0.000 0
Fig.

20

30

40

50
N o . o f J o b s (n)

60

70

80

90

lOOi

S J C om p arison o f B P N N , SPT , L PT and M W K R wifh respect to optim al total m akespan (ref. T able 5.1).

70

0.125

1

--

- S P T -- m - WINQ+PT-

A - L W K R -- ^ -- BPNN

0.100 4

/

0.075

o

Ê a

E o
c

/;

//
li' V

A

\

o 0.050

4

> a ?

0.025

0.000 0
20 30 40 50 60 No. of Jobs (n) 70 80 90
lOOi

Fig. 5.4 Comparison of BPNN, SPT, WINQ+PT and LWKR with respect to optimal total mean fiowtime (ref. Table 5.3).

7 1

done separately for the two objectives o f minimizing makespan and mean flowtime.This means to formulate a hypothesis test and to decide whether or not there is sufficient evidence to reject a null hypothesis that there is no significant difference between the means. W hile designing the experiment, the objective was to compare the performances of dispatching rules, which includes either single or a combination of dispatching rules. The between dispatching rules" source of variation, which may be assumed to be due to changes in dispatching rules or a combination of dispatching rules, is o f a different kind from that due to sampling and analytical errors. Following are the assumptions considered while carrying out the one factor ANOVA.

5.6.1 A ssu m ptions
· · The population from which the samples obtained are normally distributed. The populations have the same standard deviation ( cr ).

5.6.2 Data Representation
In this case, there are four different treatments (X=4), the first three representing different dispatching rules and the fourth one representing a neural combination o f dispatching rules. Consider 10 different random samples. Each sample is composed o f 50 problems for n=100 (as explained previously in section 5.6). Each entry is categorized as a value o f x y , which is the average makespan or fiowtime on th e /^ ' individual problem

taken from

sample.

5.6.3 General Logic - Analysis O f Variance
]n analy.sis of variance, between-group variance (difference between treatments) is tested for dissimilarity from within group variance. If the null hypothesis is true, both the above-mentioned variances are about the same, and all the variation can be attributed to random variation. On the other hand, if the between-group variance in comparison to the within-group is larger, then chances are that the samples do not come from populations with equal means. Together, the within-group and between-group variations are the two sources that contribute to the total variation. F is a statistic that represents ratio of two variances; ^
'«"o

_

B e t w e e n - g r o u p V a r ia n c e W i t h i n - g r o u p V a r ia n c e

2^

If the group means are equal, then F will equal 1.0 and the null hypothesis can be accepted. However, when the sample means are different, the difference can be attributed to the effect of the independent variable plus the sampling error. Thus,
_ E ff e c t o f in d e p e n d e n t v a r ia b l e -f S a m p l i n g error S a m p l i n g error .

5.6.4 ANOVA Calculations
A one-way ANOVA studies the effect of a single independent variable on a dependent variable. The computational formula for the involves the sum of

squares, which is the sum of squared deviations around the mean. There are two sum of squares that are considered: ( 1 ) the total of the sum of squares within the groups (SS,oiai); this is based on the deviation between each observation and X , and (2 ) the sum of squares for the group mean relative to the grand mean (SSgmup). Thus, 73

(a)SS,,,,,, =

....

(5.4)

(b) (SSgroup) =

- A' y Where, X is the grand mean.

. ...

(5.5)

X^ is the group mean. (c) Tlie total variance is composed of SSgmup & SSerror, so that SSerroT~ SSjoial" SSgroup .... (5.6)

Next is the computation of mean square obtained by dividing the sum o f the squared deviations by the degrees o f freedom. Thus,
S S group (d )M S g ro u p = d fg r o u p ····

S Serror

(e) MSerror = dfe^ror The degrees o f freedom are; (f) dfgroup = (g) d f loiai =
( h ) d f e rr o r =

KNN

1

degrees o f freedom between groups. degrees o f freedom within groups.

1

- K

The F. statistic is the ratio of the group mean square to the mean square error. Thus,
yr _ ^ ^ g ro u p
M S er ror

.....

( 5 .9 )

74

5.6.5 l^ypothesis
For the one factor ANOVA the null hypothesis is that the population means (/;) are equal, which is staled as follows;
H,,:

fJ , = /V; = ...

= A

And the alternative hypothesis is: Hp At least two means are significantly different. There are several alternative ways in which the null hypothesis may be false. For example, only pi ^ p 2 or p 3 # or all four population means are unequal, and so on.

The alternative hypothesis does not distinguish among these various possibilities, but rather asserts that a relationship exists between the independent and dependent variables such that the population means are not equal.

5.7 ANOVA Results For Makespan
The experiment is carried out with 10 samples, each sample consisting o f fifty 100-job problems. Thus, 500 problems are considered for this experiment. Table 5.5 presents the average makespan in each of the samples, where three independent dispatching rules are applied exclusively, along with the combination of dispatching rules suggested by the neural network (BPNN). The ANOVA is performed using the single factor ANOVA function in Microsoft Excel with a level of significance a = 0.05. Table 5.6 displays the ANOVA results. From the above results, the F^ statistic, F > Fo,o5 ,3,36 , and therefore the null hypotheses Hq can be rejected. It may be concluded that a significant difference exists among the dispatching rules for the performance objective of minimizing makespan. 75

S a m p le N o . ] 2 3 4 5 6 7 8 9 10

SP T 7133.1 723 6 .6 6 7 1 90 .48 7 2 4 7 .6 4 7100.54 7 0 9 0 .2 4 7257.36 7 157.24 7 2 2 8 .2 2 7 1 88 .88

LPT 7 2 4 7 .7 4 7 3 3 6 .3 6 7 3 1 0 .9 4 73 7 7 .3 7 2 4 6 .5 8 7 2 3 4 .7 4 7 3 6 7 .5 4 7 2 3 7 .1 4 7 3 6 9 .0 8 7 3 1 2 .9 6

MWKR 71 42 .5 2 7 2 8 8 .9 6 7 2 3 0 .0 4 7 2 4 6 .5 6 7 2 3 8 .2 4 7 2 0 2 .4 8 7 2 3 1 .9 6 71 9 7 .3 7 2 3 2 .6 6 7 2 3 1 ,3 8

BPNN 7 0 8 2 .2 2 7 2 1 8 .8 8 7 1 4 4 .3 4 7 2 0 2 .0 6 7 1 1 6 .4 2 7 0 8 2 .5 4 7 1 8 5 .0 2 7116.6 7 1 6 5 .8 2 7 1 7 2 .6 2

Table 5.5 Experimental data for ANOVA (average makespan). SUMMARY
G r ou p s SPT LPT MWKR BPNN Couni 10 10 10 10 Sum 7 1 8 3 0 .3 6 7 3 0 4 0 .3 8 72 24 2.1 7 1 4 8 6 .5 2 A verage 7 1 8 3 .0 3 6 7 3 0 4 .0 3 8 7224.21 7 1 4 8 .6 5 2 V ariance 369 7 .7 9 4 6 4 9 3394.519951 1441.709533 23 16 .1 17351

ANOVA
Source o f Variation B etw een Groups W ith in G r ou p s Total SS 1 3 4 363.4292 9 7 6 5 1 .2 7 3 3 6 2 3 2 0 1 4 .7 0 2 6 df 3 36 39 MS 4 4 7 87.80973 2 7 1 2 .5 3 5 3 7 1 F 1 6 .5 1 1 4 1 9 6 2 P -v a lu e 6.5068E -07 F crit 2.8 6 6 2 6 5 4 4 7

Table 5.6 ANOVA results for the performance objective of minimizing makespan. However, it is still unknown which rules are significantly different from the others. Therefore, a multiple comparison procedure using Fisher's Least Significant Difference (LSD) method (also referred to as the protected t-test) is carried out to compare two group means simultaneously. This tests the significant difference between the means. The formula for the LSD procedure is:
k - ^ 2) ^1 ]^
-- I- --

t

....

(5.10)

-

\

M ,

n '2 V

Wliere .v,, X; = mean for group] and group

2

respectively.

MSgjTor = mean square within groups (from ANOVA result).
76

n,. n, - Sample sizes for group] and 2. In this case BPNN results are compared with the results of all three different dispatching rules and the computation of Fisher's LSD is presented in Table 5.7
BPNN M ea n S a m p le size ^^error P airs B P N N and SPT y , = 7 14 8.65 2 n, = 10 SPT y , = 7 1 8 3 .0 3 6 n2 LPT y , = 7 3 0 4 .0 3 8 n^ = 10 2712.535371 Calculation o f protected t / = --------- :------------------ =--------- ^ = -1 .4 7 6

M W KJR
y , =7 22 4.21 n ^ = 10

= 10

(7148.652-7183.036) 2*2712.535

B P N N and LPT

/ =

V 10 (7148.652-7304.38) --------;---------: -----: ------- r---- - = (2*2712.535 V 10 (7148.652-7224.21)
= =

-6.671

B P N N and

/=

--------=

-- ^ = -3 .2 4 4

MWKR

2*2712.535
10

Table 5.7 LSD calculations for the minimization of makespan experiment. The critical value for t at a significance level a = .05 and df = 36 is t - 2.02 (from the standard t distribution table) (Montgomery, 1997). The protected t-tests show significant differences between LPT and MWKR with respect to BPNN. On the other hand, there is no evidence of a significant difference between the group mean of SPT and BPNN. This means that SPT might be improving as the problem size increases. The results could also indicate that the neural network may be getting weaker as n grows. Similarly, it has been observed that there is a significant difference between other groups of means such as MWKR and LPT, except between SPT and MWKR. Thus, it is further evident that SPT improves as the number of jobs increase, as compared to both BPNN and MWKR.

77

5.8 A N O V A Results For Mean Fiowtime
A similar ANOVA analysis is also carried out for the mean fiowtime result. The details o f the experiment and the ANOVA results are presented in Table 5.8 and Table 5.9 respectively.
S a m p le N o , 1 2 3 4 5 6 7 8 9 10 SP T 3 8 2 7 ,6 1 0 4 3 9 0 2 .2 4 8 4 3 9 0 0 .6 8 6 8 3 9 4 4 .2 4 7 6 3 9 3 1 .0 5 7 6 3 9 6 3 .7 2 5 4 3 8 6 1 .4 9 8 2 3 8 7 9 .2 9 6 6 3 9 2 4 .7 2 0 4 3 9 13 .94 5 6 W IN O -PT 3 7 21 .1 7 1 6 3 7 62 .88 3 4 3 7 8 0 .1 1 8 4 3 7 73 .67 1 8 3 8 38 .70 3 2 3827 .3 9 7 6 3 734,0116 3 7 7 3,5 42 4 3 7 83 .5 3 8 8 3767.9396 LWKR 3 5 6 3 .1 4 3 2 3 5 97 .30 1 8 3 6 4 6 .3 3 5 4 3 6 19 .79 7 2 3 6 5 5 .1 4 8 4 3 6 7 7 .5 5 4 3 5 6 7 .4 5 7 8 3 5 9 4 .8 1 6 3 6 3 4 .7 3 3 6 2 4 .6 5 5 4 BPNN 3 5 1 2 .4 9 4 3 5 4 5 .1 2 2 3 5 8 3 .5 2 7 3 5 7 8.5 26 3 6 0 5 .3 4 8 3 6 17 .02 5 3 5 2 5 .1 5 3 3 5 3 5 .0 0 1 3 5 7 7 .8 5 8 3 5 65 .21 3

Table 5.8 ANOVA experimental data (average mean fiowtime). SUMMARY
Groups SPT W IN O +P T LWKR BPNN Coum 10 10 10 10 Sum 3 9 0 4 9 .0 3 7 3 7 7 6 2 .9 7 8 4 3 6 18 0 .9 3 9 2 35645.2654 Average 3 9 0 4 .9 0 3 7 3 7 76 .29784 3 6 1 8 .0 9 3 9 2 3 5 6 4 .5 2 6 5 4 V ariance 1 6 3 3 .9 1 8 5 5 4 1 2 9 5 .9 9 6 3 2 8 1398.511358 1 1 8 4 .8 5 0 0 2 6

ANOVA
Source o f V a r ia tio n B etw een G ro u p s W ith in Groups T ota l 7 6 8 1 2 1 .8 7 6 9 39 SS 71 8 5 0 2 .3 9 0 5 4 9 6 1 9 .4 8 6 4 df 3 36 MS 2 3 9 5 0 0 .7 9 6 8 1 3 7 8 .3 1 9 0 6 7 F 1 7 3 .7 6 2 9 5 7 1 P -va lu e 1 .8 1 6 6 E - 2 ] F cril 2 .8 66265447

Table 5.9 ANOVA results for the performance objective of minimizing mean fiowtime. From the ANOVA results, since F > F q.qs.s.bô. null hypothesis can be rejected

and it can be concluded that there is a significant difference among the mean of different dispatching rules and BPNN in minimizing the mean fiowtime. Fisher s LSD procedure is also carried out and Table 5.10 depicts these calculations. The protected t-tests show that there is a significant difference between BPNN on

78

BPNN M ean S a m p le Size M Serror y , = 3 564.53 n| = 10

SPT = 3 9 0 4 .9 0 02 = 10 1378.319

W IN Q +PT V, = 3 7 7 6 .2 9 n^ = 10

LWKR = 3 6 1 8 .0 9 " 4 = 10

Pairs
B P N N and SPT

Calculation o f protected 1

(3564.53-3904.90) 1^1378.319 \l 10 (3564.53-3776.29) [2*1378.319 V 10 (3564.53-3618.09)
= - V =

B P N N and W INQ+PT

/ = -------:-----^------ r ^ - --------^ = - 1 2 . 7 5 5

B P N N and LWKR

/ = -------P

-3 .2 2 6

2*1378.319 V
10

Table 5.10 LSD calculations for the mean fiow tim e experim ent.

one hand, and all three dispatching rules, SPT, WhNQ+FT and LWKR on the other. In this chapter, the performances of different dispatching rules including BPNN are compared with the optimal results for various problem sizes. The results show that BPNN performs better than individual dispatching rules. The BPNN results were close to optimal for both minimizing makespan and mean fiowtime. Also, for larger size problems, ANOVA tests were performed on both the performance objectives separately. This was for examining significant differences between different groups of means. The results show that for the makespan criterion, in the larger size problem, there is a significant difference between means of BPNN and LPT, and BPNN and MWKR, but not between SPT and BPNN. On the other hand, for the mean fiowtime criterion, ANOVA shows a significant difference among all the dispatching rules and BPNN, for the large size problem. In the next chapter, a summary of the results, followed by the conclusion and further research, is discussed.

79

CHAPTER 6 C onclusion and Further research

6.1 Summary
This research has considered the problem of selecting an appropriate combination o f dispatching/priority rules for scheduling job shops. The dispatching rule selection problem is shown to be a prediction problem that can be successfully solved using a proposed neural network. For building the neural network, simulations were carried out to constitute a training set. Once the network was trained, it was shown to be able to select well-suited dispatching rules for new problems. Computational results showed that the proposed neural network correctly predicted makespan in more than
75%

of the

problem instances. Further, the effectiveness and generalization capability of the proposed neural network was retained with increase in the number of jobs. Results indicated that, for the problem situation considered, the concept of using a combination o f dispatching rules for different situations (number of jobs) in a neural network yields better results than using a single dispatching rule. Tlte proposed neural network for minimizing the makespan criterion produced an average improvement of about 1.5% over MWKR (the best competing individual rule) and
6%

over LPT (the poorest performer

among the three rules considered), as seen in Table 5.1. On the other hand, for minimizing the mean flowtime objective, the proposed neural network performed better by an average o f 1% over the best competing individual rule (LWKR), and by 6.5% over SPT (the poorest performing rule considered), according to the results in Table 5.3.

80

In this siudv. the test problems were randomly generated, and a small fraction of the
120

possible job routes for five machines were considered in each problem instance.

This simulates a realistic scenario where jobs with the same routes belong to either a part family or a group, and have similar sequences of operations but different processing times.

6.2 Conclusion
The proposed neural network is an input-output model. In this researeh the input to the proposed network consists of total processing time, variance of processing time and the mean routing order on each machine. The trained neural network identifies such characteristics and quickly assigns one of three dispatching rules to use on each machine. It has been obser\'ed during the study that if only a single dispatching rule like smallest processing time (SPT) is applied on all five machines, than the optimality of the desired performance objective (in this case minimizing makespan) is not always achieved in the shop. On the other hand, if neural selections of dispatching rules are applied to each of the machines, then better results are obtained as compared to using a single dispatching rule. Although the neural network was trained with small job numbers, the efficacy of the trained neural network for larger size problems appears to be maintained. For example, the BPNN results deviate only 0.429 % from the total of the makespans achieved by the optimal mix of the three dispatching rules for job shops with n=50. Hence, the BPNN, from a practical application point of view, can be used as a tool to aid scheduling decisions in a plant, or can be embedded in a production computer integrated manufacturing system for automated and dynamic selection of appropriate 81

priority rules. The developed model offers significant advantages regarding time consumption and simplicity for scheduling new job shop problems.

6.3 Further Research
The suggested methodology is not restricted to a static job shop model, but there are a number o f fruitful directions that can be identified for further research from this study, some o f which are: 1) Dynamic job shop problems. 2) Different performance objectives. 3) Relaxation o f assumptions. 4) Number o f dispatching rules. 5) Job shops with more machines. The details o f each future direction are as follows: The present study can be extended to consider a dynamic job shop problem, where the arrival times o f jobs are not known (in advance) before scheduling. The jobs may arrive at any time; in such a real-world situation, the job characteristics are fed to the neural network. The trained network can decide in real-time when to change rules on specific machines in response to the changing characteristics o f the work-in process so as to better meet the performance objectives. It is worth investigating how this network trained for a static job shop, will perform when subjected to a dynamic environment. A similar neural network approach can be used for other performance objectives where the jobs will have an additional characteristic such as due date, like minimizing mean tardiness, minimizing number o f tardy jobs or minimizing the cost. The current study made some assumptions, like no machine breakdowns, material-handling systems 82

arc available ai all limes, etc. The neural network can be trained by relaxing such

assumptions in order to capture more realistic real world situations. In a flexible manufacturing system that resembles the job shop model, if one of the material handling systems breaks down, then in such a situation an intelligent neural network can optimize the job sequence and thus the desired perfonnance objective. In the present study, a set of three dispatching rules was considered for two different perfonnance objectives, minimizing makespan and mean flowtime. The study can be extended to test the possibilities of getting better solutions if combinations of more than three dispatching rules are used for the same or different performance objectives. In the present study, a five-machine job shop problem was considered; a similar approach can be extended to a higher number of machines, for example, ten machines for the same or different performance objectives. Summing up, the objective/purpose of this research was to develop and train an artificial neural network to select the best combination among three dispatching rules for a five-machine job shop problem and two different performance criteria. Experimental results showed that the trained neural network was able to successfully predict good rules to use on each machine, leading to better satisfaction of the performance criteria when compared with the alternative of using one identical rule on all machines.

83

References

Baker K. (2002) Elements o f sequencing and scheduling. Hanover N.H., ISBN 0963974610.

Bialey D. and Thompson D. (1990) How to develop neural-network. Al expert, vol.5 (3), pp.38-47.

Blackstone J., Phillips D. and Hogg G. (1982) A state of-the-art survey of dispatching rules for manufacturing job shop operations. International journal of production research, v o l.2 0 (l), pp.27-45.

Blazewicz J., Domschke W. and Pesch E. (1996) The job shop scheduling problem: conventional and new solution techniques. European journal of operations research, vol. 9 3 ,p p .l-3 3 .

Conway R., Maxwell W.and W Miller L. (1967) Theoiy o f Scheduling. Addison-Wesley.

French S. (1982) Sequencing and scheduling: An introduction to the mathematics o f the jo b shop. John W iley & sons Inc., New York.

Poo S.Y. and Takefuji Y. (1988a) Stochastic neural networks for solving job shop scheduling: Part 1, Problem definition. Proceedings o f the 1988 IEEE international conference on neural networks, San Diego, vol.2, pp.275-282.

84

Foo S.Y. and Takefiiji Y. (1988b) Stochastic neural networks for solving job shop scheduling; Part 2, Architecture and simulations. Proceedings of the 1988 IEEE international conference on neural networks, San Diego, vol.2, pp.283-290.

Garson D. (1999) Neural ueiworks an iniroducloiy guide fo r social scientists. SAGE publications, ISBN 0-7619-5731-6.

Klimasauskas, C.C. (1992) Applying neural networks in finance and Investing. R.R Trippi and E. Turban (eds), Chicago: Probus, pp.47-72.

Jain A. and Meeran S. (1998) Job-shop scheduling using neural networks. International journal of production research, vol.36 (5), pp.1249-1272.

Jones A., Rabelo L. and Yih Y. (1996) Job shop scheduling. Encyclopedia of operations research, (http://www.mieI.nist.gov/msidlibrarv/doc/iobshopl.pdfl.

Kiran, A. S. and Smith M. L. (1984) Simulation Studies in Job Shop Scheduling-1. Computer. & industrial engineering, vol.8 , pp.87- 93.

Kaastra I. and Boyd M. (1996) Designing a neural network for forecasting financial and economic time series. Neurocomputing, vol.lO ( 3 ), pp.215-236.

Kelton D., Sadowski R and Sadowski D. (2002) Simulation with Arena. McGraw-Hill series, ISBN 0-07-239270-3.
85

Kumar H and Srinivasan G. ( 1996) A genetic algorithm for job shop scheduling --A case study. Computers in industry, vol.31, pp.155-160.

M ontgomery D. (1997) Introcluciion to statistical quality control - Third edition, John W iley & Sons, Inc New York.

Neuroshell 2 software manual. Ward systems group, Inc. http://www.wardsvstems.com.

Panwalkar S. and Iskander W. (1977) A survey of scheduling rules. Operations research, vol.25 (1), pp. 45-61.

Pierreval H. (1992) Training a neural network by simulation for dispatching problems. IEEE Proceeding o f the third rensselear international conference on computer integrated engineering, pp.332-336.

Pierreval H. (1993) Neural network to select dynamic scheduling heuristics. Journal of decision science, vol.2 (2), pp. 173-190.

Pinadeo M. (1995) Scheduling theoiy, algorithms and systems. Prentic Call Englewood Cliffs, NJ.

Rabelo L. (1990) A hybrid artificial neural networks and knowledge-based expert systems approach to flexible manufacturing system scheduling. PhD dissertation.

86

Kaghu. I .S. and Rajcndian. C. (1993). An Efficient Dynamic Dispatching Rule for Scheduling in a job shop. International Journal of Production Research, vol.28,
pp.2277-2292.

Rajendran C. and Holthaus O. ( 1999) A comparative study of dispatching rules in dynamic flowshops and job shops. European journal of operational research, vol.l 16, pp. 156-170.

Reinhardt M. (1990) Neural networks An Introduction. Springer-verlag, Berlin Heidelberg.

Sabuncuoglo 1. and Gurgun B. (1996) A neural network model for scheduling problems. European journal of operational research, vol.93, pp.288-299.

Sabuncuoglo 1. (1998) Scheduling with neural networks: a review of the literature and new research directions. Production planning & control, vol.9 (1), pp. 2-12.

Satake T., Morikawa K. and Nakamura N. (1994) Neural network approach for minimizing the makespan of the general job-shop. International journal of production economics, vol.33, pp. 67-74.

Sim S.K., Yeo T.Y and Lee W.H. (1994) An expert neural network system for dynamic job shop scheduling. International journal of production research, vol.32 (8 ), pp. 17591773.

87

Smilh K. (1999) Introduction to heural networks and dato mining fo r business applications. Eruditions publishing. ISBN
1

864910046.

Subramaniam V., Ramesh T., Lee K.G., Wong S.Y. and Hong S G. (2000) Job shop scheduling with dynamic fuzzy selection o f dispatching rules. Advanced manufacturing technology, vol.] 6 , pp.759-764.

Swingler K. (1999) Applying Neural netw'orks A project guide. Morgan kaufrnan publishers, Inc, California.

W aikar A., Saker B. and Lai A. (1995) A comparative study of some priority dispatching rules under different shop loads. Production planning and control, vol . 6 (3), pp.301-310.

Zhang H.C and Huang S.H. (1995) Applications o f neural networks in manufacturing; a state o f art survey. International journal o f production research, vol.33 (3), pp.705-728.

88

Appendix A l: Problem generation
^include -'sidio.h> f/include ''sidlib.h^ //include ''iimc.h> main(void)

;
FILE '*scheda.' *combo; float T F .R D D ,U L ; int Y | 101 ][21 ].P [2 1 J.route choice[26],load_dist[ 16],routes,Counter; int i,k.n.m .r.reps.Z,X,Wf,Wt.PP,g,q,Liner,lcounter,al,a2,a3,a4,a5; char FFname( 12].dest[30],Fdat[ 12],gstr[4],strl [ 10],str2[ 10]; if((com bo = fopen("c:\\tcWbin jobshops\\perm5!.dat","r")) = I fprintf(stderr."Cannot open output file . \n"); return I ; FFULL)

fo r ( i- l ; i < = 1 0 0 ; + + i j for(k=];k<= 21;++k) y [ i ] [ k ] = 0; printfC'What is the number o f JOBS ? \n"); scanf("%d",&n); printfC'H o w many test problems ? \n"); scanf( ",&reps) ; m=5; s r a n d (]2 l);

" % d

for{g= l;g<= reps;++ g) strcpy(strl ,"t"); itoa(g,gstr,10); strcat(strl,gstr); strcpy(Fdat.strl); strcat (Fdat,".dat"); strcpy(dest."c:\\tc\\bin\\jobshops\\daia\\"); strcat(dest,Fdat); if((scheda = fopen(dest,"w")) = = N U L L )

{ fprintf(stderr,"Cannot
return 1;

open output file . \n");

routes = random(6)+5; for(k= 1 ; k < = 2 1 ; ++ k ) P[k] = 0: for(p=0;r<=25;++r) route_choice[r] = 0; for(r= I ;K=routes;++r) route_choice(r] = ran d om (l21); for(r=0;r<=15;++r) load_dist[r] - 0; for(r=];r<=m;++r) load_dist[r] = random (49)+]0; fprintf(scheda,"%d %d \n", n,m); for(i=l ;i<=n;++j)

Al

for(k= 1 ;k<=m:-"+k)
 :

Y (i] [k ] = ra n d o m (4 1 ) - load_distJk]; P [k] = P [ k ] - Y [ i ] ( k ] :

for(i= 1 ; i<=n; + + i )
. 1

Z =)000; lco u n te r= 0 ; Liner = r o u ie _ c h o ic e [r a n d o m (r o u te s ) + 1 ]; w h ile ( L in er ! - Icounter) { fs c a n f(c o m b o ," % d %d %d %d %d \n " ,& a l,& a 2 ,& a 3 ,& a 4 ,& a 5 ); + + lc ou n ter; ) fprintf(scheda,"% d %d %d %d %d r e w in d (c o m b o ); fprintf(scheda,"% d " ,Y [ i][ a l ]); fprinif(sch ed a," % d ",Y [i][a 2 ]); fprintf(scheda,"% d ",Y [i][a 3 ]); fprintf(scheda,"% d " ,Y [i][a4]); f^ rim f(sched a," % d " ,Y (i][a5]); fprintf(scheda,"\t %d ",Z); fp rin tf(sch ed a," \n"); " ,aL a2,a3,a4,a5);

i
fprintf(scheda,"\n\n"); f c ] o s e (s c h e d a ) : + + C o u n te r; printf("counter is s : % d\n".C ounter); fc]ose(com bo);

A2

Appendix A2; Training set generation
Winclude < s ld io .h > «in clu d e ^ std lih .h > «include ''tim c .h > «in clu de < m ath .h > «in clu d e < strin g .h > struct table

I
int jo b n o ; int protim e: int duedatc; int slack; int slackp; int flo w t; int tarw t; flo at ncuro;
I ·

struct measures

I

float flw tim e ; float m eantard; int num tar; flo at m axtar; flo at m akespan;

Ii
struct p icks int partno;

float Costco;
). ) » struct probdata

1
int jo bno ; int operation! 11]; in t m ach ine no[11 ]: int D ue; int elapsed; int route track; int flo w t; int tarw t; int to talw k; \ . f ' struct tim etrack

{
int endtim e; int p artnum ; int m achine;
1.

I5

Struct e d a ta

1
int jo b in d ex; in t com pletion; int duedate; int elapsed; in t flo w t; int tarw t;
r .
)·

struct bufs
1

int queue[102]; in t m axcap;

A3

flo :u L O A IJ ( 1 i .struct m easu res M M [ lOJ: struct b u fs b u t'ie r | I 1];

1 11 .m a x lo a d :

struct tim e lr a e k e v e n t[2 I 1]; struct ctlata sink] 11 1]; struct p ro b d a ta I ' R l 1 1 0 2 ];ln t d isp a tch in g rule] 16 ]; int objsel.con .sel; in t T I M E N O W .d d .b s i; flo a t T a rd in c s s .T T a rd .B c o s t.C C o s t: in t C o u n t= 0 ,m a c h in e s ,h .v ; c h a r n c a t[3 ]; int d r o u tp u t[3 1 ]; in a in ( v o id j

I
F I L E * c o m b o .* s in p u t.'*s o u tp u t.' n e u in p u t.' neuoui: struct ta b ic s p t_ s o n ().c d d _ s o n ().m s lk _ s o n (j.lp t_ s o rt().in s lp _ .s o n ().w s p t_ s c rt(); struct ta b ic c le a r jo b (),u p d a te jo b s ().r a n d o m _ s o r t(); struct m easu res c a lc u late_ in e asu rcs (),c alcu late_co st(),.s ch e d u lc_co sts (); struct ed ata fin d schedO ; struct bufs

spt_buf(),ipt_buf().twspt_buf().CD().tspt_buf0.edd_buf().slack_buf().covert_buf(),mdd_buf(),lwkr_bufi:),twk_bufO;
struct bu fs .s n q _ b u f().w in q _ b u f().a p p e n d _ to _ b u ffe r(),u n lo a d _ m a c h in e (); int c o m b o x [ 2 4 4 ][6 ] ,k I ,k 2 .k 3 .k 4 .k 5 jj,c o m b in a tio n ; flo a t m c sq u are] I l],m a x s td e v ; flo a t best re s u lt.p e rfo rm a n c c ; in t r e m o v e J o b (),m a x _ p r o t(),m a x _ d a te ,m e m b e r _ o f(),A v e r to t; flo a t M f lo w ,B f lo w ; in t i,k .k i,u ,n ,p ,q ,r ,t,y j.jo b ,s e q u e n c e ,n e x l_ m a c x u n n a c ,b e s t_ ta rd y ; in t p la c e .fo u n d ; in t b e st_ ta rd in e .s s ,b e s t_ flo w ,b e s i_ c o m b o ,m a c _ c a n d id a te [1 4 ],m in c o s tjo b ,to rp ro ,m in c o s td u e ,m o n d u e ; int B E S T [ 3 0 0 ] [ l l ] , n j o b s [ 3 ] ; in t g re a te r o f(),c o u n te r,tie co un ter; c h a r F F n a m e [ 1 2 ] ,W W R [4 4 ].d e s t 2 [3 0 ].d e s tl[3 0 ].d e s t[ 3 0 ].F d a t[1 2 ]; c h a r * s tr l ; in t c o m b o c o u n te i= 0 ; if((s o u tp u t = fopenC 'ctW tcW binW jobshopsW dataW result.dat'V'w ")) = = N U L L ) ] fp rin tf(s td e rr," C a n n o t open output file . \n "); re tu rn 1;

) ) )
fp rin tf(s o u tp u t," \n

if((n e u in p u t = fo p e n (" c ;\\tc V 'b in \\jo b s h o p s \\d a ta \\n e u jo b .in p "," a " )) = = N U L L ) Î fp rin tf(.s td e rr," C a n n o i open output f ile . \n " ); re tu m 1; M /C 1 M /C 2 M /C 3 M /C 4 M /C 5 \n " ); in ");

fp rin tffs o u tp u t," -- -------

I*
p rin tfC 'S e le c t an O b je c tiv e F u n c tio n ; \n \n " ); p r i n t f ( " l . M in im iz in g M e a n F l o u ii m e \n " ): p r in tf(" 2 . M in im iz in g M a k e s p a n s c a n f(" % d " .& o b js e l); * / objse) = 2; if ( o b js e l= = l ) if((n e u o u t = fo p e n (" c ;\\tc \\b in \\jo b s h o p s \\d a ta \\n e u jo b f.o u t''," a " )) = = N U L L ) { fp rin tf(s td e rr," C a n n o t op en output f ile . \n "); n");

1 I
else

1 J

r e tu m 1;

if((n e u o u t = fo p e n (" c ;\\tc \\b in \\jo b s h o p s \\d a ta \\n e u jo b m .o u t" ,'a ) ) = = N U L L ) j ip rin tf(s td e rr ," C a n n o t op en output

file . W ) ;

return 1;

A4

for(k=l;l<^=30;++k) ciroulput|k]=0;
besi result = 9 9 9 9 9 9 9 9 9 .9 ; jj= > ; M k l = ];k K = 3 ;+ + k j) f o r ( k 2 = l: k 2 < = 3 ; + + k 2 } f o r ( k 3 = l; k 3 < = 3 ; + + k 3 ) fo r ( k 4 = l; k 4 < = 3 ; + + k 4 ) fo K k 5 = !; k 5 < = 3 ; + + k 5 )

I c o m b o x [jj]l l ] = k l ;
c o m b o x [jj](2 ]= k 2 ; c o m b o x (jj](3 ]= k 3 ; c o m b o x [jj](4 ]= k 4 ; c o m b o x [jj][5 ]= k 5 ;

!
fo r(c o m b in a tio n = l ;c o m b in a lio n < = 2 4 3 ;+ + c o m b in a tio n )

I
-^ c o m b o c o u ru e r; strl = "T 30 .d at"; s tp c p y (F F n a m e .s irl); strcp y(d esl,"c :\\tc \\b in \\jo b sh o p s\\d ata\V ); sircai<desi.strl ); if((sin p u t = fo p en (d est,"r")) = = N U L L )

{ fp rim f(s id e rr,"C an n o i open input file . \n ");
return 1 ;

!
fs can f(sin p u i,"% d % d ",& C o u n l,& m a c h in e s );

for(k=0;k<=IO;++k)
b u ffe r[k ].m a x c a p = 0; f o r ( k = 0 ;k < = l5 ;+ + k ) dispatching ru le [k ] = 0; fo r(k = ! ;k < = m a c h in e s ;+ + k ) dispatching ru le [k ] = c o m b o x [c o m b in a tio n ][k ];

*

for(k=l;k<=5;++k)
I p rin tf(" % d *".d is p a tc h in g _ ru le [k ]); if< d is p a ic h in g _ r u le [k ]= 0 ) g e tch ar();} fo r ( i= l; i< = 2 1 0 ; + + i) ev en t[i].e n d tim e = e v e n t[i].p a n n u m = event[ i].m a c h in e = 0;

f o r ( i= l; i< = 1 0 ; + + i) fo r(j= 0 ; j < = 1 0 1 ; + + j ) bufT er[i].q ueu e[j] = 0; f o r { i= l; i< = ll O ; + + i) s in k[i].jo b in d ex = s in k[i].co m p letio n = sin k[i],d u ed a te = sin k[i].elapsed = 0; fo r (i= l;i< = C o u n t;+ + i) fo r(k = 0 ;k < = 1 0 ;+ + k )

{ P R T [i].o p e ra tio n [k ] = 0; P R T [i].m a c h in e _ n o [k ] = 0; m c _ s q u a re [k ]= 0 ; j
fo r ( i= l;i< = C o u n t; + + i)

{
toqjro = 0; fo r (k = l ;k < = m a c h in e s ;+ + k ) fs can f(sinpu t,"% d " ,& P R T li].m a c h in e _ n o [k ]);
1 »

fo r(k = l ;k < = m a c h in e s ;+ + k )

{
fs can f(sin p u i."% d " .& P R T [i].o p e ra tio n [P R T [i].m a c h in e _ n o [k ]]); to qjro = totpro + P R T [i].o p e ra lio n [P R T [i].m a c h in e _ n o [k ]];

>

P R T [i].o p e ra tio n [0 ] = totpro;

A5

P R T (i] .io ia lw k = to tpro; P R T (i].io b n n = i; P R 11 i].elap sed = 0; PR t| i] route track = 0 ; P R T | i] P d w t = 1 ; P R T (i].ia r w t = | ; fs c a n f(s in p u t," % d " .& P R T | i].D u e ); L O A D [T n a c h in e s ] = 0; fo r ( k = 1;k < = m a c h in e s ;+ + k ) L O A D [ k ] = 0:

fo r ( k = 1 :k ^ = n ia c h in e s :+ + k ) fo K i= 1;i< = C o u n t ;+ + i) L O A D [ k ] = L O A D [ k ] -+ P R T |i].o p e r a tio n [k ]; m a x lo a d = 0; f o r ( k = 1 ;k < = m a c h in e s :+ + k ) i f ( L O A D [ k ] > m a x lo a d ) m a x lo a d = L O A D [ k ] ; f o r ( k = l ;k < = m a c h in e s ;+ + k ) fp rin ti"(n e u in p u U " % 5 .3 f " ,L O A D [k ]/m a x lo a d ) ; m a x s td e v = 0 .0 ; fo r ( k = 1 ;k < = m a c h in e s ;+ + k )

J

f o r ( i= l ;i< = C o u n i; + + i) m c _ s q u a re [k ] = m c _ s q u a re [k ] + P R T (i].o p e r a tio n [k ]* P R T [i].o p e r a iio n [k ); m c _ s q u a re [k ] = ( (m c _ s q u a re [k ] - L O A D [ k ] * L O A D [ k ] /C o u n i) /( C o u n i- l) ); if(m c _ s q u a re [k ] > m a x s id e v ) m a x s td e v = m c _ s q u a re [k ]; fo r ( k = );k < = m a c h in e s : + + k ) fp rin tf(n e u in p u t,'' % 5 .3 f m c _ s q u a r e [ k ] /m a x s t d e v ) ; f o r ( k = l ;k < = m a c h in e s ;+ + k ) p la c e = 0 ; forO'= 1d < = C o u n !; + + j)
{

V )

f o r ( p = l ;p < = m a c h in e s :+ + p ) if(P R T (j].m a c h in e _ n o [p ] = = k ) {+ + p la c e ; brea k;] else + + p la c e ; )]3 rin tf(n e u in p u i.'' % 5 .3 f " ,(flo a t)p la c e y (C o u n t*m a c h in e s ));

I )

fp r in tf(n e u in p u i," \n ''); fc lo s e (s in p u t); fd o s e (n e u in p u t); /* * * A s s ig n all jo b s to th e ir in itia l bufl'ers and in itia liz e ev en t tra c k in g *» **·» » » / f o r ( i = l ; i< = C o u n t; + + i)

{
fo u n d = 0; fo r (k = 1 ; k < = m a c h in e s ; + + k ) if(P R T [i].o p e r a tio n [k ] != 0 & & fo u n d = 0)

I
a p p e n d _ to _ b u ffe r (P R T [i].m a c h in e _ n o Ik ],i); + + P R T [i].ro u te _ tr a c k ; fo u n d = 1 ;

I )

) I

fo r ( k = 1 ;k < = m a c h in e s ;+ + k ) if ( b u f le r [ k ] .q u e u e [ l] != 0 ) add e v e n t(0 ,0 ,k );

/**»*»»·»**·*»**·*****»*****·»·»».*·*****·**»*··**··**·***·*··*··*/
T IM E N O W =0; w h ile (e v e n l[l].T n a c h in e ! = 0 )

A6

>.

scqucncc= I'RTlevcni( I ].pannuiT)].route_track; curm ac = event] I].machine: job=eveni| 1],partnum:
/» p r in tfC 'T IM E : % d - E V E N T on m ach ine % d fo r P A R T %d

\n".event] 1].c n d tim e.ev cn l] 1].m achine.event] 1].partnum );

printf("TIMENOW = %d\n",event]]].endtime);
T I M E N O W = event] l].e n d tim e ; j= '; w h ile (c v c n ty ].m a c h in e != 0 )

*/

e v e n ly ],c n d tim e = e v e n ty ].e n d lim e - T I M E N O W :

-+j:
fo rti= ';j'^ = C o u n i;-^ + j)

PRTy], elapsed = P R T y ]. elapsed + T I M E N O W ;
P R T yj.D ue = P R T y].D ue - TIM ENOW :
T I M E N O W = 0; update event list():

ifyob != 0)
/* m ove jo b to the next m ach ine's b u ffe r */

found = 0; i f( P R T y o b ],o p e ra tio n ] sequence+1 ] != 0 & & f o u n d = 0 ) n e x tm a c = P R T y o b ].m a c h in e _ n o [s e q u e n c e + l]; unload_m achine{curm ac); a p p e n d _ to _ b u ffe r(n e x t_ m a c jo b ); + + P R T y o b ].ro u te _ tra c k ; if(b u ffe r[n e x t_ m a c ].q u e u e [0 ] = = 0) lo ad_m achine(next_m ac,dispatching_Tule[next_Tnac]); a d d _ e v e n t(T lM E N O W + P R T [b u ffer[n e xt_m ac ].q u eu e] O ]].o p eratio n [n e x t_ m a c ],b u ffe r[n e x t_ m a c ].q u e u e [0 ],n e x t_ m a c ); fo u n d = l: if(fo u n d = = 0 ) a d d _ to _ s in k y o b ,T lM E N O W ,0 );

f

I

unload_m achine(curm ac);

load_m achine(curm ac,dispatching_rule[ cu rm ac]); if(b u ffer]c u rm ac ],q u eu e]0 ] != 0 ) add e v e n t (T IM E N O W + P R T ]b u ffer]cu rm ac].q u eu e ]0]].o p eratio n ]c u rrn ac],b u ffer]c u rm ac ].q u eu e]0 ],cu rm ac ); else

load _m achin e(cu rm ac,d isp atch in g_ru le]cu n T iac]);
a d d _ e v e n t(T lM E N O W +

PRT]buffer]curmac].queue]0]].operation]cuiTnac],buffer]cunnac].queue]0],curmac);

)
J

]

/ * end o f com bination fo r loop schedule_costs():

*1

I*

pn'ntfC'mean flo w tim e = % 7 . 2 f \ n " , M M ] l ] .flw tim e ): prin tfC 'makespan = % 7 .2 f \n " .M M ] 1],m akespan); p rin tf("n u m b er o f jo b s tardy = % d '\n " ,M M ]l].n u m ta r); p rin tfC 'm axim um jo b tardiness = % 7 .2 f \n " ,M M ]l] .m a x ta r ); printfi["'\n");

fcr(i=) ; i<=machines; ++i)

A7

prinifCThc maximum queue length in buffer %d = %d unints\n".i, buffct1i].maxcap); prinlfC n"); *! iKdbjsel 1 ) pedom iancc = MM[ 1].llwiim c: else perfonnance = MM[1],makespan; if( performance < best result - 0.00001) 1 best result = perfonnanee; tie counter = I; for( u= 1;u<=machines;++u) BEST|tie_counter][u] = dispatching ruleju]; | else if (perfonnance > best rcsult-O.OOOOl && performance < hest result+O.OOOOl)

-^-rtic counter; for( u= 1;u<=machincs;+-t-u) BEST|tie_counter]|u] = dispatching rule|u]; | I /***» end o f the main while * · · * / for(p= 1;p<=tie_counter;++p) for( u= 1;u<=machines;++u) if(BEST[p][u] == I ) ) lprintf(soutput,'' SPT "); ++ d r o u {p u l[3 * u -2 ]; J if(BEST[p][u] == 2) 1 f^rintffsoutput," LPT "); -h - d r o u t p u t [ 3 * u - l] ; J if(BEST[p][u] == 3) 1 fprintffsoutpui," M W K R "); ++ d r o u tp u t[3 *u ]; }

J

I
fprintf(soutpul,"\n ");

1 )

fprintf(soutput,"\n ---------------------------iffobjsel == I ) fjprinff(soutpul," mean flowtim e = % 7.2f ",best_result); else fjprintffsoutput," makespan = % 7.2f ",best_result);

\n");

for(i= I ;i<=3*m achines;++i) fprintf(neuout," % 5.2f ",(float)droutpui[i]/tie_counter); fjprinif(neuGut,"\n "); fclose(neuout); fclose(com bo); fclosefsoutput); retum(O);

i
struct bufs append_to_buffer(bnum jobnum ) int bnum, jobnum; int i= l ;
w h ile (b u ffe r [b n u m ].q u e u e [i] != 0 )

I
++i;
\ .

)*

if (i > bufTer[bnum].maxcap) buffer[bnum].maxcap = i; buffer[bnum].queue[i] = jobnum; return:

t

/* ' Function ; unload_machi( ) This function removes a job from machine (t) struct bufs unload machinefftnmac) int finmac;

*/

{
int i.finjob; fm job = buffer[fmtnac].queue[0];
L O A D jf in m a c ] = L O A D ( f in m a c ] P R T [fin jo b ].o p e r a tio n [0 ] =

PRT[finjob].operation[ftnmac];

P R T [f in jo b ].o p e r a tio n [0 ] - P R T Ifin jo b ].o p e r a tic n (fin m a c ];

b u ffe r[fin m a c ].q u e u e [0 ] = 0; re tu m ;

}

A8

a(id_cvcni(iimejob.mac) ill! limcjob.mac; ini i= 1; ini p.spoi; while (cveni|i].machine )=() && evenl(i].cndiime < lime)

I

++i;
I !

spol = i; while (cvcnti i],machine !=()) ++i; for(p=i;p>=spot;-p) eveni|p]= eveni|p-l]; cvenl|spol).endiime= lime; cvcnl(spot].pannum = job; eventl spol].machine = mac; relum; load_machine(mac,selecied) ini mac,selected;

)
int i= l; if(bufrer[mac].queue[2] != 0)

1
if(selected == if(selected == i{(selected == if]selected -- if] selected == 1) 2) 3) 4) 5) spt_buf(mac); lpt_buf(mac); mwkr_bufi(mac); winq_buf(mac); lwkr_buf]mac);

1
do bufTer[mac).queue[i-l] = buffer[mac].queue[i]; ++i;
J

1 I

while (buffer[mac].queue[i-1 ] != 0); retum;

update_eveni_list()

{
int i= I ; do event[i-l] = event[i]; ++i:

I ) ) )
{

while (event]i-l].machine != 0); retum;

add_to_sink(.iobno,eomp.due) int jobno,eomp,due; int u=l; while]sink] uj.jobindex != 0)

)

++u; sink]u].iobindex = jobno; sink]u].compleiion = comp; sinkju].duedate= due; sinkjuj-flowt = PRT[jobnG].flowt; sinkjui.tarwi = PRT[jobno].tarwt;

. I »

A9

sink] u],elapsed return; disinis.s_job(_j) int J:
int i;

PRT[sink[ u].jobindcx].elapsed + comp;

for (!=l;j<=m achines;++i) PRT[j].operationli] = 0;
P R T [ j] .D u e = 0; P R T L i].flo w t = P R T |j] .ta r w t = 0;

,

PRTLi].jobno = 0; rcium(O);
\ i

struct bufs spt buffbufiio) int bufno;

{
n d e fiiie F A L S E 0 " d e fin e T R U E 1

int j.k . sorted = FALSE; float trigl,trig2; int temp; int n = 1; while(buffer[bufno].queue[n] !=0) ++n; -n ; w hile (! sorted) sorted = TRUE; for (.1 =1 ; j<n; ++]) i f (PRT[buffer[bufno].queue[j]].operation[buffio] > PRT[bufrer[bufno].queue[j+I]].cperation[bufno]) { temp = buffer[bufho].queue[j]; bufrer[bufno].queue|j]=bufrer[bufno].queue[j+1]; buffer[bufno].queue|_j+l]=temp;
sorted = F A L S E ;

retum;
1

i
struct bufs lpt_buf(bufno) int bufno:

{
jfdefine FALSE 0 ^define TRUE I int j,k, sorted = FALSE; float trigl.trig2; int temp; int n = 1: while(buffer{bufnol,queue[n] !=0) ++n; -n ; w hile (Isorted) sorted = TRUE; fo r (j= l;j< n ;+ + j)
if( P R T [b u ffe r [b u fh o ].q u e u e [j]].o p e r a tio n [b u fn o ] < P R T [b u ffe r [b u fn o ].q u e u e [j+ l]].c p c r a tio n [b u fn o ])

temp = buffer[bufno].queue[j];
b u ffe r[b u fn o ].q u e u e lj]= b u ffe ttb u fn o ].q u e u e (j+ 1 ]; b u ffe r [b u fn o ].q u e u e [j+ 1 ]= ie m p ;

sorted = FALSE;

AlO

return;
!

struct bufs mwkr_buf(bufno) int bufno; «define FALSE 0 «define TRUE 1 int j , sorted = FALSE; int temp; int n = 1; whilc(huffer|hufno].queuc|n] !=()) ++n; -n ; while (Isoned)

{
sorted = TRUE; fo r (j= l; j<n;++j) if (PRT[buffer[bufno].queue[j]].operation(0] < PRT(buffer[bufno].queue(j+l]].operation[0])

I
temp = bufTer[bufno].queue[j]; buffer(bufno].queue|j]=buffer[bufno].queue[j+l]; buffer[bufno].queue[j+1]=temp; sorted = FALSE;
J J

return;

J
struct bufs winq_buf{ bufno) int bufno;

{
«define FALSE 0 «define TR U E ! int j,k, sorted = FALSE; int temp; int n = 1; while(buffer[bufno].queue[n] !=0) ++n; -n ; while (Isoned)

I
soned = TRUE; forCi=l; j<n; ++j) if (work_in_next_queue( buffer[bufrio].queue|j], bufno) > work_in_next_queue( bufTer[bufho].queue[j+l], bufno))

(
temp = buffer{bufho].queueü]; bufrer[bufno].queue[j]=buffer[bufno].queue[j+1]; buffer(bufTio].queue[j+ 1]=temp; soned = FALSE;

retum;

All

w o rk

in _ n c x l_ q u L u c ( i()b n u m b cr,p rcs cn l_ m a c)

in i io h n u m h c r . present m ac:

{'

'

\

:X

in i q .k 1.n u m b .v . y iiin p _ |o b .n e x t_ m a c ,w o rk in q u e u c = 0 ; f o r l q - I :q < = C ()u n l:-' + q ) if { P R T [q ].jo b n o = = jo b n u m b c r) break; fo r(k 1 = 1 ;k 1 < m a c h in c s ;+ ^ k 1 ) if(P R T | q ] m a c b in e n o |k I ] = = prcseni m ac')

·1
num b = 1 ; n c xi m ac - l'R T |q ],m a c h in e n n |k 1+ 1]; w h ild b u f f e r j next m a c ].q u e u e ]n u m b ] ' . = ())

]
w a i i i n g J o b = b u ffe r] n e x i_ m a c ].q u e u e ln u m b ];

wrjrkinqueue = workinqueue+ PRT]wailing_job].operaiion(nexl_inac];
++num b; |

workinqueue= workinqueue+ PRT(q].operationlPRTjq].machine_no[kl]]; reium(workinqueue); else

I j ]

retum(O);

struct bufs lwkr_buf(bufno) int bufno; #define FALSE 0 #define TRUE 1 int j, sorted = FALSE: int temp; int n = 1; whi)e(buffer[bufno].queue]n] !=0) ++n;
-n ;

w hile (Isorted) sorted = TRUE; for (.]= I ; j<n; ++j)
i f (P R T [b u fT e r[b u fn o ].q u e u e [j]].o p e ra tio n [0 ] > P R T Ib u fr e r ]b u fn o ].q u e u e [j+ l ]].operation[0])

]
temp = bufTer[bufno].queue[j]; buffer[bufno].queue[j]=bufrei{bufno].queueO+]]; buffer[bufno].queue[j+1]=temp; sorted = FALSE;

return:

A12

Appendix A3: Comparison o f NN results with other dispatching rule results
«include '^sidio.h> «include <stdlib.h> «include <time.h> «include <math.h> «include <siring.h> «include <neuro.h> siruci table int jobno; int protime; ini duedate: ini slack; int slackp; int flowt; int tarwt; float neuro: struct measures

{
float flwtime; float meantard; int numtar; float maxtar; float makespan;
I . i >

struct picks

I I.> )
{

int partno; float Costco;

struct probdata int jobno; int operation[l 1]; int machine_no[l 1]; int Due; int elapsed; int route track; int flowt; int tarwt; int totalwk;
\.
J '

Struct timetrack

{ I
{

int endtime; int pannum; int machine;

. J >

struct edata int jobindex; int completion; int duedate; int elapsed; int flowt; int tarwt;

) ·

I.
{

struct bufs intqueue[l02]; int maxcap;

A13

flo a t L O A D | 1 1 ] ,P L 0 A D | 1 l],m a x lo a d ; struct m easu res M M | 10]: struct bufs b u lT c r|2 1 ]: struct tim e tra c k event12 t 1]; struct edata sin k| 111]: struct p ro b d a ta P R T | 101 ];in t dispatch ing ru lc j 16]; int ru le s e l.c o n s e l; int T I M E N O W ,d d .b s t; flo a t T a rd in c s s .T T a rd .B c o s t.C C o s t; int C o u n t= 0 .m a c liin e s .h .v ; c h ar n e a t]3]; int d r o u tp u i[3 1 ]; m a in fv o id )

I

F I L E *s in p u i.* s o u tp u i: struct ta b le s p t_ s o rt().e d d _ s o rt(),m ,s lk _ s o rt( ).lpt_ so rt().m s lp _ .so rt(),w sp t_.s o n (); struct ta b le c le a r jo b ().u p d a te jo b s ().r a n d o m _ s o r t(); struct m easu res c a lc u la te tTteasures().calculate co st().sched ule_co sls(); struct edata fin d schedf); struct bufs s p t_ b u f().lp t_ b u f().tw s p t_ b u f().C D ().ts p t_ b u f().e d d _ b u f(),s la c k _ b u f().c o v e r t_ b u fi[),m d d _ b u f(),lw k r_ b u f(),tw k _ b u fl[); struct bu fs s n q _ b u f(),a w in q _ b u f().a p p e n d jo _ b u ffe r ().u n lo a d _ m a c h in e (); in t c o m b o x J 2 4 4 ][6 ],k l ,k 2 .k 3 .k 4 .k 5 J j.c o m b in a tio n ; flo a t m c _ s q u a re ]l l],m a x v a r.m a k e s p a n s ,flo w tim e s ; flo a t best re s u lt.p e rfo rm a n c e ; in t r e m o v e _ jo b ().m a x _ p r o t().m a x _ d a te .m e m b e r _ o f().A v e r to t; flo a t M f lo w .B f lo w ; d o u b le n _ v e c to r[ 1 6 ],n o u tp u t] 16 ].m a x o u t: in t i,k .k i,u ,n ,p ,q ,r ,t ,y j jo b .s e q u e n c e .n e x t_ m a c ,c u rm a c ,b e s t_ ta rd y ; in t p la c e .fo u n d ; int b e s t_ ta rd in e s s ,b e s t_ flo w .b e s t_ c o m b o .m a c _ c a n d id a te [1 4 ],m in c o s tjo b .to tp ro ,m in c o s td u e .n io n d u e ; in t B E S T [ 3 0 0 ] [ l l] ,n jo b s [ 3 ] ; in t g re a te r_ o f().c o u n te r,tie _ c o u n te r; c h ar F F n a m e ] l2 ].W W R [4 4 ].d e s t2 [3 0 ],d e s tl[ 3 0 ] .d e s t[3 0 ],F d a t [1 2 ]; c h ar * s t r l ; in t c o m b o c o u n te r= 0 ; if((s in p u t = fo p e n (" c ;\\tc \\b in '\jo b s h o p s \\d a ta \\m u lte s t.d a t" ." r" )) = = N U L L ) ] fp rin tf(s td e r r." C a n n c t open o u tp ut file . \n " ); re tu m 1; if((s o u tp u t = fo p e n (" c ;\\tc \\b in \\io b s h o p s \\d a ta \\m u lte s t.o u t" ," w " )) = = { fp rin tffs td e rr," C a n n o t open ou tp ut file . \n"); re tu m 1;

NULL)

)
p rin tfC 'S e le c t D is p a tc h in g R u le P o lic y : \n \n " ); p r in tff" I . S P T fo r a ll m ach in es \n " ); p r in tf(" 3 . L W K R fo r all m a c h in e s \n " ); p r in tf(" 2 . L P T fo r a ll m a c h in e s \n " ); p r in t f f "4. N e u ra l S e le c tio n \n " ); s c a n f(" % d " ,& r u le s e l); iffm le s e l != 1 & & ru les el != 2 & & ru les el 1=3 & & rulesel 1=4) { p rin tfC 'L e a v in g th e p ro g ra m . \n " ); re tu m 1 ;

I
»

if f ru le s e l = 1) fo r ( k = 1 ;k < = m a c h in e s ;+ + k ) iffru le s e l = = 2 ) f o r ( k = l ;k < = m a c h in e s ;+ + k ) iffru le s e l = = 3 ) fo r ( k = 1;k < = m a c h in e s ;+ + k ) m akes p an s = flo w tim e s = 0 ; w h il e( 1fe o ffs in p u t))

disp a tch in g _ ru le[k ]= d is p a tc h in g _ ru le [k ]= 2 d is p a tc h in g _ ru le [k ]= 3

{

AM

rscanf(sifipui."%d "/<.d".&Count,&.machines); if(lcoflsinpul)) break; )or(k=0;l<-^=l();-^+k) buffer] k],maxcap = 0; for(k=0:k<= l5;++k) ' dispaiching_rule(k] = 0; n_vecioiik]=0.0: | for(k= l;k`==machines;++k) dispatching_rulc[k] = rulesel: for(i= 1:i'^=2IO;++i) event[i]endtime = event] i],pannum = event] i].machiiic = 0; (br(i=l:i''=10;^+ij forfj-O; j-^=l01; ++j) buffer] ij.queue[j) = 0; for(i=l;i<=l Kk++i) stnkli].johindcx = sink[t].completion = sink]i].duedate = sink[i].elapsed = 0; for(i=0;i<=Count;++i) ror(k=0;k<=10;++k) ] PRTJij.operationfk] = 0; PRTli].machine_no(k] = 0; mc_square[k]=0; | for(i=l ;i<=Count;++i)

1
totpro = 0; for(k= 1;k<=machines;++k) fscanf(sinput,"%d ''.&PRT[i].machine_no[k]);

)
for(k= 1;k<=machines;++k) i fscanf(sinput/'%d ".&PRT(i].operation[PRT[i].machine_no[k]]); totpro = totpro + PRT]i].operation[PRT[t].machine no[k]];
f

i

PRT[i].operation[0] = totpro; PRT[i].total'wk = totpro; PRT]i] jobno = i; PRT]i].elapsed = 0; PRT[i].route_track =0; PRT]i].llowi = 1; PRT] ij.tarwt = 1; fscanf(sinput," %d",&PRT]i].Due);

)
for(k=l;k<=machines:++k) L O A D [kl=0; for(k= 1;k<=machines;++k) for(i=l :i<=Count;++i) LOAD]k] = LOAD]k] + PRT]i].operation]k]; maxload = 0; for(k= 1;k<=machtnes++k) if(LOAD]k] > maxload) maxload = LOADJk]; for(k= 1;k<=machines;++k) n_vector]k]= LOAD]k]/ma.sload; maxvai^O.O: for(k= 1;k<=machines;++k)

{
for(i= 1;i<=Count;++i) mc_square]k] = mc_square]k] + PRT[i].operation]k]*PRT]i].operaiionlk]; Tnc_square]k] =(mc_square]k] - LOAD]k]*LOADlk]/Count)/(Count-l); if(,mc_square]k] > maxvar) maxvar = mc_square]k];

for(k=macliines+l;k<=2*machines;++k) n_vector]k] = mc_squarelk-machines]/maxvar; for(k= 1;k<=machines;-H-k)

{
place=0;

A1 5

f o r ( i = 1· j < = C o u n f , + + j )

:
for(p= 1;p<=machines;++p) if(PRT[j].machine_nolp] = {-^-placc; break;! else --^pi ace;

k)

I t

n_vecior(2*machines+k] = (floai)place/(Couni*machines);

for(p=];p<=15;++p) n _vecior(p -l] = n_vec(or[p]: if(rulescl == 4)
:

neuro_dispaich(n_vecior.n_ompuO; for(k= I ;k<=machines:-r+k)

i
m axout-0.0: for(p=0;p<3:--rp) if(n_output[3*(k-l)+p] > maxout) i maxout = n_output[3*(k-l)+p]; dispatching_rule(k] = p + l;|

for(i= I ;i<=machines;++i) fprintf(soutput,"%d ",dispatching_rule[i]); /*** A ssign all job s to their initial buffers and initialize event tracking ********/

R efer A p p en d ix A2

A16

Appendix A4; Test Problem generation
^include < sld io .h > //include <sid iib .h > //include <tim e.h > /* generates test problem s for use by appendix D m ain(void)

*!

{
PILE *scheda.*com bo; float T E .R D D .U L ; int y [ 101 ]121 ] ,P |2 1].rou te_ch oice[26],load _d ist| 16],routes,Counter; int i,k ,n ,m ,r.rep s.Z ,X .W f.W t.P P ,g,q X in er,lcou n ter,al.a2,a3,a4,a5; if((scheda = ropen("c:''tc AbinV\jobshopsV\dataWmultest.dat","w")) = = fW L L ) { fprintf(stderr,"Cannot open output file . \n "); return 1 ; fW L L )

t
I

if((co m b o = fopen( "c :\ \tcWbin\y obshopsWpermS !.dat" ,"r")) = { fprintf(stderr,"Cannot open output file . An"); return I ; fo r (i= l;i< = 1 0 0 ;+ + i) fo r(k = ];k < = 2 ];+ + k ) Y [i][k] = 0; printfC'What is the number o f JOBS ? \n"); scanf("% d",&n); printfC'How m any test problem s ? \n"); scanf("% d".&reps); m =5; srand(2177); for(g= I ;g<=reps;++g)

{
routes = random (6)+5; for(k= ] ; k < = 2 1 ; ++k) P[k] = 0; for(r=0 ;r<=2 5 ;++r) route_choice[r] = 0; for(r=l ;r<=routes;++r) route_choice[r] = random (121); for(r=0;r<=15;++r) Ioad_dist[r] = 0; for(r=l;r<=m ;++r) load_dist[r] = random (49)+10; fprintf(scheda,"% d %d \n", n.m); fo r(i= ];i< = n ;+ + i) for{k=l ;k<=m ;++k)

{
Y [i][k ] = random (41) + load_dist[k]; P[k] = P[k] + Y [i][k ]; fo r (i= l; i<=n; + + i)

I
Z =1000; lcounter=0; Liner = route_choice[random (routes)+1];

A17

w h ile ( L iner != Icounter) I fscan f(com b o." % d %d %d %d %d \n " .& a l,& a 2 .& a 3 .& a 4 .& a 5 ); + + Icounter; | fprinif(scheda,"V od %d % d %d %d rew in d (c o m b o ); " .a l,a 2 ,a 3 ,a 4 ,a 5 );

fp rinif(scheda,"% d " ,Y [ i] (a l] ); fp rin if(sch ed a /'% d " ,Y [i](a 2 ]); fp rinif(scheda,"% d " ,Y [i][a 3 ]); fprinif{scheda,"% d " ,Y [i]|a 4 ]); fp r in if(sc h e d a ;'% d 'L Y [i](a 5 ]); fp rintf(scheda,"\t %d ",Z); fprintf(scheda."\n");

fp rin tf(sch ed a," \n \n "); + + C o u n ler:

printfC 'coun ter is s : % d\n",C ounter); fc lo s e (c o m b o ); fc lo s e (s c h e d a );

1

A18

Appendix B: Arena simulation model
p E'te Edit View Toot* Module B un Window Help

D ^

alMJ JJ
W a it S e rv e r

CREATE

R ea d

S e tv e r l

Assign

jP ep a itl W a it

Server
W o ite ta tio n B

W a it

BRANCH
M SO (Pfocess M S Q fP fo ctss M SQ rProcess M SQ fProcess M sl^P fo ctss P lan P lan P lan P lan P lan (Part cPart rP art (P art (P art lnda><)JS+t In d ax ilS + l In d ex llS ^ t ln d e x ilS + j l n d « x ) , i S + 1,

S erver
« « W o rk s ta tio n « « W o rk s ta tio n « « W o rk s ta tio n « « W o rk s ta tio n

««W orkstation A

B C D E

S tatistics
S e ts A T T R IB U T E S

Else

V ariab les
T 7an5f5T i 1m e

[s im u la te

Sequences

QUEUES
 .......

F IL E S

W a it CREATE, Signal
P a ri 7 R o u t# P a r t e R o u te P a rt y R o u te P a rt 10 R o u te W o llS ts tlo n 0 I R I § W o rk sta tio n E _ R _ Q

w ô fâ illo n S1R18

DELAY
n n n i

D IS P O S E

A ppendix Cl : Determining number o f hidden neurons for performance ob jective o f M inim izing Makespan

Number of hidden layer Learning rate Momentum Initial Weights Pattern Selection Weights updates Number o f training patter Number of test Number o f hidden neurons

1 0.1 0.1 0.3 Random Vanilla 2494 1200, combination o f n = l0,15,20 6 to 18

Cl

Appendix C2; Determining number o f hidden neurons for performance objective o f Minimizing Mean Flowlime

Number of hidden layer Learning rale Momentum Initial Weights Pattern Selection Weights updates Number of training patter Number of test Number of hidden neurons

1 0.05 0.05 0.3 Rotational Turboprop 2636 1200, combination of n = l0,15,20 10 to 25

C2

A ppendix D1 ; Neural W eights for Makespan network
r
In s c n this co d e im o y o u r C program lo fire ih e C :\N S H E L L 2 \S P T & L P T < S ^ M W K R netw o rk * / T h is c o d e is designed to b e sim p le atid fast fo r p o rtin g to an v m ach in e * !

I * 1 h c re fo re a ll co d e and w etgh ts are tn hn e w tth o u l lo opin g o r data storage
/» w h ic h m ig h t be h a rd er to port betw een co m p ilers. * /

ffin c lu d c < m a th .h > v o id n e u ro d is p a lc h fd o u b le *in a rra y . do u b le ' o u ia rra y ) d o u b le netsu tn; d o u b le fe a tu r e 2 |9 ]; / * in a r r a y [0 ] is to tal processing tim e on m ach in e I ' / / * in a rra y } 1 ] is to tal processing tim e on m ach in e 2 * /

I * in a r r a y [2 ] is to tal processing tim e on m ach in e 3 * ! / * in a rra y l 3 ] is to tal processing tim e on m ach ine A * J / * in a r r a y [4 ] is to tal p ro cessing tim e on m ach in e 5 * / / * in a r r a y [5 ] is v a ria n ce processing tim e on m ach in e I * / / * in a r r a y [6 ] is v a ria n c e processing tim e on m ach in e 2 * /
/* / * in a r r a y l8 ] is v a ria n c e processing tim e on m ach in e

inarray[7] is variance processing time on machine 3 * / 4 */ / * in a r r a y [9 ] is v a ria n c e processing tim e on m a c h in e 5 * / /* in array} 10] is mean routing order o f machine 1 */
/ * in a rra y } 1 1 ] is tnean ro u tin g order o f m ach in e 2 * /

I * inarrayl 12] is mean routing order o f machine 3 */

/* inarray} 13] is mean routing order o f machine 4 */ /* inarrayl 14] is mean routing order o f machine 5 */ /* outarray|0] is SPT on machine 1 */ /* outarrayl 1] is LPT on machine 1 */ /* outarray[2] is MWKR on machine 1 * 1 /* outarray|3] is SPT on machine 2 */ /* outarray|4] is LPT on machine 2 */ /* outarray|5] is MWKR on machine 2 */ /* outarray|6] is SPT on m a ch in es */ /* outarrayl?] is LPT on machine 3 */ /* outarrayl8] is MWKR on machine 3 */ /* ouiarray[9] is SPT on machine 4 */ /* outarrayl 10] is LPT on machine 4 *! /* outarrayl 11 ] is MWKR on machine 4 */ /* outarrayl 12] is SPT on machine 5 */ I* outarrayl 13] is LPT on machine 5 */ /* outarrayl 14] is MWKR on machine 5 */ inarrayl0] = 1 .0 /( 1 .0 + exp( -( inarray[0] - .8072053) / .1779804 )); inarrayl 1] = 1.0 / (1.0 + exp( -( inarrayl 1] - .7982979) / .1812125 )); inarrav|2] = 1.0 / ( 1.0 + e x p ( -( inarray|2] - .8054764) / .1795151 )). inarray|3] = 1.0 / (1.0 + exp( -( inarray|3] - .8004816) / . 1790274 )), inarray|4] =

1.0 / (1.0 + e x p ( -( inarray|4] -

.8064976) / .1814937 ));

inarray[5] = 1.0 / (I.O + exp( -( inarray|5] - .771103) / .1943457 )). inarray|6] = 1 .0 / (1.0 + exp( -( inarray[6] - .7 5 8 6 8 2 4 )/.1 9 2 2 8 6 3 )1. inarrayl7] = 1 .0 / (1.0 + exp( -( inarray[7] - .7655738) / .1942723 )). inarrayl8] = 1.0 / (1.0 + exp( -( inarrayl8] - .769994) / .1929265 ));

D1

inarray(9]= 1 .0 /( 1.0 ^ e x p (- { inarrayl 9 ] - .7713] 15)/.1924995 )); inarrayl 10]= 1 () / ( 1.0 * t-xp( -( inarrayl 10] - .5963829) / . 1359357 )); inarrayl 11]= 1 .0 /(I .O - e x p (-( inarrayl 11] - .598911 )/ .1376096));
inarrayl 1 2 ]= 1 .0 / ( I . O - e x p ( -( in a rr a y |1 2 ]- .6 0 4 0 6 9 8 ) / .1 3 7 5 6 4 ));

inarray|13]= 1 .0 /( 1.0 - e x p (-( inarrayl 13] - 596164) / .137554 )); inarray|14]= 1.0 / ( 1 0 * c x p (-( inarray] 14] - .6044775) / .1383298 )); neisum = .2456497; netsum += inarray|0] * 2.046942; neisum += inarrayl 1] · -1.739467E-02; neisum += inarray[2] * 1.950621; netsum += inarrayl3] * .7895958; netsum += inatrayl4] * -4.393935; netsum -^= inarrayl5] * .6341079; netsum-t= inarrayl6] · .7639786; netsum -* = inarray[7] · -.3696065; netsum -*-= inarray[8] * .1025962; netsum -* = inarray[9] * .2775704; netsum-*·= inarrayllO] * 2.588418E-03; netsum -*·= inarrayl 11] * -.1057494; netsum += inarrayl 12] * 1.179916; netsum+= inarrayl 13] * .2022499; netsum -*·= inarrayl l4] * -.2809177; feature210] =1 / (I + exp(-netsum));
netsum = .77 361 11;

netsum -+= narraylO] * netsum -+= n a rr a y ll] * netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum

.2019996; .1082652;  + = n a rra y l2 ] * 9.714916E-02; -+= na rrayl3] * .8964988; += na rrayl4] * .9009568; += narraylS] * 1.312576E-02; += narrayl6]*-1.136535E-03; += n arrayl7] * .5345301; -+= na rrayl8] * .429602; += n arrayl9] * .6620308; -*·= n a rra y llO ] * .3335176; -+= n a rrayll 1] * .4408983; -*-= n a r r a y ll2] * .8332653; -*-= narrayl 13] * 1.068497; -*·= n a rr a y ll4 ] * .8551623;

fe a tu re 2 ]l ] = 1 / (1 -t- e.xp(-netsum )); netsum = .2736917; netsum -*·= narrayl 0 ] * netsum -t-= narrayl netsum netsum + = neisum -r= netsum + = netsum -+= netsum netsum -*·= netsum -r= netsum + = netsum -*=

1.667238;

1] * 1.011702;

narrayl2 ] * -, 6062995;

netsum -*= narrayl 3] * n arrayl4] *

-.4746662; 6.320269E-02; n a rra y l5 ] * -.3131128; narrayl6 ] * .4646354; narrayl 7] * - .4525496; narrayl 8 ] * .4268539; n arrayl9] * .153007; narrayl 10] * 2.659758; narrayl 1 1 ] » -1.245716; narrayl 12] » 6.91551 lE-02;

D2

neisum += inarray[ 13] * -1.128644; netsum inarray[ 14] * 1. ] 68092; featurc2|2] = 1 / (I + cxp(-netsum)); netsum = ,4272505; netsum += narrayl 0]  .7500188 narrayl 1] netsum 1.000085 netsum += narrayl 2] ^ -1.751807 netsum += narrayl 3] .2685844 netsum += narray[4] 1.027902 netsum += narray|5] ' .6474725 netsum += narrayl6] ^ .1521454 netsum += narrayl 7] ' .2068681 netsum narray|8] ' -.5180956 netsum += narray|9] ^ .1931846 netsum >  = narrayl 10] * -1.069892 netsum += narrayl 11 ] .6269137 netsum += narrayl 12] 1.828143 netsum += narrayl 13] -1.174372 netsum += narrayl 14] .3147894 feature2[3] = 1 / (I + exp(-net.sum)); neisum = 1.253435; netsum += narraylO] * .4281614; netsum + = narrayl 1] * .263829; netsum -+= narray[2] * .7757708; netsum -*-= narray[3] * .8198949; netsum -*-= narray[4] * .4531571; netsum -+= narrayl 5] * .7415228; netsum -*-= narrayl 6] * .3905536; netsum += narrayl?] * .5716818; netsum -+= narrayl 8] * .5664634; netsum -* = narray[9] * .4108108; netsum += narrayl 10] ' ` 1.020716; netsum -+= narrayl 1 1] ^  .6482075; netsum + = narrayl 12] ^ .3916479; netsum + = narrayl 13] ^ .82701; netsum -*-= narrayl 14] '^ .5516309; feature2|4] = 1 / ( I + exp(-netsum)); netsum = 1.020303; neisum -+= narraylO] ^ .6176658; netsum -*-= narrayl 1] ^ -.1641615; netsum -*-= narray|2] ^ .6528724; netsum + = narray|3] ' .5118068; neisum -r= narray[4]  1.187099; netsum -*-= narray|5] ' .357643; netsum -*-= narrayl 6] ' .5779907; netsum -*-= n a M y |7 ] ' .6766816; netsum -+= narrayl 8] .337665; netsum += narray[9] ^ .5777485; netsum -t-= narrayl 10] * .7961912; netsum -t-= narrayl I I ] * .6893328; netsum -<-= narrayl 12] * .5017431; netsum += narrayl 13] * -.2587531; netsum += narrayl 1 4 ]* .545307; feature2[5] = 1 /( 1 + exp(-netsum)); netsum netsum netsum netsum = -.857754; += inarrayl0] * -.7458848; -+= inarrayl 1] * -1.533975; += inarray|2] * -1.908274;

D3

netsum ' = inarrayl 3] * 6.414888; neisum -* ^ = inarray|4] * -1.083793: neisum s = inarray|5] * -.1197345; neisum s = inarrayjO] * .1958044; neisum += inarrayl7] * .4577059; neisum += inarray|8] * -.8148838; neisum += inarray|9] * -.3346533; netsum += inarrayl 10] *-.4140265; netsum + = inarrayl 11]* -.9390895; netsum += inarrayl 12] * .1293133; netsum + = inarrayl 13] * -.2089598; netsum += inarrayl 14] * -.150009; )'eature2[6] 1 / (1 + cxp(-nelsum)); netsum = 9072358; netsum -< -= narraylO] * -4.788719; neisum += narrayl 1] * 1.544868; netsum += narray[2] * 2.137923; netsum += narray[3] * .6501429; neisum += narray|4] * 1.509744; netsum += narray[5] * -.552145; netsum += narrayl6] * .7359388; netsum += nairay(7] * -.3814477; netsum += narray[8] * .207289; netsum += narray[9] * -.3526275; netsum += narrayl 10] * .7480308; neisum += narrayl 11] * -.7977181; netsum += narrayl 12] * 1.289536; netsum += narrayl 13]* .1694474; neisum += narray|14] *-.1155627; feature2[7] = 1 / (1 + exp(-netsum)); netsum = . 2858913; netsum += narraylO] * -1.584813; netsum += narrayll]* 3.49939; neisum += narray|2] * -1.439081; netsum += narrayl 3] * .1945082; netsum += narray[4] * -2.07647; neisum += narrayl5] * -6.070224E-03; netsum += narrayl6] * -.3148423; netsum += narray|7] · .3856115; netsum += narrayl8] * -.1825979; netsum += narray[9] * .2919024; netsum += narrayllO] * -.3072858; netsum += narrayll 1]* 1.34602; netsum += narrayl 12] * -.3143554; netsum += narrayl 13] * -.6019555; netsum += narrayl 14] * -.4247503; feaiure2f8] = I / (I + exp(-netsum)); netsum = -.3141774; neisum += feature2|0] * .2389331; netsum += feature2[l] *-.54418; netsum += feature2[2] * -1.70218; neisum += feaiure2[3] * 1.199972; netsum += feature2|4] * -.4256195; netsum += feature2[5] * -.2628123; netsum += feature2[ 6] * .1917116; netsum += feature2[7] * 1.139177; netsum += feature2|8] * .2864685; outarrayl0] = 1 / (1 -I- e xp( -netsum));

D4

ne is u m = n e is u m fc a iu re 2 1 0 ] » -.6 8 8 4 7 7 8 ; .4 5 8 0 2 4 7 ;

n e isu m + = lc a iu r e 2 | 1] * -.1 2 6 8 5 4 2 ; n e is u m + = 1c atu re212] * n e is u m + = fe a lu r c 2 [3 ] * -1 .4 0 3 9 3 4 ; n e tsu m + = fc a iu r c 2 |4 ] * -5 .4 9 1 5 6 9 E -0 2 ; n e tsu m + = fe a tu r e 2 |5 ] * -.1 7 6 3 3 7 9 ; n e ts u m + = fe a tu re 2 1 6 ] * n e ts u m + = fe a tu re 2 ( 7 ] * n e ts u m + = fe a tu re 2 1 8 ] .2 3 7 3 7 5 6 ; 1.2 9 7 0 9 ; .2 6 3 8 6 1 5 ;

o u t a i r a y l l ] = 1 / ( I + exp(-net.sum )); netsu m = .1 4 5 7 0 9 1 ; .5 0 1 1 4 4 6 .2 1 7 6 7 8 ; 1.359661 .0 8 9 6 2 9 2 .3 4 4 9 1 5 6

n e ts u m + = fe a lu r e 2 (0 ] * n e is u m + = l'ea tu re2 [ 1 ] * n e tsu m + = fe a iu r e 2 [2 ] * n e ts u m + = fe a tu re 2 ( 3 ] * n e tsu m n e isu m + = fc a lu r e 2 [5 ] *

f e a iu r c 2 |4 ] * -.1 9 4 5 1 1 4

netsu m + = fe a tu r e 2 [6 ] * -.4 2 1 5 0 0 3 n e tsu m + = fe a tu re 2 [7 ] * -2 .2 3 5 4 8 7 ne tsu m + = fe a tu r e 2 [8 ] * -.5 8 5 3 0 0 1 ; o u ta r r a y [2 ] = 1 / (1 + e x p (-n e ts u m ));

neisum = -.593361 3; netsum += feature2[0] * .1887896; netsum += feature2[l] " -.4963281; n etsu m += feature2[2] * 1.171266; netsum += feature2[3] * -.2313004; netsum += feature2[4] * -.127795; netsum += feature2[5] * -8.012733E-02; netsum += feature2[6] * .5547236; neisum += feature2[7] * .5958884; netsum += feature2[8] * -1.437345; outarray[3] = 1 / (1 + exp(-netsum)); netsum = -2.756067E-Ü3; netsum += feature2[0] * -.2146351; n etsu m += featu re2[l] * .4711423; neisum += feaiure2[2] * -1.461678; netsum += feature2[3] * -.1164255; netsum += feature2[4] * 5.208082E-02; netsum += feature2[5] * .4290704; netsum += feature2[6] * 5.005599E-02; netsum += feature2[7] * -.7152472; netsum += feature2[8] * -.4304705; outarray[4] = 1 / (1 + exp(-netsum)); netsum = -.1 6 3 1 3 3 6 ; netsum += feature2[0] * -9 .1 12045E-03; netsum += feature2[l] * ;.1418273; netsum += feaiure2[2] * .3139661; netsum += feature2[3] * .4594962; netsum += feature2[4]  -.5077903; netsum += feature2[5] * -.7215487; netsum += feature2[6] * -.6620205; netsum += feature2[7] * .1405424; netsum += feature2[8] * 1.835448; outairay[5] = 1 /( 1 + exp(-netsum)); netsum = .1286572; netsum += feature2[0] * -1.496963;

D5

netsum += fcaiurc2( 1] * .6675627; netsum + - fcaiurt2[2] * 3.4672}i9E-02; netsum feaiure2[3] * -.211388; ncisum *= lcaiure2(4] * .4022121; neisum f'ealure2[5] ^ 9.279378E-02; neisum -+= lealure2|6] * .221469; neisum feaiurc2[7] * -1.08096; netsum *= fealure2|8] * .5696868; outarray(6)= 1 / ( M exp(-nelsum)); neisum - -.307033; neisum -= feature2[0] * -.2450359; neisum *= feaiure2| 1]*-1.057701; neisum -= I'eaiurc2(2] * .643006: neisum -= fealure2|3] * 1.318899; neisum -^= feaiurc2[4] * -.593664; ncisum -= fcaiurc2f5] * -.3161325; neisum -^= feaiure2|6] * .6486963; neisum -^= fealure2[7] * -.2953574; ncisum feaiurc2[ 8] * .2141867; oularray[7]= 1 /( I + exp(-nelsum)); neisum = -2 .5 7 2 132E-03; neisum -+= feaiurc2[0] * 1.719462; neisum -*  = feaiure2[l] * -.3831635; neisum -+= fealure2[2] * -.6865217; neisum -*  = feaiure2[3] * -1.433495; neisum -+= feaiure2[4] * -.1088401; neisum -· = feaiure2[5] * 1 501535E-02; neisum -+= fealure2[6] * -.8874764; neisum -*  = feaiure2[7] * 1.299559; neisum -+= feaiure2[8] * -.7590157; ouiarray[8] = 1 / (1 + exp(-neisum)); neisum = -.3838195; neisum -*-= feaiure2[0] *-.1236574; neisum -+= feaiure2[l] * -.9973538; neisum -· = feaiure2[2] * .7501443; neisum -+= feaiure2[3] * 1.06568; neisum ^-= feaiure2[4] * -.3194561; neisum -+= feaiure2[5] * .4040247; neisum -*-= feaiure2[6] *-1.106399; neisum += feaiure2[7] * -.2665701 ; neisum -+= feaiure2[8] * .2000748; ouiarray[9] = 1 /( I -t- exp(-neisum)); neisum = .253037; neisum feaiurc2[0] * .0630246; neisum feaiure2[l] * .3430368; neisum fealure2[2] * -.9319632; neisum -)-= feaiure2[3] * -.5230323; neisum -· = feaiure2[4] * .1765063; netsum  · = feature2[5] * -.2045487; neisum += feature2[6] * -1.250098; neisum -«-= feature2[7] * 4.259654E-02; neisum += feaiure2[8] *-.1516763; ouiarray[ 10] = 1 / (1 -i- exp(-neisum)); netsum = -.5481886; n e is u m fe a tu r e 2 [0 ] * .1021582; netsum += feature2[ 1] * .2943085; netsum  · = feature2[2] * .166375;

D6

ncisum -+ : = ic a lu r c 2 [ 3 ] netsum +  fe a lu r c 2 [4 ] netsum = fc a tu r c 2 |5 ] ncisum + : Ic a lu r c 2 |6 ] neisum + = fc a iu r c 2 |7 ] ncisum + = fe a iu r c 2 |8 ] ouiarray[ 1] = 1 /(W

* -.4570191; * - .25,356; * -.6339683; ' 2.116526; ' .342081; ' -.1411358; exp(-netsum));

neisum = -.1036133;  ncisum +=- fcaturc2|0] ' 1.514996; ncisum +=: feaiurc2[ 1] *-.5X60161; ncisum +== featurc2(2] * -.473146; ncisum +== feature]13] * -.5940139; netsum +== fcaiurc2[4] * .2028816; netsum +== featurc2|5] * -.1521696; ncisum + -= featurc2|6] * .1656087; neisum +== feaiurc2[7] * -.2721972; neisum += feature2[8] * .3705154; outarray[l 2] = 1 /(] + exp{-netsum)); -.2633717; neisum =  netsum += : feature2[0] netsum + - : feature2[l] netsum += : feature2[2] netsum += : feaiure2[3] netsum += : fealure2|4] netsum + - : feaiure2[5] netsum += feature2[6] netsum += : feaiure2[7] neisum += ' feature2[8] outarray[ 13 ] = 1 / ( H netsum = -.1 887729; netsum -*-= feature2[0] netsum -*-= feaiu re2[l] neisum + = feaiure2[2] netsum + = feaiure2[3] netsum -t-= feaiure2[4] netsum += feature2[5] netsum += feaiure2[6] netsum + = feaiure2[7] netsum += feature2[8] outarray[14] = 1 / (I +

* .9698912; * -.680374; * .3501442; ' -.4401273; * -.3729998; * -.5060975; * .1538693; * -.320682; * .2357144; exp(-netsum));

* -2.338608; * .5265903; * .0815384; * .7473056; *-.1 1 5 1 3 78 ; * .6119967; * -.3 6 1 6 3 2 8; * .5329512; *-.6 2 2 6 0 1 3 ; exp(-netsum));

outarray[0] = (ouiarray[0] - .1) / .8 ; i f (ouiarray[0]< 0) outarray[0] = 0; i f (outairay[0]> 1) outarray[0] = 1; ou tarray[l]= (ouiarray[ 1] - . 1) / .8 ; if (outarray[ 1]< 0) ouiarray[ 1] = 0; if (outarray[ I ]> 1) outarray[ I ] = I ; outarray[2] = (outairay[2] - . 1) / .8 ;
i f (outarray[2]< 0 ) outarray[2] = 0; i f (outarray[2]> 1) outarray[2] = 1;

outarray[3] = (outairay[3] - . 1) / .8 ; if (outarray[3]< 0) ouiarray[3] = 0; i f (outairay[3]> 1) outarray[3] = 1; outarray[4] = (outarray[4] - .1) / .8 ;

D7

il ( ( > u l i i r T a y [ 4 ]· il ( ( i u l a r i a \ [ 4 ] 

(I)o u l a r i a y l ^ J I )( i u l ; ; r r a y ( 4 ] -

U.

i.

ouiiirrayl5] = (outarrayl 5 ] - . 1 ) / ; if (outarraylS]-" 0)outarraylfi] = 0; if (outarrayjS]-'1 )outarray[5] = 1; outarray|6] = (ouiarray|6] - .1) / .8 ; if (outarray|6]< 0) outarrayl6] = 0: if(outarray|6]> I) outarrayl6] = I; outarrayl?] = (outarrayl?] - . ) ) / .8 : if (outarray|?]< 0) outarrayl?] = 0; if (outarray[?]> 1) outarrayl?] = 1; outarrayl 8 ]= (outarrayl 8] - .1) / .8 : if (outarray|8]< 0) outarrayl8] = (j; if (outarray|8]> 1) oularray|8] = 1; outarray|9]= (outarrayl9] - .1) / .8 : if (outarray|9]< 0) outarray|9] = 0; if (outarrayl9]> I) outarray[9] = 1; outarray] 10] = (outarray] 10] - . 1) / .8 ; if (outarrayl 10]< 0) outarrayl 10] = 0; if (outarrayl 10]> 1) outarrayl 10] = 1; outarrayl II ] = (outarrayl I! ] - . 1) / .8 ; if (outarrayl 11]< 0) outarrayl 11]= 0; if (outarrayl 11]> 1) outarrayl 11] = 1; outarrayl 12] = (outarrayl 12] - .1) / .8 ; if (outarrayl 12]< 0) outarrayl 12]= 0; if (outarrayl 12]> 1) outarrayl 12]= 1; outarrayl 13] = (outarrayl 13] - . 1) / .8 ; if (outarrayl 13]< 0) outarrayl 13] = 0; if (outarrayl 13]> 1) outarrayl 13]= 1; outarrayl 14] = (outarrayl 14] - .1 ) / . 8 ; if (outarrayl 14]< 0) outarrayl 14] = 0; if (outarrayl 14]> 1) outarrayl 14] = 1;

D8

Appendix D2: Neural W eights for Mean F low lim e network
/» Inscn ihis code mu. your C program lo fire theC:\NSHELL2'SPT&PTW IN0&LW KR network · /* This code is designed to he simple and fast for porting to any machine */ / Therefore all code and weights are inline without looping or data storage */ /* which might he harder to pon between compilers. A'includc <math.li> void neuro dispatchldouble *inarray, double ·outarrayl

t

double netsum; double fcature2(20]; /* inarrayl0] is B */ /* inarrayl I ] is C */ f* inarray[2] is D */ /* inarrayl 3] is E */ /* inarrayl4] is F */ /* inarraylS] is G */ /* inarraylô] is H */ /* inarrayl?] is 1 */ /* inarraylS] is J */ /* inarrayl9] is K */ I* inarrayl 10] is L */ /* inarrayl 1 1] is M */ /* inarrayl 12] is N */ /* inarrayl 13] is O */ /* inarrayl 14] is P */ /* outarraylO] is Q */ /* outarrayl I ] is R */ /* outarrayl2] is S */ /* outarraylS] is T */ /* outarrayl4] is U */ /* outarraylS] is V */ /* outarrayl 6] is W */ /* outarrayl?] is X */ /* outarraylS] is Y */ /* outarrayl9] is Z */ /* outarrayl 10] is AA /* outarrayl 11] is AB /* outarrayl ] 2] is A C /* outarrayl 13] is A D /* outarrayl 14] is A E

*/ */ */ */ */

if (inarraylO]< -33) inarraylO] = .33: if (inarrayl0]> 1) inarraylO] = 1; inarraylO] = (inarraylO] - .33) / .6?; if (inarrayl 1]< .3 1) inarrayl 1] = .31; if (inarrayl 1]> I) in a rra y ll]= I; inarrayl 1] = (inarrayl 1] - .3 1 ) / .69; if (inarrayl2]< .32) inarrayl?] ~ 32: if (inarrayl2]> 1) inarrayl?] = I; inarrayl?] = (inarrayl?] - .32) / .68: if (inarrayl3]< .31) inarrayl3] = .31; i f (inarrayl3]> 1) inarrayI3] = 1; inarrayl3] = (inarray[3] - . 3 1 ) / . 69; i f (inarrayl4]< .34) inarrayl4] = .34;

D9

if (inarray[4]> I) inarray|4] = 1; inarray(4] = (inarray[4] - .34) / ,66; i) (inarrayI5]< .49) inarraylS] = .49; if (inarray|5]> 1) inarrayjS] = 1; inarraylS] = (inarraylS] - .49) / .SI ; if (inarrayl6]< SI) inarrayl6] ^ -51; if (inarrayl 6]> I) inarrayl 6] = I; inarrayl6] = (inarrayl6] - . S I ) / . 49; il (inairay[7]-r .53) inarrayl?] = .53; if (inarrayl7]> I) inarrayl?] = inarrayl?] = (inarrayl?] - .S3) / .47; if (inarrayl8]< ,SS) inarraylS] = -55; if (inarrayl 8]> 1) inarraylS] = I: inarraylS] = (inarraylS] - .5 5 )/.4 5 ; if (inarrayl9]< .5?) inarrayl9] = .57; if (inarrayl9]> I) inarrayl9] = I; inarray]9] = (inarrayl9] - .57) / .43; if (inarrayl 10]< -22) inarrayllO] = .22; if (inarrayl 10]> 93) inarrayllO] = .93; inarrayl 10] = (inarrayllO] - .22) / .71; if (inarrayl I -25) inarrayl 1 '] = -25; if (inarrayl ' l]> -92) inarrayl 1 1] = .92; inarrayl 11] = (inarrayl 11] - 2 5 )/.6 7 ; if (inarrayl ' 2]< .22) inarrayl 12] = .22; if (inarrayl ' 2]> .96) inarrayl 12] = .96; inarrayl 12] = (inarrayl 12] - -2 2 )/.7 4 ; if (inarrayl 13]< -22) inarrayl 13] = -22; if (inarrayl ' 3]> .95) inarrayl ' 3] = .95; inarrayl 13] = (inarrayl i 3] - .22) / .73; if (inarrayl I4]< -23) inarrayl 14] = -23; if (inarrayl I4]> -96) inarrayl 14] = .96; inarrayll4] = (inarrayll4] - .23) / .73; netsum = -2.127025; neisum -»-= inarraylO] * -1.534661; netsum -+= inarrayl I ]* 1.385606; netsum - t- = inarrayP] * -1 .67123; netsum += inarrayP] * -.2299798: netsum inarrayl4] * 5.66486; netsum += inarraylS] * -.4372981; neisum -+= inarrayl6] * -.5978695; netsum - t- = inarrayl?] * -.5532722; netsum += inarraylS] * -.4884661 ; netsum += inarrayl9] * -.6174343; netsum -t-= inarrayllO] * -.3179735; netsum-t-= inarrayl 11] * -.7406918; netsum + =inarrayl 12] * -.3327855; netsum - t- = inarrayj 13] * -.5212398; netsum - t- = inarrayl 14] * -1.144423; feature210] = 1 / (I -t- exp(-netsum)); netsum = .1270918;

DIO

ncisum -f = narraylO] * 5.035597E -02; netsum -· = n a rra y ll]» 2.101648; netsum += narray|2] » -.3158018; netsum += narrayl3] » -.2156307; netsum += narray[4] * .2873424; netsum += narray|5] » -7 .6 8 7 3 16E-02; netsum +- narray|6] » -8.572415E-02; netsum += narrayl7] » -.2063143; netsum += narrayl 8] » -.2226138; netsum += narrayl9] » -.2139049; netsum += narrayl 10] * -.2817429; netsum += narrayl 11 ]* .1300127; netsum += narrayl 12] » 4.634594E-02; netsum += narrayl 13] *-8.277769E -02; netsum += inarrayl 14] * .2510398; features 11] = 1 / (1 + cxp(-netsum)); netsum = -. 3106934; netsum -r= narraylO] -.3552261; netsum += narrayl 1] .1258201; neisum + = narray[2] .2594771; neisum += narray[3] ' .1737312; netsum + = narray[4] -7.878974E -02; netsum -r- narrayl 5] -.1456324; netsum += narray[6] . 1960943; netsum -+= narrayl 7] .2037766; netsum += narrayl 8] ' -.2186606; netsum += narrayl9] * 5 .4 5 8 169E-02; netsum += narrayl 10] » .22 76493; netsum += narrayl 11] » .1183151; neisum + = narrayl 12] * .1211713; netsum -r= narrayl 13] * -5 .5 6 5 5 2 9 5 -0 2 ; netsum -r= narrayl 14] * -.2561574; features 12] = 1 /( I + exp(-netsum)); netsum = - 1 .454346; netsum - t - = narraylO] * -1.93679; netsum += narrayl 1] » -3.000002; netsum + = narray[2] * -1.563219; netsum += narray|3] * -1.450586; netsum + = narray|4] * 6.923056; netsum += narray[5] *-7.124248E -02; netsum -+= narray|6] * -.2007552; netsum + = narray[7] * 7.105684E -02; netsum += narray|8] *-.3 8 9 1 8 0 7 ; netsum += n a r r a y |9 ]* -9.084614E-02; netsum -+= narrayl 10] * -.2354679; netsum += narrayll 1] * .1472126; netsum + = narrayl 12] » -.3897824; netsum  + = narrayl 13] * -.4571439; netsum -t= narrayl 14] » -.9498448; featureS[3] = 1 / ( 1 -t- exp(-netsum)); netsum netsum netsum netsum netsum netsum netsum netsum netsum = += += += += += += += += 4064647; narrayl 0] narrayll] narrayl 2] narray[3] narray[4] narray[5] narray|6] narray[7]

* 6.080595E -02; * .5041848; * -8.277338E -02; * -4.359403; * .2567576; * -.1333682; * -.1788362; * -6.317507E -03;

Dl l

ncisum ·*= inarrayl8] * 3.I8I77IE -02; neisum += inarray(9] * -2.644823E-03; ncisum-r= inarrayl 10] * -.4072654; ncisum ·*= inarrayl 11] * .2850039: netsum+= inarrayl 12] * -1.406715E-02; ncisum -+= inarrayl 1 3) ' 1.495292; netsum += inarray] 14] * -6.633072E-03; fcalure2|4] = 1 / (1 + exp(-neisum)j; nct.sum = 3.044856; netsum += inarraylO] ' 2.98366: netsum inarray] 1] ' 9.195838E-02; netsum -+= inarray] 2] ' -.8917376; neisum + - inarray]3] ' -.5956516; ncisum -+= inarrayl 4] * .4019775; ncisum += inarray]5] ' -.2431124; neisum += inarray] 6] ' -.4644013; neisum += inarray[7] ' -.4797978; neisum += inarray] 8] ' 5.842389E-02; netsum -+= inarray[9] ' -.5646109; netsum  + =inarrayl 10] * -.4507366; netsum -+= inarrayl 11] * -.3701822; netsum -+= inarray] 12] * -4.062822E-02; neisum += inarray] 13] ' -.3810509; neisum + =inarray] 14] *-.4784395; feaiurc2[5] = 1 / (1 * exp(-netsum)); neisum = 4.163668; neisum += inarrayJO] * 4.272491; netsum + =inarray] 1] * -2.513001; neisum -+= inarray]2] * -2.495854; netsum += inarray[3] * -1.830051; netsum inarray]4] * -1.775271 ; netsum += inarray[5] ' -.3403684; neisum -+= inarray]6] * -.1819456; netsum += inarray]7] * -.2830323; neisum -* = inarray]8] * -.4010035; netsum+= inarray]9] * .247472; netsum += inarray] 10] ' -3.256461; netsum += inarray] 11] * -1.079243; netsum -t-= inarray] 12] * -.4956464; netsum + - inarray] 13] * -.6977581 ; netsum+= inarray] 14] *-.744369; feaiure2]6] = 1 / (1 + exp{-netsum));
netsum = -1 .0 6 4 9 0 7 ; netsum in array ]0] ' -1 .2 5 9 7 7 3 ; neisum -^= inarray] 1] * -1 .3 6 1 2 8 5 ; netsum + = in array ]2] ' -1 .6 3 3 7 8 2 ; netsum  + = in array ]3] * 4 .4 4 0 3 7 9 ; netsum + = in array ]4] · -1 .5 3 0 0 5 7 ; netsum + = in array ]5] · .0 2 3 4 7 7 ; neisum + = in array ]6 ] * -.5 0 0 1 9 0 1 ; neisum + = in array ]? ] * -.4 5 7 1 5 6 1 ; netsum + = in array ]8] * - 6 .4 8 3 1 5 5E -02; neisum + = in array [9] * .30 286 51 ; netsum + = inarray] 10] ' -.4 1 0 3 7 4 6 ; n e ts u m + = in array] 11] * .1 9 3 5 0 8 3 ; n e ts u m + = inarrayj 1 2 ] * ,2 0 6 3 5 5 5 ; netsum + = inarray] 13] * -.3 2 5 6 8 7 2 ; neisum + = inarray] 14] * -.2 3 3 9 5 1 1 ; fe a tu re 2 ]7 ] = 1 / (1 + exp(-netsum ));

D12

nc is u m =

.1 4 4 7 3 2 7 ;

n c is u m + = in a rra y lO ] » 1 .2 087 11 E -0 2 ; n e is u m + = in a r r a y [ l] * -1 .8 4 3 1 ; n e is u m != in a rr a y [2 ] » 9 .9 3 1 1 9 8 : n c is u m + = in a rr a y [3 ] * .7 8 5 9 2 2 1 ; n e is u m + = in a r r a y |4 ] * -.7 3 4 3 2 2 8 ; n e is u m + = in a r r a y j5 ] * -.5 4 3 7 3 3 5 ; n c is u m + = in a r r a y j6 ] * -.2 2 3 2 1 4 1 ; n c is u m + = in a r r a y j7 ] * -.4 1 3 9 3 1 4 ; n e is u m + = in a r r a y l8] * -3 .6 5 9 9 6 ; n e is u m + = in a rr a y ]9 ] * 1 .6 7 5 3 5 4 ; n c is u m + = in a rra y l 10] * -.2 9 0 1 0 7 2 ; n c is u m + = in a rra y l " ] * - 3 2 6 5 0 9 ; netsu m n e is u m n e tsu m + = in a rra y l 12] * -.4 6 3 6 9 5 ; in a rra y j 13 ] * -9 .9 8 2 1 81 E -0 2 ; in a rra y ] 14] » -.2 5 7 8 9 6 3 ;

ie a tu r c 2 [8 ] = 1 / ( I + e x p (-n e is u m ));

netsum = .6833475; netsum 4-= narray[0] .5996841; neisum 4-= narrayjl] 8.933963E-02; netsum 4-= narray[2] -.1969835; neisum 4 - = narray[3] .2354695; neisum 4 - = narrayl4] 2.379485; neisum 4 - = narrayjs] * -.6763925 neisum 4 - = narray[6] * -.4754021 neisum 4 - = narray[7] * 6.257617E-02; netsum 4 - = narray[8] * .5929466; netsum 4 - = narrayjo] * 7 .1 18178E-02; neisum 4 - = narrayl 10] ' -.4555093; neisum 4 - = narrayl 11 ] ' -.3480999; neisum 4 - = narrayl 12] ' -.2399379; neisum 4-= narrayl 13] -4.061316E -02; neisum + = narrayjl4] -5.547979E-02; feaiure2[9] = 1 / (1 4- exp(-netsum)); neisum = 2.05816; neisum 4-= inarraylO] * 2.053436; netsum 4-= inarrayl 1] * -.544432; neisum 4-= inarrayl2] * .4541343; neisum 4-= inarrayl3] * -.1915346; netsum 4-= inarrayl4] * -.3568715; netsum 4-= inarrayl5] * -.2190787; netsum 4-= inarrayl6] * -.5233611; neisum 4-= in arrayj] * -.274958; neisum 4-= inarrayj8] * -.3953035; neisum 4-= inarrayl9] * -.3348909; neisum 4-= inarrayl >0] * -.4108803; neisum 4-= inarrayl 11] * -.1989731; neisum 4-= inarrayj 12] * -.377194; neisum 4-= inarrayl 13] * 3.258267E-02; neisum 4-= inarrayl 14] * 8.858351 E-02; feature2110] = 1 / ( 1 4 - exp(-netsum)); neisum neisum neisum netsum netsum netsum neisum = -.4608839; 4-= inarraylO] 4-= inarrayl 1] 4-= inarrayI2] 4-= inarrayl3] 4-= inarrayI4] 4-= inarray[5]

* -.5784227; * -.4134966; * -.2498437; * 7.369254E-02; * ,4111851; * -.645071;

D13

ncisum += inarrayl6] * -.2638208; ncisum += inarrayl?] * -8.220226E-02; ncisum += inarrayl8] * .2338365. ncisum -t= inarray|9] * -.4312649: ncisum -+- inarrayl 10] * 1.318989; ncisum -1= inarrayl I I ] ' -.6365233; ncisum += inarrayl 12] * .390272; ncisum += inarrayl 13 ] ' 1.793994; ncisum + =inarrayl 14] * -.3162842; fcalurc2|l I] - I /( I -i cxp(-ncisum|):
ncisum = 2 .0 8 5 2 6 6 ; ncisum += in arraylO ] ' neisum + = in a rra y l2 ] ' ncisum + = in a rra y |3 ] ' 1.929439: 9 .1 13 4 0 3 E -0 2 ; 4 .3 0 I0 2 IE - 0 2 ; ncisum + = in arrayl I ] * .4223873:

neisum + = in a rra y |4 ] ' -5 .9 5 5 1 8 2 : ncisum + = in a rra y |5 ] ' - 1 .8 9 8 I4 2 E -0 2 ; neisum + = in a rra y |6 ] ' .2580843; neisum  + = in a rra y l7] * -.1 4 0 5 4 9 1 ; n e ts u m + = in a rra y |8 ] * .19 808 1; ncisum + = in a rra y |9 ] ' -6 .7 5 6 3 9 5 E -0 2 ; neisum + = in arrayf 10] * -2 .5 1 2 7 2 9 ; neisum -r= inarray] 11] * -1 .9 7 5 5 6 2 ; n e is u m + = inarrayl 12] * .29 059 51; neisum + = inarrayl 13] * .32 3 4 1 8 8 ; n e is u m + = inarrayl 14] * fe a iu r e 2 [l2 ] = 1.691879;

1 / ( I -i- exp(-neisum ));

neisum = -.1084178; neisum += narraylO] * -.2052653; neisum -+= narrayll]» .1929678; neisum -r= narray|2] * .2697195; neisum narray[3] * .7687581; neisum += narray[4] » -8.798962E-02; neisum += narray|5] * -.038018; neisum += narray|6] * -.1868947; neisum -+= narrayl?] » -.2239236; neisum += narrayl8] * -8.798546E-03; neisum += narray[9] * .1866033; neisum += narrayllO] » -4.679623E-02; neisum narrayll 1] '-5.749673E-02; neisum += narrayl 12]» 1.084791 E-02; neisum -+= narray[13]* 2.286516E-02; neisum -+= narrayl 14] » -.3621812; feaiure2[13] - 1 / (I -r exp(-nelsum)); neisum = - .368212; neisum -+= narraylO] * 1.516386; neisum -+= narrayl 1] » 1.312811; neisum -+= narray|2] * 3.834776; neisum + = nanay|3] * 1.949248; neisum += narray|4] » ·1.077547; neisum += narray|5] » 9.715545E-02; neisum += narray[6] * 2.381796E-02; neisum narrayl?] * .239951; neisum narrayl 8] * 9.391313E-03; netsum += narrayl 9] * - 5.044011 E-02; neisum += narrayl 10] .190827; neisum -+= narrayl 11 ] 4.736579E-02; neisum -+= narrayl 12] * -.2320667; netsum += narrayl 13] * -.1441375;

D14

neisum ^ = inarrayl 14] » -2.8520(J6E-02; Icaiurc2[l4] = ] / ( ] - cxp(-nen-um)); netsum = .6073152; ncisum * inarraylO] * -.2516953; netsum - inarrayj 1] * -.4811573: neisum + = inarrayl2] * .242735: netsum -+= inarrayj 3] * 1-989832: ncisum ^ = in a n a y |4 ] * -.609577; neisum ^ = inarrayjS] ' -.4903045: ncisum = inarrayj 6] * .3586577: netsum = inarrayj 7] ' -.0742056; neisum = inarrayj 8] '' -.1698011; ncisum = inarrayj9] *  5.020628E-02: ncisum - = inarrayj 10] * .2822423; ncisum - inarrayj 11] * -9 .9 2 4 1 12E-02; neisum ^ = inarrayj 12] * -.3541629: neisum = inarrayj 13] * -.1074304; neisum + = inarrayj 14] *-.2848581; fealure2[ 1 5 ] = 1 / ( I - exp(-netsum));
n e is u m = -.1 9 3 6 4 ; n e tsu m -t-= in a rra y jO ] * .2 2 1 5 0 9 4 ; n e is u m -^= in a rr a y j 1] * 2 .6 6 2 2 5 2 E -0 2 ; n e ts u m -t-= in a r r a y [2 ] * 4 .8 3 0 1 8 3 ; n e is u m -^= in a r r a y [3 ] * -.2 9 1 5 3 2 3 ; n e tsu m -+= in a r r a y [4 ] * -1 .9 6 7 8 5 6 ; n e is u m -t-= in a rr a y jS ] * n e is u m + = in a r r a y [7 ] * n e is u m + = in a r r a y j8] * n e ts u m -t-= in a rr a y j 10] * ne tsu m -t-= in a rr a y j 1 1] * .2 5 7 7 1 1 1 : .1 8 3 8 2 8 7 ; .0 4 5 8 1 ; .1 7 4 1 7 0 6 ; .3 0 4 1 2 9 7 ; n e tsu m + = in a r r a y [6 ] * -.1 6 7 8 6 2 5 :

n e tsu m -t-= in a r r a y j9 ] * -.1 4 1 6 8 5 9 ;

ne tsu m + = in a rr a y j 12] * -2 .2 0 9 : ne tsu m + = in a rr a y j 13 ] * -.3 3 1 4 4 9 7 ; ne tsu m + = in a rr a y j 14] * fe a tu r e 2 [1 6 ] = 1 .1 4 7 5 1 2 5 ;

/ ( I + exp( -n ets u m ));

netsum = -2.626755; netsum -t-= narrayjO] * -2.363863; netsum -*-= narrayjl] * 1.01529: netsum -* = narray|2] * -.213787: netsum -t-= narrayj3] * -.2568656: netsum -<  = narray|4] * -.1444943: netsum -i-= narrayjS] * .1128776; netsum -^= narray|6] * .1793339; netsum - = narray[7] *-1.73572SE -02; netsum += narrayjS] * -.303 7648; neisum -*-= narray[9] * -5.242693E -02; netsum -t-= narrayl 10] * .3987421: netsum -i-= narrayjl 1] * .4650992: netsum  + - narray[12] * -.34305: narrayjl3] * -.3603955; netsum netsum + = narrayjl4] * 6 .2 6 8 135E-02; feaiu re2jl7] = 1 / (1 + exp(-netsum)); netsum neisum netsum netsum netsum = .5671079; += inarrayjO] += inarrayj 1] -t-= inarrayj2] -+= inarrayj3]

* * * *

.7906501: -2.482796: .2383201; 1.422667;

D15

netsum += inarray[4] * 1.250559; netsum += inarrayl5] *-.2164256; netsum += inarrayl6] * -.2014819; ncisum  + = inarrayl?] * ,3662656; ncisum ·+= inarrayl8] * .1328816; ncisum  + =inarrayl9] * 3.565036E-02; netsum += inarrayl 10] * -.2044796; ncisum -t= inarrayl 11]* -.1553417; neisum += inarray] 12] * .6052962; ncisum-r= inarrayl 13] * .1387127: ncisum ·+= inarrayl 14] * -.1252561 ; fcalurc2| 18] = 1/ ( 1 + exp(-neisum));
ncisum = -.3 4 3 9 1 0 8 ; ncisum ·+= in array lO ] * -.2 6 2 2 7 9 9 ; n c is u m + = in a rra y l 1] * .66 1 9 8 5 9 ; neisum ·+= in a rra y [2 ] * -.2 7 0 0 4 5 2 ; neisum + = in a rra y [3 ] * 5 .9 9 9 9 1 5 E -0 2 ; ncisum  + = in a rra y |4 ] * 4 .3 0 9 0 7 9 ; neisum + = in a rra y [5 ] * -3 .4 6 4 2 2 3 E -0 2 ; neisum -+= in a rra y l6] * -.1 7 7 2 6 5 ; n e is u m -r= in a rra y l? ] * .2 3 7 7 1 9 8 ; n e is u m -r= in a rr a y l8] * 1.981383; ncisum -+= in a rra y l9] * 5 .5 3 8 4 4 3 E -0 2 ; n e is u m + = in array] 10] * 2 .1 5 1 3 2 7 E -0 2 ; n e i s u m i n a r r a y l 11] * .1 7 8 4 5 9 1 ; n e is u m -r= in array l 12] * .28 0 4 0 2 1 ; n e is u m + = in array l 13] * .1 0 7 3 7 9 7 ; n e is u m + = in array l 14] * 1.5 7 6 1 3 4 E -0 2 ; fe a iu rc 2 |1 9 ] = 1 / ( 1 + exp(-netsum ));

neisum = -.4018244; neisum += feaiure2|0] * -.6004633; neisum-r= feaiurc2|l] * .1079664; neisum -+= feaiure2|2] * -5.970221 E-02; neisum -+= feaiure2|3] * -.2646531; neisum -+= feaiure2|4] * -.8192776; neisum ·+= feaiure2|5] * -.2910616; neisum += feaiure2|6] * 3.688971; neisum -t-= feaiure2|7] * -2.120626; neisum += feaiure2|8] * .5011812; neisum += feaiure2[9] * -.2401061; neisum += feature2| 10] *-.1371416; neisum -+= feaiure2[l 1] * .3884444; netsum += feature2[12] » -3.652697E-02; neisum += feaiure2[ 13] * -.4143871; neisum ·+= feaiure2[14] * -1.290449; netsum ·+= feaiure2|15] * .1108165; neisum -t-= featuré2[16] » -.3153532; neisum 4-= feaiure2[17] * 9.063054; neisum += feaiu;e2[18] * .2038849; neisum += feaiure2[19] * -.3027286; ouiarray[0] = 1 / (1 + exp(-neisum)); neisum neisum neisum neisum neisum neisum neisum netsum = .3021354; += feaiure2|0] -r= feaiure2[l] += feaiure2[2] -t^= feaiure2[3] += feature2|4] += feaiure2|5] += feaiure2(6]

* .5562754; * -3.721453E-02; * .1274372; * -.2567498; * -.7406126; * -.5004629; * -1.118358;

D16

ncisum + = Icaiure2j7] * -.<120098: ncisum ·+= icaturc2|8] * .3554352; netsum -+= fcalurc2|9] * -.2985365: ncisum 4= fcaturc2[H)] * -.5600451: netsum += fcaturc2| 11] » .1336511; neisum += fcaturc2[ 12] * -.37)0997: netsum += featurc2113] * -7.074618E-02: n etsu m += fcaturc2[ 14] * .834084; netsum += )cature2| 15] * .5622373; netsum += feaiure2[16] * -.2662526; netsum-t-= fcature2[ 17] * 5.237437: netsum += fcaturc2| I 8] * -.3949004; netsum += featurc2[ 19] * -.3496082; outarray] 1] = 1 / (1 + exp(-netsum)); netsum = -2.538008E -02; netsum -+= feature2(0] * -1.063961 E-02; netsum += Ieaturc2[l] * .1183521; netsum += feature2[2] * .1714737; netsum += feature2[3] * .3209237: netsum += feature2[4] * 1.463832; netsum += feature2[5] * .2554799; netsum += feature2[6] * -3.548389; netsum += feature2[7] * 1.80822: netsum += feature2[8] * -.590492: n etsu m += feature2[9] * .289912; n etsu m += feature2[ 10] * .9830801; netsum += feature2f 11] * -.5915321; netsum += feature2[12] * .3082871; netsum += feature2[13] * 4.501532E-02: netsum += feature2[14] * .7100854; n etsu m -1-= feature2[15] * 3.106604E-02: netsum += feature2[16] * -.1417924; netsum += feature2[17] * -16.6156; netsum += feature2[18] * -7.406253E-02; n etsu m += feature2[19] * -.1195654; outarray[2] = 1 / (1 + exp(-netsum));
netsu m = - 4 .3 4 7 9 0 8 ; n etsu m + = fe a tu re 2 [0 ] * -4 .4 7 9 0 7 ; netsu m + = fe a tu re 2 [ 1] * n etsu m + = fe a tu re 2 [2 ] * netsum + = fe a tu r c 2 [3 ] * netsu m + - fe a tu re 2 [4 ] * 1 .4 3 2 9 4 7 ; .5 7 1 1 1 2 ; .6 0 5 5 5 9 7 ; .3 7 5 2 8 5 1 ;

n e ts u m + = fe a tu re 2 [5 ] * -.1 6 8 3 2 6 5 ; netsu m + = fe a tu re 2 [6 ] * -.7 0 2 5 5 7 1 ; netsu m + = fe a tu re 2 [7 ] * -1 .9 6 4 3 3 5 ; netsum + = fe a tu re 2 [8 ] * -.3 3 9 6 9 1 8 ; netsu m + = fe a tu re 2 [9 ] * n e ts u m + = fe a tu r e 2 [l 1] * netsu m + = fe a tu re 2 [1 2 ] * 6 .7 5 8 4 7 7 E -0 3 ; 1 .8 8 2 6 7 6 ; .1 5 4 3 7 7 2 ; netsu m + = fe a iu r e 2 f 10 ] * -.3 0 6 9 9 5 5 ;

netsu m + = fe a tu re 2 [1 3 ] * -.3 0 9 0 7 5 1 ; netsum + = fe a tu r e 2 [1 4 ] * -2 .5 2 0 8 7 2 ; netsu m + = fe a tu re 2 [1 5 ] * - 1 .2 6 1 7 2 7 E -0 2 ; netsu m + = fe a tu re 2 [1 6 ] * netsum -*-= fe a tu re 2 [ 1 7 ] * netsum -*·= fe a tu re 2 [1 9 ] * 3 .6 6 1 9 7 2 E -0 3 ; 1 1 .6 5 4 5 4 ; 3 .6 6 3 8 0 5 ;

netsu m + = f e a t u r e 2 [ l8 ] * -1 .1 6 9 3 9 1 ; o u ta rra y [3 ] = 1 / { I + e x p (-n e ts u m ));

netsum = .770401;

D17

ncisum -^ = feaiurc2[0] * .991416; fcaiurc2( I] *-1.889331; ncisum fcaiurc2(2] * -.139139; netsum ncisum ->= fcalurc2|3] * .4393654; netsum += fcaiure2[4] * -.5469683; ncisum -*= fcaiurc2[5] * -.3776391; ncisum += fcaiurc2[6] * -.1153614; ncisum += featurc2[7] * .454)5; ncisum += feaiurc2[8] * 5 .2 4 4 106E-02; ncisum += feaiure2[9] * .2061416; ncisum -*·= fcaiurc2[ 10] .1450569; ncisum -*= fcaiurc2[ 11] -1.786922; ncisum ~ = fcalure2[l2] -.4290357; ncisum -+= fcalurc2( 13] -.3400771; ncisum -r= fcaiurc2[ 14] .3615972; ncisum -+= fcaiurc2[ 15] 3.215213E-Ü2; neisum -+- feaiurc2[ 16] .2886165; neisum += fealure2[ 17] -8.394811; ncisum += feaiure2[18] .9913402; neisum -*·= feaiurc2[ 19] -.3295543; ouiarray[4] 1 / ( 1 + exp(-neisum)) ncisum = 3.686605; ncisum +=' fcaiure2[0] * 3.340854; netsum +=fealure2[l] * -.3266241; ncisum + - feaiure2[2] * -.383816; neisum += feaiure2[3] * -.9007248; neisum += feaiure2[4] * 2.365861 E-02; ncisum += feaiure2[5] · -1.179796; neisum += feaiure2[6] * 3.917684 E-02; neisum += feaiure2[7] * 1.091974; neisum += feaiure2[8] * .1532727; neisum += feaiure2[9] * .2134319; netsum += feaiure2[ 10] * .1552497; neisum += feaiure2[l1] ' -.130137; neisum += feaiure2[12] ' .1949993; netsum += feaiure2[ 13] ' -.2376275; netsum += feaiure2[ 14] * 1.101267; neisum += feature2[15] * -.0493302; neisum += feature2[16] ' -.2057998; neisum += feaiure2[17] *-12.09187; netsum += feature2[ IS] * -9.530596E-02; neisum += fealure2[19] *-2.167364; ouiarray[5] = 1/ ( 1 + exp( -netsum)); neisum neisum neisum neisum neisum neisum neisum neisum neisum neisum neisum neisum netsum neisum neisum neisum neisum = .2261838; += feaiure2[0] * .6117795; += feaiure2[l] * .2556338; += feaiure2[2] * .1104424; += feaiure2[3] * 1.76116; += featurc2[4] * - .8814707; += feature2[5] * .1399774; += feature2[6] * .1515178; += feature2[7] * .2513995; += feature2[8] * .2032537; += feature2[9] » .2282047; += feaiuTe2[10] * .1178242; += feaiure2[ 11] * 9.989554 E-02; += feature2[12] * -.1187363; += feature2[13] * -.1588543; += fealurc2[14] » 8.299998; += feature2( 15] * -.4917489;

D 18

neisum ncisum netsum ncisum

*= 1'caiurc2( 16] * -1.272974 -= fcaiurc2[17] * -4.948971 += l'caturc2| 18]  .2373703 - = (eaiutc2| 19] * -.1382861 oularray[6]= 1 / (I + ex p( -neisum)) netsum = .7143841; n e ts u m += feaiurc2[0] * .9115762; ncisum += featu re2[l] * -7.350495E -02; netsum += featurc2[2] * -.4 156601; netsum += fcature2[3] * -.7382054; netsum *= feature2|4] * -.5534458; netsum feature2[5] * .1255922: netsum -^= feature2[6] * .4917387; neisum feature2[7] * .1844895; neisum += feaiure2[8] * -.5002103; netsum += feature2[9] * -4.216522E -02; netsum += feature2[10] * -.4226763; n e tsu m += fcature2[l 1] * .1202922; netsum += feature2[12] * -.1485279; netsum += feature2[13] * -.1823302; netsum += featurc2[ 14] * -1.62964; netsum -+= feature2[15] * -.2875343; netsum += feature2[16] * -.3986559; netsum += feature2[17] * -3.04541; netsum += feature2[18] * -1 .760078E-02; netsum += feature2[ 19] * -.4441942; outarray[7] = 1 / (I + exp(-netsum)); netsum = -1.106611 ; netsum += feature2[0] * -1.257366; netsum += featu re2[l] * -5.262233E -02; n e ts u m += feature2[2] * .1283339; netsum += feature2[3] * 1.897849; netsum += feature2[4] * 1.263269; netsum += feature2[5] * -.0826937; netsum += feature2[6] * -4 .6 6 3 1 33E-02; netsum += feature2[7] * .1077188; netsum-i-= feature2[8] * .9141859; netsum += feature2[9] * -1.942981 E-02: netsum += feature2[10] * -.0170227; netsum += feature2[l 1] * -.5775023; netsum += feature2[12] * 6.083321 E-02; netsum += feature2[13] * .44481; netsum += feature2[14] * -7.26222; n e tsu m += feaiure2] 15] * .454815: n e tsu m += feature2[ 16] * 1.172065; n e ts u m += feature2[ 17] ' 6.179174; netsum += feature2[18] * -.2194533; netsum += feature2[19] * 2 .9 2 8 195E-03: outarray] 8] = 1 / ( 1 + exp(-netsum)): netsum = -.557344; netsum += feature2[0] netsum += feature2[ 1] netsum += feature2[2] netsum += fealure2[3] netsum += feature2[4] netsum -t-= fealure2[5] netsum -t-= feature2[6] netsum-i-= feature2[7] netsum += feaiure2[8]

* -.8355444; * -.7298361 ; * -9 .9 0 8 194E-02: * -1.338743; * 2.726421; * -.3369063; * -1.602735; * 5.736315; * .4491995;

D19

n cisum += reaiurc2(9] * -.26771; ncisum += fcniurc2| 10] ' -.3346169; ncisum -· = fcaiurc2[ I I ] ' .1424053; ncisum -1= fc;iiurc2[ 12] ' -.2673492; ncisum -+= fcaiurc2( 13] * 9.440368E-02; ncisum += fcaiurc2|l4] ' -1.682431; ncisum -+= fcaiurc2| 15] ' -6.7I9103E-02; ncisum fcaiurc2( 16] * -.2724702; ncisum += f'caiurc2( 17] ' -8.343706; ncisum -! = feaiurc2[ 18] ' .4701097; ncisum  + =feaiurc2[ 19] ' -4.028329E-02; ouiarray[9] = I / ( M cxp(-nclsum)); ncisum - .7294881; n c i s u m f c a i u r c 2 | 0 ] ' .478992; ncisum f'caiurc2( I] * -.1699279; ncisum -+= Ieaiurc2|2] ' -.2918522; ncisum *  =fcalurc2[3] * -.1400667; ncisum + = fcalurc2[4] * .3066254; neisum 4-= fealurc2[5] * ,2507746; ncisum += feaiurc2[6] * -.719865; ncisum += fealurc2[7] * -1.083979; ncisum 4-= feaiurc2[8] * -.3014532; neisum 4-= fealurc2[9] * -.300752; neisum 4-= fealure2[IO] * 2.493552E-02 neisum 4-= feaiure2[11] * -5.258249E-02 neisum 4-= feaiure2[l2] ' 7.411258E-02 neisum 4-= fcalure2[13] * -.1965154; neisum 4-= fealure2[l4] * .4011763; neisum 4-= feaiure2[l5] * -1.669259; neisum 4-= fealure2[l6] ' -3.424472E-02; neisum 4-= feaiure2[ 17] * -7.968295; neisum 4-= feaiure2[l8] * -.602005; neisum 4-= fealure2[l9] * -6.404935E-02; outarray] 10] = I / (I 4- exp(-neisum)); neisum = .362788; neisum 4-= feaiure2[0] * .0981926; neisum 4-= feaiure2[l] » 2.843576E-02; neisum 4-= feature2[2] * -.1703628; neisum 4-= fealure2[3] * .8380809; neisum 4-= fealure2[4] · -2.949265; neisum 4-= feaiure2[5] * .2401737; neisum 4-= feaiure2[6] * 1.392454; neisum 4-= feaiure2[7] * -5.736626; neisum 4-= fealure2[8] * -.3045191; neisum 4-= fealure2[9] * 7.248948E-02; neisum + = fealure2( 10] *-.2216617; neisum 4-= fealure2[l 1] * 5.055316E-03; neisum 4-= feaiure2[12] » .1508938; neisum 4-= fealure2[13] * .1570011; neisum 4-= feaiure2[14] * .8849996; neisum 4-= feaiure2[15] * 1.165793; neisum 4-= feaiure2[ 16]* .1073198; neisum 4-= feaiure2[ 17]* 10.06857; neisum 4-= fealure2[ 18] * 2.068069E-02; neisum 4-= feaiure2[19] * .2028647; ouiarray] 11] = 1 / (1 -f exp(-neisum)); neisum = .8200681; netsum 4-= feature2[0] * .9755294; netsum 4-= feature2]l] * -.2646338;

D20

netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum netsum neisum netsum netsum neisum

feaiure2[8] * -9.658501 E-Ü3; featurc2|9] * -.6434142; fcaiure2110] * -.3066177; fcature2[l 1] = * - 1.086938E-02; feature2[12] = * .9212445; 4- = fcaturc2[ 13] * -.5968158; 4-= fcaiurc2[14] = * -1.807871; 4-= = ieaiure2[15] * 3 .3 9 8 166E-02; 4 -= u re2[]6] * .2398897; 4-= ure2[ 17] * -5 .1 9 4 3 2 3 ; 4-= ure2[18] » -.1639087; 4-= urc2(19] * -.1660768; outarrayl 1 2 ] = 1 / ( 1 + exp(-neisum)): netsum = -3 .7 1 4 1 14E-04; netsum -t-= fealure2(0] * -.3926134; neisum += featu re2[l] * -.4836594; netsum += feaiure2[2] * 2 .0 1 1262E-02; netsum += feature2[3] * -.6448963; netsum + = feaiure2[4] * -.2702846; netsum += feature2[5] * -3.63141 lE -02; netsum += feature2[6] * -.6800619; netsum -+= feature2[7] * -.5383763; netsum += feature2[8] * -9.367502E-03; netsum -t-= feature2[9] * -.4512094; n e tsu m += feature2[ 10] * . 7111861; netsum -t-= feaiure2[l 1] * -1 .2 3 8 4 5 4 ; netsum-t-= feature2[ 12] * .7630368; netsum += feaiure2[13] * 5.750604E -02; netsum += feature2[14] * -.9760057; netsum  + =featurc2[15] * .3790655; netsum -t-= feature2[16] * -5.644364E-02; netsum += feature2[17] * -.5015465; netsum += feature2[ 18] * -.4474064; netsum += feature2[19] * -.3726536; outaTTay[ 13] = 1/ ( 1 -t- exp(-netsum)); netsum = -1.24 7646; netsum -+= feature2[0] * -1.071091; netsum += featu re2[l] * -2.893961 E-02; netsum += feature2[2] * -2.861611 E-02; netsum -t-- feature2[3] * -3.618598; netsum += feaiure2[4] * .6013362; netsum += feature2[5] * .1153496; n etsu m += feature2[6] * 1.41837; netsum + = feature2[ 7] * 1.492391 ; netsum -t-= feature2[8] * -.2125802; netsum feature2[9] * 1.506482; netsum -+= feature2[10] * .3234671; n e tsu m += feature2[l 1] * .3145125; netsum += feature2[12] * -1.559603; netsum += feature2[13] * 7 .9 9 0 149E-02; netsum += feature2[ 14] * 2.460018; netsum += feature2[15] * .2015774; netsum += feature2[16] * -.3996591; n e tsu m += feature2[ 17] * 7.581014;

4-= ++= += += += 4-= += += += +=

featurc2|2] fcaturc2[3] feature2[4] icature2|5]

* -.3214362; » 3.809073; * -.6226831; = * -8.820433E-02;

D21

neisum += fealurc2[ I >i] * .205i5034; netsum += feaiure2( 19] " .2459512; outarrayl 14] = 1 / (I -* cxp(-netsum));

outarrayl 9] = (ouiarraylü] - .1 ) / .8 ; if(outarray[0]< 0) outarray|0] = 0; if (outarray|0]> 1) outarray|0] = 1; outarrayl I] = (outarrayl I] - .1 ) / . 8 ; if (outarrayl l]-^ 0) outarrayl I] = 0: if (outarrayl 1]> 1) outarrayl 1] = I; outarray|2]= (outarray|2] - . I ) / ,8 ; if (outarray|2]< 0) outarray|2] = 0; if (outarray|2]> 1) outarray|2] = I ; outarrayl3 ]= (outarray|3] - . ) ) / . 8 ; if (outarray|3]< 0) outarray|3] = 0; if (outarray|3]> 1) outarray|3] = I; outarray|4]= (outarray|4] - .] ) / .8 ; if (outarray|4]< 0) outarray|4] = 0; if (outarray|4]> 1) outarray|4] = 1; outarrayl5 ]= (outarray|5] - .1 ) / .8 ; if (outarray|5]< 0) outarray|5] = 0; if (outarray|5]> 1) outarray|5] = 1; outarrayl6] = (outarray|6] - .1 ) / .8 ; if (outarray|6]< 0) outarray|6] = 0; if (outarray|6]> ! ) outarray[6] = 1 ; outarrayl?] = (outarray|7] - .1 ) / .8 ; if (outarray|7]< 0) outarray|7] = 0; if (outarray|7]> 1) outarrayl?] = 1; outarrayl8] = (outarray|8] - . 1) / . 8 ; if (outarray|8]< 0) outarray|8] = 0; if (outarray[8]> 1) outarrayjS] = 1; outarrayl9] = (outarray|9] - .1) / .8 ; if (outarray|9]< 0) outarrayl9] = 0; if (outarray|9]> 1) outarrayl9] = 1; outarrayl 10] = (outarrayl 10] - .1) / .8 ; if (outarrayl 10]< 0) outarrayl 10]= 0; if (outarrayl 10]> 1) outarrayl 10]= I ; outarrayl 11]= (outarrayl 11 ] - . 1) / .8 ; if (outarrayl 11 ]< 0) outarrayl 11] = 0; if (outarrayl 11 ]> 1) outarrayl 11 ] = 1; outarrayl 12] = (outarrayl 12] - . 1) / .8 ; if (outarrayl 12]< 0) outarrayl 12] = 0; if (outarrayl 12]> 1) outarrayl 12] = 1; outarrayl 13] = (outarrayl 13] - . 1) / .8 ; if (outarrayl 13]< 0) outarrayl 13]= 0; if (outarrayl 13]> 1) outarrayl 13]= 1 ; outarrayl 14] = (outarrayl 14] - .1) / .8 ; if (outarrayl 14]< 0) outarrayl 14] = 0; if (outarrayl 14]> 1) outarrayl 14]= 1;

t I

D22

Appendix E l: -Comparison o f B P N N with makespans from optimal njle combinations for n= 10 to 100
n=10 N 10M 1 N10M2 N10M3 N10M4 N10M5 N10M6 N10M7 N10M8 N10M9 N10M10 N10M11 N10M12 N10M13 N10M14 N10M15 N10M16 N10M17 N10M18 N10M19 N10M20 N 10M 21 N10M22 N10M23 N10M24 N10M25 Optimal 845 8 19 842 762 604 815 723 862 868 766 800 827 849 658 620 801 555 773 535 872 757 592 6 29 811 693 B PNN 845 819 880 762 604 815 753 862 899 766 931 828 849 658 626 801 555 319 535 878 757 592 641 901 693 n=10 N10M26 N10M27 N10M28 N10M29 N10M30 N10M31 N10M32 N10M33 N 10M34 N 10M35 N10M36 N10M37 N10M38 N10M39 N10M40 N10M41 N10M42 N10M43 N10M44 N10M45 N10M46 N10M47 N10M48 N10M49 N 10M50 Optimal 701 872 604 793 747 834 722 547 847 734 786 822 785 763 540 694 743 731 690 722 923 777 716 843 723 BPN N 712 890 661 793 865 838 781 567 922 774 8 25 825 795 811 573 733 770 736 690 722 958 820 737 862 731 1 n=15 N15M1 N15M2 N15M3 N15M4 N15M5 N15M6 N15M7 N15M8 N15M9 N15M10 N 15M 11 N 15M12 N15M13 N15M14 N15M15 N15M16 N15M17 N15M18 N15IVI19 N15M20 N15M21 N15M22 N15M23 N 15M24 N15M25 Optimal 1221 1151 1127 1014 1184 1238 1216 1205 1103 1078 1175 1124 781 887 1141 1083 1293 1218 1089 1169 974 1067 844 1093 1042 BPNN 1272 1151 1127 1014 1184 1248 1232 1286 1103 1088 1281 1124 781 926 1226 1089 1293 1286 1165 1228 994 1067 852 1129 1122 n=15 N15M26 N15M27 N15M28 N15M29 N15M30 N 15M 31 N15M32 N15M33 N15M34 N15M35' N15M36 N15M37 N15M38 N15M39 N15M40 N 15M 41 N15M42 N15M43 N15M44 N15M45 N15M46 N15M47 N15M48 N15M49 N15M50 Optimal 1235 1272 1081 1125 1055 1030 1064 1040 1146 1184 1185 1141 1295 1273 1218 1140 957 1199 1116 1159 9 79 1299 8 76 1155 1246 BPNN 1235 1463 1111 1125 1113 1051 1102 1040 1146 1190 1219 1166 1295 1285 1218 1140 957 1280 1170 1207 979 1300 918 1181 1246

El

n=20 N20M1 N20M2 N20M3 N20M4 N20M5 N20M6 N 20 M 7 N20M8 N20M9 N20M10 N20M11 N20M12 N20M13 N20M14 N20M15 N20M16 N20M17 N20M18 N20M19 N20M20 N20M21 N20M22 N20M23 N20M24 N20M25

Optimal 1532 1426 1167 1566 1476 1710 1772 1468 1455 1639 1137 1643 1476 1194 1351 1617 1435 1462 1496 1441 1327 1445 1270 1059 1288

B PNN 1532 1426 1354 1566 1490 1748 1781 1468 1455 1800 1196 1802 1476 1199 1351 1617 1474 1462 1654 1441 1327 1474 1270 1059 1304

n=20 N20M26 N20M27 N20M28 N20M29 N20M30 N20M31 N20M32 N 20M33 N20M34 N 20M35 N20M36 N20M37 N20M38 N20M39 N20M40 N20M41 N20M42 N20M43 N20M44 N20M45 N20M46 N20M47 N20M48 N20M49 N20M50

Optimal 1450 1196 1492 1368 1200 1634 1482 1508 1740 1441 1115 1386 1557 1446 1493 1313 1476 1665 1482 1336 1626 1624 1592 1490 1534

BPNN 1450 1196 1492 1370 1307 1641 1482 1543 1827 1441 1167 1386 1557 1448 1493 1313 1476 1666 1627 1336 1697 1682 1592 1490 1534

n=25 N25M1 N25M2 N25M3 N25M4 N25M5 N25M6 N25M7 N25M8 N25M9 N25M10 N25M11 N25M12 N25M13 N25M14 N25M15 N25M16 N25M17 N25M18 N25M19 N25M20 N 25M 21 N25M22 N25M23 N25M24 N25M25

Optimal 1982 1646 1870 1949 1903 1680 1838 1978 2023 1727 1817 1810 1373 1980 1693 2019 2058 1889 1788 1950 1985 1864 1611 1769 1709

B PNN 2028 1654 1870 1949 1903 1680 1838 1978 2023 1727 1890 1810 1439 1980 1693 2019 2058 1889 1899 1950 1985 1864 1611 1769 1776

n=25 N25M26 N25M27 N25M28 N25M29 N25M30 N25M31 N25M32 N25M33 N25M34 N25M35 N25M36 N25M37 N25M38 N25M39 N25M40 N 25M 41 N25M42 N25M43 N25M44 N25M45 N25M46 N25M47 N25M48 N25M49 N25M50

Optimal 1598 2003 2015 1538 2033 2042 1878 2021 1556 1656 1894 1708 1501 1846 1824 1826 1586 1817 1743 1880 1900 1587 1877 1695 1612

BPNN 1598 2003 2039 1538 2050 2042 1878 2021 1556 1656 1895 1708 1542 1864 1824 1881 1586 1817 1743 1888 1900 1587 1877 1695 1612

E2

n=30 N30M1 N30M2 N30M3 N30M4 N30M5 N30M6 N30M7 N30M8 N30M9 N30M10 N30M11 N30M12 N30M13 N30M14 N30M15 N30M16 N30M17 N30M18 N30M19 N30M20 N30M21 N30M22 N30M23 N30M24 N30M25

Optimal 2270 209 7 1788 1971 2280 2229 2016 2426 2219 1778 2113 2149 2171 2151 2053 2009 2104 2367 2227 2251 2049 2359 2300 2444 1650

B PN N 2332 2097 1788 1971 2280 2385 2016 2675 2219 1790 2166 2149 2171 2151 2053 2045 2130 2367 2356 2273 2049 2359 2300 2444 1650

n=30 N30M26 N30M27 N30M28 N30M29 N30M30 N30M31 N30M32 N30M33 N30M34 N30M35 N30M36 N 30 M 3 7 N30M38 N 30 M 3 9 N30M40 N30M41 N30M42 N30M43 N 30 M 4 4 N 30 M 4 5 N30M46 N30M47 N 30 M 4 8 N 30 M 4 9 N30M50

Optimal 2254 2325 2 28 2 2051 2136 1981 1973 2047 2238 2046 2456 2271 2181 2238 2172 2377 2076 2278 2005 2347 2132 2078 2302 2150 1964

B PN N 2254 2325 2282 2057 2136 2075 1973 2047 2238 2050 2456 2271 2319 2299 2309 2377 2091 2391 2005 2628 2192 2078 2391 2252 1964

n=35 N35M1 N35M2 N35M3 N35M4 N35M5 N35M6 N35M7 N35M8 N35M9 N35M10 N35M11 N35M12 N35M13 N 35 M 1 4 N35M15 N35M16 N 35 M 1 7 N35M18 N 35 M 1 9 N 35 M 2 0 N35M21 N35M22 N35M23 N 35 M 2 4 N 35 M 2 5

Optimal 2787 2561 2 74 5 2361 2727 2707 2602 2768 2666 2622 2749 2334 2716 2743 2482 2459 2654 2308 2585 2670 2308 2254 2555 2725 2405

B PN N 2793 2561 2789 2361 2727 2743 2772 2768 3099 2622 2749 2355 2716 2743 2482 2459 2654 2308 2585 2670 2308 2254 2555 2725 2405

n=35 N35M26 N35M27 N35M28 N35M29 N35M30 N 35M 31 N35M32 N35M33 N35M34 N35M35 N35M36 N35M37 N35M38 N35M39 N35M40 N35M41 N35M42 N35M43 N35M44 N35M45 N35M46 N35M47 N35M48 N35M49 N35M50

Optimal 2741 2712 2595 2020 2808 2705 2218 2062 2643 2544 2654 2066 2623 2569 2241 2636 2397 2123 2592 2496 2604 2192 2255 2 61 3 2705

BPNN 2741 2712 2 59 5 2020 2808 2705 2218 2174 2685 2544 2654 2166 262 3 2569 2241 2636 2502 2129 2749 2496 2604 2192 2255 2613 270 5

E3

n=40 N40M1 N40M2 N40M3 N40M4 N40M5 N40M6 N40M7 N40M8 N40M9 N40M10 N40M11 N40M12 N40M13 N40M14 N40M15 N40M16 N40M17 N40M18 N40M19 N40M20 N40M21 N40M22 N40M23 N40M24 N40M25

Optimal 2797 2621 3079 3040 2674 2852 2111 2153 2670 2364 2952 2314 2445 2693 2078 3062 2 63 6 2706 3112 2843 2900 2232 3059 2710 2512

BPNN 2797 2621 3079 3040 2752 2862 2111 2183 2670 2364 2952 2314 2445 2693 2241 3062 2636 2727 3112 2843 2900 2232 3059 2713 2512

n=40 N 40 M 2 6 N 40M27 N 40M28 N 40 M 2 9 N40M30 N40M31 N40M32 N40M33 N40M34 N40M35 N40M36 N40M37 N40M38 N40M39 N40M40 N40M41 N40M42 N40M43 N40M44 N40M45 N40M46 N40M47 N40M48 N40M49 N40M50

Optimal 3033 3072 3252 2882 2999 3120 2664 3065 2901 2981 2789 3080 2511 2722 1979 2321 3003 3107 3101 2944 3264 2953 3254 2388 2829

BPNN 3033 3072 3456 2882 3005 3120 2681 3065 2906 2981 2789 3138 2511 2722 1979 2321 3003 3123 3101 2944 3450 2976 3254 2566 2829

n=45 N45M1 N45M2 N 45 M 3 N45M4 N45M5 N45M6 N 45 M 7 N45M8 N45M9 N45M10 N45M11 N45M12 N45M13 N45M14 N45M15 N45M16 N45M17 N45M18 N45M19 N45M20 N45M21 N45M22 N45M23 N45M24 N45M25

Optimal 3674 3348 3 29 8 2883 3251 3456 2899 3446 3206 2484 303 4 2532 2953 3256 372 7 3399 3060 2749 342 5 3060 345 6 3268 3580 3469 2504

BPNN 4104 3374 3298 2883 3251 3456 289 9 3446 3206 2484 3034 2638 2953 3310 3792 3415 3060 2749 342 5 3060 345 6 326 8 428 9 346 9 2553

n=45 N45M26 N45M27 N45M28 N45M29 N45M30 N45M31 N45M32 N45M33 N45M34 N45M35 N45M36 N45M37 N45M38 N45M39 N45M40 N45M41 N45M42 N45M43 N45M44 N45M45 N45M46 N45M47 N45M48 N45M49 N45M50

Optimal 3274 3301 2902 3531 3297 2846 3599 3394 3047 3763 3135 2829 3382 322 8 3676 2927 2894 328 0 2917 3 40 0 2771 3410 3374 346 8 328 3

BPNN 345 5 3301 2902 3531 3297 2931 3637 3400 3047 3922 3158 2829 3382 322 8 367 6 2927 2894 3352 2917 3400 2781 384 5 3374 3481 328 3

E4

n=55 N55M1 N55M2 N55M3 N55M4 N55M5 N55M6 N55M7 N55M8 N55M9 N 55M10 N55M11 N55M12 N55M13 N55M14 N55M15 N55M16 N55M17 N55M18 N55M19 N55M20 N55M21 N55M22 N55M23 N55M24 N55M25

Optimal 4111 4382 3 78 6 3898 4084 3742 3956 3531 4015 3625 4113 3964 4413 3586 4321 3771 3950 4205 4281 3670 3920 3461 4282 3600 4451

BPNN 4111 4407 3786 4090 4084 4037 3970 3531 4015 3625 4113 3981 4531 3586 4698 3906 4060 4205 4281 3670 3920 3461 4282 3600 4760

n=55 N605M26 N55M27 N55M28 N55M29 N55M30 N 55M 31 N55M32 N 55M33 N 55M34 N55M35 N55M36 N55M37 N55M38 N55M39 N55M40 N55M41 N 55 M 4 2 N55M43 N55M44 N55M45 N55M46 N 55 M 4 7 N55M48 N55M49 N55M50

Optimal 3706 4209 3855 4124 4135 4088 3734 4282 4303 3761 4225 3919 3914 4060 3487 3820 3984 3955 4325 3961 4256 4011 4220 3768 4106

BPNN 3706 4209 3855 4124 4138 4173 3753 4282 4303 3761 4225 3919 3914 4060 3487 3820 3984 3955 4325 3965 4256 4011 4220 3768 4106

n=60 N60M1 N60M2 N60M3 N60M4 N60M5 N60M6 N60M7 N60M8 N60M9 N60M10 N60M11 N60M12 N60M13 N 60 M 1 4 N 60 M 1 5 N60M16 N60M17 N60M18 N60M19 N60M20 N60M21 N60M22 N60M23 N 60 M 2 4 N 60 M 2 5

Optimal 4001 4 55 6 4562 4197 3927 4801 3915 4523 3733 4697 3876 4546 4148 4585 3186 4599 4225 4443 4135 4060 4770 4460 3591 4304 4574

B PN N 4001 4 55 6 4562 419 7 392 7 4801 3915 4523 3733 4697 3 87 6 4546 4148 4585 3186 4599 4225 4490 4135 4060 4770 4460 3703 4304 4574

n=60 N60M26 N60M27 N60M28 N60M29 N60M30 N 60M 31 N60M32 N60M33 N60M34 N60M35 N60M36 N60M37 N60M38 N60M39 N60M40 N60M41 N60M42 N60M43 N60M44 N60M45 N60M46 N60M47 N60M48 N60M49 N60M50

Optimal 450 0 421 8 3348 4 75 5 4669 4 13 4 4556 4406 3492 4 58 3 3821 4292 4451 4260 4 65 9 4558 424 3 4 53 5 4 54 7 4660 393 5 465 7 4552 4274 437 9

BPNN 450 0 421 8 3348 4811 466 9 4134 4 55 6 4420 3495 4583 3821 4292 4451 426 0 4659 4627 424 3 453 5 4773 4660 3935 4657 4552 4441 4 37 9

E5

n=75 N75M1 N75M2 N75M3 N75M4 N75M5 N75M6 N75M7 N75M8 N75M9 N75M10 N75M11 N75M12 N75M13 N75M14 N75M15 N75M16 N75M17 N75M18 N75M19 N75M20 N75M21 N75M22 N75M23 N75M24 N75M25

Optimal 5568 5536 5246 4936 5544 5820 4 02 6 5602 5890 5433 5794 5503 5153 5244 475 0 5737 5 94 7 5712 5 47 9 5364 5870 4 26 5 561 9 569 2 547 0

B PNN 5568 5969 5246 4936 5907 6161 4055 5602 5890 5433 5794 5503 5153 5244 5270 5737 5974 5712 5479 5881 5881 4265 5650 5692 5470

n=75 N75M26 N75M27 N75M28 N75M29 N 75 M 3 0 N75M31 N 75 M 3 2 N75M33 N75M34 N75M35 N75M36 N75M37 N75M38 N 75M39 N75M40 N75M41 N75M42 N75M43 N75M44 N75M45 N75M46 N75M47 N75M48 N75M49 N75M50

Optimal 5264 5836 4985 5962 3991 5220 5792 5161 5413 5526 5372 5155 4822 5823 5714 5215 5706 5154 5815 4914 4 34 3 5651 5427 577 9 557 8

B PNN 5264 5836 4985 5962 3991 5220 5792 5161 5436 5526 5372 5157 5010 5823 5714 5215 5751 5154 6093 4 91 4 4343 5651 5427 5779 5578

n=85 N85M1 N85M2 N85M3 N85M4 N85M5 N85M6 N 85 M 7 N85M8 N85M9 N85M10 N85M11 N 85M12 N 85M13 N 85 M 1 4 N85M15 N85M16 N85M17 N85M18 N85M19 N85M20 N85M 21 N85M22 N85M23 N85M24 N85M25

Optimal 5950 5819 6523 6274 5745 6642 5748 6781 5835 6740 5526 6446 6242 5973 6231 6485 5512 6 10 0 6 34 5 6 44 7 5815 6 60 3 6513 5571 605 6

BPNN 5950 5819 6523 6274 5745 6642 5806 6962 5835 6740 5526 6446 6242 5973 6231 6485 5512 6100 634 5 644 7 5815 660 3 651 3 610 8 6056

n=85 N85M26 N85M27 N85M28 N85M29 N85M30 N85M31 N85M32 N85M33 N85M34 N85M35 N85M36 N85M37 N85M38 N85M39 N85M40 N85M 41 N85M42 N85M43 N85M44 N85M45 N85M46 N85M47 N85M48 N85M49 N85M50

Optimal 6316 645 7 5559 5553 6054 5892 6046 5660 5871 6243 6169 6412 6272 6 26 3 4396 6 25 9 6206 6 56 4 6 72 7 5081 5842 6706 5983 4801 6138

B PNN 631 6 645 7 555 9 5553 6054 589 2 6 04 6 5660 5871 624 3 6169 6504 6 27 2 6 26 3 4464 625 9 620 6 6564 6741 5081 584 2 670 6 598 3 4801 6138

E6

n=100 N 100M 1 N100M 2 N100M 3 N100M4 N100M 5 N100M6 N100M7 N100M8 N100M9 N100M10 N 100M 11 N100M12 N100M13 N100M14 N100M15 N100M16 N100M17 N100M18 N100M19 N100M20 N 100M 21 N100M22 N100M23 N100M24 N 100M25

Optimal 5 37 9 7571 7187 6 44 5 7727 7382 7605 7616 6 86 6 7940 7810 6931 7705 7183 6204 6479 5926 6189 7839 7360 6 12 4 6901 6 86 2 7667 627 6

BPNN 5849 7571 7324 6445 7727 7408 7605 7616 6 86 6 7952 7810 6931 7705 7183 6204 6479 5926 6189 7839 7388 6124 6901 6862 7667 6 27 6

n=100 N100M26 N100M27 N100M28 N100M29 N100M30 N 100M 31 N100M32 N100M33 N100M34 N100M35 N100M36 N100M37 N100M38 N100M39 N 100M40 N100M 41 N 100M42 N100M43 N100M44 N100M45 N100M46 N100M47 N100M48 N100M49 N100M50

Optimal 7279 7116 6991 5654 7737 6685 7244 7410 742 7 6858 7497 7739 7571 7709 5503 7707 7178 6310 7210 5799 7897 7037 7212 7526 7244

BPNN 7279 7116 6991 565 4 7737 6 68 5 7244 7411 7427 7113 7497 7739 7571 7709 5516 7707 7178 6491 7210 5799 7901 7037 7212 7526 7244

E7

Appendix E2: -Comparison o f BPNN with Mean Flowtimes for optimal rule combinations for n=10 to 100
n=10 N10F1 N 10F 2 N 10F 3 N 10F 4 N 10F 5 N 10 F 6 N 10 F 7 N 10 F 8 N 10 F 9 N 10 F 10 N10F11 N10F12 N10F13 N10F14 N10F15 N10F16 N10F17 N10F18 N10F19 N10F20 N10F21 N10F22 N10F23 N10F24 N10F25 Optimal 560.5 518 559.2 539.7 400.8 599.7 510.7 6 21.2 6 05.9 5 34.6 5 69.5 5 35.4 583.1 475.1 4 64 .7 536.7 4 04 .7 5 54 .6 394.1 633 508.2 4 03 .4 387.1 ' 558 5 00.4 BPNN 563.6 554.6 569.1 588.6 409.4 599.7 519.8 633 605.9 534.8 579.9 562.7 591.5 479.9 469.2 536.7 4 19 .6 573.4 406.2 648.8 583 4 17.9 3 97.7 590.8 511.6 n=10 N10F26 N 10 F 27 N10F28 N10F29 N10F30 N10F31 N 10F 32 N 10 F 33 N 10 F 34 N10F35 N 10 F 36 N10F37 N10F38 N10F39 N10F40 N10F41 N 10 F 42 N10F43 N 10 F 44 N10F45 N10F46 N10F47 N10F48 N10F49 N10F50 Optimal 471.2 671.3 445.7 502.6 533.3 585.1 525.6 371.2 590.2 514.7 592.3 539.8 560.4 523.1 391.1 4 53.5 546.3 531.4 461.2 508.9 6 41.7 560.6 524 582 4 89 .6 BPN N 475.3 708.3 487.7 512.2 544.9 595.8 555.7 386.5 619.6 544.1 612.1 550.8 591 527.5 391.1 459 555.5 531.4 503.2 522.5 6 58.9 5 60.6 5 41.5 6 09.2 4 97.4 n=15 N15F1 N15F2 N 15 F 3 N15F4 N 15F 5 N 15F 6 N15F7 N 15F 8 N 15F 9 N15F10 N15F11 N15F12 N15F13 N15F14 N15F15 N15F16 N15F17 N15F18 N15F19 N15F20 N15F21 N15F22 N15F23 N15F24 N15F25 Optimal 7 82.69 678 .67 754.87 6 23 .13 750.53 770.13 774.13 8 16.87 674 .67 701.33 709.2 6 56 .47 4 9 2 .1 3 536.4 7 13.53 6 8 7 .0 7 7 98.13 741.2 6 8 8 .9 3 762.47 6 2 5 .8 7 6 5 9 .7 3 5 64.6 7 21.33 6 3 3 .8 7 B PN N 794.67 6 79 .27 781.67 631.2 756.53 776.67 783.73 8 23.33 702.93 707 732.4 656 .47 515.2 5 3 8 .5 3 720.6 6 98 .87 834.07 7 44.2 720 .13 800.6 6 46 .47 6 8 4 .3 3 5 6 8 .9 3 744 .13 6 5 6 .7 3 n=15 N15F26 N15F27 N15F28 N15F29 N15F30 N15F31 N15F32 N15F33 N15F34 N15F35 N15F36 N15F37 N15F38 N15F39 N15F40 N15F41 N15F42 N15F43 N15F44 N15F45 N15F46 N15F47 N15F48 N15F49 N15F50 Optimal 791.67 798.8 6 7 0 .5 3 6 5 9 ,3 3 704 .13 6 4 9 .3 3 7 37.73 6 87.6 770.67 672 5 8 6 .5 3 6 00.8 6 12.4 819 6 5 1 .1 3 707 .33 799 .13 5 94.2 7 21.6 6 3 2 .6 7 7 1 6 .1 3 726 .47 5 0 9 .1 3 558 .07 8 0 7 .9 3

1

BPN N 8 0 7 .7 3 8 1 4 .3 3 689.4 6 59 .33 748.67 6 4 9 .3 3 7 37.73 7 07.27 7 57 .73 798 .93 758.2 7 32.07 7 99.6 7 8 5 .9 3 8 3 0 .6 7 7 13 .67 6 3 7 .6 7 816 .67 6 6 9 .3 3 7 75 .87 6 4 6 .9 3 8 15 .2 583 731.4 8 26 .6

n=20 N20F1 N20F2 N20F3 N 20 F 4 N20F5 N20F6 N20F7 N20F8 N20F9 N20F10 N 20F11 N20F12 N20F13 N20F14 N20F15 N20F16 N20F17 N20F18 N20F19 N20F20 N 20F21 N20F22 N20F23 N20F24 N 20F25

Optimal 9 33 .35 8 96 .5 7 05 .75 8 64 .75 8 27 .85 9 69.2 1052,3 5 8 24 .4 8 48 .05 1 013.83 7 10.45 1000.9 5 8 81 .3 747.4 7 66.55 9 66 .95 870.6 8 57.15 917.7 8 59.3 7 46.75 9 07 .95 743.8 6 40.7 8 02 .25

BPNN 9 4 5 .9 5 8 97.8 7 38.45 8 67.45 8 52.7 1 050.05 1052.3 5 8 3 0 .1 5 923.05 1051.9 713.55 1018.2 5 9 32.2 747.4 767.15 9 76,65 É70.6 8 59.45 917.7 873.9 753.7 9 32.35 743.8 649.3 8 76.2

n=20 N20F26 N20F27 N20F28 N 2CF29 N 20 F 30 N20F31 N20F32 N20F33 N 20 F 34 N 20 F 35 N20F36 N 20 F 37 N20F38 N20F39 N20F40 N20F41 N 20 F 42 N20F43 N 20 F 44 N20F45 N20F46 N20F47 N20F48 N20F49 N20F50

Optimal 8 6 7 .2 5 703.8 923.1 8 26 .55 777.1 9 5 2 .3 5 902.9 862 1019.8 8 23 .55 6 60 .6 7 80.35 9 73 .85 8 19.2 870.7 756.4 8 58.35 1014.4 908.6 787.4 1034.4 5 8 60.7 9 10.55 9 00.35 9 03.35

B PN N 8 7 6 .1 5 703.8 923.1 8 46.6 8 03 .95 955.3 950.9 866.3 1053.4 8 23.55 6 65 .75 787.9 9 83.55 8 21 .55 8 82 .85 7 67.15 8 58.35 1051.8 910.9 808.5 1064.1 873.2 9 10.55 900.35 910.9

n=25 N25F1 N25F2 N 25 F 3 N 25F 4 N25F5 N 25 F 6 N25F7 N 25F 8 N 25 F 9 N25F10 N25F11 N25F12 N25F13 N25F14 N 25 F 15 N25F16 N25F17 N25F18 N 25 F 19 N25F20 N25F21 N 25 F 22 N25F23 N 25F 24 N25F25

Optimal 1186 9 61.2 1025.92 1142.0 8 1107.4 4 9 7 2 .0 8 1101.2 1082.12 1148.6 8 1023.4 1025.56 9 79 .28 8 20 .08 1191 9 67 .72 1145.16 1144.44 1029.08 1031.52 1096.28 1094 1006.1 6 9 33.92 9 9 1 .7 6 1034.84

BPNN 1187.4 9 7 4 .4 8 1 026.92 1210.16 1125.52 9 7 2 .0 8 1106.0 4 1092.4 8 1160.12 1038.7 6 1025.5 6 992.44 837.04 1226.6 9 67.72 1147.64 1163.16 1075.64 1034.12 1096.2 8 1101 1014 9 33 .92 9 91 .76 1044.8

n=25 N25F26 N 25F27 N25F28 N25F29 N25F30 N25F31 N 25F32 N25F33 N25F34 N25F35 N25F36 N25F37 N25F38 N25F39 N25F40 N25F41 N25F42 N25F43 N 25 F 44 N25F45 N25F46 N25F47 N25F48 N25F49 N25F50

Optimal 1 B P N N 9 6 3 .1 6 1 152.88 1112 8 2 4 .9 6 1185.56 1083.92 1 08 0 .9 2 1259.16 8 66.6 9 5 8 .7 2 9 8 9 .7 6 9 96 .12 8 46 .08 101 8.7 6 1000.5 2 1041.1 6 908.8 9 46 .68 1036.6 1056.16 1090 9 4 8 .7 2 1058 9 51 .84 8 98 .56 9 66.4 1 16 8 .3 6 1120.12 8 27 .2 1 209.96 1098.64 1086.4 1 297.28 870 9 70.8 1030 1002,0 4 8 80 .6 1 028.96 1 000.68 1056.2 4 9 27 .16 9 99 .04 1055.48 1 109.72 1135.52 9 79.84 1082.72 9 58.4 9 0 9 .3 2

E9

n=30 N30F1 N30F2 N30F3 N30F4 N30F5 N30F6 N 30F 7 N 30F 8 N 30 F 9 N30F10 N 30F11 N30F12 N30F13 N30F14 N30F15 N 30F16 N30F17 N30F18 N30F19 N30F20 N 30F21 N30F22 N30F23 N30F24 N30F25

Optimal 1221.53 1087.5 1 045.73 1127.37 1251.67 1258.93 1 058.77 1289.8 1292.3 9 7 0 ,6 7 1132.1 1165.13 1130 1184.73 1116.33 1100 1219,7 1 362.57 1 270.97 1 343.87 1113.0 3 1306.07 1229.43 1 37 4.8 7 9 3 2 .1 7

BPN N 1234.8 1092,3 1174.5 7 1127,37

n=20 N 30 F 26 N 30 F 27 N30F28

Optimal 1216,9 1230,47 1264,03 1099

B PNN 1228,13 1234,4 1266.67 1099 1192.7 1128.4 1172.33 1149.87 1277.67 1197.93 1349.87 1205.57 1192.17 1266.5 1195.83 1342 1229.03 1251.37 1065.47 1316.83 1180.23 1 111.5 1 288.9 1161.77 1076.3

n=35 N35F1 N 35 F 2 N 35 F 3 N 35F 4 N 35 F 5 N35F6 N 35 F 7 N35F8 N 35 F 9 N35F10 N35F11 N 35 F 12 N 35 F 13 N 35 F 14 N35F15 N35F16 N35F17 N35F18 N35F19 N35F20 N35F21 N 35F22 N 35F23 N 35F24 N35F25

Optimal 1455.8 6 1369.2 1 530.29 1302 1537.71 1492,11 1545.17 1 449.03 1 393.49 1431.09 149 1.4 9 1 273.09 1497.54 1472.49 1318.34 1329 136 7,8 9 1298.31 1382.11 1 421.09 1 20 9.4 6 1 24 1.6 6 137 2.7 4 1 44 7 .2 9 1 320.83

1

B PN N 1463.5 4 1377.2 1575.51 1306.31 1557.34 1528.6 3 1545.1 7 1462.8 9 1430.4 1460.2 9 1512.31 1308.4 1527.3 7 1481.06 1325.09 1365.71 1373.83 1348.09 1384.29 1540.4 1209.46 1 286.06 1391.91 1 456.03 1 320.83

n=25 N35F26 N35F27 N35F28 N35F29 N35F30 N 35F31 N35F32 N35F33 N35F34 N35F35 N35F36 N35F37 N35F38 N35F39 N35F40 N35F41 N35F42 N35F43 N35F44 N35F45 N35F46 N 35F47 N 35F48 N 35F49 N35F50

Optimal 1547.09 1390.77 1466.71 1062.49 1507.9 7 1452.63 1129.31 1043.17 1482.43 1 337.23 1567 1184.31 1 353.23 1 381.89 1 214.06 1483.97 1303.6 1 13 6.2 3 1443.6 133 4.0 6 1 414.83 1 14 9 .2 6 1262.11 140 0.8 6 147 1.1 4

BPNN 1551.91 1 390.77 1480.31 1 070.43 1507.97 1 464.26 1 135.46 1049.11 1 521.03 1363.14 157 0.0 6 1 199.46 1353.23 1455.71 1260 1517.6 1 36 0.2 9 1138.4 1 45 9.3 7 139 0.0 3 1422.31 1 159.4 130 5.9 7 1469,4 1 48 9 .6 9

1270.87
1334.3 3

N30F29 N30F30
N30F31 N 30F 32 N 30F 33 N 30 F 34 N30F35 N30F36 N 30 F 37 N 30 F 38 N 30 F 39 N30F40 N30F41 N30F42 N30F43 N 30 F 44 N30F45 N30F46 N 30F47 N30F48 N30F49 N30F50

1189.37
1128.33 1166.37 1127.9 1277.6 7 1168.63 1337.2 1205.5 7 1147.43 1262.7 1194.5 1306.77 1189.57 1239,2 1 065.47 1292.5 3 1141.07 1089.97 1236.2 1148.43 1076.3

1062.97 1305.67
1294.5 9 70.67 1168.67 1176.0 7 1148.23 1206 1116.8 1110.5 1250.6 3 1368.73 1328.4 1350.8 7 1113.03 130 6.1 7 1235.97 1388.67 9 38 .43

E tO

n=40 N40F1 N40F2 N40F3 N40F4 N40F5 N 40 F 6 N 40 F 7 N 40 F 8 N 40 F 9 N40F10 N40F11 N 40F12 N 40F13 N40F14 N40F15 N40F16 N40F17 N40F18 N40F19

Optimal 1497.88 1441.12 1 579.18 1633.62 1419.3 1487.9 1062.43 1210.72 1367.9 1216 1479.68 1224.03 1279.03 1435.1 8 1117.43 1693.2 1410.0 9 1475.7 3 1649.0 3 1554 1509.62

BPNN 1529.0 3 1457.1 2 1579.18 1647.25 1505.28 1603.5 1062.43 1248.15 1380.8 1216 1481.12 1271.3 1281.3 1448.3 5 1132.47 1716.4 5 1457.0 3 1502.85 1671.22 1585.2 1532.5 1261.8 5 1568.57 1540.7 1357.8

n=40 N40F26 N40F27 N40F28 N40F29 N40F30 N40F31 N40F32 N40F33 N 40 F 34 N40F35 N40F36 N40F37 N40F38 N40F39 N 40 F 40 N40F41 N 40 F 42 N 40 F 43 N 40 F 44 N 40 F 45 N 40 F 46 N40F47 N 40 F 48 N 40 F 49 N 40 F 50

Optimal 1535.4 1664.8 1685.53 1 578.85 1604.07 1629.6 1512.6 2 1638.4 5 1504.0 7 1638.35 1551.5 1729.6 5 1211.12 1407.9 1058.75 1 199.78 1579.95 1632.35 1633 1540.3 5 1766.22 1552.55

BPN N 1543.82 1697.7 1699 1628.7 1776.4 3 1629.6 1547.3 2 1719.32 1506.7 1716.32 1603.8 1763.22 1223.6 1434.5 3 1065.72

n=45 N45F1 N 45 F 2 N45F3 N 45 F 4 N 45F 5 N45F6 N 45 F 7 N 45 F 8 N 45 F 9 N 45 F 10 N45F11 N45F12 N 45 F 13 N 45 F 14 N45F15 N45F16 N45F17 N 45 F 18 N45F19 N 45 F 20 N45F21 N 45 F 22 N 45 F 23 N 45 F 24

Optimal 2 0 2 5 .5 8 1745.09 1638.73 1464.33 1 727.36 1826.71 1508.8 1914.7 8 1623.09 1371.76 1 661.07 1358.7 8 1639.4 1732.96 1862.67 1758.11 1655.44 1370.3 6 1835.5 3 1605.04 1823.4 4 1696.71 1920.98 1799.69

BPNN 2 0 5 3 .7 3 1771.33 1642.69 1 464.33 1729.22 1849.0 4 1527.11 1947.49 1624.98 1378.56 1721.3 8 1424.3 6 1644 1757.8 1892.04 1799.3 8 1697.91 1376.64 1849.7 3 1617.78 1843.6 7 1712.38 1956.6 4 1809.8 7 1384.47

n =4 5 N45F26 N45F27 N45F28 N45F29 N45F30 N45F31 N45F32 N45F33 N45F34 N45F35 N 45F36 N45F37 N45F38 N45F39 N45F40 N45F41 N45F42 N45F43 N 45 F 44 N45F45 N45F46 N 45F47 N45F48 N45F49 N45F50

Optimal 171 9.5 3 1779.11 1530.4 1 827.78 169 9.3 6 1607.87 1852 1 839.73 1605.84 1986.38 1 590.53 1 495.73 1661.2 4 1689.51 1 979.67 1611.31 150 9.6 9 1 740.67 1566.33 1821.58 1 491.69 1 872.42 1867.93 1875.16 1725.31

BPNN 1741.91 1792.64 1540.64 1827.78 1729.6 1677.02 1 875.42 1845.76 1651.5 3 1986.3 8 1594.31 1530.24 1 669.29 1689.73 2 02 8.3 3 1642.44 1571.18 1788.6 1615.44 1831.2 2 1527.8 1877.51 1922.84 1895.2 1744.11

1202.78
1590.5 1668.97 1649.25 1540.35 1821.53 1554.35 1726.82 1216.7 1410.88

N40F20 N40F21
N40F22 N40F23 N40F24 N40F25

1198.72
1567.7 1516.2 8 1340.5

1726.55
1214.72 1410.88

N 4 5 F 2 5 1 1 384.47

El l

n=55 N55F1 N 55 F 2 N55F3 N55F4 N 55 F 5 N 55F 6 N 55 F 7 N 55F 8 N 55F 9 N55F10 N55F11 N55F12 N55F13 N55F14 N 55F15 N55F16 N55F17 N55F18 N55F19 N55F20 N55F21 N55F22 N55F23 N55F24 N55F25

Optimal 2 0 9 7 .3 6 227 2.0 9 1949.6 4 2 103.84 2 154.02 1980.65 1970.5 3 1766.71 2 062.84 1892.9 3 2187.91 2 02 8.4 5 2 375.44 1936.8 2334.51 2 04 6.6 7 1 959.47

BPNN 2 13 0.8 9 2 288.35 1949.6 4 2115,02 2 159.02 1980.6 5 2040.47 1766.71 2070.75 1892.93 2211.98 202 8.4 5 247 5.1 6 2016.31 2 35 6 .6 9 211 1.5 3 1972.11 2 18 0.5 8 2 28 3 .2 4 1 967.02 1954.8 1925 2 24 4.1 6 1857.55 2 3 7 9 .0 4

n=55 N 55 F 26 N55F27 N 55 F 28 N 55 F 29 N 55F 30 N55F31 N 55 F 32 N 55 F 33 N 55F 34 N 55 F 35 N55F36 N 55F 37 N55F38 N55F39 N55F40 N55F41 N55F42 N55F43 N 55 F 44 N55F45 N 55 F 46 N55F47 N55F48 N55F49 N55F50

Optimal 1942.71 2272.96 2063.78 214 1.9 3 2 137.78 2 201.27 1970.8 7 2272.6 2114.49 1920.04 2 281.33 2 04 6 .8 5 1945.4 2 08 5.2 2 1751.87 1959.18 1940.84 1954.76 2 290.8 2104.71 2 216.45 205 9.3 8 2321.11 190 0.2 7 2 08 3.0 2

B PNN 2 02 5.4 5 2 352.35 211 3.9 8 224 0.3 8 2 137.78 2212.6 1990.71 2 279.85 2 117.05 1924.13 2 281.33 2 087.64 1947.93 2 086.2 1773.6 1960.8 5 1972.9 8 1970.87 2 294.85 2104,71 2 21 6.4 5 2060.58 2 35 1.4 9 1900.27 2 083.02

n=60 N60F1 N 60 F 2 N 60F 3 N 60F 4 N 60F 5 N 60 F 6 N60F7 N 60F 8 N 60F 9 N 60F 10 N60F11 N 60 F 12 N60F13 N 60 F 14 N60F15 N60F16 N60F17 N60F18 N60F19 N60F20 N 60F21 N60F22 N60F23 N60F24 N60F25

Optimal 2 08 6 .9 3 223 6.1 3 240 0.2 8 2 14 7.4 8 2 054.78 2 465.25 1 922.17 2 20 4 .2 8 1910.1 7 2463.3 1925.38 232 0.5 2 2 20 1.5 224 7.5 5 1 718.22 225 0.0 2 2 27 8.2 8 2 4 3 4 .2 5 2 13 6.5 7 1966.78 2 3 9 7 .0 2 2 27 8 .2 8 1 921.82 2 1 4 1 .7 7 2 38 3 .4 7

1 B PN N 2 12 6.2 2238.17 240 0.2 8 215 0.0 3 205 9.4 8 247 1.8 3 1922.1 7 2 204.28 2031.88 2 475.5 1 925.38 2320.52 2 31 2.0 3 2 24 7.5 5 1737.6 2 2 258.02 2 27 9.2 2 502.22 214 7.6 2 1966,78 2 3 9 7 .4 3 2 34 4.5 1952.1 2 1 4 3 .7 2 40 1.1 7

n=60 N60F26 N 60 F 27 N60F28 N60F29 N60F30 N60F31 N60F32 N60F33 N60F34 N60F35 N60F36 N60F37 N60F38 N60F39 N60F40 N60F41 N60F42 N60F43 N60F44 N60F45 N60F46 N60F47 N60F48 N60F49 N60F50

Optimal 2 2 4 2 .7 3 2 10 1 .7 2 1724.43 236 8.5 8 2 50 7.3 2 216 9.5 7 2 31 8.5 3 2 38 6 .3 5 1 88 5 .1 5 2 26 0.5 7 2011 2 16 3 .8 2 2 3 1 2 .5 3 2 1 9 7 .2 5 2 3 7 3 .0 8 2 4 2 8 .4 8 2256 2 3 6 1 .2 2 2225 2 2 9 4 .4 8 1973.8 2 4 0 6 ,6 8 2357 2 2 7 4 .2 8 2 2 7 2 .9 8

B PN N 2 2 4 4 .6 3 2 11 6 .6 2 1 760.8 2 38 0 255 5.7 7 223 9.8 2 242 9.4 2 2 47 7.6 1 902.03 2 262.43 2 01 2 .0 7 2 1 6 4 .8 2 47 4 .5 2 2 1 9 9 .2 8 2 3 8 6 .2 2 2 4 3 8 .7 8 2335.1 2 36 3.1 7 2 2 2 7 .6 8 2 2 9 6 .5 2 1 974.53 2 4 1 6 .0 8 2 3 7 8 .8 8 2 3 0 9 .0 3 2 4 8 4 .6 2

2180.58
2 2 2 7 .6 5 1934.2 1 95 0.0 2 192 4.7 3 2 25 2.8 7 1857 2 3 5 8 .5 5

E12

n=75 N75F1 N75F2 N75F3 N75F4 N75F5 N75F6 N75F7 N 75 F 8 N75F9 N75F10 N 75F11 N75F12 N75F13 N75F14 N75F15 N75F16 N75F17 N75F18 N75F19 N75F20 N75F21 N75F22 N75F23 N75F24 N75F25

Optimal 2 8 5 1 .1 6 2888.11 2 7 4 8 .1 7 2511.31 2 8 9 0 .3 9 2 9 6 9 .5 9 2 1 3 8 .5 9 2887 293 1.5 5 2 832.8 2 9 8 7 .3 7 2 734.88 270 0.9 9 2 603.77 2494.01 2 908.52 2986 2805.21 2 69 8.6 7 2 769.4 3 02 1.5 5 208 8.4 9 302 1.5 5 2 932.59 2 834.67

BPNN 2 9 9 1 .1 7 2 9 0 1 .8 8 2 7 6 6 .1 3 2541.51 2 998.4 3 03 0.8 5 2 14 1.0 4 2887 2 97 4 .9 6 2 85 4 .6 9 2 99 3 ,4 8 2 735.2 2 730.56 2612 2528 3 025.49 3003.8 2805.21 269 8.6 7 2 793.55 3129.35 2088.49 2880.2 2956.39 2954.33

n=75 N75F26 N75F27 N75F28 N75F29 N75F30 N 75F31 N75F32 N 75 F 33 N75F34 N75F35 N75F36 N75F37 N75F38 N75F39 N75F40 N75F41 N75F42 N75F43 N75F44 N75F45 N75F46 N75F47 N 75 F 48 N 75 F 49 N 75 F 50

Optimal 276 6.8 7 301 3.0 7 257 0.7 6 3 01 8.7 7 1921.05 269 9.7 9 2 848.44 2 56 5.6 2 69 4 .8 9 280 4.8 4 2 67 8.8 8 2617.49 2 482.28 302 2.6 5 2 823.95 264 2.8 3 2 881.43 2 592.92 2848.63 239 1.9 5 2172.41 273 6.4 5 2582.29 2 949.23 272 2.4 5

BPNN 288 4,3 9 3061.61 258 8.5 6 3 02 7.7 3 1924,57 2 71 9.0 8 2 848.65 2 620.24 269 4.8 9 2 804.84 268 6.3 2 2 695.67 2 584.56 3 022.65 2 828.83 2712.24 288 3.4 9 2 60 2.9 5 2882.16 2396.99 2 175.57 2736.45 2583.37 304 8.2 7 2722.89

n=85 N85F1 N85F2 N85F3 N 85F 4 N 85 F 5 N 85 F 6 N 85 F 7 N 85F 8 N85F9 N85F10 N85F11 N85F12 N85F13 N85F14 N85F15 N85F16 N85F17 N85F18 N85F19 N85F20 N85F21 N85F22 N85F23 N 85 F 24 N85F25

Optimal 3 04 7 .9 8 2 96 1 .8 2 3330.11 318 1.1 3 2 81 5.1 7 3 37 0.5 6 2 951.12 3 41 4 .5 9 3 01 4.1 2 3 4 4 5 ,2 9 2 862.02 329 5.6 6 3244.08 2891.46 3 142.85 330 5.3 8 2817.04 3112,8 3078,24 3 182.84 2 965.42 3296.13 3155.89 2856.11 3 075.45

BPNN 3 0 8 9 .7 8 3 00 6,0 7 3 40 2.3 2 322 9 .2 9 2 82 8 .2 9 337 0.5 6 2962.91 3 41 4 .7 9 3 01 9,2 8 3 50 7.7 6 287 2.5 9 3 29 5.6 6 330 5.5 5 3 02 1.8 5 3 186,96 3 35 7 .5 2 2 88 4 .0 8 318 2.3 6 3 080.74 3 313.68 2 97 7.9 5 3 309.12 3 22 8.9 9 2872.75 3118.6 9

n =8 5 N85F26 N85F27 N85F28 N85F29 N85F30 N85F31 N85F32 N 85F33 N85F34 N 85F35 N85F36 N85F37 N 85F38 N85F39 N85F40 N85F41 N85F42 N85F43 N 85 F 44 N85F45 N85F46 N85F47 N85F48 N85F49 N85F50

Optimal 3 3 1 7 .2 7 3 0 9 7 .3 6 2 7 9 8 .9 3 2 7 6 5 .4 8 2939.81 3 0 0 2 .9 6 3 1 1 8 .4 7 2 9 5 9 .1 6 2877.71 3 0 8 4 .3 9 3 2 0 3 .5 3 3 16 9.9 2 3 07 7 .6 7 3191.71 2162.98 3 20 8.1 4 3 11 9 .9 3 3 17 5.6 342 8.3 8 2 5 8 6 .1 8 2 8 7 8 .2 8 3 3 0 1 .3 3 3 0 9 3 .5 4 2 4 1 7 .1 6 3149.61

BPNN 3 3 4 8 .7 4 3 1 1 0 .9 6 2 8 0 3 .3 8 2 76 6.2 5 3 11 4.2 4 3 1 1 5 .6 3 2 8 3 .8 8 2 97 9.3 8 2 8 8 7 ,1 2 3 10 7 ,1 4 3 24 1,1 3 3 1 7 2 .2 3 09 5,6 5 3 25 0,7 2 2214,61 3 2 5 1 ,3 2 3 1 8 8 ,2 2 3 17 7,6 6 3 432.4 2 606,24 2 87 8.2 8 3 3 0 1 .3 3 3 09 3.5 4 2 4 3 3 .1 2 3 2 1 5 ,1 8

E13

n=100 N100F1 N100F2 N100F3 N100F4 N100F5 N100F6 N100F7

Optimal 2 58 5.3 2 3 67 8.6 2 3 461.73 3 21 8,0 8 3 923.19 3 64 6.6 8 382 7.3 4 371 8.6 5 3 45 7.3 9 3 971.52 4 02 2 .0 8 3397.91 3803.01 3546 3 08 4.2 312 8.9 2 306 3.1 3 3 17 0.2 4 3 79 2.3 6 3 7 3 4 .7 291 3.1 5 3 2 6 7 .3 8 3 30 5.3 8 3 93 3.6 2 98 5.0 9

BPNN 2 655.24 367 9.9 9 3 464.72 3218.08 408 6,9 9 3646.68 3 926.24 371 8.6 5 349 5.4 7 4 000.86 406 2.2 8 3520.6 2 4071.1 3616 3137.01 3 143.04 3 075.42 3 175.86 3808.72 376 3.0 4 2 913.15 3 26 7 .3 8 3312.41 4 01 6.9 299 0.7 6

n=100 N100F26 N100F27 N100F28 N100F29 N 10 0 F 3 0 N 100F31 N100F32 N100F33 N100F34 N100F35 N100F36 N100F37 N100F38 N100F39 N100F40 N 100F41 N100F42 N100F43 N100F44 N100F45 N100F46 N100F47 N100F48 N100F49 N100F50

Optimal 3 458.92 3429.26 3469.9 3 2 853.93 3 919.49 3305.1 3643.62 3 583.86 3709.51 3433.8 4 3 671.19 3775.62 3850.58 3 77 0.5 8 2 72 1.5 9 3809.51 3 603.32 3 23 7,5 7 361 4.8 9 2764.21 402 1.8 2 3 50 8.7 6 3 50 9 .4 9 3 7 9 0 .3 8 3 60 2.4 4

BPNN 345 8.9 2 3 430.57 3473.85 2861 3955.75 3305.1 3 70 9 .3 9 3593.1 371 1.4 8 3444.6 3678.15 3 775.62 3861.1 3 77 0 .5 8 283 5,3 8 381 2.0 3 361 4.6 4 3 241.3 370 2.3 2 2777.81 4 09 3 .2 9 3 57 7 .0 5 3 517.79 382 1.6 7 3 64 8.2 9

N100F8
N100F9 N 10 0F 1 0 N100F11 N 10 0 F 1 2 N100F13 N100F14 N100F15 N100F16 N100F17 N100F18 N100F19 N100F20 N100F21 N100F22 N100F23 N100F24 N100F25

E14

