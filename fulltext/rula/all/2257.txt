NOTE TO USERS

This reproduction is the best copy available.

®

UMI

FEATURE RECOGNITION IN GEOMETRIC REVERSE ENGINEERING
by M uham m ad A rshad B.E. (M ech), N ED U niversity o f Eng. & Tech. 1993

A thesis presented to R yerson U niversity in partial fulfillm ent o f the requirem ents for the degree o f M aster o f A pplied Science in the program o f M echanical Engineering.

Toronto, Ontario, Canada, 2004 © M uham m ad Arshad, 2004

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

UMI Number: EC52913

INFORMATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

®

UMI
UMI Microform EC52913 Copyright 2009 by ProQuest LEG. All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code. ProQuest LLC 789 E. Eisenhower Parkway PC Box 1346 Ann Arbor, Ml 48106-1346

B orrow er's Page: R yerson U niversity requires the signatures o f all persons using or photocopying this thesis. Please sign below, and give address and date.

N um ber 1 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Print N am e

Address

Signature

Date

Ill

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Supervisor: Dr. V incent Chan

Abstract

T itle : Feature Recognition in G eom etric Reverse Engineering. N am e : M uham m ad Arshad Program ; M ASc., M echanical Engineering, Ryerson University, 2004.

An artificial neural network based feature extraction system for finding three dim ensional features from physical objects is presented. As part o f a geometric reverse engineering system, the feed-forward neural network allows for the efficient

im plem entation o f feature recognition. Reverse engineering o f m echanical parts is the process o f obtaining a geom etric CA D model from the measurements o f an existing artifact. Ideally, the reverse engineering system w ould autom atically segm ent the cloud data into constituent surface patches and produce an accurate solid model. In order to accomplish this intent, a neural netw ork is used to search and find the features in the initial scan data set. In this work, feature extraction for geometric reverse engineering has been accomplished. W ork has also been done to extract features from the m ultiple shapes. The technique developed w ill reduce the tim e and effort required to extract features from scanned data o f a physical object.

IV

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Table of Contents

T itle page.................................................................................................................................. i A uthor's declaration...............................................................................................................ii B orrow er's p a g e ......................................................................................................................iii A bstract..................................................................................................................................... iv T able o f contents.....................................................................................................................v List o f figures...........................................................................................................................vii List o f tables............................................................................................................................ ix N om enclature table................................................................................................................ x C hapter 1- introduction.....................;................................................................................1 1.1 T he geometric reverse engineering............................................................ 1 1.2 Traditional geom etric reverse engineering............................................... 2 1.3 Feature extraction in reverse engineering................................................ 3 1.3.1 Data collection.....................................................................................4 1.3.2 Data registration..................................................................................4 1.3.3 Pre-processing..................................................................................... 4 1.3.4 Data segm entation.............................................................................. 5 1.3.5 N orm alization......................................................................................5 1.3.6 Feature extraction............................................................................... 5 1.3.7 D ata classification.............................................................................. 6 1.3.8 Post-processing................................................................................... 6 1.4 Features recognition for geometric reverse engineering data.............. 6 1.5 Legal standing o f revere engineering........................................................ 11 1.6 Potential benefits............................................................................................12 1.7 Scope o f this w ork.........................................................................................13 Chapter 2Features extraction - Literature review .................................................... 15 2.1 Introduction....................................... .· ............................................................. 15 2.2 T}-pes o f feature recognition....................................................................... 16 2.2.1 Param etric m atching.........................................................................16 2.2.2 Syntactic feature recognition.......................................................... 17 2.2.3 V olum e decom position....................................................................17 2.3 Feature extraction - Literature review .......................................................17 2.4 Feature recognition in reverse engineering.............................................. 20 Chapter 3 - Artificial neural network - An overview...................................................22 3.1 Introduction..................................................................................................... 22 3.2 Types o f neural netw ork............................................................................... 22 3.2.1 M ulti layer feed forward neural netw ork......................................23 3.2.2 H opfield netw orks............................................................................. 23 3.2.3 Kohonen self organizing netw ork.................................................. 24 3.3 Artificial neural netw ork - Literature review .............................................25

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

C hapter 4 4.1 4.2 4.3 4.4 4.5

N eural netw ork based feature extraction.................................................... 29 Introduction.......................................................................................................29 Selection o f artificial neural netw ork.......................................................... 29 A rtificial neural netw ork configuration...................................................... 30 Feed-foi-ward neural netw ork........................................................................32 Input and output vectors.................................................................................34 4.5.1 Segm entation algorithm for single shape objects.......................35 4.5.2 Segm entation algorithm for multiple shape objects....................40 4.6 Back propagation training algorithm .......................................................... 43 Testing o f the neural network algorithm.................................................... 49 Training o f the neural netw ork..................................................................... 49 Testing o f the neural netw ork....................................................................... 53 Testing o f the algorithm with real reverse engineering data.................. 54

Chapter 55.1 5.2 5.3

Chapter 6 - Conclusion and Future w ork......................................................................... 62 Appendix A ............................................................................................................................... 64 A ppendix B ............................................................................................................................... 75 Appendix C ............................................................................................................................... 78 References..................................................................................................................................101

VI

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

List of Figures

Figure 1: Block diagrams for feature recognition system ...............................................3 Figure 2; Picture o f "circle" test object.............................................................................. 7 Figure 3; Picture o f "ellipse" test object............................................................................ 7 Figure 4: Picture o f "square" test object............................................................................ 8 Figure 5: Picture o f "rectangle" test object....................................................................... 8 Figure 6: Picture o f "triangle" test object......................................................................... 9 Figure 7: Picture o f "diam ond" test object....................................................................... 9 Figure 8; Picture o f "w heel" test object............................................................................. 10 Figure 9: Picture o f "Roland PIX - 30 3D Scanner" .......................................................10 Figure 10: Systematic diagrams for feature recognition system.................................... 11 Figure 11 : M ulti layer feed-forward neural netw ork........................................................23 Figure 12: Simple Hopfield netw ork...................................................................................24 Figure 13: Kohonen self-organizing netw ork.................................................................... 25 Figure 14: Neural network configuration...........................................................................31 Figure 15: Neural network for feature recognition........................................................... 33 Figure 16: Digitized points from the physical object "w heel"...................................... 34 Figure 17: Isometric view o f the digitized physical object "wheel" .............................35 Figure 18: Flow chart for the segmentation o f data for single object...........................36 Figure 19: Test object "diam ond" and its segmented boundaries................................ 37 Figure 20: Test object "square" and its segmented boundaries..................................... 38 Figure 21 : Test object "ellipse" and its segmented boundaries..................................... 39 Figure 22: Flow chart for the data segmentation for multiple shapes object............. 41 Figure 23: Test object "wheel" and its segmented boundaries...................................... 42 Figure 24: Flow chart for neural network-based feature recognition...........................48 Figure 25: Sample input vectors presented to neural netw ork.......................................50 Figure 26: Test sample "circle" and its segmented boundary........................................54 Figure 27: Test sample " ellipse" and its segmented boundary...................................... 55

Vll

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Figure 28: T est sam ple "triangle" and its segmented boundary......................................56 F igure 29: Test sample "rectangle" and its segmented boundary.................................. 57 Figure 30: Test sam ple "diam ond" and its segmented boundary................................... 58 Figure 31 : T est sam ple "square" and its segmented boundary....................................... 59 Figure 32: Input vectors derived from real reverse engineering data............................60 Figure 33. T est sample "square" and its segmented boundary....................................... 65 Figure 34. Test sample "circle" and its segmented boundary......................................... 66 Figure 35. T est sample "ellipse" and its segmented boundary....................................... 67 Figure 36. Test sample " ellipse" and its segmented boundary....................................... 68 Figure 37. Test sample "rectangle" and its segmented boundary...................................69 Figure 38. Test sample "square" and its segmented boundary....................................... 70 Figure 39. Test sample "square" and its segmented boundary....................................... 71 Figure 40. Test sample "triangle" and its segmented boundary..................................... 72 Figure 41. Test sample "triangle" and its segmented boundary..................................... 73 Figure 42. Test sample "diam ond" and its segmented boundary................................... 74

vm

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

List of Tables

Table 1; Set o f classification values with sigmoid transfer function............................. 52 Table 2: Set o f classification values with hyperbolic-tangent transfer function.........52

IX

I

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Nomenclature Table
A param eter - learning rate o f connectors betw een hidden and output layer m ean distance from center to object boundary point param eter - learning rate o f connectors betw een hidden and input layer m axim um distance from center to object boundary point change o f the weight between the i"' elem ent o f the input layer and the j dv(2 .i),(3 j) element in the hidden layer.

a B

b dW( 1 ,i)j2 ,j)

change o f weight betw een the j''' elem ent o f the hidden layer and the i`'' element in the output layer.

Eoutpui.i Ehiddcn.i k Rj W {I ,i),(2 ,i) V(2 ,i).(3 .i) n
oc

error at the output layer, neuron location i error at the hidden layer, neuron location i num ber o f iterations for the neural netw ork training reflected vector at the output layer connector weight betw een neurons at input and hidden layer. connector weight betw een neuron at hidden and output layer, num ber o f comers for geometric objects m ean angle between two lines o f consecutive boundary points neuron value at input layer, neuron location i neuron value at hidden layer, neuron location i neuron value at output layer, neuron location i

À , (i,i) X X (3 j)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 1- Introduction

1.1

The Geom etric R everse Engineering Reverse engineering is the process o f converting 3D surface data collected from a

laser scanner or touch probe mounted on a coordinate m easuring m achine into a fom i com patible w ith CAD/CAM packages. The gathered data, nonnally huge in size and unstructured in nature are often called cloud data. Reverse engineering is used in industry for a num ber o f reasons, such as m odification o f prototype parts after testing, or the custom fit o f prosthesis for better com fort in the case o f knee or hip replacem ents and the reproduction o f broken m achine parts w hose drawings are not available.

There are two m ain applications o f reverse engineering: 1. To provide digital infonnation for a product for w hich no CAD m odel is available. 2. To support the redesign o f an existing product.

Either o f these goals could be achieved by m aking sure that the 3D scanned data are com plete and accurate. The dim ensions o f the part or its shape can then be derived from the digitized points. The fitting o f one or m ultiple surfaces to the point data is then

1

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

necessary to generate a CAD m odel. Beyond the domain o f p rism atic and cylindrical objects, feature handling is still a m ajor research area. In an ideal reverse engineering system, the cloud data w ould autom atically fragm ent into constituent surface patches and generate an exact solid m odel. In order to realize this objective, a neural netw ork is em ployed to explore and find th e features in the initial data set. B esides the encouraging progression o f several researchers, reverse engineering is a diverse and com plex problem , to which a direct distinct solution has not been established.

1.2 T raditional G eom etric Rev erse Engineering Traditionally, the process o f reverse engineering em ployed a touch probe, w hich w as m ounted on a coordinate m easuring m achine (CM M ). In order to accurately define

th e surface contours o f an object, which needs to be reverse engineered, a CM M operator is required to m anually guide the sensor to collect tliousands o f data points. This is a slow process w hich requires expensive equipm ent and takes a considerable am ount o f time. On the other hand, advancem ents in m achine vision technology provide a m eans to collect 3D data from the object surface with non-contact sensors like an active laserbased range finder. CAD m odels are then created from this data for any com puter-

based design, analysis or m anufacturing tasks. The adoption o f m achine vision-based reverse engineering in the last 15 years has been the result o f dem ands for increased quality control and low er product cost coupled w ith ever increasing m anufacturing throughput requirem ents.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1.3

Feature E xtraction in R everse Engineering D etection and localization o f 3D objects in scenes represented by single or

m ultiple 2D im ages has becom e a w ell-established teclinology. A related, but not so deeply investigated problem deals w ith the identification o f 3D objects directly from 3D data. A num ber o f engineering applications rely on robust and efficient shape feature recognition in 3D data, w here these data can be either digitized points or synthetic data from a CAD m odeling system. The dim ension o f the part or its shape can be derived from the digitized points. T he generation o f a CA D m odel requires the fitting o f one or m ultiple surfaces to the point data and to construct an appropriate surface or solid model. This step can only be fully autom ated for som e special cases. The level o f autom ation depends on the intended purpose o f th e CAD m odel. Technical feature recognition systems are com posed o f consecutive blocks, each perform ing its predefined task in the processing. This system can be described as a

b lock diagram . In the sim plest form , it is shown in Figure 1.

Data Collection

Data Registration

PreProcessing

Segm entation

N orm alization

Feature Extraction

Classification

Post processing

Figure 1. B lock diagram s for feature recognition system.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1.3.1

D ata C ollection D ata collection is the first stage in any feature recognition system. B efore an input

vector is m ade up o f a set o f m easurem ents, these m easurem ents need to be taken. For exam ple, video cam eras and scanners are used in the case o f character recognition and a luicrophone, in the case o f speech recognition. D ata collection devices m ust be able to record the object, ideally, w ith the highest reliability available. N oise is considered a disadvantage in order to perform the successful operation o f any system.

1.3.2

D ata R egistration Elem entary m odel fitting can be perform ed in data registration. The objective

could be achieved by som ehow fixing the internal coordinates o f the recognition system to the actual data acquired. A priori know ledge surrounding the system is utilized in

designing the registration stage. For exam ple, in the case o f optical recognition, the system m ust locate in the input im age and the area o f interest.

1.3.3

Pre - Processing In the real w orld, especially in the case o f reverse engineering, data always has

som e degree o f noise and therefore requires a preprocessing stage. The te n u noise is used in broad sense, but can be sim ply defined as, "A nything that hinders a recognition system to fulfill its com m ission m ay be regarded as noise, no m atter how inlierent this `noise' in the nature o f the data." A lso, preprocessing enhances some o f the desirable properties in the data that are fed into the recognition system.

Reproduced with permission of the copyright owner. Further reproduction prohibited w ithout permission.

1.3.4 Data Segm entation The data, w hich have already been registered and preprocessed, are split into subparts. This process is called data segmentation. In this work, data points w hich formed the boundary o f the objects are segmented. This task is accom plished by developing M ATLAB codes. T he segm entation processes are outlined in detail in sections 4.5.1 and 4.5.2.

1.3.5

N orm alization A com m on characteristic o f feature recognition systems is the inherent variance

o f the objects to be recognized.

The main problem in feature recognition is how these

\'ariances are accounted. There are m any possibilities, one is to use feature extraction or a classification algorithm , which can deal with the variations in the outcomes o f the object. The side effect o f nonnalization is a loss o f degrees o f freedom, i.e., the dim ension reduction in the intrinsic dim ensionality o f the data.

1.3.6 Feature Extraction The dim ensionality o f data is reduced during the process o f feature extraction. This is necessary as a result o f lim itations in m em ory and com putation time. A reliable

feature extraction schem e can m aintain and enhance those features o f the input data which make distinct feature classes separate from each other. Also, the system m ust be restrained with respect to variation produced by both the humans and the m easuring devices used in the data acquisition stage.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1.3.7 Data C lassification C lassification is the most crucial step in the process o f feature recognition. All the previous stages are designed and tuned w ith the aim to have success in the classification phase. In the sim plest way, the operation o f classification is the transform ation o f quantitative data to qualitative output information.

1.3.8 Post-processing A fter the classification stage, some data processing is perfom ied in m ost feature recognition systems The post processing subroutine carries forward som e a priori

infonnation about the neighboring world into the system. This additional step helps in improN'ing the overall classification accuracy. T he post-processing phase is generally possible if the individual objects or segm ent make up meaningful entities such as bank account num bers or sentences.

1.4 Features R ecognition for G eom etric Reverse Engineering D ata: T he "Feature" driven CAD m odeling packages provide the vital link between design and m anufacturing. In the same way, "Feature" driven reverse engineering would allow for m ore flawless application o f the CAM software. In this work, the features arc extracted from geom etric reverse engineering data. To test the proposed algorithms, seven different geom etric objects are created; circle, ellipse, square, rectangle, triangle, diamond and wheel. These objects are shown in Figures 2, 3, 4, 5, o, 7 and S.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

s *

Figure 2. Picture o f "circle" test object.

P Ït

Figure 3. Picture o f "ellipse" test object

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Figure 4. Picture o f "square" test object

Figure 5. Picture o f "rectangle" test object

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4-,/
^ 'k

V
ç ^r A

V . Tj"*jpp

Figure 6. Picture o f "triangle" test object

S B

Figure 7. Picture o f "diam ond" test object

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Figure 8. Picture o f "wheel" test object

A Roland Dr. Picza PIX - 30 3D laser scanner is used to collect range data o f different accuracy levels and densities. Based on a piezo sensor, the PIX - 30 is a contact scanner.

Figure 9. Roland PIX - 30 3D Scanner

10

Reoroduced with permission of the copyright owner. Further reproduction prohibited without permission.

This scanner gathers the data from the surfaces o f the object. The collected m easured data points are fitted with a suitable primitive geometric shape. A program m ing m ethod base on the hum an brain architecture, Meural Network, is used for the recognition o f im portant features on the object's surface. These features provide a m ore intuitive m eans for engineers to develop object definition. M ATLAB codes are developed first for data segmentation and then eventually for feature recognition. In a simple form, the process is oudined in Figure 10.

Laser Scanning

Pre - processing

Data segmentation / Geometric Extraction

Features Extraction

Figure 10. Systematic diagrams for feature recognition system.

1.5 Legal Standing o f Reverse Engineering The legal standing o f reverse engineering has long been an issue for the engineering discipline. Several U.S. Supreme Court rulings and congressional legislations are in place which allow the use o f reverse engineering for development and innovative purposes. Reverse engineering has long been held as a lawful form o f discovery in both

11

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

legislation and court opinions

The Supreme Court o f U SA has confronted the issue o f

reverse engineering in mechanical technologies numerous times, upholding it under the principle that it is an im portant technique o f dissem ination o f ideas and encourages innovation in the m arket place. The U.S Supreme Court addressed the first principle in K ew anee Oil v. Bicron, a case concerning trade secret protection over the m anufacturing o f synthetic crystals by defining reverse engineering as "a fair and honest m eans o f starting w ith the know n product and working backwards to divine the process which aided in its developm ent or m anufacture." There was another principle that encourages the innovative use o f reverse engineering articulated in Bonito Boats, v. Thunder craft. This case involving laws forbidding the reverse engineering o f the molding process o f boat hulls. In this case, the U.S Supreme Court said "the competitive reality o f reverse engineering may act as a spur to the inventor, creating an incentive to develop inventions that m eet the rigorous requirem ents o f patentability."

1.6 Potential Benefits This research looks at automating the collection o f surface data points and the m odeling o f the surfaces in a com puter aided design (CAD) program. The 3-D laser allows the gathering o f prelim inary surface inform ation that could subsequently be used to locate im portant features on the object being examined. A pplications o f this research range from the geom etric reverse engineering o f physical m odels to quality control. This research will allow m anufacturers to reduce design cycles and to quickly bring products to market.

12

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1.7 Scope o f this w ork This thesis is arranged in chronological fashion o f the steps required to carry out feature extraction. Chapter 1 discusses reverse engineering and the conventional reverse engineering tecliniques. This chapter describes the problem and proposes the m ethodology to extract features for geom etric reverse engineering data. Potential benefits o f this research are also discussed at the end o f the chapter. There is no universally agreed definition o f a feature. T he word "feature" has a different m eaning for different researchers. The m ost com m only used feature definitions are described in Chapter 2. This Chapter also discusses types o f feature recognition and the related literature review o f features recognition techniques. Chapter 3 describes different types o f neural networks and discusses their stm cture. This chapter also looks at the related literature on artificial neural netw ork techniques. Chapter 4 discusses the selection o f the artificial neural network m ethod applied in this work to extract the im portant features, its configuration and potential benefits. This chapter also describes the algorithm s that were developed for the segm entation o f the boundaries and then calculation o f the param eters that form the input vector to the neural network. Finally, the feed-forward neural network algorithm applied for feature recognition is described. The testing o f the neural network algorithm is a very im portant part o f this technique. T he algorithm is first tested with manually created synthetic data and then on the real reverse engineered data derived from the test samples. After the successful

13

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

training o f th e algorithm , the algorithm is tested on an unseen exam ple. C hapter 5 presents the training and testing o f the neural network algorithm. C hapter 6 discusses the conclusion and future work.

14

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 2: Features Extraction - Literature Review

2.1

Introduction M any techniques have been developed for feature identification fi'om CAD

m odels. However, the literature is scarce in the area o f extracting features from reverse engineered data. M ost o f the methods are based on m atching algorithm s, in w hich the data are com pared w ith a predefined set o f surfaces and edges. The features are usually defined generically before any m atching process m ay be initiated, as a com bination o f topological entities. The overall aim in feature recognition is to convert low level geom etric inform ation into a high level description in terms o f form, functional, m anufacturing or assem bly features. This description could be for design, m anufacturing, engineering analysis or even for adm inistrative purposes. It is well known that recognizing features

th at are required for m achining m ay be considerably different from recognizing features useful for casting or for assem bly purposes. In other w ords, features are context dependent entities.

15

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

"F eature" is general term and has been used to describe a nu m b er o f different things. F o r exam ple, V andenbrande et. al. define that a feature is a "region o f an object

that is m eaningful for a specific activity o r application". Schulte et. al.^^^ considered a feature as "geom etry associated with a specific operation" . A nother definition by Silver describes a feature as "A region o f interest consisting o f voxels satisfying a set o f pre defined criteria" . Shah defines a feature as a physical constituent o f a part that can be

m apped to a generic shape and "represents the engineering m eaning o f the geom etry o f a p art o f assem bly". For this work, a feature will be defined as "a recognizable topological pattern o f a set o f edges" .

2.2 Types o f Feature R ecognition T here have been various techniques developed to extract features from a geom etric m odeling database. B y and large, feature recognition can be divided into three m ain categories.    Param eter m atching Syntactic feature recognition V olum e D ecom position

2.2.1

Param etric M atching In param etric m atching, the features are first characterized in te n n o f their

geom etric and topological form. The algorithm searches the solid m odel data base, m easures against topological type, connectivity and adjacency to decide if any o f the characteristic fe tures are available.

16

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

2.2.2

Syntactic Feature R ecognition In syntactic pattern recognition, the geom etry is represented in term s o f a

language gi-ammai- that describes the order o f the lines and cui-ves. T h e description o f the object is then m atched against gram m ar to recognize the features.

2.2.3

V olum e D ecom position In the third category, i.e., volum e decom position, the rem oved base stock material

is identified and then broken dow n into distinct m achining operations. This volum e is decom posed into sm aller volum es or "features" , which confonn to m achining operations.

2.3

Feature E xtraction - Literature Revie\v F ree-fo n n features are acquiring a great deal o f attention since they are considered

the im portant constituent in product styling, aesthetic design and shape conceptualization. Recently, som e C om puter-A ided Industrial Design (CAID) systems have surfaced, each o f w hich is in som e m eans based on surface features or free-foim features. Various

system s are dedicated to particular types o f features, for exam ple, protrusions and depressions. To m ake these type o f systems truly flexible and useful, free-form features (shape patterns) are required to be extracted from existing objects, w here these objects are either physical (and to be 3D scanned) or virtual (and to be sampled). The m ain obstacle is

the fitting o f 3D shape patterns against 3D point sets. A necessary requirem ent o f the fitting m ethod is that it should not only acquire placem ent and scale param eters for the pattern, b u t also shape defoim ation parameters.

17

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

In a graph based approach for feature recognition, boundary representations are b u ilt upon a graph structure. B oundary representation model faces can be considered as nodes o f a graph w hile face-face relationships form the arcs o f the graph. As described b y W u and Liu graph based approaches first represent an outline o f the required

topological and geom etrical constraints for recognizing the feature. O nce the graph which identifies a feature class has been defined, such a graph has to be searched in the object structure, w hich is a graph as well. T he problem o f recognizing a given sub-graph in a graph is fairly com plex problem and its com puting tim e in the w orst case grows exponentially. M any authors proposed various search strategies to w ork out this problem . Som e authors argued that the adjacency inform ation available is usually not adequate for feature recognition. F or this reason a num ber o f augm ented graphs have been recom m ended. O ne o f the m ain draw backs o f graph-based feature recognition techniques is the difficulty in recognizing interacting features. This is due to the fact that a feature characteristic pattern is changed w hen features intersect each other. Hint based reasoning a p p r o a c h e s ^ h a v e been developed to overcom e this drawback. In the h int based approach, developed b y Requicha et. al. those characteristic traces that features

leave in the nom inal geom etry o f the part are searched. These traces present hints for the potential existence o f volum etric features even when features intersect. These hints are processed to generate the largest possible volum etric feature that is com patible with the hint and does not intrude into the feature. In the pioneering w ork o f K ypnanou feature gramm ars are described for the took allowance o f

extraction tlirough syntactic feature recognition. Falcidieno et. al.

18

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

K yprianou's m ethod for the identification and extraction o f feature inform ation from a boundary representation o f an object. The fundamental concept is to define a pattern description language w here suitable rules are defined in order to create an applicable com position o f primitives. The authors defined tliree basic prim itives: convex edge, concave edge and sm ooth edge. This graph parsing based feature recognition schem e was focused on the identification o f depression and protrusion features. Di Stefano introduced the concept o f scm antem a as the minimal element o f m eaning that defines the sem antics o f the representation. This approach requires the statem ent o f the m inim al set o f sem antem a that identifies the feature clearly. V olum e decom position m ethodologies operate m ore directly on the threedim ensional representation o f volum es instead o f w orking on the boundary representation gi'aph o f solid models. Such approaches have been generally em ployed for the

recognition o f m achining features. There are two main approaches for volum e decom position, alternating sum o f volum es (ASV), where an object is articulated in

ternis o f a hierarchical structure o f convex components, and delta volum e decom position, w here the intent is to recognize the volum e to be machined and then decom pose it into a set o f non-overlapping entities corresponding to different m achining operations. In the pioneering work o f W oo, ASV decom position is applied to indicate a

non-convex object by a hierarchical structure o f convex components. This approach has been proven non-convergent in certain cases. Kim proposed an enhanced convex proposed the

decom position approach to address this issue. Kim and W ang et. al.

alternating sum o f volumes w ith partitioning (ASVP) approach. This is a convex decom position method based on a convex hull, set difference and cutting operations. In

19

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

this scheme, the boundary faces o f a part are arranged in an outside-in hierarchy and volum etric com ponents are related with these faces. The ASVP decom position is transform ed, b y m eans o f com bination operation between com ponents, into a set o f feature volum es (Form Feature D ecom position - FFD) corresponding to significant high level constituents o f the product shape. FFD is then transfonned into a N egative Feature D ecom position (NFD) by means o f positive-to-negative conversion. The m achined face inform ation is obtained from negatiye feature stand for removal volumes. Kailash et. al described a m ethod dedicated to machining feature extraction o f

casting and forging components. In this scheme, machining rem oval volum es w ere first obtained by subtracting the final part model from a row part model. M achined faces (Mfaces) are then recognized and collected into groups (M-groups). Finally, M -groups are m apped into all possible machining process forms. This feature identification approach is process oriented as M -group is mapped to various processes.

2.4 Feature R ecognition in Reverse Engineering It is difficult to classify feature extraction methods into precise, organized groups as there is a considerable overlap betw een the various techniques. The m ajority o f the methods, as discussed in Section 2.2, use a m atching algorithm which com pares data with predefined generic features. Features extraction algorithms m ay include the following specific tasks: i. ii. Generic definition o f a features topology. Searching the database to match topological patterns.

20
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

iii.

Extracting the recognized features from the database (rem oving a portion o f the m odel associated w ith the recognized feature).

iv.

D eterm ining the feature parameters (hole, diameter, num ber o f com ers).

The features m ust be generically defined b y a combination o f topological entities required to illustrate the features before any matching process m ay be initiated. For example, a hole could be described as combination o f two circular edges suirounding a cylindrical surface. Secondly, the topological database would be searched for

connectivity and adjacency to determ ine which o f these features are present in the solid monel. The features will be further limited in scope to specific m achining operations for the creation o f hole, square, diamond, ellipse, triangle, rectangle and wheel. There are, o f course, many reasons for limiting this definition. As this work is prim arily concerned w ith reverse engineering, the emphasis is on the reconstruction o f the solid parts, not on the design o f parts. The reverse engineering algorithm is to model the part with some rudim entary editing features. It is not m eant as a replacement for a CAD package.

21

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 3 --Artificial Neural Netw ork : An Overview

3.1

Introduction This chapter presents different types o f neural networks. The related literature

review on the artificial neural network is the main focus o f this chapter.

3.2 Types o f Neural Network There are m any t}^es o f neural networks. Each type has the characteristics o f parallel processing from an interconnected network o f computational elements. Several structures o f neural networks are possible by connecting the elements together. There are two m ost com m only used structures from the neurons connection point o f view; m ulti layer neural network and fully connected neural network (Hopfield network). These networks differ from one another in architecture and training algorithm. The following three are the m ost com m only used networks.

22
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

3.2.1

M ulti Layer Feed F onvard Neural Network The multi layer feed-forward neural network has many successful applications

and is the most commonly used neural network. As shown in Figure 11, the neurons are airanged in several layers. Any num ber o f neurons and number o f layers are possible.

Input vectors

Hidden layers

Output vectors

Figure 11; Multi layer feed-forward neural network

The layers are classified into tlii'ee types: input, hidden and outer layers. Any number o f hidden layers is possible but the connections are allowed only in the feed forward directions.

3.2.2

Hopfield networks In the early 1980`s, Jolm H opfield's pioneering work gave credibility to the

fledging neural network field. Contrary to the multi layer feed-forw^ard neural network, a Hopfield network is defined as a feed-back system with the output o f one complete forward operation o f the network ser\dng as the input to the next network operation. The Hopfield network is also called the recument network as this network operates as a feed back system. This scheme is illustrated in Figure 12.

23

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Figure 12. Simple Hopfield Network

Each forw ard operation o f the network is called an iteration. The process is repeated until the output remains constant. Hopfield showed in his work that if the weight matrix is sym m etrical with zero diagonal elements and the elements are updated asynchronously. the network will always converge.

3.2.3

Kohonen self organizing network In the early I9 8 0 's. Teuve Kohonen developed an algorithm to m im ic the brain's

ability to organize itself in response to external stimuli. Kohonen called his algorithm a s e lf organizing feature map. Kohonen's algorithm represents a type o f neural network that is capable o f learning without supervision. In this technique, the weights

strengthen themselves. The first layer is the input and the second layer, the output. It is a two dim ensional grid where the self organizing takes place as illustrated in Figure 13. In the winner take all competition, the output neuron w ith the highest value wins. The structure o f the output vectors is laid out in a grid like pattern, this allows the concept o f neighborhoods.

24

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Output

(:) + 0 iC:)

to

1 0

,

Input Figure 13. K ohonen self-organizing network

Once the w inner clem ent is found, the weights for that output elem ent and its neighborhood are updated. The connectors are iterated until the t'alue converges, i.e.. no more winning neurons are declared.

3.3 Artificial Neural N etwork - Literature Review: For classification problems, the neural network is able to give statistical information about the classification and is easy to train, but it is often not clear how the neural network has arrived at its answer. On the other hand, the operations o f rule-based algorithm s are traceable, but the set o f rules chosen may be more difficult to train and may not generalize as well as a neural network. Recognition is one o f the most complex problems in the com puter and m achine vision area. The major concern associated with the use o f artil'icial neural networks for feature recognition is the formulation o f an appropriate codification o f the topological and geometrical entities in order to present a numerical input to the network. 25

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

In order to m eet the neural network input requirements, Prabhakar and H enderson defined a feature as a mathem atical function, in which geometrical and topological data w ere variables, derived from the solid m odel o f the part. In this method, these variables represented the net input and were aiTanged in a two dim ensional m atrix called the adjacency matrix. Each element (i,j) o f the adjacency m atrix represents the relationship o f face j to face i. N on-adjacent faces i and j were represented by zeros whereas different integer values denote different t>Tpes o f edges. The sign o f the adjacency m atrix elem ent indicates w hether the edge is concave or not. The process o f recognition is then reduced to row -by-row parsing o f the adjacency matrix. Zulkifli et. al. proposed a m ethod to recognize the interacting features. The

authors used a B-rep solid model as input for the feature recognition system. This method is based on a layering technique to find interacting features. After selecting the principle direction, this technique searches for any volum e that exists between two successive layers o f the part. These volumes are then checked to find out if they represent the result o f interacting features. This task is accomplished by means o f a Kohonen self- organizing feature m ap (SOFM ) neural network, which is used to create maximal rectangular regions which are then intersected with the resultant area. Primitive features are then obtained fi'om resulting regions. Also, the second stage o f SOFM was applied on the resulting regions to decom pose them into prim itive regions. As described by the authors, this technique is limited to apply only for the features that have identical thickness and a com m on bottom face. C han et. al. present a method in which stereo pairs o f images are used to plan

the path for a co-ordinate m easuring m achine (CMM). The Kohonen self-organizing map

26

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

(SOM ) netw ork is used for the segm entation o f the CCD images. The authors incorporate the charged coupled device (CCD) cam era and a CM M touch probe digitizer together to accom plish this aspect o f reverse engineering. In this reverse engineering system, an accurate solid data were obtained from the automatic digitization o f the object using the CCD images. M ethods that require no explicit models, e.g., neural networks, case-based reasoning and inference have been developed, but their ability is saturated at a certain level. Two methods to evade the lim its have been attempted, one is to build a more concrete model and the other is to fuse these methods together. Y ata's digit is a

rem arkable and promising success in the latter method. The main idea is to utilize m any neural networks at the same tim e that construct total model ''M ulti-M odel N eural N etw orks" (KfNNs). D espite its very sim ple and easy implementation, the prelim inary results showed that NfNNs significantly increase sensitivity. In 2001, Y oshihara et. al. applied a multi-model neural network to identify exon-intron boundaries (splice site) in D N A base sequences. The M NNs provide a higher identification rate o f 95% , as com pared to 83.4% w ith a single NN. None o f the reverse engineering packages address the autom ation o f feature extraction due to the size, incom plete and unstiuctured nature o f scanned data. The reverse engineering packages having provision for feature recognition rely on an interactive user interface Thom pson et. al. proposed a REFAB (Reverse

Engineering - FeAture-Based) system. In the proposed system, the authors used an interactive graphics workstation that segments the reverse engineering data into features. REFAB allows a user to specify the types o f manufacturing features and approxim ate

27

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

location o f each feature in the object. The REFAB system then fits a feature to the scanned data b y using an interactive refinem ent process. M achine vision systems facilitate sophisticated industrial applications, such as classification and process control. Artificial neural networks (A NNs) and m achine vision bonded together provide a new scheme for solving com plex com putational problem s in m any areas o f science and engineering. Farhad et. al. investigated several novel uses

o f m achine vision and ANNs in the processing o f single cam era m ulti-positional images for 2D and 3D object recognition and classification. The authors used the boundary contour infonnation as a m ethod o f representing the industrial com ponent. A num ber o f shortcom ings w ere found m ost im portantly the identification o f unique start point, vital for rotation invariance.

28
I Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 4 - Neural Network Based Feature Extraction

4.1

Introduction The objective o f this w ork is to recognize features in the scan data. This chapter

w ill look at the artificial neural netw ork in depth, its im plem entation on physical objects to extract features and potential benefits for this specific problem.

4.2

Selection of A rtificial N eural N etw ork Current com m ercial reverse engineering software packages have not been

addressing the autom ation o f feature identification. Artificial neural networks are a good choice for feature extraction ft-om the reverse engineered data due to the nature o f scan data which have the follow ing inherent traits:     N oise in the scan data set. U nstructured and large in size. Incom pleteness o f the data. Defects in the scan object.

29

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

D ue to the nature o f reverse engineering data, a robust m ethod is required for the im plem entation o f a feature recognition algoiithm . R ule based algorithm s rely on searching through a set o f rules, selecting the rules w hich w ill advance the search state ftom one state to th e next until the final s'ates are found. T hese algorithm s need concise and accurate data to test the topology. As reverse engineering data is often incom plete and often cairy a substantial am ount o f noise, a neural netw ork based algorithm is considered to be m ore robust. Artificial Neural N etw orks (A NN s) have shown considerable prom ise in a w ide variety o f application areas and have been particularly useful in solving problem s for which traditional techniques have failed or proved inefficient. Neural netw orks have seen m any successful applications in m achine vision feature recognition problem s. The neural network technique is used in this w ork because: It has proven robustness in m any 2- D m achine vision problem s. H as the ability to learn and w ork in m any different situations. It is not susceptible to incom plete data sets as m uch as rule based algorithm s. · H igher com putational ability because o f m assive parallelism A m enable to m achine learning

4.3

A rtificial N eural N etw ork C onfiguration T he neural netw ork in this w ork has an input layer, hidden layer, an output layer,

weights, bias and a transfer function. The inputs are m ultiplied by w eights, bias is added and the transfer function operates on the total to give the output. G enerally, linear transfer functions are best suited to linear problem s, and non-linear transfer functions are best

30

I

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

suited to non-linear problem s. Graphically, the neural netw ork configuration is sho'v\m in Figure 14.

W|

Transfer function

 O utput Net Bias

Wn

Figure 14. Neural netw ork configuration. In Figure 14: N et = E (Xj Wj + bias) .(4.1)

f1 O utput = i lO W eights are updated:

if net > threshold

if net < threshold

W| (t+1) = W| (t) + sxj

W here. z = EiTor = desired output - cal. output and (xi, x2, x3, ... x,,) are input vectors.

31

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

T he com m only used transfer functions for classification are step functions, linear functions, sigm oid functions, hyperbolic-tangent functions. D ue to the w ide range o f problem s to w hich neural networks have been applied to, it is difficult to generalize w hich types o f transfer functions are b est suited to certain types o f problem s. For this work, a hj'perbolic-tangent transfer function is used between the hidden and output layer to m ap the result at the output layer. The hyperbolic-tangent transfer function has the properties to vary from -1 to +1.

4.4 Feed-Forw ard N eural Network The neural netw ork selected for this work is a feed-forward (back propagated) based netw ork w ith one hidden layer. The num ber o f input elements should equal the num ber o f param eters needed to define each feature, whereas the num ber o f output elem ents should represent the num ber o f different tjqies o f features w hich can be found as shown in Figure 15. For this w ork, the neural netw ork has four elements in the input vectors and seven elements in the output vector. The four elements define the geom etry o f the objects and fonn the input vector that was presented to the neural network. These param eters are discussed and calculated in Section 4.5. Also, each neuron at the output layer defines the feature type to recognize the target object from seven different objects. T he term back-propagation refers to the training o f the algorithm rather than the network architecture. In a feed forward network, the networks feed the input forward, that is, tow ards the output. It is thought that adding an additional layer, a so-called hidden

32

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

layer, would soften the effects o f input noise. However, w hether or not to use one hidden layer or two or more hidden layers has still not been w orked out.

Input Weights

Output Weights

O utput N euron

Input layer

Hidden layer

O utput layer

Figure 15. Neural network for feature recognition.

The number o f elements in the hidden layer camiot be determined except tlmough experim entation. N ezis and Vosniakos found that increasing the num ber o f hidden

layers did not change the results, but did increase the training times significantly. They also found that increasing the num ber o f elements in the hidden layer resulted in better m apping, how ever at the penalty o f increased training times. For this work, one hidden layer was used to soften the effect o f input noise. There are seven neurons em ployed in the hidden layer that was m apped on the output layer to generate the output values.

33

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4.5 Input and O utput Vectors B efore im plem entation, the neural network algorithm m ust receive the topology and its associated geom etry in a form that can be presented to the input layer. Therefore, to search for features in the database, fragments o f the topology geom etry database must be coded into a form at understandable to the neural network. However, w ith reverse

engineered data, the geom etry m ay also prove to b e an important indicator o f the possible features, as the faces th at m ake up a feature m ay not be fully defined. Since the reverse engineering scanned data is huge and unstructured in nature, two different segm entation algorithms, one for a single object and the other for multiple objects, are developed for the cloud data to segm ent the boundary o f the object and hence calculate th e param eters that m ake the input vectors. The digitized wheel object and its isometric view are show n in Figure 16 and 17.

 ie iB iiiB B B a o o B Q a iiiB a iia i> « * a B a iia i« O K B a e iitia ic a a a o B « D Q iB B e a o <

laaaBiegiM aaa'
iB B BB iBaeseaaaB i

aaaaaaaQ B aaaaaM aB aaaaaaeo B Q D H o iaaaciaaPB B aaapaaaaaaB B Q sasaaB Q i

to a a a a o e a a a a B B a a a a B a a a B B a B p a a a sa a a a a a a a a a a e e a B a a e a a c

Figure 16. D igitized points from the physical object "w heel"

34

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

mm

Figure 17. Isom etric view o f the digitized physical object "w heel" .

4.5.1

Segm entation A lgorithm for Single Shape Objects This segmentation algorithm looks at the physical object with a single geom etric

shape, like a circle, an ellipse, etc. This algorithm first segments the data points from the cloud data that form the object's shape. For this purpose, this algorithm com pares the data points that establish the depth o f the physical object with those on the surface. These segmented data points are then further processed to find the boundary points o f the object and hence calculate the im portant parameters that form the input vectors. This whole process was program m ed in MATLAB. A flow chart for this segm entation process is shown in Figure 18.

35

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Start

Load data text file

Find mean o f z coordinate

No z < Mean z Yes List all the z coordinate and the corresponding x and y coordinates Find centre o f the object by taking mean o f x and y Find the min. and max, o f x and y in horizontal and vertical directions Eliminate all points

Elim inate all the duplicated points

Rearrange the points in the clockwise direction Plot the points and generate the shape

Calculate the input vectors

End

Figure 18. Flow chart for the segmentation o f data for single object.

36

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

The follow ing Figures 19, 20 and 21, show the segmented boundaries o f the objects obtained from the reverse engineering cloud data by using the above segmentation scheme.

i D.i^ W @1 4 À / ` , -y - f j s ^ . o .:

10

12

14

16

18

20

22

24

26

Figure 19. Test object "diamond" and its segmented boundaries.

37

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Ti*LMgKnTMBm*rfmrm
F ilé E d it ;^ ie w in se rt % o o ls ^ in d o w H e lp

il,D ^ , «

> t: Ai,^

a

"igure 20. Test object "square" and its segmented boundaries.

38
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

£ i)e

E d ii

.V ie w . i n s o j t

% ooW

W in d o w

H elp

IIQ

e ^

A ^

/ ; ! j®

ë> T\I t H I H U M 14-

·4-H'HH++I4'4"f+10 15 20 25 30 35 4D 45

Figure 21. Test object "ellipse" and its segmented boundaries.

39

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4.5.2

Segm entation A lgorithm for M ultiple Shape Objects This segm entation algorithm looks at the physical object w ith m ultiple geom etric

shapes, like a wheel. B efore initiating the segmentation algorithm, the object is divided into four quadrants. T he segmentation algorithm then searches the object shape in each quadrant by com paring the data points that form the depth o f the physical object w ith those on the surfaces. These segmented data are then further processed to find the boundary points o f each object.. Once the shapes are segmented, all four quadrants are then assembled together to calculate the im portant parameters that form the input vectors. A flow chart for the segm entation o f multiple objects like wheel is shown in Figure 22. The segmentation algorithm for single shape and multiple shape objects calculate the four im portant param eters. These four parameters (a, b, oc, n) foim ed the input vectors. D ue to the noise and incomplete data set, the param eter `a ' is calculated as the mean distance and `b ' as the m aximum distance between the object boundary point and the center. The angle oc is calculated by using the law o f cosine betw een the two adjacent points at the object boundary. For this purpose, the distance fonnula is used to calculate the distance between the object boundary points and distance from centre. A lso to m ark the corner, the two adjacent angles that were calculated at the object boundary are add^d. This sum was subtracted from 180 degree. The output forms the corner if the difference varies m ore than 5 degree.

40

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Start Load data text file Find m ean o f z
No

If z >M ean z
Y es

Eliminate all points

Find all

c o iT e s p o n d in g

x

and

y

p o in ts .

Find min. and max. o f x and y in horizontal and vertical direction. Eliminate all the duplicated points. R ean ange & assembled the points in clockwise direction. Plot the points and generate the shape. Find centre o f the object by taking m ean o f x and y

Find mean o f x and y

If X >m ean x y < m ean y

If X <mean x y > m ean y

If X <m ean x y < m ean y

Calculate the input vectors
1r

End

If X >m ean x y > mean y

Figure 22. Flow chart for the data segmentation for m ultiple shapes object.

41

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

The segm ented boundary o f the wheel from the reverse engineering clouded data is shown in figure 23.

DIX F ile ' £ d il . -. V iew insert; ; CEoolsW -Wmdbw'^v H elp i-

60

50

20

10

20

30

40

50

GO

Figure 23. Test object "wheel" and its segmented boundaries.

42

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

The set o f param eters (a, b, oc, n) which are calculated from above schem e, forms the input vector. This input vector is presented to the input layer o f the artificial neural netw ork to recognize the features. Input vector = (a, b, oc, n) W here, a = M ean distance from center b = M axim um distance from center oc = M ean angle betw een two lines n - N um ber o f comers (4.2)

To recognize a target object from the seven defined features, the desirable output can form one o f seven vectors. T he physical object which is associated with a circle will be m apped by the recognition system as vector (1 0 0 0 0 0 0) and a diam ond as vector (0 1 0 0 0 0 0 ) and so on.

4.6

Back Propagation Training Algorithm: In order to correctly recognize the feature, it is very im portant for the netw ork to

perfonn the correct m apping o f the input param eters to produce the output classification. To obtain this, the weights are adjusted to the optimal values. This could be achieved by `training' the network. One o f the problem s o f training a m ulti-layer network is how the w eights o f the connectors are updated. The m ost popular m ethod for training a feed forw ard netw ork is called back propagation.

43
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

The size o f th e neural netw ork is dictated b y the size o f the input and the output vectors. O nce the architecture has been fixed, the values o f the connecting w eights determ ine the behavior o f a feed-forw ard network. Let; X (i,i) = Input neuron value at neuron location i. X (2 ,i) = H idden neuron value at neuron location i. X (3 ,i) = O utput layer value at neuron location i. W(i,i),(2 ,i) = C onnector w eight between neuron at the input and hidden layer.
V ( 2 .i),( 3 .i)

= C onnector w eight betw een neuron at the hidden and output layer.

W here i = 1 to n, depends on the respective layer as shown in the Figure 15. Therefore, the neuron value on the hidden layer can be calculated as:
n ^ (2,i) -- S m=l W ^ (l,m ) (4 .3 )

A lso, the output neuron values are calculated as the product o f connector w eights and the coiTesponding values at the hidden layer. A hyperbolic-tangent transfer function is used to bias the output neuron tow ards unity, so that.

n

n

^ ( 3 ,i) = ( 1 -

e x p

[ - Z V ( 2 ,m ),( 3 ,i) · m =l

^

( 2 ,i)] ) / ( 1

+

G X p [ - Z V ( 2 ,m ).( 3 ,i) · m=l

^

( 2 .1 )] )

(4 4)

The eiTors in the output vector are used to adjust the connector w eights betw een the output and the hidden layers and tlien betw een the input layer and the hidden layer, i.e., this eiTor is back propagated to adjust the weights betw een the input and hidden layers.

44

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Firstly, th e output error is calculated. This error is the difference o f the desired output and the calculated output and is given as: E output, i = desired output i - l ( 3j) .............(4.5)

The reflected vector is the product o f the error vector, E output,i and the calculated output vector X oj). This product is scaled b y the com plem ent o f the output vector X pj) for num erical stability. T he reflected vector can be calculated as: R i ~ E output,! · ^0 ( 3 J ) . (1- 1 (3 ^i) ) .............(4.6)

The reflected vector is used to calculate the adjustm ents to the connectors betw een the neuron in the hidden layer and the i^'' neuron in the output layer.

The adjustm ent o f w eights betw een the output and hidden layer can be calculated as: dV(2,i),(3j) = A . Rj . X (2,i) W here. · dv(2 .i).(3 j) is the change o f w eight betw een the j^'' element o f the hidden layer 2 and the i"' elem ent o f on the output layer 3. · · X (2 ..i) is the i'*' neuron value on the hidden layer Constant A is the learning rate. The error o f the hidden layer is found b y taking the product betw een the reflected vector and vector consisting o f the connector weights betw een the hidden neuron X ,2 , j) and the output. A gain, this product is scaled by the product o f the neuron and its com plem ent for num erical stability. n (4.7)

E hiddenj

^ (2,i) · (1 " A . (2,i) ) · E R ^ . V(2,i),(3,m) m=l

(4-8)

45

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Finally, the adjustm ent o f w eights betw een the input layer and the hidden layer is calculated as:
d W ( i j ) ,( 2j ) =

B .E

(hidden,j) · ^ (I,i)

(4 .9 )

W here, · dw(i,i),(2 .j) is the change o f the w eight betw een the i'`' elem ent o f the input layer 1 and the j · · elem ent in the hidden layer 2.

/> . (1,i) is the i*'' neuron value at the input layer. C onstant B is the learning rate.

A djusting the w eight sets between the layers and calculating the outputs is an iterative process and is repeated until the errors fall below a predeterm ined tolerance value. T he allowable tolerance level and learning rates A and B are determ ined through experim ent. Large error tolerances will result in a poorly perform ing neural network, w hile a very small allow able error will result in an excessively long training time. There are two m ost com m only used techniques for setting up the tolerance level and learning rate for netw ork training. i. Starting w ith relatively large eiTor value and reduce it to a desired level, as training is achieved at each succeeding level. ii. I f the network fails to train the network at certain error tolerance value, then increm entally low er the learning rate.

T he follow ing is the flow o f the feed forward (back-propagation) neural netw ork procedure: 1, Initialize the weights.

46

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

2. Present the input data vectors. 3. O perate the neural network. 4. C om pute the error betw een desirable and calculated output. If the eiTor is sm aller than the preset tolerance, stop the algorithm (Current w eights are the final weights). 5. Propagate the eiTors back to all the units towards the input layer. 6. Com pute the adjusting value according to the e n o r and adjust all the weights. 7. If num ber o f iterations exceeds the predetennined number, stop (unsuccessful, adjust learning rates and try the procedure again). Once the training is concluded and the weights are adjusted, the network is available to use for features identification. A flow chart for the feature recognition from geometric reverse engineering data is show n in figure 24.

47
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Start Initialize the w eight
'

" " V

Present the input vectors Assigning the value for each shape

Calculate the weight values between input and hidden layer Calculate the hidden layer neuron value Calculate the output neuron values

Set the condition for the error

Calculate the error

Y es

H

End

Adjust the weights between output and hidden laver
% _________________

Calculate hidden layer error

Adjust the weight between hidden and input layer

Figure 24: Flow chart for neural network-based feature recognition.

48

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 5 - Testing of the Neural Network Algorithm

In this chapter, the training and the testing o f the algorithrn are discussed. The algorithm was first trained with m anually created synthetic data and then tested with noisy data. The algorithm was subsequently tested with real reverse engineered data to fully test the robustness o f the neural network.

5.1 Training o f the N eural Netivork The neural netw ork recognition algorithm m ust first be trained to recognize features from the seven different shapes. M anually created synthetic data is presented to the neural network for training. The desired output for the seven shapes form ed the seven dimensional vectors are as follows: Circle: Diamond : Ellipse: Rectangle: 10 0 0 0 0 0 01 0 0 00 0 00 1 0 0 0 0 0 0 0 10 0 0

49

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Square: Triangle: W heel:

0 0 00 1 0 0 0 0 00 0 10 0 0 00 0 0 1

Figure 25 represents the sample input vectors and corresponding desired outputs. 6.0000 6.0000 0.0936 0

7.0000 7.0000 0.0887 4

9.0000 14.0000 0.0516 0

9.0000 12.0000 0.0494 4

7.0000 7.0000 0.0704 4

5.0000 6.0000 0.1093 3

28.0000 28.0000 0.0195 0

Figure 25. Sample input vectors presented to neural network.

In Figure 25, the first line shows the input vectors while the second line represents the desired output vectors. The next two lines show the sets o f input and output vectors. The sample training vectors file were created by using random num ber generator. The first set o f sample input vectors were the perfect dimensions for each shape. A random num ber generator is then used to create the rest o f the input sample vectors. This input sample file was presented to tire neural network for the training.

50

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Since the solution to the input vectors during the training phase is know n in advance, the neuron values are coiTected by triggering the back - propagation training algoritluu. The transfer function perform s the task to process the neuron values fi'om the input layer to the hidden layer or from the hidden layer to the output layer to generate the desired output. For this work, the h>'perbolic-tangent transfer function is used between the hidden and output layers to train the neural network algorithm. Adjusting the error tolerance to obtain the desirable results played an important role in training the neural network. The eiTor tolerance must be set before the neural network algorithm stops correcting the neuron connectors. One technique to train the neural network algorithm is to start with a large error value and then successively lower the value as the network starts learning. Through testing, an error value o f 0.05 and learning rate values o f 0.4 for A and 0.2 for B showed good agreement and gave reliable results. Also, it is important to decide the number o f training vectors presented to the neural network. It played an important role in the training o f the neural network used

in deciding the type o f feature that was being presented. For this purpose, synthetic data are created to train the neural network algorithm. The neural network algorithm was also tested after adding 10% noise in the synthetic data. The results are mixed, as the neural netw ork algorithm is able to recognize most o f the desired features depending upon the w eight values to initiate the training. It was found that a minimum o f 90 vectors is required to train the neural network to recognize seven different types o f features.

51

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

D uring training, it was also been found that the network was sensitive to the order the training vectors presented. For example, if the first vector pairs presented were to represent a circle, the netw ork biased towards the circle feature. Table 1 shows the output for th e learning rate values o f 0.4 for A and 0.2 for B. The initial weights between the input and hidden layers and hidden to output layers are w = v = 0.005 for k = 400 iterations.

Desired Classification
C ir D ia Elps R ec S q r T ri \V1 C ircle D iam ond E llipse

Network classification
R ectangle S quare T riangle W heel

Circle D iam ond Ellipse Rectangle Square Triangle Wheel

1 0 0 0 0 0 0

0 1 0 0 0 0 0

0 0 1 0 0 0 G

0 0 0 1 0 0 0

0 0 0 0 1 0 0

0 0(19776 0.0224 0 0 LOOOO 0.0000 0 0 EOOOO 0.0000 0 0 TOOOO 0.0000 0 0 TOOOO 0.0000 1 0 TOOOO (10000 0 I 1.0000 0.0000

0.0224 0.0000 0.0000 0.0000 0.0000 0.0000 (10000

0.0224 0.0000 0.0000 0.0000 (10000 (10000 (10000

0.0224 0.0000 0.0000 0.0000 (10000 0.0000 (10000

0.0224 0.0000 0.0000 0.0000 (10000 (10000 (10000

0.0224 0.0000 (10000 0.0000 (10000 (10000 (10000

Table 1 : Set o f classification values with sigmoid transfer function. Table 2 represents the outputs w ith the hyperbolic-tangent transfer function for the sam e param eters (A=0.4, B=0.2, w = v = 0.005 and k =400).

D esired classification
C ir D ia E lps R ec S q r T ri W1 C irc le D iam ond

Network classification
Ellipse R ectangle S q u a re T ria n g le W heel

Circle D iam ond Ellipse Rectangle Square Triangle W heel

1 0 01 00 00 00 00 00

0 0 1 0 0 0 0

0 0 0 1 0 0 0

00 0 00 0 00 0 00 0 10 0 010 00 1

0.0615 0.0036 0.0025 0.0018 0.0000 0.0000 N aN

0.0033 0.0033 0.0033 0.0033 0.0033 0.9687 0.0030 0.0030 0.0030 0.0030 0.0107 0.9722 0.0022 0.0022 0.0022 (k0040 (10067 0.9676 (10017 0.0017 0.0079 0.0000 0.0078 0.9660 0.0000 O.OOOl 0.0000 O.OOOl 0.0134 0.0000 NaN NaN NaN NaN NaN

0.0033 0.0030 0.0022 (h0017 0.0000 0.0000 NaN

Table 2; Set o f classification values with hyperbolic-tangent transfer function.

52

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

W here N aN in Table 2, is the IEEE arithmetic representation in M ATLAB for Not-a-Number. A N aN is obtained as a result o f mathematically undefined operations like 0.0/0.0 and Inf-inf. During the training, it has also been found that the netw ork recognized the circle feature by setting the initial weights w - v - 0.1. It has also been found that the network is not able to distinguish the diamond feature from rectangle and square features if the learning rate \a lu e s are set as A =B -0.2. By setting A = 0.4 and B=0.2, the network clearly recognized the diamond, square and rectangle features. The neural network algorithm was tested on a Dell PHI com puter with 256 RAM, and the total processing time to recognize the features is approxim ately four seconds. The network trained for the geom etric shapes recognized m ost o f the shapes as shown in table 2, but failed to recognize the wheel. One o f the possible reasons is that the wheel has multiple shapes (circle, hexagon, etc.) and the network m ay not be able to recognize all the shapes together.

5.2 Testing o f the Neural Nctrvork After the training was concluded, the neural network should respond to the items not in the training set. One o f the approaches to do this is to select the noise option. The usefulness of the neural network is measured from its response to noisy data, but at the same time the intention for the neural network is not to tolerate unlimited noise. Therefore, to fully test the ability o f the network, the data coirupted with 10% noise was presented to the network. For this purpose, a separate sample input file was created by introducing 10% noise. This sample input file corrupted with noise was presented to the

53
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

neural network. The outputs are significant as the network was able recognize m ost o f the geom etric shapes.

5.3 Testing o f the Algol ithm with Real Reverse Engineering Data To test the robustness o f the segm entation algorithm developed in section 4.6 and the neural network algorithm to recognize features, real reverse engineering data derived from three test samples o f each object feature at different orientations and dimensions w ere used. The results o f segmentation algorithm are shown in Figures 26, 27, 28, 29, 30 and 31.

? .0 l

'

/i

10

15

30

25

a

%

40

Figure 26. Test sample "circle" and its segm ented boundary

54

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

1

'-t S'J'V'ï,'

/

T

\

< * *

Js)xl

, «i-rW +

· +W

4 4

* ._

10

1(i

20

:C

5(1

3^

^0

$0

Figure 27. Test sample "ellipse" and its segmented boundary

55
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

0 a« 5> t K^ /

Figure 28. Test sample "triangle" and its segm ented boundary

56

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

IS
î'ü lri«rt I«fc f nJ*-'

·ùt^aa \ h ^ / ,èér-.

Figure 29. Test sam ple "rectangle" and its segm ented boundary

57
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

^ i< « f»\ e' '

ÇJt W* ÏW î IwM ic e t ^n&Mü y *

'.. T

Jab

:.3 iS Bô ^ A/ /

-I

e

£·

1i)

12

1.1

16

1S

2C

Figure 30. T est sam ple " diam ond" and its segm ented boundary

58

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Jsijd

I r

Figure 31, Test sam ple "square" and its segm ented boundary

The results fi'Om both the segm entation and the neural netw ork algorithm s are prom ising. T he rest o f the test samples and segm entation algorithm results are show n in A ppendix A.

59
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

T he neural netw ork was used to recognize the features o f the object described in Section 1.4. The outputs are satisfactory as the netw ork is able to recognized m ost o f the geom etric object features. Figure 32 represents the input vectors derived from the real reverse engineering data from the objects described in Section 1.4 and presented to the netw ork for feature recognition.

a=5.8159; b=6.1433; alpha=5.3634*pi/180; nc=0; Circle = [1 0 0 0 0 0 0 ] ; a=6.7775; b=7.8032; alpha=5.0817*pi/I80; nc = 4; D iam ond = [ 0 1 0 0 0 0 0]; a=9.1873; b=14.2755; alpha=2.9577*pi/180; nc = 0: Ellipse = [0 0 1 0 0 0 0 ]; a=9.2376; b = l 1.5956; alpha=2,8298*pi/l 80; nc = 4; R ectangle = [0 0 0 1 0 0 0]; a=7.6679; b=8.2436; alpha=4.0350*pi/l 80; nc = 4; Square = [ 0 0 0 01 0 0 ]; a=4.5739; b=6.0755; alpha=6.2626*pi/180; nc = 3; Triangle = [0 0 0 0 0 1 0]; a=28.3722: b=28.7355; alpha=l.l 181*pi/180; nc = 0; Wheel = [0 0 0 0 0 0 I];

Figure 32. Input vectors derived from real reverse engineering data.

The segm entation algoritlmrs developed for this work are able to correctly identify the boundaries o f the physical objects from the huge, noisy, incom plete and unstructured scanned data and then calculate the im portant param eters o f each object that

60
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

fo n n the input vectors for the neural network. The feed-forw ard neural netw ork used for this w ork to recognize features from reverse engineered data are efficient in terais of: 1. Processing Time; The neural network algoritlim took approxim ately

four seconds for processing to correctly recognize the feature. 2. Efficiency; The neural network algoritlim when used with the

hyperbolic-tangent transfer function is m ore efficient in recognizing the features o f the geom etric object as compared to the sigmoid transfer function which was biased towards one shape. 3. Tolerance Level; The neural network algorithm w orks w ithin the

error tolerance level o f 0.05 and learning rates o f 0.4 for A and 0.2 for B. The results o f the teclmique (Feed-Forward Neural N etw ork) used in this work for features recognition and the algorithms developed for segmentation to obtain im portant param eters are prom ising to recognize the feature, both in terms o f efficiency and processing time.

61

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 6 - Conclusion and Future Work

In this work, a feed-forward neural network based feature extraction system for geom etric reverse engineering data is presented. Geometry and topology data associated w ith the object boundary are derived from m ethods described earlier in Section 4.5, and an appropriate input vector for the neural network algorithm was derived. The main emphasis is to construct a CAD model fi'om geom etric reverse engineering data b y applying a feature recognition teclmique. Neural netw ork feature recognition from reverse engineering is promising. Its capability in handling the noisy and often incom plete data set confinns its desirable feature over conventional rule-based algorithm s. Two segm entation algorithms, one for a single shape and the second for a set o f m ultiple shapes (wheel), w ere developed to first segment the boundaiy o f the object from huge and unstimctured cloud data and then calculate the im portant param eters that form the input vector. A feed-forward neural network is used to recognize the features. Both, the segm entation and neural netw ork algorithms are program m ed in M ATLAB.

62
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

The testing o f the algorithm shows promising results, as the neural netw ork was able to recognize the features with an error tolerance o f 0.05. W hether this technique can provide sufficient recognition capability to seiwe a universal set o f features in any category o f physical parts and how the artificial neural netw ork should be structured for this purpose require further research. A lthough considerable work to extract features from scan data for geom etric shapes has been realized. Expansion o f the num ber o f defined features, m ultiple shape objects as well adding to the number o f surfaces o f which the feature can be com posed, for example, filleted comers and other types o f curved surfaces and finer details will be a future extension.

63

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Appendix A
Test sam ples and their segm ented boundaries

64

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

m m
' » T {.' V'»»-__,

f* »- %JT4, % ,..; :D ô =« *', > l -A A;/r V » J5 ^

awmi

10

f ^ S lu rll ^

** g J a p { » r u j.< _

0

-Mic . )

IC (\C :\P oC '.fn m W 4n. llfflfigufpNo. i

Figure 33. Test sample "square" and its segmented boundary

65

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

'^ F l g i a e t o . ! ..

P ie 'E (S V w wIn W 'T W i^ W n ik w .riH e ÿ z /: jo Q

+*.+ + + t+ + + + K
15 20 a

j;s ta rt| l é S IbJ " ^/«ieMk_B-t.*:..i |

: | #C;mo&mnKai.,.||8|]fK»MNo, l

;ip7;5ÏAM '

Figure 34. Test sample "circle" and its segmented boundary

66
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4

;» .,,I m . -u

4 5

OS Hg

W -

-f '^""1 e * 5 Z 3

Mr... | * W T u n

_ _

| ?>c ''Oar-irenu a-. . 1 |B f^u,!'N o.

1

Figure 35. Test sample "ellipse" and its segmented boundary

67
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Mm»

fit

Cd> V*ew lm «l Tool: · Windcw Hcb

oaea^tA/ / ^ 0 c

10

15

20

25

33

40

4C

M

55

i

S ta rt| ^

iJ

"

· 1^^MATLAB__________ | ^ C lD c c u m e r itS ^ . ..

||B

n g u rc No. 1

Figure 36. Test sample "ellipse" and its segmented boundary

68
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

.

F<« Ê il Vcvj Initfil Toofc W 'jA V Heb '

;D ô: B s

»( A y

/

ü' O

r*
S i iij " - Me. j C^WTL&B I Ji-C tC o-m ^n'. JWINq. 1 » QiilAM

Figure 37. Test sam ple "rectangle" and its segmented boundary

69

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

: '.-? V :i3

Fik

Ell*

V*s«v l»«eil

T w it

W mjk'w

Hr%* '

:0 G ^B @ ^ A / / i

10

11

12

13

^ S ta r 11 j û S

IE) ^ >^-^O CK txlK j «f-lc... I «^XMATLAO

j ^ C ^ D o a r o n t s an... | j^

rk x « g No. l

,** B 1 2 iM

Figure 38. Test sam ple "square" and its segm ented boundary

70

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

' 

.

'

Fwg £ti*

v^v,

TocJj

W f x tw

He\?

o i ^ wa t A / /

10

:

J Start)

^APtCT^i<,B-m.- 1c^HATUB_______ | ^ CACW/TOlb ^1. llHFtgure No. 1

,« S.lSfM

Figure 39. Test sample " square" and its segm ented boundary

71

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

£<W Vn t* lm « i Tcwfa Wndow He*>

 ©: H

S ` *t A /

/

^ &n

i i S U r t | .-©

y

^

ftpcenda.g »><V:.,. |

K&TIAP_________ } % C \DocimenU an... jlB O q u r p No. 1

|«

8:16 Al/

Figure 40. Test sample "triangle" and its segm ented b o u n d a r

72
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Fie Ed* View )p>w( T w t. Wi>dow ,ht*) :* D c ^ ' c

.

. T S f d r tf ^

s . ] [ i J *· fj} A ? p g n d r» .6 -M c ... I tjtFMATLAg

'

| ^C ;\C « X u 7 iefn s a i... )) B R g u r e NO. 1

"

8 16 4 «

Figure 41. T est sumple "triangle" and its segm ented boundary

73

Reproduced with perm ission of the copyright owner. Further reproduction prohibited without permission.

-':' -.< ' > : !  : ffîiîS

Ffe Edi» V«*< |nM4 % » i: WiVkw

-

«b cs;b ;^!LV^`>:"^ i #%-

t i StM ll it> E i S

" @ A K « K k _ e - l i t . . . I  ^ tw U A E

I Sfii:;iÔ oaroenlsjn.,. il B n g u r e No. 1

"

TÆSÂh

Figure 42. T est sam ple "diam ond" and its segm ented boundary

74

Reproduced with permission o f the copyright owner. Further reproduction prohibited without permission.

Appendix B
G lossary o f Term s

75

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

TeiTO

Definition

AÎ4N

Artificial N eural N etw ork - a com puter algorithm based on the architecture o f a biological brain. Inform ation that was know n beforehand. Boundary Representation - A m ethod used by CAD Programs to model a solid w ith its boundaries. M ethod to update connector w eights in m ulti-layer neural networks based on the error. Com puter Aided D esign - A com puter program that allows for design on a computer. Com puter Aided M anufactudng - a com puter program to aid in the planning o f a m anufacturing process. Charged Coupled Device -- a light sensitive m icrochip used to capture im ages in video camera. Term used to describe the cloud like structure o f data collected by scanner or sensor. Coordinate M easuring M achine - a precise m achine used in industry to measure surface points Type o f neural network where in training, there can only be one correct neuron. W eighted links between neurons. A com bination o f geom etric entities that together have a m eaningful purpose. A surface not made o f any geom etric primitives. S elf O rganizing M ap - A neural netw ork based on com petitive learning among neighboring neurons A node in a neural network.

A Priori B-rep

Back-Propagation

CAD

CAM

CCD

Cloud D ata

CM M

C om petitive learning

Connectors Feature

Free-fonn surface K ohonen SOM

N euron

76

Reproduced with permission o f the copyright owner. Further reproduction prohibited without permission.

Topology

The spatial relationship o f different surfaces to each other. A sensor used for m aking measurements. A cubic volume derived from a large volume.

Touch probe Voxel bin

77

Reproduced with permission of the copyright owner. Futthor reproduction prohibited without permission.

Appendix C
M A TLAB Program codes

78
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

% ----------------------------------------------------------% M ATLAB code for segm entation o f data for single shape object % -----------------------------------------------------------------------------------------------clear all close all load circle.txt; % Loading Circle text file X = circle(-.,l); y = circle(;, 2 ); z = circle(:,3); m z = mean(z); % A verage (mean) o f z ordinate [n f]=size(z); pitch = 0.5 % Scanning pitch c= 0; for m = 1 :n i f z(m) < mz c = c+ 1 ; zx(c,l)=x(m ); zy(c,I)=y(m ); end end % ------------------- Centre o f the geometric object---------------------------zxm = m ean(zx); zym = m ean(zy); % M ean o f x ordinate % M ean o f y ordinate

% -------------------------Boundary points for constant y----------------------y_m in = min(zy); % M ininum values o f y y_m ax = max(zy); % M axim um values o f y a=zy.'; % y values in rows.

% nl -number o f different points on y after eleminate the noise. y 1 =y_min :pitch; y_max; [fl,n l]= siz e (y l); for j = 1 :nl y_num = find(a==yl(j)); x_val = zx(y_num);

79

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

x_m in = m in(x_val); x_m ax = m ax(x_val); p(j) = x_min; q(j) = x_m ax; end % -------------------------- Boundary points for constant xx_m inl = niin(zx); % M ininum values o f y x_m axl = m ax(zx); % M axim um values o f y al=zx.'; %x values in rows. X1 -x _ m in l :pitch:x_m axl ; [f 2 ,n 2 ]= size(xl); % size o f the data for u = 1 :n2 x_num = tin d (al= = x l(u )); y v a l = zy(x_num ); y_m inl = = m in(y_val); y m a x l = m a x (y v a l); pp(u) = y_m inl ; qq(u) = y_ m ax l; end % ------------------- Elim ination o f duplicate points------s= 0 ; for k = l:n 2 for g = l:n l if (((x 1 (k)==p(g)|x 1 (k)==q(g))& (yl (g)==pp(k)|y 1 (g )= q q (k ))) l((xl (k)==p(g)|xl (k)==q(g))& (yl (g)~=pp(k)|yl (g)~=qq(k))) l((x 1 (k)~=p(g)|xl (k)~=q(g))& (yl (g)==pp(k)|yl (g)==qq(k)))) s=s+l ; X (s)= xl(k);

Y(s)=yi(g);
end end end % ------------- Re-arrange the boundary points in clockwise o rd e rX C =X ([l:6 9 10 13 14 16 17 19:2:51 54 53 57 63 62 61 67:-l:64 60 59 58 56 55 52:-2:18 15 11 12 7 8 ]); Y C =Y ([l:6 9 10 13 14 16 17 19:2:51 54 53 57 63 62 61 67:-l:64 60 59 58 56 55 52: 2:18 15 11 12 7 8 ]); %n3 -no.of boundary points after eleminate the same points.

80

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[f3,n3]=size(XC); %------------------- Distance between boundary points and centi'efor w = l :n3 dl(w ') = sqrt((YC(w)-zym)^2 + (XC(\v)-zxm)^2); end %

% distance from the center

Calculating the angles between the lines-------------------

%d 2 distance between 2 adjacent point. % angl, angle between 2 lines by using cosine rule(from center). % ang 2 , angle between 2 lines by using cosine rule (from 1 st coner) for w l = l:(n 3 'l) d 2(w l) = sqrt((Y C (w l)-Y C (w l+l))^2 + (X C (w l)-X C (w l+ l)r2 ); a n g l(w l) = a c o s(((d l(w l ) ) '^ 2 + (d l(w l+ l))^ 2 (d2(w] )r 2 )/(2 * d l (w l)*d ] (w l+1 )))* 180/pi; ang 2 (w l) = acos(((dl(w d ) ) ^ 2 +(d2(wl))^2(d 1(w ] +1 ))^2)/(2*d 1(w l )*d2(wl )))* 180/pi; end %------------------------Calculating the corner formed-----------------------------% adding the
2

adjacent angles on the a boundary point to find the corner.

for w 2=l:(n3-2) ang3(w2) = (ang2(w 2+l )+(180-(ang2(w2)+angl(w2)))); end %ang3.' num _of_coners=fm d(ang3==l 80); [fc,nc]=size(num_of_coners); dispC Y X') [XC.',YC.'] dispCDistance from center') d l .' dispCang(ccnter)') an g l.' plot(XC,YC,'x') hold on plot(zxm,zym,'x') disp('M ean distance horn center') m ean(d 1 )

81

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

dispCM aximuin distance from center') m ax (d l) dispCM ean angle betw een two lines') m ean (an g l) disp('N um ber o f coners') nc-4 % iprintf('\n') % fprintf(' % 12.1f\t % 12.If\n % 12.1f\t % 12.1frn',F0,FLF2,F3)

82

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

%----------------------------------------------------------------------------------------------------------

% M ATLAB codes for data segmentation for multiple shape object (wheel) % ----------------------------------------------------------------------------------------------------------clear all close all load w heel.txt; % Loading wheel text file X = w heel(:,l); y = w heel(;, 2 ); z = wheel(:,3); mz - mean(z); % A verage (mean) o f z ordinate [n f]=size(z); pitch = 0.5 % Scanning pitch %mean o f z < z values c= 0; for m = I ;n if z(m) > m z c = c+ 1 ; zx(c,l)=x(m ); zy(cj)= y(m ): zz(c,l)= z(m ); end end
0/

70------------------------Centre o f the wheel % M ean o f x ordinate % M ean o f y ordinate

zxm = m ean(zx); z>TO = mean(zy);

% Find the min & max y values o f the d a ta ------------------y m i n = min(zy); y m a x = max(zy); ay=zy.'; % y values in rows. yl=y_m in:pitch;y_m ax; [fy l,n y l]= siz e (y l); % -- Boundary points on the outer circle & innner circles for const y -- sy=0; for Jy = 1:nyl

83

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

y_niam = find(ay= =yl (jy)); x_val = zx(y_num); x_m in(jy) = m in(x_val); x_m ax(jy) = m ax(x_val); [ny 2 fy 2 ]=size(x_val); for iy = l:(n y 2 - l) ; if ((x_val(iy+1)-x_val (iy) )> I ) sy=sy+l ; % sy n o .o f pts inside the big circle X b l(sy ) = x v a l(iy ); Xb2(sy) = x _ v al(iy+ l); Y b l(sy ) = yl(jy); end end px(jy) = x__min(jy); % outer circle pts qx(jy) = x m a x fjy ); end % -- Separate the RHS points belongs to 2 circles and groubed as 1st & 4the Q uadrant e y l= 0 ; ey 2 = 0 ; ey3=0; ey4=0; ey5=0; fo rty = l:(s y -l) if ((Yb 1(ty)>=33)& (X bl (ty)>zxm)) e y l= e y l+ l; Y C b l(ey l)= Y b l(ty ); X C b l(ey l)= X b l(ty ); X Cbl2(ey1)=Xb2(ty); elseif ((Yb 1(ty)>zym )& (X b 1(ty)<27.5)) ey 2 =ey 2 + l ; Y Cb2(ey2)=Ybl(ty); XCb2(cy2)=Xbl(ty); XCb22(cy2)=Xb2(ty); elseif ((Y bl(ty)<25.5)& (X bl (ty)<zxm)) ey3=ey3+l ; YCb3(ey3)=Ybl(ty); XCb3(ey3)=Xbl(ty); XCb32(ey3)=Xb2(ty); elseif ((Yb 1(ty)<zym )& (X b 1(ty)>3 5)) ey4=ey4+l ; Y Cb4(ey4)=Y b 1(ty); X Cb4(ey4)=Xbl (ty); XCb42(ey4)=Xb2(ty);

84

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

else

cy5=ey5+l ; YM b(ey5)=Y b 1(ty); X M b l(ey 5 )= X b l(ty ); X M b2(ey5)=X b2(ty); end end % M in. & max. x values o f the data x_m in = min(zx); x_m ax = max(zx); %x values in rows. X 1 =x_m in ;p itch ;x j n ax ; [fx l,n x l]= siz e (x l); % Boundary points on the outer circle & innner circles sx= 0 ; fo rjx = 1 :nxl x_num = find(ax==x 1 (jx)); y_val = z y (x n u m ): y m in(jx) = m in(y_val); y_max(jx) = max(y_val); [nx 2 fx 2 ]=size(y_val); for ix= l ;(nx 2 -l): if ((y_val(ix+l )-y_val(ix))>l )
S X --S X + 1 ;

Y l(sx ) = y_val(ix); Y2(sx) = y_val(ix+ l); XI (sx) = x l(jx ): end end py(jx) = y_min(jx); qy(jx) = y_max(jx); end e x l= 0 ; ex 2 = 0 ; ex3=0; ex 4=0; ex5=0; for tx=l ;(sx -l) if ((Y 1(lx)>=33)& (X l (tx)>zxm)) e x l= e x l+ l; X C l(ex l)= X l(tx );

85

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Y C l(e x l)= Y l(tx ); Y C 12(exl)=Y 2(tx); elseif ((Y 1(tx)> zym )& (X l (tx)< 27.5)) ex 2 = ex 2 -i 1 ; X C 2(ex2)=X l(tx); Y C 2(ex2)=Y l(tx); Y C22(ex2)=Y 2(tx); elseif ((Y l(tx )< 2 5 )& (X l (tx)<zxra)) ex3= ex 3 + l; X C 3(ex3)=X l(tx); Y C 3(ex3)=Y l(tx); YC 32(ex3)=Y 2(txl: elseif ((Y l(tx )< zy in )& (X l(tx )> 3 5 ,5 )) ex4=ex4+l ; X C 4(ex4)= X l(tx); Y C 4(ex4)= Y l(tx); Y C42(ex4)=Y2(tx); else ex5 = ex 5 + l; XI\/l(ex5)=Xl(tx); Y M l(cx 5 )= Y l(tx ); Y M 2(ex5)=Y2(tx): end end % -- Elem inating the duplicate points and Re-aiTanging the points in clockw ise direction m 3=0; m4=0; ni5=0; m6=0; m7=0; for m l= l x x l for n i 2 = l :ey] if (((X C 1(m I )==XCb l(m 2) |XC 1(m 1) = X C b 12(m2))&( YCb 1(m 2 )= Y C 1(m 1)|YCb 1(m2) = Y C 12 (m I ))) |((XC 1(m l ) = X C b 1(m2)|XC 1 (m 1) = X C b 12(m2))&(YCb 1(m 2)-= Y C I ( m l)|Y C b l(m 2 )-= Y C 1 2 (m l)))|((X C l(m l)-= X C b l(m 2 )|X C l(m l)-= X C b l2 (m 2 ))& (Y C b 1(m 2 )= Y C 1(m 1)| YCb I (m 2 )= Y C 12(m 1)))) m 3= n i3 + l; X A l(m 3 )= X C l(m l); Y A l(m 3)= Y C b l(m 2); end end end for in 8 = l :exl

86
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

if(Y C l(m 8 )= m in (Y C l)) m 4 "m 4 + l ; X A 2(m 4)=X C l(m S): Y A 2(m 4)= Y C l(m 8); end end fo r m 9 = l;e x l if(Y C 12(m 9)> = 49.5) m 5 = m 5 + l; X A 3(m 5)=X C l(m 9); YA3(m 5)=YC 12(m 9); end end fo r in 1 0 - 1 ;eyl if (XCb 1im 1 0 )= m in (X C b I )) m 6 =m 6 + l ; X A 4(m 6)=X C b 1(m 10); YA4(mCl)=YCb](mlO); end end for m l 1 = 1 : 1 if (XC 'b ;. l(m 11 )==m ax(XCb 12)) m 7= ni7+ 1, X A 5 (m 7 )= X C b l2 (m ll); Y A 5 (m 7 )= Y C b ](m ll); end end X A 6= [X A 4 X A l X A 5 X A2 XA3]; Y A 6= [Y A 4 Y A l Y A 5 Y A 2 YA3]; X A =X A 6([1:10 13:15 18 20 21 23:2:35 84:95 39:2:51 54 57 56 62 61 69:-l:66 73:-l:70 65 64 63 60 59 58 55 53 52 50:-2:38 37 83:-l :74 36:-2:22 19 16 17 11 12]): Y A =Y A 6([1:10 13:15 18 20 21 23:2:35 84:95 39:2:51 54 57 56 62 61 69:-l:66 73:-l:70 65 64 63 60 59 58 55 53 52 50:-2:38 37 83:-l:74 36:-2:22 19 16 17 11 12]); [X A .'Y A .']; % ----------------------------n3=0; n4=0; n5=0; n6 =0 ; n7=0; for n l= l:e x 2 for n 2 = l :ey 2 if (((X C 2 (n l)= X C b 2 (n 2 )|X C 2 (n l)= X C b 2 2 (n 2 ))& (Y C b 2 (n 2 )= Y C 2 (n l)|Y C b 2 (n 2 )= Y C 22(n l)))|((X C 2(n l)= X C b 2(n 2)|X C 2(n l)= X C b 22(n 2))& (Y C b 2(n 2)-= Y C 2(n l)|Y C b 2

87

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

(n 2M = Y C 22(nl)))|((X C 2(nl)-^ X C b2(n2)|X C 2(nl)-= X C b22(n2))& (Y C b2(n2)= Y C 2(n l)|Y C b2(n2)= = Y C 22(nl)))) n 3 = n 3 + ]; X B l(n 3)= X C 2(n l); Y B i(n3)=Y C b2(n2); end end end for n 8 = l :ex 2 if(Y C 2 (n 8 )= m m (Y C 2 )) n4 = n 4 + l; XB2(n4)=XC2(n8): YB2(n4)=YC2(n8); end end for n9=l ;ex2 i f C\^C22(n9)=m ax(YC22)) n 5 = n 5 + l; XB3(n5)=XC2(n9); YB3(n5)=YC22(n9); end end fo r n lO = l;e } 2 i f (XCb2(nl 0)==mm(XCb2)) n 6 =n 6 + l ; XB4(n6)=XCb2(nlO); YB4(n6)=YCb2(nlO); end end for n l l = l;e y 2 i f (X Cb22(nl 1)= m ax(X C b 22)) n7=ii7+l ; X B 5(n7)= X C b 22(n ll); Y B 5(n7)= Y C b2(nll); end end X B 6=[X B 4 XB1 XB5 XB2 XB3]; Y B 6=[Y B 4 Y B l YB5 YB2 YB3]; XB=XB6([1:11 14 15 18 19:2:39 40 89:94 41:2:61 65 64 70 69 68 80:-l:71 67 66 63 62 60:-2:42 88:-l:81 38:-2: 20 16 17 12 13]); YB=YB6([1:11 14 15 18 19:2:39 40 89:94 41:2:61 65 64 70 69 68 80:-l:71 67 66 63 62 60:-2:42 88:-l:81 38:-2: 20 16 17 12 13]); [X B .',YB.']; %--------------------------------------------------p3=0; p4=0;

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

p5=0; p6=0; p7=0; for p l= l;e x 3 for p2=l:ey3 if (((X C 3(p l)= = X C b 3(p 2)|X C 3(p l)= X C b 32(p 2))& (Y C b 3(p 2)= Y C 3(p l)|Y C b 3(p 2)= Y C 32(p l)))|((X C 3(p l)= X C b 3(p 2)|X C 3(p l)= X C b 32(p 2))& (Y C b 3(p 2)--Y C 3(p l)|Y C b 3 (p2)-=Y C 32(pl)))|((X C 3(p])-=X C b3(p2)|X C 3(pl)-=X C b32(p2))& (Y C b3(p2)=Y C 3(p l)|Y C b 3 (p 2 )= Y C 3 2 (p l)))) p3=p3+l ; X D l(p3)=X C 3(pl); YDl(p3)=Y Cb3(p2); end end end fo rp 8 = l:e x 3 if(Y C 3 (p 8 )= m in (Y C 3 )) p4=p4+] : XD2(p4)=XC3(p8); YD2(p4)-Y C3(p8); end end for p9=l ;ex3 i f (YC32(p9)==max(YC32)) p5= p5+ l; XD3(p5)=XC3(p9); YD3(p5)=YC32(p9); end end for plO =l:ey3 if (XCb3(p 10 )= m in (X C b 3)) p 6 = p 6 + l; X D4(p6)-XCb3(plO); YD4(p6)=YCb3(plO): end end for p i 1= 1:ey3 if (XCb32(pl l)= m a x (X C b 3 2 )) p7=p7+l ; X D5(p7)=XCb32(p11); Y D 5(p7)=Y C b3(pll); end end X D 6=[X D 4 X D l XD5 XD2 XD3]; Y D 6=[Y D 4 Y D l Y D5 Y D2 YD3];

89

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

XD=XD6([1 2 7:11 15 16 18 19 22:2:36 38:43 94 95 44:46 48:2:62 65 64 68 73 72 71 83:-l:74 70 69 67 66 63:-2:47 93:-l:84 37:-2:23 20 21 17 12:14 3:6]); YD=YD6([1 2 7:11 15 16 18 19 22:2:36 38:43 94 95 44:46 48:2:62 65 64 68 73 72 71 83:-l:74 70 69 67 66 63:-2:47 93:-l:84 37:-2:23 20 21 17 12:14 3:6]); [X D .',Y D.'l; %-------------------------------------q,;.=0 ; q4=0; q5=0: q6 = 0 ; q7=0; f o r q l= l:e x 4 for q2=l :ey4 if (((XC 4(ql )= X C b 4(q2)|X C 4(q 1)= X C b 42(q2))& (Y C b 4(q 2)= Y C 4(q 1)|Y C b 4 (q 2 )= Y C 42(q])))|((X C 4(q l)=X C b 4(q 2)|X C 4(q l)=X C b 42(q 2))& (Y C b 4(q 2)-= Y C 4(q l)|Y C b 4 (q2)-=Y C 42(ql)))|((XC 4(qI)-=XC b4(q2)!XC 4(ql)-=X C b42(q2))& (Y Cb4(q2)==Y C4(q l)|Y C b 4 (q 2 )= Y C 4 2 (q l)))) q 3 = q 3 + l; X E l(q3)=X C 4(ql); YEl(q3)=YCb4(q2): end end end for qS=l :ex4 if(Y C 4 (q 8 )= m in (Y C 4 )) q 4 = q 4 + l; XE2(q4)=XC4(q8); YE2(q4)=YC4(q8); end end for q9= l:ex 4 if (Y C 42(q9)=m ax(Y C 42)) q5= q5+ l; XE3(q5)=XC4(q9); YE3(q5)=YC42(q9); end end for qlO =r.ey4 if (XCb4(q 10)= m in (X C b 4)) q 6 =q 6 + l ; XE4(q6)=XCb4(qlO); YE4(q6)=YCb4(qlO); end end

90

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

for q l l= l;e y 4 if(X C b 4 2 (q ll)> = 5 2 ) q 7= q7+ l; X E5(q7)=X C b42(qll); Y E 5(q7)=Y C b4(qll); end end XE6=[XE4 X E l XE5 XE2 XE3] Y E6=[Y E4 Y E l YE5 YE2 YE3] X E=X E6([]:10 14:16 18 21 22:2:38 56 55:-2:39 85:-l:75 37:-2:23 19 20 YE=YE6([1:10 14:16 18 21 22:2:38 56 55:-2:39 85:-l:75 37:-2:23 19 20 [XE.\YE.'];
%-------------------------------------------------

86:96 40:2:54 58 57 62 61 74:-l:67 66:-l:63 60 59 17 11:13]); 86:96 40:2:54 58 57 62 61 74:-l:67 66:-l:63 60 59 17 11:13]);

r3=0; r4=0; r5=0; r6 = 0 ; 1-7=0; for rl= l:e x 5 fo rr2 = l;e y 5 if (((X M (r])=X M bl(r2)|X M (rl)= X M b2(r2))& (Y M b(r2)== Y M l(rl)|Y M b(r2)=Y M 2(rl )))|((X M (rl)= X M bl(r2)|X M (rl)= X M b2(r2))& (Y M b(r2)-= Y M l(rl)|Y M b(r2)-= Y M 2( rl)))|((X M (rl)-=X M bl(r2)|X M (rl)-=X M b2(r2))& (Y M b(r2)=Y M ](rl)tY M b(r2)=Y M 2(rl)))) r3=r3+l ; XFl(r3)=XM (rI); YFl(r3)=YMb(r2); end end end for rS=l:ex5 if(Y M l(r 8 )= m in (Y M l)) r4=r4-t-l ; XF2(r4)=XM(r8); YF2(r4)=YM 1 (1-8 ); end end for r9=l :ex5 i f (YM 2(r9)==max(YM 2)) r5=r5+l ; XF3(r5)=XM(r9); YF3(r5)=YM2(r9);

91

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

end end for rlO = r.ey5 if (XM b 1(r 10)==m in(X M b 1)) i'6 = r 6 + l; XF4(r6)=XM bl(rlO); YF4(r6)=YM b(rlO ); end end for r l l = l :ey5 if (XMb2(rl 1)= m a x (X M b 2 )) r7 = r7 + l; X F5(r7)=XM b2(rll); Y F 5 (r7 )= Y M b (rll); end end XF6=[XF4 X Fl XF5 XF2 XF3]; YF6=[YF4 Y F l YF5 Y F2 YF3]; XF=XF6([1:4 6:9 U :2:17 18 42:45 22:2:26 29 34 33 38:-I:35 32:-l:30 28 27:-2:21 20 19 41 40 39 16 14 12 10 5]); Y F -Y F 6 ([l:4 6:9 11:2:17 18 42:45 22:2:26 29 34 33 38:-l:35 32:-l:30 28 27:-2:21 20 19 41 40 39 16 14 12 10 5]); [XF.',YF.']; %-----------------------------------------s3=0; for s l= l;jx for s2 =l :jy if (((xl(sl)=^=px(s 2 )|x l(sl)= = q x (s 2 ))& (y l(s 2 )= = p y (sl)ly l(s 2 )= ^ q y (sl)))|((x l(sl)= = p x (s 2 )| x l(s l)= = q x (s 2 ))& (y l(s 2 )~ = p y (sl)|y l(s 2 )~ = q y (sl)))|((x l(sl)~ = p x (s 2 )|x l(sl)~ = q x (s 2 ))&( y l( s 2 ) = p y ( s l) |y l ( s 2 )== qy(sl)))) s3= s3+ l; X G l(s3 )= x l(sl); Y G l(s3)= yl(s2); end end end XG=XG1([1:10 18:23 28:31 35:37 41:43 46:48 51 52 55 56 59 61 62 65 67 68 71:2:79 80:2:242 245 244 247:2:251 254 257 256 260 263 262 267 266 270 274 273 279:-l :277 285:-l:283 291:-1:289 299:-l:296 314:-1:308 322:-l:315 307:-l:300 295:-l:292 288:1:286 282:-l:280 276 275 272 271 269 268 265 264 261 259 258 255 253 252:-2:246 243:-2:81 78:-2:72 69 70 66 63 64 60 57 58 53 54 49 50 44 45 38:40 32:34 24:27 11:17]); YG=YG1([1:10 18:23 28:31 35:37 4 1 :43 46:48 51 52 55 56 59 61 62 65 67 68 7 1 :2:79 80:2:242 245 244 247:2:251 254 257 256 260 263 262 267 266 270 274 273 279:-l :277

92

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

285:-l:283 291:-1:289 299;-l:296 314:-1:308 322;-l:315 307:-1:300 295;-l:292 288:1:286 282:-l:280 276 275 272 271 269 268 265 264 261 259 258 255 253 252:-2:246 243:-2:81 78:-2:72 69 70 66 63 64 60 57 58 53 54 49 50 44 45 38:40 32:34 24:27 11:17]); [XG.',YG.']; %------------ Distance between center and a boundary p o in t-------------ma=(m3 +m 4+m 5+m6+m7) ; nb=(n3+n4+n5+n6+n7); pd=(p3+p4+p5+p6+p7) qe=(q3+q4+q5+q6+q7): fl~ ( 1-3 +r 4 -i-r5 +r6+r7 ) ; %-------------- 1St Qiiard circle---------------- -for w a= l ;ma da(wa) = sqrt((YA(wa)-mean(YA))'^2 + (XA(wa)-mean(XA))^2); end %-------------- 2 nd Quaed c irc le ---------------for w b = l :nb db(wb) = sqrt((YB(wb)-mean(YB))'^2 + (XB(wb)-mean(XB))^2); end %---------------3rd Quaed c irc le -----------------for w d=l :pd dd(wd) = sqrt((YD(wd)-mean(YD))'^2 + (XD(wd)-mean(XD))^2); end %--------------- 4th Quaed c irc le ------------------for w e=l :qe de(we) = sqrt((YE(we)-mean(YE))^2 + (XE(we)-mean(XE))^2); end % ---------------- middle c irc le ---------------------for w f= l :rf df(wf) = sqrt((YF(\vf)-mean(YF))^2 + (XF(wf)-mean(XF))^2); end %---------------- outer c irc le ---------------------for w g = l :s3 dg(wg) = sqrt((YG(wg)-mean(YG))'^2 + (XG(wg)-mean(XG))^2); end

93

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

% ----------------------Calculating the angles betw een the lines% Finding th e distance betw een 2 adjacent point. % Finding the angle betw een 2 lines b y using cosine rule(from center). % Finding the angle betw een 2 lines b y using cosine rule (from 1st coner o f the triangle). for w a l= l ;(m a-l) d a l(w a l) = sqrt((Y A(wa 1)-Y A(wa 1+1 ))'^2 + (X A (w al)-X A (w al+ l))^2); a n g A l(w a l) = acos(((da(w al))^2 + (d a(w al+ l))^2 (d a l(w a l ))'^2 )/( 2 * da(w al)* d a(w al+ l )))*! 80/pi; angA 2(w al) = acos(((da(wal))^2 + (d al(w al))^2 (d a(w al + 1 ))'^2 )/( 2 *da(w al )*dal (w al )))*! 80/pi; end for w b l= l :(nb-l) d b i(w b l) = sq rt((Y B (w bl)-Y B (w bl+ l))^2 + (X B (w bl)-X B (w bl+ l))'^2); an g B l(w b l) = acos(((db(wbl))^2 +(db(w bl+ l))'^ 2 (db 1(wb l))^2)/(2*db(wb 1)*db(wb 1+1 )))* 180/pi; angB 2(w bl) = acos(((db(wb 1))^2 +(dbl(w bl))'^2(db(wb l+ l))^ 2)/(2 *db(wb 1)*db 1(wb I )))* 180/pi; end for w d l= ] :(pd-l) d d l(w d l) = sqrt((YD(wdl)-YD(wdl+]))'"2 + (X D(w dl)-XD (w dH -l))^2); ar.g D l(w d l) = acos(((dd(wd 1))^2 +(dd(w dl+l))'^2(dd 1 (wd 1 ))'^2 )/(2 *dd(w dl )*dd(w dl + 1 )))* 180/pi; angD 2(w dl) = acos(((dd(w dl))^2 +(ddl(w dl))'^2(d d (w d l+ l ))'^2 )/( 2 *dd(w dl)*ddl(w d !)))*] 80/pi; end for wel = l ;(qe-l) d e l(w e l) = sqrt((Y E (w el)-Y E (w el+ l))^2 + (XE(we 1)-XE(we 1+ 1))^2 ) ; a n g E l(w e l) = acos(((de(w el))^2 + (d e(w el+ l))^2 (d el(w el))'^ 2 )/( 2 *d e(w el)* d e(w el+ !)))* ! 80/pi; angE 2(w el) = acos(((de(wel))'^2 + (del(w el))^ 2(d e(w el+1 ))^2)/(2*de(wel )*del (w el )))* 180/pi; end for w fl= l:(rf-l) d fl(w fl) = sqrt((Y F(w fl)-Y F(w fl+l))^2 + (X F(w fl)-X F (w fl+l))^2); a n g F l(w fl) = acos(((df(wfl))'^2 +(df(w fl+l))'^2(d fl (w fl ))^2)/(2*df(wfl )* d f(w fl+ l )))* 180/pi ; angF 2(w fl) = acos(((df(wfl))'^2 + (d fl(w fl))^ 2 (d f(w fl+1 ))^2)/(2*df(wfl )*dfl (w fl )))* 180/pi; end

94

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

for w gl=^l:(s3-l) d g l(w g l) = sqit((Y G (w gl)-Y G (w gl + l))^2 + (X G (w gl)-X G (w gl+l))-^2); an g G l(w g l) = acos(((dg(w gl))^2 +(dg(w gl+ l))'^ 2 (dgl (w gl ))'^2 )/( 2 *dg(w gl )* d g (w g l + 1 )))* 180/pi; angG 2(w gl) = acos(((dg(w gl ))'^2 +(dgl(w gl))'^2(dg(wg 1 + 1 ))'^2 ) / ( 2 * dg(wg l)* d g l (wg 1 )))* 1 8 0 /pi ; end %------------------------------------------dispC YA X A ') [XA.\YA.'] disp('Distance from center (da)') da.' disp('angA l (center)') angA l.' dispC YB XB ') [XB.',YB.'] disp('Distance from center (db)') db.' disp('angB 1(center)') angB 1.' dispC YD X D ') [XD.',YD.'] disp('Distance from center (dd)') dd.' dispCangD 1(center)') angD l.' dispC YE XE ') [XE.',YE.'] dispCDistance from center (de)') de.' disp('angEl (center)') angE l.' dispC YF XF ') [XF.',YF.'] dispCDistance from center (df)') df.' disp('angF 1(center)') an g F l.' dispC YG [XG.',YG.'] XG ')

95

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

dispCDistance from center (dg)') dg.' dispCangG l (center)') an g G l.' %-------------------% 1st Quard circle for w a 2 = l;m a (da(w a 2 )-mean(da)) end % plot the boundary points. p]ot(XA,YA,'g*') hold on plot(XB,YB,'x') hold on plot(XD ,Y D ,'m +') hold on plot(XE,YE,'ro') hold on plot(XF,YF,'*') hold on plot(XG,YG,'kx') hold on plot(m ean(X A ),m ean(Y A),'*') plot(m ean(X B),m ean(Y B),'*') plot(m ean(X D ),m ean(YD ),'*') plot(m ean(X E),niean(Y E),'*') plot(m ean(X F),m ean(Y F),'*') p]ot(m ean(X G),m ean(Y G),'kx')

96

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

%-----------------------------------------------------------------------------------------------------------------------

% M ATLAB codes for Feed-Forw ard Neural Netw ork (Back-Propagated) Algoritlim % -----------------------------------------------------------------------------------------------------------------------% % % % % % % % w : Input-to-hidden layer weights V ; H idden-to-output layer weights k : # o f iterations R : Reflected vector A ; Learning rate b/w hidden and output connectors B : Learning rate b/w input and hidden connectors hidden : H idden neuron values output : O utput neuron values

clear all close all tic % Start a stopwatch tim er

load input_data.txt load desired_output.txt
%-

Initialize the weights

for i= l;4 for j= l :7 w (ij)= 0.005; end end for i= l:7 fo rj= l:7 v(ij)= 0.005; end end a = circle_ d atal(:,l); b = circle_datal (:, 2 ); alpha = circle_datal (:,3); nc = circle_datal(:,4); p ] =desired_output( 1 ) p 2 =desircd_output(:, 2 ) p3=desired_output(; ,3);

97

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

p4=desired_output( ;,4) p5=desired_output(:,5) p 6 =desired_output(:, 6 ) p7==desired_output(:,7) [n l f]=size(a); % A ssigning the value to the shapes for 11=] :nl D esired = [p l(n ) p2(n) p3(n) p4(ti) p5(n) p 6 (n) p7(n)]; input(])=a(n); input( 2 )=b(n); input(3)=alpha(n); input(4)=ne(n);

% --------------------- N etw ork Trainig for k= 1:400 % caleulate the hidden neuron values for i=I :7 hidden(i)= 0 ; for j= l :4 hidden(i)=hidden(i)+(w (j,i)*input(j)); end end % calculate output neuron value for i= l:7 output(i)= 0 ; for j= l :7 output(i)=output(i)+(vG, i) *hidden(j )) ; end output(i)=(l-exp(-(output(i))))/(l+exp(-(output(i)))); end % calculate total error total_error= 0 ; for i=l :7

98

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

total_eiTor = total_eiTor + abs(D esired(i)-output(i)); end % -------------------- B ack propagation o f the en o r  if total_eiTor < 0.05; fprintf('lf\n', output) else % calculate reflected vector for i= l ;7 · R(i)=(D esired(i}-output(i))*output(i)*(l-output(i)); end %------------------------Update the w eig h ts---------------------% update the weights b/w the hidden & the ouput layer A=0.2; for i= l :7 for j= l :7 v ( ij) = v (ij) + (A*R(j}*hidden(i)); end end % calculate hidden layer error for i= l:7 E_hidden(i)=0; for j= l :7 E_hidden(i) = E_hidden(i) + (R(j)*v(i,j)); end E_hidden(i)=hidden(i)*(l-hidden(i))*E_hidden(i); end % update the w eights b/w the input & hidden layer B=0.2; f o r i= l;4 f o r j= l:7 w (ij) = w (ij) +(B*E_hidden(j)*input(i)); end end end end

99

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

dispC'The w eights b /w input & the hidden') w dispCThe w eights b /w the hidden & th e output')
V

dispCThe output') output end toe

% Stop the stopw atch tim er and display the elapsed tim e

100

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

References

[1].

V.H. C han, C. Brad el ey, G. W . Vickers, "A m ulti-sensor approach to autom ating co-ordinate m easuring m achine-based reverse engineering" com puter in industry' vol. 44, pp.105-115, 2001.

[2].

V incent H. Chan, "Feature based reverse engineering em ploying autom ated m ulti sensor scanning" Ph.D dissertation, D epartm ent o f m echanical engineering. U niversity o f V ictoria, 1999.

[3]. [4],

http:.';\v\v\v.dat.x.c o .u k dated 6/13/2003. J. Lam pinen, J. Laaksonen, E. Oja, "Neural network system , techniques and applications in pattern reco g n itio n ", Research reports, Laboratory of

com putational engineering, Helsinki U niversity o f Technology, Finland, 1997. [5] [6].
w w .c h iH in a e fF e c ts .o rg 're v e rs e /

M iestentie 3,

dated 09/26/2003.

B handarker M. P., Nagi, R, " STEP-based feature extraction from STEP geom etry for A gile m anufacturing" , computers in industry, vol. 41, pp. 3-24, 2000,

[7].

Joshi S, C hang T. C., "Graph-based heuristics for recognition o f m achined features from a 3D solid m odel" Com puter-Aided Design, vol. 20, no 2, pp. 5866, 1988.

[8].

J.H. V andenbrande, A.A.G. Requicha, " Spatial reasoning for the autom atic recognition o f m achinable features in solid m odels" IEEE transactions on pattern analysis and m achine intelligence, vol. 15, no. 12, ppl269-1285, 1993.

101

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[9],

M. Schulte, C. W eber, S Rainer, "Functional features for design in m echanical engineering" . C om puter in industry, vol. 23, no. 1, pp. 15-24, N o v '93.

[10].

D. Silver, " 3D Feature Extraction for unstructured grids" http://'WAv\v.caip.rutgers.edu/~silver/nasa/nasa2.htm l

[11].

J.J. Shah, "A ssessm ent o f features technology" . Com puter aided design, vol. 23, no. 5, pp. 331-343, Ju n e'91.

[12].

V.H. Chan, M. A rshad, "Recognition o f Features in cloud data for Reverse Engineering" , proceedings o f CSM E conference at Kingston, Canada, M ay 21-24.

2002 .
[13]. [14]. J. Row e, " Surface m odeling" . C om puter graphics, vol. 20, no. 9, pp 47-52, 1997. G. W}will, D. M cRobie. M. Gigante, "M odelling with features" , IEEE Com puter graphics and applications, vol. 15, no. 5, pp. 40-46, 1997. [15]. J. C. Cavendish, "Integrated feature based surface design w ith free fonn defonnation" , C om puter aided design, vol. 27, no. 9, pp. 703-711, 1995. [16]. J. S. M. V ergeest, I. Horvath, J. Jelier, Z. Rusak, "Free-fonn surface copy and paste techniques for shape synthesis" , Proc. 3'^'^ Int. sym posium o f com petitive engineering. D elft U niversity press. Delft, pp. 395-406, 2000. [17]. W u, M. C., Liu, C. R., "A nalysis on m achined feature recognition techniques based on B -rep" Com puter-Aided Design, vol. 28, no.8, pp. 603-616, 1996. [18]. J. Han, " Survey o f feature research" . Technical report IRIS-96-364, Institute for robotics and intelligence systems, USC. USA, 1996. [19]. J. Han, A.A.G. Requicha, "Feature recognition ftom CAD m odels" , IEEE com puter graphics and applications, vol. 18, no. 2, 1998.

102
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[20].

K yprianou L. K., " Shape classification in Com puter-Aided D esign" , PhD dissertation, Christ College, U niversity Cambridge, Cambridge, U.K., July 1980.

[21].

B. Falcidieno, F. Giannini, "Autom atic recognition and representation o f shape based features in a geom etric m odeling system " , Teclmical report no. 10/88, Instituto per la niatem atica applicata. 1988.

[22].

P. Di Stefano. "A feature based representation scheme for design" , Proc, o f the FE.A.TS 2001 International conference, Valenciennes, France, June 2001.

[23].

J. J. Shah, M. M antyla, "Param etric and feature based CA D /C A M ", John W iley & Sons, 1995.

[24].

T. W oo. " Feature extraction by volum e decom position" , Proc. Conf. CA D/CAM technology in mechanical engineering, Cambridge, MA, USA, 1982.

[25].

Y. S. Kim, " Recognition o f form features using convex decom position", Com puter-Aided Design, vol. 24, no. 9, pp. 461-476, 1992.

[26].

E. W ang. Y. S. Kim. "Fonn feature recognition using convex decom position" , results presented at the 1997 ASM E CIE feature panel session, Com puter-Aided Design, vol. 13, no. 9, pp. 983-989, 1998.

[27].

S. B. Kailash. Y. F. Zhang, J. Y. H. Fuh, "A volume decom position approach to m achining feature o f casting and forging com ponents" , Com puter-Aided Design, vol. 33, no. 8, 2001.

[28]. [29].

http://aitech.ac.ip/-kurokaw a/kurokaw a/neural net/net final.htinl S. T. W elstead, ''Neural N etw ork and fuzzy logic Applications in C/C++" , John w iley & sons, Inc, Toronto., 1994.

103

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[30].

M. P. Carven, K. M. Curtis, B. R. hayes-Gill, C. D. Thursfield, "A H ybrid Neural N etw ork/R ule-Based Technique for on-line gesture and hand-w ritten character recognition" , Proceedings o f the fourth IEEE International Conference on Electronics circuits and systems. Cairo, Egypt, vol.2, pp. 850-853, D ec 15-18,

1997.
[31]. S. Prablikar, M. R. Henderson, "A utom atic form -feature recognition using neural netw ork-based techniques .on boundai'y representations o f solid m odels" ,

Com puter-Aided Design, vol. 24, no. 7, pp. 381-393, 1992. [32]. A. H. Zulkifli, S. M eeran, "Decomposition o f interacting features using a Rohonen self-organizing feature map neural network" . Engineering applications o f artificial intelligence, vol. 12, pp. 59-78, 1999. . [33]. I. Yoshihara, Y. K am im ai, M. Yasunaga, "Feature Extraction from genom e sequence using M ulti-M odal Netw ork" . Faculty o f engineering, M iyazaki University, Japan, G enom e Informatics 12; pp. 420-422, 2001. [34]. W illiam B. Thom pson, J. C. Owen, H. J de St. Germain, S. R. Stark, T. C. Henderson., " Feature-based reverse engineering o f mechanical parts" , IEEE Transaction on Robotics and Automation, vol. 15, no. 1, 1999. [35]. Farhad Nabhani, Terry Shaw, "Perform ance analysis and optim ization o f shape recognition and classification using A NN ", Robotics and Computer Integrated M anufacturing 18 (2002), pp. 177-185. [36]. V. H. Chan, "AI for M echanical Engineers ", Class Lecture notes, June2002, Ryerson University.

104
Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

[37].

K. N ezis, G. Vosniakos, "Recognition 2

D shape features using a neural

network and heuristics" , Computer-Aided Design, vol. 29, no. 7, pp. 523-539. 1997 [38]. A. D. K ulkam i, "Com puter vision and fuzzy-neural system s", Prentice Hall PTR, Upper Saddle River, N J, 2001.

105

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.


