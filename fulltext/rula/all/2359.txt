3D RECONSTRUCTION OF EXPOSED UNDERGROUND UTILITIES USING PHOTOGRAMMETRIC METHODS

by

Wensong Hu B.Sc., Wuhan University, China, 1992

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Applied Science in the Program of Civil Engineering

Toronto, Ontario, Canada, 2005 © Wensong Hu, 2005

UMI N um ber: E C 5 3 4 3 3

INFORMATION TO USERS

The quality of this reproduction Is dependent upon the quality of the copy submitted. Broken or Indistinct print, colored or poor quality Illustrations and photographs, print bleed-through, substandard margins, and Improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, th ese will be noted. Also, If unauthorized copyright material had to be removed, a note will Indicate the deletion.

UMI
UMI Microform E C 5 3 4 3 3 Copyright2009 by ProQ uest LLC All rights reserved. This microform edition Is protected against unauthorized copying under Title 17, United States Code.

ProQ uest LLC 789 East Elsenhower Parkway P.O. Box 1346 Ann Arbor, Ml 48106-1346

DECLARATION
I hereby declare that I am the sole author of this thesis.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. 0

Wensong Hu

^

Department of Civil Engineering Ryerson University

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Wensong Hu

[j

Department of Civil Engineering Ryerson University

BORROWER'S PAGE
Ryerson University requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date.

Name of Borrowers

Date

Address

Signature

3D RECONSTRUCTION OF EXPOSED UNDERGROUND UTILITIES USING PHOTOGRAMMETRIC METHODS
Wensong Hu Master of Applied Science, 2005 Department of Civil Engineering Ryerson University

ABSTRACT
This thesis addresses the topic of three-dimensional (3D) reconstruction of exposed underground utilities using photogrammetric methods. Research on this topic is mainly motivated by the need for improved information on the location of underground utilities and, thus, to provide reliable information for the management of buried assets.

In this thesis, a system of photogrammetric software programs is developed for 3D reconstruction of underground utilities. Camera calibration programs are used for computing interior elements and lens distortion coefficients of digital cameras and saving them in a lookup table (LUT). The accuracy of calibrated image coordinates satisfies the photogrammetric processing demand. An automatic image point detection method is proposed and achieved in these programs. External orientation programs are used for calculating exterior elements of the digital images. Based on geographic information system (CIS) and global positioning system (GPS) techniques, a new ground control points (GCPs) collection method is proposed and implemented in these programs. A 3D reconstruction program provides corresponding functions to obtain and edit 3D information of underground utilities. Epipolar lines are employed as an assisting tool that helps operators easily find homologous points from different digital images. The study results indicate that photogrammetric methods for reconstructing 3D information of underground utilities are effective and low cost.

ACKNOWLEDGEMENTS
First of all, I would like to express my great appreciation and thanks to my supervisor, Professor Dr. Michael A. Chapman, for his valuable advice and ideas as well as constructive suggestions and productive comments on my thesis. I also thank him for providing me the great opportunity to continue my study in the field of photogrammetry and remote sensing in Canada and to work with him as a research assistant during my years at Ryerson University. I would also like to thank Professor Dr. Jonathan Li, my co-supervisor, for his encouragement, patience and critical guidance. His innovative suggestions and valuable ideas had a great influence on my thesis study. I greatly appreciate their efforts to provide me financial support throughout my course and thesis studies. Without their support, I would never have completed this thesis work.

I would like to acknowledge my research partner, Mark Tulloch. With his help including background investigation, project organization, field work and valid suggestions, this research works effectively and successfully.

This study was sponsored by the City of Toronto under the "Underground Infrastructure Mapping" project. I would like to thank Kevin Tierney in the City of Toronto for his valuable discussions, suggestions and also the purchase of the mapping-related equipment was very much appreciated.

I also attribute my accomplishment to Professors Dr. Songnian Li, Dr. Bhagwant Persaud, Dr. Mohamed Lachemi and other faculty and staff members in the Department of Civil Engineering, for their help and support. Many thanks go to Leah Stanwyk, Kim Kritzer and Dianne Mendonca for their administrative support. Many thanks are also extended to Desmond Rogan and Domenic Valle for their technical assistance for solving computer and software problems.

The School of Graduate Studies of Ryerson University is acknowledged for providing me the Ryerson Graduate Scholarships and the opportunity to conduct my research at Ryerson University from 2004 to 2005.

Last, but not least, I wish to express my gratitude, from my deepest heart, to my parents for their unconditional love and support, and to my wife, Xu Sun, for her endless love, patience, and encouragement. Without her help and understanding, I would never complete this thesis.

TABLE OF CONTENTS
3D RECONSTRUCTION OF EXPOSED UNDERGROUND UTILITIES USING PHOTOGRAMMETRIC METHODS....................................................................... i DECLARATION......................................................................................................... ii BORROWER'S PAGE.............................................................................................. iü ABSTRACT................................................................................................................iv ACKNOWLEDGEMENTS........................................................................................ v TABLE OF CONTENTS..................................... vii

LIST OF TABLES........................................................................................................x LIST OF FIGURES.................................................................................................... xi LIST OF ABBREVIATIONS...................................... ,........................................... xiii 1 INTRODUCTION............................................................................................... 1 1.1 1.2 1.3 1.4 2 Motivations of the Study...................................................................................1 Problem Statement............................................................................................3 Objectives of the Study.....................................................................................5 Thesis Organization..........................................................................................6

BACKGROUND AND PROBLEMS.................................................................9 2.1 2.2 2.3 2.4 2.5 2.6 Ground Penetrating Radar System Approach.................................................. 9 Global Position System Method..................................................................... 12 Integrated GPS/GPRS Approach.................................................................... 15 Conventional Surveying Methods.................................................................. 17 Subsurface Utilities Engineering Approach....................................................18 Chapter Summary...........................................................................................21

3

METHODOLOGY............................................................................................23

3.1

Camera Calibration.........................................................................................23 Introduction............................................................................................. 23 Establishing a Control Pattern................................................................. 26 Automatic Detection of Orientation Points............................................. 28 Calculation of Interior Elements Using Direct Linear Transformation ..35 Calculation of Lens Distortion Coefficients............................................ 38

3.1.1 3.1.2 3.1.3 3.1.4 3.1.5 3.2

External Orientation and GCPs Collection.................................................... 39 Introduction............................................................................................. 39 External Orientation.................................................................................40 GCPs Collection...................................................................................... 42

3.2.1 3.2.2 3.2.3 3.3

3D Reconstruction.......................................................................................... 53 Introduction............................................................................................. 53 Space Intersection.................................................................................... 55 Bundle Adjustment..................................................................................58 Epipolar Geometry...................................................................................61

3.3.1 3.3.2 3.3.3 3.3.4 3.4 4

Chapter Summary........................................................................................... 65

CASE STUDY OF CAMERA CALIBRATION...............................................67 4.1 Automatic Detection Program........................................................................ 67 The Detection of Reference Points..........................................................68 Obtaining Accurate Positions Using LSM.............................................. 70 The Combination of Reference Points and GCPs...................................71

4.1.1 4.1.2 4.1.3 4.2

Camera Calibration and Generation of a Look Up Table...............................77 Data Processing....................................................................................... 78 Result Analysis........................................................................................ 80

4.2.1 4.2.2 4.3 5

Chapter Summary........................................................................................... 84

CASE STUDY OF EXTERNAL ORIENTATION AND GCPS

COLLECTION.......................................................................................................... 85 5.1 5.2 GIS Data Transformation Program................................................................. 85 GCPs Collection Program...............................................................................87
viii

5.3

External Orientation Program.........................................................................91 95

5.4 Chapter Summary............................................................................. 6

CASE STUDY OF 3D RECONSTRUCTION..................................................97 6.1 6.2 6.3 Model Management Module...........................................................................98 Object Measurement Module........................................................................ 100 Graphics Editing Module...............................................................................102

6.4 Chapter Summary.......................................................................................... 104 7 CONCLUSIONS AND RECOMMENDATIONS..........................................107 7.1 7.2 7.3 7.4 Summary of the Study...................................................................................107 Limitations of the Study................................................................................ 109 Conclusions................................................................................................... 110 Recommendations for Future Research.........................................................112

BIBLIOGRAPHY....................................................................................................113

LIST OF TABLES
Table 3-1 List of control points used in camera calibration........................................ 28 Table 3-2 Group code value types of DXF format used in this study..........................44 Table 4-1 An example of data set computed by the automatic detection program 77

Table 4-2 The first part of the LUT..............................................................................82 Table 4-3 The second part of the LUT.........................................................................83

LIST OF FIGURES
Figure 2-1 Major activities in subsurface utility engineering (Jeong, et al., 2004).....20 Figure 3-1 Control pattern........................................................................................... 27 Figure 3-2 Image Matching......................................................................................... 30 Figure 3-3 Two kinds of fiducial images..................................................................... 31 Figure 3-4 Flow chart of LSM.....................................................................................34 Figure 3-5 Space intersection 1 ................................................................................... 56 Figure 3-6 Space intersection 2 ................................................................................... 57 Figure 3-7 Point correspondence geometry......................................... 62

Figure 3-8 Epipolar geometry...................................................................................... 63 Figure 4-1 The interface of automatic detection program........................................... 68 Figure 4-2 Flow chart of detecting initial positions of reference points..................... 69 Figure 4-3 Flow chart of obtaining accurate positions using LSM..............................70 Figure 4-4 An example result of automatic detection of reference points.................. 71 Figure 4-5 GCP selection window used in determining the relationshipbetween reference points and GCPs.......................................................................................... 73 Figure 4-6 Automatically combining reference points and corresponding GCPs........75 Figure 4-7 An example of combination result............................................................. 76 Figure 4-8 The interface of camera calibration program............................................ 78 Figure 4-9 The data processing flow chart of camera calibration...............................79 Figure 4-10 The calibration result and error vectors....................................................80 Figure 4-11 The distribution of lens distortion............................................................ 81

Figure 5-1 Flow chart of GIS data transformation.......................................................86 Figure 5-2 The interface of GCPs collection program.................................................87 Figure 5-3 Zoomed graphics with vertices displayed..................................................88 Figure 5-4 Select an intersection as centre point of displayed graphics......................89 Figure 5-5 GCPs management window...................................................................... 90 Figure 5-6 The interface of external orientation.......................................................... 92 Figure 5-7 Flow chart of calculating exterior elements...............................................93 Figure 5-8 Results of external orientation................................................................... 94 Figure 6-1 User interface of 3D reconstruction........................................................... 97 Figure 6-2 Model management window..................................................................... 98 Figure 6-3 The interface of object measurement module...........................................101 Figure 6-4 The legend of three kinds of features....................................................... 103 Figure 6-5 Delete a selected object................................... 104

LIST OF ABBREVIATIONS
2D 3D CAD CADD DLT DOT FHWA GCPs GIS GPR GPRS GPS LSM LUT MTM NMEA RTK SUE VDOT Two-Dimensional Three-Dimensional Computer Aided Design Computer Aided Design and Drafting Direct Linear Transformation Department of Transportation Federal Highway Administration Ground Control Points Geographic Information System Ground Penetrating Radar Ground Penetrating Radar System Global Positioning System Least Squares Matching Lookup Table Modified Transverse Mercator National Marine Electronics Association Real Time Kinematic Subsurface Utilities Engineering Virginia Department of Transportation

1 INTRODUCTION
1.1 Motivations of the Study
The demand for accurate mapping underground utilities is growing rapidly. It involves the systematic location and mapping of buried utilities within a geographic area. The information is recorded into databases for future development and maintenance projects. The management of buried infrastructure becomes important because visual inspection of the underground utilities is not possible. It is difficult to determine or estimate when to rehabilitate or replace utilities. Underground asset management applications assist municipalities in managing their buried infrastructure. Reliable

asset management depends heavily on accurate information. Determining the location of buried utilities is one source of information that constitutes good asset management practice (Tulloch and Hu, 2005).

Furthermore, design and construction projects in areas of concentrated underground utilities require precise utility location information to avoid costly delays, conflicts, redesigns, safety hazards and service disruptions. Using computer-assisted design (CAD) or geographic information system (GIS) technology, underground utility mapping provides a straightforward source of information indicating utility location. Underground utility mapping provides multiple levels of sophisticated, descriptive information about underground utilities. With underground utility mapping, it is

possible to track a utility from its point of origin to its destination, provide the best routing for the utility line when it was installed as well as periodic maintenance records. Three-dimensional imaging shows the utility from different angles as well as its relationship to other underground objects, minimizing conflicts and allowing for design alternatives.

However, incorrect information is often found on existing underground utility maps. Sometimes utility pipes or cables are relocated during repairs or renovations, but maps are not updated. Sometimes maps represent proposed plans that do not show as-built locations. Occasionally, old maps are simply lost or destroyed. People who fulfill their legal obligation to contact a central utility marking clearinghouse before digging or drilling may feel a false sense of security since lost or mis-mapped utilities generally will not be marked as a result of such a request. The result of digging or drilling in the presence of unknown, unmarked, unmapped, or incorrectly located utilities can be wasted excavation time (and money), expensive damage (and cost), utility downtime (and expenses), and, worst of all, personal injury or death.

This thesis is mainly motivated by the need for improved information on the location of underground utilities, thus, to provide reliable information for buried asset management. Through the solution for accurately mapping the exposed underground utilities that will be mentioned in this study, the risk of construction of underground

utilities could be reduced greatly. And at the same time, public dollars on infrastructure upgrades will be saved based on the information about the underground utilities.

1.2 Problem Statement
The reason that the mapping of underground utilities is gaining more and more attention derives from three main aspects. The first is that the construction risk of underground utilities is increasing due to the uncertainty of the location of the utilities. The ideology of `out of sight, out of mind' cannot apply to underground utilities any longer. This problem leads to project delays, extra work orders, change orders,

construction claims, contingency bidding, loss of service, property damage, and worst of all, injury and death (Anspach, 1996, Tulloch, 2005). Two accidents occurred in Ontario in 2003 due to insufficient of accurate underground information. Eight individuals were killed during these two accidents (Tulloch, 2005). Accurate mapping of the underground utilities is, thus, highly needed for the updating and management of such information.

The second problem is that a considerable outlay of public money for the updating or repairing of the underground utilities will be required due to insufficient and unreliable information related to these utilities. The Municipality of Halton reported that it has spent an average of $10 million per year on replacing underground sewer

and water pipes in the past eight years (Bejon, 2004; Tulloch, 2005). Often, these utilities may be in very good condition at that time. By providing accurate and updated information about underground utilities capital and labour costs will be saved.

The third problem is the disadvantages of existing underground utility mapping methods. The features of the underground utilities are often not detectable using a GPRS approach. The low resolution of GPRS may also produce an incorrect recognition of the underground utility. In addition, it takes a long time for GPS methods to obtain space coordinates of exposed underground utilities. On the other hand, conventional surveying approaches are time and resource intensive as well as limited by environmental factors such as weather. Furthermore, it is very dangerous for surveyors to work in a high-traffic area. SUE is a good approach to obtain spatial information of the underground utilities although the process is very expensive.

The benefits of using photogrammetric methods are numerous. Firstly, this approach is low cost. In particular, the cost of commercial digital cameras is becoming lower and lower. Based upon using these kinds of digital cameras, the cost of digital images is veiy low. Secondly, a visual record of exposed underground utilities can be permanently saved in the digital images. Thirdly, a safe working environment is provided since most of the data processing operations are performed in the office.

1.3 Objectives of the Study
The main objective of this study is to develop a system for providing three-dimensional (3D) information of the exposed underground utilities using photogranunetric methods. The 3D information of the underground utilities can be used to update the underground utility geo-database and allow for increased public safety, and for better underground asset management.

In order to achieve the above objective, the following key issues are addressed in this study:

·

Calibration of the commercial digital camera (Nikon COOLPIX 8800), which will be used in this study. The purpose of camera calibration is to compute the interior elements and lens distortions of the digital camera. The establishing of a control pattern, automatic orientation point detection and the calculation of the camera focal length, principal point and lens distortion coefficients will be included in this section.

·

External orientation and ground control points (GCPs) collection. The GIS utility dataset transformation, receiving and locating in the database of the global positioning system (GPS) signal and the selection and recording of the GCPs coordinates will be also included in this section.

·

3D reconstruction of the underground utilities. In this section, the space intersection of target point, the bundle adjustment and the epipolar geometry

used for homologous points detection will be included.

1.4 Thesis Organization
This thesis consists of seven chapters and is organized as follows:

Chapter 1 describes the general goal and problems in underground utilities mapping and sets forth the purpose of this study.

Chapter 2 presents an overview of the methods used in underground utilities mapping. Details are given for the GPS method, photogrammetric method and the subsurface utility engineering (SUE) process.

Chapter 3 proposes a methodology developed and used in this study, including camera calibration, external orientation, GCPs collection and 3D reconstruction of the underground utilities.

Chapter 4 presents a case study in the use of proposed methodology to perform the calibration of a non-metric digital camera.

Chapter 5 describes the results and discussion of external orientation and GCPs collection.

Chapter 6 gives the 3D reconstruction results of the underground utilities using the methods mentioned in Chapter 3.

Chapter 7 presents conclusions related to the findings from the study and suggests future work to extend the study in this area.

2 BACKGROUND AND PROBLEMS
Whether installing new underground utilities or repairing old ones, the process of excavation is an important component of civil infrastructure. However, because of the lack of information associated with the exact location and orientation of buried utilities, many problems arise. Some of the construction dangers are the possibility of fatalities, and some may cause considerable economics loss. Hence, accurate detection and mapping of the underground utilities is of utmost importance and has gained more and more attention to the researchers in this area. This chapter attempts to review the relevant research systematically in order to obtain an overview of research trends and to identify the key achievements of various approaches. It is not meant to be an exhaustive review of the literature, but more and indication of the theoretical development of, and problems associated with the mapping of underground utilities.

2.1 Ground Penetrating Radar System Approach
Ground penetrating radar (GPR) is a popular technology for mapping subsurface, features, including the public underground infrastructure. Applications of this technology include efficient and precise mapping of buried utilities on a large scale; inspection of the subsurface prior to construction; comparison of "as-builts" to construction plans; inspection of bridge decks and roadbeds; environmental

monitoring and assessment; near-surface geological assessment; and "non-invasive" archaeology.

GPR involves locating underground infrastructure more reliably and accurately than standard techniques using metal detectors. By providing accurate coverage in 3D, GPR will move the utility industry towards non-invasive management of underground infrastructure, avoiding the hazards and inconvenience of digging. It will also improve construction planning and engineering by showing what lies below the surface before the shovels enters the ground. This technology has incorporated a complete system for underground imaging: 1) an array of antennas to make underground mapping by radar feasible on a large scale; 2) advanced signal processing--using 3D imaging techniques adapted from seismic imaging in oil exploration--to convert radar echoes into 3D underground images; 3) precise positioning of the images relative to ground features by monitoring sensors with a survey total station (e.g. laser theodolite), 4) advanced image processing to extract and display underground features in 3D and archive the results in computer aided design (CAD) or GIS. GPRs use a high-efficiency GPR array, which can be towed

by a vehicle or pushed in front of a modified commercial lawnmower at speeds up to about 1 km/h (30cm/s). Standard GPRs use a fixed array of 9 transmitters and 8 receivers. It works by emitting a short pulse of energy into the ground. Echoes are returned from the different interfaces between different materials in the ground. As the antennae emit a "cone" shaped pulse of energy an offset target showing a

10

perpendicular face to the radar wave will be "seen" before the antenna passes over it. A resultant characteristic diffraction pattern is, thus, built up in the shape of a hyperbola. A classic target generating such as the diffraction pattern is a pipeline when the antenna is traveling across the line of the pipe. However, it should be pointed out that if the interface between the target and its surrounds does not result in a marked change in velocity then only a weak hyperbola will be seen, if at all.

The advantage of the GPR technology used for underground mapping is that it could overcome the problem of radio-detection methods, which are unable to detect non-metallic buried services including plastic water and gas pipes and clay drainage pipes. At the same time, GPR is a comprehensive and efficient geophysical technology for non-invasive mapping of the underground down to depths of about 6 to 10 feet. By producing a continuous 3D image of the subsurface, GPR can identify the best locations for test pits or vacuum excavation "potholes" to positively identify utilities in place and can fill in gaps between holes. GPR can also be used to quickly develop base maps in areas where records of underground structure are missing or poor. The digital images and maps created by Ground Penetrating Radar System (GPRS) can also be archived for future use in determining possible changes in infrastructure over time by repeated surveys at the same location. But, the GPR devices are very expensive and their penetration capabilities are dependent upon soil conditions. Another major disadvantage of GPRS is the speed. Since the system can only travel at a speed of 1 km/h, the amount of time needed to collect the underground

11

utility information is considerable (Tulloch, 2005).

2.2 Global Position System Method
Global Position System (GPS) is a satellite-based navigation system made up of a network of 24 satellites placed into orbit by the U.S. Department of Defence. GPS was originally intended for military applications, but in the 1980s, the government made the system available for civilian use. GPS works in any weather conditions, anywhere in the world, 24 hours a day. There are no subscription fees or setup charges to use GPS. GPS provides specially coded satellite signals that can be processed in a GPS receiver, enabling the receiver to compute position, velocity and time. Based on their parallel multi-channel design, today's GPS receivers are extremely accurate within 15m on average.

Underground utility mapping and basemap conversion into GIS and CAD formats have been performed predominantly using photogrammetry or map atlas digitizing methods. Advances in GPS technology during the last few years have allowed new mapping methods that result in comparable costs, shortened duration, and veiy high quality data collection.

GPS provides a rapid means of field surveying of utility locations. Real-time kinematic (RTK) positioning GPS techniques enable centimeter-level positioning
12

instantly at the time of observation. This process of field surveying can replace conventional surveying utility positioning and cost-effectively locate every utility. Moreover, maintaining consistent positioning of additional or relocated facility components can easily be performed to a consistent accuracy specification.

Firstly, utility records from all public and private utility owners should be obstained. With those records in hand, the field crew performs a "top-side" survey--visiting the site to assess whether GPS is the best technique. For example, a site must be relatively open and free of any tree canopies to receive an unobstructed GPS signal. If the surveyors estimate that 60 percent of the features in the area can be located via GPS, it is deemed to be the recommended method.

Secondly, a GPS base station is established, then mobile units carried in backpacks by surveyors are carried to each valve, maintenance hole and other utility features. The surveyor sets up a survey pole with a GPS antenna over each utility feature. Centimeter level positioning can be achieved with one minute of GPS observation data (Gooch, 1997). During this time, the surveyor associates feature codes identifying the feature. Daily production rates vary depending on the density of features. As each point is collected, it shows up on the pre-uploaded digital planimetric base maps, that provides an immediate positional accuracy check for every point. The more planimetric detail carried in the field, such as building
13

footprints or sidewalks, the better the opportunity for isolating erroneous GPS positions. For instance, if the surveyor is standing at a fire hydrant 10 feet behind the face of curb and the GPS-observed feature shows up in the middle of the road, the surveyor will identify an obvious need to recollect the point. The newest technological improvements allow surveyors to work smoothly between parts of the site that have a clear GPS signal and those that are obscured from receiving reflected GPS signals by trees or tall buildings. The data collector can be disconnected from the RTK unit, plugged into a total station, and used with conventional

techniques--without interrupting the flow of data gathering. Perfected in the last couple of years, this technique eliminates the need to return to the office, download GPS-derived information, upload a data collector and then return to the field. Continuing all field collection with the same data collector provides much smoother transitions between GPS and conventional data areas.

The biggest benefit from using GPS is the durability of the data. In six months, all the paint markings that were applied will be gone. But the GPS derived data and attribution will continuously be available to support new projects numerous times. But, GPS methods can not be used where structures obscure satellite visibility, and conventional surveying must be used at this time. This is a limiting factor in highly developed downtown areas that have tall buildings. The RTK method requires continuous radio transmission between the GPS base station and field unit. At times, interference or loss of signal can cause complications and delays. Mapping of the
14

facility network is similar to the photogrammetric approach once the surface features are surveyed. Digitizing is used to connect pipes between the surveyed features guided by plans and atlas sheets. The reference land base map must be highly accurate so that the surveyed locations are positioned relative to road centerlines and other base map features.

2.3 Integrated GPS/GPRS Approach
New GPR technology holds promise as a simple, non-invasive approach for sub-surface characterization of proposed underground utility routes. The U.S. Air Force EarthRadar technology-- originally developed for detecting and identifying buried unexploded ordnance-- is indicating that it can provide the most economical and environmentally friendly tool for subsurface underground utility route investigations. The configuration of the system hardware and signal processing makes it ideal for applications such as mapping subsurface geological features, locating cavities and collapsed features and identifying contaminated ground. It can detect the soil/bedrock interface along the route of proposed underground utilities.

In this method, a high-resolution (± 1cm) integrated "differential" GPS is used to trigger the GPRS. The global/local coordinates provided by GPS are used in two ways. (1) For location determination and (2) coordinate calculation, which allows surface information to be transformed to the target depth and to aid in 3D image
15

reconstruction for discrimination purposes.

A commercially available network analyzer is used in this technology to convey signals via a pair of transmission lines to resonate a suitable antenna. The operating requirements and design specifications of the antenna can change depending on the type, depth, and size of a buried target and the ground conditions. In the process of operating the system, a permanent reference point for the local coordinates GPS is first established. Site characterization tests are performed to calculate the electromagnetic wave speed in the soil. The radar system with its integrated GPS is then set up and all electronics and data files are initialized. Then the system can profile in different colour formats to determine the location and depth of potential buried targets. The 3D images of the potential targets are then constructed.

After the 3D images of the potential targets are obtained, the system could realize the image plots process by changing the data to a frequency domain format. The plots could be either colour plots or greyscale plots. By adding more 2D view of the buried targets, the system may even generate a global coordinate frame to yield a 3D reconstruction of signal. This resulting reconstruction will more clearly indicate a target object's true dimensions and position of burial.

This technique has provided a simple and relatively low cost solution to the underground utilities detection. It combines the advantages of the GPS and GPRS

16

methods into one complete system, and, thus, could provide more accurate underground information. But the disadvantage of this system is also obvious. As a

system of the U.S. Air Force, the use of this system is very limited, not all the users would be permitted to use this technology. At the same time, it is not originally designed for underground mapping, so it is convenient to use in some conditions, such as urban areas and it also could not be used when the underground utilities are constructed.

2.4 Conventional Surveying Methods
Currently, conventional ground surveying and mapping are still the methods used by some users of underground utility mapping to acquire their data. The advantage of the conventional method is that it can provide required accuracies for collecting underground utility information (Tulloch, 2005). But the disadvantages are significant. Firstly, this method is time and resource intensive since it requires significant data collection and reduction to provide the level of detail necessary for facility location. Secondly, this method is also limited by environmental factors such as weather. With conventional surveying, data collection occurs almost entirely in the field and may require that data collection personnel locate on or near heavily traveled roadways. Additionally, because of extensive in-field data collection, its use is impractical for sizeable projects. And thirdly, once the field data is obtained, a significant amount of processing is necessary before any useful underground utility information is available.

17

The result is the passage of a significant amount of time between project inception and final mapping of the underground utilities.

2.5 Subsurface Utilities Engineering Approach
Subsurface Utilities Engineering (SUE) is an engineering process that uses new and existing technologies to accurately identify, characterize, and map underground utilities. It is an emerging engineering process that has been proved to be an effective tool to reduce underground utility accidents, damage, utility related claims, and construction delays. This process aims to accurately locate and depict utilities and disseminate the information prior to commencing construction so that conflicts and disasters can be minimized. The practice of SUE has been developed and refined over many years and was systematically put into professional practice in the 1980s (Lew and Anspach, 2000). A state utility engineer in the Virginia Department of Transportation (VDOT) sensed the potential of SUE and allocated $10,000 for a trial project in late 1983. This was the first official SUE contract by a state department of transportation (DOT). VDOT reported to the Federal Highway Administration (FHWA) that over $1 million in savings to the taxpayer were realized from this project (FHWA, 2002). State DOTs and FHWA since then have taken a leading role in the promotion of SUE, and the term Subsurface Utility Engineering was coined at the 1989 FHWA National Highway Utility Conference. Today, SUE is officially utilized in many agencies, such as the Department of Defence, the Department of Energy, the General Service Administration, as well as many municipalities and engineering firms.

18

The SUE process can be categorized into the five distinctive activities as shown in Figure 2-1. It is a combination of geophysics, surveying, civil engineering, and data management. Fieldwork involves three different activities, i.e., subsurface utility designating, subsurface utility locating and surveying. Subsurface utility designating determines the existence and approximate horizontal position of underground utilities using surface geophysical techniques, which include pipe and cable locators, magnetic methods, metal detectors, GPR, acoustic emission methods, etc. In the subsurface utility locating activity, minimally intrusive methods of excavation are used such as vacuum excavation, allowing the determination of the precise horizontal and vertical position of the underground utility line to be documented. Surveying instruments such as levels, staffs and theodolites are typically used for the surveying activities. The GPS is now widely accepted for surveying purposes. Its improved accuracy, e.g., when using real time kinematic technology, and the ease of data transfer to computer aided design and drafting (CADD) and GIS environments have accelerated its use. The data management activity ranges fi'om updating information on existing utility drawings or construction plans to the production of completely new utility maps. In the final engineering service activity, the SUE engineer provides consultation, conflict determinations, and utility coordination and design.

19

In the Held In the Office
(1) Subsurface

Vtility
Designating (3) Surveying (Traditional Surveying or, GPS) (4)Da/û Management

(S) Engineering Service

Figure 2-1 Major activities in subsurface utility engineering (Jeong, et al., 2004)

The obvious advantage of SUE is that it can be systematically incorporated during different construction stages. The use of SUE in the preliminary design stage involves all existing utilities designated at the proposed areas of work. It is an approximate horizontal location performed using the surface geophysical methods. The acquired data is transferred on to preliminary plans for the project through a CADD system or GIS. The location of proposed work can be optimized with respect to the horizontal location of the existing utilities. At the final design stage, locations, where conflicts with existing utilities may occur can be identified. And the data collected from the SUE could be used to adjust the final location of the proposed work. The systematic approach allows SUE engineers to narrow down the geographic region where higher quality information is required for the construction project and could then minimize the budget (Jeong, Abraham and Lew, 2004)

There are numerous reports that outline the benefit/cost relationship between SUE and construction projects, and the general benefit/cost ratio is 4:1, meaning for every
20

dollar spent on SUE, a savings of $4 will occur for the overall construction cost (Lew, 1996; Tulloch, 2005). Despite the apparent cost effectiveness of SUE, the process is very expensive. It costs approximately $1 per linear foot to accurately map underground utilities, and to perform non-destructive air-vacuum excavation while a borehole costs $1000. These costs can quickly accumulate (Tulloch, 2005). At the same time, the lack of professional SUE providers is also a main drawback of SUE method.

2.6 Chapter Summary
Detection and identification of buried and submerged structures and utility lines are of interest in many disciplines such as military, law enforcement agencies, construction industry, power suppliers and telecommunication industries. Because of the variable characteristics, degree of saturation, salt and clay mineralogy content of the ground, commercially available detection technology is inadequate for such applications. This chapter presented five popular methods used now in the field of underground utility mapping. The advantages and disadvantages of each method were also described from the view of operation and economic cost. These five methods have provided significant contribution to the application of mapping underground utilities despite some of their disadvantages. The common disadvantages of all these five methods are speed, cost and the fact that they could not be used in the first time of the construction of the underground utilities.

21

Based upon the summary of the background and associated problems, this study focuses on the use of photogrammetric method to provide high accuracy and fast 3D reconstruction of the exposed underground utilities. Chapters 3 through details on the methodology and its implementation.
6

will give

22

3 METHODOLOGY
This chapter describes the methodologies used in this study. Section 3.1 gives conceptual description and methodology of camera calibration. Section 3.2 presents the methodology of external orientation and how to collect GCPs using GPS a receiver and an existing GIS data set. Two 3D reconstruction methods called space intersection and bundle adjustment are introduced in Section 3.3. The concept of epipolar geometry is also provided in this section. Section 3.4 gives a summary of this chapter.

3.1 Camera Calibration 3.1.1 3.1.1.1 Introduction What Is Camera Calibration?

In this study, a commercial digital camera is used for obtaining space coordinates of target points. The objective of camera calibration is the determination of the transformation that maps 3D points in a certain scene or object into their corresponding 2D projections onto the image plane of the camera. This transformation depends on three sets of parameters: ( ) internal parameters called interior elements,
1

(2) external parameters called exterior elements, and (3) lens distortion coefficients. There are three interior elements; focal length ( / ) and the 2D coordinates (xQ,yo) of

23

the principal point. The internal geometry of the camera can be described by these elements. The position and orientation of the camera can be determined by
6

independent exterior elements with respect to a given reference system. These include three translation parameters (Xs, Ys, Zs) to determine the position of projection centre and three angular parameters (co,^,k) to determine the orientation elements of its space axis system in the reference coordinate system. Due to theoretical and technical reasons, it is impossible to manufacture a lens without any distortion. The lens distortion consists of two types of systematic distortions: symmetrical distortion and asymmetrical distortion. Both of them induce a displacement of the image point. Such distortions can be described by distortion coefficients: symmetrical distortion and (kQ,kj,k ,k^)
2

for

for asymmetrical distortion. After calibrating

the camera, all of above parameters can be calculated and can be used in 3D reconstruction. The transformation can be described by Equations (3-1), (3-2) and (3-3).
X -- Xq --

Ax ---- f

- A f) + r,2(f^ - Ys) + r^2{Z ^ - Z s)

'31 ^

+ (7a ~ + (^A ~ + (fx - + ~Zy) r,,{X,-Xs)^r,2{Y,-Ys) + r2,{Z,-Zs) ~
'3 2 '3 3 '2 2 ' 2 3

(3-1)

Ax = x(l-A:o -k^r^
Ay =

-k^r^)^p^{r^ + x^) + /> ^
2 2 2

(3-2)

-k {r^ - k j r ^ -AT)/'^) + 2/»,xy +

+2x^)

'ii

'*12 '2 2 '*32

'*13 ''23 ' 33 . =

CO SK -s in x " 0

s in x r COSRT 0

o'
0

cos^ 0 sin ÿ )

0 1

-s in ^ 0 COS(f>

1 0

0 CO SO -s in o

0 s in o CO SO

R=

'21

(3-3)

3i

0

0

24

where x,y : X q, jV o: / : X Y^,Zy. image coordinates of control points the principal point position of a digital camera the focal length of a digital camera space coordinates of control points the photograph position parameters of a digital camera the photograph orientation parameters of a digital camera image point excursion generated by lens distortion · symmetrical distortion coefficients Pi,P 2

X^,Y^,Z^: (o,^, k : Ax, Ay :

asymmetrical distortion coefficients r=

r:

3.1.1.2

Why Is Camera Calibration Needed?

In this study, a Nikon COOLPIX 8800 digital camera is employed It has a 35-350mm I Ox Zoom-Nikon ED lens (35mm equivalent) used in capturing different types of digital images. Although the reference focal length can be browsed from INFO.txt file stored in the camera memory, its accuracy is not appropriate for photogrammetric calculations. The location of the principal point and the distortion coefficients according to each focal length are still unknown and needed to be calculated through camera calibration.

25

3.1.1.3

Strategy of Camera Calibration

Camera calibration consists of the following procedures: (1) Establishing a control pattern; (2) Automatic image points detection; (3) Direct linear transformation (DLT); (4) The calculation of lens distortion coefficients and coordinates re-correction; (5) Repeat (3) and (4) until the average error of homologous points is less than the threshold value (e.g.
6 0 .1

pixel);

( ) Save internal parameters and distortion coefficients into a LUT. The details about camera calibration are given in the following sections.

3.1.2

Establishing a Control Pattern

As mentioned in above section, the objective of camera calibration is the determination of the transformation that maps 3D points of a certain scene or object into their corresponding 2D projections onto the image plane of the camera. Hence, the 3D position (X^,Y^,Z^) of points must be established as control points before camera calibration. The distribution of control points should be carefully designed. Firstly, all these points should not be located in the same plane as just one image is used in camera calibration. The following paragraphs describe this trivial case.

Suppose < y=

= /r = , if all the Z values of control points equal 0, Equation (3-1)
0

26

can be transformed to Equation (3-4). x -X q -A x = / ^ ^ î --^ Zs y - y o ~ ^ y = f --z -- Zs

(3-4)

From Equation (3-4), it can be seen that there is a strong correlation between / and Zs. It means whatever f is, we can adjust the Zs value to get the same result. If all

control points are located in the same plane, the same problem will occur again.

Control points also should be distributed in a wide rectangle range that can be projected to most of the image areas. This type of distribution ensures calculations that are much more stable.

Figure 3-1 Control pattern
27

For above reasons, a control pattern shown in Figure 3-1 is used. Cross points of grids are chosen as control points because they are easy to find. Table 3-1 lists part of the
point numbers and corresponding 3D coordinates o f such points, A ll o f these coordinates were measured by using a total-station.

Table 3-1 List o f control points used in camera calibration Point Number 102 103 104 105 106 107 110 X(m) -0.2366 -0.2048 -0.1749 -0.1442 -0.1134 -0.0827 -0.2357 Y(m) -0.0298 -0.0476 -0.0615 -0.0771 -0.0926 -0.1066 -0.0299 Z(m) -0.1693 -0.1685 -0.1687 -0.1686 -0.1685 -0.1686 -0.1352

378 379 380 381 382 383 384 385 386 387

0.2639 0.2954 0.0807 0.1114 0.1415 0.1722 0.2028 0.2334 0.2647 0.2964

0.0078 -0.0079 0.1056 0.0876 0.0719 0.0567 0.0401 0.0244 0.0094 -0.0076

0.1361 0.1364 0.1699 0.1698 0.1698 0.1699 0.1699 0.1701 0.1706 0.1711

3.1.3

Automatic Detection of Orientation Points

From Equation (3-1), the image coordinates of control points also need to he measured. There are two ways to measure them. The first method is to manually measure them by using a digital image processing software program such as
28

"Microsoft Paintbrush". This method needs a long time for an operator to complete it. The seeond method is the automatic detection of orientation points. This method first employs standard patterns to detect the initial positions of target points. Then least squares matching (LSM) is used in measuring the coordinates to sub-pixel accuracy. Finally, target image points are connected to corresponding control points as orientation points.

As the second method is based on automatic searching algorithm, it is easy for users to complete the measuring operation. Hence, the second method is chosen to gain orientation points. The following sections introduce the methodology of automatic orientation point detection.

3.1.3.1

Image Matching

Image matching is used for detecting target points in photogrammetry and remote sensing. It can be defined as the establishment of the correspondence between a fidueial image and a search image. In order to find target part of search image, a template image window is shifted pixel by pixel across a larger search image (see Figure 3-2).

29

Fiducial image

Search image

.

.............

TT-r
\k Figure 3-2 Image Matching

TTT

Cross correlation is often employed in image matching. The cross correlation coefficient p between fiducial image and the corresponding part of the search image is computed according to Equation (3-5).
R C

S Y j(/(^ ' c) - u)(g(r, c) - v) P=
r=l c=l

R

C

R

C

(3-5)

r = \ c=I

r=l c=l

where / (r, c) : g(r,c) : individual grey values of the fiducial image matrix individual grey values of the corresponding part of the search image matrix u: v: mean grey value of the fiducial image matrix mean grey value of the corresponding part of the search image

30

m a trix

R,C:

number of rows and columns of the fiducial and search image matrices

As shown in Figure 3-3, two kinds of images are employed as fiducial images. Target point locates at the centre of fiducial image.

8 8

S

8 8 8

IS I! 88IIS !

H

i
15 -9

Figure 3-3 Two kinds of fiducial images

Through image matching, initial positions of target points can be established. These positions will be used in LSM as initial values to get more accurate result.

3.1.3.2

Least Squares Matching

LSM is the most accurate image matching technique. Similar to cross correlation, it is based on the similarity of grey values and determines the parameters of an affined transformation (geometric distortion) and illumination and reflectance differences between corresponding patches of two or more images.

31

The objective of LSM is to minimize grey value differences between the fiducial window and the matching window by changing the matching window's position and shape, as determined in the adjustment process. The change process stops when the grey value differences between the deformed matching window and the template reach a minimum value. This method not only considers illumination and reflectance differences between the two images but also the geometric distortion of the regular image tessellation caused by unknown orientation parameters, tilted surface patch, a surface patch with relief, etc. The relationship between fiducial window and matching window can be described by Equation (3-6), /(x ,y ) = \ + Ksi.^0 + « ^ + «zT,
1 ^ 0

+

+ b^y)

(3-6)

where
X, y

:

image coordinates grey value of fiducial image grey value of matching image

/ : g:

H q, A g illumination and reflectance difference parameters « , a ,, Û » » '
0 2 ^ 0 ^ 2

· geometric distortion coefficients

Equation (3-6) is a nonlinear equation that makes its direct solution difficult and uneconomical. It can be linearized as Equation (3-7) V= c^d\ + c^dh^ + c,da^ + c^da, + c^da, + c^db^, + c^db^ + c^db, - Ag (3-7)

32

where the unknowns dh^^dh^daQ,...,db ^rQ the corrections of the corresponding
2

coefficients, their initial values are: K = , /^i = , Ü Q= ,
0 1 0

ûfj = ,
1

~ ^5 Z ? o= , 6, = 0,
^ 2 0 &2

=

1

C\ --IjCj --g 2
g

^ ^ 2

^

âxj dO f, dxj doy _
% 2 ^ ^ 2

c = Ê h ..^ dy^ dbo %|
^ ^ 2

^

2

^

2

Ag

: grey value difference of the corresponding image point

LSM is very sensitive with respect to the quality of the approximations, requiring, for example, the prior information of correspondences within an accuracy of a few pixels. Thus, it always follows the application of cross correlation and, as a final step, improves the match accuracy (Xie, 2004). Figure 3-4 illustrates the flow chart of LSM.



Geometric distortion correction

y2

+b^x + b^y

Re-sample

g a K +a^x + a^y,bo +b^x+b^y)

Illumination and reflectance difference correction Use LSM to calculate distortion parameters

Correlation coefficient< threshold

Yes

Calculate the best position of matching point

No End

Figure 3-4 Flow chart of LSM

34

3.1.4

Calculation of Interior Elements Using Direct Linear Transformation

The transformation that maps 3D points of a certain scene or object into their corresponding 2D projections onto the image plane of the camera can be described by the collinearity equations (see Equation 3-1). Collinearity equations are nonlinear with respect to the variables involved since the rotation matrix R is based on three angular parameters. Direct solution of this equation is quite impractical, particularly when they are used in conjunction with least squares estimation for redundant cases. There are two methods to solve this problem. The first method is to apply Taylor's series expansion to linearize them. But the requirement of good initial values is its defect. The second method uses a DLT solution. It does not need good initial values. Equation (3-1) can be rearranged as Equation (3-8): L,X " hL Y + L-tZ + x - A x = --------- ------- --------LgX + Ljq Y + LyjZ + 1
2

L^X + L(^ + L-jZ + Lg y-- Ay -- LgX ·¥L^gY + Z ||2 + 1

(3-8)

where

r _ % -A , D T _ ^0^32 fin
---------

j _ % -^ 3

35

r _ ( /n i )^ s + ifin - X o f - 3 2 + (fi' n~ ------------------------------------ D

)4

L. = >^on - A D
2

2

j _ y o h i-f r i2

r

_ (A l *

+ ( ^ 2 -y o n 2 )^ +(/23 - ^ ^ D
0

3

) 4

i ,,= |-

(3-9)

Coefficients L\ to L\\ in (3-8) are the DLT parameters that reflect the relationships between the object-space reference frame and the image-plane reference frame. The relationship between correlations between and Ax,Ay is defined in Equation (3-2). Since there are and L., and Ax, Ay are very small, the calculation of A :,

and L, should be separated into two steps. The first step is solving the transformation parameters L. . The second step is calculating distortion parameters A :, and p . . For this reason. Equation (3-8) can be transformed to Equations (3-10) L^X -f Z/jT + L-^Z + Z + 0 ·(Z/j + Z ,g + Z + Z,g) -- x{LgX +
-4 <7
2

-t- L^^Z) --x

(3 -- 10a) (3 -- 106)

0 ·(Lj + L +Z,j +Z. ) + L^X + Lf^Y+ LjZ + Zg -- y{LgX + L^^Y + Z,,]2) = y
4

36

These equations are linear equations that can be directly solved. The calculation of Lj is not our objective. Our purpose is getting internal and external parameters. All of them can be calculated from L, .

3.1.4.1

Calculation of Camera Position and the Principal Point

From Equation (3-9) ~ "-^4 L,X ,+LJ,-^L,Z,=-L , L^X^ + L^Q Yÿ =--
1

( - )
3 1 1

Similarly, from Equation (3-9)
r2 , r2 ,
t2

_

^31 + h i

1

(3-12)
=

·^9 Ao

AI

and
(Z > Z ,) ( Z > Z p ) + ( Z > Z ,) ( Z ) Z ,o ) + ( Z ) Z J ( Z » Z ,, )

~ - ^ o ( ^ 3 I "^^32

^^12^32 * ^ ^ 13^33) ~ ·^0

( Z ) Z ,)( Z ) Z p ) + ( f ) Z J ( Z > Z ,o ) + ( Z > Z ,) ( Z ) Z ,, )

" To (^31 ^3 2 ^ 3 3) ~ AA21^3I
X. = Z ) > ( i ,i, * L ,L ,,

^22^32 %^3 ) ~ To
Lo + An + Ai

(3-13)

To

= Z > ^ (Z ;Z p

ItT T +, Z T T , TT \ g Z ,g + Z p Z , |)

= --

-- AA AAo ~^AAl
2 , r2 ,
t2

Zp + Ao + Ai Where point.
37

and

represent the x and y coordinates, respectively, of the principal

3.1.4.2

Calculation of Focal Length and Rotation Matrix

From Equation (3-9)
XpLg L^ -^oAo ^2 ^oA l ^

fu R=
^21 ^22

'* 1 3 %

f yp^9 ~^S f Lg

f ^oAo f

"

-^ 6

f ^oAl

/
(3-14)

-'10

=D'[(%o4 -4 )' +(^oAo - 4 ) ' +(^o4. -4 )']
(xo
4

~ 4 )^ +4o4o ~ 4 )^ + 4 o 4 i ~ 4 )^

4 +4o +4i

D can be either positive or negative while calculating rotation matrix. Firstly, use the positive value first and compute the determinant of the transformation matrix obtained. If the determinant is positive (i.e., a right-handed system), D must be positive and the current matrix is acceptable for further use. If the determinant is negative (left-handed), D must be negative. Three angular parameters can be computed from the nine elements of the rotation matrix although it is not necessary.

3.1.5

Calculation of Lens Distortion Coefficients

As mentioned in Section . . , the lens distortion consists of two types of distortions:
3 1 1

symmetrical distortion and asymmetrical distortion. The effect of above distortions can be described in Equation (3-2). Since 4 is a function of the focal length, it

38

should be separately computed from the focal length. Hence, internal parameters (including focal length) and external parameters should be computed first. Then compute distortion coefficients and correct the image coordinates for target points. After that, repeat these steps until the correction of focal length is smaller than a given threshold (e.g., 0.1 pixel).

3.2 External Orientation and GCPs Collection 3.2.1 Introduction

Equation (3-1) describes the transformation that maps 3D points of a certain scene or object into their corresponding 2D projections onto the image plane of the camera. This Equation also can be used for gaining 3D coordinates of target point if more than two digital images overlap target area and the interior elements, distortion coefficients and exterior elements are known. The interior elements and distortion coefficients are computed from camera calibration and can be used as known parameters. Hence, exterior elements should be calculated first. The inverse computation for the exterior elements, and ^,co, k is called external orientation. Section 3.2.2

describes the methodology of external orientation.

From Equation (3-1) at least three points with known ground coordinates are required to determine the six exterior elements. Such points are called GCPs. The image coordinates (x, ÿ) of GCPs can be directly measured in the overlap area of the images.
39

Section 3.2.3 presents how to collect GCPs used in this study.

3.2.2

External Orientation

Although the DLT can be used for external orientation, at least 6 GCPs are required. In general, it is difficult to obtain enough GCPs. On the contrary, collinearity equation only needs 3 GCPs to calculate 6 unknown parameters. Hence, collinearity equation is chosen for external orientation in this study.

Equation (3-1) is nonlinear and should be linearized by using a Taylor's series expansion. In linearizing them. Equation (3-1) is rewritten as follows:
F = Xo + A x - f -- = X

Q

(3-15)

G =yo+Ay-f--=y

where Q = r ,,iX -X ^ ) + r ,,iY -Y J + r ,,( Z - Z J f? = r ,, ( X - X J + r ,, ( 7 - y j + r,3 (Z -Z J
5 = Tj, (X +V 2 2(y - y^) + r23( Z - Z J

(3-16)

According to Taylor's series. Equation (3-15) can be expressed in linearized form by taking the partial derivatives with respect to the unknowns: ^0 + Go + dZ^ = X (3-17) dZs = y j

(d ^ J ^
dco \d(/>)

[d K j
ydK)

[d X s j
'a o''

'

[d Y j
( dG_

'

laz,

's

40

In Equation (3-17) Fq and Gq are functions F and G of Equation (3-15) evaluated at the initial approximations for the 6 unknown exterior elements; the terms
dF^ f d G ] f

day

U < ^ J

-- I, etc., are partial derivatives of functions F and G with
d<f>)

respect to the indicated unknowns evaluated at the initial approximations; and da) ,d</>,d K , etc., are unknown corrections to be applied to the initial approximations. The units of dco ,d^ and c/vare radians. Equation (3-17) can be simplified to the following form:
h^^da)^h^^dl|> + b^^dK-h^^dXs -bi^dYs -b^^dZ^ = J + v^

b2\do) +

bj^dK -- b2^dXg -- b2$dY^ -- b2^dZ^ -- K + Vy

In Equation (3-18), J and K are equal to x-7^ and y-G^, respectively. The eoefficients bjj are equal to the partial derivatives. For convenience, these coefficients are given below. Numerical values for these coefficient terms are obtained by using initial approximations for the unknowns.

K

+ r,,AZ) - 6(-r,,A y + r,, AZ]

R(cos <^àX + sin cosin ^AT - cos cosin ^AZ) - Q {- sin ^ cos KàX + sin oe cos (pcos k/YY - cos o) cos (j>cos a'AZ

b^2 =

q

(r,,A% + r,,A7 + r,]AZ)
-

^14 = '4 '('3i-^" 'iiô ) q 6.5 = ^ (^ 3 2 ^ -'l2 Ô ) 6.6 = ^ {r2 iR -rn Q )
J = X-Xn 0~àx + f -- Q

41

& 2, = ^ [S i.-r,,à Y + r,,à Z )-Q {-r^ à Y + r^àZ] 9 s {cos ^IsX + sin (ù sin ^A7 - cos cosin ^AZ) ^22 cos co cos ^ sin atAZ q - Q {- sin ^ sin KtJC + sin a cos ^ sin KàY --
633

= - --(r,,AX + r,2A7 + /* , AZ) q
3

6 ,4

=-4'(^3i` ^-''2iÔ ) q
- r^Q )

b\i =

(3 -1 9 )

q

K = ^ ( ^ -^-^230)
33

X = y -y ^ -A y + f -- AX, AY, and AZ are equal to X -X ^ ,Y -Y ^ and Z - Z^, respectively.

3.2.3

GCPs Collection

Traditionally, the ground coordinates of GCPs can be computed through block triangulation or obtained using a high accuracy GPS receiver. Block aerotriangulation is a good method to determine the range of control extension in aerial photogrammetry. But it is difficult to find enough appropriate points in urban areas. On the other hand, although the ground coordinates of GCPs can be measured by using a high accuracy GPS receiver, GPS signals are often blocked by large structures. Hence, both of above methods are not suitable for obtaining GCPs in urban areas.

A new method to complete GCPs collection is proposed in this study. Firstly, a GIS dataset supported by the City of Toronto that contains the ground coordinates of

42

utilities is transfonned to special format designed for this study and displayed in a navigating program. Secondly, a commercial GPS receiver that supports a lOm-accuracy location is employed to get initial positions. Thirdly, the GIS dataset is navigated to the current position and appropriate vertices are chosen as GCPs. The sections below describe the details of this method.

3.2.3.1

GIS Dataset Transformation

The utilities GIS dataset supported by the City of Toronto is made in AutoCAD2005 with "dwg" format. It is difficult to directly extract the GCPs and use them in our external orientation program. Furthermore, not every set of coordinates is useful for GCPs collection. For these reasons, we need to transform this dataset into our special format that discards unnecessary parts of the utilities.

Since the AutoCAD2005 "dwg" format is so complex, we transform it into AutoCAD DXF format. The DXFTM format is a tagged data representation of all the information contained in an AutoCAD® drawing file. Tagged data means that an integer number that is called a group code precedes each data element in the file. A group code's value indicates what type of data element follows. This value also indicates the meaning of a data element for a given object (or record) type. Virtually all user-specified information in a drawing file can be represented in DXF format. Group codes define the type of the associated value as an integer, a floating-point number, or a string.
' PRO PERTY OF

43

according to the Table 3-2 of group code ranges.

Table 3-2 Group code value types of DXF format used in this study Group code -5 -4 -3 -2 -1 0 1 2 3-4 5 6 7 8 9 10 Description__________________________________________ APP: persistent reactor chain APP: conditional operator (used only with ssget) APP: extended data (XDATA) sentinel (fixed) APP: entity name reference (fixed) APP: entity name. The name changes each time a drawing is opened. It is never saved (fixed) Text string indicating the entity type (fixed) Primary text value for an entity Name (attribute tag, block name, and so on) Other text or name values Entity handle; text string of up to 16 hexadecimal digits (fixed) Linetype name (fixed) Text style name (fixed) Layer name (fixed) DXF: variable name identifier (used only in HEADER section of the DXF file) Primary point; this is the start point of a line or text entity, center of a circle, and so on DXF: X value of the primary point (followed by Y and Z value codes 20 and 30) APP: 3D point (list of three reals) Other points DXF: X value of other points (followed by Y value codes 21-28 and Z value codes 31-38) APP: 3D point (list of three reals) DXF: Y and Z values of the primary point DXF: Y and Z values of other points DXF: entity's elevation if nonzero Entity's thickness if nonzero (fixed) Floating-point values (text height, scale factors, and so on) Linetype scale; floating-point scalar value; default value is defined for all entity types Repeated floating-point value. Multiple 49 groups may appear in one entity for variable-length tables (such as the dash lengths in the LTYPE table). A 7x group always appears before the first 49 group to specify the table length Angles (output in degrees to DXF files and radians through
44

11-18

20, 30 21-28, 31-37 38 39 40-48 48 49

50-58

60 62 66 67 68 69 70-78 90-99 100

102

105 1000 1001 1002 1003 1004 1005 1010

1020, 1030

AutoLISP and ObjectARX applications) Entity visibility; integer value; absence or 0 indicates visibility; 1 indicates invisibility Colour number (fixed) "Entities follow" flag (fixed) Space-that is, model or paper space (fixed) APP: identifies whether viewport is on but fully off screen; is not active or is off APP: viewport identification number Integer values, such as repeat counts, flag bits, or modes 32-bit integer values Subclass data marker (with derived class name as a string). Required for all objects and entity classes that are derived from another concrete class. The subclass data marker segregates data defined by different classes in the inheritance chain for the same object. This is in addition to the requirement for DXF names for each distinct concrete class derived from ObjectARX Control string, followed by " {<arbitrary name>" or "}". Similar to the xdata 1002 group code, except that when the string begins with it can be followed by an arbitrary string whose interpretation is up to the application. The only other control string allowed is "}" as a group terminator. AutoCAD does not interpret these strings except during drawing audit operations. They are for application use Object handle for DIMVAR symbol table entry ASCII string (up to 255 bytes long) in extended data Registered application name (ASCII string up to 31 bytes long) for extended data Extended data control string (" {"or "}") Extended data layer name Chunk of bytes (up to 127 bytes long) in extended data Entity handle in extended data; text string of up to 16 hexadecimal digits A point in extended data DXF: X value (followed by 1020 and 1030 groups) APP: 3D point DXF. Y and Z values of a point

Using these group code and value pairs, a DXF file is organized into sections composed of records, which are composed of a group code and a data item. Each

45

group code and value is on its own line in the DXF file. Each section starts with a group code 0 followed by the string SECTION and then a group code 2 and a string indicating the name of the section follows. Each section is composed of group codes and values that define its elements. A section ends with a 0 followed by the string ENDSEC.

There are 7 sections in a DXF file. The overall organization of a DXF file is as followings: HEADER section. Contains general information about the drawing. It consists of an AutoCAD database version number and a number of system variables. Each parameter contains a variable name and its associated value. The following is an example of the HEADER section of a DXF file:

0 SECTION 2 HEADER 9 $<variable> <grotip code> <value> 0 ENDSEC

Beginning of HEADER section

Repeats for each header variable

End of HEADER section

CLASSES section. Holds the information for application-defined classes, whose instances appear in the BLOCKS, ENTITIES, and OBJECTS sections of the database. A class definition is permanently fixed in class hierarchy. The following

46

is an example of the CLASSES section of a DXF file:

0 SECTION 2 CLASSES 0 CLASS
1

Beginning of CLASSES section

Repeats for each entry

<class dxf record> 2 <class name> 3 <app name> 90 <flag> 280 <jlag> 281 <Jlag> 0 ENDSEC

End of CLASSES section

TABLES section. Contains definitions for the following symbol tables: APPID (application identification table) BLOCK_RECORD (block reference table) DIMSTYLE (dimension style table) LAYER (layer table) LTYPE (line type table) STYLE (text style table) UCS (user coordinate system table) VIEW (view table)

47

ASCII DXF Files 1165 VPORT (view-port configuration table)

BLOCKS section. Contains block definition and drawing entities that make up each block reference in the drawing.

ENTITIES section. Contains the graphical objects (entities) in the drawing, including block references (insert entities). The following is an example of the ENTITIES section of a DXF file:

0 SECTION 2 ENTITIES 0 <entity type> 5 <handle> 330 <pointer to owner> 100 AcDbEntity 8 <layer> 100 AcDb<classname> . <data> 0 ENDSEC

Beginning of ENTITIES section

One entryfor each entity definition

End of ENTITIES section

OBJECTS section. Contains the nongraphical objects in the drawing. All objects that are not entities or symbol table records or symbol tables are stored in this
48

section. Examples of entries in the OBJECTS section are dictionaries that contain line styles and groups.

THUMBNAILIMAGE section. Contains the preview image data for the drawing. This section is optional.

The first group of data that we want to determine is the range of GIS data. This group is stored in HEADER section with name of "$EXTMIN" and "$EXTMAX". The part of this section is shown as below format.

0 SECTION 2 HEADER 9 SEXTMIN 10 313607.630 20 4832880.255 30 69.9936 9 SEXTMAX 10 315698.825 20 4835669.674 30 360.642 0 ENDSEC
49

X coordinates of left-bottom corner Y coordinates of left-bottom corner minimum Z coordinates of map

X coordinates of right-top corner Y coordinates of right-top corner maximum Z coordinates of map

After determining the range of map, the properties and coordinates of utilities should be extracted from the DXF file. All these data are stored in ENTITIES section. For example, information of a manhole is saved in DXF file with following format.

0 INSERT 5 CID 330 IIADB 100 AcDbEntity 8 ROUND_MANHOLE 100 AcDbBlockReference 2 44 10 314613.946 20 4833499.7061 30 76.254 41 transformation 0.999990686758433 42 0.999990686758433 43 0.999990686758433

Class marker Entity ID

Is Entity Property Subclass marker Block name X coordinates o f insert point Y coordinates o f insert point Z coordinates of insert point 41,42,43 are not used in

After needed the required information is extracted and saved in an ASCII file with following format. minimum X coordinates maximum X coordinate
50

minimum Y coordinates minimum Z coordinates total number of entities

maximum Y coordinate maximum Z coordinate

entity property number of points of Entity .. .X Y Z value of each point appendix value(optional)

3.2.3.2

GPS Navigation

Most GPS receivers support a navigation function in their specialized software programs. However, vendors do not supply users with an interface to use GPS receivers in their own programs. A GARMIN GPS 10 was selected because it is a bluetooth-enabled wireless GPS receiver and inexpensive. When it is working in the National Marine Electronics Association (NMEA) mode, this kind of GPS receiver can send latitude, longitude coordinates and other information as an ASCII sentence to a target computer through a virtual serial communication port (performed by bluetooth technology). NMEA is a standard protocol, used by GPS receivers to transmit data. NMEA output is RS-232 compatible and uses 4800 bps, 8 data bits, no parity and one stop bit. NMEA sentences are all ASCII. Each sentence begins with a dollar sign ($) and ends with a carriage return linefeed character sequence. The following NMEA sentence describes the position information of the current GPS receiver.

51

"$GPGGA, hhmmss.ss, 1111.11, a, yyyyy.yy, a, x, xx, x.x, x.x, M, x.x, M, x.x, xxxx*hh"

where: GGA = Global Positioning System Fix Data

hhmmss.ss = UTC of Position 1 1 1 1.11 a yyyyy.yy a
X

= Latitude = North or South = Longitude = East or West = GPS quality indicator (0=invalid; 1=GPS fix; 2=Diff. GPS fix)

X X x.x x.x M x.x

= Number of satellites in use [not those in view] = Horizontal dilution of position = Antenna altitude above/below mean sea level (geoid) = Metres (Antenna height unit) = Geoidal separation (Diff. between WGS-84 earth ellipsoid and mean sea level.)

M x.x xxxx *hh

= Metres (Units of geoidal separation) = Age in seconds since last update from diff. reference station = Diff. reference station ID# = the Checksum data

52

Since GPS 10 supports geodetic coordinates where the coordinates of GIS dataset is the Modified Transverse Mercator (MTM) projection (The scale factor for this projection is 0.9999 and the false easting is 304800.0 metres.), we must transform geodetic coordinates of GPS position to the MTM coordinates system when working on this project. After this transformation, GPS receiver can be used for navigating in the GIS data set to complete GCPs collection.

3.2.S.3

GCPs Collection

The GIS data set is scaled to suit the site scale then the user navigates to the target area by using the GPS receiver. The program will display all needed vertices on the computer screen. All vertices have 3D coordinates and can be used as GCPs. Each GCP has the information of point name, X, Y and Z coordinates. After moving the mouse cursor over a vertex, the coordinates of that vertex can be obtained and saved into a GCP file. This file then can be directly used in the external orientation program.

3.3 3D Reconstruction 3.3.1 Introduction

Here, the expression 3D reconstruction means using photogrammetric methods to obtain 3D coordinates of target point and to save them with special features. There are

53

two main photogrammetric methods that can be used in computing 3D coordinates, space intersection and bundle adjustment. Space intersection is used when only two images are employed. This method does not need good initial approximations. But errors cannot be corrected through this method. Bundle adjustment uses the collinearity equations to calculate 3D coordinates of ground point. It is used when more than three images overlap the target area. However, collinearity equations are nonlinear and good initial approximations are needed. The current solution uses space intersection to calculate space coordinates as the first step; if 3 or more images overlap target area, using the results of the space intersection as initial approximations and using the bundle adjustment to correct them. The methodologies of space intersection and bundle adjustment are described in Sections 3.3.2 and 3.2.3.

Whether space intersection or bundle adjustment is used, the image coordinates of homologous points are required. Generally, homologous points are not easy to measure in large-scale images used in our project, as most target points do not have point features. Epipolar geometry is normally used in stereo vision to solve the correspondence problem in two images, i.e. for a given 2D point in the first image, it would he found the corresponding 2D point in the second one. Section 3.3.4 describes the methodologies of epipolar geometry.

54

3.3.2

Space Intersection

If two images are employed in calculating these coordinates, this procedure is known as space intersection, so called because corresponding rays to the same object point from the two images must intersect at the point, as shown in Figure 3-5. Here, (X^,Y^,Z^y and (X ,Y ,Z Yare ground coordinates of homologous points in first
2 2 2

and second image. Equation (3-20) shows how to compute these coordinates. (X^,Y^,Z^yare the coordinates of ground point. The computation of (X^,Y^,Z^y is described as follows.

Yi

and

Y2 K~fj

UJ
where:

(3-20)

image coordinates of first image image coordinates of second image R,: R2-. / rotation matrix of first image rotation matrix of second image the focal length of a digital camera

55

Bz

Za Xa

Ya

Bx

Figure 3-5 Space intersection 1

Figure 3-6 is the result of projecting Figure 3-5 to the X-Z plane. In Figure 3-6,

D X ,= X ,- X s , DX -- -- X^ 2 DZ^ = " Z^j
2

D Z ^= Z ^-Z ^2 ~ ^S2 " ^S\ ~ ^S2 ~ ^S\ " ^S2 ^S\

(3-21)

56

Bz

DZi

D 2^

DX2 Z Xa
a

Bx

Figure 3-6 Space intersection 2

From Figure 3-6, we have:
_ z, DZ, DZ,
since

and

DX^

DZ^

(3-22)

D X ,= X ^ - X ,,, D X ,= X ,- X s ^ DZ^ = --Z ji, DZ = Z^ -- Z
2

52

~ X^^ ,

= Z^2 ~ ^Si

Equations (3-22) can be transformed to Equations (3-23) _ A _ {Â) and Bx+DX^ B +DZ
2 2

DX

2

DZ

{b )
2

(3-23)

57

(B) can be transformed to (C) X. (C)

Substitute (C) into (A), we obtain - X ,Z .) = {Z,B^-X,B^)Z^

Then

X ,Z ,-X ,Z , and Z a =D Z^+Z s2

Based on the same method, we obtain
ny _ ~ ^ \^ z Y ny Y ryv _ ~ ^ \^ Z y

'
0

X.Z^-X^Z,
2 2

' -- DY + I
52

X .Z j-X jZ , '

'

'

X .Z j-Z ^Z ,

'

(3-24)

XA = X +-^S2 »

> " ^ A-- DZ^ +Z^2

From this method we can see that if there are errors in exterior elements or interior elements, a wrong result will be generated as it is derived directly from such elements. A solution that can be used to correct such problems is a bundle adjustment.

3.3.3

Bundle Adjustment

As mentioned in Section 3.2.1, Equation (3-1) can be used for generating 3D coordinates of target point if more than two digital images overlap target area and the

58

interior elements, distortion coefficients and exterior elements are known. As mentioned in Section 3.2, Equation (3-1) is nonlinear and can be rewritten as Equation (3-15). According to Taylor's series. Equation (3-15) can be expressed in a linearized form by taking partial derivatives with respect to the ground coordinates:

^0 +

+1 ^

Lay
( dG

\dY, + fdO'^ (3-25) dZ^ = y

In Equation (3-25),

and

are functions F and G of Equation (3-15) evaluated

at the initial approximations for the three unknown ground coordinates; the terms
( d f \ 'da'

UzJ

, are partial derivatives of functions F and G with respect to

the indicated unknown (X^,Y^,Z^y evaluated at the initial approximations; and dX^,dY^ ,dZ^ , are unknown corrections to be applied to the initial approximations. The initial approximations can be directly from space intersection. Equation (3-25) can be simplified as following forms:

budX^ +bi^dY^ +bi^dZ^ = J + v^ b^dX^ +b^^dY^ +b^^dZ^ = K + v^

(3-36)

where

59

^14

^15 =^('*32-^" 'l2ô) bl6 = ^ (^ 3 3 ^ -n 3 ô ) J = x-Xq-Ax + / -- Ô
^24 = ^ ( ' 3 l `^ " ^2lô) = / =-^(^32'^" ^22Ô) q

625

_ ^23k:; V '33*-' ~ '23 ^26 ~ L (^33 Ô) 5 K = y -y o ~ ^ y + f-Q

This method is called a bundle adjustment where collinearity equations based on light ray bundles are employed. As three or more images are used for calculating space coordinates, which means there are six or more conditions to calculate 3 unknowns, the errors can be adjusted using the least square adjustment method.

Considering the exterior elements contain errors, these elements also can be corrected using a bundle adjustment. Equation (3-1) can be rewritten in a linearize form as follows:

60

^0 + i f ) dco + Kdco) dY ,+ dG\, , \''r y
+

^ dF ^ 'd p ' ^ dF ^ dX ^ + dZ, dY^ + W s) la y j

(S)
f dG)

-X

(3-27) dXs + ^dG^ f/y.+l " 1 dZ^

Equation (3-25) can be simplified to the following forms:
b^^dci}+ b^2d^-^b^^dK-- b^^dXg -- by^dY^ -- bi^dZ^ +

b^^dXA + b\^dYA + bj^dZA = ·/ + b2\dco + bjid^ + b2^dK -- b2^dX^ -- b2^dY^ -- b2^dZ^ + bi^dXA +b2sdYA +b2^dZA = K + v ^

(3-28)

where by can be computed using the same method as shown before. The initial approximations can be directly obtained from the external orientation and space intersection results.

3.3.4

Epipolar Geometry

Epipolar geometry expresses the geometric relationship between two images of a rigid body. Two perspective images of a single rigid object scene are related by the so-called epipolar geometry, which can be described by a 3 X 3 matrix. If the interior elements of the images (e.g., the focal length, the coordinates of the principal point, etc.) are known, the normalized image coordinates can be used, and the matrix is known as the essential matrix. Otherwise, the pixel image coordinates are used, and
61

the matrix is known as the fundamental matrix. It contains all geometric information that is necessary for establishing correspondences between two images, from which 3D structure of the perceived scene can be inferred.

3.3.4.1

What is Epipolar Geometry?

The epipolar geometry between two views is essentially the geometry of the intersection of the image planes with the pencil of planes having the baseline as axis (the baseline is the line joining the camera centres). This geometry is usually motivated when considering the search for corresponding points in stereo matching, and we will start from that objective here. Suppose a point X in 3-space is imaged in two views, at point x in the first image, and x ' in the second image. As shown in Figure 3-7, the image points x and x ', space point X, and camera centres are coplanar where this plane is denoted as n. Clearly, the rays back-projected from x and x intersect at X, and the rays are coplanar, lying in n. It is this latter property that is of most significance in searching for a correspondence.

a
Figure 3-7 Point correspondence geometry
62

b

/ p i p o l a r line for X

Supposing now that only x is known, how the corresponding point x ' is constrained. The plane n is determined by the baseline and the ray defined by x. From the above, the ray corresponding to the (unknown) point x ' lies in n. Hence, the point x ' lies on the line of intersection 1' of s'with the second image plane. This line 1' is the image in the second view of the ray back-projected from x. In terms of a stereo correspondence algorithm, the benefit is that the search for the point corresponding to x need not cover the entire image plane but can be restricted to the line 1'.

baseline

baseline

a

b

Figure 3-8 Epipolar geometry

The geometric entities involved in epipolar geometry are illustrated in Figure 3-8. The terminology is; The epipole is the point of intersection of the line joining the camera centres (the baseline) with the image plane. In other words, the epipole is the image in one view of the camera centre of the other view. It is also the vanishing point of the baseline (translation) direction. An epipolar plane is a plane containing the baseline. There is a one-parameter

63

family (a pencil) of epipolar planes. An epipolar line is the intersection of an epipolar plane with the image plane. All epipolar lines intersect at the epipole. An epipolar plane intersects the left and right image planes in epipolar lines, and defines the correspondence between the lines.

3.3.4.2

The Fundamental Matrix F

The fundamental matrix is the algebraic representation of epipolar geometry. It satisfies: (1) F is a rank 2 homogeneous matrix with 7 degrees of freedom, (2) If X and x' are homologous points, then x'^ Fx = 0, (3) l'=Fx is the epipolar line corresponding to X , (4) / = F^x' is the epipolar line corresponding to x' and (5) Epipoles satisfy Fe = 0 and F^e'= 0. The fundamental matrix can be computed from camera projection matrices P, P'. The ray back-projected from x by P is obtained by solving x =PX. Or
X

y =p
\ j
1

Y Z

(3-29)

where P is a 3 X 4 matrix. Equation (3-29) can be rewritten to Equation (3-30) X{1) = P^x +XC (3-30)

where P^ is the pseudo-inverse of P, i.e. P P^ = 1, and C its null-vector, namely the
64

camera centre, defined by PC = 0. The ray is parametrized by the scalar X . In particular, two points on the ray are P^x (at X = 0), and the first camera centre C (at X = co). These two points are projected by the second camera P ' at P'P^x and P'C, respectively, in the second view. The epipolar line is the line joining these two projected points, namely 1 - (P'C) X (P'P^x). The point P'C is the epipole in the second image, namely the projection of the first camera centre, and may be denoted by e'. Thus, r = e 'x(PP*)x = Fx where the fundamental matrix F can be ealculated as F = e'x(PP^) (3-32) (3-31)

When a target point in the first image is measured, the corresponding epipolar line can be generated and drawn in the second image since digital images are employed in our project. It will help operators easily find corresponding point in the second image. Furthermore, if three images overlap target area and the imagepositions of the first two images are measured, two epipolar lines can be computed and intersect in the third image. The point of intersection gives the initial position of target point in the third image.

3.4 Chapter Summary
This chapter describes the processing steps and their methodologies of this procedure.

65

In photogrammetry, the interior elements that include focal length and the position of principal point should be computed through camera calibration before the camera is used. The distortion coefficients of lens are also calculated through this procedure. In order to make the processing easier and achieve a higher accuracy result, image matching and LSM are involved.

External orientation is a procedure to determine the position and orientation parameters called exterior elements. Without external orientation, the space coordinates cannot be calculated. GCPs have important rolls in external orientation. An impact method is presented in this chapter that can easily obtain GCPs used in external orientation to calculate exterior elements.

After both interior and exterior elements are known, space coordinates of target point can be computed using space intersection if two images overlap target area. Furthermore, if three images overlap the target area, a bundle adjustment can be employed for correcting space coordinates and adjusting any errors. Epipolar geometry is also introduced in this chapter. The fundamental matrix is used in calculating epipolar lines that can help operators easily measure image coordinates of target point in digital images.

66

4 CASE STUDY OF CAMERA CALIBRATION
This designated study involved building two software programs to complete camera calibration. The objective of the first program is to automatically detect image coordinates of GCPs that can be used for calibrating our digital camera. The second program is used for camera calibration. The distribution of errors and distortions are shown in this program. The corresponding results obtained from the analysis are presented in Section 4.2.2 of this chapter.

4.1 Automatic Detection Program
This program is a data preparation program used for camera calibration. The objective of this program is to automatically detect image coordinates of GCPs called reference points in this chapter. This program is a Microsoft Windows program coded by Visual C++. Figure 4-1 shows the interface of this program.

After loading the digital image, the program can automatically detect image coordinates of GCPs. This procedure can be divided into three processing steps. The first step is to automatically detect the initial positions of reference points in digital image using image matching technology. The second step is obtaining the accurate positions of reference points using LSM. The third step is merging such reference points and corresponding GCPs together in order that they can be used for camera

67

calibration.

Figure 4-1 The interface of automatic detection program

4.1.1

The Detection of Reference Points

As mentioned in Section 3.1.3, based on the control pattern, two kinds of images are employed as fiducial images in order to detect the reference points. And, an image window called matching window is used for extracting grey values from original image at the start position (left-bottom comer). If the current point in the original image is checked as a reference point, it will be added to the reference point table. The start position is changed pixel by pixel and the above steps are repeated until the matching window reaches the top-right comer of the original image. Then initial positions of all reference points are, thus, detected. Figure 4-2 illustrates the flow chart of this procedure.
68

Define 3 15X15 arrays named `M atchDatal", "MatchData2" and "Tai^etData" in memory

Yes

Load the grey values of 2 fiducial images into arrays "MatchDatal" and "MatchData2"

Check if there is a reference point already been found around current point

Yes

Get grey values from a 15X15 window of source image and load them into "TargetData". The start point o f this window is (i, j). The initial values o f i and j are 0 They will be changed in another step. Add current point as a reference point Calculate the correlation coefficient named "coe" in program between "MatchDatal and "TargetData". Check if i=image width-16 and j= image height-16 Coe>threshold No Yes Calculate the correlation coefficient between "MatchData2' and "TargetData". End No No

Coe>threshold No If i=image width-16 then i=0, j=j+l Else i=i+l

Figure 4-2 Flow chart of detecting initial positions of reference points

69

4.1.2

Obtaining Accurate Positions Using LSM

After the initial positions of reference points are detected, the next step is obtaining the accurate positions using LSM. As mentioned in Section 3.1.3, LSM not only considers illumination and reflectance differences between the two images but also the geometric distortion of the regular image tessellation caused by unknown orientation parameters, tilted surface patch, a surface patch with relief, etc. The previous step gives the approximations within accuracy of a few pixels. These approximations can be used in LSM as initial values. Figure 4-3 illustrates the flow chart for obtaining accurate positions using LSM. Figure 4-4 shows an example result of automatically detected reference points.
Get image coordinates o f current reference point from reference point table Add the result o f LSM to

reference point table.

Load grey values from fiducial image If current point is the last point o f the table ? Load grey values o f the original image into a 45X45 matching window using the reference point as centre point No Move to the next point

Yes

LSM End

Figure 4-3 Flow chart of obtaining accurate positions using LSM.
70

S « a rch P h o to P o » n ts-- ClYll E ngbw pfing D e p a rtm e n t o f R y e rso o U nlversR y

( a" C M C C I -| ! 'r = = = = = = = g V f Search % |
1:1 .·  > ^ 'S C
'.

I:

I % Save Data ' I

P Process Display ;

Figure 4-4 An example result of automatic detection of reference points

4.1.3

The Combination of Reference Points and GCPs

Although reference points are obtained through the above two steps, the relationships between them and GCPs are still unknown. Camera calibration cannot be processed without building up such relationships.

The DIT can be used for building up these relationships. If the relationships between at least
6

reference points and their corresponding GCPs that have a good distribution

are known, a rough set of interior elements and exterior elements can be computed using the DLT method. Based on these data and letting Ax = = 0, approximate

image coordinates of each GCP can be computed using Equation (3-1). The program

71

then compares each pair of image coordinates to each reference point by calculating the distance between them, if the distance is less than 40 pixels, then current GCP corresponds to the current reference point. At least
8

pairs of reference points and

GCPs are used in this program for a least-squares adjustment. As the rough set of interior and exterior elements of the camera is easy to calculate, the key problem that must be solved is how to determine the relationships between these reference points and their corresponding GCPs. There are two methods presented in this thesis and coded in the software program to solve this problem.

The first method is to directly determine corresponding points step by step. As shown in Figure 4-4, when a reference point is selected, the GCPs window will pop up as shown in Figure 4-5. The distribution of GCPs is displayed in this window. So the corresponding GCP of that selected reference point could be found and selected in this window. This procedure is repeated until all
8

pairs of reference points and

corresponding GCPs are determined. Then the result can be used for computing the approximate parameters of interior and exterior elements and combining the reference points and corresponding GCPs together, respectively. The advantage of this method is that it works for most cases such as big angular orientations. The disadvantage of this method is that it requires a long time for operator to complete it and it is possible to make mistakes during this procedure as operator should choose target point from hundreds of points.

72

,:lL -£ 3
383 384 375 376 166167 160 169 170 W1

e e e e a e

I i Selected Point : '

®e e ® ® ® eeee e e 158 159 ISO 161 162 163
150151 152 153 154 155

e e @® ® (
i 267 268 269270271 3 S 259 260 261 262263 3

367 368 359 360

e e

® ® e ®©(

a e e e e e

® ® © ® ® ^ a 251 2 5 2 253264255 3

II:

a a 343 a 335 a 327 a 319 a 311 a 303

351 352

a a 344 a 336 e 328 0 320 0 312 0 304

. (D a 370 371 a a 362 363 a a 354 355 a a 346 347 a a 338 339
330

Ir /' % '' '"' ::c z r:

© 33, ®

& &
if- &

Figure 4-5 GCP selection window used in determining the relationship between reference points and GCPs

Another method automatically finds several pairs of such points. Here ^ is defined as orientation angle about Y axis. Based on the distribution of GCPs and if (p is not so big, all GCPs can be projected to an image plane as a similar pattern. As shown in Figure 4-5, this pattern consists of 22 columns and 11 rows. In each row, the distance between two neighbour points in X direction is much greater than it is in Y direction. On the other hand, in each column, this distance between two neighbour points in Y direction is much greater than it is in X direction. These features relationships can be used for automatically finding target points. The following steps describe how to find a reference point next to basic point on the same row in negative X direction.

73

Stepl: Define a variable named "mindistance" in order to save the minimum distance between basic point and other reference points. Define a variable named "nFind" in order to save the found point number. Step2: Let "mindistance=10000". This value is greater than the maximum distance between two reference points. So the value of "mindistance" can be substituted in following steps. Let "nFind-- ". This value will be
1

substituted with current point number if it satisfies the condition mentioned in step <4>. Step3: Calculate the differences of x, y between the first reference point and basic point and distance between these two points. Step4: If values calculated in step <3> satisfy the conditions below; - the difference of x is smaller than -30 pixels; - absolute value of x difference > absolute value of y difference; - the distance between these two points is smaller than "mindistance" Then Let "nPind= current point number"; Let "mindistance=the distance between current point and basic point". StepS: Repeat steps <3> and <4> by changing current point one by one from reference point table until all reference points are involved. Step : If "nFind" is not equal to -- 1, the value of "nFind" indicates the neighbour
6

point that is on the same row of basic in negative x direction.

74

Determine the corresponding reference point o f GCP 207 and make it the basic point Find the neighbour reference point with respect to last found point on the same column in the positive y Find neighbour reference point with respect to basic point on the same row in negative x direction. direction one by one until no point can be found. The last found point corresponds to GCPs 287. Use this point as the basic point

Find neighbour reference point with respect to last found point on the same row in negative x direction one by one until no more points can be found. The found points correspond to GCPs 206,205,204, and so on. Use the same method to find all reference points on the same row of basic points. and so on. The found points correspond to GCPs 286,285,284

^ r
Find the neighbour reference point with respect to basic point on the same row in the positive x direction Calculate the approximations of interior and exterior elements using the DLT method based on the above results. Find the neighbour reference point with respect to last found point on the same row in positive x direction one by one until no more points can be found. The found points correspond to GCPs 300,301,302, and so on. Combine the reference points and corresponding GCPs together.

Find the neighbour reference point with respect to basic point on the same column in the positive y direction

End

Figure 4-6 Automatically combining reference points and corresponding GCPs.

75

Figure 4-6 is the flow chart of automatically combining reference points and corresponding GCPs. The advantage of this method is that it works automatically except specifying the first basic point. The disadvantage of this method is that it cannot work within a big ^ orientation angle. As the operator can easily maintain the camera within a small ^ , the second method is advised. In Figure 4-7, the green circles and digital numbers are the result of such an approach.

Cancel

I Auto_Merge Save Data

F? Process Display

Figure 4-7 An example of combination result

Table 4-1 shows an example of data set computed by this program. This data set is used for camera calibration computing, which will be described in Section 4.2.

76

Table 4-1 An example of data set computed by the automatic detection program X ground Coordinates (m) -0.2366 -0.2048 -0.1749 -0.1442 -0.1134 -0.0827 -0.2357 Y ground Coordinates (m) -0.1693 -0.1685 -0.1687 -0.1686 -0.1685 -0.1686 -0.1352 Z ground Coordinates (m) 0.0298 0.0476 0.0615 0.0771 0.0926 0.1066 0.0299 X image Coordinates (pixel) -1514.9 -1407.9 -1294.5 -1173.6 -1043.1 -901.5 -1521.7 Y image Coordinates (pixel) -850.0 -885.0 -922.7 -962.0 -1003.2 -1045.8 6 6 8 . 0

Point Number
1 0 2

103 104 105 106 107
1 1 0

378 379 380 381 382 383 384 385 386 387

0.2639 0.2954 0.0807 0.1114 0.1415 0.1722 0.2028 0.2334 0.2647 0.2964

0.1361 0.1364 0.1699 0.1698 0.1698 0.1699 0.1699 0.1701 0.1706 0.1711

-0.0078 0.0079 -0.1056 -0.0876 -0.0719 -0.0567 -0.0401 -0.0244 -0.0094 0.0076

1345.4 1554.6 313.0 463.9 621.9 789.8 966.6 1152.9 1350.4 1561.7

886.5 898.8 1004.9 1019.0 1034.0 1048.3 1064.1 1080.1 1097.9 1116.6

4.2 Camera Calibration and Generation of a Look Up Table
This process is used to calibrate digital cameras, compute lens distortion coefficients and analyze errors using the DLT method. The fonction of adding calibration result into a Look Up Table (LUT) is also included. This is a Microsoft Windows based program coded using Visual C++. Figure 4-8 illustrates the interface of camera calibration program. This program involves the following steps: Stepl : the loading and displaying positions of reference points; ·

77

Step2: automatic computation of the calibration result; StepS: the generation of a LUT.

A C a m e ra C alibration

Focal Length=3386.013 pixels xO= 48.3 pixels yO= 71.0 pixels Distoriorj Coefficients ( k0=-2.55e-002 k1=7.68e-009 k2=3.60e-G15 k3=-5.11e-022 p1=5.21e-0D8 p2=-1.49e-006 J Photo Station (X s^O .l45 Ys=O.OSSe Zs^0.58B phUO.1623 ome=-0.0961 kap^O.0117

Load C oordinates ; Data

1 =

` -D isp la y M ode------------ r ; -----------;------------------- :-- -----------------------------------------------------------r : ----ri ' Q G C Ps ;: ]Photo Points^ C Corrected Photo P oints T Distortion Vectors T Error Vectors

Figure 4-8 The interface of camera calibration program

4.2.1

Data Processing

Although the interior elements, exterior elements and distortion coefficients can be computed at the same time, we divide this process into two parts as there is a strong correlation between the focal length and symmetrical distortion coefficient . This

kind of processing method makes the computed result more stable. Figure 4-9 shows the flow chart of data processing.

78

Load calibration data set that consists of image and space coordinates of GCPs into memory

Calculate 11 unknown coefficients using DLT.

Calculate interior and exterior elements based on 11 DLT coefficients

Calculate lens distortion coefficients for symmetrical distortion and asymmetrical distortion)

,k^,k2,k^ for

Correct image coordinates o f GCPs by using lens distortion coefficient computed from last step.

Calculate mean difference between corrected image coordinates and the coordinates projected from GCPs using collinearity equations

Check if this mean difference is smaller than 2 pixels

No

Yes

End

Figure 4-9 The data processing flow chart of camera calibration

79

4.2.2

Result Analysis

Figure 4-10 shows the result of camera calibration when the actual focal length is
9

. mm. The error vectors are also shown in this figure. From this figure, we can see
5

that most errors are smaller than 2 pixels. It means the accuracy of camera calibration is very good. Figure 4-11 shows the distribution of lens distortion. From this figure, we can see that the lens distortion is very big at a small focal length. The correction of lens distortion is, therefore, required.

4 » C am ef a C alibration

....
p2=-1.49e-006 1

-X J Ij":
: toad ,
<Coordinates a Data '

Focal Length=3386.013 pixels xtl= 48.3 pixels yO= 71.0 pixels Oistorion Coefficients kO--2.55e 002 k1=7.68e-009 k2=3.60e-015 k3= 5.11e 022 pi =5.2le 008 Z s -0 .5 8 6 phl=0.1623 ome=-0.0961 kap=0.0117 Photo Station [^s=(k1

Add to ; Lookup Table

r - Display

M odeO Corrected Photo Points . O Distortion Vectors f? jError V ector^. '

r : GCPs ; : r : Photo Points

Figure 4-10 The calibration result and error vectors

80

Focal Length=3616.825 pixels xO= 51.3 pixels yO= 69.0 pixels Distorion Coefficients I k 0 -'t.9 8 e 002 kl~4.17e-009 k2~3.27e-015 k3~-3.55e-022 p1=1.20e-007 Photo Station {Xs=0.160 Ys=0.065e Zs=0.631 phi-0.1601 ome=-0.0968 kap-0.0150

p2"-1,42e-006 1

' Cancel

,
; . \ CoânAlnàtca ( . ' ' Oal a

c 1' : - T able

-D isp lay Mode-- r ------:------ --------------r---

-- r r ------ -- :-- -- ----- r --

r - ----------------

' ; C GCPs, C Photo Points

Corrected Photo Points <

jPistortion Veirtorg : Cj Error Vectors

Figure 4-11 The distribution of lens distortion

After calibrating the camera at all actual focal length settings, a LUT can be established. It is indexed by actual focal lengths, which are saved in the first column. These data can be easily used in external orientation program and 3D reconstruction program. As this lookup table contains 12 columns and is not easy to be shown in this thesis, it is divided into two tables for displaying. Table 4-2 shows the first part of the LUT. There are
6

columns named real focal length, equivalent focal length, x

coordinates of principal point, y coordinate of principal points, mean errors in x direction and mean errors in y direction in this table. From the mean errors columns, we can see that all these mean errors are less than 2.0 pixels. This means that the accuracy of camera calibration using this method is very good and validates

81

p h o to g ra m m e tric p ro c e s s e s u s e d in o u r p ro je c t.

Table 4-2 The first part of the LUT
Focal length (mm) Equivalent focal length (pixels)
X coordinate o f principal point (pixels)

y coordinate o f principal point (pixels)

Mean errors in X direction (pixels)

Mean errors in y direction (pixels)

8.9
10.2
11.0 11.8

3386.0 3866.8 4137.4 4445.2 4822.2 5234.9 5666.8 6193.6 6763.3 7494.5 8281.0 9076.7 9967.9 11089.7 12087.9 13566.5 14835.5 16657.9 18383.4 19832.3 21438.1 23005.4 24106.7 25452.1 26508.8 27332.8 29882.1 32496.6 33619.4 34343.5 35056.5

48.3 41.1 46.1 50.1 38.2 55.0 53.3 50.2 59.3 62.2 70.6 82.8 107.4
100.2

71.0 62.9 57.4 54.9 47.9 41.3 42.4 45.0 40.2 36.8 35.6 38.8 33.2 36.1 36.6 17.7 26.5 14.4 10.9 9.3 9.7 5.1 -5.7 -14.9 -19.8 -15.4 -2.3 -19.8 -40.6
-

1.2

1.0

1.3 1.3
1.0

1.2
1.1

0.7
1.1

12.7 13.8 15.0 16.4 18.0 19.8
21.8

1.3
1.0

0.5 0.5 0.5 0.7 0.8 0.9 0.9 0.9 0.9
0.8
1.0

0.6 0.6
1.0
1.2

1.3 1.3 1.3 1.5
1.2

24.0 26.5 29.3 32.4 35.9 40.0 44.5 49.0 53.3 57.3 61.0 64.5 68.0 71.0 74.0 77.0 80.0 83.0 86.0 89.0

103.4 143.1 153.0 126.5 139.7
202.6

1.4 1.4 0.9 0.9 1.5 1.5
1.0

1.0

0.7 0.7
1.0

162.7 170.9 245.4 193.5 198.4 264.0 290.9 263.0 269.2 263.7 257.6

0.9
0.8

1.5
1.1

0.9
0.8

1.0

0.8 0.9
1.1

1.5 1.4 2.0 1.5
1.6 1.6

1.2
1.1

11.1

1.0 1.0

-20.4

Table 4-3 shows the second part of the LUT. There are 7 columns named focal length, KO, K l, K2, K3, PI and P2. KO, K l, K2 and K3 are symmetrical distortion

82

coefficients where PI and P2 are asymmetrical distortion coefficients correspond to each focal length setting. All these coefficients are used for correcting image

coordinates. Table 4-3 The second part of the LUT
Focal length (mm)
8.90 10.20 11.00 11.80 12.70 13.80 15.00 16.40 18.00 19.80 21.80 24.00 26.50 29.30 32.40 35.90 40.00 44.50 49.00 53.30 57.30 61.00 64.50 68.00 71.00 74.00 77.00 80.00 83.00 86.00 89.00 -2.548277e-2 -1.343463e-2 -7.629539e-3 -4.216220e-3 -5.003205e-4 2.2731716-3 2.6802090-3 3.2986570-3 3.9173460-3 4.1432720-3 4.4900330-3 4.5215920-3 6.4014940-3 4.5507790-3 6.2733800-3 6.6236460-3 6.6109960-3 6.7837840-3 6.9692070-3 6.1651930-3 4.6498870-3 6.7190390-3 6.0179230-3 6.5751420-3 6.5836270-3 5.7479520-3 6.2637980-3 6.4116150-3 7.2538490-3 6.2392320-3 6.7343280-3 7.6801510-9 1.2074500-9 -2.4064540-9 -3.1078290-9 -4.4042310-9 -5.1854750-9 -4.2389290-9 -4.5597260-9 -4.4364980-9 -4.1932320-9 -4.2835520-9 -4.4758890-9 -5.8846210-9 -4.1408120-9 -6.3043940-9 -5.5458370-9 -5.6246720-9 -4.7312280-9 -4.8283100-9 -4.6515790-9 -4.3257090-9 -4.6213200-9 -4.5975850-9 -5.0131930-9 -4.9651510-9 -4.2691080-9 -5.9316650-9 -5.5603780-9 -6.8816240-9 -5.5769700-9 -6.2797200-9 3.6040530-015 2.9593060-15 3.7532450-15 2.8643180-15 2.2877250-15 1.9797630-15 1.2297390-15 1.1368120-15 9.7195860-16 6.8932980-16 6.5798410-16 7.2240150-16 9.6118680-16 5.1560720-16 1.5278330-15 7.7125410-16 7.5101130-16 -3.5745720-17 1.4242500-17 2.0635860-16 7.1974680-16 -8.4217620-17 2.7798440-16 3.7903980-16 2.5494110-16 1.0390400-16 1.0952860-15 7.2451100-16 1.2793390-15 7.0736410-16 1.2136610-15 -5.1134950-22 -2.7694830-22 -4.1844760-22 -2.4745250-22 -1.1025650-22 -8.2141430-23 4.9397020-24 4.6323990-23 2.8546150-23 7.6788960-23 7.3330590-23 7.8825640-23 5.8630000-23 9.8145330-23 -8.3281080-23 5.5350240-23 7.9559080-23 2.2051290-22 2.0436890-22 1.6223060-22 4.2658810-23 2.2531970-22 1.3711570-22 1.2769630-22 1.6464370-22 1.7237950-22 2.8988840-23 1.3027620-22 5.9314550-23 1.4280510-22 7.2644100-24 5.2139910-8 4.5463670-9 1.7142090-7 1.2856860-7 6.7726500-9 1.7592690-7 2.7161290-8 -6.1106970-8 -1.9216880-7 -2.4393970-7 -2.4979010-7 -2.6303620-7 3.2579690-8 -2.7018710-7 -6.0668440-8 3.2092920-8 4.4819640-8 8.1333740-8 8.4508560-8 6.5174380-8 -1.7007010-7 9.8554000-8 7.4617230-8 1.1302000-7 1.1013470-7 8.7269440-8 1.1087230-8 -9.1734190-8 -2.7968130-8 -4.3079380-8 -5.1475690-8 -1.4871800-6 -1.1072490-6 -9.5808160-7 -5.9932490-7 -4.5966840-7 -1.5941750-7 -8.8532690-8 -1.0739710-7 3.3837630-8 3.8587050-8 4.8542640-8 4.3090040-8 1.2704590-7 5.1698330-8 1.3495310-7 1.1760420-7 1.4823100-7 1.7805050-7 1.6343580-7 1.4316080-7 2.8704080-8 9.5662560-8 9.2921050-008 1.3239850-007 1.3234620-007 1.1259000-007 1.5495890-007 4.0056010-007 4.5328830-007 2.9398480-007 4.2377220-007 KO Kl K2 K3 PI P2

83

4.3 Chapter Summary
This chapter presents a digital camera calibration solution in this study. Two software programs are introduced in this chapter. The first program is for automatically detecting image coordinates corresponding to GCPs that can be used for camera calibration. Image matching and LSM are employed in this program. Image matching is used for obtaining initial positions of corresponded image points (reference points) where LSM is used for computing accurate positions by correcting geometric distortion, illumination and reflectance differences. This program also provides two functions to combine GCPs and corresponded reference points. The result of this program can be loaded as an input file used for camera calibration.

The second program is a camera calibration program. The distribution of errors and distortions are shown in this program. It also provides a function to add the calibration result into a LUT, which can be used in other photogrammetric program. The corresponding results indicate that it is a rigid camera calibration program because the interior and exterior elements computed by this program are stable and mean errors of image coordinates are less than 2 pixels.

Both of these two programs are easy to use since good graphical interfaces are provided and most operations can be completed automatically. The objective of developing these programs is realized.

84

5 CASE STUDY OF EXTERNAL ORIENTATION AND GCPS COLLECTION
This chapter describes the designed study of external orientation and GCPs collection. Three software programs are developed for this study. The objective of the first and second program is to complete GCPs collection using methodologies mentioned in Section 3.2.3. These collected GCPs will be inputted to the second program. The third is an external orientation program used for calculating exterior elements of digital images, which are used for 3D reconstruction.

5.1 GIS Data Transformation Program
As mentioned in Section 3.2, external orientation cannot be processed without GCPs. This study presents a new method to collect GCPs from an existing GIS data set. Firstly, the existing GIS data, which is written in AutoCAD 2005 drawing format (.dwg), should be transformed to a special format defined in Section 3.2.3 that can be used in developed GCPs collection program. This transformation includes two steps. The first step is transforming the old GIS data from drawing format to corresponding DXF format by using AutoCAD program. The second step is extracting and transforming needed data from DXF format to our own graphic format (.gph). The designed purpose of GIS data transformation program is to complete the second step. Figure 5-1 shows the flow chart of GIS data transformation program.
85

-

Read a set o f tagged data from DXF file. If following then is string = else, SEXTMAX, comer Check if group code Yes top-right

obtained;

bottom-left comer is obtained

= 9?

No

Check if group code No
=

Check if following string =CIRCLE, INSERT or POLYLINE

No

0

?

Yes

Check if following string = EO F? No Yes Read next set of tagged data from DXF file.

Yes Write the information Extract property information and coordinates o f object vertices from DXF file

extracted from DXF file into a ".gph" format file.

End

Figure 5-1 Flow chart of GIS data transformation

86

5.2 GCPs Collection Program
This program is used for collecting GCPs from the field based on GIS and GPS technology. It is a Microsoft Windows program coded by Visual C++. Figure 5-2 shows the interface of this program.

m m ##

i
*

Load G raph ||

Z oom

|

M ove

|

|

P oint

G PS_O nllne

|

O Hsel settin g |

GCP

j

V ettex |

Figure 5-2 The interface of GCPs collection program

This program provides three main functions that help operators collect GCPs from graphics data transformed from existing GIS data. The first function is graphics displaying. After loading graphics document, it is easy to display, zoom and move these graphics data on screen window. The positions of vertices and collected GCPs

87

can also be displayed as rectangles with different colours. Figure 5-3 shows an example of zoomed graphics. Vertices are also shown as blue rectangles on the screen. This function helps operators easily determine which vertex can be chosen as a GCP.

Load G raph

Zoom

M ove

P oint

GPS Online

O ffset settin g

GCP

[ v c rte x

Figure 5-3 Zoomed graphics with vertices displayed

The second main function is GPS navigation. After initializing the status of serial port and set GPS receiver on National Marine Electronics Association (NMEA) mode, this function receives GPS signals every 100 milliseconds. The coordinates of GPS receiver are transformed from a geodetic system to a Modified Transverse Mercator (MTM) system in order to correspond to the local GIS coordinates system. The centre point of displaying area is set to this GPS position so that the graphics data of working

area can be always displayed on the screen with an appropriate scale. A defect of this function is that GPS signals may be interrupted by some high structures so that the position of GPS receiver cannot be obtained everywhere. An aiding function is used for solving this problem. As shown in Figure 5-4, the coordinates of some intersections are stored in a document and can be selected from a list box of a pop up window. The coordinates of selected intersection will be loaded and set as the centre point. This aiding function is useful when there is no GPS signal.

Input thl,ôf]fset

X:

[Ô

i V ^ ^ Y:; |Ô

iîm

Please choose an Intersection from list

· 





...
Cancel

Young & King Young & Queen Young & Dandas University & King University & Queen University & Dandas University & College Spadina & King Spadina & Queen Spadina & Dandas Spadina & College

iîZJ'

Figure 5-4 Select an intersection as centre point of displayed graphies

The third main function is collecting GCPs from GIS data. GPS receiver gives the
89

centre point of working field. The following steps describe the process of how to collect a GCP. Stepl: The displaying, zoommg and roaming of graphics map and all vertices. Step2: The selection of interest vertex. StepS: The point ID editing of interest vertex. As shown in Figure 5-5, a pop up window shows 3D coordinates of current vertex and default point ID. The point ID can be changed and saved as desired.
GCP Management P o in t ID 1 2 3 4 , V 4 8 3 4 6 8 7 .5 3 5 2 4 8 3 4 6 7 0 .9 4 3 6 4 8 3 4 7 2 9 .9 6 4 0 4 8 3 4 6 8 3 .0 4 6 0 2 9 1 .0 8 0 0 1 1 4 .6 5 2 4 9 1 .0 8 9 6 9 0 .9 3 7 6 SaueRExit P o in t ID it 'til

X '  3 1 4 3 3 8 .9 0 1 2 3 1 4 3 9 7 .1 5 7 6 3 1 4 3 3 7 .0 3 6 8 3 1 4 3 4 7 .9 0 6 0

1 .......... 1 3 1 4 3 4 7 .9 8 6 I

g

jl483l|683.0i46

2
9 B .9 3 7 6

Add C u rren t GCP.

b

:

Figure 5-5 GCPs management window

This program provides a good graphical interface that makes it easy to collect GCPs from GIS data that are located in the working area. For a skilled operator, this processing needs no more than 20 minutes. In contrast to traditional methods, such as aero-triangulation or using high accuracy GPS receiver, it has a significant advantage. The accuracy of such vertices is smaller than 0.1m , which satisfies the demand of
90

underground utilities mapping.

5.3 External Orientation Program
The purpose of this program is the calculation of the exterior elements of digital images. It is a Microsoft Windows program coded by Visual C++. Figure 5-6 illustrates the interface of this program. There are three sub windows in this program. The biggest window is for displaying the digital image. The measurement of image coordinates works in this window. The top-right window is for displaying full digital image with a small scale. A green rectangle indicates corresponding area displayed in the biggest window. The bottom-right window lists the GCPs imported from GCPs document created from GCPs collection program. The following information indicates the status of GCPs. "Available" means this GCP is involved in calculating the exterior elements. "Unavailable" means it is not used. This status can be changed by double-clicking left mouse button after moving mouse cursor on the corresponding item. As mentioned in Section 3.2.2, collinearity equations are used for external orientation. Since space coordinates of GCPs used in this program are already obtained from GIS data, the main function of this program is providing a user interface where image coordinates of GCPs can be measured and for calculating the exterior elements.

91

C ancel

| ^«r I ro p o it C o n tro l P o i n t e f ile

j |i

C a lc tja te

| ^ t n te r te r P a r a m e t e r s | | C o n t r o ^ l ^ n t s

Available Available Available

Figure 5-6 The interface of external orientation

The calculation of external orientation cannot proceed without image coordinates corresponding to GCPs. The measurement of image coordinates is divided into following steps: Stepl : The selection of a target GCP. Step2: The determination of the rough image position corresponding to the target GCP that can be used for displaying the sub image in the measurement window. StepS: The measurement of image coordinates corresponding to the target GCP.

After both GCPs and their corresponding image coordinates are obtained, the calculating of external orientation can be processed. Figure 5-7 shows the flow chart
92

of this processing.

Input ground coordinates of GCPs and their corresponding image coordinates

Set initial approximations of exterior elements

Calculate

and other coefficient

defined in Equation (3-19)

Compute the corrections of exterior elements

If these corrections are smaller than the threshold

No

Yes
yr

End

Figure 5-7 Flow chart of calculating exterior elements

The calculating results and errors of GCPs will be automatically saved and displayed in a pop-up window as shown in Figure 5-8. If large errors are found in the error list,

93

it means that wrong image coordinates may have been measured. Such image coordinates must be measured again and such mistakes can be corrected. Erroneous ground control coordinates could cause similar errors.

E x te r io r P a r a m e t e r s I n f o m a t io n

P o s itio n P ara m e ters: Xc = 8 2 .4 8 6 7 6 2 Vc = 4 8 .2 9 0 2 1 0 Zc = 5 .9 3 5 2 7 5 A n g u la r O r i e n t a t i o n s : Omega= 8 4 .5 6 4 2 5 7 P h i = 5 .8 9 7 4 7 5 Kappa= 2 .3 9 7 2 2 9 R o ta tio n M atrix : 0 .9 9 3 8 3 6 8 3 0 .1 061 594 1 -0 .0 4 160595 0 .0 9 0 3 6 8 0 8 0 .1 0 2 7 4 8 7 0 -0.99023421 E rror L is t: xp P o in t 4 0 3 .4 9 6 6 59 3 -4 9 6 7 3 5 0 .4 9 6 68 -7 0 .5 0 4 23 -5 1 .5 0 4 5 6 1 3 .4 9 6 64

0 .0 3 1 9 1 4 4 4 0.9 9 5 0 3 8 9 7 0.0 9 4 2 2 7 9 9

iiP -6 0 .7 2 7 -2 8 .7 2 7 2 7 7 .2 7 3 2 7 4 .2 7 3 -6 4 .7 2 7 1 9 2 .2 7 3

dxp -2 .1 3 5 1 .0 9 2 -1 .9 2 0 -0 .8 2 5 1 .8 7 1 1 .7 0 9

dyp 0 .0 8 7 -1 .0 9 7 -0 .2 7 7 0 .7 0 8 0 .0 2 0 0 .5 0 7

Figure 5-8 Results of external orientation

The external orientation program assists users when completing orientation operations. The results of this program are used as input data for 3D reconstruction.

94

5.4 Chapter Summary
Exterior elements determine the photograph position and angular orientation of image camera. The calculating process of exterior elements is called external orientation. Both GCPs and their corresponding image coordinates are required in external orientation. A new method for obtaining GCPs by using GIS and GPS technologies is proposed in Chapter 3. Sections 5.1 and 5.2 present two software programs used for realizing this method. The collected GCPs can be used in external orientation program. This program provides image coordinates measurement function and the computation of exterior elements. With known interior and exterior elements, ground coordinates of target objects are easy to calculate.

95

96

6 CASE STUDY OF 3D RECONSTRUCTION
This chapter describes the study of 3D reconstruction using photogrammetric methods. A software program is designed for this objective. It is a Microsoft Windows program coded by Visual C++. There are 3 modules named model management, object measurement and graphies editing in this program. The details of above modules are described in the following sections. Figure 6-1 shows the main interface of this program.

' !-Z: "zll

Photo 1

.Exit Measurement Mode: ;* F.r Single Point ; O PolyMne -----------O Poh^on & ^MeaSuremcirt Delete Ojbect ' F EpIpolarUnes t;

> '

Model

External Orientation ,

Figure 6-1 User interface of 3D reconstruction

97

6.1 Model Management Module
This module is used for initializing, editing and saving model data. The term model in this program means a set of data including names of three digital images, map name, interior elements, exterior elements and fundamental matrix of three digital images. All of these data elements are needed for computing space coordinates of target object, using the photogrammetric methods.

As shown in Figure 6-2, a pop up window shows the names of 3 digital images and associated map. These names can be specified either in corresponding editing window or a "File open" window as character strings. These strings include the file paths and file names.

M od el m a n a g e m e n t

r-

First Photo ' Second Photo

E:\thesis\question\1727_A.bmp E:\thesis\question\1727_C.bmp |j

^

. |E:\thesls\question\1727_D.bmp Third Photo 1 1 Map Name. ! : E:\thesis\question\thesis.map

V:^;i:'Savc&Exit;

Cancel

Figure 6-2 Model management window

An external orientation program loads the interior elements and computes the exterior

98

elements. The orientation results are stored in a Visual C++ structure. This structure is

called "PPF Struct" and is shown as follows:

typedef struct

{
CPoint StartPoint; CPoint EndPoint; }LineSegment; typedef struct

{
bool int double double double double double double double double double double double double double double double double double int int double double double double double bOrientation; PhotoID; dFocalLength; dxO; dyO; dKO; dK l; dK2; dK3; dPl; dP2; dXc; dYc; dZc; dOmega; dPhi; dKappa; RM[9]; FM[9]; nControlNumber; PointID[1000]; Xg[1000]; Yg[1000]; Zg[1000]; xp[1000]; yp[1000]; //X coordinates of photograph station //Y coordinates of photograph station //Z coordinates of photograph station //omega angular orientation //phi angular orientation //kappa angular orientation //rotation matrix //fundamental matrix //Amount of GCPs used in orientation // GCPs ID used in orientation // X ground coordinates o f GCPs / / Y ground coordinates of GCPs // Z ground coordinates of GCPs // X image coordinates of GCPs // Y image coordinates o f GCPs //the first epipolar line //the second epipolar line // asymmetrical distortion coefficient // orientation flag //photo ID //focal length //x coordinates of principal point //y coordinates of principal point // symmetrical distortion coefficient

LineSegment Epipolinel ; LineSegment Epipolinel; }PPF_Struct;

99

The model data set is automatically saved on a hard disk after the external orientation or model management operation. A C++ class defined by the following code is used for loading and saving the model data.

class CModelDoc : public CDocument

{
public:

CModelDocO;
public:

// constructor used by dynamic creation

DECLARE_DYNCREATE(CModelDoc) virtual void Serialize(CArchive& ar); virtual BOOL OnNewDocumentQ; // Implementation public: P P F S truct FirstPhoto; P P F S truct SecondPhoto; P P F S truct ThirdPhoto; B 00L 0penFlag[3]; BOOL EpipolarFlag[3]; CString FirstPhotoName; CString SecondPhotoName; CString ThirdPhotoName; CString MapName; CMapDoc Map; virtual -CModelDoc(); // Generated message map functions }; // overridden for document i/o

6.2 Object Measurement Module
With the model data mentioned in Section 6.1, the ground coordinates of the target object can be computed from the measured image coordinates. This module initially provides a measurement function that can be used for gaining image coordinates. As shown in Figure 6-3, there are four image windows in the program interface. Three
100

small image windows display the scaled reductions of the three digital images. Although every image can be activated, only one image is active at a given time. A green rectangle displayed in each of these windows illustrates the range of active image shown in the large image window. The large image window shows a part of active digital image in a 1:1 scale. A pink cross shows the position of this point. Sometimes, the target object does not have a point feature. As such, it is difficult to determine the position of homologous points. Fortunately, homologous points are located in corresponding epipolar lines as mentioned in Section 3.3.4. Epipolar lines can be computed and displayed on all image windows as an assist function since, in most cases, they have intersections with this kind of target object. This assist function can be enabled or disabled in this program. HMIHffBB

m

Measurement Mode:
C Single Point C Polyline Polygon

Measurement

Delete Ojbect

I

^ lEpipelar Une$

External Orientation

Figure 6-3 The interface of object measurement module.

101

The image coordinates of target point can be measured by setting three digital images active, sequentially, and moving the pink cross on the target image point. Then the ground coordinates of target object can be computed. If just two of these three images overlap the target object, the ground coordinates are computed using space intersection. If all of three images overlap target object, a bundle adjustment is employed for calculating the ground coordinates. The results of space intersection using first two images are employed as approximations.

6.3 Graphics Editing Module
Although the ground coordinates of the target object are measured, its feature code is still unknown. In addition, only the vertices of target object are obtained. The shape of target object cannot be drawn without specifying its feature code. This module is used for specifying the feature code of target object and providing simple functions to drawn objects and correcting measurement mistakes.

In this study, three kinds of feature primitives are defined to distinguish between different objects. The first kind of feature is a single point. All independent objects that have fixed shapes can use this kind of feature. The second kind of feature is a poly-line. All objects that have line shapes whether straight line or curve can use this kind of feature. The third kind of feature is a polygon. All objects that cover an area

102

can use this kind of feature. This module provides a simple function to specify the features of target object. As shown in Figure 6-3, the feature can be chosen in "Measurement Mode" box when program is working in measurement mode. Figure 6-4 shows the legend of these kinds of features.

Single point

Poly-line

Polygon

Figure 6-4 The legend of three kinds of features

The three kinds of features defined above definitely cannot describe all underground utilities. The objective is to use these kinds of features to save the vertices in different ways in order that these data can be subsequently edited in AutoCAD. Operators can set the other information based on corresponding digital image by using AutoCAD. There is no need to make more kinds of editing functions, as AutoCAD is a kind of powerful graphics editing software with lots of special functions. However, a delete function is needed, as the measurement mistakes can be found only in this program. If an incorrect operation is found, such as measuring a wrong image point, the delete operation can erase it. As shown in Figure 6-5, the program will work in deleting mode by pressing "Delete Object" button. The graphics of interest object can be chosen and drawn with yellow colour. This chosen object may be erased if desired. By means of this function, the correctness of mapping objects is improved:

103

i
m
M easurem ent Mode Single Point Polyline f» Polygon M easurem ent Delete Ojbect F" È pipolar Ù n e ^ External Orientation

Figure 6-5 Delete a selected object

6.4 Chapter Summary
This chapter presents the solution of 3D reconstruction. A software program that consists of three modules is made for completing 3D reconstruction. The model management module is used for managing model data. This is a data preparation module that can be used to load, modify and save digital images, orientation data and graphics data. The object measurement module is used for measuring image coordinates from digital images and computing ground coordinates based on orientation data loaded in the management module. The graphics editing module is used for organizing measured ground coordinates by specifying different object

104

features, displaying these objects on the corresponding digital image and deleting objects that contain mistakes. The 3D construction results are saved in graphics data file. If transformed to DXF format, they can be edited in AutoCAD.

105

106

7 CONCLUSIONS AND RECOMMENDATIONS
This study presents a photogrammetric solution to complete 3D reconstruction of underground utilities. In this chapter, the major achievements of the developed approach are summarized in Section 7.1. Some limitations are discussed in Section 7.2. In Section 7.3, conclusions are drawn based on previous discussions. Finally, recommendations for future research are given in Section 7.4.

7.1 Summary of the Study
Photogrammetry is a technology for obtaining reliable information about physical objects and the environment through processes of recording, measuring, and interpreting photographic images and phenomena. In this research work, a photogrammetric solution based on a set of software programs is developed to realize 3D reconstruction of underground utilities. This set of software programs can be divided into three parts, which are categorized as (I) camera calibrations programs, (2) GCPs collection and external orientation programs, and (3) 3D reconstruction program.

A commercial digital camera is employed to provide digital photographic images. Its interior elements and distortion coefficients are computed by using camera calibration programs. These programs consist of two programs. The first program is developed

107

for obtaining image coordinates of corresponding GCPs where a special pattern is designed for providing 3D space coordinates. Different from traditional method, image matching and LSM techniques are employed in this program to detect such image coordinates based on the designed pattern. Image matching is used for automatically detecting the approximations of corresponding GCPs. These approximations help LSM detect accurate image positions of GCPs. The second program employs a DLT method to complete computation of the interior elements and lens distortion coefficients. These parameters can be saved in a LUT indexed by the corresponding actual focal length provided by the current digital camera. With this LUT, digital images captured under different focal lengths can be processed together. The distributions of distortions and resection errors are also displayed on the screen that help the operator check the results.

In order to obtain space position of the target object, the exterior elements of photographic images must be calculated. A group of GCPs appearing on digital images is needed in this calculating process. A new GCPs collection method that employs both GIS and GPS techniques is proposed in this study and realized in the GCPs collection software program. A GIS data transformation program is developed for transforming DXF format GIS data to our own graphics data format. An external orientation program provides not only a graphical interface to assist the operator when measuring image coordinates of corresponding GCPs from digital photographic images, but also the computation of the exterior elements.

108

With interior elements, lens distortion coefficient and exterior elements, space coordinates of the target object can be solved if corresponding image coordinates of at least 2 digital images are measured. 3D reconstruction program is developed for loading digital images, managing corresponded parameters including interior and exterior element and distortion coefficients, measuring image coordinates of target object, calculating space coordinates and editing these space coordinates. The positions and shapes of underground utilities can be obtained in this program.

7.2 Limitations of the Study
Although the objective of this study is realized, there are still some limitations which are summarized as follows.

Firstly, the digital camera must provide the actual focal length in the camera calibration program because this value is used as an index in the LUT. If not, the camera can only work using a nominal focal length.

Secondly, the camera calibration program requires a good distribution of GCPs shown on the digital image because the calibration results are very sensitive to this distribution. Furthermore, in order to use the automatic combination function, the 0 and w angular orientations should be very small.

109

Thirdly, at least 3 GCPs are needed in the measurement area. Otherwise, external orientation cannot be easily processed. These GCPs must not be distributed on or near a straight line. In this case, erroneous results may be computed during external orientation.

Finally, in order to make the operation of 3D reconstmction straightforward, just three kinds of features, which are defined as single point, poly-line and polygon, are provided. The actual features of measured objects should be renamed, for example, a gas pipe, in AutoCAD program.

7.3 Conclusions
Nowadays, space information of underground utilities can be obtained by using different methods as mentioned in Chapter 2. The common disadvantages of all these methods are the slow speed, high cost and the fact that they could not be used during the construction of the underground utilities. Based on digital images, a photogrammetric method provides a high accuracy and fast 3D reconstruction tool for mapping exposed underground utilities. This thesis has presented a solution of obtaining space information of underground utilities using photogrammetric methods. Several findings and conclusions are summarized below:

no

First, Commercial digital cameras can be used for providing original digital images. An automatic detection program provides an automatic method to obtained image coordinates of GCPs. Based on these coordinates, the camera calibration program computes not only the interior elements of digital cameras but also the lens distortion coefficients. The interior elements include the focal length and the position of the principal point. These elements are the basic parameters of camera interior geometry in photogrammetry. After correcting the lens distortion, the accuracy of these images satisfies photogrammetric requirement. A LUT lets digital cameras work within different focal lengths if the corresponding actual focal lengths are provided.

Second, Collecting GCPs from an existing GIS data set by using GPS receiver is effective. GCPs collection programs provide corresponding functions and make it easy to complete this collection. Based on the collected GCPs, the exterior elements of digital images can be calculated in an external orientation program.

Third, Spatial information of target object can be obtained in the 3D reconstruction program. In this program, space eoordinates are computed after corresponding image coordinates are measured where the feature of target object is also determined. These data are also drawn on the images that help operators easily check incorrect operations. A deleting function can assist in finding and deleting erroneous data. These space data are saved in a graphics file and can be

in

transformed to a DXF file for further editing in AutoCAD software.

7.4 Recommendations for Future Research
Based on the proposed methodology and current research works, recommendations can be summarized in order to extend the study in the future. First, Digital image matching technique can be used in 3D reconstruction. This technique will help operators easily find homologous points within digital images. However, based on the methodology of image matching, this locating task will not be successful all of the time. Hence, it should be used as an aiding function. some

Second, Automatic line and edge detection is recommended in 3D reconstruction. Most underground utilities have line or edge features. It is easy to extract such features from digital images by using automatic line and edge detection method. Operators simply have to choose target objects from the detection results.

Third A transformation software program should be developed for transforming graphics files to DXF format. Without this program, the graphics file cannot be edited again in AutoCAD.

112

BIBLIOGRAPHY
Abdel-Aziz, Y.I. and H.M. Karara, 1971. Direct linear transformation into object space coordinates in close-range photogrammetry, Proceedings of the Symposium on Close-Range Photogrammetry, Urbana-Champaign, IL, January 1971, pp. 1-18. Ahmed, M. and A. Farag, 2001. Non-metric calibration of camera lens distortion. Proceedings of the International Conference on Image Processing, Thessaloniki, Greece, September 2 (2001), pp. 157-160. Ahn, S.J., W. Rauh, and S. I. Kim, 2001. Circular coded target for automation of optical 3D-measurement and camera calibration. International Journal of Pattern Recognition and Artificial Intelligence, 15(6): 905-919. Anspach, J. H., and Stanley E. Wilson, 1994. A case study of an underground 138 KV transmission line design utilizing subsurface utility engineering. Proceedings of the 56th Annual American Power Conference, 56:(1): 37-141. Anspach, J. H, 1997. Design and Construction risk management for existing utilities. Construction Congress V , pp. 301-306. Anspach, J. H, 1995. Subsurface utility engineering: upgrading the quality of utility information. International Conference on Advances in Underground Pipeline Engineering - Proceedings, pp. 813-824. Anspach, J. H., 1996. Integrating Technology into Subsurface Utility Engineering Projects, Pipeline & Gas Journal, 223(6):. 41-42. AutoCAD DXF Reference, Autodesk, Inc, 2005. Bacakoglu, H. and S. Kamel, 1997. A tree-step camera calibration method, IEEE Transactions on Instrumentation and Measurement, 46 (5): 1165-1172. Baillard, C., and O. Dissard, 2000. A stereo matching algorithm for urban digital elevation models, Photogrammetric Engineering & Remote Sensing, 66(9): 1119-1128. Bragin, S.I., l.V. Bragin, V.P. Sgibnev, S.E. Chadov, V.l. Gusevsky, Y.B. Bragina, A.A. Morozov, and V.V. Tsutskov, 2000. Remote detection of objects in soil using a microwave and IR scanner, Internationl Conference on Microwave and Millimeter Wave Technology Proceedings, pp. 607-610.
113

Brown, D.C., 1971. Close-range camera calibration, Photogrammetric Engineering, 37(8): 855-866. Caldecott, R., M. Poirier, D. Scofea, D.E. Svoboda, and A.J. Terzuoli, 1988. Underground mapping of utility lines using impulse radar, lEE Proceedings, 135 (4): 343-353. Chua, C., Y. Ho, and Y. Liang, 2000. Rejection of mismatched correspondences along the affine epipolar line. Image and Vision Computing, Vol. 18, pp. 445-462. Chen, Q., H. Wu, and T. Wada, 2004. Camera calibration with two arbitrary coplanar circles, Proc. European Conf. Computer Vision, Vol. 3, pp. 521-532. DeAgapito, L., E. Hayman, and I.D. Reid, 2001. Self-calibration of rotating and zooming cameras. International Journal of Computer Vision, 45 (2): 107-- 127. Faugeras, O.D., L. Quan, and P. Sturm, 2000. Self-calibration of a ID projective camera and its application to the self-calibration of a 2D projective camera, IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (10): 1179-1185. Fitzgibbon, A. W., 2001. Simultaneous linear estimation of multiple view geometry and lens distortion. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'Ol), Kauai, HI, December 1 (2001), pp. 125-132. Gonzalez, R. C. and R. E. Woods, 2001. Digital Image Processing, Prentice-Hall, Upper Saddle River, New Jersey, 793p. Gulch, E., 1991. Results of test on image matching of ISPRS WG 111/4, ISPRS Journal of Photogrammetty and Remote Sensing, 46(1): 1-8. Handlon, B., S.J. Lorenc, L. Bemold, and G. Lee, 2000. Tool integrated electromagnetic pulse induction technology to locate buried utilities, ISCAS IEEE International Symposium on Circuits and Systems, May 28-31, Geneva, Swizerland, pp. 529-532. Hansen, T., M. Oristaglio, D. Miller, M. Bums, A. Derubeis, P. Albats, R. Casadonte, R. Deming, R. Birken, and J. Haldorsen, 2005. Effectient large-scale underground utility mapping wit a multi-channel ground-penetrating radar system, T1RÏ.: http://www.ursi.org/Proceedings/ProcGA02/paDers/pl 137.pdf. (last accessed: July 30, 2005). Heikkila, J. 2000. Geomatric camera calibration using circular control points, IEEE
114

Trans. Pattern Analysis and Machine Intelligence, 22(10); 1066-1077. Hu, R. and Q. Ji, 2001. Camera self-calibration from ellipse correspondences, Proc. IEEE International Conference, Robotics and Automation, Vol. 3, pp. 2191-2196. Jenus, J. and K. Bakhtar, EarthRadar application for detecting/mapping underground utilities, 2005. URL: httD://www.federallabs.org/utilities/Presentations/EarthRadar for Underground Utilities Mapping Bakhtar.pdf. (last accessed: July 30, 2005). Jeong, H. S., D. M. Abraham, and J. J. Lew, 2004. Evaluation of an emerging market in subsurface utility engineering. Journal of Construction Engineering and Management, ASCE, March/April, pp. 225-234. Jokinen, O., 1998. Area-based matching for simultaneous registration of multiple 3-D profile maps. Computer Vision and Image Understanding, 71(3): 431-447. Kim, J., R Gurdjos, and I. Kweon, 2005. Geometric and algebraic constraints of projected concentric circles and their application to camera calibration, IEEE Transaction on Pattern Analysis and Machine Intelligence, 27(4): 637-642. Kim, J.S., H.W. Kim, and I.S. Kweon, 2002. A camera calibration method using concentric circles for vision applications, Proc. Asian Conf, Computer Vision, Vol. 2, pp. 515-520. Lavest, J.M., M. Viala, and M. Dhome, 1998. Do we really need an accurate calibration pattern to achieve a reliable camera calibration? Proceedings of the European Conference on Computer Vision (ECCV'98), Freiburg, Germany, pp. 158-174. Lenz, R.K., and R.Y. Tsai, 1988. Techniques for calibration of the scale factor and image center for high accuracy 3D machine vision metrology, IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(5): 713-- 720. Lew, J. J., 1996. Subsurface utility engineering: An initial step in project development, ASC Proceedings of the 32nd Annual Conference, College Station, Texas, pp. 217-222. Li, M. and J.M. Lavest, 1996. Some aspects of zoom lens camera calibration, IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(11): 1105-1110. Liang, T., and C. Heipke, 1996. Automatic relative orientation of aerial images, Photogrammetric Engineering & Remote Sensing, 62(1): 47-55.

115

Lucchese, L., 2005. Geometric calibration of digital cameras through multi-view rectification, Image and Vision Computing, Vol 23, pp. 517-539. Lucchese, L., 2003. Estimating the pose and focal length of a camera from the perspective projection of a planar calibration plate. Proceedings of the Fifth lASTED International Conference on Signal and Image Processing (SIP 2003), Honolulu, Hawaii, August 2003, pp.201-206. Lucchese, L., 2003. Digital camera calibration. Part I: internal and external geometry from multi-view alignment. Proceedings o f the 3rd lASTED International Conference on Visualization, Imaging, and Image Processing (VHP 2003), Benalma'dena, Spain, September III (2003), pp. 1061-1065. Lue, Y. and K. Nova, 1991. Recursive grid- dynamic window matching for automatic DEM generation, GIS/LIS ACSM-ASPRS Fall Convention, 1991, pp. A254-A260. Luong, Q.T., and O.D. Faugeras, 1997. Self-calibration of a moving camera from point correspondences and fundamental matrices. International Journal of Computer Vision, 22 (3): 261-- 289. Madani, E. M., J. S. Bethel, and J.D. McGlone, 2001. Introduction to Modern Photogrammetry, John Wiley & Sons, New York, NY, 479 p. Malis, E. and R. Cipolla, 2002. Camera self-calibration from unknown planar structures enforcing the multiview constraints between collineations, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(9); 1268-1272. Maybank, S. and O.D. Faugeras, 1992. A theory of self calibration of a moving camera. International Journal of Computer Vision, 8 (2): 123-151. Mcintosh, J. H. and K. M. Mutch, 1988. Matching straight lines. Computer Vision Graph Image Processing 43, pp. 368-408. Medioni, G, and R. Nevatia, 1984. Matching image using linear features, IEEE Transactions Pattern and Mach Intell, Vol. 6, pp 675-785. Medinoi, G. and R. Nevatia, 1985. Segment based stereo matching. Computer Vision Image Processing, Vol. 31, pp. 2-18. Meng, X. and Z. Hu, 2003. A new easy camera calibration technique based on circular points. Pattern Recognition, 36(5): 1155-1164. Mount, D. M., N. S. Netanyahu, and J. L. Moigne, 1999. Efficient algorithm for
116

robust feature matching, Pattern Recognition, Vol. 32, pp. 17-38. O'Neill, M. and M. Denos, 1996. Automated system for coarse-to-fine pyramidal area correlation stereo matching. Image and Vision Computing, 14(3): 225-236. Otto, G. P., 1988. Rectification of SPOT data for stereo image-matching. International Archives of Photogrammetry and Remote Sensing, 27(B3): 635-645. Otto, G, and T. Chau, 1989. A region growing algorithm for matching of terrain image. Image and Vision Computing, Vol. 7, pp. 83-94. Pollefeys, M., R. Koch, and L. Van Gool, 1998. Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters, InternationalJournal o f Computer Vision, 32 (1): 7-25. Sturm, P.P. and S.J. Maybank, 1999. On plane-based camera calibration: a general algorithm, singularities, applications. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'99), Fort Collins, CO, June 1 (1999), pp. 23-25. Sturm, P.P., 1997. Self-calibration of a moving zoom-lens camera by precalibration. Image and Vision Computing, Vol. 15, pp. 583-589. Swaminathan, R. and S.K. Nayar, 2002. Nonmetric calibration of wide-angle lenses and polycameras, IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (10): 1172-1178. Tsai, R., 1987. A versatile camera calibration technique for high accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses, IEEE J. Robotics and Automation, 3(4): 323-344. Tulloch, M, 2005. Proposed solution for mapping underground utilities for buried asset management. Internal Report, 37p. Tulloch, M and W. Hu, 2005. A proposed solution for mapping underground utilities for buried asset management, Ryerson Graduate Student Research Article Competition 2005, 4p. Valkenburg, R.J., 1996. Camera calibration using multiple references. In Image and Vision Computing New Zealand, pp. 61-66. Veneziano, D., S. Hallmark, and R. Souleyrette, 2002. Comparison of LIDAR and conventional mapping methods for highway corridors studies. Final Report of
117

Centerfor Transportation Research and Education, October, 2002, 58p. Wang, Z., 1990. Principle of Photogrammetry: with Remote Sensing, Press of Wuhan Technical University of Surveying and Mapping and Publishing House of Surveying and Mapping, Wuhan, China, 575p. Weng, J., P. Cohen, and M. Hemiou, 1992. Camera calibration with distortion models and accuracy evaluation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 14 (10): 965-980. Wolf, P. R. and B. A. Dewitt, 2000. Elements of Photogrammetry: with Application in GIS, 3^ edition, McGraw-Hill, New York, 608p. Wong, K.Y., P.R.S. Mendonca, and R. Cipolla, 2003. Camera calibration from surfaces of revolution, IEEE Transactions on Pattern Analysis and Machine Intelligence, 25 (2): 147-161. Wu, Y., H. Zhu, Z. Hu, and F. Wu, 2004. Camera calibration from quasi-affine invariance of two parallel circles, Proc. European Conf Computer Vision, Vol. 1, pp. 190-202. Xie, P., 2004. A Web-based 3D Visualization Prototype System for High-resolution Satellite Colour Stereo Images. M.Sc.E. thesis. University of New Brunswick pp.31-32 Yang, C., F. Sun, and Z. Hu, 2000. Planar conic based camera Calibration, Proc Int'l Conf Pattern Recognition, Vol. 1, pp. 555-558. Zembillas, N. M., 2003. Subsurface utility engineering: A technology-driven process that results in increased safety, fewer claims, and lower costs. Pipelines, Vol. 2, pp. 1422-1428. Zhang, Z., 1999. Flexible camera calibration by viewing a plane from unknown orientations. Proceedings of International Conference on Computer Vision (ICCV'99), Corfu, Greece, September 1999, pp. 666-673. Zhang, Z., R. Deriche, O. Faugeras, and Q. Luong, 1995. A robust technique for matching the uncalibrated images through the recovery of the unknown epipolar geometry. Artificial Intelligence, Vol. 78, pp. 87-119. Zhang, Z., J. Zhang, M. Liao, and L. Zhang, 2000. Automatic registration of multi-source imagery based on global image matching, Photogrammetric Engineering & Remote Sensing, 66 (5): 625-629.

118


