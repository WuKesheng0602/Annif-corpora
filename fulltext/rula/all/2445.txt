Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2013

Designing and Evaluating an Interface for the Composition of Vibro-Tactile Patterns Using Gestures
Sai Chaitanya Cherukumilli
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Computer Sciences Commons Recommended Citation
Cherukumilli, Sai Chaitanya, "Designing and Evaluating an Interface for the Composition of Vibro-Tactile Patterns Using Gestures" (2013). Theses and dissertations. Paper 2065.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

DESIGNING AND EVALUATING AN INTERFACE FOR THE COMPOSITION OF VIBRO-TACTILE PATTERNS USING GESTURES

by Sai Chaitanya Cherukumilli B.Eng, Ryerson University, Toronto, Ontario, 2010

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science Toronto, Ontario, Canada, 2013 Â©Sai Chaitanya Cherukumilli, 2013

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS
I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

DESIGNING AND EVALUATING AN INTERFACE FOR THE COMPOSITION OF VIBROTACTILE PATTERNS USING GESTURES
Sai Chaitanya Cherukumilli

MSc, Computer Science, Ryerson University, 2013

ABSTRACT
Human-computer interaction systems have been providing new ways for amateurs to compose music using traditional computer peripherals as well as gesture interfaces. Vibro-tactile patterns, which are a vibrational art form similar to auditory music, can also be composed using human-computer interfaces. This thesis discusses the gesture interface system called the Vibro-Motion, which facilitates the composition of vibro-tactile patterns in real-time on an existing tactile sensory substitution system called the Emoti-Chair. The Vibro-Motion allows users to control the pitch, magnitude of the vibration as well as the position of the vibration. A usability evaluation of Vibro-Motion system showed it to be intuitive, comfortable and enjoyable for the participants.

iii

ACKNOWLEDGEMENTS
I would like to thank many people, without whom I would not have been able to complete the research and implementation of my thesis over the course of my time at Ryerson University. First and foremost I would like to express my sincere gratitude and appreciation to my thesis supervisor Deborah Fels who supported and guided me through my research and studies at Ryerson University. I would like to thank the members of my thesis committee Frank Russo, Eric Harley and Tim McInerney for their time and effort in reviewing my thesis and providing valuable feedback. I would also like to thank the former members of my thesis committee, Alex Ferworn and Sophie Quigley for their time and effort in reviewing my thesis and providing valuable feedback. I would like to thank my mother, Rajyalakshmi Addepalli and my brother Raghu Cherukumilli; without their constant positive, emotional, moral, and loving support I would not be where I am today. Finally, I would like to thank my colleagues Lisa Copeland and Paul Church for creating motivational wallpapers that kept me company during the long hours I spent working in the lab and providing me the belief that "I can quantify my results in this experiment using two siege tanks and a bunker. Not the one with the upgrade, the regular bunker". I would like to thank Jorge Mori, Leshanne Pretty for being very patient and helpful while writing my thesis. And I would also like to thank all my friends and colleagues within the Inclusive Design Media Center (IMDC) at Ryerson University, who provided such an amazing environment to work in, showed interest as my thesis work progressed, and were constantly by my side to offer help.

iv

Table of Contents
List of Tables ................................................................................................................................................. x

List of Figures .............................................................................................................................................. xii

1

Introduction .......................................................................................................................................... 1

1.1

Thesis statement ........................................................................................................................... 4

1.2

Contributions of the Thesis ........................................................................................................... 6

1.3

Thesis Outline: .............................................................................................................................. 8

2

Literature Review .................................................................................................................................. 9

2.1.1

Music, its impact and its creation ....................................................................................... 10

2.1.2

Music, Gesture and Emotion............................................................................................... 11

2.1.3

Human-Computer Musical Interfaces (HMCI) .................................................................... 14

2.1.4

Gesture Interfaces............................................................................................................... 22

2.1.5

Gesture Musical Instruments: ............................................................................................. 29

2.1.6

Physiology of the tactile sense ............................................................................................ 33

2.1.7

Sensory Substitution ........................................................................................................... 35

2.1.8

Tactile systems and Vibro-tactile Feedback in Human Computer Interfaces ..................... 37 v

2.1.9

Conclusion ........................................................................................................................... 44

3

System Design, Implementation and Evaluation Methodology ......................................................... 46

3.1.1

Overview of the Vibro-Motion System design .................................................................... 46

3.2

Program flow:.............................................................................................................................. 47

3.2.1

Calibration: .......................................................................................................................... 48

3.2.2

Vibro-Tactile Pattern Composition: .................................................................................... 51

3.2.3

Gesture Zones ..................................................................................................................... 52

3.3

Vibro-Motion Equipment: ........................................................................................................... 56

3.3.2

Computer System specifications: ........................................................................................ 57

3.4

Software: ..................................................................................................................................... 58

3.4.1

Vibro-motion software........................................................................................................ 58

3.4.2

Kinect SDK: .......................................................................................................................... 58

3.4.3

Ventuz OSC package: .......................................................................................................... 59

3.5

Process / Code Structure / Design: ............................................................................................. 59

3.5.1

Frequency: .......................................................................................................................... 59

3.5.2

Amplitude:........................................................................................................................... 61 vi

3.5.3

Position: .............................................................................................................................. 62

3.5.4

Vibrational window design in the first design strategy: ..................................................... 63

3.5.5

Vibrational window design using the second design strategy: ........................................... 68

3.5.6

Communication between C# and Max/MSP: ...................................................................... 69

3.6

Final Version of the Vibro-Motion: ............................................................................................. 69

3.7

Vibro-Motion Study: ................................................................................................................... 70

3.8

Research Questions: ................................................................................................................... 70

3.9

Mimicry ....................................................................................................................................... 71

3.10

The Vibro-Motion Pilot Study: .................................................................................................... 73

3.11

Vibro-Motion Evaluation Study:.................................................................................................. 74

3.12

Study Procedure.......................................................................................................................... 76

3.12.1

Practice Mode ..................................................................................................................... 77

3.12.2

Phase 1 ................................................................................................................................ 78

3.12.3

Phase 2: ............................................................................................................................... 80

3.12.4

Phase 3: ............................................................................................................................... 81

3.12.5

Analysis of Recorded Data: ................................................................................................. 82 vii

3.12.6

Statistical analyses of the user data.................................................................................... 88

4

Evaluation ........................................................................................................................................... 90

4.1

Results ......................................................................................................................................... 90

4.1.1

Frequency Error Analysis for Phase 1 Stimuli: .................................................................... 90

4.1.2

Frequency Error Analysis for Phase 2 Stimuli: .................................................................... 92

4.1.3

Position Error Analysis for Phase 1 Stimuli: ........................................................................ 93

4.1.4

Position Error Analysis for Phase 2 Stimuli: ........................................................................ 96

4.1.5

Strength Error Analysis for Phase 1 Stimuli: ....................................................................... 97

4.1.6

Strength Error Analysis for Phase 2 Stimuli: ....................................................................... 98

4.1.7

Co-variate Analysis: ............................................................................................................. 99

4.1.8

Questionnaire data: .......................................................................................................... 100

4.1.9

Open Ended Questions: .................................................................................................... 102

4.2

Discussion.................................................................................................................................. 106

4.2.1

Research Question 1: Influences over Accuracy: .............................................................. 108

4.2.2

Research Question 2: Impact of Gesture Gaming Experience on Accuracy ..................... 123

4.2.3

Research Question 2: Impact of Music Experience on Accuracy ...................................... 125 viii

4.2.4

Research Question 3: Participant Perceptions of controllability ...................................... 127

4.2.5

Research 4: User Experience with the Vibro-Motion system: .......................................... 134

4.3

Limitations: ............................................................................................................................... 137

4.3.1

Kinect's technical issues: ................................................................................................... 137

4.3.2

Lag time: ............................................................................................................................ 138

4.3.3

Order of the study stimuli: ................................................................................................ 139

4.3.4

Limited Participants: ......................................................................................................... 140

4.3.1

Analysis of total number of attempts: .............................................................................. 140

4.3.2

Quality of the questionnaire questions: ........................................................................... 140

5

Conclusions and Future Work ........................................................................................................... 142

5.1

Future work ............................................................................................................................... 146

References ................................................................................................................................................ 210

ix

List of Tables
Table 1: Frequency zones and corresponding frequencies. ....................................................................... 60

Table 2: Vibro-tactile notes and corresponding frequencies developed by Branje & Fels (2012). ............ 74

Table 3: The parameters used for the vibrations in phase 1 stimuli. ......................................................... 79

Table 4: The parameters used for the vibrations in phase 2 stimuli. ......................................................... 81

Table 5: Pairwise t-tests for significant phase 1 frequency pairs ............................................................... 90

Table 6: Pairwise t-tests for significant phase 2 frequency pairs ............................................................... 93

Table 7: Pairwise t-tests for significant phase 1 position pairs. .................................................................. 94

Table 8: Pairwise t-tests for significant phase 2 position pairs. .................................................................. 96

Table 9: Pairwise t-tests for significant phase 2 strength pairs. ................................................................. 98

Table 10: Chi-Square results for Questionnaire answers. ........................................................................ 100

Table 11: Themes and definitions used for thematic analysis. ................................................................. 103

Table 12: Number of occurrences of each theme in all of the qualitative questions. ............................. 104

Table 13: The parameters used for the vibrations in phase 1 stimuli. ..................................................... 106

Table 14: The parameters used for the vibrations in phase 2 stimuli. ..................................................... 107

Table 15: Number of occurrences of each theme in all of the qualitative questions. ............................. 135 x

Table 16: An example of VTC class variables for phase 1 stimuli patterns. .............................................. 190

Table 17: Data extracted from the sample line in Figure 57. ................................................................... 194

Table 18: An example of a vibrational window strength allocation using the second design strategy ... 203

Table 19: An example of channel strength allocation. ............................................................................. 205

xi

List of Figures
Figure 1: Diagram of a One-to-One Mapping. ............................................................................................ 19

Figure 2: Diagram of a Many-to-One Mapping (Convergent Mapping). .................................................... 19

Figure 3: Diagram of a Many-to-Many Mapping. ....................................................................................... 19

Figure 4: Diagram of a One to Many Mapping (Divergent Mapping). ........................................................ 19

Figure 5: Image stream seen from the VGA camera. .................................................................................. 21

Figure 6: The depth stream of the same image appearing in Figure 5. ...................................................... 21

Figure 7: The Microsoft Kinect Sensor. ....................................................................................................... 21

Figure 8: The Nyko Zoom Add-on. .............................................................................................................. 22

Figure 9: The Teleharmonium (Cahill, 1897)............................................................................................... 29

Figure 10: The Electronic Sackbut (Young, 1999) ....................................................................................... 29

Figure 11: The Theremin (Theremin, 1919) ................................................................................................ 30

Figure 12: The Brain Opera (Machover, 1996) ........................................................................................... 30

Figure 13: The Digital Baton (Paradiso, 2002) ............................................................................................ 30

Figure 14: The Viblotar (Marshall & Wanderley ,2006) .............................................................................. 40

Figure 15: Dayton Audio Sound Exciters (Dayton, n.d.).............................................................................. 41 xii

Figure 16: The C2 Tactor (C2 Tactor, n.d.). ................................................................................................. 41

Figure 17: The Emoti-Chair with the voice coils exposed. .......................................................................... 42

Figure 18: Bark Scale. .................................................................................................................................. 43

Figure 19: The Pyramid Amplifier (Pyramid, 2010). .................................................................................... 43

Figure 20: The PreSonus Firepod (PreSonus, 2010). ................................................................................... 44

Figure 21: The system design diagram. ...................................................................................................... 47

Figure 22: The calibration window demonstrating a user calibrating their left hand. ............................... 49

Figure 23: The calibration window while a user is calibrating their hand distance furthest from the Kinect sensor. ......................................................................................................................................................... 50

Figure 24: The calibration window while a user is calibrating their hand distance closest to the Kinect sensor. ......................................................................................................................................................... 50

Figure 25: The gesture window as it is being used by a user. The operating range is the area bounded by the operating range markers, i.e. the square brackets. ............................................................................. 54

Figure 26: The Emoti-Chair emits a weak (small white circle) high-frequency vibration (thin high green circle) in a medium-low position (position of white circle) ........................................................................ 54

Figure 27: The Emoti-Chair emits a strong (large white circle) low-frequency vibration (low thick green circle) towards the top of the chair (position of the white circle). ............................................................. 55

xiii

Figure 28: The Emoti-Chair emits a very weak (tiny white circle) medium-frequency vibration (medium green circle in medium frequency zone) in the middle of the chair (position of white circle) .................. 55

Figure 29: The gesture window, indicating the three frequency zones and vibration position on the right side of the window...................................................................................................................................... 60

Figure 30: Formula for calculating frequencyOut for all Zones. ................................................................. 61

Figure 31: Formula for calculating the vibrational strength of the signal based on the right hand's distance from the Kinect sensor. ................................................................................................................ 62

Figure 32: Vibrational strength adjustment................................................................................................ 62

Figure 33: Example of Case 1. Vibrational window lies entirely in Channel 1. ........................................... 64

Figure 34: Example of Case 2, where the vibrational window lies entirely in Channel 8. .......................... 65

Figure 35: Example of Case 3 where the vibrational window lies across two channels. ............................ 66

Figure 36: Example of Case 4 where the vibrational window lies exactly on one channel ........................ 67

Figure 37: Vibrational Window with the three sections. ............................................................................ 68

Figure 38: One complete test case attempt by a participant, with their sudden dips in right hand activity movement as labeled. In this example, dip 1 corresponds to the participant clicking "Replay"; dip 2 corresponds to the participant clicking "Final Attempt"; dip 3 corresponds to the participant clicking "Next". ........................................................................................................................................................ 83

xiv

Figure 39: One complete test case attempt by a participant in the study. Multiple attempts for the same stimulus are highlighted. ............................................................................................................................ 83

Figure 40: Tracked user data showing the vibrational strength, right hand position, and frequency. ...... 85

Figure 41: Expected vibrational input derived from the stimuli. ................................................................ 85

Figure 42: The Error areas of the stimuli .................................................................................................... 85

Figure 43: Formula for the Error Area. ....................................................................................................... 86

Figure 44: Number of occurrences for each theme. ................................................................................. 105

Figure 45: Phase 1 frequency error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph. ............................................................................. 111

Figure 46: Phase 1 position error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph. ................................................................................. 113

Figure 47: Phase 1 strength error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph. ................................................................................. 114

Figure 48: Phase 2 frequency error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph................................................................................ 118

Figure 49: Phase 2 position error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph................................................................................ 120

xv

Figure 50: Phase 2 vibrational strength error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph. ................................................................ 122

Figure 51: Phase 1 frequency error graph for participants with and without gesture gaming experience. The details for the parameter that changes in between the stimuli are shown underneath the graph.. 124

Figure 52: Phase 2 frequency error graph for participants with and without musical experience. The details for the parameter that changes during the stimuli are shown underneath the graph. ............... 126

Figure 53: The structure of the Variable Test Case Class. ......................................................................... 189

Figure 54: The formulas used to calculate the right hand coordinate. .................................................... 191

Figure 55: The process of calculating the rate of change variables for frequency, right hand coordinate and vibrational strength. .......................................................................................................................... 192

Figure 56: A flowchart illustrating the way in which the four timers interact with one another in the system. ...................................................................................................................................................... 193

Figure 57: A sample of the recorded data from a participant in the study. This data is parsed to gather the above information. ............................................................................................................................. 194

Figure 58: Formula for calculating the mid-point location of the vibrational window. ........................... 197

Figure 59: The Pseudocode that handles the occurrence of Case 1 and Case 2 vibrational window placements. ............................................................................................................................................... 197

Figure 60: Pseudocode for calculating the channel strengths for Case 3. ................................................ 198

xvi

Figure 61: Initialization of Vibrational Window bounds and strength...................................................... 200

Figure 62: Pseudocode for calculating the channel amplitude if the there is an occurrence of Case 1 or Case 2. ....................................................................................................................................................... 200

Figure 63: Pseudocode for calculating the channel amplitudes for Case 3. ............................................. 201

Figure 64: The pseudocode for channel amplitude normalization. .......................................................... 202

Figure 65: An example of the Vibrational Window designed using the second design strategy. ............ 205

Figure 66: An example of the OSC message to transmit frequency to the MAX/MSP patch. .................. 206

Figure 67: The MAX/MSP patch ................................................................................................................ 208

xvii

List of Appendices
Appendix A ................................................................................................................................................ 152

Ethics Approval letter............................................................................................................................ 152

Appendix B ................................................................................................................................................ 156

Consent Form ........................................................................................................................................ 156

Appendix C ................................................................................................................................................ 165

Questionnaire ....................................................................................................................................... 165

Appendix D ................................................................................................................................................ 178

Source Code: ......................................................................................................................................... 178

Appendix E ................................................................................................................................................ 179

List of constants used in the program .................................................................................................. 179

Appendix F ................................................................................................................................................ 187

Vibro-Motion Timers:............................................................................................................................ 187

The Recording timer: ........................................................................................................................ 187

The Countdown Timer: ..................................................................................................................... 188

The Muter Timer: .............................................................................................................................. 188 xviii

The Variable Test Case (VTC) Timer: ................................................................................................. 189

Matlab Analysis of Kinect's Tracked Data ............................................................................................. 194

Sample line of tracked data that's stored in the recorded files ....................................................... 194

The code process for calculating the vibrational strengths for the first design strategy: .................... 196

The code process for calculating the vibrational strengths for the second design strategy: ............... 199

Communication between C# and Max/MSP:........................................................................................ 206

MAX/MSP Patch: ............................................................................................................................... 207

xix

1 Introduction
Music has the power to reach out to people and can cause physical reactions such as swaying, moving and dancing, and/or tactile sensations such as feeling `pounding in the chest' or vibrations on the skin. It can also evoke emotions (Barbre et al. 1999). Narrow definitions of music consider it as an art form based on acoustics (Nattiez, 1990). The acoustic structure of music consists of tones, sounds created by people, duration, energy or amplitude, pitch or the frequency, rhythm and timbre. However, music can also be conveyed non-acoustically, and perceived and appreciated through senses other than hearing, for example through musical visualizers. Another example, around which this thesis is built, is the Emotichair (Karam, Russo & Fels, 2009), a system that can be used to translate sound into vibrations. It uses voice coils to produce vibrations that can be felt along the human back through a chair in which a user sits.

This thesis focuses on the design of Vibro-Motion, an alternative gesture-based human-computer interface for creating vibrational patterns (or feedback) for the Emoti-chair. Since the field of vibrotactile composition is relatively new, and previous research is limited, I use music composition and human computer musical interfaces (HCMI) including gesture interfaces as my starting point. Existing computer software and computer-human music interfaces have methods that make it possible to create and record auditory music through gesture (and without a physical musical instrument). For example, musicians such as Deadmau5, and Skrillex are only using computers to produce music; the computer has become their musical instruments. Examples of one type of computer human music interface can be found in video games such as "Rock Band" or "Guitar Hero", however, these games do not provide enough control to be creative as the users can only play recorded songs in these games. Computer 1

human musical interfaces should enable creative control over the music being composed while alleviating the responsibilities of sound creation (Overholt, 2009). The notion of composing vibro-tactile patterns through gesture then seems to be a feasible extension of these methods.

Computer interfaces (and software) and computer-based "instruments" that support music composition and/or music play have been researched since 1963 (Sheridan & Ferrell, 1963) and although there are no guidelines for the design of such systems, some commonalities have surfaced over the years. These commonalities include input and output mappings between how the user controls the system and the resulting sounds and/or visual representations produced by the computer that have meaning to the user. Some examples of input and output mappings include: one-to-one (e.g., one user control equals one sound such as what happens with an online piano keyboard), many-to-one (e.g., user controls many elements like fretting the guitar note with one hand, while picking the string with the other in order to produce one sound as an output), many-to-many (e.g., while traditional instruments are not designed as many-to-many, except possibly for playing chords or harmonies on some string or keyboard instruments, computer-mediated instruments can be designed to allow many inputs from the user like fuzz, delay, and phaser effect, etc., where the user can produce many audio outputs based on the selected inputs) (Overholt, 2009). See Figures 1, 2, 3 and 4 for diagrams of each of these mappings.

One of the common assumptions in these computer music systems is the notion that the output is sound (some systems will produce visual music notes or scores but most do not). Input is also often restricted to common computer input devices such as a keyboard and mouse or to computer instruments similar to musical instruments such as a midi keyboard that takes as input hand movements such as the striking of a key. It may be possible to consider alternative input and output devices and mappings to create music. For example, body gestures could be considered to control visual, sound or 2

tactile output to create music. One example of a gesture based interface for music composition is the Virtual Orchestra system which allows users to control the tempo and the loudness of a pre-recorded track using hand gestures (Schertenleib, Gutierrez, Vexo, & Thalmann, 2004).

In order to use gestures to produce music, human-computer musical interfaces must be able to accurately track human gesture (Iverson & Goldin-Meadow, 1998). One of the most versatile gesture tracking computer systems available in the market is the Microsoft KinectTM. This system, which includes a video camera, an infrared sensor and an array of microphones, allows gesture recognition and tracking for two people (Zhang, 2012). Researchers, such as Blaine & Forlines (2002), have used the gesture interfaces to create collaborative environments to compose music.

Instruments for the creation of music with vibro-tactile output are also beginning to be developed. Examples of vibro- musical instruments include Vibloslide, Viblotar and the VibroChord (Marshall, 2005). These vibro-tactile instruments work by allowing the user to interact with the system like a musical instrument except that a vibro-tactile output is produced instead of an auditory output. However, vibrotactile musical instruments have different performance constraints compared with sound-based instruments due to the differences between the human perception of sound and touch: skin can only sense vibrations between 20 hertz and 1000 hertz (Verillo, 1991). In addition, the vibrational output from the instrument needs to be felt by the player in less than 45 milliseconds in order for him/her to perceive the playing of this instrument as a real-time music experience (Sheridan & Ferrell, 1963).

In this thesis I have developed a gesture based musical interface for the Emoti-Chair called VibroMotion, using Microsoft's Kinect, and have also begun to study the user experience that comes with Vibro-Motion. With more traditional computer-musical interfaces, manipulating the properties of music

3

such as pitch, beat and timbre and then delivering it to the human senses (the main one being hearing) can give rise to emotional, physiological and physical reactions (Scherer & Zentner, 2001). However, what does it mean to create a tactile equivalent of sound-based music in the first place, and what are enjoyable vibro-tactile patterns? Vibro-Motion is a gestural/vibro-tactile system that can be used to start exploring these types of questions which are important ones not just for people who prefer vibrotactile interfaces, such as people who are deaf, but also for everyone, as composing vibro-tactile patterns could be an art form in itself.

1.1 Thesis statement
This thesis discusses and explores the idea of composing vibro-tactile patterns using gesture interfaces through a new system called Vibro-Motion. Vibro-tactile patterns refer to meaningful vibrational patterns which can be provided to people through the tactile medium (it may be possible to consider these patterns as vibro-tactile music but for this thesis, they will be called patterns in order to differentiate them from the sound musical output which is also produced by the Emoti-Chair). VibroMotion combines the Microsoft Kinect gesture recognition system with an existing sensory substitution system called the Emoti-chair (see Section 2.1.8.1). The Vibro-Motion makes it possible for a single user to compose vibro-tactile patterns by making gestures, while simultaneously feeling these vibro-tactile patterns on the Emoti-chair.

The main hypothesis of this thesis is that it is feasible for users to create vibro-tactile patterns with the Vibro-Motion system that are entertaining and require little effort to produce. Creating vibro-tactile patterns requires deliberate variations in the frequency, vibrational strength and position of the vibration that are felt on the user's skin. These changes in frequency, vibrational strength and position

4

can be controlled by the user's hand movements. The user can control the frequency by moving his/ her left hand vertically while facing the Kinect; the vibrational strength can be controlled by moving the right hand towards and away from the Kinect, and the position of the vibration can be controlled by moving the right hand vertically while facing the Kinect (see Section 2.1.2 for more details). The details of the algorithm for producing the vibrations are discussed in Section 3. Understanding how these parameters can be controlled and how they interact with each other is important in determining the mechanisms for creating vibro-tactile patterns.

A user study to examine the feasibility and entertainment value of using the Vibro-Motion and Emotichair combination was conducted to explore this innovation and the impact of varying the levels of each parameter. The user study showed that the Vibro-Motion was fun, entertaining and a feasible approach towards composing vibro-tactile patterns using gestures. Finally, I investigated the impact of having gesture gaming experience, or formal music training on people's ability to either create or understand vibrational patterns. One would expect that participants with gesture gaming experience might have an easier time operating a gesture interface; however, the study results showed that this was not the case. Similarly, participants with musical experience may be expected to have an easier time composing vibrational music because of a similar approach to composing heard music. For this reason participants with musical experience were compared with participants without musical experience, and participants with gesture gaming experience were compared with participants without gesture gaming experience when measuring how accurately they were in mimicking the vibrations felt on the Emoti-Chair. For the purpose of the study, participants with one year of music training were considered to be musically experienced and participants who had used a gesture gaming device at least once were considered to be experienced with gesture gaming devices.

5

The research questions are:

1. How do changes in frequency values, vibrational strength and position of a vibro-tactile stimulus presented to a user through the Emoti-Chair influence the user's accuracy in mimicking that stimulus? 2. What is the impact of music experience and gesture gaming experience on user ability to control the frequency, strength and position of vibrations? 3. What is the participant's perception of the controllability of the Vibro-Motion system (accuracy, and ease of control and use)? 4. What is the user's experience with and enjoyment of the Vibro-Motion system?

1.2 Contributions of the Thesis
Research has already been conducted to study music composition using human-computer interfaces (see section 2.1.3); however, the concept of composing vibro-tactile music using gesture interfaces is fairly new. I submit that the research presented in this thesis is one of the first studies performed to consider the idea of vibro-tactile composition using gesture interfaces.

1. The main contribution of this research is the actual design of the Vibro-Motion system and its user interface which includes gesture-based input with real-time visual, auditory, and tactile feedback. The Vibro-Motion was created through this research to enable users to compose vibrational patterns with hand gestures. The vibro-tactile patterns that can be created have elements that correspond to those found in heard music, such as frequency and amplitude. In addition to this, the vibrational position of the patterns on the Emoti-Chair can be controlled by a user. The study of my system showed that the system is easy and fun to use, while also

6

allowing people to create vibro-tactile patterns successfully. The study of my system uncovered a potential relationship between the different parameters of a vibrational stimulus (frequency, vibrational strength and position) and the level of accuracy that can be achieved in mimicking the vibrational stimulus using hand gestures. 2. My exploratory study with a small number of users uncovered possible effects of interactions between the three elements of vibration: frequency, position and strength resulting from the gestures made by users with the Vibro-Motion system and the subsequent perception of these interactions by users sitting on the Emoti-chair (see Section 4). My exploratory study also uncovered many limitations of and suggested improvements to approaching the composition of vibrations using hand gestures. The results, recommendations and issues identified in this study can thus form a basis for on-going and future studies. 3. The mapping structure between the gestures and vibrations felt on the Emoti-chair, the corresponding visual feedback created for the Vibro-Motion was designed to allow the users to operate the system using simple gesture technique that did not require any experience with gesture-based systems such as those available for video gaming (e.g., games for the WiiTM or XboxTM) or musical knowledge (for additional information about the procedure, see Section 3). As part of the user study methodology I also developed a training regime for users to learn how to use the Vibro-Motion gestures to produce vibro-tactile stimuli for the Emoti-Chair. This training regime may also be useful in further studies with Vibro-Motion as well as for other research involving the Kinect system and gestures for controlling vibro-tacile output systems. 4. The user study results also demonstrated that participants found the interface easy to use, novel and fun. This indicates that it is indeed feasible and easy for users to create vibro-tactile patterns with Vibro-Motion, and enjoy the experience of doing so. Whether the composed 7

vibro-tactile patterns are enjoyable or can evoke emotions in the audience similar to heard music remains to be explored. Whether there is a connection between gestures used for the Vibro-Motion system and the vibrations produced needs to be explored. For example: Does gesturing to express an emotion evoke a similar emotion when the vibration is perceived? The Vibro-Motion system might make it possible to explore these areas.

1.3 Thesis Outline:
The thesis is structured as follows:

Chapter 1: Serves as an Introduction to the Thesis, This chapter gives an overview of the goal of the study and background information for the thesis.

Chapter 2: Provides a Literature Review for the thesis. This chapter explains the history and the current state of Vibro-tactile music composition using Human-Computer Music Interfaces, the physiology of vibro-tactile feedback system, sensory substitution and how all this information was used in the design of Vibro-Motion.

Chapter 3: Presents the architecture, design and implementation of the Vibro-Motion System. This chapter provides a detailed description of the software and its capabilities.

Chapter 4: Describes the usability study process and procedure and presents the results, findings and discussion of this study.

Chapter 5: Presents the conclusions, limitations of the thesis and also suggests future research.

8

2 Literature Review
When exploring how gesture-based interfaces can be used for the production of vibro-tactile patterns, a number of fields are relevant: human-computer interaction, human factors, music theory, the psychology of emotion and music, tactile perception, and human-computer music interfaces (HCMI). All of these motivate and inform this work: 

While this thesis does not purport to develop a vibro-tactile notation system or a comprehensive vibro-tactile system, an understanding of the general properties of music and music theory is important to develop an approach towards designing human-computer interface which allows the composition of vibro-tactile patterns.



The literature review of the research conducted in the field of music and emotion provides an understanding of the relationship of music to human experience and perception. As the concept of vibro-tactile patterns is novel and relatively unexplored in the literature, it is important to gain an appreciation of the possible effect of vibro-tactile patterns on people.



The physiology of the tactile sense relates to the research involving sensory substitution and how people react to information from one sensory domain being presented in a different domain.



It is also important to understand how the work in this thesis fits within the field of humancomputer interaction and more specifically human-computer music interfaces (HCMI) because the Vibro-Motion can be considered very similar to an HCMI and should be discussed within the context of other HCMI interfaces.

9



In particular, literature on the use of gesture in HCI and HCMI will provide insight towards the current state of the field along with design insights which were considered for the design of the Vibro-Motion.

Furthermore, specific Human factors literature on reach and gesture comfort zones must be applied in order to ensure the users are can perform the required gestures. This literature review will present some background information on music, gestures, emotion and how they are integrally connected. Following this, a brief history of human-computer interfaces used for the purpose of music composition is presented. Background information regarding gesture interfaces in general and specific examples of how these interfaces are used for music is presented. Finally, this literature review presents background information on the physiology of the tactile sense and vibro-tactile feedback in human computer interfaces. 2.1.1 Music, its impact and its creation

Narrow definitions of music consider it as an art form based on acoustics (Nattiez, 1990). The acoustic structure of music consists of tones, sounds created by people, duration, energy or amplitude, pitch or the frequency, and timbre. In a traditional orchestral setting, elements such as pitch, melody, and harmony are produced by a wide variety of instruments or combinations of instruments including stringed, wood-wind, brass and electronic instruments while percussive instruments provide a basis for rhythm (Nattiez, 1990). In a broader sense, however, music is an expressive art form that is celebrated across all human cultures (Carr, 2004). Music has the power to reach out to people and can cause physical reactions such as swaying, moving and dancing, and/or tactile sensations such as feeling `pounding in the chest' or vibrations on the skin.

10

Modern research has even shown that music positively affects our autonomic nervous system and can improve immune functions, such as decreasing blood pressure and reducing stress (Scherer & Zentner, 2001). Describing the emotional and physical impact of music on us (e.g., "this tune makes me sad", or "the song makes me want to dance") instead of the structural elements of music such as frequency and timbre makes music more accessible and understandable to individuals who do not have specialized music training (Zangwill, 2007; Scherer & Zentner 2001). This is also a natural way to describe music since music is an expressive art form through which compositions convey feelings, moods, and thoughts.

Creating music can be accomplished in many different ways. Traditionally, music is produced using gestures combined with physical devices or instruments. When a musical instrument is touched, hit, strummed, shaken, blown through, etc., the musical properties of pitch, loudness, and beat result which can then, in turn, be heard, seen and/or felt by an individual or a group of people. Computers have introduced new ways of creating music that use indirect methods of making music, such as gesturing without any physical contact with an instrument, playing a tune from a computer-based compositional tool through speakers without any instruments, or using one instrument such as a midi keyboard to play the sounds of a completely different instrument such as a violin. As a result of the flexibility of these human-computer music interfaces, it is possible to explore non-conventional relationships between the theoretical properties of music and how they can be manipulated and expressed through novel means. 2.1.2 Music, Gesture and Emotion

It is human nature to respond to beats and rhythm by movement. The addition of gesturing to music leads to the art form of dance. Musical gestures are a strong response of people's comprehension of music; these gestures can be implicit or culturally learned (Gody & Leman, 2010). These gestures represent the user's attempts to mimic the sounds made by a real instrument and indicate the person's 11

appreciation of music. Gesturing can also enhance musical listening such as for audiences at a concert (Izzetta, 1997). Gestures are also closely connected to emotion in a similar manner that music is connected to emotion (further discussion of this concept is presented in section 2.1.24).

As Henrotte (1992) suggested, "Gesture is not only physical motion, but attitude". Gestures are a gateway to understanding the emotions of a person. The number of body movements can communicate the intensity of the emotions. For example, a reduced number of hand and body movements can be observed in depressed patients and this can be connected in turn with low intensity; low energy or somber musical notes (Carr, 2004). Different features in movements and postures allow the identification of specific emotions. Merola (2007) considers six dimensions of expressivity for communicative gestures: 1) spatial domain around the gesturer, 2) temporal domain, 3) power behind the gesture, 4) fluidity of the gesture, 5) repetition, and 6) overall activity, where the quantity of body movements over a time span is described by overall activity.

Although there are different approaches to deciphering gestures, most agree that human emotions can be deduced from gestures. Quintillianus (1966) attempts to classify gestures into four categories: gestures that indicate a state of mind, gestures which indicate expressions or attitudes, gestures used to point, and gestures that emphasize key words that are being spoken and help in "delivering words to the audience".

Argenot (1973) furthers this claim by suggesting that arm and hand gestures can provide information about some elements of expression. These elements include a wide range of complex emotional elements such as interrogation, frankness, tenderness, dominance, rejection, etc. Austin (1966) suggests that there are four points of view for considering hand gestures: the instrument that performs the

12

gesture: whether the gesture is done by the dominant hand or the subordinate hand; the signification or the meanings of the gestures, and whether these meanings are `natural' or `instituted'; the quality that a gesture may have; and the style of delivery of the gesture (Austin, 1966). Gestures are considered a natural aspect of communication in all human beings and it can be observed in people that are sighted and blind. A study performed by Iverson & Goldin-Meadow (1998) found that, just like sighted individuals, individuals who are blind use gesture during communication with both blind and sighted individuals. Furthermore, both groups showed no significant difference in the rate at which they used gestures in conversation. Regardless of the intent of the original gestures, they are used by everybody; because of this, gestures are a great resource for interactive design.

Spatial concepts that were discussed above were used in the design of the Vibro-Motion. The gestures used by the users allowed them to control the parameters for frequency, position and vibrational strength on the Emoti-Chair. It is possible to create different gesture-vibration mappings on the EmotiChair using the Vibro-Motion. However, basedon the novelty of the interface, the exploratory nature of my research, and some guidance from the literature, I designed gesture-vibration mappings which I thought would be most obvious to users and simplest. Moving the hands close and far with respect to the Kinect sensor could be interpreted intuitively as taking and giving and translates to decreasing and increasing the vibrational strength (magnitude). High and low frequencies are naturally gestured in high and low spaces respectively; therefore moving the left hand up increases the frequency while moving the left hand down decreases the frequency. The location of the vibration can be easily mapped to the right hand's vertical movement as this allows the vibration to simply follow the right hand spatially.

13

2.1.3

Human-Computer Musical Interfaces (HMCI)

Formal musical composition is typically recorded using a complex musical notation, and this is a great obstacle for amateurs who have no musical knowledge. HCMIs can bridge the gap by providing easy to use interfaces for users who have no training in formal music notation but can play a musical instrument to compose/create music. Although music produced using HCMIs does not necessarily produce the same reactions in listeners as human made music, Unehara & Onisawa (2005) found that computers can assist the amateur with music composition, and that this allows users to have control over the creative process of making music. HCMIs bridge the gap between musical knowledge and composition for the users; they provide virtual scenes where the user has the ability to control various elements of the HCMI, while the system generates the sounds.

With human-computer musical interfaces, the production of music is becoming simpler. Popular music is becoming increasingly dependent on Human-Computer Musical Interfaces like Fruity LoopsTM and CakewalkTM (Fruity loops studio.2012; Roland, 2012). However, it appears that simpler is not always better. Marshall & Wanderley (2006) asked volunteers to try two musical interfaces; one with a simple one-to-one mapping, and the other with a much more complicated many-to-many mapping (see Figure 1 2, 3 and 4). The volunteers of the study quickly understood the controls behind the simple interface, and half-heartedly practiced with it for a few minutes before moving on. The volunteers using the instrument with many-to-many mapping were engaged for a longer period of time because they were forced to balance various effects of the system. The unusual mapping kept the participants interested in figuring out the instrument. Wanderley (2003) suggests that the mapping between input parameters and the outputs should not be so simplistic so that the users quickly become bored because of a lack of challenge in using the system. The difficulty level of an HCMI mapping should be similar to a real musical 14

instrument where the users can immediately start creating sounds but they are still driven to overcome frustration and constantly discover new and exciting aspects of the interface. At this time, there are no generally applicable theories for designing HCMIs; however, there are many input/output mapping strategies which could be used as guidelines for designing HCMIs.

Esmerado & Thalmann (2002) suggest that the interaction between a virtual scene and the user should be set up in a comprehensive and appropriate manner. The system's interface should seamlessly integrate the user into the scene, and the movement and the actions of the virtual objects should react appropriately and intuitively. Although it is possible for any person to compose music using traditional instruments, the process involves a lot of effort and time. The effort and time required may be too high for novices who only want to create music. HCMIs decrease the distance between achieving a level of control of the instrument and composing music (Drummond, 2009).

Unehara & Onisawa (2005) designed a system which uses genetic algorithms to allow users without musical knowledge to compose music. Their system works by providing users with a pre-made backing track and computer generated melodies in 4-bar pieces which can then be selected and combined by the users. They found that users were able to compose music using this technique because this system had the right balance between machine and human control. Biles (1999) also designed a system called "GenJam", which allowed users to improvise jazz songs by using an interactive genetic algorithm. The users provide feedback on the melodic ideas generated by GenJam, which influences whether melodic ideas are scrapped or further developed.

Behringer (2007) designed a computer-controlled musical synthesizer which used mouse movements to emulate a conductor's baton movements. These mouse movements allowed the user to control the

15

tempo and the amplitude of a pre-made MIDI file of a classical orchestral piece on a computer. A one-toone mapping was chosen; the vertical movements of the mouse indicated the tempo of the music, where the first long vertical movement of the mouse signaled the first beat in the measure. The amplitude was mapped based on the vertical and horizontal direction. The tempo of the music was calculated using the duration between two beats, the change in the duration controls of the mouse, and the change in tempo.

Schertenleib et al. (2004) designed Virtual Orchestra, a system which allowed a conductor to keep track of virtual instruments in an automated orchestra in a 3-D sound environment using gestures similar to those of an orchestra conductor. There were limitations associated with providing a real-time experience to the conductor including providing convincing (seamless) animations in the video and transitions in the music played. In order to compensate for the changes in tempo, the music played through the system had to be pre-recorded at different tempos and accessed using a database. It was observed that such an immersive system would need to respond very quickly to human inputs which is a problem because the sound systems were affected by audio processing delay.

Young & Serafin (2003) explored the use of the gestures of a violinist in order to simulate the sounds of the violin using a virtual instrument. The definition of playability is different between virtual musical instruments and real musical instruments. The playability of a virtual musical instrument is defined to be a measure of how well the inputs of the system map to the outputs of the system, and also includes the quality of the sounds produced. The Virtual violin instrument uses a many-to-many mapping scheme as multiple changes are required to affect the variety of sounds produced by the instrument; elements like bow pressure, bow velocity, bridge distance and bow width are used as the input parameters, and a wide range of sounds can be produced depending on the combinations used. 16

A number of other simpler systems have been designed to allow people to use gestures to create music. for example: Bottoni, Faralli, Labella & Pierro (2006) designed a system which allows the creation of music using existing software programs like GO and Max; Chen (2009) designed a system using RadioFrequency Identification (RFID) to allow users to interact with the computer using gestures. However, the drawback of these systems is that the design of the system was focused on creating a feasible input/output mapping rather than creating a high quality output feedback.

Although no standardized design models have been developed for HCMIs, Overholt (2009) suggests that the evaluation criteria for HCMIs, specifically those interfaces which rely on gestures, include: ease of use; engagingness of the gestures; accuracy of the gesture-tracking; behaviour of the interface; instrument's uniqueness; richness of the mapping methodology; and wideness of the range of gestural expression received by the interface. Overholt also suggests that although there can be two extremes for the balance between human and machine control of HCMIs, the best combination would be to have a system which has equal parts of human control and machine control. For example, a mostly machinecontrolled system would have algorithmic compositions and pre-arranged sound files, leaving little freedom for personal expression whereas a mostly human-controlled system could simply be a physical musical instrument that controls computer sound. Examples of balanced systems with equal human and machine control can be observed in video game consoles such as "Rock Band" (Snow, 2008) or "Guitar Hero" (Lynch, 2009).

These games are not only entertaining at the beginning, they allow the users to start producing music immediately, and they keep them engaged by increasing the difficulty levels progressively. A high level of immersion can be achieved by designing a system to increase and decrease the difficulty of the tasks

17

to match the ability of the users. However, designing a dynamic system can be complicated, so creative mappings can be used to generate the same effect. 2.1.3.1 Mapping Strategies: There are a wide variety of mapping alternatives available for mapping inputs and outputs of an HCMI system; they are: one-to-one, many-to-many, one-to-many (also known as "divergent") and many-toone (also known as "convergent") (Overholt, 2009 and Rovan et al., 1997), (see Figures 1 to 4 below). In these mappings, the inputs are the user's actions on the musical instrument and the outputs are the sounds produced by this instrument as a result of these actions. A one-to-one mapping is a mapping where the user performs one action which results in one sound, such as what happens with an online piano keyboard, or on a physical xylophone. A one-to-many mapping is where an input directly controls the output, for example, hitting one key on the chorused keyboard plays multiple notes. A many-to-one mapping is a mapping where the user controls many elements (like fretting the guitar note with one hand, while picking the string with the other) in order to produce one sound as an output. A many-tomany mapping is a mapping where the user controls many elements and this results in multiple outputs. While traditional instruments are not designed as many-to-many, except possibly for playing chords or harmonies on some string or keyboard instruments, computer-mediated instruments can be designed to allow many inputs from the user like fuzz, delay, and phazer, etc. allowing the user to produce many audio outputs based on the selected inputs.

18

Figure 3: Diagram of a Many-to-Many Mapping. Figure 1: Diagram of a One-to-One Mapping.

Figure 4: Diagram of a One to Many Mapping (Divergent Figure 2: Diagram of a Many-to-One Mapping Mapping). (Convergent Mapping).

The HCMIs and the issues they presented discussed in this section provided a basis from which the Vibro-Motion's user experience was designed. The control of the system was designed to provide a level of challenge for the user which would not bore or discourage the user from wanting to use the VibroMotion system. Among the many available mappings, a many-to-one mapping seemed most appropriate for the Vibro-Motion as the separate controls for the vibrational strength, position and frequency created a single vibrational output on the Emoti-Chair. This was because the three controllable features of the vibration, amplitude or strength, pitch and position were controlled separately but can only be experienced as a combined signal due to the nature of how the signals are processed by the voice coils. Frequency cannot be experienced separately from strength or position even though each can be 19

controlled separately, and thus, all three elements play a role in the final vibration played through the Emoti-Chair. The Vibro-Motion system also produces some sound as well as there is a visual output but these outputs are secondary compared with the mapping between gesture and strength, position and frequency. The visual output is for training purposes indicating the limits of the Kinect calibration to the user and eventually, after the user becomes proficient, the visual interface would no longer be needed. For my research, the sound is deliberately masked by white noise so it will not interfere with the vibrotactile experience. As a user learns how to play Vibro-Motion, the mapping could be considered a manyto-many mapping. 2.1.3.2 Microsoft Kinect system The Kinect system is a motion tracking hardware/software device The Kinect system consists of a microphone array in the sensor, an infrared (IR) sensor and Red/Green/Blue (RGB) camera. The device uses proprietary algorithms to allow motion tracking by using the IR sensors and because of this there is little control over which processes are used. The Kinect system uses the recorded scene as the input and creates the output in the form of a depth stream. This depth stream contains greyscale data of the scene, where the closest objects are lighter than the furthest objects; see Figure 6 below. The Kinect system also creates a RGB stream output of the scene that is being recorded, see Figure 5 (Zhang, 2012).

The Kinect system can track movements of up to two people at a time, at a maximum speed of 30 frames per second (FPS). The Kinect communicates with the PC by sending information collected from the IR sensor data stream and the RGB sensor stream. The input data, consisting of the user's joint locations in a three dimensional (3-D) space, was then processed by the PC for analyzing the data. These joint locations refer to where the user's skeleton's joints are located at any given time while the user is

20

being tracked by the Kinect. The PC used was sufficiently powerful for the visual feedback and the vibrotactile feedback to be perceived simultaneously with a very small delay (delay was not measured, but a couple of participants mentioned that the delay affected their performance).

Figure 5: Image stream seen from the VGA camera.

Figure 6: The depth stream of the same image appearing in Figure 5.

Figure 7: The Microsoft Kinect Sensor.

2.1.3.3 Nyko Zoom: Due to space constraints at the usability lab where the system evaluation was performed, a Nyko Zoom add-on was used for the Kinect. This add-on zooms on the user and decreases the open space required by the Kinect by 40%. The accuracy of the system also improved when using this add-on because the system is made closer to the user, which improves the Kinect's ability to track fine movements. Although

21

the Nyko Zoom enhances tracking data, it also creates a fish eye effect on the RGB image stream, as seen in Figure 5. Fortunately, this side-effect does not negatively affect the functionality of the system. Due to its zooming effect, the Nyko allows the Kinect to maintain a higher accuracy when tracking the hand gestures while minimizing the skeletal jitters.

Figure 8: The Nyko Zoom Add-on.

2.1.4

Gesture Interfaces

Traditional human to computer communication is mostly limited to keyboards and pointing devices such as mice, touch pads, and track balls. This type of communication can be limiting to users since it does not enable them to realize their full expressive potential which normally also includes speech, facial expressions, and body gestures. McNiell (2005) suggests that gestures are symbolic and help communicate thoughts using action. Since gestures do not have a coding system because most gestures do not have a standard specific meaning except in gesture-based languages such as sign languages, there are no regulated or standard guidelines for how to use them in gesture interfaces. Therefore, ideas for which gestures to use in my research were informed by research in Gesture Interfaces 22

Gesture recognition by computers, however, is still emerging. The term "Gesture Interface" refers to a user's ability to communicate with a system through their gestures. Although the term "Gesture Interface" is most commonly used in reference to gaming and entertainment products, mobility products such as Segway PTTM also use gestures in the operation of their products. In particular, the Segway PT requires users to lean their body in the direction that they want their machine to move. These gestures are detected using physical accelerometers and sensors which allow users to control the machine's movement (Canny, 2006).

Robineau, Boy, Orliaguet, VÃ¡zquez-Buenosaires, Demongeot, & Payan (2006) designed a gesturecontrolled tactile system which allows doctors to perform surgical procedures from a remote location. Their design uses a Tongue Display Unit (TDU) that provides vibro-tactile feedback to the tongue, in addition to visual feedback produced on a computer screen. The usability and efficiency of the TDU feedback as opposed to the visual feedback was tested. The study showed that the TDU was superior to the visual feedback because it did not require the participants to look away from the operating area. Generally speaking, multi-modal displays can provide information to the users using a medium that does not distract the users from their task.

Menelas (2011) designed a haptic interface which allows the users to manipulate a synthetic 3-D terrain. The 3-d terrain is generated using geological datasets and is presented on a computer screen. The haptic device used in their design is called the PHANTOM Omni, and it provides the user with a total of six degrees of freedom, three of which are available for force feedback. The hand gestures were tracked using the Swiss Ranger (SR4000) camera, and the system was developed using C++ and OpenGL. Future work to evaluate the usability of the system is pending.

23

Fohrenbach, Konig, Gerken, Reiterer, (2009) developed a gesture tracking system which uses six infrared cameras to track the markers placed on users to gather movement data. The system also uses a large scale display with a resolution of 4640x1920 pixels and a data-glove which was used for tracking finger movements. Fohrenbach et al. performed a study where the participants were asked to observe the visual display for vertical or horizontal movement and to select the appropriate movement on the display remotely using the data glove. Selecting buttons on the screen with gestures was accompanied by tactile feedback that informed participants that the button had been clicked. The study concluded that there was no difference between the performance of users with tactile feedback present, and the users that received no tactile feedback. They suggested that there were reasons to indicate that users merely tolerated proactive tactile feedback provided by these systems because participants in the study did not take advantage of the tactile feedback and relied mainly on the visual feedback.

Prasad, Saxena, Javar, Kaushik, Chakraborty, & Nandi, (2010) designed an algorithm which uses stereo cameras to create three-dimensional vision. Three-dimensional vision allows the images to be processed directly by detection algorithms without making modifications to two-dimensional images for analysis. This algorithm creates advantages in detecting shape, distances, actual dimensions of objects which are free from rotation, translation and illumination variations. The iNErtial Module (iNEMO) is a sensorbased module which consists of an inertia tracker, magnetometer, gyroscope, temperature and pressure sensors that provide the user with a high degree of freedom of movement. Systems like this can be integrated into a single package and can be used in wide variety of industries including consumer electronics, home automation, and even healthcare (Juneja, 2010).

Jiang, Gao, Yao, Zhao, & Chen (2008) developed a Vision Based Interface (VBI) system called Sign Language Recognition (SLR) that attempted to decode sign language gestures by electronically tracking 24

the hand shape, orientation, position, movement and facial expressions because they are all important inputs for decoding sign language. In this system, the user wears a CyberGlove digitally embedded with eighteen sensors on each hand. A Polhemus 3-D tracker is used to track all the components. Hidden Markov models, explained below, were used to isolate hand signs with high accuracy. While this system was sensitive enough to detect many of the inputs aforementioned, difficulties arose when some hand shapes were very similar. For example, the letters "i" and "j" were very similar, but they were different in one dimension. In order to predict the correct letters expressed by the user, algorithms like Kohonen's K-means and clustering algorithms were used to accurately predict the signs (Jiang et al., 2008). Their user study found that participants recorded a 92% accuracy rate for a vocabulary of 262 unique signs. Hidden Markov models are statistical predictive algorithms which were used to accurately predict patterns.

The gaming industry has successfully adopted gesture interfaces. This is exemplified by the many gesture based gaming technologies produced by this industry, such as the Microsoft KinectTM, Nintendo WiiTM and Sony PlayStation MoveTM for physical gestures (Juneja, 2010). Some of these technologies rely on alternate devices with sensors, emitters, sensors, accelerometers, etc., which may be partly or fully wearable and which turn the gamer's body into an input device for the game, making the game an immersive realistic experience within a virtual world, which adds to the thrill of game (Juneja, 2010). Gesture-based video games still have a low level of gesture recognition that is mostly limited to dynamic gestures; however, they can still provide a level of expressiveness which was not available before gesture gaming technologies emerged.

QuiQui's Giant Bounce game HÃ¶ysniemi, HÃ¤mÃ¤lÃ¤inen, Turkki, & Rouvi (2005) is a "Wizard of Oz" themed, gesture style experiment designed for children where users move their hands and legs to control 25

QuiQui, the dragon on screen. HÃ¶ysniemi et al. (2005) have suggested that there are some key requirements for the success of computer-vision based games. These requirements include responsiveness, intuitiveness, robustness, and physical appropriateness in relation to the user.

Fluid interaction between a computer and user is a goal of the gaming industry as well as many other industries (Thilmany, 2008). In a study by Correa, Marques, Marichal, & Macq (2008) techniques were developed which do not rely on people wearing suits fitted with sensors and accelerometers. Instead, their study captured gestures directly and communicated this input to a computer. Two stereo cameras were used to record the same scene to simplify the gesture, and body recognition algorithms.

The KinectTM (Zhang, 2012), introduced on the market in 2010, is a gesture recognition hardware and software device mainly used for gaming purposes however; its computer vision capabilities are being applied in multiple areas of research. The device uses a learning algorithm to extrapolate human gestures from experience as it recognizes gestures in real-time. The sensors on the Xbox Kinect allow a user to move freely without the need to wear spandex suit with sensors attached, which is normally used in conventional motion-capture methods. This device also does not require a green screen background to perform user tracking tasks. The Kinect's sensors include a video camera, an infrared sensor and an array of microphones. The Kinect's infrared sensor technology improved the process of gesture tracking by working with only 20% of the tracked data.

Microsoft competes with other companies in the gaming industry such as Sony, who have also produced a stereo video and depth camera system, called Playstation Move, similar to Kinect for capturing motion for gaming. Canesta is another company that produces computer-vision hardware in partnership with companies such as Hitachi and GestureTek to develop a remote control system. This system allows users

26

to interact with a television set without the use of a hand-held remote control system. As innovative as both systems are, purely gesture interfaces run the risk of not providing sufficient feedback to users since there is no tangible physical object for the users to touch during interaction (Kuchinskas, 2010).

Vibro-tactile feedback used in conjunction with gesture interfaces has been explored by several researchers to address the feedback issue for users. Frati & Prattichizzo (2011) developed a system which uses the Microsoft Kinect to track body and hand gestures. They used the Kinect with an OpenNI platform and OpenCV to track the body movements. The hand tracking algorithm uses depth image and processes to compute a virtual bounding box around the hand which is being tracked, feature detection to extract the positions and trajectories of the finger tips and tracking of other important zones of the hand like the base and wrists. This gesture recognition system is combined with a wearable haptic glove. The haptic feedback is provided to a user's fingers while they are interacting with the virtual world. Frati & Prattichizzo (2011) concluded that Kinect technology would greatly enhance the world of haptic interfaces.

However, there are some limitations with the Kinect that need to be considered when using it for nongaming applications. The Kinect's ideal operating distance for skeletal tracking ranges between 1.2 meters and 3.5 meters but its optimal finger tracking operating distance is between 0.9 meters and 1.2 meters. This is a problem when attempting to design systems which perform both skeletal gesture tracking and finger tracking because, as the hand moves further away from the sensor in order to optimize the skeletal gesture tracking, the hand becomes too small for the system to accurately track the movement of its fingers (Zhang, 2012).

27

With virtual interfaces, it is necessary to realize that system delay can greatly enhance or limit the experience a person has when operating a system. While an intuitive mapping can enhance the user enjoyment with the system, any noticeable delay between input and feedback can negatively impact the user experience. Delays over 300 milliseconds or more are considered sluggish and require the users to stop and wait for the system. Experiences such as this can potentially increase user frustration levels and decrease the enjoyment of using a gesture interface (Sheridan & Ferrell, 1963). To overcome this, a gesture-based system should aim to have a delay that is lower than 45 milliseconds for the users to have a real-time experience with the system. Furthermore, gestures that are concise and comfortable to a user are preferable as they tend to encourage good ergonomics, avoid strains and fatigue caused by having to perform awkward gestures and postures (Wachs, Kolsch, Stern, & Edan, 2011).

Trans-domain mapping or multi-modal mapping is a translation method which translates the features of one creative domain to another domain. The aspects to trans-domain mapping include input sensing and data acquisition: interfacing the software framework with the real world environment; feature detection and tracking: using algorithms to locate and follow features in the input data; mapping the inputs and outputs appropriately: using a set of functions to arrange the way the outputs are affected by inputs; output and simulation and finally, translating the features of the input domain into the features of the desired domain (Ng, 2002).

Since the Vibro-Motion only uses gesture as it input system rather than developing a gesture recognition system, it was important to use an off the shelf solution. The Microsoft Kinect system offered a very usable API (Application Programming Interface) which allowed the Vibro-Motion to track the movements of the user; the Kinect also provided the data in a way which was suitable for the VibroMotion system. However, due to the limitations of the Kinect and the study area, which was smaller 28

than the space recommended for the Kinect, the Vibro-Motion system design had to compensate for the limitation. This issue was overcome with the use of the Nyko Zoom device (see Figure 8) which modifies the Kinect's field of view and allows the users to interact with the system at a closer distance. This decreases any potential effects that a loss of accuracy might have on the functionality of the system. The detailed design of Vibro-Motion with the Kinect and the Nyko Zoom device is fully outlined in Section 2.1.3.2 and Section 2.1.3.3 respectively.

2.1.5

Gesture Musical Instruments:

Many gesture-based musical instruments have been developed in the past including the Teleharmonium (see Figure 9), Electronic sackbut (see Figure 10), Theremin (see Figure 11), Brain Opera (see Figure 12) and the Digital Baton (see Figure 13) (Paradiso, 1997).

Figure 10: The Electronic Sackbut (Young, Figure 9: The Teleharmonium (Cahill, 1897) 1999)

29

Figure 11: The Theremin (Theremin, 1919) Figure 12: The Brain Opera (Machover, 1996)

Figure 13: The Digital Baton (Paradiso, 2002)

The Teleharmonium developed by Cahill (1897) was an early electronic musical instrument which used "tone-wheels" which could be turned to generate electronic signals that created sound. The Theremin (The London Mercury, 1928) could be considered a successful gesture-based musical interface; the interface is simple, however experience and practice is required before the instrument can be played well. The electronic sackbut (LeCaine, 1948) was an early synthesizer which could produce music and it could be played as a keyboard with the right hand, while the organ-like properties of the instrument were controlled with the left hand. The Brain Opera (Machover, 1996) is a large gesture controlled system which uses a large interactive display, voice and rhythm recorders, and gesture tracking 30

interfaces to allow users to compose music. The digital baton (Borchers, Lee, Samminger, & MÃ¼hlhÃ¤user, 2004) was a gesture based music composition device which worked using an Infrared (IR) Light Emitting Diode (LED) at the tip of the baton, which was placed close to a photosensitive sensor array which was used to record the movement of the baton.

Generally speaking, gesture interfaces which respond directly to users create an engaging environment and improve the interactivity through dynamic movement. A number of researchers have applied this theory. Spasov (2011) applied this theory in the design of an interactive, multi-modal music composition system called ENACTIV. This system uses an optical motion capture device to capture the video stream of the user's gestures. MAX/MSP software with QuickTime plugins are used to make adjustments to the captured video, in particular the hue, luminosity and saturation before user gestures are mapped to the synthesized audio outputs.

Prasad, Nandi, & Kumar (2009) designed a gesture-based music generation system which uses the SkillSpector software to track a user's physical movement including body joint angles, positions, velocity and acceleration (Video4Coach, 2008). Joint locations and velocity values are used to control elements such as pitch and intensity. In this system, wrist movement is used to control the pitch, and elbow movement is used to control the intensity of the sound. A visual feedback of a robot that mimics the movements of the user on screen is provided to users. The approach in this study allowed users who have no prior music experience to create MIDI (Musical Instrument Digital Interface) tones. Prasad et al. (2009) suggested that they would like to design a system which could be used to compose music instead of sound patterns, and have considered piano and drums as templates for designing gesture models for composing music using the SkillSpector.

31

Smimov (2000) created an interface called Sonochronotops which allows users to compose music using gestures in a 3-D space through the use of instruments similar to that of the Theremin. The Sonochronotops system allows users to modify the sound created by their musical instrument by tracking user movements using Theramin sensors while the user is playing a musical instrument.

Zamorano (2012) designed an interface called Simpletones, which allows collaboration between many people to create sound through the use of gestures. The system has a main controller called the "basic triangle" which consists of three coloured balls at each of the vertices used for tracking movements. Using the basic triangle, users collaborate with other users using non-verbal communication to compose music. The aim of the system is to act as a catalyst for groups to reach a state of flow, during which the group can maintain a constant rhythm. This system takes care of the music composition and balancing the level of expressivity by tracking the movement of the balls on the triangle. The system allows the users to work on the expressive elements of composing music in a group setting.

One of the drawbacks of interactive gesture interfaces which are designed to allow users to conduct music is that they tend to provide unsatisfactory user experiences because these systems are designed with gesture recognition optimization as a priority, and they tend to ignore the importance of the quality of the audio-visual output (Borchers et al., 2004).

The strengths and drawbacks of gesture-based instruments which are outlined in this section were considered in the design of the Vibro-Motion gesture interface. From the above research the VibroMotion system was designed to provide the participants with a fun and an entertaining interaction with the system, while maintaining the challenge level of the system which encourages the participants to

32

experiment and learn the Vibro-Motion system. The intuitive control system makes the Vibro-Motion interface easy to use, while maintaining a challenge level that would keep the users engaged.

The Vibro-Motion's design also aims to provide a good quality of vibro-tactile output while creating a feasible input/output mapping. Different approaches to the vibro-tactile feedback were considered but the goal was to provide vibro-tactile feedback that felt organic or natural.

2.1.6

Physiology of the tactile sense

Tactile sensory systems work by applying physical pressure and vibrations to the user's skin. The skin detects these sensations via mechanoreceptors in the skin.

Mechanoreceptors in the skin can be classified into four types: Meissner's corpuscles, Pacinian corpuscles, Ruffini corpuscles and Merkel's disks (Johnson, 2001). Miessner's corpuscles are recruited for feeling lower frequencies at about 50 Hz (ParÃ©, Mazurkewicz, Smith & Rice, 2001) and Pacinian corpuscles are used for detecting vibrations ranging from 1 Hz to 1000 Hz with the highest sensitivity around 150 Hz to 250 Hz (Verillo, 1991). Merkel's disks detect pressure (Munger, Pubols, Pubols, 1971) and Ruffini corpuscles also respond to sustained pressure (Barrett, Boitano, Barman & Brooks, 2009).

A study performed by Verillo (1991) showed that the glabrous (hairless skin such as finger tips, tongue etc.) mechanoreceptors in the skin are sensitive to vibrations in the ranges of 40 Hz and 1000Hz, with the highest sensitivity at 250 Hz. A study performed by Bikah, Hallbeck, and Flowers (2008) showed that the highest sensitivity of non-glabrous (hairy skin such as arms, legs and back) mechanoreceptors in the skin is also around 250 Hz.

33

Craig & Evans (1987) conducted a study which revealed that its participants were able to identify vibrotactile patterns more accurately when stimuli were applied for a longer duration than when it only lasted a short duration, but only up to a point. The vibrations were presented to participants for the varying durations of 300ms, 600ms and 1200ms. Although participants claimed that stimuli of longer durations were easier to perceive, the participants did not feel the vibro-tactile patterns that lasted longer than 1200msec as accurately, likely because the mechanoreceptors in the area of the skin where the vibration was felt became saturated. The Optacon device was used to provide the vibro-tactile feedback to the fingertips; typically this device is used as a reading aid for the blind.

Kitawaza (2002) also carried out a study to explore the conscious sensation of vibro-tactile feedback. Researchers used an apparatus that produces vibro-tactile stimulation under multiple conditions where the stimuli lasts 80ms to one of the two hands, 80ms to both hands, and no stimulation to both hands. The results showed that once a stimulus had been activated, it took approximately 80ms for it to be perceived, and that the stimulus was accurately perceived only after it was active for more than 600ms.

A wide range of tactile interfaces have been tested to try and determine the best anatomical regions to use with computer-tactile systems. These included the back, abdomen, fingers, forehead and the tongue (Bach-y-Rita, 2004). They found that the tongue was the best location for a vibro-tactile interface due to the high number of nerve endings and high sensitivity. However, Jones & Sarter (2008) found that the vibrational stimulus is perceived similarly on different parts of the body. Because of this finding, the impact of feeling the vibrational stimulus on the back as opposed to the finger tips or the tongue was not considered a factor in the scope of my thesis.

34

Summers et al. (1997) also found that the human tactile sense was insensitive to changes in shape of the vibrational waveform (e.g., sawtooth vs sinusoidal). However, experiments by Russo, Ammirante and Fels (2012) showed that it is possible for people to perceive changes in vibrational waveforms or timbre. Although the impact of vibrational waveform was not considered a factor in the scope of my thesis, future research is suggested to measure the effect of vibrational waveforms on the user's performance.

2.1.7

Sensory Substitution

Sensory substitution is the conversion of information from one sensory domain into a new sensory domain. There is a body of HCI research that explores tactile feedback as a sensory substitution technique to replace visual or auditory feedback rather than as distinct or complimentary feedback such as that used in force-feedback or virtual reality systems.

Tactile technologies allow users to obtain information through their tactile sense rather than their audio-visual perceptual system. This technology could potentially decrease the cognitive load of a user because tactile displays would not require a user to view or listen for feedback. The eyes and ears of a user are therefore freed to focus on information expressed through alternative modalities. This also allows users who are missing access to these visual and auditory perceptual systems (e.g., people who are deaf, blind or deaf-blind) to experience the technology the same way as the users who do have access to these perceptual systems (Cholewiak & McGrath, 2006).

Here are some studies that reinforce the validity of sensory substitution as an appropriate and useful method to convey information:

35

Ward & Meijer (2010) discovered that the act of perceiving an image happens after the visual image passes through the optic nerves, and at this point the image itself is no longer maintained in the memory as an image. This disconnect between how an image is perceived by the eye and how the information contained in this image is processed by the brain would indicate that humans may be receptive to sensory substitution, and this could provide an opportunity to present information from different modalities in a novel manner. Bach-y-Rita (2004) also found that conveying visual information through the tactile domain does not obstruct the brain's ability to process other visual information.

Vries, Erp & Kiefer (2009) explored a vibro-tactile feedback system for use in cars to provide information to drivers without requiring any visual attention. The study showed that information provided using vibro-tactile displays decreased driver effort and that these results could also be applied to pilots, astronauts, speed boat drivers and long distance truckers because vibro-tactile feedback systems do not interfere with visual or auditory perception when driving. Their studies also showed that the comprehension rate of vibrations was low when it was felt by participants for less than 250 milliseconds (ms) however; comprehension was higher when the vibration was felt for over 500ms or more.

Proulx (2010) also determined that presenting visual information through another sensory domain may make it possible to avoid distracting the user from the task. This was determined in a study that compared a tongue camera with a visual camera. A tongue camera uses tactile stimuli on the tongue to represent a visual image seen by the camera. They found that participants using the tongue camera could improve accuracy and efficiency, validating the potential usefulness of sensory substitution.

A study was performed by Riggs et al. (2006) to observe the effect of using tactile perception as a substitute for the visual system. An apparatus was built using software which provides vibro-tactile

36

stimulation to a user through the fingertips. Participants were administered vibrations through their fingertips on both hands and asked to name where they felt the vibrations and the order in which they felt the vibration.

Finally, an earlier study by Cheng (1968) suggested that if information from one modality is derived from another modality, the information between the modalities should aim to have a good connection. He explains this through the following example: an object in a virtual space that moves up and down in the visual domain could have corresponding audio that plays high and low. When information is translated from its original modality to the other, the information should be calibrated in terms of how the information would normally be perceived in its original modality. In a situation where people receive information from multiple modalities, our ability to perceive this information greatly improves if the modalities are structured in such a way that the information complements each other (JousmÃ¤ki & Hari, 1998).

2.1.8

Tactile systems and Vibro-tactile Feedback in Human Computer Interfaces

A considerable amount of research has been done in designing human computer interfaces which provide vibro-tactile feedback. Tactile feedback is an essential part of the experience of composing vibro-tactile patterns: Keele (1973) performed a study which revealed that tactile feedback is more useful when learning musical instruments than visual feedback. Therefore, it is important to incorporate some vibro-tactile feedback in the design of virtual instruments. In addition, vibro-tactile feedback can also make music accessible to deaf or hard of hearing audiences who access music through tactile perception as well as enhancing the musical listening experience of hearing members of the audience.

37

Musical interfaces with vibro-tactile feedback work by translating the two sound parameters of auditory music, pitch and loudness, into tactile stimuli. Because the vibration can be provided to different parts of the skin, vibro-tactile feedback has a third element, position on the body, associated with it. The vibro-tactile position, frequency and strength thus constitute the three parameters of the vibro-tactile feedback.

Birnbaum & Wanderley (2009) found that some elements of the audio domain can be substituted directly in the vibro-tactile domain. For instance, loudness can be best represented by the amplitude or magnitude of a vibrational stimulus applied to the skin. Brightness of the signal can be represented by tactile stimulus changing from a smooth sensation to a more rough sensation; in the auditory domain brightness of the signal can be translated to the amount of distortion in the signal. Lower brightness has a higher distortion, while a higher brightness has a lower distortion. These changes in the roughness can be executed by modifying the signal waveform from a sine wave, to a square wave to a noisy wave. The noisiness of the signal can be mapped to brightness levels, and loudness can be mapped to the magnitude of the vibration. Sound and vibration are also both time dependent waves, and therefore the dynamic vibrational qualities of sound, such as attack, sustain and decay, can be easily represented with very similar changes in vibrations.

Although vibro-tactile instruments show promise in acting similarly to their acoustic counterparts, it is not necessary to limit the approach to only "acoustic vibration simulation" to model musical vibrotactile interfaces in any way. For example, Birnbaum & Wanderley (2009) suggest that for complex musical applications, extracting the most exciting features of musical elements and then resynthesizing them as tactile stimuli may lead to better tactile perception than trying to represent the entire piece in the tactile domain. 38

Cholewiak & Collins (2000) suggested that the vibro-tactile patterns that accompany an interface should contain appropriate meaning within their situation. This means that the vibro-tactile outputs should correspond intuitively to the input controls provided by the user. They suggested that vibro-tactile patterns should be explored to find the best patterns dependent upon the context of the expected scenario; however, simple patterns like linear vibrational patterns, which stimulate the skin using linearly placed tactors, can be considered to be reliable patterns because they are consistent with what is expected by the users.

Lim, Kim, Kyung, & Kwon (2006) conducted two experiments involving vibro-tactile perception. The first experiment involved measuring frequency perception received through user fingertips based on frequencies varying between 6.31 Hz to 398.1 Hz at a constant decibel range. The second experiment included changing the shapes of the vibration by creating changes in the frequency. The study concluded that vibrational frequency can be perceived as shapes through the fingertips without providing vibrational amplitude changes as additional information about the vibration.

Marshall & Wanderley (2006) developed two vibro-tactile instruments called the Vibloslide and Viblotar. The Viblotar (see Figure 14) allows users to create vibro-tactile patterns by touching the instruments to activate the pitches. The pressure on the hand can be used to select the pitches and the user can dynamically control the pitch and amplitude using their right hand. The user's left hand can control pitch bending and vibrato using pressure sensors. The Vibloslide is a vibro-tactile instrument which represents a woodwind instrument. The user can activate the pressure sensors mounted on the instrument itself to control the vibrations produced. Both systems use voice coils for vibro-tactile feedback.

39

Figure 14: The Viblotar (Marshall & Wanderley ,2006)

Marshall (2005) discusses the general requirements of vibro-tactile feedback in any interactive system. These include that the range of frequency of the vibrations must fall in the 40Hz to 1000Hz range so that the system can produce stimuli that can be felt by the skin.

In order to produce vibro-tactile feedback, it is important to select a vibro-tactile device that can produce the appropriate stimuli. There are a fair number of tactile devices available in the market, these include voice coils (or sound exciters) (see Figure 15), tactors (see Figure 16), piezo-electric devices, motors or solenoids to produce the vibrations. An evaluation of these devices showed that the motor and solenoids were the worst options for vibro-tactile feedback, whereas voice coils and tactors were the best because they had the best feedback response (Marshall, 2005).

40

Figure 16: The C2 Tactor (C2 Tactor, n.d.). Figure 15: Dayton Audio Sound Exciters (Dayton, n.d.)

The voice coils devices are used to convert electrical current into sound and they are also referred to as sound exciters because they are the main drivers of sound in a speaker. The main difference between a speaker and a voice coil is that the voice coil does not have a speaker cone. This reduces the sound transmitted by the voice coils; however, if one were to touch the sound exciter while it is active, the vibration produced by the sound exciter could be felt. 2.1.8.1 Emoti-Chair Karam et al., (2009) designed a sensory substitution technology called the Emoti-Chair, which is a chair which incorporates tactile feedback and is used to provide auditory information in the tactile domain. The Emoti-Chair was the vibro-tactile device for the Vibro-Motion because it was an entity backed by prior research as a vibro-tactile display. The system they designed allows tactile perception of auditory music using embedded voice coils in the chair. There are a total of sixteen embedded voice coils in the Emoti-Chair, which are located along the back, arranged into eight channels, where each channel consists of two voice coils (Figure 17). These voice coils make it possible to feel the vibrations on one

41

back when sitting in the chair. Tactile display alternatives were assessed and the sound exciters were found to be the best technology for the Emoti-Chair. The Emoti-Chair is the sensory substitution technology used by the Vibro-Motion.

Figure 17: The Emoti-Chair with the voice coils exposed.

There are two strategies which can be used to translate audio data into vibration. These are known as the track based model and the frequency based model.

The track based model allows live music or recorded music to be translated into a tactile experience through the chair. Its design allows each musical track to be played exclusively through one channel, which therefore allows up to 8 instruments to be mapped through the chair.

42

The Frequency based model allows live or recorded music to be translated into a tactile experience based on the frequency of the signal. This design method takes a full range audio signal and splits it into eight different frequency bands. The frequency scale (i.e. definition of the eight frequency bands) used in this design is the Bark scale (see Figure 18) (Zwicker, 1961).

Figure 18: Bark Scale.

2.1.8.1.1 Amplifiers: The Emoti-Chair requires amplifiers to power the eight channels. Two pyramid audio amplifiers were used for this purpose as each amplifier was capable of powering four channels (Pyramid, 2010).

Figure 19: The Pyramid Amplifier (Pyramid, 2010).

43

2.1.8.1.2 Firepod: A Digital Audio Workstation (DAW) was used to convert digital audio signals to analog audio signals and to also allow communication between the computer and the amplifiers. The PreSonus Firepod was the DAW used for this thesis project (PreSonus, 2010).

Figure 20: The PreSonus Firepod (PreSonus, 2010).

2.1.9

Conclusion

To summarize, this literature review provided insight into the current state of human-computer music interfaces as well as gesture-based input control and vibro-tactile feedback that could be employed with HCMIs. The information and guidelines reviewed were used to inform the design and evaluation of my research project, the Vibro-Motion, as it is a system that uses gesture control and vibro-tactile feedback. The system was built using a many-to-one input/output mapping with the aim of creating a system that would accept different input gestures that could control the set of voice coils that provide vibro-tactile output in the Emoti-Chair. A detailed description is provided in Section 3.The skin sensitivity limitations provided by physiological research were used as a guideline for setting the frequency limits of the system. In addition, one of the important aspects that seems to be missing from much of the literature 44

reporting on HCMI systems is that they are not evaluated with users or the evaluation is limited. I wanted to ensure that I not only designed a gesture-based, vibro-tactile system, but also I wanted to understand how users would interact with it. This includes whether users could actually control the system by replicating patterns they felt and producing their own patterns as well as whether it is an enjoyable experience. Section 4 provides the results and discussion of a user study of Vibro-Motion.

45

3 System Design, Implementation and Evaluation Methodology
This chapter presents the design of Vibro-Motion and the technologies it uses as well as the methodology used in the evaluation of Vibro-Motion.

Before implementing the system, the gesture tracking capabilities and Microsoft Kinect sensor limitations had to be considered. The information gathered in the researched literature was used as a basis for the gesture set which was mapped to the vibrations on the Emoti-chair. In addition to this preliminary work, it was important to consider the limitations of human arm movement as well as the comfortable arm-reach area in order to design a comfortable gesturing space for the Vibro-Motion. Research in ergonomics suggests that the design of gesture zones can be modeled to design good work space zones (Oborne, 1987). The design of the Vibro-Motion's gesture zones will be discussed further in Section 3.2.3.

3.1.1

Overview of the Vibro-Motion System design

An overview of the Vibro-Motion System can be seen in Figure 21. As seen in this figure, the user interacts with the system by gesturing with his/her hands towards the Kinect. These hand gestures are processed by a computer program written in C# and MAX/MSP which creates visual and vibro-tactile feedback. The vibro-tactile feedback is presented to the user through the Emoti-Chair, whose vibrations reflect the hand gestures in real time. The visual feedback, also presented in real-time, contains animated circles which visually depict how the produced vibrations feel. Each of these elements are discussed in more detail in the following sections.

46

Figure 21: The system design diagram.

3.2 Program flow:
The Vibro-Motion program has two stages: 

A preliminary calibration stage for the next user where the range of arm movement is defined for that user.



The compositional stage, the core of Vibro-Motion, where the user interacts with Vibro-Motion with hand and arm gestures to create vibro-tactile patterns for the Emoti-chair.

47

3.2.1

Calibration:

The calibration stage is the first stage of the Vibro-Motion program. This stage is required to set the parameters for the virtual space. To do this, the users are asked to move their hands around in the space in front of them. During this stage, the program records the user's left hand reach and right hand reach (see Figure 23 and 24). The program also captures the furthest and closest right hand positions from the Kinect sensor (see Figure 25). Using these ranges of movement, the calibration stage creates a comfortable gesture zone based on the user's comfortable reach positions. This is necessary so that users do not fatigue as easily or are required to reach beyond their comfortable area as recommended by Canadian Center for Occupational Health and Safety, 2005.

To create the comfortable reach area, users are also asked to position their hands to match the hand positions that appear on the screen marked with a hand icon. The hand positions captured by sensors are soft-coded into the program, and appear at different distances from the body depending on the user's size. Users are instructed to not over-extend their arms for calibration, and are instead asked to move their hands in the general direction of the hand icon on screen. Once the user has reached the hand icon, a timer begins and the user must hold their arm in this position for ten seconds at each of the ten calibration points. The system then records the X, Y and Z co-ordinates of the user's hand. The Z coordinates are taken from the Kinect's depth measure while the X and Y co-ordinates are taken from the screen X and Y co-ordinates. Although the X, Y, and Z coordinates are measured during calibration, only the Y and Z coordinates are used during the study, the Y-coordinate to detect vertical movements of left and right hands which control frequency and vibrational position respectively, and the Z-coordinate to detect the movements of the right hand to control the strength of the vibration . Since horizontal

48

movement was not used to control the Vibro-Motion system, data from the horizontal movement of the left and right hands were not used in the study.

Once the calibration is complete, the set of variables obtained are used to create a virtual space, called the "gesture zone" in which the user can create vibro-tactile feedback for Vibro-Motion by moving their hands. The program offers to save all user calibration data in a text file that can be re-loaded at a later time.

Figure 22: The calibration window demonstrating a user calibrating their left hand.

49

Figure 23: The calibration window while a user is calibrating their hand distance furthest from the Kinect sensor.

Figure 24: The calibration window while a user is calibrating their hand distance closest to the Kinect sensor.

Due to the format of the live video steam provided by the Kinect, the visual feedback shows the user's hand movements as they would be seen in a mirror.

50

3.2.2

Vibro-Tactile Pattern Composition:

The gesture window is the area where the user's right and left hands are tracked by the program and the vibrations are produced in response to the gestures (Figure 29). 3.2.2.1 User Interaction: As discussed previously in Section 2.1.2, the user interaction with the system was designed to take advantage of real-world gesture archetypes similar to those discussed by Argenot (1973). In VibroMotion, hand gestures are used to control the frequency, vibrational strength and position of the vibrations produced through the voice coils on the Emoti-Chair as follows: 

As the user moves his or her right hand closer and further from the Kinect sensor in VibroMotion, the strength of the vibration increases and decreases respectively. The actions of moving this hand closer and further away from the Kinect can be interpreted as giving and taking the strength from the vibration, respectively.



The high and low frequencies are also gestured naturally by moving the left hand high and low, respectively; the frequency increases and decreases as the left hand moves up and down, respectively, in the gesture space.



The position of the vibration on the Emoti-Chair is mapped similarly to the frequency, but with the right hand: as the user moves his or her right hand up, the vibration moves up on the EmotiChair; moving the right hand down causes the vibration to move down on the Emoti-Chair.

51

3.2.3

Gesture Zones

As a participant completes the calibration phase, Vibro-Motion creates a gesture zone that is bounded by the participant's comfortable reach areas (see Section 3.2.1 for how this is determined). Visual feedback showing the boundaries of this zone as well as the user's position within it is provided to the user on the screen via operating range markers, and animated circles. 3.2.3.1 Operating Range Markers: Since the system does not have a physical object which gives immediate sensory feedback as with an ordinary musical instrument, visual markers on the screen, called "Operating Range Markers", are used to help the user monitor the range within which the user can operate the system and visualize their gestures within that range (see Figure 25, Figure 26, Figure 27 and Figure 28). These markers are displayed on the screen as coloured vertical square brackets from the bottom of the range to the top of the range. The left bracket on the screen is the left hand frequency marker. It displays the full operating range and is divided into three brackets: a top green bracket for the upper frequency range, a middle red bracket for the middle frequency range, and a bottom red bracket for the lower frequency range. The right hand's operating range is displayed by the yellow bracket which displays the lowest and highest limits of the right hand. These ranges are used to inform the user visually of the space within which to operate the program as shown in Figure 25, Figure 26, Figure 27 and Figure 28. 3.2.3.2 Animated Circle Visualization: In addition to the operating range markers, two animated circles appear on the screen to provide the user with useful information regarding the vibration that is played through the Emoti-chair. These circles are displayed on the right side of the screen and change their shape relative to the tactile output 52

delivered to the user. If the user moves their left hand up to alter the frequency, the green animated circle also changes position and moves up. At the same time the thickness of its outline is decreased to depict the thin, pinching sensation associated with high frequencies. Once the user moves their left hand down, the circle changes position downward and the thickness of the its outline is increased to depict the rough tactile sensation associated with low frequencies. The design of the animated circles was based on research by Marks (1989), who suggested that there is a relation between the physical or virtual size and frequency. Research (see Pouris & Fels (2012)) also found a relation between spatial positioning and frequency, where a higher position is associated with higher frequencies and lower position is associated with lower frequencies.

Similarly, user movement of the right hand affects the circle that visually represents signal amplitude and position. Once a user moves their hand closer to the Kinect sensor, the white animated circle responds by growing larger in size as the strength of the vibration also becomes stronger. Correspondingly, the circle decreases in size as the tactile sensation weakens when a user moves their right hand away from the Kinect sensor. Movements by the right hand in upward and downward motions will correspondingly change the position of the circle as it follows the user's hand movement up and down. These changes can be seen in the figures below.

53

Figure 25: The gesture window as it is being used by a user. The operating range is the area bounded by the operating range markers, i.e. the square brackets. In this example, the thick outline of the green frequency circle and its low position depict a low frequency output to the Emoti-Chair. The medium size of white vibrational circle depicts a medium vibrational strength of output to the user. The high position of this circle indicates that this vibration will be located high on the Emoti-Chair (closer to the neck)

Figure 26: The Emoti-Chair emits a weak (small white circle) high-frequency vibration (thin high green circle) in a mediumlow position (position of white circle)

54

Figure 27: The Emoti-Chair emits a strong (large white circle) low-frequency vibration (low thick green circle) towards the top of the chair (position of the white circle).

Figure 28: The Emoti-Chair emits a very weak (tiny white circle) medium-frequency vibration (medium green circle in medium frequency zone) in the middle of the chair (position of white circle)

55

3.3 Vibro-Motion Equipment:
This section discusses the equipment that makes up the Vibro-Motion system. The hardware components: the Kinect, the Nyko Zoom, and the Emoti-Chair system are discussed in Section 2.1.3.2. The Kinect's limitations and software components are discussed in this section. 3.3.1.1.1 Limitations of the Kinect System: There were several limitations of the Kinect system that needed to be addressed before an evaluation could begin. The first limitation of the system is that any excess light reflected or directed to the IR sensors causes the depth stream analysis to fluctuate. This sensitivity to excess light resulted in a great deal of variable and unpredictable skeleton jitter for a user. To minimize this effect, room lights had to be dimmed using semi-opaque sheets of paper directly applied to light fittings. This reduced the amount of light reflected from the walls that was creating excess light tracked by the IR sensors. This change also provided more controlled lighting for improved data input for user tracking.

Other limitations of the Kinect sensor are that the sensor was primarily designed to track users in a standing position as they make wide and dynamic gestures. This limitation was important to consider as users of Vibro-Motion are required to be seated in the Emoti-Chair in a reclined position. As the recliner design of the Emoti-Chair was in conflict with the Kinect sensor and created excessive jitter in the skeleton image tracked by the Kinect, the Emoti-Chair hardware was refitted to a new chair that supported an upright seating position. This solution was effective in overcoming the skeleton jitter problems, resulting in fewer system resets and fewer system errors.

56

Even with all of the available precautions taken, it was not possible to remove all skeleton jitters that were tracked. This skeleton jitter was the result of the proprietary tracking algorithm that was frequently making minute adjustments to the tracked skeleton. Whilst no amendments can be made to the algorithm from which skeleton jitters originate, it was observed that this automatic system adjustment did not have any effect on the functionality of the program. This was because the frequency and vibrational changes caused by the automatic adjustments were too small to be detectable by the participants. Latency between the input and the response was a limitation of the Kinect. The latency tests which were performed suggested that the Kinect's latency borders on what is acceptable for a realtime system, one latency test resulted in a latency of 80 ms, while another test resulted in a latency of 35-40 ms (Wunschel, 2011; Synthhead, 2011).

Another limitation of the Kinect system is that its tracking capability is limited to a distance of 3.05 meters (10 feet). Minor adjustments are required in order for the Kinect to track people at this range (Rautaray & Agrawal, 2010; Zhang, 2012). This limitation did not affect the study as the participants were seated within the operating range of the Kinect's sensors.

3.3.2

Computer System specifications:

The specifications of the computer used for the study are:

Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40Ghz 4 GB RAM 64 Bit Windows 7 Enterprise Operating System 57

3.4 Software:

3.4.1

Vibro-motion software

The Vibro-Motion software was written in C# and Max/Msp. The components written in C# handle the skeleton joint data acquisition, gesture processing and mapping required to produce the vibrations for the Emoti-Chair. The Max/Msp component is responsible for connecting the PC with the Emoti-Chair system and generating the vibrations calculated in C#. The skeleton joint acquired by the Kinect provides data indicating the three dimensional positions of the joints and also includes the corresponding screen position of each of the joints. The X and Y co-ordinates of the joints also correspond to the screen X and Y co-ordinates, whereas the Z co-ordinates are calculated based on the distance of each joint from the Kinect sensor.

3.4.2

Kinect SDK:

The demo version of the Microsoft Kinect software development kit (SDK) was used as it had sufficient capabilities for this project and any additional functionality was not necessary. The sensor is capable of tracking up to two people at a time, and operates at a maximum of 30 frames per second (FPS). The SDK libraries also have access to the individual joint locations of both skeletons in the 3-D space in relation to the Microsoft Kinect sensor. For the Vibro-Motion, individual joint data was used to track gestures performed by the users. Since the Kinect is capable of tracking two people at a time, the Vibro-Motion could also be designed to track two people at a time, however, for the purpose of the study, the VibroMotion system only tracks one user.

58

3.4.3

Ventuz OSC package:

The Ventuz OSC (Open Sound Controller) is a free Microsoft .Net package which allows the implementation of the OSC protocol (Ventuz, 2006). This package allows communication between Microsoft .NET programs and MAX/MSP through the use of the User Datagram Protocol (UDP).

3.5 Process / Code Structure / Design:

3.5.1

Frequency:

The left hand vertical movements that fall along the y-axis are used to control frequency output. To provide better ergonomics, the left hand's Y positions on screen are calculated and classified into one of three zones: the high zone, mid zone or the low zone (see Figure 29). The total frequency range of the system is 300 Hertz (Hz). The "low zone" of this frequency range is 0 Hz to 30 Hz, the "mid zone" range is from 30 Hz to 270 Hertz and the "high zone" range is from 270 Hertz to 300 Hertz. The frequencies produced by the left hand's movements are mapped into frequency zones, as seen in Table 1, and further discussed in the next section.

59

Figure 29: The gesture window, indicating the three frequency zones and vibration position on the right side of the window.

Table 1: Frequency zones and corresponding frequencies.

Frequency Zone

Frequency Range

Frequency Offset

Low Zone

0 Hz Â­ 30 Hz

0 Hz

Middle Zone

30 Hz Â­ 270 Hz

30Hz

High Zone

270 Hz - 300 Hz

270Hz

Once the system has determined the corresponding frequency zone for the left-hand position, the frequency that is to be played on the Emoti-Chair is calculated using the following formula (where Zone Size is the range of the frequency zone in hertz: the size of high and low zones is 30 Hz and the size of

60

the middle zone is 240 Hz. FrequencyOffset refers to the ZoneSize of the frequency zone beneath the current frequency zone):

(

)

Figure 30: Formula for calculating frequencyOut for all Zones.

Once the output frequency (FreqOut) has been calculated, the program communicates with the Max/Msp patch which plays the frequency through the Emoti-Chair. For more details regarding how the Max/Msp patch works, refer to Appendix F.

3.5.2

Amplitude:

The proximity to the Kinect sensor of the user's right hand, which is measured on the z axis of the gesture space, determines the amplitude or strength of the vibration felt on the chair. The closer the right hand is to the sensor, the stronger the signal, and as the right hand moves away from the sensor, the vibrational strength decreases. The two calibration points, with the right hand close to the Kinect and the right hand at the furthest distance away from the Kinect, are used to calculate the maximum and minimum limits of vibrational strength provided through the Emoti-Chair.

In order to calculate the vibrational strength the z-coordinate of the user's right hand is measured in the sensor's z-directional range. The difference between the calibrated maximum (closest) right hand position and the current right hand position is divided by the total z-directional operating distance see Figure 31. The resulting value is multiplied by the system maximum amplitude to calculate the output 61

amplitude. If the right hand is closer to the Kinect sensor than the closest calibrated point, the amplitude output will be set to the maximum amplitude see Figure 32.
Vibrational Strength = Maximum Vibrational Strength Current Right Hand Distance Closest Calibrated Distance Ã Furthest Calibrated Distance Closest Calibration Distance

Figure 31: Formula for calculating the vibrational strength of the signal based on the right hand's distance from the Kinect sensor.

if (Vibrational Strength > Maximum Vibrational Strength) { Vibrational Strength = Maximum Vibrational Strength; }

Figure 32: Vibrational strength adjustment.

3.5.3

Position:

The position of the vibration applied to the Emoti-Chair is tracked using the vertical movement (y direction) of the right hand. Although an ergonomic non-linear zonal system similar to how the left hand controls the frequency was considered, it was decided that the zonal system was not appropriate for the vibrational positions because the voice coils are evenly and linearly laid out along the back of the EmotiChair. Instead, a linear mapping was chosen. The position of the vibration on the Emoti-Chair can be controlled by moving the right hand up and down; moving the right hand up causes the vibration to go

62

up along the back of the Emoti-Chair while moving the right hand down causes the vibration to move down.

Two design strategies were considered in order to provide vibrational feedback. Both strategies used a vibrational window, which is a vibrating area on the Emoti-Chair centred around the position on the Emoti-Chair that corresponds to the position of the user' hand. A vibrational window distributes the vibrations over multiple channels in order to make the vibration and its transition feel natural for the user. In the first design strategy the width of the vibrational window was set to be the same as the width of one channel (which thus distributed the vibration over at most 2 channels) and the vibration distributed evenly in the vibrational window. This first design was discarded after testing on this prototype revealed that the signals on the chair did not feel natural to the users. Although this first design was discarded, some of its ideas were reused in the design of the second algorithm. In the second design strategy, the vibrational window is wider and the strength of the vibration is not distributed evenly within the window but rather tapers off at the edges. A detailed description of both algorithms for position assignment can be seen in sections 3.5.4 and 3.5.5.

3.5.4

Vibrational window design in the first design strategy:

The vibrational window designed in the first strategy consists of a single window section which is centered on the right hand's current position on the y-axis. The affected channels are simply assigned vibrational strengths based on how much percentage of the vibrational window overlaps the channels.

The channel position tracker uses a signal window to chart the position of the right hand on the screen. When the hand positions are used to calculate the vibrational window, there are four cases that could

63

occur: vibrational window is completely within channel one (Figure 33), vibrational window is completely within channel eight (Figure 34), the vibrational window is located somewhere in between two channels (Figure 35) and the vibrational window is located exactly on a channel (see Figure 36).

Figure 33: Example of Case 1. Vibrational window lies entirely in Channel 1.

64

Figure 34: Example of Case 2, where the vibrational window lies entirely in Channel 8.

65

Figure 35: Example of Case 3 where the vibrational window lies across two channels.

66

Figure 36: Example of Case 4 where the vibrational window lies exactly on one channel

As can be seen in the diagrams, because the vibrational window is the width of a single channel, depending on the position of the user's right hand, the vibrational window either falls entirely within one channel, or is split between two channels. When the vibrational window overlaps only one channel, then the full strength of the vibration is simply applied to this channel. However, when it overlaps two channels, the strength of the vibration is divided amongst the two channels in proportion to the overlap of the vibrational window on each channel.

The Vibro-Motion pilot study (Section 3.7) showed that the movement of the vibrations between channels was not pleasant for the users and felt too sharp. In order to produce a vibration that feels 67

smoother, I redesigned the vibrational window to involve more than two channels and degrade more smoothly.

3.5.5

Vibrational window design using the second design strategy:

In the second design strategy, I used a vibrational window with blurred edges. This is accomplished by splitting it into three sections (Figure 37); the middle section contributes to 80% of the vibrational strength, while the two neighbouring sections contribute to 10% strength each.

Figure 37: Vibrational Window with the three sections.

This vibrational window is then applied to the affected Emoti-Chair channels as follows:

1. The current location of the right hand is identified, and the vibrational window is overlapped on the Emoti-Chair's channels. 2. Percentages of vibrational strengths are allocated to each of the channels; this is done by checking the overlap of each of the vibrational windows' sections over the channels.

68

3. The amplitudes for each channel are calculated based on the vibrational strengths calculated in step 2. Further explanations can be found in Appendix F.

3.5.6

Communication between C# and Max/MSP:

Once the output frequency and channel amplitudes are calculated in C#, the C# program communicates with MAX/MSP which produces the vibrations using the frequency, strength and the channel positions assigned using the vibrational window. The MAX/MSP patch is a separate software language which allows the C# program to communicate and control the Presonus Firepod; it produces the vibrations on the Emoti-Chair based on the instructions provided by the C# program.

3.6 Final Version of the Vibro-Motion:
The source code for the final version of the program is available at

https://www.dropbox.com/sh/e4blsukqgaaryhv/6EU6-7kOPf in order to facilitate future improvements or modifications. The final version of the program currently contains the study software as a learning module to help users learn to use the system.

69

3.7 Vibro-Motion Study:
The research questions, the Vibro-Motion pilot study, and the final procedures for the evaluation of the Vibro-Motion are discussed in the next few sections. The pilot study was carried out to test the original procedure for the evaluation study, and to find and correct any issues that arose in this original procedure. The pilot study was also useful in developing the second design strategy. The issues and corrections found are discussed in section 3.10 and the final procedure for the evaluation study is presented in Section 3.11.

3.8 Research Questions:
In the first chapter of this thesis, some of the broader research questions that sparked the development of this thesis are described: what does it mean to create a tactile equivalent to sound-based music, and can this tactile equivalent have a similar emotional impact on people? These questions are beyond the scope of a Master's thesis, particularly since the idea of composing vibro-tactile patterns is still in its infancy. Instead, the focus of this thesis is on preliminary issues: is it possible to design a vibro-tactile instrument for the Emoti-Chair, i.e., is it possible to design a gesture-based user interface for the EmotiChair that is intuitive, and fun to use, and which can be used to generate vibro-tactile feedback? To answer this question the Vibro-Motion interface described earlier was designed and implemented, and then a usability study was conducted to begin assessing the quality of its design in order to guide its future direction. This study was designed to answer initial questions about the possible playability of this new instrument. Some of these questions can be determined objectively:

70

1. How do changes in frequency values, vibrational strength and position of a vibro-tactile stimulus presented to a user through the Emoti-Chair influence the user's accuracy in mimicking that stimulus? 2. What is the impact of music experience and gesture gaming experience on user ability to control the frequency, strength and position of vibrations? The next two questions are more subjective, but nonetheless just as important to understanding the playability of Vibro-Motion: 3. What is the participant's perception of the controllability of the Vibro-Motion system (accuracy, and ease of control and use)? 4. What is the user's experience with and enjoyment of the Vibro-Motion system? These four questions are the focus of the usability study and form the research questions of this thesis.

3.9 Mimicry
The first usability question in this thesis is about mimicking a stimulus. In this section, the justification for selecting mimicry as a way of measuring usability will be discussed.

Vibro-Motion is a completely new user interface for creating vibro-tactile patterns. None of the users or participants in the VIbro-Motion study would have any experience with it, and therefore they would all have to learn how to "play" it, i.e how to use the Vibro-Motion user interface to produce vibrational patterns on the Emoti-Chair, either before or during the user study. It was decided to incorporate this learning process into the early phases of the study. To determine a strategy to support this learning, I investigated the various strategies used for learning how to play a musical instrument. There are three

71

main methods available to facilitate learning how to play a musical instrument: symbolic representation, visual notation, and mimicking.

Symbolic learning aids are similar to a guitar tablature which gives a visual representation of where the various fingers should be placed on the neck of guitar (Eckels, 2009). This method did not seem appropriate for the Vibro-Motion because a standard gesture placement aid was not available or developed as it was too early in the development of the Vibro-Motion system. Once Vibro-Motion has been further refined, a similar aid could be produced.

Learning an instrument through a visual representation uses formal music notation such as notes, key signatures, bars and clefs (Spruce, 1996). This method was not chosen because there was no standardized notation system for vibro-tactile patterns and converting formal music notation into some vibro-tactile equivalent was beyond of the scope of this thesis.

The training process of learning a musical instrument through mimicking involves mimicking someone else's work by repeating a piece played by another musician (e.g., this is usually called "playing by ear") without any formal representation system (McGrain, 1990). Understanding how people could learn to produce vibro-tactile patterns using mimicking seemed to best fit the study design of the Vibro-Motion system. This is therefore the approach I decided to take: the first two phases of the study consisted of asking the user to mimic vibrations they felt. For each vibration, the participants were first in a "learning mode" where they could repeat their gestures as much as they wanted until they felt that they had learned how to produce the vibration.

72

3.10 The Vibro-Motion Pilot Study:
In the early design phases of Vibro-Motion, an evaluation protocol to collect information for the four research questions was designed. A pilot study was carried out with five volunteers to test the evaluation protocol, the equipment and the data recording system. As a result of this pilot, changes were made to the protocol:

1. The first design method for the vibrational window was discarded and the second design was adopted. 2. Originally I planned to have participants mimic with their hand movements a set of 54 vibrational patterns. However, participants could not complete the study for this many patterns within the one-hour time limit of the study. The maximum number of patterns that participants could complete within study time was 16. The number of patterns for the formal study was thus reduced to 16. 3. Based on user suggestions during this study, it was found that additional information regarding where people's hands were with respect to their calibration limits and their operating zone was required. This was made explicit by displaying animated circles that represented the hands and a line indicating the operating range was displayed on-screen for participants to view (as seen Figure 26, Figure 27 and Figure 28). 4. The user interface contained bugs which corrupted the data recorded by the Vibro-Motion. These bugs were fixed and the evaluation procedure re-tested to ensure that the recorded data would be clean and uncorrupted. 5. Pilot study participants suggested that a countdown timer would help them know when the next stimulus would be presented, and they could prepare for it. A countdown timer indicating how long it would be until the next stimulus was added to the user interface. 6. It was found that small jitters in the skeleton tracking system caused major jumps in frequency which affected the accuracy of the captured data. These jitters were caused by the minor inconsistencies in the Kinect sensor as discussed in Section 1.1.1.1.1. As a result, the effect of these jitters on the vibrations produced by the Emoti-Chair was minimized. 73

The vibro-tactile frequencies developed by Branje & Fels (2012) were used as a guideline for the frequencies selected for the individual stimuli in the pilot study and also the evaluation study. These vibro-tactile frequencies were selected because they satisfy Weber's law of just noticeable difference; the frequency differences between notes in Vibro-Motion are greater than the minimum threshold for noticing differences between frequencies (Hugh, 1911). Table 2 shows the frequency notes:
Table 2: Vibro-tactile notes and corresponding frequencies developed by Branje & Fels (2012).

Note Frequency

1 40

2 56

3 78.4

4 109.76

5 153.66

6 215.13

7 301.18

3.11 Vibro-Motion Evaluation Study:
The purpose of the evaluative study was to explore a user's ability to mimic vibro -tacile stimuli presented to the user on the Emoti-Chair. Participants would use gestures as described in 3.2.2 to try and mimic those stimuli. The independent variables were frequency, strength, position and time. The dependent variables were accuracy, time and the usability factors of ease of use, ease of system control and enjoyment. Two additional factors, gesture gaming experience and musical training experience, which were thought to potentially have an important impact on participant performance in the study were also included. For the purpose of the study, participants with one year of music training have been considered to be musically experienced and participants who have used a gesture gaming device at least once have been considered to be experienced with gesture gaming devices.

The study was carried out in three phases. In the first two phases the participants were asked to feel vibrations (called stimuli throughout this thesis) presented to their back through the Emoti-Chair and then mimic those vibrations using their hand gestures. In Phase 1 the vibrations were static: they did 74

not change during the stimulus time; in phase 2 the vibrations were dynamic: the frequency, position or the vibrational strength, or any combination of those varied gradually during the stimulus. In phase 3 the participants were asked to play freely with the Vibro-Motion system and to try to compose a vibrational piece which lasted between 15-30 seconds.

The evaluation method for the Vibro-Motion system was approved by the Ryerson Ethics Board (see Appendix A). The procedure required that all study participants be over eighteen years old. Demographic data was collected from participants including their names, ages and genders. All participant information was connected to their study data using only a numbered identifier code.

Twenty-nine participants were recruited for the study using email and campus advertisements. While twenty-nine participants were recruited for the study, the data from four participants were not used due to technical errors that occurred in data acquisition. As a result, the data of only twenty-five participants (sixteen male and nine female) are reported in this thesis.

Twenty participants were aged between 18-24 years old and five were aged between 25-34 years old. Eleven participants had musical experience (at least one year of musical training), while the remaining fourteen participants identified themselves as having no musical experience. Eight participants had gesture gaming experience (i.e. they had played at least one game involving gestures) and seventeen did not have any gesture gaming experience. None of the participants had prior experience with the Microsoft Kinect.

75

3.12 Study Procedure
When the participants arrived, they were required to read through the study information form and sign a consent form prior to participating in the study (see Appendix B). Participants were given the opportunity to ask questions about the study and the procedures.

Once consent was given and their questions about the study were answered, participants were then asked to complete a 13-question pre-study questionnaire (see Appendix C), which gathered the background information of the participants. During all phases participants wore ear plugs and headphones playing white noise at a level that ensured that they were unable to detect any sound produced by the voice coils in the Emoti-Chair. All participants were informed that they were able to take breaks and ask questions about the study or procedures at any time throughout the study.

The participants were then asked to sit in the Emoti-Chair and to begin system calibration of their movements (see Section 3.2.1 for a detailed explanation of the calibration procedure). To calibrate the system, participants were asked to face their palms towards the Kinect and to follow the hand icon that appeared on the screen. To ensure they were using an optimum ergonomic position, participants were asked to not fully extend their elbows at any point. Once the calibration process was complete, participants were informed as to how the system worked and how to control the various features of the Vibro-Motion system (as described in the Section 3.2.3). It was explained that right hand movement controlled the strength of the vibration. As the right hand moved closer to the Kinect sensor the vibration would get stronger and as it was moved further away from the Kinect, it would get weaker. Participants were also informed that the right hand controlled the position of the vibration (as the right hand was moved higher along the body, the position of the vibration moved higher on their back and as

76

the right hand was moved lower, the vibrations would be located on their lower back and legs). The pitch could be increased or decreased by moving the left hand up or down from the mid-line of their chest, respectively (higher pitch was up above the mid-line of the chest and lower pitches below).

The participants were given approximately five minutes to explore the system and become familiar with how it worked using their hands. Participants were provided with additional instruction on how to operate the interface (e.g., the meanings of the buttons and the procedure for submitting their final attempt).

Once participants were ready to proceed to the Phase 1 of the study, they were asked to press the "Start" button to begin. Participants alternated between the Practice mode (see Section 3.12.1) and submitting their mimicked response to each stimulus in each phase. After each of the three phases of the study, participants completed a post-phase questionnaire (see Appendix C) which lasted about five minutes on average.

3.12.1 Practice Mode The purpose of this mode is for users to learn and practice mimicking vibrations that they feel through the Emoti-Chair in order to become familiar with the task that will be carried out in the formal component of the user study. The interface for this mode is designed to allow users to control their progress through the study. When they are ready to begin a stimulus mimicking task, they press the "Start" button on the touch screen. The target stimulus is then played. They can then use the "Replay" button to repeat the current stimulus as many times as they want. Once users have determined that

77

they have had sufficient practice with that one stimulus, they can move to the submit answer stage by pressing the "Attempt Answer" control to exit the training mode and submit their gestures.

Once they have submitted their attempt, they can advance to the next stimulus or go back to the previous stimulus if they want to retry their answer. They can repeat this procedure for each stimulus.

3.12.2 Phase 1 For the first phase of the study, each participant felt a target vibration (stimulus) from the Emoti-Chair and then was asked to use their hand gestures to replicate or mimic that stimulus that they felt. They were asked to replicate the stimulus as accurately as possible by matching the frequency, vibrational position and vibrational strength. They were also told that all vibrations lasted for a total of three seconds and that they should replicate the timing as well. They were also notified that while the program was playing a target stimulus, they could not perform any gestures as they would not be recorded. They were instructed to wait for entire stimulus to be played before attempting to mimic it. Participants were exposed to sixteen target stimuli; Table 3 and Table 4 show the frequency, position and strength values of each stimulus in phase 1. In addition to the 11 stimuli shown in this table, the participants were given four initial practice stimuli so that they could become accustomed to the format of the study and how to carry out the mimicking tasks, gaining a sense of holding their gesture positions for three seconds (these four patterns are not listed in Table 7 since they were not used for the evaluation of the study). The next eleven stimuli involved patterns where the frequency, vibrational strength and the vibrational position were held constant during the three-second interval presented to the participant. The participant then tried to mimic the stimulus. They were permitted to play the target stimulus as many times as desired using the Replay button and practice before submitting their final

78

response using the Submit button. The participant could advance to the next stimuli by touching "Next." The details of the stimuli and their parameters are shown in the table below; this table is provided again in Section 4.2. The initial and final frequencies, positions and vibrational strengths are constant for each stimulus in Phase 1 as this Phase measured how accurately participants mimicked static stimuli.
Table 3: The parameters used for the vibrations in phase 1 stimuli.

Initial Initial Static Stimuli Stimulus Frequency (Hz) Final Frequency (Hz) Initial Vibrational Strength % Final Position Vibrational on EmotiStrength % chair

Final Position on Emotichair

Vibrational 1 Strength 250 250 60 60 4 4

Vibrational 2 Strength 250 250 80 80 4 4

Vibrational 3 Strength 250 250 100 100 4 4

Frequency

4

40

40

90

90

4

4

Frequency

5

78

78

90

90

4

4

Frequency

6

154

154

90

90

4

4

Frequency

7

300

300

90

90

4

4

Position

8

110

110

75

75

2

2

79

Position

9

110

110

75

75

4

4

Position

10

110

110

75

75

6

6

Position

11

110

110

75

75

8

8

3.12.3 Phase 2: Once phase 1 was completed, participants were then exposed to dynamic stimuli where frequency, vibrational strength, vibrational position, or combinations of those variables changed as outlined in Table 3. To mimic these more complicated stimulus patterns participants had to move both hands through a series of gestures over the three second time interval. Similar to phase 1, participants were able to replay and practice the target stimulus patterns as often as desired before submitting their final response. A combination of frequencies, positions and vibrational strengths that were changing over the duration of each stimulus were presented to participants in phase 2.

Once participants completed phases 1 and 2, they were asked to complete an eleven question postphase questionnaire (see Appendix C) about their experience with Vibro-Motion and its ease of use and controllability, followed by a 10-minute break. This questionnaire asked for participants to evaluate their performance and to comment on what they found was positive and negative about the Vibro-Motion system. In this questionnaire, participants were also asked to rate their level of fatigue and cognitive workload. The details of the stimuli and their parameters are shown in the table below; this table is provided again in Section 4.2 (for details on how the Phase 2 stimuli are calculated and played on the Emoti-Chair, see Appendix F: Variable Test Cases).

80

Table 4: The parameters used for the vibrations in phase 2 stimuli.

Initial Initial Dynamic Stimuli Stimulus Frequency (Hz) Final Frequency (Hz) Initial Vibrational Strength % Final Position Vibrational on EmotiStrength % chair

Final Position on Emotichair

Frequency

12

110

215

90

90

3

3

Position

13

154

154

90

90

2

5

Vibrational 14 Strength 215 215 90 30 3 3

Frequency and Vibrational Strength 15 215 40 95 65 4 4

Frequency and 16 Position 56 154 90 90 7 3

3.12.4 Phase 3: For phase 3 of the study, participants were given the opportunity to experiment with the Vibro-Motion system to compose their own vibrational patterns (free-play). They were encouraged to make 81

compositions that last between 15 to 30 seconds. This final phase of the study lasted approximately five minutes. The participants were then asked to complete a phase 3 questionnaire in order to comment on: 1) their experience in phase 3; 2) what they had wanted to achieve; and 3) whether they accomplished their goal. Once they had completed phase three, they were asked to complete the remaining 12 questions in the post-phase questionnaire that asked them to measure how they thought they performed, how fatigued they were and their opinions about the Vibro-Motion system.

3.12.5 Analysis of Recorded Data: The recorded data were analyzed using Matlab and the following data points were extracted:

1. Total number of times a participant replayed a stimulus. 2. Participant movements tracked for right and left hands. 3. Corresponding data for the frequency, position and amplitude. Hand movement data were recorded every 2 or 3 milliseconds. Since the fastest human muscle twitch occurs every 20 milliseconds (Fischer & Ramsperger, 1984), all movement data would be recorded and interpolated within this timeframe. Recorded hand movements were plotted on a graph using Matlab to determine which movements were meaningful to the study.

82

Figure 38: One complete test case attempt by a participant, with their sudden dips in right hand activity movement as labeled. In this example, dip 1 corresponds to the participant clicking "Replay"; dip 2 corresponds to the participant clicki ng "Final Attempt"; dip 3 corresponds to the participant clicking "Next".

Figure 39: One complete test case attempt by a participant in the study. Multiple attempts for the same stimulus are highlighted.

As it can be seen in Figure 38 and Figure 39, various intended movements were easy to detect. For example, sudden dips in the position axis suggested that the participant was moving his/her hand toward the mouse to interact with the system (see Figure 38). This occurred when a participant clicked "Next", or "Replay" (a marker in the measurements was recorded when the participant clicked "Next" or "Replay" which occurred at the same time as the dip). These markers served as indicators for when a 83

participant's attempt began and finished. If the user clicked a button, and the graph continues, they have clicked the "Replay" button, and if the graph ends after the user has clicked a button, it can be concluded that the user clicked the "Next" button. The last portion of the graph can represents the final attempt by the user to mimic the stimulus. These peaks and dips of a participant's right hand position were used to identify the multiple attempts that each participant has performed (see Figure 39). Time stamps are used to plot the changes in frequency; position and vibrational strength in time. Time is graphed in milliseconds, while the vibrational strength is graphed based on the percentage of vibrational strength, the frequency is graphed in hertz, and the vibrational position is graphed by the screen pixel location recorded by the Kinect.

This response is compared with the expected values of each variable, i.e. the original target stimulus (see Figure 41). The error is then defined as the summation of the difference between the participant's responses and their expected response. For frequency, vibrational strength and right-hand movement, the errors are the sum of the shaded areas in Figure 42 which show the differences between the target stimulus and a participant's response. The time duration errors are also measured; a duration that lasts less than three seconds or over three seconds is considered a time duration error.

Where participants either exceeded or fell short of the three-second time window, only the time where the response and target stimulus overlap is assessed for frequency, position or strength error. Any remaining or missing sections are considered only time errors. Since the participants are asked to mimic the vibrations for three seconds, any data after the 3 second mark are removed for the analysis of frequency, position and strength errors, and are counted as part of time duration error.

84

Figure 40: Tracked user data showing the vibrational strength, right hand position, and frequency.

Expected Frequency (Hz)

Expected Expected Vibrational Strength Position (Screen (Max/Msp Magnitude Y-Coordinate) (Vibrational Strength %)) 40 169 136 (90%)

Figure 41: Expected vibrational input derived from the stimuli.

Figure 42: The Error areas of the stimuli

85





=





    







 

Figure 43: Formula for the Error Area.

The calculated error area totals and the error rate for frequency, position, and the vibrational strength, along with the time duration, time duration error and the total number of attempts were stored in a spreadsheet for statistical analyses. 3.12.5.1 Error Measurements 3.12.5.1.1 Frequency It is unclear what the relationship is between a frequency applied to the skin and ability to perceive it. For example, it is unknown whether an error of 50 Hertz for the target frequency of 100 Hz feels the same as an error of 50 Hz for the target frequency of 300 Hz. However, in western music the frequency scale is a logarithmic scale where the frequency of one note is multiplied by 2 (1/12) or 1.059 of the frequency of the previous note. The scale used in this study was developed for vibro-tactile interfaces by Branje & Fels (2012); this scale is also logarithmic where the frequency of one note is multiplied by 1.4. The frequency error (or frequency percentage error) is thus calculated using the following formula: 



(

)

86

3.12.5.1.2 Position The recorded screen position was not used for calculating the position error because the right hand's vertical ranges (determined for each participant during the calibration stage) were not uniform. However, the entire span of the right hand vertical range was divided into eight equal parts to match the eight channels of output by the voice coils where each part's size corresponds to the size of one channel. Since the channel sizes and their boundaries are relative to the right hand range of an individual, the position error (channel percentage error) was calculated using the formula below:  |(  



)|

3.12.5.1.3 Strength The vibrational strength percentage of the stimulus was used to measure the strength error. Verillo (1969) described the relationship between the strength of a vibrational stimulus and the subjective assessment of the magnitude of that stimulus as linear. He found that the magnitude/sensation function for vibro-tactile sensation was a power function with a constant slope of 0.89 for frequencies up to 350 Hertz. Since a linear relationship was found to exist between the subjective vibrational magnitude and the amplitude of a tacile stimulus, the vibrational strength percentage error was calculated simply using the following formula:     

87

Verillo (1969) also found a linear relationship between vibrational frequency and the perception of vibrational strength. However, the range of frequencies covered in his study was a subset of the frequencies used in my study. Although research exists showing a relationship between vibrational frequency and the perception of vibrational strength (e.g., Verillo, 1969), I did not include any effect of frequency in the vibrational strength calculation. A future study is suggested which extrapolates the relation between vibrational frequency and perception of vibrational strength determined by Verillo (1969) for all frequencies used in the study.

3.12.6 Statistical analyses of the user data. The user study is a mixed factor design with 4 within-subjects factors (frequency, position, duration and strength errors) and two between-subjects factors (gaming experience and music experience). The null hypotheses are that there are no differences in errors between stimuli for the within-subjects factors, and that there are no differences in errors for frequency, position, duration and strength errors between participants with/without gaming experience, and with/without music training experience. A repeatedmeasures ANOVA is carried out for the within-subjects factors and a t-statistic is used to analyse the between-subjects factors. Mauchly's sphericity test was performed on the ANOVA data to verify the condition that the variances of the differences in the independent variables were equal. Where the assumption of sphericity is not met for the repeated-measures ANOVA, the correction factor used to determine significance is that factor with the highest epsilon value (the factor used is reported with each affected result). Post-hoc, pair-wise t tests are then carried out to determine significant differences between stimuli for any significant ANOVA result. The mean, standard deviation and t-statistic are reported for the post-hoc, pair-wise comparisons of the significant ANOVA results in order to support

88

comparisons and conclusions made from the data. As three hypotheses are being tested, a Bonferroni correction, /3, (where  is the probably of falsely finding significance and set at 0.05 for this thesis) was used to lower the chance of finding significance where none exists. Only significant statistics (p<0.02) are reported for the error data. For a discussion of detailed description of the statistical procedures and reporting required for this type of data see (Field, 2005).

Likert scale data from the post-study questionnaires were analysed using chi-square and descriptive analyses to assess user responses (p-value of 0.05 was used).

89

4 Evaluation
This section presents the significant statistical results of the evaluation study in Section 4.1; an analysis and discussion of the meaningfulness of these results is presented in Section 4.2; and the limitations of my research are presented in Section 4.3.

4.1 Results

4.1.1

Frequency Error Analysis for Phase 1 Stimuli:

A repeated measures ANOVA was carried out with phase 1 data with stimulus as the within-subjects variable and frequency error, and gesture gaming experience as the between-subjects variables. The assumption of sphericity was not met, therefore a Huynh-Feldt correction (epsilon value = 0.37) was used. There was a significant main effect of vibrational stimulus for frequency error [F(3.71, 89.14)=1.28, p=0.01). There was no significant interaction and between subjects effects for gesture gaming experience and musical experience for phase 1 frequency error.

A paired t-test was carried out between frequency errors for phase 1 stimuli and there were 25 significant differences. The significant pairs and their respective mean and standard deviation were shown in the table below. All results have 24 degrees of freedom and are reported to a p < 0.02 level.
Table 5: Pairwise t-tests for significant phase 1 frequency pairs Mean (Frequency Error %) Pair 1 Pair 2 Stimulus 1 Stimulus 1 68.87 68.87 Mean (Frequency Error %) Stimulus 2 Stimulus 3 51.69 37.79

Std. Dev.

Std. Dev.

t

26.82 26.82

21.94 22.93

2.91 5.138

90

Pair 3 Pair 4 Pair 5

Stimulus 1 Stimulus 1 Stimulus 1

68.87 68.87 68.87

26.82 26.82 26.82

Stimulus 4 Stimulus 6 Stimulus 7

149.99 33.86 43.65

94.291 19.88 21.75

-4.028 4.8 4.114

Pair 6 Pair 7 Pair 8 Pair 9

Stimulus 1 Stimulus 2 Stimulus 2 Stimulus 2

68.87 51.69 51.69 51.69

26.82 21.94 21.94 21.94

Stimulus 11 Stimulus 3 Stimulus 4 Stimulus 6

46.72 37.79 149.99 33.86

39.042 22.93 94.291 19.88

2.805 3.142 -4.847 3.58

Pair 10 Pair 11 Pair 12 Pair 13

Stimulus 2 Stimulus 3 Stimulus 3 Stimulus 3

51.69 37.79 37.79 37.79

21.94 22.93 22.93 22.93

Stimulus 9 Stimulus 4 Stimulus 5 Stimulus 8

71.71 149.99 69.81 59.3

39.64 94.291 52.57 33.695

-2.52 -5.61 -2.843 -2.885

Pair 14 Pair 15 Pair 16 Pair 17

Stimulus 3 Stimulus 4 Stimulus 4 Stimulus 4

37.79 149.99 149.99 149.99

22.93 94.291 94.291 94.291

Stimulus 9 Stimulus 5 Stimulus 6 Stimulus 7

71.71 69.81 33.86 43.65

39.64 52.57 19.88 21.75

-3.768 3.856 5.784 5.064

Pair 18 Pair 19 Pair 20 Pair 21

Stimulus 4 Stimulus 4 Stimulus 4 Stimulus 4

149.99 149.99 149.99 149.99

94.291 94.291 94.291 94.291

Stimulus 8 Stimulus 9 Stimulus 10 Stimulus 11

59.3 71.71 48.25 46.72

33.695 39.64 28.18 39.042

4.323 3.648 5.215 4.817

91

Pair 22 Pair 23 Pair 24

Stimulus 5 Stimulus 6 Stimulus 6

69.81 33.86 33.86

52.57 19.88 19.88

Stimulus 6 Stimulus 8 Stimulus 9

33.86 59.3 71.71

19.88 33.695 39.64

3.387 -3.514 -4.456

Pair 25

Stimulus 7

43.65

21.75

Stimulus 9

71.71

39.64

-3.175

4.1.2

Frequency Error Analysis for Phase 2 Stimuli:

A repeated measures ANOVA was carried out with phase 2 data with stimulus as the within-subjects variable for frequency error and gesture gaming experience as the between-subjects variable. The sphericity assumption was not met therefore a Huynh-Feldt correction (epsilon value = 0.78) was used. There was a significant main effect for frequency error [F(4,72)=4.75, p=0.002]. There was no significant interaction effect between frequency error and gesture gaming experience [F(4,72)=0.73, p=0.5], and there was no significant effect between-subjects effect [F(1,18)= 0.107, p=0.75].

A repeated measures ANOVA was carried out with phase 2 data with stimulus as the within-subject variable for frequency error and musical experience as the between-subjects variable. The assumption of sphericity was met. There was a significant main effect for frequency error [F(4,72)=8.72, p=0.002]. There was no significant interaction effect between frequency error and musical experience [F(4,72)= 1.20, p =0.67], and there was no significant between-subjects effect [F(1,18)= 87.89, p=0.142].

A paired t-test was carried out between frequency errors for phase 2 stimuli and there were 7 significant differences (see table below). All results have 24 degrees of freedom and are reported to a p < 0.05 level.

92

Table 6: Pairwise t-tests for significant phase 2 frequency pairs Mean (Frequency Error %) Pair 1 Pair 2 Trial 12 Trial 12 42.23 42.23 Std. Dev. 23.96 23.96 Trial 14 Trial 15 Mean (Frequency Error %) 78 82.48 Std. Dev. 22.51 39.17

t

-6.356 -4.501

Pair 3 Pair 4 Pair 5 Pair 6

Trial 12 Trial 13 Trial 13 Trial 13

42.23 32.22 32.22 32.22

23.96 19.02 19.02 19.02

Trial 16 Trial 14 Trial 15 Trial 16

101.4 78 82.48 101.4

35.59 22.51 39.17 35.59

-6.911 -8.934 -5.847 -7.445

Pair 7

Trial 14

78

22.51

Trial 16

101.4

35.59

-2.67

4.1.3

Position Error Analysis for Phase 1 Stimuli:

A repeated measures ANOVA was carried out with phase 1 data with stimulus as the within-subject variable for position error and gesture gaming experience as the between-subjects variable. The sphericity assumption was met. There was a significant main effect of vibrational stimulus for position error [F(10,230)= 4.12, p=0.00]. There was no significant interaction effect between position error and gesture gaming experience [F(10,230)= 0.60, p =0.86], and there was no statistically significant between subjects effect [F(1,23)= 3.69, p=0.11].

A repeated measures ANOVA was carried out with phase 1 data with stimulus as the within-subject variable for position error and musical experience as the between-subjects variable. The sphericity assumption was met. There was a significant main effect of vibrational stimulus for position error [F(10, 320)= 4.16, p=0.00]. There was no significant interaction effect between position error and musical 93

experience [F(10,230)= 1.80, p=0.11], and there was no statistically significant between subjects effect [F(1,23)= 0.45, p=0.51].

A paired t-test was carried out between position errors for phase 2 stimuli and there were 18 significant differences (see Table 7). All results have 24 degrees of freedom and are reported to a p < 0.05 level.
Table 7: Pairwise t-tests for significant phase 1 position pairs.

Mean

Std. Dev.

Mean

Std. Dev.

t

Stimulus Pair 1 1 10.86 6.23

Stimulus 4.00 3 6.55 3.67

Stimulus Pair 2 1 10.86 6.23

Stimulus 5.14 5 7.00 3.10

Stimulus Pair 3 1 10.86 6.23

Stimulus 5.14 6 7.00 3.10

Stimulus Pair 4 1 10.86 6.23

Stimulus 5.71 7 7.14 2.82

Stimulus Pair 5 2 7.43 7.28

Stimulus 12.00 9 6.75 -2.87

Stimulus Pair 6 3 4.00 6.55

Stimulus 12.00 9 6.75 -5.53

Stimulus Pair 7 3 4.00 6.55

Stimulus 9.71 10 6.80 -2.83

94

Stimulus Pair 8 3 4.00 6.55

Stimulus 10.86 11 7.47 -4.10

Stimulus Pair 9 4 6.86 7.28

Stimulus 12.00 9 6.75 -3.17

Stimulus Pair 10 5 5.14 7.00

Stimulus 12.00 9 6.75 -3.67

Stimulus Pair 11 5 5.14 7.00

Stimulus 9.71 10 6.80 -2.87

Stimulus Pair 12 5 5.14 7.00

Stimulus 10.86 11 7.47 -2.83

Stimulus Pair 13 6 5.14 7.00

Stimulus 12.00 9 6.75 -3.67

Stimulus Pair 14 6 5.14 7.00

Stimulus 9.71 10 6.80 -2.55

Stimulus Pair 15 7 5.71 7.14

Stimulus 12.00 9 6.75 -4.34

Stimulus Pair 16 7 5.71 7.14

Stimulus 10.86 11 7.47 -2.57

Stimulus Pair 17 8 6.29 7.24

Stimulus 12.00 9 6.75 -2.83

Stimulus Pair 18 8 6.29 7.24

Stimulus 10.86 11 7.47 -2.55

95

4.1.4

Position Error Analysis for Phase 2 Stimuli:

A repeated measures ANOVA was carried out with phase 2 data with stimulus as the within- subject variable for position error and gesture gaming experience as the between subjects variable (see Section 3 for a detailed description of how all errors were derived). The assumption of sphericity was met. There was a significant main effect for position error [F(4,92)= 6.79, p=0.001]. There was no significant interaction effect between position error and gesture gaming experience [F(4,92)= 0.49, p=0.38], and there was no significant between subjects effect [F (1,23)= 0.99, p=0.15].

A repeated measures ANOVA was carried out with phase 2 data with stimulus as the within- subject variable for position error and musical experience as the between subjects variable. The assumption of sphericity was met. There was a significant main effect for position error [F(4,92)= 8.21, p=0.005]. There was no significant interaction effect between position error and gesture gaming experience [F(4,92)= 1.25, p=0.64], and there was no significant between subjects effect [F (1,23)= 0.80, p=0.8].

A paired t-test was carried out between position errors for phase 2 stimuli and there were 6 significant differences (see Table 8). All results have 24 degrees of freedom and are reported to a p < 0.05 level.
Table 8: Pairwise t-tests for significant phase 2 position pairs.

Mean (Position error %) Std. Dev.

Mean (Position error %) Std. Dev. t

Stimulus Pair 1 12 8.57 8.25

Stimulus 16.00 16 6.28 -3.38

96

Stimulus Pair 2 13 12.00 5.35

Stimulus 6.29 15 7.24 4.00

Stimulus Pair 3 13 12.00 5.35

Stimulus 16.00 16 6.28 -2.58

Stimulus Pair 4 14 11.43 5.83

Stimulus 6.29 15 7.24 2.82

Stimulus Pair 5 14 11.43 5.83

Stimulus 16.00 16 6.28 -2.87

Stimulus Pair 6 15 6.29 7.24

Stimulus 16.00 16 6.28 -5.42

4.1.5

Strength Error Analysis for Phase 1 Stimuli:

A repeated measures ANOVA was carried out with phase 1 data with stimulus as the within-subject variable for strength error and gesture gaming experience as the between subjects variable. The assumption of sphericity was met. There was no significant main effect for strength error [F(7.72,177.61)= 1.70, p=0.40]. There was no significant interaction effect between strength error and gesture gaming experience [F(7.72,177.61)= 0.73, p=0.53], and there was no significant between subjects effect [F(1, 23) = 2.74, p=0.88].

A repeated measures ANOVA was carried out with phase 1 data with stimulus as the within-subject variable for strength error and musical experience as the between subjects variable. The assumption of sphericity was not met and therefore a Huynh-Feldt correction (epsilon value = 0.73) was used. There was no significant main effect for strength error [F(7.34, 168.79)= 2.29, p=0.21]. There was no significant 97

interaction effect between strength error and musical experience [F (7.34, 168.79) = 1.29, p=0.37], and there was no statistically significant between subjects effect [F(1, 23) = 3.31, p=0.10].

4.1.6

Strength Error Analysis for Phase 2 Stimuli:

A repeated measures ANOVA was carried out with phase 2 data with frequency as the within-subject variable for strength error and gesture gaming experience as the between subjects variable. The assumption of sphericity was not met and therefore a Huynh-Feldt correction (epsilon value = 0.77) was used. There was a significant main effect for strength error [F(4,70.86)= 3.69, p = 0.01]. There was no significant interaction effect between strength error and gesture gaming experience [F(4,70.86) = 0.41, p=0.75], and there was no significant between subjects effect [F (1, 23) = 1.03, p=0.32].

A repeated measures ANOVA was carried out with phase 2 data with frequency as the within-subject variable for strength error and musical experience as the between subjects variable. The assumption of sphericity was not met and therefore a Huynh-Feldt correction (epsilon value = 0.78) was used. There was a significant main effect for strength error [F(4,72.13)= 4.09, p=0.01]. There was no significant interaction effect between strength error and musical gaming experience [F(4,72.13) = 1.85, p=0.14], and there was no significant between subjects effect [F(1, 23) = 0.37, p=0.55].

A paired t-test was carried out between strength errors for phase 2 stimuli and there were 3 significant differences (see Table 9). All results have 24 degrees of freedom and are reported to a p < 0.05 level.

Table 9: Pairwise t-tests for significant phase 2 strength pairs.

Mean

Std. Dev.

Mean

Std. Dev.

t

98

Stimulus Pair 1 13 12.16 7.89 Stimulus 14 24.20 18.10 -3.71

Stimulus Pair 2 14 24.20 18.10 Stimulus 15 13.74 10.48 2.86

Stimulus Pair 3 14 24.20 18.10 Stimulus 16 13.57 10.79 2.96

4.1.7

Co-variate Analysis:

A repeated measures ANOVA with covariate analysis was carried out with phase 1 data with strength as the within-subject variable for frequency error. The assumption of sphericity was not met and therefore a Huynh-Feldt correction (epsilon value = 1.00) was used. There was a significant interaction effect between frequency error and strength error on stimulus 4 [F(10,130)= 2.28, p=0.003].

A repeated measures ANOVA with covariate analysis was carried out with phase 2 data with strength as the within-subject variable for frequency error. The assumption of sphericity was not met and therefore a Huynh-Feldt correction (having the highest epsilon value = 0.87) was used. There was a significant interaction effect between frequency error and strength error on stimulus 12 [F(3.54,67.34) = 4.35, p = 0.005].

A repeated measures ANOVA with covariate analysis was carried out with phase 1 data with strength as the within-subject variable for position error. There was a significant interaction effect between strength and position on stimulus 4 [F(10,140)= 2.08, p = 0.01].

99

There was no significant interaction effect between position error and frequency error for phase 1 and 2 stimuli. There was no significant interaction effect between position error and strength error for phase 1 and phase 2 stimuli. There was no significant interaction effect between frequency error and strength error for phase 1 and 2 stimuli. There was no significant interaction effect between frequency error and position error for phase 2 stimuli. There was also no significant interaction effect between strength error and frequency error for phase 2 stimuli.

4.1.8

Questionnaire data:

A chi-square test was carried out for all Likert-scale post-study questions (phase 1, 2 and phase 3) to compare the participant responses with chance. There was a significant difference between actual responses and chance for 14 of the 25 total questions with p < 0.05 (see Table 10). Table 10 shows questions from 17 an onwards; questions 1-16 are related to the participants' background information and questions 22-25 are open ended questions.
Table 10: Chi-Square results for Questionnaire answers.

ChiSquare

df

Q17: Rate the fatigue of your arms

10.64

2

Q17: Rate the fatigue of your hands

21.44

2

Q17: Rate the fatigue of your legs

14.44

1

Q17: Rate the fatigue of your torso

28.88

2

100

Q18: How did you feel about the ease of changing the location of the signal?

9.08

3

Q18: How did you feel about the ease of repeating what you felt on the Emoti-Chair?

14.80

4

Q19: Indicate the level of mental demand you felt when using the Emoti-Chair and VibroMotion.

26.80

4

Q19: Indicate the level of temporal demand you felt when using the Emoti-Chair and Vibro-Motion

11.20

4

Q19: Indicate the level of effort you exerted when using the Emoti-Chair and Vibro-Motion

12.40

4

Q21: Rate your level of agreement with the statement: The vibrations on the Emoti-Chair did what I thought they were going to do.

11.00

3

Q21: Rate your level of agreement with the statement: I had fun.

11.84

2

Q21: Rate your level of agreement with the statement: I could not make the system do what I wanted it to do.

11.00

3

Q21: Rate your level of agreement with the statement: I am satisfied with what I produced.

10.04

3

Q26: How enjoyable did you find making vibrational patterns during free-play?

7.28

2

A crosstabs analysis was carried out with the two between-subjects groups, gesture and music experience for all post-study questions.

There was a significant difference in the participants' response to "being able to make the system what they wanted it to do" [2(3) = 9.29, M = 3.24, SD = 0.97, p < 0.05], and to their response to "strength of

101

signal vibration being important during free play", [2(1) = 4.74, M = 3.44, SD = 0.96, p < 0.05] for participants with/without gaming experience.

There was a significant difference in the participants' response to "frequency of the signal vibration being the most important during free play", [2(1) = 6.51, M = 3.44, SD = 1.12, p < 0.05] for participants with musical experience and without musical experience.

4.1.9

Open Ended Questions:

A thematic analysis (Aronson, 1994) was performed to analyze the comments and statements made in the opened questions of the questionnaire as follows:

1. What do you like most about the Vibro-Motion? 2. What do you like least about the Vibro-Motion? 3. What did you want to accomplish in the creative segment of the test? 4. What were you able to accomplish in the creative segment of the test?

Themes and their corresponding definitions are shown below. Forty percent of all comments were categorized into the themes by two independent raters. An Intra-Class Correlation (ICC) statistic was performed for all themes. All ICC values were at 0.65 or greater where 0.65 is moderate agreement. All remaining data was then analyzed by one rater. The number of occurrences of each theme is shown below in

Table 12, also see Figure 44.

102

Table 11: Themes and definitions used for thematic analysis.

Theme

Definition

Novelty

Something new, interesting.

Anything that is fun to use. If they got a fun experience, Fun/Entertainment enjoyable experience. Vibrations felt good.

Ease of use

User friendly, easy to use or simple design.

Being able to control the parameters (freq, strength, and Being in control position).

Being able to control a lot of things at the same time (frequency, strength, and position), getting the feel that the Interactive system is interactive. Using Gestures and Kinect to create vibrations.

Issues with the Kinect which affect the person's ability to use Technical issues the system: The Kinect tracking system failing, lagging, or the vibration not corresponding properly to the gestures.

Lack of resolution, hard to feel the vibration exactly for some Difficulty feeling vibrations frequencies and amplitudes.

Composition

Creating vibrational patterns, or musical patterns, with

103

notes, tones, beats and or tempo.

Experimenting with the parameters (frequency, position, or strength), Learning comfortable with the interface or trying to learn to do different things. and trying out different things. Getting

Pleasure

Feeling good, and relaxed.

Successful at goals

Mostly reaching goals, or fully successful at reaching goals.

Table 12: Number of occurrences of each theme in all of the qualitative questions.

Number Positive Theme Occurrences

of

Number Negative

of

Occurrences

Novelty

6

0

Fun/Entertainment/Pleasure

21

5

Ease of use

6

1

Interactive

6

0

Being in control

10

2

104

Technical issues

0

9

Difficulty feeling vibrations

0

11

Composition

34

0

Learning

18

1

Successful at goals

20

0

Total

121

29

40 35 30 25 20 15 10 5 0

Number of Occurances

Number of Positive Occurrences Number of Negative Occurrences

Figure 44: Number of occurrences for each theme.

105

4.2 Discussion
The discussion in the section below will refer to the vibrational parameters used in each of the stimuli. These parameters can be found in Table 13 and Table 14:
Table 13: The parameters used for the vibrations in phase 1 stimuli.

Initial Initial Static Stimuli Stimulus Frequency (Hz) Final Frequency (Hz) Initial Vibrational Strength % Final Position Vibrational on EmotiStrength % chair

Final Position on Emotichair

Vibrational 1 Strength 250 250 60 60 4 4

Vibrational 2 Strength 250 250 80 80 4 4

Vibrational 3 Strength 250 250 100 100 4 4

Frequency

4

40

40

90

90

4

4

Frequency

5

78

78

90

90

4

4

Frequency

6

154

154

90

90

4

4

Frequency

7

300

300

90

90

4

4

Position

8

110

110

75

75

2

2

106

Position

9

110

110

75

75

4

4

Position

10

110

110

75

75

6

6

Position

11

110

110

75

75

8

8

Table 14: The parameters used for the vibrations in phase 2 stimuli.

Initial Initial Dynamic Stimuli Stimulus Frequency (Hz) Final Frequency (Hz) Initial Vibrational Strength % Final Position Vibrational on EmotiStrength % chair

Final Position on Emotichair

Frequency

12

110

215

90

90

3

3

Position

13

154

154

90

90

2

5

Vibrational 14 Strength 215 215 90 30 3 3

Frequency and Vibrational Strength 15 215 40 95 65 4 4

Frequency and 16 Position 56 154 90 90 7 3

In this section, the user study findings are discussed in relation to each research question. The effect of the elements of accuracy, usability and control, and the impact of gesture gaming experience and music

107

experience on user's performance and enjoyment of the Vibro-Motion for simple vibration patterns (phase 1) and more complicated patterns (phase 2) are discussed. In phase 3, the free play session, participants were able to set their session goals and then commented during the post-study questionnaire whether they were successful in reaching their goals. The overall user experience and user performance with the Vibro-Motion had some expected and unexpected results. In addition, it seems that having gesture gaming experience may actually have a negative effect on the frequency accuracy while music experience seems to improve the frequency accuracy for some situations.

4.2.1

Research Question 1: Influences over Accuracy:

This section aims to consider the first research question in regards to phases 1 and 2: How do changes in frequency, channel position and vibrational strength values of the vibro-tactile stimulus presented to a user through the Emoti-Chair influence the user's accuracy in mimicking that stimulus. The discussion for this section substantiates my claim for the contribution of uncovering potential relationship between the three vibrational parameters (frequency, vibrational strength and position) and the ability to accurately mimic the vibration.

Phase 1 of the study was structured to isolate the effect of the three parameters on user accuracy. Each stimulus was static, i.e. the strength, frequency, and position did not change during the three seconds when the each stimulus was presented to the user. In addition the stimuli were grouped into three sections as follows:    From stimuli 1 to 3, only the strength of the signal changed. From stimuli 4 to 7, only the frequency of the signal changed. From stimuli 8 to 11, only the position of the signal changed. 108

Participants were not informed of how the parameters would change during the study and they made no mention of noticing any constant parameters. The stimuli were presented in a short, three second time window and there were only three stimuli for each parameter. This likely did not allow participants enough opportunity to acclimatize to the unchanging parameters in each stimulus. To determine whether participants would notice an unchanging parameter, further studies with more stimuli would be required. In phase 2, each stimulus became dynamic: one or more of the three parameters of the vibration pattern changed during the playing of the stimuli:   In stimuli 12 to 14, only one parameter was dynamic and the other two were static. In stimuli 15 to 16, one parameter was static and the other two dynamic.

For more details, see Table 12 and Table 13. For all 16 stimuli, accuracy for all three parameters, strength, frequency, and position, was consistently measured and calculated as described in Section 3.12.5 in order to compare between the stimuli in each phase and between the phases. The analyses of the results are presented below. 4.2.1.1 Accuracy in Phase 1: 4.2.1.1.1 Impact on Frequency Accuracy for Phase 1: As seen in Figure 45, the frequency accuracy produced by the participants varies as they progress from stimulus 1 to stimulus 11. It is evident that stimuli 4, 5, 8 and 9 have noticeably higher error rates compared to the rest of the stimuli. Stimulus 4 has the highest frequency error in phase 1 and the statistical analysis showed that stimulus 4 is statistically different from all the other phase 1 stimuli. As seen in Table 12, the frequency levels for all four stimuli (40Hz, 78Hz, 110 Hz and 110 Hz respectively) 109

are lower than the ideal range for best tactile perception lies between 150 and 250 Hz applied to bare skin as reported by Verillo (1991). Stimulus 4 was at the lowest frequency (40Hz) used in the study and was likely the most difficult to perceive of all frequencies used resulting in the highest error level (M=149.99, SD=94.29). It is suggested that this frequency was too low for Vibro-Motion for the mimicking task. All of the other lower frequency stimuli could thus be considered at frequencies that would be difficult to perceive and that perceptibility was likely dampened due to the clothing worn by participants.

The errors for stimuli 8 and 9 could also have affected by stimulus 7 which had a frequency of 300 Hz. The change from 300 Hz in stimulus 7 to 110 Hz of stimuli 8 and 9 may have affected the frequency error for these stimuli. The vibrational strength also decreases from stimulus 7 (90%) to stimuli 8 and 9 (75%), and this may have caused the vibration to be difficult to perceive.

As seen in Figure 45, there appears to be a possible effect between the vibrational strength and frequency error. For stimuli 1, 2, and 3 the frequency error decreases as the signal strength increases while the frequency is held constant at 250Hz and the position is maintained on channel 4. However, the difference in frequency error could be the result of a learning effect as the error levels improve over time resulting from practice. The frequencies for stimuli 1, 2 and 3, and those for stimuli 6 and 7 lie within the frequency range for ideal tactile perception. As seen in Figure 45, the frequency error for stimulus 3 is similar to those for stimuli 6, and 7 (M = 37.79, SD = 22.93). In these three conditions, the vibrational strength is either 100% or 90%. However, stimuli 1 and 2 have vibrational strengths of 60% and 80% respectively and higher frequency errors.

110

Figure 45: Phase 1 frequency error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph.

The subjective data gathered in the questionnaire responses correspond with the findings from the objective data. Participants' comments suggested that the vibrations were tough to feel at times, and that they found that they were improving as they progressed through the stimuli over time. For example, participants stated:

"it was difficult to feel low volumes and some frequencies were hard to feel (lower back specifically)"

"I could not feel the vibrations sometimes"

111

"The controls were quite straight forward and the spheres on the screen helped translate gestures into vibrations."

"Hard at first to make out which hand gesture does what... Need practice" 4.2.1.1.2 Impact on Position Accuracy for Phase 1: As seen in Figure 46, there seems to be a learning effect (as described by Hughes & Bartlett, 2002) with some exceptions (plateauing at stimulus 5, M= 5.14, SD= 7.00). Stimuli 9-11 show a comparative rise in position error. Stimulus 9, 10 and 11 have the same frequency and vibrational strength (see Figure 46) and change only in vibrational position (stimulus 9 occurs at channel 4, stimulus 10 at 6 and 11 at 8). This change of vibrational position, from a mid-back position to the lowest position possible to the midhamstring position respectively, between the stimuli seems to have caused an increase in the position error in stimuli 9 (M= 12.00, SD= 6.75), 10 (M= 9.71, SD= 6.80) and 11 (M= 10.86, SD= 7.47). The other stimuli occur above the mid-back position. Three possible reasons for the increased error rate for stimuli in the lower regions of the back and upper legs could be: 1) there is a damping effect because the voice coils in the lower areas of the back and upper legs are bearing most of the weight of the user; 2) the body shape and chair shape are mismatched at these locations, and/or 3) the vibrational strength or frequency are not optimized for this position on the body (higher vibrational strength and lower frequency may be required). In addition, two-point touch discrimination and point localization thresholds in the back are higher compared to other parts of the body. The two-point touch discrimination threshold in the back is 42.5mm (separation between two different points in order for them to be perceived as two different stimuli) and it is the third highest threshold in the skin; the point localization threshold in the back is 12.5mm (the accuracy of locating a point applied to the skin) which

112

is the second highest threshold in the skin (Lederman, 1997). It seems likely that the high threshold in perceiving vibrational position in the back had an effect on the vibrational position accuracy for stimuli located in the mid-back position.

As with the results in phase 1, participant comments supported the findings from the objective data: "I could not feel the vibrations sometimes"

"It was difficult to feel all the vibrations because of the chair design"

Figure 46: Phase 1 position error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph.

113

4.2.1.1.3 Impact on Vibrational Strength Accuracy for Phase 1: As expected and as seen in Figure 47, there appears to be a learning effect for vibrational strength as the signal accuracy increases between stimuli 1 and 11 with some exceptions. However, there is no significant difference between the stimuli accuracies (see Section 4.2.1.1.2 for a discussion of the issues with channel 4)

Stimuli 4, 5, 7, 8 and 9 appear to be higher from stimulus 11 which has the lowest vibrational strength error and was at 75% strength. This could be because there was not that much variation in the strength parameter or that strength is easier to detect and mimic accurately that the other parameters.

Figure 47: Phase 1 strength error graph for all participants. The details for the parameter that changes in between stimuli are shown underneath the graph.

114

The overall downward trend is interrupted for the first time at stimuli 4 and 5. A possible explanation for this could be the linkage between frequency and vibrational strength perception. Lower frequency seems to decrease the vibrational strength accuracy. Stimulus 4 has a frequency at 40 hertz, vibrational strength at 90%, and located at channel 4 and stimulus 5 has frequency at 78 hertz, vibrational strength at 90% and located at channel 4. This result may indicate that lower frequencies override the vibrational strength accuracy. Covariate analysis showed that frequency has a statistically significant interaction with vibrational strength for stimulus 4. There are different types of mechanoreceptors involved in feeling the pressure and texture and lower frequencies may be recruiting some of those different mechanoreceptors creating some confusion regarding the vibration felt. Meissner's corpuscles which detect vibrations around 50 Hz and Merkel's discs which are good at detecting touch and pressure may have been recruited along with the Pacinian corpuscles which detect vibrations around 150 Hz and 250 Hz. The low frequency might have caused confusion between the types of mechanoreceptors which feel frequency and vibrational strength (pressure) (Verillo 1991; ParÃ© et al., 2001; Munger et al., 1971).

Frequencies which lie in the ideal range for perception may have a positive effect on vibrational strength accuracy as observed in stimuli 6 (frequency at 154 hertz, vibrational strength at 90%, and located at channel 4) and 7 (frequency at 300 hertz, vibrational strength at 90%, and located at channel 4). Both stimuli have a vibrational strength error that is similar to stimulus 11. The only difference between the patterns observed in stimuli 4, 5 and stimuli 6, 7 is that the frequency lies in the ideal range for the latter group of stimuli (6 and 7). As a result, the vibrational strength accuracy is higher in these stimuli.

The overall downward trend which can be attributed to the learning effect continues in stimuli 6 and 7 however, it changes once again at stimuli 8 and 9. This may be caused by the change in vibrational position from channel 4 to channel 2 after all prior stimuli were presented at channel 4 to participants. 115

This indicates a possible relationship between vibrational position change and vibrational strength accuracy. However, this assumed relationship does not seem to apply to stimuli located at channels 6 and 8 as observed in stimuli 10 and 11. Here, the vibrational strength accuracy continues to improve while the vibrational position changes from channel 6 to channel 8.

Similar to participant's comments for position and frequency, comments about strength corresponded with the objective data. They tended to indicate that the vibration was hard to feel when the frequency was low, they also commented about how they liked being in control of the vibrational strength during the study. Some examples can be seen below:

"[When] the frequency is low, it's kinda hard to locate the vibration."

"I enjoyed how I was in control of the vibrations. Especially the loudness and softness control."

Further research is recommended where the participants could be asked to rate the strength of the vibrations they feel in each of the channels as vibrations of different strengths and positions are applied systematically, This could then assist in determining vibrational strength thresholds for different parts of the body. 4.2.1.2 Accuracy in Phase 2: 4.2.1.2.1 Impact on Frequency Accuracy for Phase 2: In phase 2, more complicated stimuli were presented to users. For example, in stimuli 12 the frequency changed from 110 to 215 Hz over a period of 3 seconds while the other parameters were held constant. The frequency error for stimuli 12 and 13 were low compared to most of the other stimuli in this phase. The frequency parameters of these stimuli lie in the ideal range for tactile perception (for discussion of 116

ideal range for tactile perception see Section 4.2.1.1.1), with the vibrational strength at 90% for both stimuli. As a result the frequency error was lower for these stimuli when compared with stimuli 14, 15 and 16. Stimulus 14 had a significantly higher frequency error compared to stimuli 12 and 13. In stimulus 14, the vibrational strength changes from 90% to 30% with other parameters being held constant (frequency at 215 Hz and position at channel 3). As found in phase 1, vibrational strength seemed to affect the frequency accuracy, as the vibrational strength decreased from 90% to 30%, the latter half of the vibration was difficult to perceive regardless of frequency, resulting in a higher frequency error.

117

Figure 48: Phase 2 frequency error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph.

Stimuli 15 and 16 have the highest frequency errors in phase 2. In stimulus 15, the frequency initially starts in the ideal range for tactile perception but drops from 215 Hz to 40 Hz. The vibrational strength also drops from 95% to 65% which may have made the vibration difficult to perceive. A combination of decreasing frequency and strength that may go beyond ideal ranges may have caused a high frequency 118

error. Another possible reason for a high frequency error might be that to tracking two changes simultaneously may have added extra difficulty to the mimicking task. Users may either become confused with or be less able to manage this task resulting in more errors.

Like stimulus 15, stimulus 16 also has the frequency parameter change from a frequency of 40 Hz (which lies below the ideal-range) rising to 154 Hz. The frequency error could have also be higher as most of the frequencies in this stimulus occur outside of the ideal range. The vibrational position also changes from channel 7 to channel 3. Again, the change of two parameters may have been difficult for users to track particularly as the frequencies do fall within the idea range and may have caused such a high frequency error. As position does not have any significant effect on frequency accuracy, it is suggested that having to track two stimuli with at least frequency being in a less than ideal range, it the more likely cause of the higher frequency error. 4.2.1.2.2 Impact on Position Accuracy for Phase 2: The position accuracy was affected by low vibrational strength when the vibrational strength changed. Stimulus 15 (M = 6.29, SD = 7.24) has a significantly lower position error than stimulus 14 (M= 11.43, SD=5.83). This was likely because the vibrational strength drops from 90% to 65% for stimulus 15, whereas the vibrational strength drops from 90% to 30% in stimulus 14. The vibrational strength drop to below 65% in stimulus 14 makes the vibration difficult to perceive and increased the position error.

119

Figure 49: Phase 2 position error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph.

Position accuracy was also affected by variable position parameters. It appears that having two changing parameters is a challenge for the participants as they had a much higher position error for stimulus 16 than any other phase 2 stimulus. Stimuli 13 had only a position change and this resulted in a fairly high error (M= 12.00, SD=5.35). It would seem that tracking position is more difficult than tracking either strength or frequency alone.

Stimuli 16 has the worst position error of all stimuli (M= 16.00, SD=6.28). For this stimulus, frequency and position were changing simultaneously (a potentially realistic type of vibro-tactile composition for

120

the Emoti-chair). Future research should consider realistic vibro-tactile composition situations where more than one stimulus is changing. 4.2.1.2.3 Impact on Strength Accuracy for Phase 2: Figure 50 shows that the vibrational strength error for stimulus 14 (M=11.43, SD=5.83) is higher than all of the other stimuli. Stimulus 14 also results in high errors for all three parameters; strength, frequency and position (see Figure 48, Figure 49 and Figure 50). The vibrational strength varies from 90% to 30% and this causes the vibration to become difficult to perceive during the second half of the stimulus. It would seem then that varying vibrational strength, particularly to the lower levels causes all of the vibrational parameters to become difficult to perceive.

121

Figure 50: Phase 2 vibrational strength error graph for all participants. The details for the parameter that changes during the stimuli are shown underneath the graph.

Between phase 1 and 2 data, it would seem that it is more difficult to determine the strength of a signal below a 60-65% threshold. In phase 1, stimulus 1 had a vibrational strength at 60% and in phase 2, stimulus 15 dropped considerably below this threshold. Further studies would be required to determine

122

the limen differences (Lim et al., 2006) where the vibrational strength becomes difficult to perceive for all ranges of frequencies and vibrational positions produced by Vibro-Motion and the Emoti-Chair.

The overall impact on accuracy shows that phase 2 stimuli are harder to mimic than phase 1 stimuli. This can be observed in the frequency, position, and the vibrational strength error rates. These findings are supported by Vries et al. (2009), who concluded that a vibration must be kept constant for at least 500 milliseconds in order for it to be comprehended accurately. The study found that phase 1 stimuli that lasted 3000 milliseconds had a higher accuracy than phase 2 stimuli which were dynamic in nature. These findings were contradictory to Craig & Evans (1987) who found that vibro-tactile patterns that last longer than 1200 milliseconds cause greater inaccuracies in comprehension of the stimulus due to oversaturation of the skin where the stimulus is present. Further studies could be performed to measure the effect of perception and mimicking accuracy with differentiating vibrational durations.

4.2.2

Research Question 2: Impact of Gesture Gaming Experience on Accuracy

The aim of this section is to discuss the research question: What is the impact of gesture gaming experience on user ability to control the frequency, strength and position of vibrations? The discussion for this section substantiates my claim for the contribution of developing a vibro-tactile system for artistic expression using hand gestures which does not require gesture gaming experience to use the system. 4.2.2.1 Frequency In phase 2, participants were required to use dynamic gestures, which is more consistent with gaming console gesture controls. The learning that participants with gaming experience brought to the study as

123

a result of the gesture gaming experience may have transferred to the dynamic gestures used in phase 2. However, the benefit of gaming experience did not transfer to the more static gesture productions required in phase 1. Further studies may be required to explore whether participants with gesture gaming experience have an advantage when attempting to compose a vibro-tactile pattern that may require dynamic gestures.

Figure 51: Phase 1 frequency error graph for participants with and without gesture gaming experience. The details for the parameter that changes in between the stimuli are shown underneath the graph.

4.2.2.2 Vibrational Position Statistics showed that there was no significant difference between those with gesture gaming experience and those without for vibrational position accuracy. This was expected as gesture gaming is a 124

way of controlling game interaction that is normally combined with visual rather than vibro-tactile feedback. Having gesture gaming experience would not prepare a participant in determining the position of vibro-tactile stimulation. 4.2.2.3 Vibrational Strength Statistics showed that there was no significant difference between those with gesture gaming experience and those without for vibrational strength accuracy. This might be expected as detecting vibrational strength has to do with perception rather than cognitive skill. Vibrational strength accuracy may depend on how well the vibration was perceived. Also, gesture game experience does not seem to contribute to participant's ability have a higher vibrational strength accuracy on the Vibro-Motion. However, further research with additional participants is necessary to explore the impact of gesture gaming experience on user performance with the Vibro-Motion.

4.2.3

Research Question 2: Impact of Music Experience on Accuracy

This section aims to answer the research question: What is the impact of musical experience on user ability to control the frequency, strength and position of vibrations? The discussion for this section substantiates my claim for the contribution of developing a vibro-tactile system for artistic expression which does not require music experience to use the system. 4.2.3.1 Frequency The frequency errors for participants with and without musical experience can be seen in Figure 52. It appears that music experience does not affect the frequency accuracy for phase 1 stimuli (see Figure

125

45). Participants without music experience did have a statistically significantly higher frequency error for phase 2 for stimuli 12, 15 and 16 (see figure below).

Figure 52: Phase 2 frequency error graph for participants with and without musical experience. The details for the parameter that changes during the stimuli are shown underneath the graph.

For phase 2 stimuli, participants with musical experience seem to perform better than participants without musical experience suggesting that musical experience aids in vibro-tactile pitch perception. Although the Vibro-Motion system is a vibro-tactile instrument, its functional properties can be compared to a musical instrument. It is thus not surprising to find that participants with musical training were able to feel and mimic frequency with more accuracy. Perhaps an inbuilt attentiveness that was

126

developed through musical experience has caused this effect. Further research is recommended with musicians with experience in different musical instruments to measure their attentiveness towards perceiving frequencies. 4.2.3.2 Vibrational Position Statistics showed that there was no significant difference between those with musical experience and those without on the vibrational position accuracy. As stated in section 4.2.2.2, this might be expected. Therefore, it may not be necessary to have musical experience in order to have higher vibrational position accuracy on the Vibro-Motion. Further research comparing musicians with non-musicians is necessary to explore this concept. 4.2.3.3 Vibrational Strength Statistics showed that there was no significant difference between those with musical experience and those without for vibrational strength accuracy. As stated in section 4.2.2.3, this might be expected as detecting vibrational strength is a perceptual skill rather than a cognitive skill. Vibrational strength accuracy instead may depend on how well the vibration was perceived, and it does not seem necessary to have musical experience in order to have higher vibrational strength accuracy on the Vibro-Motion. Further research with musicians and non-musicians specifically examining the impact of musical experience on people's ability to perceive vibrational strength is required.

4.2.4

Research Question 3: Participant Perceptions of controllability

This section aims to discuss the research question: What is the participant's perception of the controllability of the Vibro-Motion system (accuracy, and ease of control and use)? The discussion for 127

this section substantiates my claim for the contribution of developing a vibro-tactile system which is easy and fun to use. This is done by analyzing participants' comments and responses to the post study questionnaire. The contribution of developing a vibro-tactile system which allows people to successfully is also supported by the discussion in this section. 4.2.4.1 Impact on control: In response to the post study questionnaire question: "How easy was it to repeat the vibrations on the Emoti-Chair?", 12% (3/25) of the participants said that they found it very easy, 44% (11/25) of the participants said that they found it easy, 32% (8/25) of the participants said that they were neutral, 4% (1/25) of the participants said that they found it hard. 8% (2/25) of the participants said that they found it very hard. The participants commented positively saying:

"[I liked] being able to control the frequency, amplitude, and position of the vibrations I was feeling"

"I enjoyed how I was in control of the system. Especially the loudness and softness control"

Although the majority of participants said they found it easy to repeat the vibrations felt on the EmotiChair, the participants also commented negatively on the control and the technical issues:

"[It was] a little hard to control specifically the strength and position. Seems Kinect is not as good when I am sitting down"

"[It] was sometimes hard to control and comprehend the vibrations".

128

Forty-four percent (11/25) of the participants thought that the temporal demand was neither high nor low, however, 40% (10/25) of participants thought that the temporal demand was high or very high, compared to the four participants who said that it wasn't very high. This response from the participants can be expected because the participants had three seconds per vibration to determine all of the parameters, and this time may have been too short of a time to clearly feel all aspects of the vibrational stimulus. Also, they had to keep track of how long they were mimicking the stimulus, and try not to exceed the three second time duration.

Participants reported that they had to expend a high degree of effort throughout the study; 64% (16/25) of the participants said that they expended high or very high effort whereas only 12% (3/25) participants thought that the effort was not high and the remaining 28% (6/25) were neutral towards the effort required. This could be a result of having to perform accuracy focused tasks on a novel interface. However, participants stated that they thought that they had a relatively easy time repeating what they felt on the Emoti-Chair and commented on the interactivity of the system saying:

"the controls were straight forward, and the spheres on the screen helped translate gestures into vibrations"

"It felt cool. It helped me be aware of my hand motions and placement."

Even though participants thought that the mimicking task was effortful, it would seem that the interface was not the cause of the difficulties with the mimicking task. Not only did participants report that the interface was easy to use (see Section 4.1.8) but they also commented that the interface seemed to enable control and interactions. It would then seem that mimicking patterns is a difficult task regardless of the interface used. 129

It may be that some of the technical issues with the system caused people to report greater effort in accomplishing the study tasks. Participants commented on their concerns with technical issues during the post study questionnaire saying:

"[the system] wasn't very accurate"

"there were some glitches, and in my arms and legs the responsiveness of the chair was better than in [the] lower back"

"It didn't mimic some of the vibrations I wanted to create. There was a bit of lagging!"

Participants also reported that they found some of the vibrations were difficult to feel, particularly vibrations which had a low vibrational strength, or if the vibration was located at the lower back of the chair. They commented:

"[when] the frequency is low, it's kinda hard to locate the vibration"

"Not being able to sense the lower tones."

"It was sometimes hard to control and comprehend the vibrations."

It would then seem that vibrational strength is more important than the pitch/frequency or the vibrational position as it controls whether the vibration is perceived at all. Position seems to be the next important element as the vibrational position is perceived before the finer details like pitch/frequency is perceived as discussed by Meirovitch (1975).

130

4.2.4.2 Impact on usability: Although 40% (10/25) of the participants were indecisive about whether the system did what they thought it would do in response to their gestures, remaining 60% participants agreed or strongly agreed that Vibro-Motion did what they expected it to do. This might suggest that the participants generally thought that the Vibro-Motion system matched their expectation regarding how it worked and performed. The participant responses to the level of satisfaction with what they have produced may also support that the system was easy to use. 4% (1/25) of the participants said that they were very satisfied, 48% (12/25) of the participants said that they were satisfied, 28% (7/25) of the participants said that they were neutral and 20% (5/25) of the participants said they were not satisfied with what they have produced on the Vibro-Motion system.

The themes found during the participant comments during the post study showed that there were more positive comments than negative comments regarding the ease of use (positive: 6; negative: 1), and being in control (positive: 10; negative: 2) of the Vibro-Motion system. However, participants also found the vibrations difficult to feel (there were 11 occurrences regarding the difficulty of feeling the vibrations). This seems to indicate that while differences in the various parameters of the vibrations are hard to feel, controlling those parameters with gestures is not. This may be because the parameter settings fall outside of an ideal perceptual range (e.g., frequencies lower than 150Hz are hard to feel), or that individual differences such as skin sensitivity or clothing thickness may have interfered with vibrotactile perception among participants.

Some participant comments regarding usability factors were:

131

"I liked the feedback the chair put out and it was easy to know where the vibrations were coming from"

"It was user friendly and interactive,"

"The simplicity of the hand gestures required to create the vibrations."

"it was fun and easy to use."

Most of the objective data in the study seems to support the subjective results which showed that the participants were able to improve their accuracy over the course of the study suggesting that participants could learn to use the system during of the first two phases of study, and then work with it to produce their own composition that they believed fit with their intentions.

The post study questionnaire also revealed that participants did not experience any fatigue in their hands, arms, legs, torso and eyes after completing the study. As the Vibro-Motion system was designed and customized within recommended reach envelops (according to the Canadian Center for Occupational Health and Safety, 2005), this allowed people to be seated comfortably, and the computer screen was placed at a comfortable distance according to human factors standards (Oborne, 1987), this was an anticipated result. However, longitudinal studies are required to determine whether this would be the case during long term use. 4.2.4.3 Feasibility of self-directed composition As I did not have any objective data for the free play (phase 3) session, the analysis of participant's responses is limited to their reports in the post study questionnaire. Participants reported that they

132

believed they were successful at accomplishing their goals during the free-play session (phase 3). For example they commented:

"I was able to achieve a thumping bass beat the best,"

"I was able to create a range of different low bass beats that was really enjoyable to do once I started to get control over it."

Most of the participants were able to accomplish some of their goals, and none of the participants commented on not being able to accomplish their goals for phase 3. It might be possible that the participants did not know enough about the system to bring the most out of it, or that they truly were able to accomplish what they set out to do. The goals set by the participants might have been simple and easily achievable. As the Vibro-Motion system was novel to all participants and no participant had ever had any experience with composing vibrational patterns, it was expected that the goals for the free play phase would likely be exploratory, and relatively simple and uncertain. Further study is required to explore and understand the notion of composition for vibro-tactile stimuli. For example, participants could be given a compositional goal to acheive (e.g., create a happy vibro-tactile composition A comparison could then be made between the set goal, the participant's output and user assessments which could then, in turn, shed some light on the feasibility of, user assessments of and creative possibilities for composition. Longitudinal studies could also be performed to explore how compositional goals change over time and from experience with Vibro-Motion.

133

4.2.5

Research 4: User Experience with the Vibro-Motion system:

The overall participant response to the Vibro-Motion was positive, and their comments reflected themes such as "Novelty", "Fun/Entertainment", "Composition", "Pleasure" and "Learning." There w ere a total of 121 occurrences of positive themes compared with 29 occurrences of negative themes. The discussion for this section substantiates my claim for the contribution of developing a vibro-tactile system which is easy and enjoyable to use. This is done by analyzing participants' comments and responses to the post study questionnaire.

When the participants were asked if they had fun in the post-study questionnaire, most participants (24 out of 25 participants) reported that they had fun; only one participant was indifferent. Among the themes identified in the responses to the opened questions, 17% of comments were that participants found that Vibro-motion was fun or entertaining (see Table 11). The corresponding thematic results from the participants' written responses to questionnaire questions, the participants reported that the system was fun to use, interactive and provided them with a new experience. Some examples from their comments are:

"It was an interesting experience, nothing like I've ever done before"

"[It's] cool that positions and vibrational strength can change with gestures."

134

Table 15: Number of occurrences of each theme in all of the qualitative questions.

Number Positive Theme Occurrences

of

Number Negative

of

Occurrences

Novelty

6

0

Fun/Entertainment/Pleasure

21

5

Ease of use

6

1

Interactive

6

0

Being in control

10

2

Technical issues

0

9

Difficulty feeling vibrations

0

11

Composition

34

0

Learning

18

1

Successful at goals

20

0

Total

121

29

When asked if participants were able to do what they wanted to do on the system: 8 participants reported that they could not make the system do what they wanted it to do, while 13 of the participants said that they were able to make the system do what they wanted to do and 4 participants were neutral towards the question. This could mean that the design of the system allowed people to have control 135

over the various parameters, and not be overwhelmed. This likely resulted in the positive user experience as well as the overall decreasing error rates found in the objective data. This level of comfort and control may have also translated into participant's beliefs that they performed well. Participants reported that they were satisfied with what they produced during phase 1 and phase 2 stimuli; 13 of the participants were satisfied with what they have produced, whereas only five participants were not satisfied, the remaining seven participants were indifferent regarding their satisfaction level for all phases. The participants said:

"The technology is so fun to use"

"it was fun repeating the gestures from what I felt, [and] what I wanted to reproduce"

"it was fun and easy to use".

In the questions following the phase 3 free play session of composing vibro-tactile music, 22 of the participants said that they enjoyed the creative aspect of composing vibrations on the Emoti-Chair, three of the participants were neutral, and no participants thought that the free play session was not enjoyable. Participants confirmed this by commenting that they thought the Vibro-Motion interface was a new and interactive experience and that they had fun. Participants had diverse goals for their phase 3 sessions; some of them included making a piece that flowed, or a musical score. Participants also had more sound effects oriented goals such as creating a piece that sounded like a race car accelerating, receiving a massage or simply learning and experimenting with the capabilities of the Vibro-Motion interface. Eighty percent (20/25) of the participants thought they were successful in their phase 3 goals, and the remaining 20% (5/20) said they were somewhat successful, and, there were no participants that said that they could not achieve their phase 3 goals. 136

From the thematic analysis, negative aspects of Vibro-Motion were mostly related to difficulties in feeling the vibrations (11 comments of a total of 29 negative comments or 38%) and technical issues (9 comments of 29 negative comments or 31%). As seen in sections 4.2.1 and 4.2.2, the error rates increased when the signal strength was too low, or when the frequency was high (about 300 Hz) or when the vibrational position was in the lower back. The technical issues that most often arose were the lag of the Vibro-Motion system, the tracking capabilities of the Kinect and the difficulty feeling vibrations in the lower back.

4.3 Limitations:
As with any study, there were a number of limitations in the study which may have affected the results. These include the technical issues of the Kinect, the lack of randomizing the order of the stimuli which may have caused an order effect, the limited number of participants and trials, and the quality of the questionnaire. Each of these limitations along with suggestions for future work to address these limitations is presented in this section.

4.3.1

Kinect's technical issues:

There were unavoidable technical difficulties with the Kinect particularly when it was tracking the movements of the participants. These difficulties include lag time and glitches in tracking the movements of participants seated in the Emoti-Chair. Lag is the difference in time between the gestures made by the user and resulting vibration presented through the Emoti-Chair. There was a slight delay (which was not measured), between the movement and the vibration felt. One participant made

137

remarks regarding this: "[Vibro-Motion did not mimic] some of the vibrations [that I] wanted to create. There was a bit of lagging!"

Due to the proprietary black-box design of the Kinect Application Programming Interface (API), it was not possible to make any modifications to how the Kinect's tracking system. In an environment with a lot of light, the Kinect behaves erratically if the participant wears reflective colours as the light reflected off the participant interferes with the IR sensors that are necessary in skeletal tracking. This was corrected by dimming the lights to aid the Kinect's tracking

Since the Kinect was not meant for tracking the movements of seated users, there were issues such as incorrect skeleton tracking which sometimes incorrectly associated the joint locations of the participants. In extreme cases, the Kinect would detect two users when a single participant was being recorded. This was corrected by asking the participants to stand up and sit down again in order to reset the skeleton tracking system of the Kinect. Although participants did not comment on this specifically, this coaxing process may have frustrated the participants by disrupting their concentration levels. Disrupting participant's concentration may have affected their accuracy because they could have forgotten what they had been doing prior to the interruption or it may have reduced their desire to do well in the study which then affected their performance.

4.3.2

Lag time:

In the study, I measured the amount of time taken by the participants before they made the attempt to mimic the stimulus on their first attempt for each of the phase 1 stimuli. I thought that this information may be a useful factor in measuring how fast different types of users can achieve a reasonable revel of

138

accuracy. However, there was no comparable measure for phase 2 since users move their hands in that phase. Therefore I decided instead to use the accuracy of the final attempt since this could be measured for both phases, and not to use the lag time data in my study.

4.3.3

Order of the study stimuli:

The pre-set, non-random order of the stimuli in the study could be considered a limitation. As seen in Table 13, in the first three stimuli, the strength is the only changing parameter; in the next four stimuli, the frequency is the only changing parameter, and in the last four stimuli, the position is the only changing variable. Participants were all presented with this same order, and this may have caused an order or learning effect that masked their real performance. The order presentation was not randomized so that participants could practice each type of hand movement one at a time and demonstrate learning before moving to the next parameter that changed and then further challenging them with the more difficult stimuli situations in phase 2. Also, if the stimuli were randomly selected, it can be speculated that the the learning effect was bound to happen even after the training period as Vibro-Motion was such a novel system may not have been seen and stimuli that were different from the learning effect may not have been identified. As seen Figure 45, Figure 46 and Figure 47 the learning effect seems obvious as are the effects that are not a result of learning.

Now that it would seem that Vibro-Motion can be learned and used, future research is needed to determine the level of impact each of the different parameters has on user accuracy and performance in order to fine tune the parameter ranges. This would involve a study that randomizes the presentation of the stimuli as well as increasing the quantity of trials.

139

4.3.4

Limited Participants:

The quality of the study could have been improved by having participants from wider age groups, and finding a more equal number of participants from each group (i.e. Gesture gaming experience, no gesture gaming experience, music experience and no music experience) as well as participants from the deaf community. A larger group of participants might provide greater insight towards the effects of gesture gaming or musical experience with respect to the participants' demographic.

4.3.1

Analysis of total number of attempts:

I also wanted to analyze the total number of attempts made for each stimulus per participant. This data could have been useful in determining how the learning effect influences the performances of participants from different demographics. The total number of attempts was measured but not used, to avoid over-using the study data. Furthermore, there were several cases where participants skipped back and forth between the stimuli, which reset the total number of attempts made and rendered the data unusable. It was unfortunate that I had overlooked this bug in the code as it did not appear during the Vibro-Motion pilot study.

4.3.2

Quality of the questionnaire questions:

Along with asking the participants what they liked and disliked, asking "why" may have provided additional insight into the psychological effects/impact, emotional effects/impact of using the vibromotion, and could have provided greater insight which could be useful in creating a better user experience in the future iterations of the system. Additionally, the questions of the questionnaire could have been phrased either using neutral terms or positive terms, rather than alternating between 140

positive and negative phrasing. Future questionnaires should focus on asking why the participants liked or disliked a certain aspect, along with questions about how they would redesign the system's mappings if they were given the opportunity.

141

5 Conclusions and Future Work
A system, called Vibro-Motion, was designed to provide a novel approach to composing vibrational patterns using hand gestures. This approach towards creating vibrational patterns is an emerging field of research. From this research, I found that it was feasible for people to manipulate the Vibro-Motion system with perceived success, and feel and mimic fairly simple and short duration stimuli applied to their backs through the voice coils of the Emoti-chair. The design strategy of using gestures and limited range of movement thresholds coupled with a visual display that provided feedback on these limits seemed to facilitate interaction with the Vibro-Motion system. Positive participant comments regarding the enjoyability and types of functions available through the system suggests that the system could be learnable and easy to use. Further research is required to determine whether the system allows users to achieve proficiency over a long duration and whether the ease of use is practical for compositional purposes.

Regarding the individual factors of the Vibro-Motion system that were manipulated during the user studies, it would seem that vibrational strength has the largest impact on the accuracy of mimicking frequency and position, as well as the vibrational strength itself. Vibrational strength seems to be the most important element for perceiving vibrations in the studies conducted in this research because the lack of vibrational strength increased the error rate for the frequency, position and the vibrational strength. It appears that the vibrational strength accuracy improved at higher frequencies, and the vibrational strength accuracy decreased at low frequencies. It appears that vibrational strength accuracy is affected by the change in the vibrational position. As previous research would suggest, it is observed

142

that signal frequencies that lie outside the range for ideal tactile perception (150 Hz to 250 Hz) make it difficult to perceive the frequency accurately for all participants.

All participants had a higher error rate for vibrational position, frequency and strength when more than one of these parameters was changing, this could be due to the increased perceptual and cognitive load of monitoring and tracking multiple changing stimuli. As expected, phase 2 stimuli incurred higher error rates than phase 1 stimuli. Participants also had higher error rates during phase 2 because of the constantly changing properties of the vibration, which did not allow the participants enough time to comprehend the vibration clearly. Whether or not people improve their performance depends on their practice with the Vibro-Motion system. At first glance it appears that the participants show a tendency to improve over the duration of the study because of how the error rates decrease as they progress from stimulus 1 to stimulus 11 (see Figure 45,Figure 46, and Figure 47). Future work is required to determine how practice and familiarity with the Vibro-Motion system impacts the accuracy over time.

The overall effect of gesture gaming experience was minimal for the frequency, position, and vibrational strength accuracy rates; participants with and without gesture gaming experience did not perform differently for static stimuli in phase 1. However, gesture gaming experience had some impact on frequency accuracy for stimuli in phase 2 as participants with gesture gaming experience had lower errors for three of the five stimuli. Practice and previous exposure to performing dynamic gestures may have helped participants with gesture gaming experience to mimic dynamic stimuli more accurately. Future research is necessary to explore whether participants have an advantage when attempting to compose a vibro-tactile pattern.

143

The overall effect of music experience on participant performance is also limited. Music experience had a positive impact on the frequency accuracy as participants with music experience had higher frequency accuracy for phase 2 stimuli. However, there is no additional effect on performance as both groups had similar accuracies for vibrational position, and vibrational strength accuracy.

The research found a learning effect which was noticeable for phase 1 stimuli which seemed to eventually plateau. Participants' comments after the study supported this plateauing effect as they commented about the system being easy to use, interactive, and that the participants were in control of the system by the end of the study. Learning the system does not seem to be a frustrating experience despite the level of difficulty of the task, but fun, novel and interesting as reported by participants. Participants also reported enjoying the process of composing their own short-duration piece with VibroMotion. It can thus be concluded that Vibro-Motion seems not only learnable and easy to use, but also enjoyable to play and compose. However, whether or not Vibro-Motion can be used for extensive vibrotactile compositional purposes remains to be studied. A future study where the Vibro-Motion is combined with other vibro-tactile instruments in a vibro-tactile orchestra or a band setting is suggested.

The Vibro-Motion is not without limitations and areas for improvements. The participants also expressed their discontent with the Kinect's tracking capabilities and the lag between the action and response of the system.

In summary, the design of and user study results evaluating the Vibro-Motion system suggests that composition of vibro-tactile feedback using gestures is not only feasible, but also fun and interactive. The system allows the user to control different elements of vibration such as vibrational frequency, position and strength while also providing the sense of being in control of the system. The results of the

144

research study showed that the system was easy to learn as the accuracy of the participants improved as they progressed through the stimuli. The study also showed that the interaction of the three vibrational elements affected the participants' accuracy in mimicking the vibrational stimuli. Low vibrational strength caused the accuracy to decrease as the vibration was difficulty to perceive. The threshold for feeling the vibrations seemed to be around 60-65% of the maximum amplitude of the voice coils. Frequencies in the 150-250 Hz range made it easier to perceive frequency while frequencies outside of this range made frequency harder to perceive. An interaction between the elements was also discovered, a lower frequency causes the vibrational strength accuracy to decrease while a higher frequency caused the vibrational strength accuracy to increase. The vibrational strength was the most dominant element because low vibrational strength affected the accuracy of all elements.

The contributions of my research are listed below:

1. I have developed a novel system which allows the composition of vibro-tactile patterns using hand gestures. The user study performed on this system showed that this system is fun and easy to use. The research study contributed data and knowledge in the field of vibro-tactile composition.

2. I have considered various methods for teaching beginners to use a vibro-tactile gesture instrument and developed a method for teaching them to use this instrument. This method involved using mimicry for learning how to use a vibro-tactile gesture instrument.

3. The user study designed to evaluate Vibro-Motion gathered data on how different parameters of vibrational stimuli (frequency, vibrational position and strength) can be manipulated by users within the context of mimicking vibro-tactile stimuli. 145

4. A list of future research ideas have resulted from the user study (see section 5.1).

5.1 Future work
The research reported in this thesis has just begun to explore the concept of gesture-based control of a vibrational system, Vibro-Motion. While this initial investigation seems to indicate that this system is engaging to users and enables them to exercise some control over the system, there remains a considerable amount of work to carry out in the space opened up by this research. An important and pragmatic issue that needs to be addressed prior to further research on vibro-tactile composition or control is the ergonomics of the Emoti-Chair so that there is full contact between the chair surface and the back. Of particular importance is addressing the dampening effect caused by the user's weight in the seat of the chair and the loss of contact caused by the misalignment between the user's body shape and the shape of the chair.

Once other technical limitations such as Kinect's tracking and lag are addressed, future research discussed in section 4.2 could also be performed to determine the effects of the system in greater detail. These future suggestions are recapped below in two sections, research suggestions and technical improvement suggestions.

Research Suggestions:

1. High frequency seems to decrease frequency perception, however due to the evident learning effect for all factors, it was difficult to isolate the effect of any single factor. Future research should be performed to measure this effect after the participants are given a chance to move

146

beyond the learning curve (see section 4.2.1.1.1). Alternatively, future research could also be carried out to measure this effect by presenting stimuli in a randomized manner. 2. Three possible reasons for the increased error rate for stimuli in the lower regions of the back and upper legs were isolated. These include: 1) there is a damping effect because the voice coils in the lower areas of the back and upper legs are bearing most of the weight of the user; 2) the body and chair shape are mismatched at these locations, and/or 3) the vibrational strength or frequency are not optimized for this position on the body (higher vibrational strength and lower frequency may be required). Further research is recommended where the participants are asked to rate the strength of the vibrations they feel in each of the channels (see section 4.2.1.1.3). By gathering this data, it could provide vibrational thresholds for different locations on the body; these thresholds may be used to adjust the vibrational strengths for different parts of the body to compensate for the sensitivity of the skin at different parts of the body. 3. A possible relationship between vibrational position change and vibrational strength accuracy was discovered. A further study is required to better understand and map the relation between vibrational position change and the vibrational strength accuracy (see section 4.2.1.1.3). 4. Stimuli 16 had the worst position error of all stimuli probably because two of the three parameters (frequency and vibrational position) were changing simultaneously. This simulates a realistic vibro-tactile composition, and future research could focus on realistic vibro-tactile composition situations where more than one stimulus is changing (see section 4.2.1.2.2). 5. Vibrational strength below 60-65% threshold makes it difficult to perceive the vibrations. Further studies are required to determine the just-noticable differences where the vibrational strength becomes difficult to perceive for all ranges of frequencies and vibrational positions produced by Vibro-Motion and the Emoti-Chair (see section 4.2.1.2.3). A perceptual map of the 147

sensitivity of the human back to vibrational strength, frequency and position along the back could be generated. This map could assist designers in creating vibro-tactile systems for using on the back. 6. Further studies are required to explore impact of gesture gaming experience on the time and effort required to learn how to use the Vibro-Motion system and subsequent advantages in composing vibrational patterns on it (see section 4.2.2.1). 7. In this research, it was found that having music training seemed to have a positive impact on people's ability to mimic dynamic vibrational stimuli (see section 4.2.3.1). Further research is recommended to better understand the impact that musical experience has on the ability of people to learn and use the Vibro-Motion system and why. This could then translate into the development of training regiments for the Vibro-Motion system that take advantage of these research findings. 8. All participants seemed to think that they were successful at accomplishing their compositional goals (See section 4.2.4.3). Whether the goals that were set were realistic, playful, compositional, entertaining or to learn was not identified as this part of the research was very preliminary. Further studies on the possible implications for Vibro-Motion composition and the subsequent audience response to them are required. It is envisioned that considerable care is required to design studies that explore the concept of Vibro-Motion composition due to the novel, creative and abstract aspects of vibro-tactile composition. 9. The participants during the study commented that they were successful in creating their own compositions during free play (see section 4.2.4.3). However, future research is necessary to explore whether the Vibro-Motion is suitable for composing extensive vibro-tactile patterns.

148

One suggestion is combining the Vibro-Motion with other vibro-tactile instruments like the Vibro-Chord in a vibro-tactile orchestra. 10. The study showed that there was statistically significant frequency, position and vibrational strength errors between different types of stimuli, but the practical significance of these errors is unknown. Further research is necessary to determine whether the statistically significant errors are meaningful to the practical use of Vibro-Motion. One suggestion is to ask one group of participants to mimic a full vibro-tactile composition and ask another group of participants if they can feel the differences between the original vibro-tactile composition and the mimicked compositions. Statistically significant errors between the mimicked compositions and the original could indicate whether there is sufficient fidelity to uniquely identify one composition versus another for copyright purposes or for performance consistency. If the proposed research shows that the participants are able to differentiate between two the mimicked compositions and the original composition, it might even be possible to explore an area of research involving vibration as a language. 11. Although participants were not informed of how the parameters would change during the study and they made no mention of noticing any constant parameter, it is possible that the participants may have noticed the common vibrational elements parameters in between stimuli. A future study is required to determine if it is possible to notice unchanging parameters among the stimuli. If it is possible to notice the unchanging parameters, another study is suggested to determine the minimum threshold which allows the differences to be noticed. 12. Majority of the participants commented that the system was enjoyable and easy to use during the study. Further research is required to determine if the system allows users to achieve proficiency over a longer duration, and whether the ease of use is practical for compositional 149

purposes. One suggested method is to ask one group of participants to compose vibro-tactile pieces which express an emotion and ask if the participants thought they were successful in their attempt. Another group of participants could be asked to feel these vibro-tactile pieces and describe the emotion conveyed in these pieces.

Technical improvement suggestions:

1. The most recent version of the Vibro-Motion source code and the corresponding MATLAB analysis code is openly available at: https://www.dropbox.com/sh/e4blsukqgaaryhv/6EU67kOPf. This source code can be used to improve the project further. By making the VibroMotion's software code more robust to allow installations on various operating systems, it might be possible to provide the Vibro-Motion system as either a stand-alone product for gesture based interaction for composing vibro-tactile patterns, or as part of the Emoti-Chair bundles commercialized by TAD Inc. 2. There are always technical innovations on the horizon that could be used to improve the functionality and technical aspects of the Vibro-Motion system. For example, finger tracking technologies such as Leap Motion 3D (Leap Motion, 2012) which may provide a suitable interface for tracking hands and fingers. These finger gestures could be tracked in order to provide access to greater range of vibration types (i.e., square wave, saw-tooth wave, pulsating wave etc.) produced by the Vibro-Motion system. 3. A suggested design modification could be allowing the left hand's frequency control to change the frequencies logarithmically instead of changing them linearly. This design change would complement the logarithmic scale used in selecting the stimuli frequencies.

150

The Vibro-Motion system has opened the door to one possible way of producing vibro-tactile patterns using gestural control. Future work in this area may lead to greater insights and innovation in vibrotactile composition and performance which will then add to the growing body of research in gestural interfaces, human-computer interaction computer human musical interfaces and vibro-tactile feedback.

151

Appendix A
Ethics Approval letter

Sai Cherukumilli

REB 2012-008

Project Title: The effectiveness of Vibro-Motion device in music/vibration composition.

Dear Sai,

The Research Ethics Board has completed the review of your submission. Your research project is now approved for a one year period as of Feb 20, 2012.The approval letter is attached in Adobe Acrobat (PDF) format.

Congratulations and best of luck with the project.

152

Please note that this approval is for one year only and will expire on February 20, 2012. Shortly before the expiry date a request to complete an annual report will be automatically sent to you. Completion of the annual report takes only a few minutes, enables the collection of information required by federal guidelines and when processed will allow the protocol to remain active for another year.

Please quote your REB file number (REB 2012-008) on future correspondence.

If you have any questions regarding your submission or the review process, please do not hesitate to get in touch with the Research Ethics Board (contact information below).

No research involving humans shall begin without the prior approval of the Research Ethics Board.

Record respecting or associated with a research ethics application submitted to Ryerson University.

Yours sincerely,

153

Nancy Walton, Ph.D.

Chair, Research Ethics Board

Associate Professor

Ryerson University POD470B

350 Victoria St., Toronto, ON

(416)979-5000 ext. 6300

nwalton@ryerson.ca

rebchair@ryerson.ca

http://www.ryerson.ca/research

___________________________________________________________

Toni Fletcher, MA

Research Ethics Co-Ordinator

Office of Research Services

Ryerson University

(416)979-5000 ext. 7112

toni.fletcher@ryerson.ca 154

http://www.ryerson.ca/research

155

Appendix B
Consent Form

On Ryerson University Letterhead CONSENT AGREEMENT SUMMARY

1. You will have the opportunity to participate in an evaluation of our Vibro-Motion system.

2. The researchers are interested in your experience with music and your opinion of the system.

156

3. This is a Graduate Student Research Project; Professor Deborah Fels is the supervising this study.

4. Agenda: Consent Form, Pre Questionnaire, 2 test segments, Post Questionnaire

5. Participation is voluntary and you can stop at any time.

6. Everything you say will remain confidential.

157

Consent Agreement

Principal Investigators: Sai Chaitanya, Ryerson University

Sai.cherukumilli@ryerson.ca

Deborah Fels, Ph.D., P.Eng. Ryerson University

(416)-979-5000 ext. 7619 or dfels@ryerson.ca

Project Title:

The effectiveness of Vibro-Motion device in music/vibration composition.

You are being asked to participate in a research study. Before you give your consent to be a volunteer, it is important that you read the following information and ask as many questions as necessary to be sure you understand what you will be asked to do.

Purpose of the Study: We have developed a new technique of composing vibrational music using hand gestures to make music using the X-Box Kinect system. Musical elements of pitch and volume can be controlled with hand gestures, which are produced as vibrations on the Emoti-chair. By conducting this study, we want to find out participants' impression of the Vibro-Motion interface, their understanding of 158

the different elements, and how easily they are able to use the interface to create meaningful vibrational music. We want to compare learning curves of musicians and non-musicians as they learn to replicate the vibrations they feel on the chair.

Description of the Study: First, you will be asked to complete a pre-study questionnaire to collect background information and to collect your opinions on, and experience with, music. The Vibro-Motion system will then be calibrated to your upper body by having you move your hands up and down, side to side, and forwards and backwards at chest level. Next, you will be trained on how to use the system. Training will involve instructions on how your hands will affect the different vibrational elements followed by performing specific gestures to change the location and strength of various vibrations on the Emoti-chair. At this point we will ask you to wear noise cancelling earplugs and headphones which play white noise to block out the noise from the Emoti-chair for the duration of the study. You may wish to take as many breaks as you need while doing the study. You will then be given 5 minutes to experiment with the system yourself and ask additional questions. You will be sitting in the Emoti-chair for duration of the study.

You will then be asked to mimic two sets of 24 short vibrations, about five seconds each, presented to you on the chair. Before we proceed to the post-study questionnaire, we will give you a short break. After the break, you will be asked to try and create a short vibrational piece of your own through free play and experimentation with the system. Throughout the study, you will be asked to express your thoughts by talking out loud about what you are doing and how the system responds to you. You will 159

also be asked to provide your opinion and feedback on the experience as part of the post-study questionnaire.

We will setup a camera system on a table in front of you so that we can observe how you interact with the system. All information obtained in this study will be confidential and the system will not be used in a public setting.

Risks or Discomforts: The risks associated with the study are minimal. You might feel uncomfortable or fatigued while using the Vibro-Motion system and/or responding to the questionnaire. If you feel tired or uncomfortable, you may take a break to rest or discontinue participation in the study either temporarily or permanently. You may also feel uncomfortable being video-taped. We will turn on the camera during the pre-questionnaire so that you can become used to it being on. If that does not help, then we will stop the study.

Benefits of the Study: It is not foreseen that you will personally benefit from participation in this study other than enjoying the vibration music. However, the results from this research will contribute to the development of meaningful guidelines for using gesture to compose vibratory music which will allow the deaf and hard of hearing to experience music.

160

Confidentiality: All data will remain confidential; will be secured at the Centre for Learning Technologies at Ryerson University and destroyed after five years. Data will only be presented in summary form and no one individual will be identified. Number codes will be used to link data with personal information. We will also be recording the study on video. We will not use this footage in any public setting, and the footage will be stored on our password protected lab servers.

Costs and/or Compensation for Participation: There are no costs associated with your participation. You will be compensated with $15.00 in cash for completing the entire study.

Voluntary Nature of Participation: Participation in this study is voluntary. Your choice of whether or not to participate will not influence your future relations with Ryerson University. If you decide to participate, you are free to withdraw your consent and to stop your participation at any time without penalty or loss of benefits to which you are allowed. At any particular point in the study, you may refuse to answer any particular question or stop participation altogether.

Questions about the Study:

If you have any questions or concerns, please do not hesitate to call Sai

Chaitanya at 416-979-5000 ext. 2523. In addition to the student researchers and their supervisor, The Research Ethics Board may also be contacted should there be any complaints or concerns about the project, c/o Office of the Vice President, Research and Innovation, Ryerson University, 350 Victoria St., Toronto, ON M5B 2K3, Tel: 416-979-5042.
161

162

Agreement:

Your signature below indicates that you have read the information in this agreement, have had a chance to ask any questions you have about the study, and know that your participation is entirely voluntary. Your signature also indicates that you agree to be in the study and have been told that you can change your mind and withdraw your consent to participate at any time. You have been given a copy of this agreement.

You have been told that by signing this consent agreement you are not giving up any of your legal rights.

____________________________________

Name of Participant (please print)

_____________________________________

__________________

Signature of Participant

Date

163

_____________________________________

__________________

Signature of Investigator

Your signature below indicates that you agree to be video-taped during the study.

___________________________________

____________________

Signature of Participant

Date

164

Appendix C
Questionnaire
Vibro-Motion: Pre-study Questionnaire

Purpose of pre-study questionnaire: The purpose of this question is to collect general information about you and your music preferences. It should take less than 5 minutes to complete this questionnaire.

1. Please indicate your age: (please check one)  18 Â­ 24  25 Â­ 34  35 Â­ 44  45 Â­ 54  55 Â­ 64  65 +

2. Please indicate your gender: (please check one)  Male  Female

165

3. What is your highest level of education completed? (Please check one)  No formal education  Elementary school  High School  College  University  Graduate School

4. How often do you use a computer? (Please check one)  Everyday  Every 2 Â­ 3 days  Once a week  Once a month  Never

5. How often do you listen to music? (Please check one)  Everyday  Every 2 Â­ 3 days  Once a week  Once a month  Never

166

6. What genres of music do you listen to? (Please check all that apply)  Rock  Rap/Hip-hop  Country  Classical  Metal  Other:_________________

7. How do you experience music? (Please check all that apply)  iPod/MP3 Player/Portable CD player  Computer speaker system  Home theatre system  Stereo  Live concerts  I don't listen to music

8. Why do you listen to music? (Please check most important one)  Enjoyment  Relaxation  Therapy  Emotional Experience

167

 Distraction  I don't listen to music  Other:_________________

9. What do you do most often when listening to music? (Please check one):  Driving  Work  Exercise (running/walking/etc.)  Have a meal  I don't do anything when listening to music  Other:_________________

10. What do you do most often when listening to music? (Select all that apply):  Stringed instrument __________________  Woodwind __________________  Percussion instruments __________________  Other _________________________  I do not play any musical instruments.

11. How long have you been playing your musical instrument?

168

12. What gaming consoles devices do you own??  Microsoft Kinect  Sony Playstation Move  Nintendo Wii  None

13. For what types of games do you use the gesture feature (please check all that apply)?  Sports  Racing  Shooting  Platformer  Dancing  Puzzles  I do not use or have a game console that has gesture recognition.  Other, please specify _____________________________.

14. Rate the likability of the gesture feature of your game console (the gesture feature is where you use your body movement to control or manipulate a game).

169

I

like

the I like

somewhat It is neither I the good nor bad

somewhat I dislike the I the gesture feature

do

not a

gesture feature

dislike gesture feature

have gesture feature console.

gesture feature

15. Why do you like or dislike the gesture feature of your game console?

170

Gesture-vibe Post-Study Questionnaire

Purpose of the post-study questionnaire: The purpose of this questionnaire is to gather your opinion of the system, the difficulty of using the system as well as your likes and dislikes of the experience. It should take you about 15 minutes to complete this questionnaire.

It It was very somewhat easy. easy.

was It was neither It easy hard. nor somewhat hard.

was It was very hard.

1.

How

easy was it to use the system in one.

general?

Please

circle

2. Rate your level of fatigue for the following (please check one for each body part).

Body part

Not tired at all

Somewhat tired

Very tired

Arms

Hands

Legs

171

Torso

Eyes

3. Rate the ease of using each of the following dimensions with your body.

Dimension

Very to do

difficult Difficult to do

Neither or difficult

easy Easy to do

Very easy to do

Change

the

strength of the signal

Change frequencies

the

Change

the

location of the signal

Repeat what I felt from the Emoti-chair

172

4. Please indicate the level of demand on you for the following elements when using the gestures to control the Emoti-chair:

173

5. Rate your level of agreement with the following statements:

Strongly agree

Agree

Neither agree Disagree nor disagree

Strongly disagree

The vibrations on the Emoti-chair did what I thought they were going to do

174

I had fun

It

was

hard

to the

control

vibrations with my gestures

It

was

easy

to the

understand mapping my what

between and was

gestures

happening on the chair

I could not make the system do what I wanted it to do

I am satisfied with what I produced

6. What did you like most about the gesture-vibe?

175

7. What

did

you

like

least

about

the

gesture-vibe?

Optional Part 2: To be completed after free-play:

8. What was most important to you during free-play? (Mark all that apply)  Controlling the strength of the vibration  Controlling the frequency of the vibration  Controlling the position of the vibration  Other: _____________________________

9. Rate the level of difficulty you had with making the vibrational patterns during free-play (please circle one)?

Very easy

Easy

Neither easy Hard. nor hard.

Very hard.

176

10. How enjoyable did you find making vibrational patterns during free-play (please circle one)?

Very enjoyable

Somewhat enjoyable

Neutral

Not enjoyable

very Not

at

all

enjoyable

11. Rate the accuracy of the translation between your gestures and the vibrational patterns produced on the Emoti-chair (Please circle one)

My gestures were translated very accurately.

My were

gestures Some gestures were accurate, while gestures were not. some

Most of my None of my gestures were gestures not translated were accurately. translated accurately.

translated somewhat accurately.

12. What

did

you

want

to

accomplish

in

the

creative

segment

of

the

test?

177

13. What

were

you

able

to

accomplish

in

the

creative

segment

of

the

test?

Appendix D
Source Code:
The source code for the Visual Studio C# solution and the corresponding MATLAB analysis code is available at: https://www.dropbox.com/sh/e4blsukqgaaryhv/6EU6-7kOPf

178

Appendix E
List of constants used in the program

const int frequencyZero = 0;

This constant provides the value for 0 Hz.

const int frequencyLow = 30;

This constant contains the value for the low zone's highest frequency.

const int frequencyMid = 270;

This constant contains the value for the middle zone's highest frequency.

const int frequencyMax = 300;

This constant contains the high zone's highest frequency, and also the highest frequency played by the system.

int rangeMid;

This variable holds the y-coordinate range of the middle zone.

int rangeHigh;

This variable holds the y-coordinate range of the high zone.

179

int rangeLow;

This variable holds the y-coordinate range of the low frequency zone.

const int frequencyRangeLow = 30;

The range of the frequencies in the low frequency zone.

const int frequencyRangeMid = 240;

The range of the frequencies in the middle frequency zone.

const int frequencyRangeHigh = 30;

The range of the frequencies in the high frequency zone.

double ratioLow;

The ratio between the low frequency range and the y-coordinate range of the low frequency zone.

double ratioMid;

The ratio between the middle frequency range and the y-coordinate range of the middle frequency zone.

double ratioHigh;

The ratio between the high frequency range and the y-coordinate range of the high frequency zone. 180

const int amplitudeMax = 157;

The maximum signal strength played through the system. This value 157 complies with the highest volume gain value in MAX/MSP.

const int amplitudeZero = 0;

This is a zero amplitude variable.

int rhClose;

This is the closest right hand coordinate (z-coordinate).

int rhFar;

This is the furthest right hand coordinate (z-coordinate).

int rangeAmplitude;

This is the range of the z-coordinate positions of the right hand (The range between the closest and the furthest right hand positions)

double ratioAmplitude;

This is the ratio between the range amplitude and the maximum amplitude (157).

int screenX;

The screen x-coordinate for drawing on canvas.

181

int screenY;

The screen y-coordinate for drawing on canvas.

const int zScale = 1000;

This variable is used to convert the z-coordinate depth into milliseconds. It is set of 1000 by default.

const int NUMBER_OF_CHANNELS = 8;

The total number of channels are 8, to comply with the total channel count of the Emoti-Chair.

Double amplitudeWindowSize;

The size of the amplitude window.

double[] channelAmplitudes; //0-7 correspond to channels 1-8

This array contains all the channel amplitudes.

double amplitudeWindowHIGH;

This value contains the highest position of a given channel.

double amplitudeWindowLOW;

This value contains the lowest position of a given channel.

double amplitudeWindowSize_Half;

182

This variable holds the size of half of the window size.

Point amplitudeChannels; //x stores low range, y stores high

This point is used to store the low and high locations, the x-coordinate is used to store the low range, and the high location is stored in the y-coordinate.

double[] amplitudeChannelHigh;//stores the highest positions from channel 1-8. (0-7)

This array holds the high locations for all the channels.

double[] amplitudeChannelLow;//stores the lowest positions from channel 1-8. (0-7)

This array holds the low locations for all the channels.

DispatcherTimer recordTimer;

This is the record timer used to record the skeleton tracking data.

Boolean record = false;

This Boolean variable is used to control when to record the skeleton tracking data.

string recordFilename;

This string is used to name the file for recording the skeleton tracking data.

string vtcFilename;

this string contains the location for the variable stimuli file with the stimuli parameters.

183

StreamWriter recordLog;

Used for recording.

StreamWriter vtcLog;

Used for recording the variable stimuli data into a file.

int timestamp;

This variable is used to keep track of time elapsed while recording.

string recordTag;

This string is used to set up each line before writing to the recording line.

DispatcherTimer Muter;

Controls how long the system is muted allowing the stimulus vibrations to be played without participant's movements affecting the vibrations.

Boolean mute = false;

int stimulusPlayBackTimeSpan = 0;

This variable controls how long the muter timer is activated after pressing the playback button.

DispatcherTimer countdownTimer;

184

This timer controls the animation to show the count down on the screen before the stimulus is played.

int countdown = 3;

this variable makes the countdown start at 3

calibrationStruct calibrationData;

This structure contains the calibration data.

Boolean finalAttempt = false;

Used to mark the participant's final attempt into the data stream that is being recorded into the recorded data.

int currentStimulus = -3;

this marks the stimulus count to start at -3 which allows the participants to practice with stimuli -3,-2 ,-1 and 0 before stimulus 1 is played and recorded.

int totalstimuli = 17;

VariableTestCase vtc;

DispatcherTimer vtcTimer;

int vtcCounter = 0; //increments by 100 each time, representing 100 ms

double freqOut; 185

This variable is for the output frequency that's played through the game.

private UdpWriter OSCSender = new UdpWriter("127.0.0.1", 6601);

private UdpWriter MusicVizSender = new UdpWriter("127.0.0.1", 6771);

int replaycount = 0;

This variable counts the number of times a stimulus was replayed.

int MidWindowSize;

This variable contains the mid window size of the vibrational window.

int NeighborWindowSize;

This variable contains the upper and lower neighbouring window sizes of the vibrational window.

const int MidWindowStrength = 80;

The mid window's strength can be initialized using this constant.

const int NeighbouringWindowStrength = 10;

The neighbouring windows' strength can be initialized using this constant.

186

Appendix F
Vibro-Motion Timers:
There are four timers used in the Vibro-Motion program to perform various tasks including:

1. Recording Timer: Recording the skeleton data tracked by the Kinect. 2. Countdown Timer: Providing the countdown before the vibrational stimulus is played. 3. Muter Timer: Disabling the user input for the duration of the vibrational stimulus 4. Variable Test Case (VTC) Timer: Providing the vibrational stimulus and returning enabling the user input after the vibrational stimulus is played.

Each of these timers will be discussed in detail below.

The Recording timer: The Recording Timer is used to capture hand movements of study participants at regular intervals so that their gestures can be further analyzed in MATLAB (See section 3.12.5). When the system is operating optimally, recording occurs every 2 or 3 milliseconds. The Recording Timer also increases a time stamp at every tick in increments. This time stamp is used each time the program records hand movements made by the participants. When these hand movements are captured, the program records the X, Y and Z co-ordinates of the participant's left and right hand joints, the frequency and the amplitude of the channels.

187

The Countdown Timer: The Countdown Timer is used to display a timed countdown from 3 to 1 before the computer outputs the vibrational stimulus to the user. The purpose of this timer is to provide a "3..., 2..., 1..." countdown for the users so they can prepare for the stimulus that will be presented. This timer ticks three times over a total of 500 milliseconds, with each timer tick occurring at 166 milliseconds. At the beginning of the countdown timer the countdown value is set to three and decreases by one number at each tick as the timer counts down to zero. The countdown value is displayed within the "Kinect Skeleton Frame Handling Event". The Kinect skeleton frame handling event is the thread which handles the calculation of the frequency, amplitude and position variables; this thread also tracks what elements need to be displayed on screen.

The Muter Timer: The Muter Timer is used to disable the participant input while the vibrational stimulus is output; in some sense, the Muter Timer mutes the participant's actions while the stimulus is played by the Vibro-Motion. The Kinect Skeleton Frame Handling Event does not track movement of participants if the Muter Timer is active. This timer is activated when the participant clicks the "Start", "Next", or "Replay" buttons which allow the program to output the stimuli. The timer will then finish after the vibrational stimulus is output, for a total duration of 4 seconds: 500 milliseconds for the countdown, 3 seconds for the vibrational stimulus, and another 500 milliseconds for a small pause before the program tracks the participant movements.

188

The Variable Test Case (VTC) Timer: The Variable Test Case Timer is used to calculate the parameters of the vibrations during the stimulus. This timer is connected to the Variable Test Case class. Creating parameters for phase 1 stimuli: The VTC class is named "Variable Test Case" class; however this class is also referred as Variable Stimuli class in order to avoid confusion with the vibrational window cases. The VTC class contains the details for all of the vibrational stimuli in the program.

The Variable Test Case class:
initialFrequency; initialSignalStrength; initialRHC; finalFrequency; finalSignalStrength; finalRHC; (RHC = right hand coordinate) rateOfChangeFrequency; rateOfChangeRHC; rateOfChangeSignalStrength; timedelay; timespan; vtcTickTime = 100;

Figure 53: The structure of the Variable Test Case Class.

Here the initial frequency, final frequency, right hand coordinate (RHC) and the vibrational strength is recorded. The VTC class also contains data that informs the rate of change for the frequency, RHC, vibrational strength, time span of the vibrational stimulus, the time delay before the stimulus begins

189

output and the "vtcTickTime" which dictates the rate of change of frequency, vibrational strength, and the RHC. The time delay is set to 500ms and the time span is set to 3000ms, which are constant for all the stimuli experienced by the users.

The VTC class processing for phase 1 stimuli is basic as the initial and the final frequencies have the same frequency value and therefore rates of change are zero (see Figure 55).

Table 16: An example of VTC class variables for phase 1 stimuli patterns.

Variable

Value

Units

timespan

3000 Milliseconds

timedelay

1000 Milliseconds

initFreq

50 Hertz

finFreq

50 Hertz

initStrength

80 % of Total Strength

finStrength

80 % of Total Strength

initChPos

1 Channel Position

190

finChPos

1 Channel Position

The right hand position is chosen using channel positions, which are then calculated within the class before the initial and final right hand coordinates are set.

The initial and final right hand positions are calculated using the channel location, the total range of movement of the right hand and the lowest Y co-ordinate on screen. The pseudo-code displayed below outlines the formulas used to perform this process:

a = channelSize Ã positionOfChannel 1 b = channelSize Ã positionOfChannel b a a+ 2 c= 100 right hand coordinate = c Ã right hand range + right hand minimum coordinate

Figure 54: The formulas used to calculate the right hand coordinate.

Creating parameters for phase 2 stimuli: The initial and final values of the VTC variables are calculated using the same method as the phase 1 VTC variables however, to control the changes between the initial and final values, rate of change variables are also calculated for frequency, strength and channel position.

The rate of change of vibrational strength is calculated using the initial value, final value, time span and the vtcTickTime as follows:

191

final frequency initial frequency timespan final right hand coordinate initial right hand coordinate rocr = vtcTickTime Ã timespan final signal strength initial signal strength rocs = vtcTickTime Ã timespan rocf = vtcTickTime Ã

rocf = Rate of change of freqency rocr = rate of change of right hand coordinate rocs = rate of change of signal strength

Figure 55: The process of calculating the rate of change variables for frequency, right hand coordinate and vibrational strength.

The "rocs" variables are calculated using the rate of change formulas as shown above where the difference between the initial and final vibrational strength creates the change in the y-axis and the timespan creates the total change in the x-axis. Dividing the total y-domain change by the total value of the x-domain provides the rate of change value. Multiplying this value with the vtcTickTime normalizes the Rate of Change of Strength per timer increment.

These rates of changes for frequency, RHC and vibrational strength are used to increase and decrease the frequency, amplitude and position of the signal for output.

The VTC Timer Tick Event calculates the active frequency, vibrational strength and RHC for output through the Emoti-Chair and sends the UDP stream to MAX/MSP using the OSC message handler.

This event handler also ensures that the countdown timer is displayed to the user and that the tactile stimulus has finished playing. It then deactivates the Muter Timer, activates the Record Timer, and deactivates the vtcTimer.

192

The flowchart below illustrates the way in which the four timers in this system interact with each other is displayed below:

Figure 56: A flowchart illustrating the way in which the four timers interact with one another in the system.

193

Matlab Analysis of Kinect's Tracked Data

Sample line of tracked data that's stored in the recorded files Data stored in the recorded files can be parsed to determine the right hand position tracking information as well as frequency and vibrational strength information. The time stamps are also parsed during the same process. For sample data and a parsing example see Figure 57 and Table 17.

Sample Line: 357,142,2046/147,133/70/50/38832 Format: Left hand X, Left hand Y, Left hand Z/ Right hand X, Right hand Y/ Frequency Out/ Amplitude Out/Timestamp

Figure 57: A sample of the recorded data from a participant in the study. This data is parsed to gather the above information.

Table 17: Data extracted from the sample line in Figure 57.

Variable

Value

Left hand X

357

Left hand Y

142

Left hand Z

2046

194

Right hand X

147

Right hand Y

133

Frequency Out

90

Amplitude Out

50

Timestamp

38852

195

The code process for calculating the vibrational strengths for the first design strategy:
1) The first step in this method is to search all of the upper and lower bounds of the channels to determine which channel contains the vibrational position; this channel location will be saved in the "wlocation" variable. See Figure 33. 2) The second step in this method involves classifying the window position into one of the three aforementioned, possible cases. Case 1 and Case 2 are straightforward in their classification because the entire signal lies within one channel and the amplitude of the channel is the total amplitude of the "amplitudeOut" variable. This is calculated using the z-coordinate of the right hand as discussed in the Amplitude section (see section ## 3) Amplitude:). See Figure 33 and Figure 34. 4) For Case 3, the vibration window's strength must be divided into two parts based on its positioning over the two channels that share the vibrational window. The amplitude of the channels can be adjusted based on the percentage of the vibration window that overlaps a channel (See Figure 35).

The pseudo-code for calculating the vibrational position is as follows:

196

 

 

Right Hand Y Coordinate  channel  . lowerbound  Right Hand Y  channel  . upperbound  wlocation = 

Figure 58: Formula for calculating the mid-point location of the vibrational window.

 (Right Hand Y Coordinate <= amplitudeChannelLow 0 +

vibrationalwindow )) 2

 

channel 0 Amplitude = amplitudeOUT vibrationalwindow )) 2

 (Right Hand Y Coordinate >= amplitudeChannelLow 7



channel 7 Amplitude = amplitudeOUT

Figure 59: The Pseudocode that handles the occurrence of Case 1 and Case 2 vibrational window placements.

197

Else upper segment size = amplitudeChannelHigh wlocation Right Hand Y Coordinate lower segment size = Right Hand Y Coordinate amplitudeChannelLow wlocation  upper segment size  lower segment size  upper segment size channel amplitudes wlocation = amplitudeOUT Ã ( ) amplitude window size channelAmplitudes wlocation + 1 amplitude window size upper segment size = amplitudeOUT Ã ( ) amplitude window size   upper segment size > lower segment size lower segment size channel amplitudes wlocation = amplitudeOUT Ã ( ) amplitude window size channel amplitudes wlocation + 1 = amplitudeOUT amplitude window size lower segment size Ã ( ) amplitude window size

Figure 60: Pseudocode for calculating the channel strengths for Case 3.

198

The code process for calculating the vibrational strengths for the second design strategy:

Each of the sections of the vibrational window is considered separately when computing the vibrational strength of each of the affected Emoti-Chair's channels. Channel strengths are allocated in the following way:

1) The first step involves locating the upper and lower bounds of each of the vibration channel windows. In order to do this, the mid-window's upper and lower bounds are calculated based on the y-coordinate set by active location of the right hand. The mid-window's upper bound becomes the lower bound for the neighbouring-window that is above the mid-window; and the mid-window's lower bound becomes the upper bound for the neighbouring-window that is below the mid-window, see Figure 33. 2) The next step involves finding the location of the channel which contains the center of the midwindow; the process for this is the same as the process discussed in section 3.5.5. 3) In the final step, the channel vibrational strength is then applied to each channel based on the quantity of overlap in the channel. The algorithm checks each section of the vibration window and how much of it overlaps with adjacent channels. Since each section of the vibrational window is processed separately, they can be classified into the same three cases as in Figure 34 and Figure 35.

199

Step 1:
//Initializing Mid Window.. location 1. MidWindow.setUpperBound( (Right_Hand.Y + (MidWindowSize/2) )); Midwindow.setLowerBound( (Right_Hand.Y - (MidWindowSize/2))); Midwindow.setStrength(80.0); //Initializing Lower Window.. location 0 NeighbourWindowLower.setUpperBound(MidWindow.getLowerBound()); NeighbourWindowLower.setLowerBound((NeighbourWindowLower.getUpperBound() - NeighborWindowSize)); NeighbourWindowLower.setStrength(10.0); //Initializing upper window.. location 2 NeighbourWindowUpper.setLowerBound(MidWindow.getUpperBound()); NeighbourWindowUpper.setUpperBound(NeighbourWindowUpper.getLowerBound() + NeighborWindowSize); NeighbourWindowUpper.setStrength(10.0);

Figure 61: Initialization of Vibrational Window bounds and strength.

The vibrational strength of each channel is the summation of all the vibrational windows and their respective proportions that occupy a channel. Step 2:





   

window i . upperbound  channel 0 . upperbound  window i . lowerbound  channel 0 . lowerbound channelamplitude[0] = window i . strength window i . upperbound  channel 7 . upperbound  window i . lowerbound  channel 7 . lowerbound channelamplitude 7 = window i . strength

Figure 62: Pseudocode for calculating the channel amplitude if the there is an occurrence of Case 1 or Case 2.

200

Step 3:



 

 (channel j . upperbound  window i . upperbound  channel j . upperbound  window i . lowerbound  channel j . lowerbound  window i . lowerbound  (case12 = true))



channel j . upperbound window i . lowerbound window i . upperbound window i . lowerbound percentage Ã window i . strength channelamplitude j += 100  percentage = 100%  channelamplitude j = window i . strength percentage = 100 Ã



channel i . lowerbound  window j . upperbound  channel i . lowerbound  window j . upperbound  channel i . upperbound  window j . upperbound  (case12 = true)

 window i . upperbound channel j . lowerbound window i . upperbound window i . lowerbound percentage Ã window i . strength channelamplitude j += 100  percentage = 100%  channelamplitude j = window i . strength percentage = 100 Ã

Figure 63: Pseudocode for calculating the channel amplitudes for Case 3.

201

Once the vibrational window is established, the program checks if the window section creates either Case 1 or Case 2 and if the window position falls into either of these cases. The total strength of the window is assigned to the channel that contains the vibrational window since the vibrational window's section falls within the channel entirely. The algorithm that checks the overlap between the channels and the vibrational window iterates through all the sections of the vibrational window to determine the individual channel vibrational strengths. Once the overlap is calculated, channel strengths are assigned.

The last step involves normalizing the channel vibrational strengths. This is calculated based on the channel with the highest vibrational strength and the amplitude output, calculated using the z coordinate of the right hand. This calculation is performed by locating the channel with the highest vibrational percentage and creating a normalized value by dividing the output amplitude using the highest vibrational percentage among all the channels. The channel vibrational strength is multiplied by the normalized value which then normalizes all the channel vibrational strengths. This method is explained in the pseudo-code as below:



             

= 100 { 

> 

    

  =   =    

}  }  {   =   Ã    

Figure 64: The pseudocode for channel amplitude normalization.

202

Figure 65 shows an example of how the output signal is distributed across more than one channel. The stimulus appears approximately mid-way in Channel 4 (as indicated with the addition symbol on the yaxis). Using the span of the vibrational window over the channels, it can be calculated that 25% of the upper neighbouring window overlaps with Channel 2, while 75% of the upper neighbouring window overlaps with Channel 3. 25% of the mid-section window overlaps with channel 3 while 75% of the midsection window overlaps with channel 4. 25% of the vibrational lower neighbouring window section overlaps with channel 4 and 75% of the lower neighbouring window section overlaps with channel 5. This results in the channel vibrational strength being allocated as follows:

Table 18: An example of a vibrational window strength allocation using the second design strategy

Percentage of Strength Assigned from Vibrational Windows

Lower Neighbour Window Section Mid Window Section Upper Neighbour Window Section Total (Before Strength = 10% Strength = 80% Strength = 10% Normalization) Strength

Channel 2

25%

0%

0%

2.50%

Channel 3

75%

25%

0%

27.50%

Channel 4

0%

75%

25%

62.50%

Channel 5

0%

0%

75%

7.50%

203

204

Figure 65: An example of the Vibrational Window designed using the second design strategy.

The next step in this process involves using the calculated percentages and channel amplitudes to normalize all of the channel vibrational strengths. This is done by identifying the channel with the greatest vibrational strength and then dividing the amplitude output by the largest vibrational strength. From this calculation, the resulting ratio is multiplied with all the channel vibrational strengths.

In a scenario where the amplitude output is set to 150, the normalizing ratio would be (150/62.5) or 2.4 when expressed as a normalization value. This normalizing ratio is multiplied against all the channel vibrational strengths and the new vibrational strengths are calculated as shown in the table below.

Table 19: An example of channel strength allocation.

Total Strength per Channel before Normalization Normalization Value Vibrational Strength

Channel 2

2.50

2.4

6

Channel 3

27.50

2.4

66

Channel 4

62.50

2.4

150

Channel 5

7.50

2.4

18

205

Communication between C# and Max/MSP:
Once the output frequency and channel amplitudes are calculated in C#, the C# program communicates with MAX/MSP which produces the vibrations using the frequency, strength and the channel positions assigned using the vibrational window. using a User Data Protocol (UDP) stream. This is done using the loopback IP address of 127.0.0.1 along with the UDP packets which are received by MAX/MSP. A third party and open source library called Ventuz-OSC (Open Sound Control) is used to program the UDP communication modules in C# and in MAX/MSP.

The port number 6601 was selected to allow communication between the Vibro-Motion program and the MAX/MSP patch. The input/output (I/O) messages were sent using the "OSCSender.send()" function. These messages can be bundled, and entire bundled messages can be sent through UDP. MAX/MSP will decode and separate these messages from the bundle and play the vibrational output through the Emoti-Chair.

The OSC message format is structured using identifiers separated by the ` / ` symbo l. An example of this is shown below:
OscMessage message = new OscElement("/joint/element/Frequency", frequencyOUT); bundle.AddElement(message);

Figure 66: An example of the OSC message to transmit frequency to the MAX/MSP patch.

206

The above two lines of code set the output frequency value to the tag, `/joint/element/Frequency'. This allows MAX/MSP to decode the message to extract the frequency that needs to be output through the Vibro-Motion.

MAX/MSP Patch: As seen in Figure 67, the MAX/MSP patch allows port 6601 to be set internally. And the object OSCroute listens for any OSC packets sent to port 6601. When an OSC message is received by MAX/MSP, the program guides the appropriate tags to the corresponding output stream using the OSC-route objects. Tags matching `joint/element/Frequency/' are used to extract the frequency to be set to the vibrational output and the cycler object is used to create a sine wave using the frequency extracted from the OSC message sent from C#.

207

Figure 67: The MAX/MSP patch

The channel amplitudes calculated by the program are also sent to MAX/MSP using this same tagging method. Once these variables are communicated to the MAX/MSP patch, the sinusoid wave with the appropriate frequency is routed to all the channels in the Emoti-Chair. While this occurs, the channel amplitudes are used to set the strength of each of the channels. The channel vibrational strengths are routed using the vibrational strength tag for each channel. The integer value associated with the tag controls the strength of the vibration that the channel will output to the user.

208

In Table 19, where only Channel 2, Channel 3, Channel 4, and Channel 5 are producing a tactile output, the other channels will have a vibrational strength of 0 and will therefore output no tactile information for the user in these channels.

The last element in the MAX/MSP patch is the dac~ object (see Figure 67). This object connects to any Digital Audio Workstation that is connected to the computer. The setup uses the PreSonus Firepod with audio amplifiers to play vibrational stimuli through the Emoti-Chair.

209

References
1. 2. 3. Argenot, M. (1973). "Les traites de 'eloquence do corps". Semiotica, 8(1), 60-82. Aronson, J. (1994). A pragmatic view of thematic analysis. The Qualitative Report, 2(1). Austin, G. (1966). London 1806, Ed. Robb M.M., Thonssen, L. Chironomia or, a treatise on rhetorical delivery. Southern Illinois University Press: Carbondale. 4. Bach-y-Rita, P. (2004). Tactile sensory substitution studies. Annals of the New York Academy of Sciences, 1013, 83-91. 5. Barbre, C., Akers, A. M., Hirschman, L., Gorewitz, S., Drobin, F. A., Musgrave, B. A., Liechty, D., Epperly, B. G., Crockett, C., Lynch, A.S.K., Shaw, B., Downey, G., Hutchinson, M.T., Humez, N., Brandenstein, N., Bouchard, C., Ross, T.M., Ulanov, B. (1999). Reviews: Books, films, music. Journal of Religion and Health, 38(4), 347-374. 6. Barrett, K., Boitano, S., Barman, S., & Brooks, H. (2009).Ganong's review of medical physiology. (23 ed., p. 150). Nodia: Tata Mcgraw Hill Education Private. 7. Behringer, R. (2007). Gesture interaction for electronic music performance. Human Computer Interaction, 564-572. 8. Bikah, M., Hallbeck M.S. & Flowers, J.H. (2008). Supracutaneous vibrotactile perception threshold at various non-glabrous body loci. Ergonomics, Volume 51, Issue 6, 920-934. 9. Biles, J. A. (1999). GenJam: Interacting with a musical IGA. International Conference on Systems, Man, and Cybernetics, 3, 652-656. 10. Birnbaum, D. M., & Wanderly, M. M. (2009). A systematic approach to musical vibrotactile feedback. In Proceedings of the International Computer Music Conference (ICMC), 2, 397-404.

210

11.

Blaine, T., & Forlines, C. (2002). Jam-O-world: Evolution of the jam-O-drum multi-player musical controller into the jam-O-whirl gaming interface. Proceedings of the 2002 Conference on New Interfaces for Musical Expression, Dublin, Ireland. 1-6.

12.

Borchers, J., Lee, E., Samminger, W., & MÃ¼hlhÃ¤user, M. (2004). Personal orchestra: A real-time audio/video system for interactive conducting. Multimedia Systems, 9(5), 458-465.

13.

Bottoni, P., Faralli, S., Labella, A., & Pierro, M. (2006). Mapping with planning agents in the max/MSP environment: The GO/max language. Proceedings of the 2006 Conference on New Interfaces for Musical Expression, Paris, France. 322-325.

14.

Branje, C., & Fels, D. I. (2012, submitted for review). The vibrochord: Validating a theoretical framework for the evaluation of musical input devices, Computer Music Journal.

15. 16.

C2 tactor. Retrieved May 15, 2012, from http://www.eaiinfo.com/Tactor%20Products.htm Cahill, T. (1897). Teleharmonium [Print Photo]. Retrieved May 15, 2011, from

http://upload.wikimedia.org/wikipedia/commons/5/56/Teleharmonium1897.jpg 17. 18. 19. Canadian Center for Occupational Health and Safety. Office Ergonomics Safety Guide, 2005. Canny, J. (2006). The future of human-computer interaction. Queue - HCI, 4(6), 24-32. Carr, D. (2004). Music, meaning, and emotion. The Journal of Aesthetics and Art Criticism, 62(3), 225-234. 20. Chen, L. L. (2009). Using RFID to realize human-computer interaction. Joint Conferences on Pervasive Computing. 859-864. 21. Cheng, M. F. (1968). Tactile-kinesthetic perception of length. The American Journal of Psychology, 81(1), 74-82.

211

22.

Cholewiak, R. W., & McGrath, C. (2006). Vibrotactile targeting in multimodal systems: Accuracy and interaction. Proceedings of the Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 413-420.

23.

Cholewiak, R., & Collins, A. (2000). The generation of vibrotactile patterns on a linear array: Influences of body site, time, and presentation mode. Perception, & Psychophysics, 62(6), 12201235.

24.

Correa, P., Marques F., Marichal, X., & Macq B. (2008). 3D posture estimation using geodesic distance maps. Multimedia Tools and Applications, 38(3), 365-384.

25.

Craig, J., & Evans, P. (1987). Vibrotactile masking and the persistence of tactual features. Perception, & Psychophysics, 42(4), 309-317.

26.

Dayton.

(n.d.).

DAEX25

sound

exciter

pair.

Retrieved

May

15,

2012,

from

http://www.daytonaudio.com/index.php/daex25-sound-exciter-pair.html 27. 28. 29. Drummond, J. (2009). Understanding interactive systems. Organised Sound, 14(2), 124-133. Eckels, S. Z. (Ed.). (2009). Teaching classroom guitar (Teacher`s edition) Lanham: R&I Education. Esmerado, J., Vexo, F., & Thalmann, D. (2002). Interaction in virtual worlds: Application to music performers. Advances in Modelling, Animation and Rendering, 511-527. 30. Field, A (2005). Discovering Statistics Using SPSS, second edition. Thousand Oaks: Sage Publications. 31. Fischer, B., & Ramsperger, E. (1984). Human express saccades: Extremely short reaction times of goal directed eye movements. Experimental Brain Research, 57(1), 191-195. 32. Fohrenbach, S., Konig, A.W., Gerken, J., & Reiterer, H. (2009). Tactile feedback enhanced hand gesture interaction at large, high-resolution displays. Journal of Visual Languages and Computing, 20(5), 341-351. 212

33.

Frati V., & Prattichizzo D. (2011). Using Kinect for hand tracking and rendering in wearable haptics. IEEE World Haptics Conference, 317-321.

34.

Gody, R. I., & Leman, M. (2010). Music and Gestures: Sound, Movement and Meaning. Oxford: Routledge.

35.

Henrotte, G. A. (1992). Music and Gesture: A Semiotic Inquiry. The American Journal of Semiotics, 9(4), 103-114.

36.

HÃ¶ysniemi, J., HÃ¤mÃ¤lÃ¤inen, J., Turkki, L., & Rouvi, T. (2005). Children's intuitive gestures in visionbased action games. Communications of the ACM, 48(1), 44-50.

37.

Hugh, C. (Editor) (1911). Weber's Law. Encyclopedia Britannica (11th edition.). Cambridge, Cambridge University Press.

38.

Hughes M.D., & Bartlett, R.M. (2002). Performance analysis. Journal of Sports Sciences, 10(20), 735-737.

39.

Iverson, J. M., & Goldin-Meadow, S. (1998). Why people gesture when they speak. Nature, 396(6708), 228-228.

40.

Izzetta, F. (1997). Meaning in music gesture. International Association for Semiotic Studies VI International Congress.

41.

Jiang, F., Gao, W., Yao, H., Zhao, D., & Chen, X. (2008). Effort analysis in signer-independent sign gestures. Journal of Experimental & Theoretical Artificial Intelligence, 20(2), 133-152.

42.

Johnson, K.O. (2001). The roles and functions of cutaneous mechanoreceptors. Current Opinion in Neurobiology, 11(4), 455-61.

43.

Jones L., & Sarter N. (2008). Tactile displays: Guidance for their design and application. Human Factors and Ergonomic Society, 50(1), 90-111.

213

44.

JousmÃ¤ki, V., & Hari, R. (1998). Parchment-skin illusion: Sound-biased touch. Current Biology, 8(6), R190.

45.

Juneja,

M.

(2010).

Motion

gaming.

Retrieved

May

15,

2012,

from

http://computer.financialexpress.com/20101011/trend01.shtml. 46. Karam, M., Russo, F. A., & Fels, D. I. (2009). Designing the model human cochlea: An ambient cross modal audio-tactile display. Haptics IEEE, 2(3) 160-169. 47. Kaufman, E. L., Lord, M. W., Reese, T. W., & Volkmann, J. (1949). The discrimination of visual number. The American Journal of Psychology, 62(4), 498-525. 48. 49. Keele, S. W. (1973). Attention and human performance. Pacific Palisades. CA: Goodyear. Kitawaza, S. (2002). Where conscious sensation takes place. Consciousness and Cognition, 11(3), 475-477. 50. Knaian, A. N. (n.d.). Ara's brain opera pictures. Retrieved May 15, 2012, from http://www.mit.edu/people/ara/ 51. 52. 53. Kuchinskas, S. (2010). Naked gaming. Scientific American, 302(2), 24-24. Leap motion. (2012, May 12). Retrieved May 15, 2012, from https://leapmotion.com/. Lederman, S.J. (1997). Skin and Touch. Encyclopedia of human biology, Volume 8. (2nd edition, pp. 49-61). San Diego: Academic Press. 54. LeCaine, H. (Designer). (1948). Electronic Sackbut [Web Graphic]. Retrieved May 15, 2012, from http://www.hughlecaine.com/images/sb4.jpg. 55. Lim, S.C, Kim, S.C, Kyung, K.U, & Kwon, D,S. (2006). Quantitative analysis of vibrotactile threshold and the effect of vibration frequency difference on tactile perception. SICE-ICASE, 1927-1932. 56. Lynch, K. (2009, November 13). Game review: Band hero. Retrieved May 15, 2012, from http://www.mirror.co.uk/lifestyle/staying-in/video-games/game-review-band-hero-430688. 214

57.

Machover, T. (1996).

Brain opera project overview. Retrieved May 15, 2012, from

http://park.org/Events/BrainOpera/project-overview.html. 58. Marks, L. E. (1989) On Cross-Modal Similarity: The Perceptual Structure of Pitch, Loudness, and Brightness. Journal of Experimental Psychology: Human Perception and Performance, 15(3), 586-602. 59. Marshall, M. T. (2005). The viblotar - a big box that makes noise (and vibrates too!). Technical Report (Unpublished), McGill University. 60. Marshall, M. T., & Wanderley, M. M. (2006). Vibrotactile feedback in digital musical instruments. Proceedings of the 2006 Conference on New Interfaces for Musical Expression, Paris, France. 226-229. 61. 62. 63. 64. McGrain, M. (1990). Music notation. Boston: Berklee Press. McNeil, D. (2005). Gesture and thought. Chicago: University Of Chicago Press. Meirovitch, L. (1975). Elements of vibration analysis. New York: McGraw-Hill. Menelas, B. (2011). Haptic and gesture-based interactions for manipulating geological datasets. Systems, Man and Cybernetics (SMC), 2051-2055. 65. Merola, G. (2007). Emotional gestures in sport. Language Resources and Evaluation, 41(3/4), 233-254. 66. Munger, B.L., Pubols, L.M., Pubols, B.H. (1971) The Merkel rete papilla -- a slowly adapting sensory receptor in mammalian glabrous skin. Brain Research. 29(1), 47-61. 67. Nattiez, J.J. (1990). Music and discourse: Toward a semiology of music. Princeton University Press, 48-55. 68. Ng, K. (2002). Interactive gesture music performance interface. Proceedings of the 2002 Conference on New Interfaces for Musical Expression, Dublin, Ireland, 1-2. 215

69. 70.

Oborne, D. J. (1987). Ergonomics at work. University College of Swansea: John Wiley & Sons. Overholt, D. (2009). The musical interface technology design space. Organised Sound, 14(2), 217226.

71. 72.

Paradiso, J. A. (1997). Electronic music: New ways to play. IEEE Spectrum, 34(12), 18-30 Paradiso, J. A. (2002). Digital Baton [Web Graphic]. Retrieved May 15, 2012, from http://web.media.mit.edu/~joep/TTT.BO/Baton2.gif

73.

ParÃ©, M., Mazurkiewicz, J.E., Smith, A.M., and Rice, F.L. (2001). The Meissner Corpuscle Revised: A Multiafferented Mechanoreceptor with Nociceptor Immunochemical Properties. The Journal of Neuroscience, Volume 21, Issue 18, 7236-46.

74.

Pouris, M. & Fels, D. I. (2012). Creating an entertaining and informative music visualization. ICCHP 2012 - Linz. Lecture Notes in Computer Science. 7382. 451-458.

75. 76.

Prasad, J. S., Nandi, G. C., & Kumar, A. (2009). Gesture based music generation, ICETET. 209-214. Prasad, J. S., Saxena, A., Javar, N., Kaushik, K. B., Chakraborty, P., & Nandi, G. C. (2010). Gesture recognition by stereo vision. Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia, 155-162.

77.

PreSonus.

(2010,

July

23).

Firestudio.

Retrieved

May

15,

2012,

from

http://www.presonus.com/products/FireStudio. 78. Proulx M.J. (2010). Synthetic synaesthesia and sensory substitution. Consciousness and Cognition, 19(1), 501-503. 79. Pyramid. (2010). Pb442x: 300 watt, 4 channel amplifier. Retrieved May 15, 2012, from http://www.pyramidcaraudio.com/sku/PB442X/300-Watt-4-Channel-Amplifier. 80. Quintilianus, M. F. (1966). The institutio oratoria of Quintilian. Cambridge: Harvard University Press. 216

81.

Rautaray, S. S., & Agrawal, A. (2010). A novel human computer interface based on hand gesture recognition using computer vision techniques. Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia. 292-296.

82.

Riggs, K. J., Ferrand, L., Lancelin, D., Fryziel, L., Dumur, G., & Simpson, A. (2006). Subitizing in tactile perception. Psychological Science, 17(4), 271-272.

83.

Robineau, F., Boy, F., Orliaguet, J.P., VÃ¡zquez-Buenosaires, J., Demongeot, J., & Payan, Y. (2006). Control of surgical gesture under lingual electro-tactile stimulation. Human-Computer Interaction, 110-117.

84.

Rovan, J., Wanderley, M. M., Dubnov, S. & Dapalle Philippe (1997). Instrumental gestural mapping strategies as expressivity determinants in computer music performance. Proceedings of the Kansei - The Technology of Emotion. 68-73.

85.

Scheibert J, Leurent S, Prevost A, DebrÃ©geas G. (2009). The role of fingerprints in the coding of tactile information probed with a biomimetic sensor. Science, 323 (5920), 1503-1506.

86.

Scherer K.R, & Zentner M.R. (2001). Emotional effects of music : production rules. Music and Emotion: Theory and Research, Oxford University Press, 361-392.

87.

Schertenleib, S., Gutierrez, M., Vexo, F., & Thalmann, D. (2004). Conducting a virtual orchestra. IEEE Multimedia, 11(3), 40-49.

88.

Sheridan,T.B., & Ferrell,W.R. (1963). Remote manipulative control with transmission delay. IEEE Trans, Human Factors Electronics, 4(1), 25-29.

89.

Smirnov, A. (2000). Music and gesture: Sensor technologies in interactive music and the theremin based space control systems, ICMC, 2000.

90.

Snow, Blake (February 1, 2008). "Rock Band Page". GamePro. Original date: 2008-02-04. Retrieved Jan1, 2013. 217

91.

Spasov, M. (2011). Music composition as an act of cognition: ENACTIV - interactive multi-modal composing system. Organised Sound, 16(1), 69-86.

92. 93.

Spruce, G. (1996). Teaching music Routledge. CRC Press. Summers, I. R., Cooper, P. G., Wright, P., Gratton, D. A., Milnes, P., & Brown, B. H. (1997). Information from time-varying vibro-tactile stimuli. Journal of the Acoustical Society of America. 102(6), 3686-96.

94.

Synthhead. (February 15, 2011). Is the Microsoft Kinect useless as a musical tool? Retrieved May 27, 2012, from http://www.synthtopia.com/content/2011/02/15/is-the-microsoft-kinect-

useless-as-a-musical-tool/. 95. 96. The Theremin. (1928). The London Mercury, 17(99). Theremin, L. (Designer) (1919). Etherwave theremin kit. Retrieved April 28, 2012, from http://upload.wikimedia.org/wikipedia/commons/c/c8/Etherwave_Theremin_Kit.jpg. 97. 98. Thilmany, J. (2008). Fluid Gaming Movement. Mechanical Engineering, 130(11), 16. Unehara, M., & Onisawa, T. (2005). Music composition by interaction between human and computer. New Generation Computing, 23(2), 181-191. 99. Ventuz. (2006). Ventuz OSC documentation. Retrieved May 15, 2012, from

http://opensoundcontrol.org/implementation/osc-net-v1-2. 100. Verillo, R.T. (1969). Sensation magnitude of vibrotactile stimuli. Perception and Psychophysics. 6(6a). 366 Â­ 372. 101. Verillo, R.T. (1991). Vibration sensing in humans. Music Perception, 9(3), 281-302. 102. Video4Coach. (May, 05, 2008). Movement analysis with SkillSpector. Retrieved May 15, 2012, from http://video4coach.com/images/SkillSpector%202D%20Intro%20ENG.pdf.

218

103. Vries, S. C., Erp, J. B. F., & Kiefer, R. J. (2009). Direction coding using a tactile chair. Applied Ergonomics, 40(3), 477-484. 104. Wachs, J. P., Kolsch, M., Stern, H., & Edan, Y. (2011). Vision-based hand-gesture applications. Commun. ACM, 54(2), 60-71. 105. Wanderley, M. M. (2003). The importance of parameter mapping in electronic instrument design. Journal of New Music Research, 32(4), 429-440. 106. Ward, J. & Meijer, P. (2010). Visual experiences in the blind induced by an auditory sensory substitution device. Conscious Cognition, 19(1), 492-500. 107. Wunschel, F. (2011, March 11). Hardware latency. Retrieved May 15, 2012, from http://1024d.wordpress.com/2011/03/10/hardware-latency/. 108. Young, D., & Serafin, S. (2003). Playability evaluation of a virtual bowed string instrument. Proceedings of the 2003 Conference on New Interfaces for Musical Expression, 104-108. 109. Young, G. (Uploader) (1999). Electronic sackbut. Retrieved May 15, 2012, from http://www.hughlecaine.com/en/sackbut.html 110. Zamorano, F. (2012). Simpletones: A system of collaborative physical controllers for novices. NIME Online Proceedings. 111. Zangwill, N. (2007). Music, metaphor, and emotion. The Journal of Aesthetics and Art Criticism, 65(4), 391-400. 112. Zhang, Z. (2012). Microsoft Kinect sensor and its effect. IEEE Multimedia, 19(2), 4-10. 113. Zwicker, E. (1961). Subdivision of the audible frequency range into critical bands. The Journal of the Acoustical Society of America, 33(2), 248-248.

219

