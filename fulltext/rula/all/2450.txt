HARDWARE-SOFTWARE CO-SYNTHESIS FOR DISTRIBUTED MEMORY ARCHITECTURES

by

Usman Ahmed, B.Eng., 2000, National University o f Sciences and Technology, Pakistan.

A thesis presented to Ryerson University in partial fulfillment o f the requirement for the degree o f Master o f Applied Science in the program o f Electrical and Computer Engineering

Toronto, Ontario, Canada, 2005 © Usman Ahmed 2005

PROPmTYOF R Y U R S O M libraw

UMI N um ber: E C 53001

All rig h ts re s e rv e d INFORMATION TO USERS

T he quality of this reproduction is dep en d en t upon the quality of the copy subm itted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignm ent can adversely affect reproduction. In the unlikely event that the author did not send a com plete m anuscript and there are missing pag es, th e s e will be noted. Also, if unauthorized copyright material had to be rem oved, a note will indicate the deletion.

UMI
UMI Microform EC53001 Copyright 2008 by ProQ uest LLC All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United S ta te s Code.

P roQ uest LLC 789 E ast E isenhow er Parkway P.O. Box 1346 Ann Arbor, Ml 48106-1346

I hereby declare that I am the sole author o f this thesis.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose o f scholarly research.

USman Ahmed

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request o f other institutions or individuals for the purpose o f scholarly research.

Usman Ahmed

u

Ryerson University requires the signatures o f all persons using or photocopying this thesis. Please sign below, and give address and date.

m

Hardware-Software Co-Synthesis for Distributed Memory Architectures
Usman Ahmed, Master o f Applied Science, 2005, Electrical and Computer Engineering, Ryerson University

Abstract
Hardware software co-synthesis problem is related to finding an architecture, subject to certain constraints, fo r a given set o f tasks that are related through data dependencies. The architecture consists o f a set o f heterogeneous processing elements and a communication structure between these processing elements. In this thesis, a new algorithm fo r co-synthesis is presented that targets distributed memory architectures. The algorithm consists o f four distinct phases namely, processing element selection, pipelined task allocation, scheduling and best topology selection. Selected processing elements are finally mapped to a regular distributed memory architecture comprising o f mesh, hypercube or quad-tree topology. The co-synthesis method is demonstrated by applying it to MPEG encoder application and various size large random graphs.

IV

Acknowledgements
I would like to express m y gratitude to my supervisor, Dr. Gul N. Khan, for his support during the course o f this project. I am grateful to Ryerson University and POA Educational Foundation for providing scholarships and to National Science and Engineering Research Council o f Canada (NSERC) for providing financial support through a research grant. Support o f Canadian Microelectronics Corporation (CMC) for providing the prototyping systems was also very valuable. Finally, I would like to thank Rehan Ham eed (Stanford University) and Fahad Ali (Gwangju Institute o f Science and Technology) for various discussions on the algorithm and M PEG encoder

implementation.

Table of Contents
A bstract............................................................................................................................................ iv Acknowledgements..........................................................................................................................v Table o f Contents............................................................................................................................vi List o f T ab les................................................................................................................................ viii List o f Figures................................................................................................................................. ix 1 INTRODUCTION.................................................................................................................. 1 1.1 1.2 2 Overview...........................................................................................................................1 Thesis O rganization....................................................................................................... 2

HARDWARE SOFTWARE CODESIGN.......................................................................... 4 2.1 2.2 2.2.1 2.2.2 2.2.3 2.2.4 2.3 2.3.1 2.3.2 2.3.3 2.3.4 2.4 2.5 2.5.1 2.5.2 2.5.3 Overview.......................................................................................................................... 4 Codesign M ethodology..................................................................................................7 Specification/Requirements A nalysis................................................................. 7 Partitioning.............................................................................................................. 8 Estim ation............................................................................................................. 16 Co-simulation / Co-verification......................................................................... 17

Hardware Software Co-synthesis............................................................................... 20 Co-synthesis Phases............................................................................................ 21 Target A rchitecture............................................................................................. 26 Co-synthesis Approaches....................................................................................26 Significant Co-synthesis Environments........................................................... 27

Fault T olerance.............................................................................................................31 Distributed M emory Architectures............................................................................33 Mesh Topology................................... · .....................35

Hypercube T opology.......................................................................................... 35 Tree Topology...................................................................................................... 36

3

HARDW ARE SOFTW ARE COSYNTHESIS FOR DISTRIBUTED M EMORY

SYSTEMS....................................................................................

VI

3.1 3.2

Introduction.....................................................................................................................38 Processing Elem ent Selection and Pipelined Task A llocation............................. 41 Processing Element Selection............................................................................43 Pipelined Task Allocation.................................................................................. 48

3.2.1 3.2.2 3.3

Topology Selection....................................................................................................... 51 Topology Generation and A ddressing..............................................................52 Topology M apping...............................................................................................59 Best Topology Selection.....................................................................................61

3.3.1 3.3.2 3.3.3 3.4 3.5 4

Scheduling....................................................................................................................... 63 Pipeline Period Reduction............................................................................................ 65 67

EXPERIM ENTAL RESULTS..................................................................................... 4.1 4.2 4.3 4.4 4.5

Introduction..................................................................................................................... 67 M PEG E ncoder.............................................................................................................. 67 Parallel M PEG D ecoding..............................................................................................83 Random G rap h s............................................................................................................. 87 Algorithm Execution T im e ................................................................................106

5 6

CONCLUSION AND FUTURE W O R K .........................................................................107 R E F E R E N C E S ..................................................................................................................... HO

vu

List of Tables

Table 4.1 Processing Element Information for M PEG A pplication......................................71 Table 4.2 Task Execution Times for MPEG Encoder A pplication.......................................72 Table 4.3 Time/Area Results for MPEG Encoder....................................................................74 Table 4.4 Topology Information for MPEG Encoder.............................................................. 75 Table 4.5 Time/Area Results for Parallel MPEG Decoding................................................... 85 Table 4.6 Topology Information for Parallel MPEG D ecoding............................................ 86 Table 4.7 Time/Area Results for 50 Node G raphs...................................................................90 Table 4.8 Topology Information for 50 Node Graphs............................................................. 91 Table 4.9 Time/Area Results for 100 Node G raphs................................................................ 93 Table 4.10 Topology Information for 100 Node Graphs.........................................................94 Table 4.11 Time/Area Results for 200 Node G raphs.............................................................. 96 Table 4.12 Topology Information for 200 Node G raphs.........................................................97 Table 4.13 Time/Area Results for 300 Node G raphs.............................................................. 99 Table 4.14 Topology Information for 300 Node Graphs.......................................................100 Table 4.15 Time/Area Results for 400 Node G raphs............................................................ 102 Table 4.16 Topology Information for 400 Node G raphs.......................................................103

viu

List o f Figures
Figure 2.1 Typical System Design Practice.................................................................................5 Figure 2.2 V erilog based Co-sim ulation.....................................................................................19 Figure 2.3 Example Task Graph with Possible M apping....................................................... 25 Figure 2.4 Task Allocation and Sequential Execution.............................................................25 Figure 2.5 Task Allocation and Pipelined E xecution.............................................................. 25 Figure 2.6 Original Task-graph and Task-graph with Fault Detection T a sk s.................... 32 Figure 2.7 Processing Elements Arranged in Mesh Topology with N = 3 ............................ 35 Figure 2.8 Processing Elements Arranged in Hypercube Topology with N = 4 .................. 36 Figure 2.9 Binary Tree with 3 Levels.........................................................................................37 Figure 2.10 Quad Tree with 3 L evels.........................................................................................37 Figure 3.1 Co-synthesis A lgorithm ............................................................................................. 40 Figure 3.2 Processing Element Selection and Pipelined Allocation..................................... 43 Figure 3.3 Pseudo Code for Mesh Topology Generation and Address A ssignm ent......... 55 Figure 3.4 Eight Node Mesh Topology with Address A ssignm ent...................................... 56 Figure 3.5 Pseudo Code for Hypercube Topology Generation and Address Assignment 57 Figure 3.6 Eight Node Hypercube Topology with Address A ssignm ent............................ 57 Figure 3.7 Pseudo Code for Quad-Tree Topology Generation and Address Assignment 58 Figure 3.8 Eight Node Quad-Tree Topology with Address A ssignm ent............................ 59 Figure 3.9 Pseudo code for Topology M apping....................................................................... 61 Figure 3.10 Pseudo code for Scheduling................................................................................... 65 Figure 4.1 M PEG Encoder Task G raph..................................................................................... 69 Figure 4.2 Design Space Exploration for MPEG Encoder A pplication.............................. 74 Figure 4.3 Irregular Processing Element Topology (Test case 1 ) .........................................77 Figure 4.4 Processing Elements for MPEG Encoder Arranged in M esh Topology.......... 77 Figure 4.5 Schedule Map for Mesh Topology.......................................................................... 78 Figure 4.6 Irregular Processing Element Topology (Test case 4 ) ........................................ 79 Figure 4.7 Processing Elements for MPEG Encoder Arranged in Hypercube Topology 79

IX

Figure 4.8 Schedule Map for Hypercube Topology................................................................ 80 Figure 4.9 Irregular Processing Element Topology (Test case 5 ) ......................................... 81 Figure 4.10 Processing Elements for MPEG Encoder Arranged in Tree Topology...........81 Figure 4.11 Schedule Map for Tree Topology.......................................................................... 82 Figure 4.12 Schedule Map with only a Single Processing Element in the S y stem .......... 83 Figure 4.13 Parallel MPEG Decoding Task G rap h ..................................................................84 Figure 4.14 Comparison Results for Parallel MPEG D ecoding............................................ 86 Figure 4.15 Randomly Generated 50-node G raph....................................................................89 Figure 4.16 Design Space Exploration for 50 Node G rap h s.................................................. 92 Figure 4.17 Design Space Exploration for 100 Node G raphs................................................95 Figure 4.18 Design Space Exploration for 200 Node G rap h s................................................98 Figure 4.19 Design Space Exploration for 300 Node G rap h s..............................................101 Figure 4.20 Design Space Exploration for 400 Node G rap h s..............................................104 Figure 4.21 Topology Mapping for 400-node Graph (Graph `e', T p e r i q d = 100000)...... 105 Figure 4.22 Topology Mapping for 200-node Graph (Graph `d ', T period= 125000)...... 105 Figure 4.23 Topology M apping for 300-node Graph (Graph `c ', T period=200000)....... 105 Figure 4.24 Algorithm Execution Tim e................................................................................... 106

CHAPTER 1 INTRODUCTION

1.1

O verview

M odem multimedia, DSP and data communication applications are computationally very intensive. Computational requirements prohibit the use o f a single processor to provide all the functionality at desired throughput. These performance requirements are further strained by low area (power) constraints and small window for time-to-market. Traditionally these systems were developed as a two stream process, hardware engineers delivering general purpose computer systems which were programmed by software engineers. Optimal performance is achieved when hardware and software are properly `tim ed' to each other. Typical design cycle required the system to be partitioned very early in the design cycle leaving very little room for modifications later in the design stage.

Early system partitioning, along with the separate design flows for hardware and software modules, does not fully explore design space and is prone to high cost, inefficient hardware and software. The solution is to combine hardware and software design efforts by considering the efficiency o f both options and to find a design implementation that fulfills all the specification requirements with a minimal cost. Also, with increasing design spaces, selecting a feasible solution from a set o f different design options becomes m ore and m ore demanding and hence a need to automate this process. The automated

process is usually referred to as hardware software co-synthesis. M ain phases in this process include selection o f processing elements (ASIC or general purpose CPU), allocation o f tasks to these processing elements, creating a communication structure between processing elements and assigning start and finish time to each task (scheduling).

In this thesis, a co-synthesis algorithm has been presented that targets regular distributed memory architectures. Regular architectures have multiple communication paths between processing elements and therefore offer inherent support o f fault tolerance. The

algorithm has two distinct phases of pipelined processing element allocation and mapping the processing elements to regular distributed memory architectures. The algorithm iteratively selects processing elements and allocates tasks to these processing elements. Pipeline stages are created during the allocation process. Finally the processing elements are arranged in a regular fault tolerant topology.

1.2

Thesis Organization

This thesis is organized into five chapters. This chapter provides an introduction to co design process and outlines the thesis organization. Chapter 2 surveys hardware software co-design and co-synthesis. This chapter also provides some details on distributed, regular architectures like hypercube, mesh and tree arrangements. Chapter 3 describes all phases o f the proposed co-synthesis algorithm. These include algorithms for iterative selection o f processing elements, pipelining and task allocation, mapping processing elements network to a topology and selecting the best topology for the given application.

Chapter 4 presents the experimental results. Algorithm is dem onstrated by application on large random task graphs and an M PEG encoder application for a range o f constraints. O utput from each phase is discussed here. Chapter 5 concludes the thesis by stating some directions for future w ork that can improve the proposed algorithm.

CHAPTER 2

'

HARDWARE SOFTWARE CODESIGN

2.1

Overview

Performance requirements o f most real-time embedded applications make it impossible to execute the entire application in software. To meet the performance constraints, computationally intensive portions of the application are extracted realized in the form of specialized hardware. These systems were developed as a two stream process, hardware engineers deliver the hardware components and software engineers program general purpose or application specific processors for software components. Design methodology for these systems starts with requirement analysis. Based on requirements, application is partitioned into hardware and software components and system architecture is defined. After that, hardware and software components are designed and an interface between hardware and software components is described. These components are then verified for functionality and then integrated together. Finally, the integrated environment is tested and this concludes the design. A typical design methodology for these systems is shown in Figure 2.1 [1].

Optimal performance is achieved when hardware and software are properly `tuned' to each other. Early partitioning along with the separate design flows for hardware and software does not fully explore design space which may result in an imperfect solution; a design that is prone to high cost and inefficient hardware/software. Also fi-om Figure 2.1,

R equirem ents Definition 6-12 Months Architecture Definition

Hardw are Design

Softw are D esign

Interface Design 25-49 Months H ardware M anufacture and T est Softw are C ode and T est

Hardware Software Impiementation and T est D eliverables Docum entation 6-12 Months D eploym ent Field T est

Figure 2.1 Typical System Design Practice

it can be seen that the m ost time consuming stage is the developm ent o f hardware and software components. A ny problems (e.g. performance/area, cost issues etc.) identified in the later h a lf o f the design which may require a different hardware software partitioning w ould require a new hardware and software design flow costing 25-49 months worth o f effort. W ith decreasing time-to-market windows, this m ay prove to be extremely costly.

To effectively address these design challenges, a unified approach that considers the efficiency o f both hardware and software options is required. This approach is commonly

known as to as hardware software co-design. Hardware software co-design refers to a mechanism o f jointly designing hardware and software components o f a system. It is a broad term encompassing methodologies and tools that allow a designer to create hardware-software systems from a single starting point, a set o f specifications. Co-design keeps hardware and software within the same `stream' o f work, with the hope o f achieving better results through an integrated approach. This concept attempts to join hardware and software design efforts into a combined methodology that improves cycle time and quality while enhancing the exploration of the Hardware/Software design space. The impetus for this effort lies in the following reasons [2];
a)

Computing systems deliver increasingly higher performance to end users

b) New architectures based on programmable hardware circuits can accelerate the execution o f specific computations or emulate new hardware
c)

Recent progress in synthesis and simulation tools has paved the way for integrating CAD environments for co-design o f hardware software systems

In co-design process, at the very begirming o f system development, constraints and requirements are analyzed to specify the system. The specified system is subject to an automated partitioning algorithm which partitions the system specification into hardware and software blocks. The individual hardware and software blocks are integrated and incorporated along with their interface. The integrated system is then co-simulated and evaluated for timing and resource constraints. The whole process is repeated until a satisfactory hardware software implementation is reached. Once the current

hardware/softw are partition satisfies all the constraints, it is integrated into a complete system and checked for functional verification.

2.2

C odesign M ethodology

Co-design m ethodology provides techmques for analyzing the perform ance, area and pow er o f the system and it provides methods for evaluating a large num ber o f feasible design options [3], In this section, these phases are discussed and some o f work in each o f these phases is reviewed.

2.2.1

Specification/Requirements Analysis

Codesign process starts with description o f both hardware and software components. D escribing the system level behavior is a challenging problem as it requires a high level o f abstraction while requiring fine details to make it unambiguous. The output o f this phase is a functional specification, which lacks any implementation detail. Different schemes have been used to specify the requirements. These include using representation o f Com m unicating Sequential Processes (CSP) [4], VHDL [5], Codesign Finite State M achine (CFSM ) [6] and cV Hardware C [7, 8 and 9].

Different m ethods have been explored to specify the system requirements. These schemes range from hom ogeneous modeling to heterogeneous modeling. In homogenous

m odeling all the requirem ents are specified by a common language. The examples given

above relate to homogeneous modeling. Other alternative (heterogeneous modeling) is to use hardware description languages to describe obvious hardware functions and use a software language to deseribe the other funetions. So, when functions move across the partition only a small portion of the specification need to be translated [3]. However, software based languages, like C/C++, bias the implementation in favor o f software while on the other hand the hardware deseription languages, like Verilog, VHDL, etc., favor the hardware implementation. Also, software based representations, e.g., C/C++, lack the mechanism to specify concurrent processes.

A recent step in this direction is the introduction o f a new language to model system SystemC [10]. System C combines the features o f hardware description languages and software languages to model both software and hardware components o f a system. Many companies like Synopsys, Cadence etc., are building CAD tools to simulate and synthesize a system specified in System C.

2.2.2

Partitioning

a) Graph Structure
Once a satisfactory representation of system specification is obtained, it is subject to a partitioning algorithm. The partitioning algorithm assigns parts o f system description to heterogeneous implementation units e.g. ASICs (Hardware), standard or embedded microprocessors (Software), memories and so forth. The aim o f partitioning task is to find a design implementation that fulfills all the specification requirements (functionality.

goals and constraints) at a m inim um cost. The cost could be area cost, power cost or dollar cost o f the resulting system.

Specifications for the system are read into an internal data structure. This internal representation is m ostly some form o f a graph. Structure o f the graph is also very im portant for the operation o f partitioning algorithm. M ost commonly used graph structures are dataflow graphs and control flow graphs.

Dataflow graphs jo in the nodes by their data dependencies. M ost digital signal processing (DSP) applications are data-flow dominated which nicely fit this graph structure. Partitioning on these graphs is performed by scheduling the nodes o f the graphs to available hardw are and software (general purpose processor) resources. M ost commonly used scheduling algorithms are based on H u 's scheduling algorithm [11]. These algorithms are list scheduling [12] and force directed scheduling [13]. This form o f graph can efficiently extract the parallelism during partitioning. However, the main

disadvantage is that it cannot handle conditional branches, e.g., if-el se constructs.

Control flow graph on the other hand join the nodes by their control dependencies. These graphs suit control applications which have a large number o f `if-else' constructs. Most com m only used scheme is path based scheduling algorithm [14]. This form o f graph has the obvious advantage o f handling the control dependencies. However, its complexity is dependent on the num ber o f paths in the graph and it also cannot extract parallelism efficiently.

An interesting approach has been presented by Bergamaschi et al. in [15]. They combine control-flow and data-flow approaches in an adaptive scheduling algorithm. The algorithm operates on a control-flow graph and integrates the data-flow techmques into a path-based scheduling algorithm.

b) Granularity
The partitioning algorithms work by mapping the system components to heterogeneous resources (hardware or software). The size of components moved to hardware/software defines the granularity o f the partitioning algorithm. Coarse grain approaches assign

complete ftinctions or processes to hardware or software resources. Nodes o f the input graph in this case represent a large block of system functionality. This high level representation prevents excessive data communication across the whole application. Common examples for coarse level granularity are MPEG decode, Fast Fourier Transform (FFT), Discrete Cosine Transform (DCT) etc. Partitioning performed with at coarse granularity is also known as high level granularity.

Fine grain approaches, on the other hand deal with small operations. These operations may be either a single instruction or a bunch o f instructions. Fine grain approaches usually result in a large amount of data transfers across the application. A commonly used method is to group all instmctions which do not have any conditional expression among them. This group is known as basic block. Fine grain approaches are also known as low level approaches.

10

G ranularity level directly affects the partitioning process. Optimization potential in partitioning improves from coarse grain to fine grain approaches. Data communication overhead and com plexity also increases as granularity varies from high level to low level [16].

Both coarse grain and fine grain approaches have their disadvantages. For most applications, the com putationally intensive parts are small loops which are hidden inside a function or process. For these cases coarse grain approaches provide costly designs by m oving m any redundant parts to hardware. On the other hand, fine grain approaches suffer from the fact that their space for feasible designs is so large that it is usually hard to find the global optimum. Henkel and Ernst have proposed an interesting approach which uses flexible granularity. Basic blocks are clustered together in partitioning objects w hich range from a single block to an entire function [17].

c) Optimization Techniques Optimal Approaches
A t the heart o f partitioning process, there operates an optimization algorithm. Goal o f the optim ization algorithm is to select an optimum hardware software partition. Problem can be setup in different ways, e.g., to maximize the performance, to minimize the cost (area, pow er etc.) or to m axim ize the performance with minimum area. Simple hardware software partitioning is known to be an NP (Non-polynomial) complete problem [18]. Different optim ization algorithms have been used to partition the system in hardware and software. A sim ple approach is to try out all possible solutions and then select the best

11

option. This approach, though gives the best result, is not feasible because design space is very large even for simple problems.

Integer linear programming (ILP) is another technique that can be used to solve partitioning problem [19]. Linear program is a formulation in which a set o f linear equations define the objective function and associated constraints. If the desired solution can only have integer values, the formulation is known as ILP. Simplex technique is a well known method to solve such linear equations. Partitioning problem can be expressed as a set of linear objective function subject to linear constraints. Prakash and Parker have provided such a formulation for hardware software partitioning [20]. Although this approach provides optimal result but the computation time makes it infeasible for large problems.

Branch-and-bound algorithm is another procedure to solve optimization problems. It is a very general algorithm that can even be used to solve ILP formulations. Feasible solutions are arranged in form o f a decision tree where leaves o f the tree represent all possible options, Algorithm makes a decision at node (branch) based on some criteria and computes a lower bound for all solutions in the corresponding sub-tree. If the bound has higher cost than any o f the previously found solutions, the sub-tree is excluded from further search [12]. Hafer and Hutchings have used branch and bound approach to solve ILP problem [21]. Vemuri and Chatha have used a branch and bound algorithm for hardware software partitioning [22]. Branch and bound algorithms can considerably

12

reduce the search space however the worst case com plexity o f this algorithm still remains exponential.

A nother

interesting

optimization

approach

is Dynamic

programming.

Dynamic

program m ing decom poses optimization problem into a sequence o f stages such that the optim al solution to the original problem must contain optimal solutions to each o f these stages. A lgorithm operates at each stage, makes a decision and moves to the next stage. D ecisions m ade at a stage do not depend on the previous decisions [13]. Shrivastava et al. used dynamic program m ing to solve partitioning problem [23]. Chang and Pedram applied dynamic program m ing to solve coarse grained partitioning problem [24], while Knuds en and M adsen used dynamic programming for performing hardware software partitioning at fine granularity [25]. Use o f dynamic programming can be efficient how ever efficiency m ainly depends on how the problem has been divided into stages and com plexity to reach an optimal solution at a given sub-problem.

Heuristic Approaches
All the m ethods m entioned above provided an optimal solution. However, because o f the exact nature, all the algorithms are computationally very expensive. These algorithms can only be applied to small problems (e.g, a task graph with a small number o f nodes). For larger problem s, heuristics are often employed as they provide a `good quality' solution in a reasonable am ount o f time. Heuristic techniques only perform a limited search on the feasible design space and they cannot guarantee optimality o f the solution.

13

Greedy algorithms are commonly used as heuristics. In greedy algorithm, the decision is made at each step without taking into account any previous or later decisions. Greedy algorithms are usually `up-hill' or `down-hill' techniques and therefore, are liable to stuck in local minima or local maxima. Optimality can be guaranteed if problem exhibits certain conditions [12], but most real life problems do not follow these eonditions and thus greedy approaches are only used as a heuristic. Ernst et at., used a greedy approach by starting from all software solution and moved parts to hardware [8]. Gupta and Micheli used a complementary greedy approach by starting with an all hardware solution and moving parts to software at each step [9].

Simulated annealing is another popular heuristic optimization technique [26]. Simulated annealing based on annealing which is a process to obtain low energy states of a solid material in a heat bath. The temperature of the heat bath is increased to a maximum, which melts the solid. The temperature is then slowly decreased according to a given cooling schedule until a low energy state of the solid (a perfect crystal) is reached. The algorithm is controlled by a temperature parameter, which begins at a high value and decreases as the system "cools" and stabilizes. A cost function is required to evaluate the system for each state. Initially, during high temperature, states which increase system quality are always accepted, and states which decrease design quality are accepted randomly. As the temperature approaches zero, only the states which decrease system cost are accepted. The conditional acceptance is based on the probability P, which is given as:
p (a F^=
/r

'

where AE represents the change in cost o f the overall system.

Unlike greedy heuristics, simulated annealing often accepts changes which decrease the

14

quality o f a design, in hope o f achieving a better final design. Henkel and Ernst have used sim ulated annealing in hardware software partitioning [17].

Tahu search is another heuristic optimization technique to allow local search methods to overcom e a local optim um [27]. Tabu search uses a form o f short-term memory used to keep a search from becom ing trapped in local minima. A Tabu list is formed that contains the m oves o f the recent past but they are forbidden for a certain num ber o f iterations. D uring the optim ization process, solutions are checked against the Tabu list. A solution that is on the list will not be chosen for the next iteration. Tabu list forms the core o f the search and keeps the process from cycling in one neighborhood o f the solution space. Som etim es it m ay be useful to overrule the Tabu condition by what is known as `aspiration criteria'. A commonly used aspiration criterion is to accept a system state if it results in an overall low cost. A technique to expand the search o f design space is called the `diversification strategy'. A simple diversification strategy is to restart the search from the initial system state after a specified number o f iterations. Eles et al. applied Tabu search on hardware software partitioning and compares the results with a simulated annealing based approach [28].

Genetic A lgorithm is also a heuristic technique that can be used to solve optimization problems. A lgorithm starts from an initial population and selects a set o f `parents' based on a `fitness function'. These parents are then used to breed a next generation by perform ing `crossovers' and `m utation'. Main idea is that as the algorithm progresses, stronger and fitter chrom osom es will survive and their next generation will also have a

15

high probability o f being fitter. Dick and Jha used genetic algorithm in performing hardware software partitioning [29]. Wiangtong et al., provide an interesting comparison between simulated annealing, tabu search and genetic algorithms, when applied to partitioning [30].

2.2.3

Estimation

During the partitioning process, it is often required to estimate the performance o f a task on a specific processing element (software or hardware) and the cost associated with the processing element (area, power etc.). This is needed to determine the quality o f a particular hardware software partition.

Estimating the hardware performance required to determine the maximum clock frequency o f the hardware block. At the same time this has to be done in quick time so that the partitioning algorithm can analyze a large number of possibilities. Solution is to use high level synthesis techniques to determine the clock frequency for the longest path through the logic. Henkel and Ernst have developed a path scheduling based estimation technique that can estimate hardware performance very efficiently [31, 32]. Vahid and Gajski have also proposed an estimation technique [33]. Their technique used incremental hardware cost estimation by updating a previous estimate rather than reestimating for a new partition.

16

Software perform ance estimation problem is also sim ilar to hardware `worst-case execution tim e problem. Earlier approaches used instruction set simulators to estimate the software tim e [7], which is similar to the use o f synthesis techniques to estimate hardware perform ance. Park and Shaw proposed one o f the earliest software estimation algorithm s using path enumeration [34]. Li et ah, developed another efficient software estim ation algorithm which also considered the effect o f instruction caches [35]. Ye et ah also developed an algorithm for software performance estimation [36]. Finally, on system perform ance estimation. Yen and W olf developed an algorithm for estimating the perform ance o f a set o f tasks on a multiprocessor system [50]. Each processor in the system used a rate m onotonie scheduler to schedule the tasks.

2.2.4

Co-simulation / Co-verification

A fter successftil partitioning, system needs to be verified against specifications for com pleteness and functional correctness. A common verification strategy is to simulate the final system. Co-sim ulation refers to an integrated simulation o f hardware and software components. Challenges in hardware-software co-simulation include `speed', `accuracy' and `interactive debugging'. Speed is required to simulate a reasonable num ber o f test sequences. Accuracy refers to generating the same simulation outputs in as the im plem entation would result. Interactive debugging is the ability to step through system execution, exam ining intermediate values, and backtracking to debug the system.

17

Separate verification strategies for hardware and software exist but these are quite different. For hardware software co-design where the final system contains

heterogeneous components (hardware and software) simulation is a problem. One approach is to simulate hardware and software separately. This teehnique can be efficient but it leads to problems with synchronizing the results. Another co-simulation approach could be to use register transfer level (RTL) or gate level hardware models o f the processor to simulate software execution in a hardware verification environment. However, these simulations are too slow to simulate software in a reasonable amount o f time. A traditional approach to co-simulate hardware and software is to use Verilog simulator. Verilog's Programming Language Interface (PLI) was used to co-simulate hardware and software components. Hardware component is described using Verilog and software interacts with the hardware using U nix's sockets. This is illustrated in the following figure.

18

îrilog Hardware Sir
A p p lic a tio n S p e c if ic H a r d w a r e M o d u le

1 1

Hardware Process 1
1 I

P ro c e ^ i

r

1

B u s In te rfa c e M o d u le

Verilog PLI

Figure 2.2 Verilog based Co-simulation

The Ptolem y environment [37], developed at Berkeley, is also a key research tool in this area. It focuses on system-level modeling and simulation and provides a high-level support for a variety o f applications. Cortes et al., have proposed another co-verification m ethodology for embedded systems using a Petri-net based representation [38]. They used symbolic m odel checking to prove the correctness o f the system. Ghosh et al.,

provided a set o f techniques to speed up simulation o f processors and peripherals without significant tim ing accuracy loss [39]. They developed a eo-simulator that can be used for jo int debugging o f hardware and software. Finally, Hsiung proposes a formal verification approach using linear hybrid automata [40]. His approach simplifies the state-space explosion that occurs in formal verification o f complex systems.

19

2.3

H ardware Software Co-synthesis

In hardware design, synthesis refers to construction of a structure o f digital circuit starting from a specification in the form of some data-flow graph [12]. This structure basically represents collection of some resources (e.g., adder, multiplier, ALU etc.), interconnection between these resources and corresponding control logic. On the other hand, in software domain, synthesis refers to the conversion o f system specifications into a group o f basic instructions (software program) that can be executed on some general purpose processor. This software synthesis is mostly done by a compiler which converts programming language based specification into hardware specific instructions.

In the domain o f hardware software co-design, where the resulting system consists of both hardware and software components, co-synthesis process is the conversion of system specification (along with a set of technology parameters) into hardware architecture and corresponding software architecture. System specification identifies both functional and non functional requirements and is specified in the form o f a task graph. Hardware architecture consists of processing elements (PE's) and communication channels connecting these P E 's, which can either be a general purpose processor or an application specific hardware (ASIC). Software architecture defines the allocation of tasks on PE's, scheduling o f tasks and scheduling of communication channels [41].

Hardware software co-synthesis is tightly coupled with hardware software partitioning, which was described earlier. Partitioning process is a subset of co-synthesis. Co-synthesis process selects the processing elements, maps tasks onto them, arrange them in a specific

20

architecture and identify the schedule for each task. Partitioning, on the other hand is m erely the process o f m apping tasks to hardware or software components. However, this difference is subtle and partitioning and co-synthesis terms are also used interchangeably in literature.

2.3.1

Co-synthesis Phases

Co-synthesis process involves selecting processing engines, allocating tasks to processing engines and scheduling tasks on processing engines. For increased throughput requirem ents, co-synthesized system may also need to be pipelined. However, it m ust be noted that these phases are not always cleanly separated and some o f these phases may be m erged together in a co-synthesis environment. In the following sections, each o f these phases is described.

a) Resource selection
Co-synthesis environments are mostly library based, where a num ber o f different processing elements are available to be added in to the system. Processing elements are the im plem entation units which can be either an application specific hardware or a program m able processor. Performance o f each task on the available processing element is also known. Resource selection refers to the selection o f processing elements that when added to the system improves the performance without violating any non functional param eters (area, pow er etc.). Techniques for estimation earlier can be used to estimate

21

the performance and cost for adding a specific processing element. Resource selection involves the number as well as type o f processing elements added into the system.

b) Task Allocation
Next phase in co-synthesis process is to allocate the tasks to the processing elements that have been added into the system. This phase uses the techniques described earlier in the partitioning phase. Tasks are allocated to the processing elements such that overall performance o f the system is maximized. Another important parameter to be considered is the inter-task communication (also known as inter-process communication; IPC). Data communication between two tasks can be of the same magnitude as their execution times. Allocating two tasks to two different processing elements based on individual performance gain can actually reduce the system execution time if the tasks have high data communication among them. Task allocation is known to be NP-complete and is therefore a computationally hard problem [18]. Heuristics are often employed to perform task allocation.

c) Scheduling
Scheduling is the process o f assigning start times to each task after they have been allocated to some processing engine. Processing elements may have more than one task allocated onto them and the order in which various tasks execute on a processing element has a direct influence on the overall system performance. Tasks cannot be executed unless the required data is available. This data is available only after the execution o f

22

parent tasks. I f tasks allocated to a processing element have a data dependency, there is a fixed order o f execution. However, if there is no such data dependency, the tasks can execute in any order.

M any scheduling algorithms exist to schedule tasks in a multiprocessor environment. Scheduling is also a NP complete problem so heuristics are usually used to perform scheduling. A com m on heuristic is a list scheduling [12] algorithm which is a variation o f H u 's optim al algorithm [11]. In list scheduling, a priority is assigned to each task. Tasks w hich are on the critical path (from performance point o f view) have higher priority than the other tasks. A common priority measure is the sum o f execution time (maximum, m inim um or m edian) o f each task along its longest path to final task. Tasks are scheduled in the order o f decreasing priority such the dependent tasks are scheduled after their parents.

M ultiprocessor task scheduling is a well researched topic. M any algorithms for scheduling have been proposed for multiprocessor environments [42]. Scheduling algorithm s for distributed architectures have also been proposed. Sih and Lee present an interesting algorithm, known as generalized dynamic level (GDL) scheduling [43]. In this algorithm, the priority assigned to each task is dynamic and changes as the algorithm proceeds. Priority depends on a number o f factors including the static priority (sum of execution tim e along its longest path to final task), inter-task data communication and descendant consideration.

23

d) Pipelining
Sequential implementations can constrain the throughput o f the system. Concurrent execution o f different tasks can drastically increase the performance o f the system. Pipelining is a useful technique to execute different tasks in parallel when the tasks execute in a loop fashion for a long time. Tasks executing concurrently operate on delayed data; data produced by the parent task in the previous iteration. Pipelining is also known as retiming transformation.

To illustrate the effect o f pipelining, consider an example task graph shown in Figure 2.3. Task graph consists o f four tasks and three different processing elements are available for task allocation. Execution time of each task is also shown. Ignoring the data communication time between tasks, a possible task allocation with corresponding sequential system execution is shown in Figure 2.4. System executes the tasks sequentially in 35 time units. Next, a pipelined execution with same task allocation is shown in Figure 2.5. Dotted lines indicate pipeline stages. Tasks A and C execute in first pipeline state, task B in second and task D in third pipeline stage. With pipelined execution, tasks now execute in 15 time units.

In software (VLIW compilers) and hardware synthesis pipelining is a well researched problem however, use o f pipelining in hardware software co-synthesis is relatively recent [22], [44]. Like multiprocessor scheduling, pipelining under resource constraints is also NP complete, which leads to the use of heuristic techniques [18, 22].

24

PEO; 10 PEI: 20

PEO: 25 P E 1 :15

PEO: 5 PEI: 5

PEO: 20 PEI: 20 PE2: 15

Figure 2.3 Example Task Graph with Possible Mapping

jS
PE1 PEO

0

S

10

15

20

25

30

35

Tim e

Figure 2.4 Task Allocation and Sequential Execution

PEI

PEO

0

5

10

IS

20

25

30

38

T im e

Figure 2.5 Task Allocation and Pipelined Execution

25

2.3.2

Target Architecture

Processing elements together with interconnection describe the architecture o f the system. This architecture can range from simple shared memory based design to complex distributed memory schemes. Most simple case is a uni-processor approach, where most o f the application runs on a single processor and computationally intensive parts are executed on custom hardware which communicates with the processor through a shared memory. In this case, co-synthesis reduces to a simple hardware software partitioning problem. For more complicated approaches, a distributed memory scheme is used as shared memory can become a bottleneck when a large number o f processing elements communicate through single memory. Distributed memory schemes connect each processing element with a limited number o f other processing elements in a regular or irregular fashion. With these schemes, data communication between different tasks becomes important as data goes through a number o f stages in order to reach the destination, adding extra overhead. Finally, the target architecture o f the system m ay not be pre-defined and is shaped during the co-synthesis process, which complicates the task allocation and scheduling processes even further.

2.3.3

Co-synthesis Approaches

Co-synthesis can be performed using optimal or heuristic based algorithms. Optimal algorithms include exhaustive search, integer linear programming approach or branch and bound algorithms. Design space for co-synthesis is much larger than simple hardware software partitioning, therefore optimal approaches are very restricted in application.

26

Heuristic approaches search only a limited design space can therefore not guarantee optim al results. However, m ost heuristic approaches provide good results in a reasonably small time. Heuristic approaches are divided into two main categories; Iterative approaches and constructive approaches. Iterative approaches start with an initial solution w hich is a high-cost (high area, high power) system with usually the maximum possible processing elements. This solution is subsequently improved at every iteration. As the algorithm proceeds solution is refined until is satisfy the cost constraint. Constructive algorithm s on the other hand are characterized by building solution step by step. A w orking system is not available unless the algorithm has finished its execution.

2.3.4

S ig n ifîcan t C o-synthesis E nvironm ents

In this section, an overview o f some o f the popular co-synthesis environments is presented. Pioneering work in co-synthesis was performed by Prakash and Parker [20]. They developed an algorithm for synthesis o f application specific heterogeneous m ultiprocessor systems known as SOS. The algorithm described a formal mechanism to synthesize a heterogeneous multiprocessor system by creating a mathematical model for the constraints and objective. The problem is set up as a Mixed Integer Linear Program (MILP). Equations for constraints and objective function are developed and then linearized. Linear equations were then solved through simplex technique using Bozo program [21]. This algorithm took the input in the form o f a data flow graph and synthesized an arbitrary multiprocessor topology. Algorithm produces optimal results, however, because o f M ILP, it is slow and is limited to only small applications.

27

VULCAN is an earlier co-synthesis environment developed by Gupta and DeMicheli [9]. The input for the co-synthesis process is specified in a C like language known as HardwareC. HardwareC has some modifications with simple C syntax to model hardware unambiguously. The specifications are translated into a system graph model which is control flow graph at fine granularity. A simple architecture is used which consists o f a single software processor and multiple hardware blocks. Hardware software

communication takes place through shared memory and to simplify computational model, hardware and software portions execute in mutual exclusion. Partitioning is performed using an iterative algorithm. Initially all the tasks are put to hardware. This mechanism is used to check if a solution exists for the given constraints. Then, tasks are subsequently moved to software to reduce hardware cost.

Another early co-synthesis environment is COSYMA, developed by Henkel and Ernst [7]. Input was specified using a superset of C language called C*. Specifications are translated to a control flow graph at a fine granularity level of base block. As VULCAN, simple target architecture is assumed which consists of a single processor and a hardware block. All the hardware modules are implemented in a single hardware block. Data communication takes place through shared memory. Earlier version o f the algorithm operated at fine granularity level and used simulated annealing to partition the graph into hardware and software components. Later version introduced the idea o f flexible granularity [17]. Partitioning is performed through a dynamically weighted,

multidimensional objective function which takes into account area cost as well as timing

28

constraints. Software tim ing is estimated through profiling and hardware estimates are obtained through a path based estimation algorithm. This environment also assumed hardw are and software block operating in mutual exclusion.

K alavade and Lee developed a constructive algorithm for Ptolemy environment [37, 45]. Task execution tim e and system cost can be used as two objective functions. Their algorithm introduced the idea o f Global Criticality Local Phase (GCLP). Global criticality m easure identifies whether area is critical or time is critical. Local phase is used to classify tasks into extreme task, repeller task or a normal task. Algorithm works on coarse granularity. It initially concentrates on solving two-way partitioning problem and is then extended to solve multi-way partitioning problem where different an im plem entation bin also needs to be selected. Different implementation bins correspond to having m ore that two (hardware or software) possible mappings for a task. The environm ent concentrates on real time applications implemented by means o f DSP-based architectures.

W o lf presented an architectural co-synthesis algorithm for distributed systems [41]. In this algorithm, a heterogeneous multiprocessor system is constructed iteratively. Algorithm w orks on a data-flow graph specified at a coarse granularity level. Initially all the tasks are out to the fastest processing element. Tasks are then re allocated based on processing elem ent utilization to minimize cost. After that, tasks are again re-allocated to m inim ize com m unication between various processing elements. Finally based on data com m unication betw een various processing elements, an irregular topology is created.

29

Tasks are scheduled by finding the longest path through the task-graph. Since task allocation is known, they are easily scheduled by forcing an order o f execution. Because o f heuristic nature algorithm is very fast, however it does not pipeline the execution.

Vemuri and Chatha presented a co-synthesis algorithm which supports pipelining [22]. Algorithm works on a data-flow graph, specified at a coarse granularity. Algorithm targets simple shared bus architecture with a single processor and multiple hardware blocks. Execution times o f hardware and software implementation are assumed to be known in advance. Tasks are moved to hardware or software implementation using a branch and bound algorithm. Initially a hardware software partition is created which is followed by pipelined scheduling technique called RECOD (REtiming heuristic for optimal resource utilization with least shared memory utilization for hw/sw CODesigns). RECOD algorithm used heuristic techniques to create pipeline stage and utilized simulated annealing to minimize additional pipeline memory. If pipeline scheduling fails, branch and bound partitioner is invoked to get a new partition and process is repeated. Algorithm produced optimal results but is limited to task-graphs with 30 tasks.

Bakshi and Gajski also presented a co-synthesis algorithm with pipelining [44]. This algorithm also operates on a data-flow graph at coarse granularity. This algorithm tries to minimize the cost o f hardware and it can perform pipelining even at task, loop and operation level. Tasks are executed in hardware only if a software implementation cannot meet timing constraints. Number o f software processors is not limited and processors are selected by an exhaustive search. Pipeline stages are inserted when a task cannot be

30

executed in the current pipeline stage. This algorithm targets simple shared bus architecture and it does not take into account the data communication times between various tasks. Also, it does not consider the hardware cost o f software processors.

A constructive co-synthesis algorithm for distributed hypercube architectures has been presented by Levman [46]. This algorithm operates on coarse grained data-flow graphs. Tasks are initially clustered into groups and assertion tasks are added for fault detection. Algorithm then iteratively adds processing elements into system taking into account perform ance im provem ent versus area cost. Task graph is pipelined a maximum number o f pipelining stages by repeatedly using RECOD algorithm [22]. Tasks are allocated on processing elem ents to balance utilization. Processing elements are then arranged in a hypercube topology and tasks are then scheduled on each processing element. Scheduling is perform ed by assigning high priority to tasks that are on critical path. Tasks are re allocated iteratively during scheduling in order to reduce excessive data communication betw een various processing elements.

2.4

F a u lt Tolerance

Fault tolerance is a process through which a system continues to work in the presence o f faults. M ajor steps in tolerating a fault include fault detection and fault recovery. Fault detection is the process o f identifying a fault. Faults can be identified by using assertions or by duplicating a task (usually by implementing it in an alternative way) and comparing its output w ith the original output. Assertion is the method to verify the correctness o f the

31

output without regenerating it. Common examples o f assertions are CRC check, parity check etc. Assertion requires less overhead compared to duplication where all the functionality o f the task is implemented, however assertions cannot always be used. After fault has been detected, fault recovery is then performed by re-executing the faulty task again on some spare hardware.

Co-synthesis o f fault tolerant systems is performed by inserting extra tasks in original task graph. These tasks are used as assertion tasks or duplicate-and-compare tasks to detect fault in the system. Following figure illustrates these assertion and duplicate-andcompare tasks. Assertion tasks have been added for tasks A and B (Aas and Bas) whereas duplicate-and-compare tasks have been added for tasks C and D (Cdu, Com and Ddu, Dcm)Dotted lines and shaded blocks indicate fault detection overhead.

as

as cm

Figure 2.6 Original Task-graph and Task-graph with Fault Detection Tasks

32

Y ajnik et al., proposed a fault tolerant co-synthesis algorithms called Task Based Fault Tolerance (TBFT) [47]. In this algorithm a fault detection task is added for each task. D uplicate-and-com pare tasks are only added if an assertion task does not exist. Figure 2.6 is an exam ple o f TBFT. Dave and Jha extended TBFT and developed a Cluster Based Fault Tolerance (CBFT) algorithm [48]. CBFT uses concept o f `error transparency'. Error transparent tasks are clustered together and a single fault detection task is added for the cluster w hich reduces the overhead for fault detection. Group Based Fault Tolerance (GBFT) algorithm is similar to CBFT but uses a bottom-up approach and different heuristics to merge tasks in a group [46]. Its overhead is shown to be even lower than CBFT.

2.5

D istribu ted M em ory Architectures

Shared m em ory architectures, where various processing elements are connected through a shared bus and communicate to each other through a shared memory, are simple and efficient i f tlie num ber o f processing elements is small. As number o f processing elem ents increases, shared bus tends to become the bottleneck o f the system by slowing dow n the com m unication between different processors. To overcome this problem, distributed m em ory architectures are employed in embedded systems. Distributed m em ory architecture is a scheme that connects a processing element along with its local m em ory to a processor-to-processor interconnection network [49]. Each processing elem ent in this architecture has its own local memory which is not shared with any other processor. Processing elements in these architectures communicate with each other by

33

sending a copy o f the data to other processors through the interconnection network. These architectures offer high scalability and can easily satisfy the performance requirements of modem day applications where computation on local data takes most o f the time compared to data transfers across different processing elements.

Two terms associated with distributed architectures are degree and diameter. Degree o f a node is the maximum number of communication links connected with a processing element. Diameter is the worst case measure of the number o f communication links traversed (also known as number of hops) in transferring data between two processing elements. Regular and irregular architectures are two major classes o f distributed memory architectures based on the regularity of the communication links. An architecture is regular if all nodes have same number of communication links. Most regular architectures offer inherent fault tolerance capabilities to communication link failure as multiple paths to each processing element exist.

Different distributed memory architectures have been proposed to support scalability and efficient parallel processing. These architectures differ by inter-processor communication patterns. Some common topologies of distributed memory architectures are described below.

34

2.5.1

Mesh Topology

Mesh topology has n =

nodes and all nodes except the boundary nodes are connected

to four immediate neighbors. Degree of this topology is 4 and its diameter is 2*(N-1), Following figure illustrates a mesh topology with N=3.

Figure 2.7 Processing Elements Arranged in Mesh Topology with N=3

2.5.2

Hyperenbe Topology

Hypercube topology has n = 2^ nodes and all nodes are connected to N other nodes. Degree o f this topology is N and its diameter is N (log2n). Following figure illustrates a hypercube topology with N=4.

35

Figure 2.8 Processing Elements Arranged in Hypercube Topology with N=4

2.5.3

Tree Topology

In tree topology, processing elements are hierarchically arranged with each node at a given level is connected to 2 or more nodes and the lower lever. If `k ' denotes number o f levels and `D ' is the degree of each node, then a balanced tree topology has n
D* - 1
D -\

nodes. Diameter of tree topology is 2*(k-l). Trees with D=2 and D=4 are known as binary and quad tree respectively. Unlike mesh and hypercube topologies, in tree topology there is only one path to each node and therefore cannot support fault tolerance. To overcome this limitation, nodes at each level are connected by extra links [49]. Trees with these extra links are also known as X-Trees [50]. Following figure illustrates a binary and quad-tree topology with k=3. Dotted lines indicate extra links.

36

Figure 2.9 Binary Tree with 3 Levels

Figure 2.10 Quad Tree with 3 Levels

37

CHAPTER 3 HARDWARE SOFTWARE COSYNTHESIS FOR DISTRIBUTED MEMORY SYSTEMS

3.1

Introdu ction

In this chapter, a new hardware software co-synthesis algorithm is presented which targets regular distributed memory architectures [53]. Application is specified in the form of an acyclic task graph using data flow representation. A library o f processing elements is also required. These processing elements can either be general purpose processors (software processors) or application specific hardware blocks. Algorithm also assumes that the profiling data is available which contains execution times o f each task on the available processing elements. Communication links, which connect different processing elements, are all of the same width. This is required for fault tolerant capability o f regular architectures so that data can be transferred through a different path in the presence o f a fault. Local memory interfaces to the processing elements can have different data bus widths depending on memory bandwidth requirements of the application. Data transfers between different processing elements takes place through message passing schemes. Every processing element is assumed to have a dedicated communication interface which can communicate data while a task is executed.

Figure 3.1 provides an overview of the proposed co-synthesis algorithm. Major phases of this algorithm include selecting processing elements, allocating tasks and creating

38

pipeline stages, selecting a topology for the processing elem ents, task scheduling and reducing the pipeline period. A n overview o f each o f these phases is provided here.

Initial phase o f the algorithm is selection o f processing elements iteratively followed by pipelined task allocation. Selected processing element can either be a general purpose processor or an application specific hardware block. Performance gain and additional hardw are area costs are estimated for each processing element. Processing elements w hich gives m axim um performance gain with minimum area is added to the system. A fter addition o f each processing elements, tasks are allocated to the processing elements present in the system. Pipeline stages are also created during task allocation. This phase term inates w hen all the tasks are scheduled. Otherwise it continues to add more processing elem ents until the area constraint is violated. A dirty-list o f processing elem ents is m aintained to remove an inefficient processing element after the maximum num ber o f processing elements gets added into the system.

A fter a successful task allocation, processing elements are mapped onto a regular distributed m em ory architecture. Topology o f the processing elements is not

predeterm ined and the algorithm selects the best topology from some well known topologies like m esh, hypercube and quad-tree topologies. M apping processing elements to a regular topology can add delays in communicating data from one processing element to another. The topology with minimum overhead in inter-PE com munication is selected.

39

Processing Element Selection ipelined Allocation

T asks chedule

C onstraints

Finish

Select Topology

Schedule

C onstraints

Finish (SU C C E SS)

Reduce Pipeline Period « I Processing Element Selection Pipelined Allocation

# 0 #

chedule

C onstraints

^ ,

Figure 3.1 Co-synthesis Algorithm

40

Topology selection follows the scheduling phase. Start time for each task is identified in this stage. This phase takes into account the additional data communieation delays incurred due to m apping processing elements on a regular topology. If after scheduling, all the tasks finish their execution with in required pipeline period, algorithm terminates successfully b y producing regular, distributed memory architecture o f hardware/software processing elem ents w ith schedule o f each task. If tasks cannot be scheduled, algorithm proceeds to reduce pipeline period.

In pipeline period reduction phase, the system pipeline period is decreased. This is attem pted to eope for the additional delays introduced by mapping processing elements to regular topology. O verhead introduced by the regular topology is used to reduce the pipeline period. M ore processing elements are then added in the system to satisfy the new period. A fter successful task allocation, algorithm selects a topology and tries to schedule the tasks again. This process continues till a system configuration which meets required tim ing constraints is established. Following sections describe each o f these phases in detail.

3.2

P ro cessin g E lem en t Selection an d P ipelin ed Task A llocation

In this phase, the processing elements are selected, tasks are allocated and pipeline stages are created. Processing elements are added in an iterative manner. Initially all the tasks are attem pted to be scheduled on a single processing element and if this is not possible,

41

more processing elements are added to the system. Task allocation is performed by scheduling a task on one o f the available processing elements. Task allocation takes into account external data communication time, however, a fully connected topology is assumed in the beginning.

Different algorithms have been proposed for processing element selection and task allocation but all o f them have certain limitations. W olf starts with putting each task on a separate processing element and then tries to remove less utilized processing elements [41]. Hardware processing elements are almost always fully utilized and might never be removed. The algorithm does not pipeline the tasks and therefore cannot satisfy high throughput requirements. Bakshi and Gajski allocate tasks and create pipeline stages but tasks with hardware description are always mapped onto hardware processing elements and therefore they cannot trade off between hardware and software implementations for a given task [44]. Chatha and Vemuri perform task allocation through a branch and bound partitioner and present a heuristic algorithm for pipelining [22]. However, they do not consider pipeline period while creating pipeline stages and therefore may result in redundant pipeline stages. Figure 3.2 shows the processing element selection/ pipelined task allocation phase and following sub-sections describe each o f these in detail.

42

Processing Element Selection ipeiine Allocation

All T ask s

Finish (FAIL)

\

J

Figure 3.2 Processing Element Selection and Pipelined Allocation

3.2.1

Processing Element Selection

In this stage a hardw are (HW) or software (SW) processing element is selected for addition to the system based on performance gain and its area cost. This selection involves com putation o f a selection coefficient for all processing elements available in the library ·
·
( P E
s e l e c t

) -

The selection coefficient consists o f two factors, namely
( P E p e r f j m p r )

Perform ance Im provem ent Factor A rea Cost Im provem ent Factor

( P E a r e a _ f a c t o r )

Several variables need to be defined in order to describe these factors. ·
`Tsw' denotes the set o f tasks, which do not have a dedicated hardware resource

for their execution in the current system.

43

P R O P cm op

.

`7W ' denotes the set o f tasks that have a dedicated hardware resource in the current system.

.

"PEsfv

and "PEhw' denote software and hardware processing elements

respectively. ` SYSpEs^ ' and ` SYSpE,,^ ' denote sets o f software and hardware resources respectively in the current system.

Based on these variables, the cumulative software and hardware execution times are defined:
SYS CUM_sw _TiM E

X ExecTime{J i,P E j) Ti PEj

where
Ti G T s w , are tasks with which can execute on software processing elements P E j G SYSpEsw, are software processing elements

SYScum_HW _T!M E ~ S ^E xecTim e{T i , P E j )

TiPEj

where
Ti ^ T hw ^are tasks which are executing on hardware processing elements P E j G SYSpEfj^ , are hardware processing elements

These variables indicate the execution time o f all the tasks, which execute on all software and hardware processing elements respectively. Next hardware improvement factor is defined, which is the improvement in system performance obtained by adding a hardware processing element to the system:

44

/ \ H W J m p {P E new)

s s E xecT im eiT nP E j) T. PE  = ---- '-- r -- ------j-------------- Y.E^^cTime{Ti,PENEw) I T,

w here
T i denotes the set o f tasks which can execute on P E new P E j GS Y S , is a software processing element which has already been added

to the system

U sing hardw are im provem ent factor and cumulative hardware and software execution tim es, execution tim e o f the system before {S Y S prev_tim e) and after ( S Y S p r e v addition o f a processing element are estimated.
CVC ^Y ^C U M -S W _ T IM E

time)

the

OlbPREV_TIME --

7 I

5 f

^ OIOCUM_HW_T!ME

w here denotes the num ber o f software processing elements currently in the system

' S Y

S c u m

_ s w

_ t i m

e

+

Z

ExecTime{Ti,PENEw)
~2

------------------SY S curr time --

if

e PE^yy

SYS PREY TIM E ~ H W I m p iP E new)

t f PEffgyy e PEfjfff

w here

45

Ifyv

I denotes
system

the number o f software processing elements currently in the

These variables are then used to calculate the performance improvement factor
{PEpERFjMPiù for a given PE. This factor gives a normalized measure o f the performance

gain obtained by adding a particular processing element.
S Y S c u r r
t im e

EE pERF IM PR ~ 1

SYS P R E V

TIM E

Area cost o f adding another processing element is also considered by determining area cost factor {PEareajfactor)- This factor identifies the area cost associated with a particular processing element. Area cost is normalized by dividing it by maximum area cost associated with any processing element available in the library {MAX_AREA).
_ 1

A^^<^{PEnew)

where
A rea ( P
E n e w )

is the area cost of the newly added processing element

Using performance improvement and area cost improvement factors, finally the selection coefficient o f a processing element is defined, which is the weighted sum o f performance and area cost improvement factors.

46

P E s e le c t ~

P E p e r f _iMPR + (\-k )y .p E jiR E A _ F A c ro R

w here ^ is a user defined area-performance trade-off factor and k e [0,l]

P

E

s e l e c t

is calculated for all available processing elements and the processing element

w ith m axim um selection coefficient is selected to be added into the system.

Processing elem ents are selected iteratively and it is possible that initially some slow processing elem ents get added into the system and degrade the system performance. This m ight result in system not meeting the required timing constraints. Algorithm uses a m echanism to rem ove these processing elements from the system.

Slow processing elem ents become the bottleneck when another processing element cannot be added into the system. This happens when;

w here
I T s w I is the total num ber o f tasks that execute on a software processing element

is num ber o f software processing elements in the system

O nce the above condition is satisfied, a slow processing element is removed from the system. The slow est processing element is selected to be removed, which is found by using;

47

PE SLOW =

max

X

ExecTime{Ti,PEj)

P E slo w

is the processing element which takes maximum time to execute all software

tasks. Minimum area criteria can also be used to find the slowest processing element. Once such a processing element is removed from the system, it is added to a `DrrtyPE' list. This list contains all the processing elements which have been removed from the system. Processing elements which are present in the `DirtyPE' list are not considered in subsequent phases o f processing element selection.

3.2.2

Pipelined Task Allocation

After a processing element has been added, all the tasks are then allocated to the processing elements that are currently in the system. Pipelining is also performed concurrently with the task allocation process.

First step of pipelined task allocation is to assign a priority to each task. This priority measure can be any metric but the criteria used in this algorithm assign high priority to those tasks which are on critical path (from execution point o f view). Priority measure used is;
Prionty(Ti) = Min [ExecTime[T()) + Max{^Priority(Tj^

where
ExecTime(Tj) gives the execution tune of Task T, for all the available processing

elements

48

Tj is a successor task o f 7}

Priority is assigned b y starting from the tail o f the graph and setting the priority o f each task as the sum o f its m inim um execution time (on any PE) and m axim um priority o f its successors. This priority measure is quite popular and has been used extensively in the past [12, 22]. Instead o f using minimum execution time, any other statistical property can be used e.g., m ean or m edian execution time, however, minimum metric provides slightly better results.

Pipelined allocation is the process o f assigning start time to each task such that the follow ing relationship is satisfied; 0 < StartTimeÇTj) < ExecTimeÇT^, PE ) + w here
StartTime(Ti) is the tim e at which task
Tj

starts its execution

ExecTime(TuPE) is the execution time o f Task T; on processing elem ent where it

has been allocated
TpERioD is the constraint pipeline period o f the system

Pipeline allocation process initially finds starting and finishing tim e o f each task for each processing elem ent present in the system. Start time for each task is defined in terms o f earliest start tim e { E a r lie s tS ta r tT im e ) and idle time o f a processing element (PEIdleTime). These param eters are defined as:

EarliestStartTim e(Ti) = M AX {^F inishTim e[PR E D {Ti)))

49

where
PRED(Ti) = Set o f all predecessors o f task Ti

PEIdleTime{PEj) - FinishTime (Last task on P E j)

PEIdleTime is the time when a processing element becomes idle by completing the

execution o f all the tasks allocated to it.

Next, data communication time for task T, is defined when T is allocated to PEj. This is the time taken to transfer all the required input data of T to PEj. This time is obtained by adding data transfer times o f all the predecessors of T, that are not allocated to PEf.
CommTime[Ti, P E j) =

E DataXfr [PRED (7] )) \fPRED[T.yPEj

where
PRED(Tj) = Set o f all predecessors o f task T

Now based on 'EarliestStartTime', 'PEIdleTime' and 'CommTime', we define start and finish times for a task T, when it is allocated to PE/.
StartTime (r,-, P E j)

= MAX(EarliestStartTime(T^ ), PEIdleTime(PEj ) )

FinishTime[Tf, PEj ) = StartTime (7],P E j) + CommTime (7%,P E j) + ExecTime (7].,P E j )

Using the above equations, finish time of the highest priority ready task is calculated for all processing elements that have been added to the system. Ready task is the task which

50

has all its predecessors allocated. Task is allocated to the processing element that has the earliest finish tim e. I f the earliest finish time violates the pipeline period constraint, a new pipeline stage is created and task is added to the new pipeline stage. Earliest start tim e o f the task is set to zero and finish time is calculated for all the processing elements again. The task is then allocated to the processing element, in the new pipeline stage that has earliest finish time. If earliest finish time o f the task violates the pipeline period even in this new pipeline stage, pipelined allocation fails and another processing element needs to be added into the system.

3.3

T opology S election

Task allocation process (after its completion) results in a set o f processing elements w hich com m unicate w ith each other based on the inter-task communication. Tasks are scheduled to execute in different pipeline stages based on the timing constraints. Tasks are also assigned a start tim e and a finish time. System at this point meets all the constraints, how ever the processing elements are connected in an irregular topology. These processing elem ents are mapped onto a regular topology during topology selection phase.

Irregular topologies have certain disadvantages. Primarily, in irregular topologies more than one data com m unication paths are not guaranteed that are needed to support fault tolerance. I f a com m unication link or a processing elem ent fails, data can always be routed to other processing elements through a different path. M oreover, data routing is

51

complex in irregular topologies and such mechanisms may prove to he costly in terms o f area, power etc.

In the proposed algorithm, irregular interconnection of processing elements (PE) is converted to a regular PE network. This mapping can increase data communication time, as data might have to go through a number of links. As a result additional processing elements may be added during this process. Topologies considered in this thesis are mesh, hypercube and quad-tree topologies. The algorithm selects a best topology out o f these three, however approach presented here is not limited only to these schemes and any other regular topology can be added as well.

Selection o f a regular topology is performed by initially generating regular topologies and assigning addresses to each location. All the processing elements are then mapped to these topologies. Mapping is performed by assigning neighbors to each processing element based on the magnitude o f data traffic. Finally overhead for each topology is calculated and topology with least overhead is selected as the best topology. Following sections describe each o f these steps in detail.

3.3.1

Topology Generation and Addressing

First step in topology mapping and selection is to create an empty template o f nodes and assign addresses to each node. All the topologies are defined using the same template so

52

that the m apping does not depend on the type o f topology. Topology tem plate is defined as a graph consisting o f nodes (Vr) connected by edges (E t)
A = { V t, E x }

where, } » is the set o f nodes o f the topology
E t = {^, = I ' is the set o f edges which connect the nodes

Each node is assigned a unique address 'Addr(vi) ' and topology has same num ber o f nodes as the num ber o f processing elements in the system, that is;
Vt --
+

A set o f neighbors is defined for every node in the topology. Set o f neighbors contain nodes w hich are adjacent to the current node. Set o f neighbors is defined as;
 ^ V ,. = { " I

U sing these equations, topologies are generated and addresses are assigned to each node. N ow the process for m esh, hypercube and quad-tree topologies is described.

a) Mesh Topology
M esh topology is generated by arranging nodes in a grid. To determine num ber o f rows and colum ns o f grid, first num ber o f processing elements in the system (SYS_PE^ is calculated;

53

SYS PE =

+

Using this number, number of rows and number of columns are determined using following equations;
MAX _ ROWS = I"yjSYS _ PE MAX _ COLS = \(S Y S _ P E ^ MAX _ RO W S)\

Finally, bits required to represent a row/column address is calculated as;
ADDR _ BITS = I"log; {MAX _ ROW S)]

Using these variables, nodes are added on a grid in a row-wise fashion. Addresses are assigned to each node by concatenating row and column addresses. After address assignment, the neighbors are added if they exist for given number o f nodes. The existence is checked by using boundary conditions. Following pseudo code illustrates the process.

54

node = 0; ïf^R row=0 to MAX_ROWS-l FOR COl=0 to MAX_C0LS-1 //Assign address to node by concatenating //addresses Addr(Vnoa,) = (row « ADDR_BITS) | colj ^^^node = = IF { // set neighbor count to 'O' ) // add upper neighbor row .and column-

'·

Exist(UPPER_NEIGHBOR) AddNeighbor(UPPER);

ENDIF IF ( Exist(LOWER_NEIGHBOR) AddNeighbor(LOWER); ENDIF IF ( Exist(LEFT_NEIGHBOR) AddNeighbor(LEFT); ENDIF IF ( Exist(RIGHT_NEIGHBOR) AddNeighbor(RIGHT); ENDIF node++; IF( node == SYS_PE RETURN; ENDIF ENDFOR BNDFOR ) // add right neighbor )
:

) // add lower neighbor i i>

// add left neighbor

)

//return when all nodes have been added

Figure 3.3 Pseudo Code for Mesh Topology Generation and Address Assignment
Follow ing figure shows the application o f above algorithm for eight nodes. At this point,
only a blank tem plate has been created and no processing element has been allocated to

any node o f the topology.

55

Figure 3.4 Eight Node Mesh Topology with Address Assignment

b) Hypercube Topology
Hypercube topology is generated by initially assigning addresses to each node. Addresses in this case range from `0' to `SYS_PE', where SYS_PE is same as defined for mesh topology. After assigning the addresses, neighbors for a node are added. Degree of hypercube identifies number of neighbors for each node. Degree o f a hypercube is defined as;
DEGREE = plog; {SYS _ P E )\

Adjacent nodes in hypercube topologies have only 1-bit difference between the addresses [49]. This fact is used to assign neighbors for each node. Following figure illustrate this process.

56

./·/Assign address to each node

, v - .s

Addr (
ENDFOR

} -- node #
// set neighbor count to 'O'

N . ^node =0;

//Set neighbors for each node FOR i=0 to SYS_PE-1 FOR j=0 to SYS_PE-1 IF { AddrBitDiff(Vi,Vj,DEGREE) == 1 ) AddNeighbor( Vi,Vj) ; ENDIF ENDFOR ENDFOR // add Vj as Vi's neighbor

Figure 3.5 Pseudo Code for Hypercube Topology Generation and Address Assignment

Follow ing figure shows the application o f above algorithm for eight nodes.

Figure 3.6 Eight Node Hypercube Topology with Address Assignment

c) Quad-Tree Topology
Tree topology is generated by assigning address and adding hierarchal neighbors as a node is created. Each created node is added to a FIFO to add further nodes and neighbors.

57

Finally after hierarchal links have been created, additional links are added at the same level between adjacent neighbors to support fault tolerance. Following figure describes this process.
nodeaO; Addr(v,,oa,)=l; node++; AddtoFlFO{v,,o<i«) ; WHILE ( Vf = GetfromFIFOO ) //Continue till FIFO is empty

//Add four new nodes (quad-tree) and update neighbors FOR 1=0 to 4 Addr(v,,od.) = Addr(Vf)*4 + 1; //Address for current node =0; // set neighbor count to '0'

AddNe ighbor (Vf, Vaoj,) ; node++; IF (node == SYS_PE) BREAK; //Break when all the nodes have been created ENDIF AddtoFIF0(v,,oa,) ; ENDFOR ENDWHILE //Add extra links by connecting adjacent nodes at the same level to //support fault tolerance CreateSaneLevelNeighbors(); //Add the new node to FIFO

Figure 3.7 Pseudo Code for Quad-Tree Topology Generation and Address Assignment

Following figure illustrate the application of above algorithm for a quad-tree with eight nodes. Dotted lines in the figure indicate the extra links which provide multiple communication paths between different processing elements.

58

Figure 3.8 Eight Node Quad-Tree Topology with Address Assignment

3.3.2

Topology Mapping

Processing elem ents are mapped to a topology in a manner such that data communication delays can be m inim ized. To minimize the delay, processing elements are allocated to topology nodes based on their volume o f communication with their neighbors. Traffic betw een different processing elements is defined using data communication with already allocated neighbors o f a topology node. A node is allocated if a processing element has been assigned to that node (processing element has a unique address). Consequently allocated neighbors o f node v,- are those nodes which are adjacent to v, and have been assigned a processing element. These neighbors are denoted by N eighbor traffic is then defined as; , and ç
.

59

N eighTrqffîc(PEj,Vj)=

% ] CommTime (^PE^, PE [v^fj Vv.eN^j

where
PE (vJ is processing element allocated to topology node Va CommTime(PEi, PEj) gives the time taken to transfer data between PEi and PEj

A FIFO list is used during allocation of processing elements to topology nodes. Unallocated neighbors o f the current node are stored in the FIFO. This ensures that processing elements are assigned first to the immediate neighbors o f allocated nodes. Following pseudo code illustrate the mapping of processing elements to topology nodes.

60

//Assign first PE to start node of the topology;-. PEg = ALLOCATED; //Add all the neighbors of V q to FIFO A d d t o P I F O ( N ); //Continue processing while there is any un-allocated node WHILE ( = GetfromFIFOO ) //Continue till FIFO is empty //Calculate neighbor traffic for //unallocated processing elements FOR ALL UNALLOCATED PEs: PEj, FOR j=0 to current node and all ~, ,

,

v i

CalculateNeighborTrafficCPEj., Vj) ; ENDFOR ENDFOR //Get PE which has maximum neighbor traffic PE* = GetMaxTraf fi c P E O ; //Assign PE* to

PEj,
P E j,

;
= ALLOCATED; to FIFO

//Add unallocated neighbors of AddtoF I F O ( N ENDWHILE - N* );

F ig u re 3.9 Pseudo code for Topology M apping

3.3.3

B est T opology Selection

B est topology is selected by calculating the overhead o f each topology. Overhead is the extra tim e spend in communicating data from one processing element to another. It is com puted relative to the irregular topology, where a processing element can

com m unicate w ith all the required processing elements directly. To describe overhead, neighbors o f processing elements and hops required for data communication need to be

61

defined. Neighbors o f a processing element forms a set Npg _, where each member is a processing element that needs to communicate with PEi. Hops refer to the number of communication links traversed in a topology in order to transfer data between two given processing elements. Number of hops, denoted by
, can be calculated from the

addresses of each topology node. Following sections describe mechanism to calculate number of hops for each topology;

a) Mesh Topology
For a mesh topology, address of each node is assigned by concatenating row and column addresses. Number o f hops needed to communicate data from one node to another is the sum o f absolute differences of corresponding row and column addresses. For example in Figure 3.4, data transfer from node `0' (binary: 00 00) to node `9' (binary: 10 01) would require 3 hops (differences in row and column addresses is 2 and 1 respectively).

b) Hypercube Topology
For a hypercube topology, number o f hops needed to communicate data from one node to the other is number o f bit differences in the corresponding addresses. For example, in Figure 3.6, number of hops needed to communicate data from node `1' (binary: 001) to node `6' (binary: 110) is 3.

62

c) Quad-Tree Topology
N um ber o f hops for quad tree is sum o f level difference o f two nodes and twice the level difference betw een node at lower level and first common parent o f two nodes. Level o f a node is defined as |_log^ (^ i)J · For quad tree o f Figure 3.8, node `16' is on level 2 and node 1 is on level 0. Difference between these levels is 2. Also, first comm on parent o f two nodes is node `1', therefore level difference between node at lower level (node `1') and first com m on parent is 0. Thus, number o f hops required to comm unicate between node `16' and node `1' is 2+2(0) = 2.

N ow, based on Npg and

overhead o f a topology is defined as;

O verhead p =

{SYSPE-l)\^^<\ r \ \ ! \ X! \^ h o p s \^ P E i,P E j\-\\^ o m m T im e\^ P E ^ ,P E j\ ,=0 y=0 ^ ^ ^

where,
CommTime(PEi, PEj) gives the time taken to transfer data between PEi and PEj

This overhead gives the extra time taken to transfer data between all the processing elem ents o f the system. Topology which minimizes this overhead is selected as the best topology.

3 .4

S ch ed u lin g

A fter processing elem ents are mapped to a topology, all the tasks o f the application are rescheduled. Scheduling at this phase is necessary because o f the extra communication

63

delays introduced by the regular topology. List scheduling is used to schedule tasks in this phase. The technique is similar to one used in pipelined task allocation except that every task is scheduled on the processing element which was selected for it during pipelined task allocation phase. Pipeline stages are not modified during this phase and each task is scheduled in the same pipeline stage where it was scheduled during the pipelined task allocation phase.

Scheduling is performed by selecting the highest priority task that has all its predecessors already scheduled. Priority assignment process is described in Section 3.2.2. Earliest start time and processing element idle time are then calculated for the selected task. Start time o f the task is the maximum of these two quantities. Data communication time is then calculated taking into account the location of processing element for the current task and processing elements o f all its predecessor tasks. Finally, finish time of the task is calculated by taking into account exact communication delays and execution time o f the selected task. If task completes its execution within constraint pipeline period, ready task list is updated (as a result of scheduling of a task, more tasks may have all their predecessors scheduled). Otherwise, scheduling fails and more processing elements need to be added to the system by reducing the pipeline period. Following pseudo code illustrates the scheduling phase.

64

//Add the first task of the application to ready task list = Tasko; //Continue processing while there is any un-allocated node WHILE ( Task* = GetReadyTask () ) //Get the PE where task was allocated PE = 6etTaskPE(Taskj.) ; //Get the time when all parent tasks of TaskR finish their //execution. This time is '0' for a new pipeline stage EST = GetEarliestStartTime (TaskR) ; //Time when PE finishes the execution of all tasks //allocated before 'Task%' PE_IDLE_TIME = GetPEIdleTime(PE) ; //Time to transfer all input data from parent tasks. This //time takes into account extra hops required as a result //of regular topology COMM_TIME = GetDataXfrTime (PE, Tasks) ; //Time taken by 'Task%' to complete its execution on 'PE' EXEC_TXME = GetTaskExecTime(PE, Tasks); //Time when ' T a s k s ' finishes its execution on 'PE' FINISH_TIME = GetTaskFinishTime (
T asks »

'

M A X (EST, PE_IDLE_TIME), COMMITIME, EXEC_TIME

);
IF(FINISH_TIME > T reriod) RETURN ERROR; ENDIF //Update Ready Task list as a result of scheduling 'Tasks' UpdateReadyTasks() ; ENDWHILE

Figure 3.10 Pseudo code for Scheduling

5.5

P ip elin e P e rio d Reduction

Extra com m unication delays introduced as by the topology mapping m ay cause scheduling phase to term inate unsuccessfully. I f scheduling fails, then more processing

65

elements are required to be added into the system so that tasks can complete their execution well within the target period and extra communication times do not violate the system pipeline period constraint. Pipeline period can be reduced by the maximum violation time factor; however other tasks may not violate the pipeline period with the same factor and this may redundantly add more processing elements in the system. Moreover, all pipeline stages do not have the same period as tasks in some pipeline stages complete earlier than the pipeline period. Therefore reducing the period by maximum violation time can result in expensive systems. Based on these factors an iterative pipeline period reduction mechanism is used in the algorithm. Pipeline period is reduced by a small amount every time scheduling phase fails. Pipeline reduction factor is defined as;

^ R E D _ F A C T O R~ L T T ~ , in k s^
where,

^

_ Overheadj

Overheadj- is overhead associated with the selected topology (Section 3.3.3) Links/^ is the number of communication links which are missing in the regular

topology and transfers across these links require multiple hops

After pipeline period is reduced, pipelined task allocation phase is repeated unless the system meets this new pipeline period. Processing elements are then mapped to regular topology and scheduling is performed with the original pipeline period constraint. When all tasks are scheduled, algorithm terminates successfully otherwise pipeline period is reduced further and the same process is repeated.

66

CHAPTER 4 EXPERIMENTAL RESULTS

4.1

In trodu ction

This chapter describes the results obtained by using the co-synthesis algorithm on different task graphs. The algorithm has been implemented in C++ programm ing

language and M icrosoft's Visual C environment was used for compilation and verification. A ll the experiments were conducted on a system w ith 512MB m em ory and a Pentium 4 processor running at 3.06 GHz. In the first experiment, M PEG encoder application is used for perform ing hardware software co-synthesis and second experiment w as carried out using random graphs with up to 400 tasks.

4.2

M P E G E n coder

M PE G is a com pression standard which is used to encode digital video [52]. It relies on block based m otion com pensation to reduce temporal redundancies and on DCT (discrete cosine transform ) based compression scheme to reduce spatial redundancies, MPEG produces three types o f coded frames known as 1-frames (intra-coded frames), P-frames (predictively-coded frames) and B-frames (bidirectional predictively coded frames). `P ' and `B ' fram es contain m otion estimation information, T ' frames on the other hand only contain discrete cosine transform ed data. M ost computationally intensive part o f M PEG

67

is motion estimation where a 16x16 block of current image is searched in previous or next reference images. Search results in motion vectors which together with prediction error are stored in `P ' and `B ' frames. Prediction error is coded in the same way as `I ' frames using DCT followed by VLC (variable length coding).

MPEG encoder is specified as a data-flow based task graph for input to co-synthesis algorithm. Task graph for MPEG encoder is shown in Figure 4.1. It is a coarse grained graph consisting o f 22 nodes with each node representing a large block o f frmctionality. Numbers at the edges show the amount of data transferred between the tasks. Images in the video sequence are assumed to be in RGB format. `Initialize' task performs the initialization and maintains a state machine to determine the type of coding (`I', `P ' or `B ') required for the current frame. `YCbCr Conversion' task converts current image block and reference images (for `B ' and `P ' frames) from RGB to YCyCr format. `SubSample' task performs sub-sampling of color difference components (Cy and C J for the block o f current image. `Split Fwd Ref Image' and `Split Bwd R ef Image' tasks split the forward and backward reference images into four overlapping regions for performing motion vector search over these regions in parallel. Tasks `F S l' through `FS4' and `B S l' through `BS4' perform the motion vector search over forward and backward reference images respectively. `Fwd Motion Vector' and `Bwd Motion Vector' tasks select the best forward and backward motion vectors respectively. `Interpolate' task interpolates the forward and backward motion vectors for bi-directionally coded frames. `DCT' task calculates discrete cosine transform for an image block or motion-predication error block. `Quantize' performs quantization and `DCAC Coding' task codes dc and ac components

68

8449

257

4097

R ef#

708

708

708

708

708

708

FS2

FS3

FS4

BS1

196

196

196.

196

195

195

Interpolate
193

193

,193

385

771

Figure 4.1 MPEG Encoder Task Graph

69

o f quantized discrete cosine transformed block. Task `Entropy Encode' performs variable length coding using huffinan encoding scheme. Finally, `Finalize' task packs huffinan coded symbols to form the final compressed bitstream.

Software execution time for these tasks is calculated by executing each task on Altera's Nios processor. Nios CPU is available as configurable soft macro for Altera's fgpa devices. Every task is profiled on two different variants of Nios processor. One variant uses a dedicated hardware multiplier while the other uses multiple instructions to perform multiplication. Some of the tasks are also implemented as dedicated hardware. These tasks include motion vector search, DCT, quantization and dc/ac coding. Each o f these tasks are implemented in Verilog hardware description language at register transfer level and then synthesized on Altera's Stratix fpga. Execution time is then the number o f clock cycles required to complete the task. Execution times for some of the tasks depend on the amount o f data required to be processed (e.g. run-length coding, Huffman encoding etc.). For these tasks, worst case execution time is considered. Area cost associated with each processing element is taken as the number of logic elements required to implement it on Stratix Q)ga. Table 4.1 lists the area cost of each processing element and Table 4.2 gives the execution time of each task on different processing elements.

70

Table 4.1 Processing Element Information for MPEG Application

N IO S_M U L

A ltera's Nios embedded processor with a dedicated hardw are multiplier A ltera's Nios embedded processor without a dedicated hardware multiplier Hardware core for motion vector search Hardware core for discrete cosine transform Hardware core for quantization Hardware core for coding DC and AC components

Software

4065

NIOS

Software

3662

M V S_EN G DCTENG Q U A N T_EN G D C A C _E N G

Hardware Hardware Hardware Hardware

615 1008 712 453

71

Table 4.2 Task Execution Times for MPEG Encoder Application

Initialize YCbCrConvert SubSample SplitFwdReflmage SplitBwdRefTmage FSl FS2 FS3 FS4 BSl BS2 BS3 BS4 FwdMotionV ector B wdMotionV ector Interpolate DCT Quantize DCACCoding EntropyEncoding Finalize

1194178

1245890

131524 131524 131524 131524 131524 131524 131524 131524

-

-

6141804 11261292 122539 199147

1338842 1338842

1338842 1338842

19983314 20025208

936 468 -

1214 -

19983314 20025208 19983314 20025208
19983314 20025208 19983314 20025208 19983314 20025208 19983314 20025208

19983314 20025208
52161 52161 149770 377600 145537 620181 164918 90945 90945

-

304906
775136 260677 620181

242688 334876

256904

72

M PE G encoder task graph along with task execution tim es and processing element hardw are area inform ation is fed to the co-synthesis algorithm. A range o f constraints is provided to the algorithm to obtain a wide range o f designs. The algorithm outputs a set o f heterogeneous processing elements arranged in a regular distributed m em ory topology and a pipelined schedule for the set o f tasks o f the application. M ajor factors in the output consist o f system pipeline period, area o f the system, num ber o f processing elem ents and num ber o f pipeline stages created.

A lgorithm was run for tim e constraints varying from 172000000 clock cycles to 6500000 clock cycles. Corresponding constraints on area ranged from 8000 logic elements to 38000 logic elements. Results obtained for these constraints are shown in Table 4.3. M ore processing elem ents get added into the system as the time constraints are made tighter. A lso, the num ber o f pipeline stages increase as system tim e period decreases. System corresponding to the tightest constraint (6500000 cycles) consists o f 7 software processors and 11 dedicated hardware processing elements. The hardware processing elem ents correspond to 8 m otion vector search engines and 3 hardware blocks for DCT, quantization and dc/ac coding. This system, giving the highest performance has the m axim um area and tasks execute in 5 pipeline stages. On the other extreme, system corresponding to slowest requirements is around 26 times slower and takes around 9 tim es less area. System consists o f only one processing elements and task execution is not pipelined. Figure 4.2 illustrates the design space exploration corresponding to various test cases.

73

Table 4.3 Time/Area Results for MPEG Encoder

Q
1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 6500000 7500000 8500000 9500000 25000000 40100000 45000000 55000000 61000000 63000000 95000000 172000000 38000 17000 17000 17000 15000 15000 13000 12500 11700 11500 8000 4065 6420893 7458521 7592363 8807958 21459777 40091097 41484886 53897342 60370101 61419853 90544535 171821949 36483 16309 16309 16309 14676 14655 12831 12216 11601 11389 7727 4065 18 11 11 11 9 7 6 5 4 3 2 1 5 3 3 3 4 4 4 3 4 3 2 1

7 3 3 3 3 3 3 3 3 3 2 1

11 8 8 8 6 4 3 2 1 0 0 0

40000

Constraints Actual Results
35000

30000

I

5

25000

I I

20000

15000

10000

5000

System Time (Time Units)

Figure 4.2 Design Space Exploration for MPEG Encoder Application
74

O ther than tim ing and area results, an important characteristic o f the resulting system is the arrangem ent o f the processing elements in a regular topology. Table 4.4 lists this characteristic o f the system for each test case. It shows the overhead involved in arranging processing elem ents to a particular topology and num ber o f links missing in each topology w hen com pared to a fully connected topology. It also gives the num ber o f extra processing elem ents that are added to as a result o f increased extra communication delays due to a regular topology.

Table 4.4 Topology Information for MPEG Encoder

1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12.

6500000 7500000 8500000 9500000 25000000 40100000 45000000 55000000 61000000 63000000 95000000 172000000

24051 6201 6201 5946 2034 2329 936 1880 2490 999 0 0

27 15 15 16 9 10 5 6 4 2 0 0

13037 7978 7978 9681 7458 4746 3163 2710 2905 999 0 0

18 19 19 16 14 12 7 4 3 2 0 0

14696 5394 5394 4017 2647 2942 1549 2295 2905 999 0 0

24

MESH

0 0 0 0 0 0 0 0 0 0 0 0

15 HYPERCUBE 15 HYPERCUBE 13 HYPERCUBE 9 10 6 5 3 2 0 0 TREE TREE TREE TREE TREE TREE TREE TREE

75

It is seen that as the time constraints become tighter and tighter, overhead and number of missing links increase. This is due to the fact that as timing constraints get smaller, more processing elements are added which increase the inter-processor communication. For mpeg encoder application, no extra processing element was required and algorithm selected all o f the available topologies for different test cases. For the test case 1, Mesh topology has the lowest overhead. System in this case consists of 11 processing elements and 5 pipeline stages. Figure 4.3 shows the irregular interconnection o f the processing elements before they are mapped onto a regular topology. Arrangement of processing elements in a regular topology is shown in Figure 4.4. Corresponding schedule map for each task is displayed in Figure 4.5. Figure 4.6, Figure 4.7 and Figure 4.8 show similar details for test case 4, where processing elements are arranged in a hypercube topology. Figure 4.9, Figure 4.10 and Figure 4.11 provide same details for test case 5, where tree topology has the least overhead. Finally, an interesting case is where system consists o f only a single processing element. In this case there is no overhead and all the tasks execute one after the other depending on their priority. This case is illustrated in Figure 4.12.

76

Figure 4.3 Irregular Processing Element Topology (Test case 1)

Figure 4.4 Processing Elements for MPEG Encoder Arranged in Mesh Topology

77

fNfTTALlZE (NIOS_MULO) YCBCR_CONVERT (NI0S_MUL1 )

SUBSAMPLE (N10S_MUL1)
SPLrr_FW D_RI (NIOSO) SPLrr_BW D_RI (NIOS1) F S l (MVS_ENG1) FS2 (MVS_ENG2) F S 3 (MVS_ENG3) FS4 (MVS_ENG4) FWD_MOTION_VEGTOR (NIOS2) BS1 (MVS_ENG5) BS2 (MVS_ENG6) BS3 {MVS_ENG7) BS4 (MVS_ENG8) BWD_MOTION_VEGTOR (NIOS3) INTERPOLATE (NIOS2) DGT(DGT_ENG) QUANTIZE (QUANT_ENG) DG_AG_GODING (DGAG_ENG) ENTROPY_ENCODE (N D S4) FINALIZE (NI0S4) 1000000 2000000 3000000 4000000 5000000 6000 0 0 0 7000000

Time (time units)

Figure 4.5 Schedule Map for Mesh Topology

i

78

Figure 4.6 Irregular Processing Element Topology (Test case 4)

«a;
MVS EN61

Figure 4.7 Processing Elements for MPEG Encoder Arranged in Hypercube Topology

79

INrmLIZE (NIOS_MULO) YCBCR_CONVERT (NKDS_MULO) SUBSAMPLE (NI0S1) SPLIT_FWD_RI (NIOS_MULO) SPLrr_BWD_RI{NIOSO) F S l (MVS_ENG1) F S 2 (MVS_ENG2) F S 3 (MVS_ENG3) F S 4 (MVS_ENG4) FWD_MOTION_VECTOR (NIOS1 ) BS1 (MVS_ENG5) B S2 (MVS_ENG6) B S3 (MVS_ENG7) B S4 (MVS_ENG8) BW D_M0T10N_VECT0R (NI0S1 ) INTERPOLATE (NI0S1) DCT(N IO SI) QUANTIZE (NIOS 1) DC_AC_CODING (NI0S1 ) ENTROPY_ENCODE (NI0S1) FINALIZE (NI0S1) 0 1000000 2000000 3000000 4 0 0 0000 5000000 6 0 0 0 0 0 0 7 0 0 0 0 0 0 6 0 0 0 0 0 0 9000 0 0 0

0 i
0

Time (time units)

Figure 4.8 Schedule Map for Hypercube Topology

80

Figure 4.9 Irregular Processing Element Topology (Test case 5)

Figure 4.10 Processing Elements for MPEG Encoder Arranged in Tree Topology

81

N m ALEE (NIOSO) YCBCR_CONVERT (NIOSO) SUBSAMPLE (NI0S3) SPLtr_FW D_RI (NIOSO) SPLIT_BWD_RI (NIOS1) F S l (MVS_ENG1) FS2(M VS_ENG2) F S 3 (MVS_ENG3) F S 4 (MVS_ENG4) FWD_MOT10N_VECTOR (NIOSO) BS1 (MVS_ENG5) BS2 (MVS_ENG6) BS3 (NI0S1) BS4 (NIOS3) BWD_MOT10N_VECTOR (NIOS1) INTERPOLATE (NIOSO) DCT (NIOSO) QUANTIZE (NIOSO) DC_AC_CODING (NIOSO) ENTROPY_ENCODE (NIOSO) FINALIZE (NIOSO)

f f I f f

i f

i
T
0 5000000 10000000

T
15000000

I 20000000
25000000

Time (time units)

Figure 4.11 Schedule Map for Tree Topology

82

INmALIZE (NIOS_MUL) Y C B C R _ C O N V E R T (NIOS_MUL) S U B S A M P L E (N IO S_M üL) SP L rT _F W D _R I (NIOS_MUL) S P L (T _B W D _R I {NI0S_M ÜL)

f 9 9

F S l (N IO S_M üL)
F S 2 (N IO S_M üL) F S 3 (NIOS^M UL) F S 4 (NIOS_MUL) FW D _M O T lO N _V E C T O R (N10S_M UL) B S l (NlOS_MUL) B S 2 (NIOS_MUL) B S 3 (NIOS_MUL) B S 4 (NIOS_MUL) B W D _M O TIO N _V E C T O R (NIOS_MUL) IN TERPO LA TE (NIOS_MUL) D C T (NIOS_MUL) QUANTIZE (NIOS_MUL) D C _A C _C O D IN G (NIOS_MUL) E N T R O P Y _E N C O D E (NIOS_MUL) FINALIZE (NIOS_MUL)

I

I

I

I

I

I

I

1----------- 1

20000000 40000000 60000000 80000000 100000000120000000 140000000160000000180000000

Time (time units)

Figure 4.12 Schedule Map with only a Single Processing Element in the System

4.3

P a ra lle l M P E G D ecoding

In the second experim ent, proposed algorithm is applied to a test case from Hypercube C o-synthesis algorithm [46]. Hypercube co-synthesis algorithm involves processing elem ent selection, task scheduling and pipelining however the processing elements are alw ays m apped to a hypercube topology. Also the algorithm uses RECOD pipelining m ethod ([22]) that m ay also result in redundant pipeline stages. The task graph for this test case consists o f 22 nodes with 16 MPEG decoding tasks. Pentium H processor (450

83

MHz) was used for software implementation and Altera FLEXIOKE FPGA was employed for hardware synthesis. Following figure shows the task graph for parallel MPEG decoding.

133' 133

^^gidnalN.

/^egionarv

Comparison ) (Comparison )

/Regional' legionaT ( Compansoi

133

133'

'133

133

Finalize

Figure 4.13 Parallel MPEG Decoding Task Graph

Proposed method is tested with the same constraints as used in the Hypercube co synthesis technique. Area constraints were varied from 11.5M to 7.5M area units and corresponding time constraints were 30000 to 770000. Algorithm was able to meet all the constraints. Processing elements were mapped to tree and hypercube topologies. In the first case where the resulting system consists of 7 processing elements, tree topology results in 10 commumcation links as opposed to hypercube topology which consists o f 12

84

links. This show s the savings in cost o f the resulting system. Detailed results are shown in Table 4.5and Table 4.6. Optimal results are also provided in [46]. Results obtained by the proposed co-synthesis algorithm suggest that its timing perform ance is, on an average, only 0 97/6 m ore than optimal and it utilized only 0.20% more area than the optimal results. A s a com parison, timing and area differences for hypercube co-synthesis algorithm w ere 3.75% and 0.62% respectively. This illustrates the effectiveness o f the proposed algorithm . Comparison results are shown in Figure 4.14.

Table 4.5 Time/Area Results for Parallel MPEG Decoding

m
K m # 1. 2. 3. 4. 5. 6. 7. 8. 40000 50000 60000 90000 120000 130000 600000 770000 11500000 11000000 10500000 9500000 9000000 8600000 7600000 7500000 36320 45405 45405 68069 118040 118040 588480 759296 11118452 10258292 10258292 9398132 8537972 8537972 7588611 7500000


7 6 6 5 4 4 2 1

4 4 4 4 4 4 2 1

1 1 1 1 1 1 1 1

6 5 5 4 3 3 1 0

85

Table 4.6 Topology Information for Parallel MPEG Decoding

1. 2. 3. 4. 5. 6. 7. 8.

40000 50000 60000 90000 120000 130000 600000 770000

1599 1995 1995 2394 2261 2261 0 0

4 3 3 3 2 2 0 0

2009 2675 2675 2800 1729 1729 0 0

6 5 5 3 1 1 0 0

1869 1335 1335 1862 1729 1729 0 0

5 3 3 2 1 1 0 0

TREE HYPERCUBE HYPERCUBE HYPERCUBE HYPERCUBE HYPERCUBE TREE TREE

0 0 0 0 0 0 0 0

11.5

xIO"

Constraints  0 " Optimal Results - 0 - Results of Proposed Algorithm Hypercube Co-synthesis Results

10.5

I3

I I
I « §

8.5

6.5

7.5

System Time (Time Units)

Figure 4.14 Comparison Results for Parallel MPEG Decoding

86

4 .4

R a n dom G raphs

Third experim ent is conducted by performing co-synthesis on random task graphs. These graphs are generated by random ly varying number o f predecessors and successors for each task, depth or num ber o f levels and amount o f data transferred between different tasks. Successors and predecessors for a task are varied from 2 to 20 and tasks without a successor are connected to the final task. Number o f processing elements available for the system is also random and both hardware and software processing elements are included in the library. Execution tim e o f each task depends on the area and type o f the processing elem ent. H ardw are processing elements execute respective tasks in less time compared to softw are counterparts. Large random graphs are generated to conduct the experiments. Five different types o f graphs are created with 50, 100, 200, 300 and 400 nodes. M ultiple graphs for each type are used and each o f the graphs is then tested for a range o f area and tim ing constraints. Figure 4.15 shows a random graph o f 50 nodes.

87

Figure 4.15 Randomly Generated 50-node Graph

88

T im ing constraints for are varied from 25000 to 70000 tim e units for five different task graphs (graph a to graph e ) o f 50 nodes. Area constraints range from 1800 to 16500 area units. N um ber o f processing elements ranged from 1 to a maximum o f 10 processing elem ents and pipeline stages varied from 1 to 9. All the topologies were selected for different cases and overhead o f the topologies increased as the pipeline period became smaller. A lso, for cases with tighter constraints extra processing elements were required to cater for delays introduced due to regular topology mapping. Num ber of missing links increase as tim ing constraints become smaller which is due to the fact that more processing elem ents get added into the system and communication between processing elem ents increases. Table 4.7 shows the timing/area results with number o f processing elem ents and pipeline stages. Table 4.8 provides the topology information and Figure 4.16 illustrates the design space exploration for each graph with 50 tasks. Similar results are obtained for other graphs, however, number o f missing links at smallest constraint increases w ith num ber o f tasks in the graph. Subsequent tables and figures illustrate the results for 100, 200, 300 and 400 node task graphs. Finally, Figure 4.21, Figure 4.22 and Figure 4.23 show the arrangement o f processing elements for 400-node task graph (graph `e ' w ith tim e constraint o f 100000 time units), 200-node task graph (graph e with time constraint o f 125000 tim e units), and 300-node task graph (graph `c ' with time constraint o f 200000 tim e units) respectively.

89

Table 4.7 Time/Area Results for 50 Node Graphs

m SM
a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e. 25000 30000 40000 50000 67000 25000 30000 40000 50000 60000 25000 30000 35000 40000 65000 25000 30000 40000 45000 65000 30000 40000 45000 50000 70000 8500 5500 5300 2650 1400 14200 9600 7000 5000 2400 16500 11000 8100 5500 2700 11500 9600 5300 3500 1800 11000 8100 6100 4100 2100 22574 29934 30884 46840 66795 24851 27336 35579 40417 59125 23594 29092 33442 39606 60661 24144 28298 38595 43813 61778 26350 34844 42934 49468 68840 8492 5268 5268 2634 1317 14131 9529 6903 4602 2301 16456 10742 8031 5354 2677 11390 9512 5232 3488 1744 10075 8060 6045 4030 2015 8 4 4 2 1 8 6 3 2 1 8 5 3 2 1 10 8 3 2 1 5 4 3 2 1 9 5 4 2 1 8 7 3 2 1 7 5 3 2 1 6 5 3 2 1 6 4 3 2 1 6 4 4 2 1 6 4 3 2 1 6 4 3 2 1 6 5 3 2 1 5 4 3 2 1 2 0 0 0 0 2 2 0 0 0 2 1 0 0 0 4 3 0 0 0 0 0 0 0 0

90

Table 4.8 Topology Information for 50 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

25000 30000 40000 50000 67000 25000 30000 40000 50000 60000 25000 30000 35000 40000 65000 25000 30000 40000 45000 65000 30000 40000 45000 50000 70000 1

30573 15117 15117 0 0 21260 9834 6755 0 0 28498 13322 7341 0 0 27041 21888 8756 0 0 12597 9450 5958 0 0

30 6 6 0 0 27 14 2 0 0 29 9 2 0 0 37 22 2 0 0 12 6 2 0 0

29057 7475 7475 0 0 23021 10866 6755 0 0 25701 13210 7341 0 0 31235 24441 8756 0 0 17438 7626 5958 0 0

26 4 4 0 0 22 13 2 0 0 26 7 2 0 0 33 18 2 0 0 10 4 2 0 0

17419 7475 7475 0 0 17601 9452 6755 0 0 17140 13227 7341 0 0 22872 20920 8756 0 0 13475 7626 5958 0 0

24 4 4 0 0 23 11 2 0 0 24 8 2 0 0 30 18 2 0 0 10 4 2 0 0

HYPERCUBE HYPERCUBE HYPERCUBE TREE TREE HYPERCUBE HYPERCUBB TREE TREE TREE HYPERCUBE MESH TREE TREE TREE HYPERCUBE HYPERCUBE TREE TREE TREE TREE HYPERCUBE TREE TREE TREE

2 0 1 0 0 1 0 0 0 0 3 2 0 0 0 2 1 0 0 0 0 1 0 0 0

91

9000 0000

I I

@7000

I 5000 g 4000 3000

2000 1000

,

Z5
System Time (Time Units)

5.5

6.5

Ccnstreints Actual Resutts

14000

6000 4000 2000,

2.5

3.5
System Time (Time Units)

4.5

1.0

x10^ Constraints

i0 5
0.6

0 .2 ,,

2.5

3.5

4

4.5

5.5

System Time (Time Units)

K it/
Constraints Ataual Results

6.5

12000

10000

f

8000

4000 2000

2.5

3.5

4

4.5

5.5

System Time {Time Units)

ioooo
8000 f «000

Constraints Actual R e s ti s

I 7000 I 6000
4000 3000

5.5
System Time (Time Units)

6.5

Figure 4.16 Design Space Exploration for 50 Node Graphs
92

Table 4.9 Time/Area Results for 100 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

50000 70000 90000 100000 135000 35000 50000 70000 90000 135000 40000 50000 60000 80000 120000 50000 60000 80000 100000 130000 55000 57000 65000 90000 130000

14000 6100 4600 3100 1600 70000 33000 15000 7500 3600 26000 17500 11500 5600 2800 17000 12000 5500 3800 1900 17000 15000 12000 4500 2224

40091 62675 85038 93355 134534 33824 44775 62958 89201 125803 36381 49736 56830 79706 115622 40964 53047 79409 88138 125303 52583 56430 60545 84821 117628

13045 6016 4512 3008 1504 68035 32435 14635 7120 3560 25377 17028 11412 5566 2783 16936 11075 5433 3622 1811 16504 14280 11424 4448 2224

11 4 3 2 1 21 11 6 2 1 11 8 5 2 1 11 7 3 2 1 10 9 6 2 1

11 4 3 2 1 14 10 7 2 1 10 8 5 2 1 11 7 3 2 1 10 9 6 2 1

8 4 3 2 1 19 9 4 2 1 9 6 4 2 1 9 6 3 2 1 7 6 5 2 1

3 0 0 0 0 11 7 3 0 0 2 2 1 0 0 2 1 0 0 0 3 3 1 0 0

93

Table 4.10 Topology Information for 100 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

50000 70000 90000 100000 135000 35000 50000 70000 90000 135000 40000 50000 60000 80000 120000 50000 60000 80000 100000 130000 55000 57000 65000 90000 130000

75759 30198 24300 0 0 169524 108692 35078 0 0 84391 62372 29133 0 0 86426 57457 17562 0 0 83788 64198 43530 0 0

57 6 2 0 0 151 64 11 0 0 62 29 9 0 0 64 24 2 0 0 38 32 13 0 0

75032 19159 24300 0 0 185060 94309 26265 0 0 85423 65579 20795 0 0 82731 60766 17562 0 0 75404 58827 47018 0 0

52 4 2 0 0 144 56 11 0 0 55 23 7 0 0 57 22 2 0 0 36 28 13 0
-,

57230 19159 24300 0 0 116600 80141 35065 0 0 70341 48566 29670 0 0 65858 45394 17562 0 0 64168 53242 39613 0
, -------------

48 4 2 0 0

HYPERCUBE HYPERCUBE TREE TREE TREE

6 0 0 0 0 11 4 3 0 0 3 3 0 0 0 5 3 0 0 0 5 5 2 0 0

138 HYPERCUBE S3 11 0 0 52 20 8 0 0 54 21 2 0 0 31 27 11
1

pYPERCUBE MESH TREE TREE HYPERCUBE HYPERCUBE MESH TREE TREE HYPERCUBE HYPERCUBE TREE TREE TREE HYPERCUBE HYPERCUBE HYPERCUBE TREE TREE

0 0

0

0

94

System Ares (Ares Utrits)
S ystem A rea (Area Units)

System Area (Area Units) System Area (Area UnKs)
S y stem A rea (Area Units}

%

I I
o o %
CL
I --k

I

O

Table 4.11 Time/Area Results for 200 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e.
----------

95000 125000 150000 200000 260000 95000 125000 170000 200000 260000 100000 125000 170000 200000 260000 100000 125000 175000 200000 260000 100000 120000 125000 200000 260000

25000 12000 9000 4500 2300 28000 14000 8500 5500 2800 23000 12500 9300 6200 3084 16500 11000 7500 5000 2500 20000 13000 10000 5000 2500

94430 121990 132613 184755 256035 88896 124634 155297 177806 242599 98365 123889 151555 173985 250705 96527 116951 153308 178012 250029 95929 117500 124652 171165 247290

22718 11228 8908 4454 2227 27673 13958 8229 5486 2743 22437 12336 9252 6168 3084 16035 10633 7470 4980 2490 19850 12533 9756 4878 2439

12 6 4 2 1 12 7 3 2 1 9 4 3 2 1 10 7 3 2 1 9 6 4 2 1

12 6 4 2 1 12 7 3 2 1 8 4 3 2 1 10 7 3 2 1 9 6 4 2 1

10 5 4 2 1 10 5 3 2 1 7 4 3 2 1 6 4 3 2 1 8 5 4 2 1

2 1 0 0 0 2 2 0 0 0 2 0 0 0 0 4 3 0 0 0 1 1 0 0 0

e.

e. e. e.

96

4.12 Topology Information for 200 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

95000 125000 150000 200000 260000 95000 125000 170000 200000 260000 100000 125000 170000 200000 260000 100000 125000 175000 200000 260000 100000 120000 125000 200000 260000

228059 86409 70012 0 0 204237 98766 39507 0 0 158977 76090 35657 0 0 146262 71213 34123 0 0 161684 86789 68810 0 0

80 16 6 0 0 78 18 2 0 0 38 6 2 0 0 44 18 2 0 0 47 14 6 0 0

218399 96510 45378 0 0 222386 92556 39507 0 0 155680 47717 35657 0 0 137651 96623 34123 0 0 150442 87654 43170 0 0

71 12 4 0 0 70 17 2 0 0 35 4 2 0 0 41 17 2 0 0 38 13 4 0 0

171768 90295 45378 0 0 152689 96302 39507 0 0 120176 47717 35657 0 0 118430 71770 34123 0 0 134081 88005 43170 0 0

66 13 4 0 0 69 15 2 0 0 34 4 2 0 0 38 14 2 0 0 39 12 4 0 0

HYPERCUBE TREE HYPERCUBE TREE TREE HYPERCUBE MESH TREE TREE TREE HYPERCUBE HYPERCUBE TREE TREE TREE HYPERCUBE TREE TREE TREE TREE HYPERCUBE TREE HYPERCUBE TREE TREE

4 2 1 0 0 5 3 0 0 0 2 0 0 0 0 3 1 0 0 0 3 2 0 0 0

97

I

2.4
(Time

TA -

I
I

i"
1.4 2.4

X 1 0 *

I"
1.8

I
1.8 (TlmeUnHs)

2.4

1.6

2.4

2
1 .6 -

-4 r

T

T

I
1

1 .6 -

0.4 1,4
System Time (Time Units)

2.4

Figure 4.18 Design Space Exploration for 200 Node Graphs 98

Table 4.13 Time/Area Results for 300 Node Graphs

m

m

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

150000 200000 250000 300000 400000 150000 200000 260000 300000 400000 150000 200000 250000 300000 400000 150000 200000 260000 300000 400000 150000 200000 250000 300000 400000

18000 8500 4200 3000 1500 16000 9600 6500 3200 1600 15000 11000 6100 4100 2019 12000 5600 4200 2800 1396 13500 7500 5600 3800 1852

131113 183401 249678 271854 377918 130661 177877 203441 281363 393717 147645 197525 247438 259514 369897 141493 194796 241119 263491 372880 148745 179266 223493 259033 366398

17066 8232 4116 2744 1372 15930 9558 6372 3186 1593 14249 10164 6057 4038 2019 11168 5584 4188 2792 1396 13134 7408 5556 3704 1852

14 6 3 2 1 10 6 4 2 1 9 6 3 2 1 8 4 3 2 1 8 4 3 2 1

14 6 3 2 1 11 6 4 2 1 10 6 3 2 1 8 4 3 2 1 8 4 3 2 1

12 6 3 2 1 10 6 4 2 1 7 5 3 2 1 8 4 3 2 1 7 4 3 2 1

2 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 1 0 0 0 0

99

Table 4.14 Topology Information for 300 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

150000 200000 250000 300000 400000 150000 200000 260000 300000 400000 150000 200000 250000 300000 400000 150000 200000 260000 300000 400000 150000 200000 250000 300000 400000

403973 221603 67615 0 0 358779 222590 117737 0 0 243640 155663 67490 0 0 250263 112324 60410 0 0 230686 90726 55373 0 0

120 20 2 0 0 72 20 6 0 0 40 14 2 0 0 42 6 2 0 0 33 6 2 0 0

419309 178106 67615 0 0 349114 166553 82765 0 0 245260 140544 67490 0 0 234790 90272 60410 0 0 216663 63284 55373 0 0

108 16 2 0 0 64 16 4 0 0 33 11 2 0 0 36 4 2 0 0 30 4 2 0 0

301949 177289 67615 0 0 261110 178741 82765 0 0 206090 150622 67490 0 0 176151 90272 60410 0 0 171106 63284 55373 0 0

102 pYPERCUBE 16 2 0 0 60 16 4 0 0 35 12 2 0 0 32 4 2 0 0 28 4 2 0 0 HYPERCUBE TREE TREE TREE HYPERCUBE MESH HYPERCUBE TREE TREE HYPERCUBE MESH TREE TREE TREE HYPERCUBE HYPERCUBE TREE TREE TREE HYPERCUBB HYPERCUBE TREE TREE TREE

9 2 0 0 0 5 2 1 0 0 3 2 0 0 0 3 0 0 0 0 3 1 0 0 0

100

18000 18000 14000

I

12000

1.10000

I 0 0 0 0

1.5
System TVne {Time U n li)

3 5

18000

12000

10000
8000 I ^ 6000 4000

1.5

2.5
S ystem Time (Time Unfts)

14000

I 12000

6000

4000 2000

1.5
System Time (Time Units)

3.5

12000

I ë

I
I

I
1.5 2.5 3.5 1.

I

I

I
----------------------------T i 2 '
System Time {Time Units)

«

3

Figure 4.19 Design Space Exploration for 300 Node Graphs
101

Table 4.15 Time/Area Results for 400 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

200000 250000 300000 400000 500000 200000 250000 300000 400000 500000 170000 200000 250000 350000 500000 200000 230000 250000 350000 500000 250000 300000 350000 400000 550000

16000 11500 7500 3800 2000 20000 15000 9300 4700 2500 23000 15300 10500 5100 2600 25000 21000 14000 6900 3500 10050 6500 4800 3200 1600

193942 223619 256960 354636 496379 185507 247651 262363 363667 496114 166779 198317 237502 334429 493521 194678 207343 241836 347966 497429 224331 260789 335686 368259 503096

15736 11250 7500 3750 1875 19199 14565 9268 4634 2317 22800 15267 10044 5022 2511 24094 20652 13768 6884 3442 10017 6464 4722 3148 1574

11 6 4 2 1 13 11 4 2 1 10 7 4 2 1 7 6 4 2 1 8 5 3 2 1

10 6 4 2 1 12 10 4 2 1 10 7 4 2 1 7 6 4 2 1 8 5 3 2 1

m Sm 8

3 0 0 0 0 5 5 0 0 0 1 1 0 0 0 0 0 0 0 0 2 1 0 0 0

6 4 2 1 8 6 4 2 1 9 6 4 2 1 7 6 4 2 1 6 4 3 2 1

102

Table 4.16 Topology Information for 400 Node Graphs

a. a. a. a. a. b. b. b. b. b. c. c. c. c. c. d. d. d. d. d. e. e. e. e. e.

200000 250000 300000 400000 500000 200000 250000 300000 400000 500000 170000 200000 250000 350000 500000 200000 230000 250000 350000 500000 250000 300000 350000 400000 550000

367748 269909 148085 0 0 407296 308627 165444 0 0 373574 250075 139359 0 0 318971 276251 132607 0 0 290773 146109 98506 0 0

61 20 6 0 0 67 39 6 0 0 63 24 6 0 0 30 20 6 0 0 28 9 2 0 0

327633 218650 84897 0 0 451284 317030 107113 0 0 376688 260269 95387 0 0 298067 197729 86574 0 0 302624 107334 98506 0 0

55 16 4 0 0 63 41 4 0 0 55 22 4 0 0 26 16 4 0 0 24 7 2 0 0

300090 196887 84897 0 0 331531 263956 107113 0 0 300727 190527 95387 0 0 235154 203177 86574 0 0 219367 146173 98506 0 0

49 16 4 0 0 57 35 4 0 0 53 19 4 0 0 24 16 4 0 0 22 8 2 0 0

HYPERCUBE HYPERCUBE HYPERCUBE
----------------------

6 2 1 0 0 8 7 1 0 0 3 1 0 0 0 2 2 0 0 0 3 2 0 0 0

TREE

TREE HYPERCUBE HYPERCUBE HYPERCUBE TREE TREE HYPERCUBE HYPERCUBE HYPERCUBE TREE TREE HYPERCUBE MESH HYPERCUBE TREE TREE HYPERCUBE MESH TREE TREE TREE

103

12000

2000 25 3 3.5 System Ttone (T im e Units) 4.5

- a - Constraints

1 J

· Actual R e s u k

0.4

System Time (TTme Units)

I
I

1 1.5

I
0.5 25 xiO*
Constraints - Actual ResuBs

3

3.5

4.5

System Time (Time Units)

x 1 (^

I I
I
0.5

3

3.5

4.5

System Time {Time Units)

Actual Resu#s

3,5

4

System Time (Time UMs)

Figure 4.20 Design Space Exploration for 400 Node Graphs 104

Figure 4.2 1 Topology Mapping for 400-node Graph (Graph `e', T p erio d = 1 0 0 0 00)

PEÔ:

PEG

Figure 4.22 Topology Mapping for 200-node Graph (Graph `d', T period" 125000)

Figure 4.23 Topology Mapping for 300-node Graph (Graph `c% T

p e rio d = 2 0 0 0 0 0 )

105

4.5

Algorithm Execution Time

Time taken by algorithm to generate a co-synthesized hardware software system is an important criterion for its effectiveness. To study this effect, execution time o f the algorithm for each test case was recorded. Algorithm works iteratively thus its execution time is not dependent solely on the number of nodes. Tight constraints require many iterations and therefore take more time. Figure 4.24 shows minimum, average and maximum execution times for graphs with a wide range of tasks. Algorithm was able to provide final output with in 2.5 seconds for largest graph having 400 tasks. Worst case execution time o f the algorithm for graphs up to 100 nodes is even less than 500 milliseconds. This clearly shows its usefulness as, for example Vemuri and Chatha's algorithm takes 30 minutes for a 30 node graph [22].

2.5

M ax. Execution Time Average Execution Time -A- M in, Execution Time

Î

I

0.5

too

150

200

250

300

Tasks in Application

350

400

Figure 4.24 Algorithm Execution Time

106

CH A PTER S CONCLUSION AND FUTURE WORK

In this thesis a new co-synthesis algorithm for fault tolerant applications is presented. The algorithm targets regular distributed memory architectures by arranging processing elem ents in m esh, hypercube and quad-tree topologies. A data flow graph specifying the application, library o f heterogeneous processing elements and profile information for each task is provided to the algorithm along with constraints for pipeline period and area cost o f the resulting system. The co-synthesis algorithm then operates on these inputs and selects necessary processing elements and creates pipeline stages for task execution. It perform s co-synthesis by adding processing elements in the system in an iterative manner. M ain phases o f the algorithm include processing element selection, pipelined task allocation, topology mapping and scheduling. Processing elements which give maximum perform ance are added into the system. Tasks are then scheduled on the available processing elem ents based on their priority and pipeline stages are created when a task cannot be scheduled in the current pipeline stage. When timing constraints are met, the processing elem ents are mapped to a regular topology. Topology that has the least com m unication delay is selected. Finally, scheduling is performed to see if all the tasks still m eet the tim ing constraints. More processing elements are added in the system if tim ing constraints are violated.

D ifferent experim ents were conducted to demonstrate the efficacy o f the proposed algorithm . In the first experiment, M PEG encoder application has been used for co-

107

synthesis. Application was tested with a wide range of timing and area constraints and algorithm was able to generate pipelined schedules for each of the test case in a short span o f time. Other experiments were conducted on large size random graphs consisting of up to 400 tasks. The algorithm assumes coarse grained task graphs, therefore graph with 400 tasks represent a very large application. Algorithm was able to find good results for each of the test case in a very short time. Algorithm generated results that range from a single processing to tens of processing elements based on performance requirements and it explored the design space well. Different topologies were selected depending on the nature of inter-task communication.

Although proposed algorithm gives good results, it can be improved in a number o f ways. Processing elements are currently selected depending on the performance improvement and corresponding area cost. Tradeoff between these conflicting requirements is done using a constant area-performance tradeoff factor. One o f the enhancements in this approach could be to dynamically change this factor by deriving it through some system characteristics or performance/area requirements.

Co-synthesis technique presently requires acyclic data flow graphs. Many applications require previous iteration data in tasks that lead to the same data for the current iteration. Modeling such applications with data-flow graphs require cyclic graphs which have feedback edges. The method could be enhanced to handle these applications by using a special edge such that the successor task should not wait for predecessor task to complete

108

its execution. D ata required through these edges is produced in previous iterations and is therefore alw ays available for the current iteration.

Presently, non-functional requirements are specified only as area and pipeline period constraints. M ore non-functional requirements can be added. Reliability is an important m etric for fault-tolerant systems. System reliability can be modeled and the cost function o f the algorithm can be m odified to include reliability as another constraint. Similarly, system pow er can also be used as another non-functional requirement.

Finally, another direction for future work could be to substitute reconfigurable logic devices in place o f hardware blocks for co-synthesis. If configuration time o f such devices happen to be very small compared to the task execution time, then same device can be reconfigured to perform a different task instead o f using other processing elements. This can lead to substantial savings in the cost o f the target system.

109

REFERENCES
[1] V. Madisetti, "Rapid digital system prototyping: current practice, future challenges," IEEE Design and Test of Computers, vol. 13, no. 3, pp. 12-22, August 1996. [2] G.D. Micheli, "Computer-aided hardware software codesign," in IEEE Micro, vol. 14, no. 4, pp. 11-16, August 1994. [3] W. Wolf, "A decade of hardware/software codesign," IEEE Computer vol. 36, no. 4, pp. 38-43, April 2003. [4] D.E. Thomas, J.K. Adams, H. Schmit, "A model and methodology for hardwaresoftware codesign," IEEE Design and Test of Computers, vol. 10, no. 3, pp. 6-15, September 1993. [5] S. Kumar, J. Aylor, B. Johnson, W. Wulf, "A framework for hardware/software codesign," IEEE Computer, vol. 26, no. 12, pp. 39-45, December 1993. [6] M. Chiodo, P. Guisto, A. Jurecska, H. C. Hsieh, A.S. Vincentelli, L. Lavagno, "Hardware software codesgin of embedded systems," IEEE Micro, vol. 14, no. 4, pp. 26-36, August 1994. [7] J. Henkel, Th. Benner, R. Ernst,W. Ye, N. Serafimov and G.Glawe, "COSYMA: A software-oriented approach to hardware/software codesign," The Journal of Computer and Software Engineering, vol. 2, no. 3, pp. 293-314, 1994. [8] R. Ernst, J. Henkel, T. Benner, "Hardware/software cosynthesis for microcontrollers," IEEE Design and Test of Computers, vol. 10, no. 4, pp. 64-75, December 1993. [9] R.K. Gupta and G.D. Micheli, "Hardware-software cosynthesis for digital systems," IEEE Design and Test of Computers, vol. 10, no. 3, pp. 29-41, September 1993. [10] http://www.systemc.org [11] T. C. Hu, "Parallel sequencing and assembly line problems," Operations Research, vol. 9, no. 6, pp. 841-848, 1961. [12] G. DeMicheli, "Synthesis and Optimization of Digital Circuits," McGraw-Hill, 1994. [13] P. Paulin, J. Knight, "Force-directed scheduling for the behavioral synthesis of ASIC s, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 8, no. 6, pp. 661-679, June 1989.

110

[14]

R. Cam posano, "Path based scheduling for synthesis," IEEE Transactions on C om puter-A ided Design o f Integrated Circuits and Systems, vol. 10, no. 1, pp. 8593, January 1991. R. A. B ergam aschi, S. Raje, I. Nair, L. Trevillyan, "Control-flow versus data-flowbased scheduling: com bining both approaches in an adaptive scheduling system," IEEE Transactions on VLSI Systems, vol. 5, no. 1, pp. 82-100, M arch 1997. R. Ernst, "Codesign o f embedded systems: status and trends," IEEE Design and Test o f Com puters, vol. 15, no. 2, pp. 45-54, April 1998. J. H enkel, R. Ernst, "An approach to automated hardware/software partitioning using a flexible granularity that is driven by high-level estimation techniques," IEEE Transactions on VLSI Systems, vol. 9, no. 2, pp. 273-289 , April 2001. K. M elhom , "Graph Algorithms and NP-Com pleteness," New York: SpringerV erlag, 1977. G. N em hauser, L.W olsey, "Integer and Combinatorial Optim ization," Wiley, New York, 1988. S. Prakash and A.C. Parker, "SOS: Synthesis o f application specific heterogeneous m ultiprocessor system s," Journal o f Parallel and Distributed Computing, vol. 16, pp. 338-351, D ecem ber 1992. L.J. H afer and E. Hutchings, "Bringing up Bozo," Technical Report (CM TR TR02), School o f Computing Science, Simon Fraser University, BC, Canada. March 1990. K. S. C hatha and R. Vemuri, "Hardware-software partitioning and pipelined scheduling o f transform ative applications," IEEE Transactions on VLSI Systems, vol. 10, no. 3, pp. 193-208, June 2002. A viral Shrivastava, M ohit Kumar, Sanjiv Kapoor, Shashi Kumar, M. Balakrishnan, "Optimal hardware/software partitioning for concurrent specification using dynamic program m ing," Proceedings o f International C onference on VLSI Design, Calcutta, India, pp. 110-113, January 2000. Jui-M ing Chang, M assoud Pedram, "Codex-dp: Co-Design o f communicating system s using dynam ic program m ing," IEEE Transactions on Computer-Aided D esign o f Integrated Circuits and Systems, vol. 19, no. 7, pp. 732-744 , July 2000. P. V. K nudsen, J. Madsen, "PACE: A dynamic programming algorithm for hardw are/softw are partitioning," Proceedings o f 4th International W orkshop on H ardw are/Softw are Codesign, Pittsburgh, PA, USA, pp. 85-92, 1996. S. K irkpatrick, C D. Gelatt, M.P. Vecchi, "Optimization by simulated annealing," Science, vol. 220, no. 4598, pp 671-680, M ay 1983. F. Glover, E. Taillard, D. de Werra, "A user's guide to tabu search," Annals o f O perations Research, vol. 41, no. 0, pp. 3-28, 1993.

[15]

[16] [17]

[18] [19] [20]

[21]

[22]

[23]

[24]

[25]

[26] [27]

Ill

[28] P. Eles, Z. Peng, K. Kuchcinski, and A. Doboli, "System level hardware/software partitioning based on simulated annealing and tabu search, Design Automation for Embedded Systems, vol. 2, no. 1, pp. 5-32, January 1997. [29] R. Dick, N. Jha, "Mogac; a multi objective genetic algorithm for hardware-software cosynthesis o f distributed embedded systems," IEEE Transactions on ComputerAided Design o f Integrated Circuits and Systems, vol. 17, no. 10, pp. 920-935, October 1998. [30] T. Wiangtong, P. Cheung, and W. Luk, "Comparing three heuristic search methods for functional partitioning in hardware-software codesign," Journal of Design Automation for Embedded Systems, vol. 6, pp. 425-449, 2002. [31] J. Henkel and R. Ernst, "A path-based technique for estimating hardware runtime in HW/SW-cosynthesis," Proceedings of International Symposium on System Synthesis, Cannes, France, pp. 116-121, 1995. [32] J. Henkel and R. Ernst, "High level estimation techniques for usage in hardware/software co-design," Proceedings of Asia South Pacific Design Automation Conference (ASPDAC '98), Yokohama, Japan, pp. 353-360, 1998. [33] F. Vahid and D.D. Gajski, "Incremental hardware estimation during hardware/software functional partitioning," IEEE Transactions on VLSI Systems, vol. 3, no. 3, pp. 459-464, September 1995. [34] C-Y. Park and A.C. Shaw, "Experiments with a program timing tool based on a source-level timing scheme," Computer, vol. 24, no. 5, pp. 48-57, M ay 1991. [35] Y-T. Li, S. Malik, and A. Wolfe, "Performance estimation of embedded software with instruction cache modeling," ACM Transactions on Design Automation of Electronic Systems, vol. 4, no. 3, pp. 380-387, July 1999. [36] W. Ye, R. Ernst, T. Benner, and J. Henkel, "Fast timing analysis for hardwaresoftware co-synthesis," Proceedings IEEE International Conference on Computer Design (ICCD '93), Cambridge, MA, USA, pp. 452-457, 1993. [37] J. Buck, S. Ha, E. A. Lee, D.G. Messerschmitt, "Ptolemy; A framework for simulating and prototyping heterogeneous systems," International Journal of Computer Simulation, vol. 4, pp. 155-182, April 1994. [38] L.A. Cortes, P. Eles, Z. Peng, "Verification of embedded systems using a Petri net based representation," Proceedings of 13th International Symposium on System Synthesis, Madrid, Spain, pp. 149-155, September 2000. [39] A. Ghosh, M. Bershteyn, R. Casley, C. Chien, A. Jain, M. Lipsie, D. Tarrodaychik, 0 . Yamamo, "A hardware-software co-simulator for embedded system design and debugging, ' Proceedings of Asia South Pacific Design Automation Conference (ASPDAC '95), Makuhari, Japan, pp. 155-164, September 1995. [40] P. A. Hsiung, "Hardware-software timing co-verification of concurrent embedded real-time systems," lEE Proceedings Computers and Digital Techniques, vol. 147, no. 2, pp. 83-92, March 2000.

112

[41]

W. W olf, A n architectural co-synthesis algorithm for distributed, embedded com puting system s," IEEE Transactions on VLSI Systems, vol. 5, no.2, pp 218229, June 1997. > » hf K. K onstantinides, R. Kaneshiro, and J. Tani, "Task allocation and scheduling m odels for m ulti-processor digital signal processing," IEEE Transactions on A coustics, Speech, Signal Processing, vol. 38, no. 12 pp. 2151-2161, December 1990. G. C. Sih, E.A. Lee, "A compile-time scheduling heuristic for interconnectionconstrained heterogeneous processor architectures," IEEE Transactions on Parallel D istributed Systems, vol. 4, no. 2, pp. 175-187, February 1993. S. Bakshi, D.D. Gajski, "Paritioning and pipelining for performance-constrained hardw are/softw are system s," IEEE Transactions on VLSI Systems, vol. 7, no. 4, pp. 419-432, Dec. 1997. A. K alavade, E.A. Lee, "The Extended Partitioning Problem; Hardware/Software M apping, Scheduling and Implementation-bin Selection," Journal o f Design A utom ation o f Embedded Systems, vol. 2, no. 2, pp. 125-163, M arch 1997. J. Levm an, "Hardware software co-synthesis o f heterogeneous hypercube architectures for fault tolerant embedded system s," MASc. Thesis, Dept, o f Electrical and Com puter Engineering, Ryerson University, 2004. S. Y ajnik, S. Srinivasan, N. K. Jha, "TBFT: A Task Based Fault Tolerance Scheme for D istributed System s," Proceedings o f International Conference on Parallel and D istributed Com puting Systems, Las Vegas, NV, USA, pp. 483-489, October 1994. B. P. Dave, N. K. Jha, "COFTA: Hardware-Software Co-Synthesis o f H eterogeneous D istributed Embedded Systems for Low Overhead Fault Tolerance," IEEE Transactions on Computers, vol. 48, no. 4, pp. 417-441, April 1999. R alph Duncan, "A Survey o f Parallel Com puter Architectures," Computer, vol. 23, no.2, pp. 5-16, Feb. 1990. A.M . D espain, D. A. Patterson, "X-Tree: A tree structured multi-processor com puter architecture," Proceedings o f the 5th annual symposium on Computer architecture, Palo Alto, CA, USA, pp. 144-151, April 1978. T. Y en, W. W olf, "Performance Estimation for Real-Tim e Distributed Embedded System's," IEEE Transactions on Parallel and Distributed Systems, vol. 9 no, I I , pp. 112 5 -1136, N ovem ber 1998. D idier Le Gall, "M PEG: A video compression standard for multim edia applications," Com m unications o f the ACM, vol. 34, no. 4, pp. 46-58, April 1991.

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49] [50]

[51]

[52]

113

[53] Usman Ahmed, Gul N. Khan, "A new processor allocation and pipelining approach for hardware software co-synthesis," Proceedings o f the 18*'' annual Canadian Conference on Electrical and Computer Engineering (CCECE'05), Saskatoon, SK, Canada, May 2005.

114

