Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

Brian D. Cameron
Ryerson University

digital.library.ryerson.ca/object/30

Please Cite: Cameron, B. D. (2005). Trends in the usage of ISI bibliometric data: Uses, abuses, and implications. portal: Libraries and the Academy, 5(1), 105-125. doi:10.1353/pla.2005.0003

library.ryerson.ca

Brian D. Cameron

105

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications
Brian D. Cameron

abstract: Librarians rely on the Institute for Scientific Information's journal impact factor as a tool for selecting periodicals, primarily in scientific disciplines. A current trend is to use this data as a means for evaluating the performance of departments, institutions, and even researchers in academic institutions--a process that is now being tied to tenure and promotion--despite the fact that such usage can be misleading and prejudicial. This paper will highlight the history of the development of impact factors, describe the limitations in their use, and provide a critique of the usage of impact factors in academic settings.

Introduction
nstitute for Scientific Information (ISI) citation data from the Science Citation Index, Social Sciences Citation Index, Arts and Humanities Citation Index, and the Journal Citation Reports has received widespread application, far beyond its original and intended use. While the limitations and biases of this data have been discussed in a growing body of literature, many librarians continue to rely on this data during the process of journal selection and deselection. Given the expanding use in academia and librarianship, it should not be too surprising that publishers and editors have demonstrated a growing interest in impact factors. There is now clear evidence that impact factors can be (and are being) manipulated by editors and publishers. In the new environment of the "big deal" or bundled journal packages and consortial purchases, this manipulation can be a serious problem. Perhaps even more troubling is the emerging trend to use citation data, particularly impact factors, as a performance measure by which scientists and faculty are ranked, promoted, and funded. Such ranking has expanded to departments, laboratories, universities, and even countries.

I

portal: Libraries and the Academy, Vol. 5, No. 1 (2005), pp. 105­125. Copyright © 2005 by The Johns Hopkins University Press, Baltimore, MD 21218.

106

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

The development of citation indexes and impact factors, the manner in which this data has been used in the past and present, and the implications of emerging uses are discussed. The limitations of ISI data are also presented. Despite the many problems of this bibliometric tool, it is unlikely that the most well known mechanism for addressing journal quality will be abandoned. However, it is clear that researchers, librarians, editors, publishers, and other stakeholders need a clearer understanding of ISI data if they are to use it in a more sophisticated and critical way.

Brief History of Citation Indexes and Impact Factors
In 1955, Eugene Garfield, the founder of the Institute for Scientific Information--originally called Eugene Garfield and Associates--proposed a "citation index that offered a new approach to subject control of the literature of science"; and he argued that this "association-of-ideas index" would help to bridge the gap of the subject approach to indexing.1 Garfield recognized that citation indexes offered several advantages over subject indexes. Citation indexes can assist searchers in identifying relationships among articles more easily than can be accomplished with subject indexes--"the compilation of citation indexes is . . . well suited to man-machine indexing methods that do not require indexers who are subject specialists"; and since citations are simply bibliographic descriptions, they will not become obsolete, as can happen to terms in subject indexes.2 Even at this early stage, Garfield may have been looking ahead to a unified index to scientific literature that would be "current, free of semantic difficulties, and not dependent on the subject knowledge of the indexers."3 Such an index, in Garfield's mind, could become the single point of departure for all literature searches and an indispensable research tool. More immediately, citation indexes were seen as a means of dealing with the publishing explosion. The rapid growth of scientific, technical, and medical literature after World War II resulted in some indexing delays lasting six months to several years. Therefore it took researchers much longer to locate relevant papers. This was especially true as fields became more interdisciplinary and many subject indexes restricted coverage to a single field. Garfield was aware that a citation index had been in place for legal literature since 1873 when Frank Shepard began the process of listing each time Illinois Supreme Court cases were cited or affected by a later case.4 This system became so important that Shepard's name became a verb, and case verification is generally referred to as Shepardizing. Since then, this approach has gained wide acceptance and is the basis of Google's page-rank system, which determines the value of a page based in part on the number of pages that link to it. Despite Shepard and other earlier precedents, Garfield's idea was revolutionary; primarily because he envisioned the use of computers in the process of index compilation, and he recognized that the importance--or impact--of published research could be measured through a citation index. As such, he envisioned an index that would include articles and all of the articles referred to by the article. "The system," he wrote, "would provide a complete listing, for the publications covered, of all the original articles that had referred to the article in question."5

Brian D. Cameron

107

Garfield recognized the utility of a citation index in historical research, such as "when one is trying to evaluate the significance of a particular work and its impact on the literature and thinking of the period."6 Research in the history of science would also benefit from the creation of a citation index. Such an index would make it possible to study interaction among fields and "an historical account of development of scientific thought can be constructed."7 An example is the tracing of the history of the development of DNA modeling.

Librarians and Impact Measurement
Ranking of scientific journals has been a preoccupation of the library profession for some time. One of the first reported attempts by librarians to quantify the importance of published scientific research was conducted by P. L. K. Gross and E. M. Gross and published in 1927. These researchers conducted a study of citation patterns in order to determine the most important journals in chemistry by simply counting the number of citations in the Journal of the American Chemical Society and then ranking these journals in order of frequency of citation.8 Many librarians have subsequently employed this method, expanded to include citation counts from a broad range of journals, to improve collections. A further published example is the research conducted by Herman H. Fussler to rank journals in physics and chemistry in the United States.9 In an article published in the Bulletin of the Medical Library Association in 1944, Estelle Brodman criticized the method employed by Gross and Gross as unscientific and unscholarly by challenging its underlying assumptions. These assumptions were that the utility of a periodical to a professional is in "direct proportion to the number of times it is cited," that the periodicals used as the "base for tabulation are representative of the entire field," and that if several journals are "used as a base, all of them can be weighted equally."10 She found no correlation and concluded that the method was unsound. In challenging these assumptions, Brodman asked members of the Department of Physiology of the College of Physicians and Surgeons at Columbia University to list the most valuable journals in the field of physiology. She then used a Spearman rank-difference formula to examine differences in these lists and conducted a manual citation count in three physiology journals. She was able, finally, to conclude that all three of these assumptions were indeed false.11 There are many other examples of attempts to quantify the value of published research, some of which offer the same conclusion as Brodman. Even though Garfield was aware of these criticisms, he persisted with his plan to construct a citation database. Much later he argued, "While there are problems with taking citations at face value (e.g., self-citation, negative citation, window dressing, and politically motivated flattery) there is a strong correlation between citation rates and peer judgments that holds for many disciplines.12 Garfield added that no better tool exists, and that "the use of [impact factor] as a measure of quality is widespread, because it fits well with the opinion we have in each field of the best journals in our specialty."13 Nevertheless, Garfield has referred to the development of the impact factor as a "mixed blessing."14

108

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

What is an Impact Factor?
The impact factor emerged from Garfield's work with citation indexes. He realized that he required a means for determining which journals should be included in the index. In developing the impact factor, Garfield and his colleague Irving Sher recognized that they required a simple and fair means of comparing large journals with small journals.15 Ideally, this method of calculation would allow frequently issued journals to be compared with quarterly or annual publications and to compare older journals with newer ones. As we shall see, many critics argue that Garfield's impact factor has not managed to compensate for these discrepancies. The Institute for Scientific Information's (ISI) journal impact factor is the ratio of articles published to articles cited during a rolling two-year window. ISI calculates these numbers from its citation indexes and publishes the data annually in Journal Citation Reports. For example, to calculate a journal's impact factor for the year 2000, the following formula is used: year 2000 citations to 1998 + 1999 articles articles published in 1998 + 1999 Example: Citations in 2000 to articles published in: 1998 = 22,150 1999 = 21,175 1998 + 1999 = 43,325 Number of articles published in: 1998 = 1,100 1999 = 975 1998 + 1999 = 2,075 Citations to articles Number articles published = = 43,325 2,075

=

20.878

The publication types included in the numerator and the denominator differ. The denominator includes citable--or source--resources, including articles, notes, and reviews. The numerator, on the other hand, contains ciThe benefits of using impact factors as a means tations to all document types, including letters of scholarly evaluation appear to be few. It is and meeting abstracts. considered to be an inadequate measure, and This arrangement has come to be regarded as even proponents point out its weaknesses. a serious flaw, because those journals with a vibrant correspondence section, for example, can realize much higher impact factors.

Brian D. Cameron

109

The Deficiencies of Impact Factors
Impact factors are now a well-established and best-known measure of a journal's relative importance. The benefits of using impact factors as a means of scholarly evaluation appear to be few. It is considered to be an inadequate measure, and even proponents point out its weaknesses. The deficiencies of ISI's journal impact factors have been widely reported in the scientific, medical, and technical literature, but largely ignored despite the fact that improvements and alternatives to impact factors have been suggested. The following section details the major criticisms.

The Two-Year Window
Impact factors are calculated by using citation data from the previous two years. In establishing the impact factor, Garfield examined the chronological distribution of citations, especially in biochemistry and molecular biology and noticed that approximately 25 percent of citations were to articles published very recently. "That two-year impact factor," Garfield wrote, "has proven to be a remarkable predictor of future performance but is certainly not flawless."16 The fact that Garfield concentrated on biochemistry and molecular biology has had major repercussions and has disadvantaged many low-impact disciplines and those journals with long publications lags. It is obvious that authors will cite articles published in the same journal, so faster publication will result in higher impact factors.17 Subject area, then, has an enormous effect on impact factor. Those disciplines with faster publication cycles tend to be more dynamic. In medicine, many of the pre-clinical and traditional clinical sciences are low-profile and low-impact disciplines. Anatomy is a prime example of a discipline that is disadvantaged by impact factors. Impact factors favor journals that publish research in disciplines in which ideas turn over quickly or where knowledge is added to frequently. In other words, molecular biology and molecular genetics benefit, while anatomy, histology, ecology, orthopedics, dermatology, physiology, and mathematics, among others are disadvantaged. In these disciplines, few references are made in the two-year citation window. In anatomy, it can take 50 years or longer for ideas to be challenged or overturned. Taxonomy papers are routinely cited a century after publication.18 Per O. Seglen has argued that because citation habits and dynamics are so dissimilar among research areas, it is virtually impossible to make supportable comparisons based on citation rates and journal impact.19 For example, biochemistry and molecular biology articles are typically cited 500 percent more than articles in pharmacy. The average of citations per article varies greatly; and some areas, especially in the arts and humanities, can be considered to be uncited.20

Number of Journals Published in a Discipline
Another issue regarding subject area is the number of journals published in a specific discipline. It has been reported that the greater the number of journals published in a subject area, the greater the overall impact factor for the discipline.21 This finding helps to explain why some areas, such as immunology and anatomy, are considered to be

110

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

low-impact disciplines. Low-impact disciplines tend to be low-publication disciplines with fewer journals. Publication output, then, is dependent on the discipline.22

ISI Coverage
ISI covers only a segment of the journals published in each discipline. Since the original purpose of impact factors was to assist The Institute for Scientific Information in selecting journals for coverage in Current Contents and the Science Citation Index, the coverage of journals could not be comprehensive. In 1997, it was estimated that the Science Citation Index covered a mere 2.5 percent of the world's scientific journals. In addition, the index excludes books as a source for citations. ISI argues that it indexes in the Web of Science those journals that are known to be of high quality and that have influence and impact. By this measure, new journals would appear to have faint hope of being included. In mathematics, many of the most highly cited publications are not even included in the ISI database.23

Language
The built-in bias of ISI's selection means that many foreign language journals are excluded. English language journals, as a result, have much higher impact factors.24 The result is a citation database weighted heavily in favor of English language American journals.25 The other outcome is that many researchers in non-English speaking nations choose to publish in foreign journals, even if the journal is unavailable in his/her country, because it will ensure a greater amount of prestige.26 The Canadian Medical Association Journal has conceded that many Canadians "continue to send papers to foreign journals for publication."27 Some departmental chairs encourage faculty to submit to international, English language journals, to the disadvantage of local journals.28 Another tactic is to seek international collaboration.29 Perhaps not surprisingly, editors of non-English language journals tend to favor manuscripts from their own country.30

Are Impact Factors Representative?
Among the primary criticisms of impact factors is that they can only measure average article citation rates. In other words, it indicates how many times, on average, a typical article is cited in a specific journal. It does not tell us if specific articles have been cited. Given this conclusion, one cannot assume that an article published in a high-impact journal is, by definition, an influential article or even widely cited. Seglen, in his seminal article, "Why the Impact Factor of Journals Should Not Be Used for Evaluating Research," addressed this issue and effectively argued that impact factors are misleading and an inappropriate measure of scientific impact. Seglen concluded that the distribution of citations is extremely skewed. He argued, "The most cited 15 percent of the articles account for 50 percent of the citations, and the most cited 50 percent of the articles account for 90 percent of the citations. In other words, the most cited half of the articles are cited, on average, 10 times as often as the least cited half."31 While Seglen was able to conclude that the journal's impact factor is a "characteristic journal property that stays relatively constant over time," he suggested that the "citedness of the individual articles form an extremely skewed distribution, regardless

Brian D. Cameron

111

of journal impact."32 His conclusion that the impact factor is not representative of the individual article and, therefore, "cannot be used as a proxy measure of article citedness" has been largely ignored, despite that fact that Eugene Garfield offered the same conclusion years earlier.33 Seglen added, "Articles from individual authors form equally skewed distributions, resulting in poor correlation between article citedness and journal impact, even at the author level."34

Publication Type
Criticisms of journal impact factors run deeper. In many cases, the original citation is lost. Clifford Saper has observed that some research has made such an indelible impact that "it is immediately incorporated into the conceptual basis of a field, and its origin is quickly lost in the mists of time."35 Similarly, journals that specialize in review articles (articles that publish summaries of past research) have much higher impact factors, because they function as surrogates for previously published research.36 As an example, six of the top 10 neuroscience journals publish exclusively review articles. It has been observed that a large percentage of articles published in half of the 10 leading biomedical journals, ranked by impact factor, are review articles.37 More to the point, a full 60 percent of the top 25 journals are review journals. As a result, these journals have attained a very high impact factor, because they are heavily cited.

Journal Size
Despite Garfield's claim that the number of articles a journal publishes has no influence on impact factors--because the impact factor is expressed as a ratio--it has been shown that those journals that publish a large number of articles have a consistently higher impact factor, simply because the journal receives more citations over time. In addition, there are fewer fluctuations in journal impact over time.

Other Criticisms
Among the most radical protests against the use of impact factors for performance evaluation is that voiced by Mike Sosteric in his paper "Endowing Mediocrity: Neoliberalism, Information Technology, and the Decline of Radical Pedagogy." He argues that performance evaluation based on bibliometric data is Orwellian and will lead to the "limitation of scholarly discourse and the creation of a one-dimensional intellectual edifice resistant to the development of critical discourse and radical pedagogy."38 Furthermore, he argues that it has more to do with justifying various forms of inequality in the academy.39 Women may well be cited less frequently than men, something that has proven to be case in anthropology. In sociology, it has been argued that challenges to existing theoretical orientations are likely to be ignored and, therefore, not cited. In other disciplines, substantive contributions that challenge scholarly communication may suffer the same fate. It has also been argued that academic advancement is more rapid in high- impact disciplines. Bibliometric evaluation of published articles tends to ignore the usage they receive in advancing general knowledge and professional application. For example, in clinical medicine, articles are read to improve diagnoses and save lives. Some of these articles

112

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

may never be cited or will receive a small fraction of the number of citations a paper in higher-impact disciplines might receive. In general, three arguments have been proposed to remedy the problems of impact factor: improve the instrument, abandon it entirely, or use the data more cautiously and critically. Because impact factors have already gained wideBibliometric evaluation of published spread use and attention, these voices of dissent have not been articles tends to ignore the usage they fully heard. Instead, the usage receive in advancing general knowledge of ISI data has rapidly expanded to other areas, perhaps and professional application. most notably in the assessment of faculty members for promotion and tenure. This trend toward performance-based funding has coincided with a recent growth in the number of journals being published. Examples of ISI data being used as a performance measure are common.

Usage of Impact Factors in Academia
As previously mentioned, the initial purpose of impact factors was to assist The Institute for Scientific Information in selecting journals for coverage in Current Contents and the Science Citation Index. Soon after the development of impact factors, Garfield promoted their use as a selection tool for librarians;40 and librarians--especially those in the scientific, technical, and medical disciplines--quickly began using impact factors for selection and deselection of journals once Journal Citation Reports commenced publication. Researchers are currently using this data in order to weigh the importance of individual journals and, by extension, published research--including their own. Many faculty and scientists use this data in a thoughtful way by examining citations to their publications, rather than simply reporting on the impact factor of the journals in which they have published. Nevertheless, a current trend is to use this quantitative data as a means for evaluating the performance of researchers in academic institutions, which can then be tied to promotion. In some cases, impact factors have been utilized to assess the performance of entire departments and institutions. Funding decisions have been based on the perceived impact of research areas. Another important use includes the evaluation of competing journals by editors in order to market journals and target advertisers. Impact factors were not designed for these purposes, and despite warnings that this use is improper, the practice continues and is spreading. Garfield has issued repeated warnings that journal impact factors are an inappropriate and misleading measure of individual research, especially if used for tenure and promotion.41 Garfield referred to such use as a surrogate for proper citation analysis.42 Instead of conducting a proper cited-reference search, many faculties and researchers look only to the journal impact factors, partly because it is far easier than reading and evaluating the candidate's papers. As has been widely stated, it is misleading to assess the value of the article based solely on the journal in which it has been published. Publication, in other words, in a high-impact factor journal does not mean that an article

Brian D. Cameron

113

will be highly cited, influential, or high quality. Garfield has been inconsistent in Publication, in other words, in a his warnings, however. He argued that "impact factor may be much more in- high-impact factor journal does not dicative than an absolute count of the mean that an article will be highly number of a scientist's publications."43 A further observation is more interest- cited, influential, or high quality. ing: "Using citation analysis for evaluating people is a very tricky business. . . . But if you use the data carefully, you can facilitate an intelligence gathering process than can allow for reasoned and thoughtful decisions."44 Garfield also argued that perhaps the most important and recent use of impact factors is in the process of academic evaluation and even promoted citation analysis as a means for predicting Nobel Prize winners.45 By 1983, ISI began promoting impact factors and citation analysis as means for evaluating scientific personnel.46 We are left in a situation where impact factors are now routinely used to evaluate scientists, departments, entire institutions, and even nations. This use has become so commonplace that some of the literature of bibliometrics assumes a use for impact factors that was never intended. For example, in an article in the Annual Review of Information Science and Technology, Christine Borgman and Jonathan Furner state that impact factors "were developed to assess the influence of a journal, an author, a laboratory, a university, or a country."47

Performance-Based Funding, Tenure, and Promotion
The use of impact factors and citation analysis for research assessment are becoming more widespread. The publish-or-perish culture has coincided with the growth in the number of journals being published. A trend in many countries has been to move toward a performance-based funding model for research, using impact factors as a means of assessment. Such usage is beyond the capabilities of the measure; therefore, the problems associated with it have been magnified. A few examples should demonstrate the extent of the use. In the United Kingdom, the Research Assessment Exercise is rating the quality of research projects in all university departments and every research institute. It will determine up to 30 percent of the funding of individual universities for a five-year period. It is anticipated that in the future this exercise will be automated and use, among some other indicators, ISI's impact factor.48 Some university administrators take the number of publications, multiply by the impact factor of the journal, and use this as a criterion for promotion. Research funding may be calculated based on the impact factor of the journal in which the applicant has published. Some universities in Great Britain have an expectation that biologists will publish in journals that have an impact factor of five or greater.49 P.J. Ell recently reported that administrators at the Institute of Nuclear Physics at University College London asked that impact factors for each academic staff member be reported.50 Clearly, these policies could affect promotion and tenure decisions.

114

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

In Spain, a university determines promotion by multiplying the number of articles published with the impact factor.51 In Italy, impact factor is advocated as a means to remedy "purported subjectivity and bias in appointments to higher academic positions."52 Charles Jennings has reported the following other examples. The Italian Association for Cancer Research compels grant applicants to calculate the impact factor for each journal in which they have published for the preceding five years. Appointment committees at Japanese universities are "often heavily influenced by journal impact factors."53 A 1990 study reported that approximately 35 percent of biochemistry departments and close to 60 percent of sociology departments queried had used citation data for hiring, as well as for tenure, promotion, and salary.54 By 1998, it was estimated that 5,000 departments in American universities had been evaluated with citation data.55 The annual operating grant allocations to Australian universities are calculated by each university's publication output. A composite index for distributing operating grants is calculated based on the publication output and grants. In response, the Australian Research Council conducted a study to determine the quality and impact of research publications in order to weight the impact of journals using ISI data.56 Other uses of impact factors have been suggested. In a letter to the editor of a recent issue of Nature, a proposal to make peer review more attractive to potential reviewers was offered. The writer argued that the impact factor of the publications reviewed could be used in performance assessment. Funding could then be associated with the institution's peer-review score.57 Adding to this trend is serious research undertaken to improve journal rankings. In 2002, an article in IEEE Transactions on Engineering Management reported on an integrated subjective and objective approach to measuring quality. The authors attempted to construct a formula that would take into consideration expert opinion (prestige or reputation) and citation analysis using impact factors in the Journal Citation Reports. It is interesting to note that this study neglected to consider library usage. A Web-based decision support system was devised to demonstrate the feasibility and effectiveness of the application.58

Ranking University Departments and Countries
There are many examples of university departments and countries being ranked by impact factors or citation analysis. In fact, Eugene Garfield lectured at the University of Toronto in April 1993 on the topic "What Citations Tell Us about Canadian Research." In his talk, Garfield calculated an impact factor for Canadian science. He did so by aggregating publication and citation data over five 10-year periods. Garfield then compared this number with other G7 countries over a 10-year period. He then ranked the top 20 nations by impact. Canada finished tenth. Garfield also ranked institutions, authors (by institution), most cited authors, and authors with greatest impact, most cited articles, and "hot" articles. In conclusion, Garfield stated, "While citation data create new tools for analyses of research performance, it should be stressed that they supplement rather than replace other quantitative--and qualitative--indicators. And like other indicators, the appro-

Brian D. Cameron

115

priate and balanced interpretation of citation data requires the input of information specialists working with knowledgeable scientists."59 Despite his repeated warning of this type of use, he has actively promoted it--so much so that ISI ranks countries by citation output on such Web sites as ISIHighlyCited.com and in-cites.com.

Ranking Nations
There is now a body of literature devoted to the study of national science rankings. A recent example is "Ranking of Nations and Heightened Competition in Matthew Core Journals." The authors refer to scientific publication as the Olympic Games of science. They describe right journals and left journals--also referred to as high quality and low quality, winning journals and losing journals, journals on the right or wrong track, and even right world and wrong world journals. The authors write, "Those competing are scientists, scientific institutions, and countries in science."60 Their argument is, "`Not the rich are becoming richer and the poor poorer,' but those who are most efficiently competing, irrespective of the amount of talent entrusted to them, will reach the kingdom of heaven (i.e., science)."61 According to this article, the United States wins. It also finishes first in ISI citation counts. Those nations in the wrong world include the People's Republic of China, Turkey, South Korea, India, Mexico, Brazil, Argentina, Egypt, Bulgaria, and Venezuela, among others. Those on the right are the United States, United Kingdom, Denmark, Switzerland, New Zealand, Germany, among others. In other words, Western industrial countries fare far In many nations, national scientific better than developing nations.62 policy and funding through granting The authors make a fundamental error by arguing that the impact agencies may look to impact factors factor "tells how many citations an for estimates of whom and what author can expect for his paper."63 We institutions are productive and serve know that has been proven to be incorrect. More alarming is the conclu- the national interest of science. sion that a lack of ability to compete in global science equates to a waste of national resources. In many nations, national scientific policy and funding through granting agencies may look to impact factors for estimates of whom and what institutions are productive and serve the national interest of science. If the entire nation is seen as unproductive, cuts in funding may be the result.

Scholarly Communication
Some have speculated on the possibility of shifts in scholarly discourse that may have surprising effects on higher education. Raymon Coleman has made interesting comments on the future of anatomy departments. He argues that it is not possible for classical anatomists to publish in the highest impact publications. As a direct result, anatomists may be denied consideration for or advancement in faculties. He wrote, "This

116

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

trend will have a major effect on the future of anatomy teaching if it persists. The classical anatomist is already a rare commodity, and the future of some anatomy departments is a major source of concern."64 What is perhaps more surprising is the fact that many anatomy departments have merged with high-impact departments, such as cell or molecular biology. As promotion continues to be made on the basis of impact, Coleman notes, "It is not uncommon for a PhD molecular neuroscientist, for example, to be appointed on the basis of research and journal impact factors and given responsibility for teaching basic neuroanatomy without ever having taken a course in the structure of the human brain."65 Similar concerns have been voiced with respect to other low-impact disciplines, such as physiology. The literature offers many examples of shifts in publication efforts. Scientists desire publication in high-impact journals, threatening lower-impact publications. It is widely assumed that publication of an article in a high-impact journal will make it more cited than if it were published in a low-impact journal, but this is not supported by any scientific evidence, as we have already seen. However, once the use of impact factors became established, many researchers opted to submit articles to high-impact journals, believing that this would lead to more citations. This trend has been exacerbated once it became established procedure, especially in Europe, by the practice of tying promotion to publication in high-impact journals. Seglen pointed out that "the increasing awareness of journal impact factors, and the possibility of their use in evaluation, is already changing scientists' publication behavior towards publishing in journals with maximum impact."66 Some veterinarians have considered developing the human angle of their research so as to publish in higher impact journals. In a letter to Nature, three veterinarians from the Institute of Animal Health made the following suggestion: "As veterinary researchers, we sometimes find ourselves searching for possible human angles in our work, so that we might publish in medical journals, which tend to have significantly higher impact factors than their veterinary counterparts."67 Or, even more telling, they speak of outright manipulation:
But we have spotted a simpler and more effective approach that will allow us to publish in appropriate places and still get high ratings. As an example, The Veterinary Record has an impact factor of about 1, based on approximately 600 citations and 600 papers published in 1995 and 1996. Our institute publishes about 300 papers in two years. Our director need only instruct us all to cite at least two papers from The Veterinary Record in every paper we publish from now on, however loose the connection, for the impact factor to quickly double. The Veterinary Record would move from being in the top 40% of journals to being in the top 15%. We could have an even greater impact on journals that publish fewer papers. For example, if our director applied this policy to Veterinary Research Communications, the impact factor of that journal would increase from less than 1 to more than 6, moving it into the top 3% of journals. If our institute teamed up with two or three others, we could rapidly create a competitor to Nature. Unethical, perhaps, but legal and very much in our interest.68

Brian D. Cameron

117

Manipulation of Impact Factors by For-Profit Publishers
Perhaps a more troubling issue is the manipulation of impact factors by publishers. For-profit publishers quickly recognized the importance of impact factors in market research, in such ways as selling advertising space.69 Garfield himself likened impact factors to Neilson ratings.70 Publishers' profit margins are affected by impact factors, and so they have a vested interest in improving their journals' impact factors. It has been reported that ISI frequently receives queries from editors who want to get a fuller understanding of the construction of impact factors so that they can manipulate them.71 One might argue that many editors are obsessed with impact factor. Routinely they report on the new impact factor and take great pains to establish that they are not in any way manipulating this. Editors can take several steps to artificially improve a journal's impact factor. Increasing the number of review articles and simply increasing the number of articles published in the journal can positively affect impact factor. In addition, serial publication of segments of research, multiple publication of the same research either in identical or modified form, publishing more informal items like letters, editorials, and discussion papers will increase the impact factor. An editor may require the inclusion of more references in every article published, assuming that their journal would benefit by receiving some of these citations. Requiring that a certain number of citations be made from articles previously published in their journal will increase the journal's impact factor. In addition, a journal may choose to unveil new paradigms, host controversies,72 or solicit papers from authors with a good citation history, something that Garfield feels many editors do instinctively.73 One tactic editors can take is to eliminate case reports. Short reports are excellent, because they focus attention on something that may need further development; but they are less likely to be cited, resulting in lower impact factor and thus reducing the journal's competitiveness among authors. Since case reports contain few citations, reducing the chance that the publishing journal will be cited and benefit from including the report, they may become a target for elimination. If case reports are removed, the implications for dissemination of scientific information are unknown. Case reports are the second segment in the publication sequence for intervention trials. It is conceivable, but not proven, that the elimination of case reports could have an impact on the development of clinical trials. There are many examples of publishers and editors intervening to increase impact factors. Among the most subtle are slight changes that might realize higher impact factors. For example, the editor of Clinical Radiology noted that its readers are interested in review articles, which "tend to increase impact factor; we must continue to deliver these." The editor announced that the number of case reports would be reduced: "We shall not eliminate case reports entirely, even though such a move would tend to increase the impact factor; however we do insist that case reports should be educational and we may suggest plans whereby we could learn even more from them; they then become more citable."74 The Canadian Medical Association Journal views a high impact factor as a means of attracting new authors.75 While it is easy to suggest that the focus should be on publishing the best scientific research subjected to critical peer review, so much is riding on impact factors that they become very difficult to ignore.

118

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

Some changes are more overt. Starting in the 1990s, several biology journals began to publish mini-review articles. As we know, review articles attract citations rapidly, and these publications realized a higher impact factor. In 1997, The Lancet, a very prestigious medical journal, made an interesting editorial adjustment. Many research articles are accompanied by a short editorial or comments, and these are sometimes combined with comments from the article. In 1997, after a 175-year history, The Lancet began citing the research article in these short columns, rather than including a reference in parenthesis, which is not a countable reference. This change increased The Lancet's impact factor by approximately 35 percent.76 Non-source items--such as letters, editorials, book reviews, among others--are not included in the denominator when the impact factor is calculated. But citations to these items are counted in the numerator. Some of these brief clinical studies or letters have gained status as citation classics. The trend is now to publish scientific correspondence, as Nature started to do in 1983. Together with its News and Views section, Nature has a large amount of citable non-source items. The ratio of source articles to non-source articles in Nature halved from 1977 to 1997, even though the total number of pages has remained unchanged. Incidentally, other journals have benefited from changes at ISI. For example, the journal FASEB benefited from a reclassification of its meeting reports as non-source. As a result, the impact factor for that journal increased from 0.24 to 18.3 in one year. In August 2000, the editor of AJP: Heart and Circulatory Physiology suggested ways to improve the journal's impact factor. In making his argument, the editor recognized that rank and tenure committees and department funding are based on impact factors. Although he stated that he was unwilling to focus entirely on competition with other journals, he felt that he could not ignore impact factors. The editor committed to the following plan of action. They would publish more short, focused review articles and "put more emphasis on translational reviews to expand our reader base into more traditional medical communities."77 The purpose, although twofold, was clear: "The nature of AJP journals is to publish complete works, while leaving room for breaking findings. This and the changes outlined above will enhance the quality of AJP--the purpose of such is evolution. The fallout will be an increase in our impact factor as well."78 One way for a journal to increase its impact factor is to add two self-citations in each of its source articles. Some evidence of this came from a letter Leukemia had sent to the authors of a paper submitted for publication. In 1997, BMJ published this letter, in which the editors of Leukemia asked the submitting authors to increase the number of references to papers published in Leukemia. The letter stated:
Manuscripts that have been published in Leukemia are too frequently ignored in the reference list of newly submitted manuscripts, even though they may be extremely relevant. As we all know, the scientific community can suffer from selective memory when giving credit to colleagues. While we have little power over other journals, we can at least start by giving you and others proper credit in Leukemia. We have noticed that you cite Leukemia [once in 42 references]. Consequently, we kindly ask you to add references of articles published in Leukemia to your present article.79

Brian D. Cameron

119

The editors denied the implication that the journal was attempting to manipulate its impact factor. Others have reported that some editors "are sending copies of articles published in their journals together with the review copy to the referees, asking them if it is possible to include those published articles in the reference list. It is of course a brilliant way of increasing the impact factor for that journal."80 Scientists, incidentally, can increase their own citation counts through self-serving citations and conspiratorial cross-referencing among authors, sometimes referred to as courtesy references.81 Some critics argue that "citation clubs" have formed. Anyone who has ever done a cited Anyone who has ever done a cited reference search will know that reference search will know that mistakes are common, suggesting that reference lists mistakes are common, suggesting are being copied from other papers. ISI has that reference lists are being admitted that it has no means to identify copied from other papers. and correct the citation errors in the data82 base. A recent study has proven that many references often go unread by citing authors. The argument is that citations are simply being copied. The study concluded that 22 to 23 percent of all references fall into this category.83 This alone could make impact factor and citation analysis meaningless.

Scholarly Publishing
The problems associated with inappropriate use of impact factors become even more troublesome when considered in the larger context of the crisis in scholarly publishing. Librarians making selection and deselection decisions based on journal impact factors should be aware that this data may not be a precise indicator of the journal's relative importance. In the era of consortial licensing agreements, we know that librarians are faced with the challenges associated with the so-called "big deal." Apart from the issue of ownership of acquisition decisions, it seems clear that a goal of the publisher is to establish itself as a brand, hopefully resulting in higher impact factors.

The Crisis in Scholarly Publishing
Librarians are aware that scholarly publishing has been in crisis for the past 15 years. We have been faced with spiraling subscription prices and reduced budgets. Statistics from ARL indicate that from 1986 to 2000, the unit cost of serials increased by 227 percent. In the same time period, the consumer price index increased by only 64 percent.84 Serial costs continue to increase by 7.7 percent per year, while budgets increase by only 6.7 percent per year.85 Many research libraries spent 300 percent more on serials in 2000 compared with 1986, and the number of titles purchased declined by 7 percent. Finally, the erosion of monograph purchases by libraries forced publishers to increase prices by as much as 6 percent over the same period.86

120

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications

Much has come out of this situation. SPARC, the Scholarly Publishing and Academic Resources Coalition, is one example. Organic Letters, a SPARC alternative journal, has measured its success partially on its impact factor, which increased by 10 percent in its second year of rating, and is now the second highest ranked primary journal in organic chemistry based on impact factor.87 The most important result, from the library perspective, is that this journal has had a moderating effect on price increases of competing journals, especially Tetrahedron Letters.88 The effort of library consortia to negotiate a best-price scenario is a strategy that seems to offer hope. The Canadian National Site Licensing Project is a good example. But while these journal bundles/big deals may help with costs--itself a debatable issue--they may well exacerbate the problems with impact factor. Librarians need to But while these journal bundles/ be aware of how impact factors can be affected by such deals. Consortial arrangebig deals may help with costs-- ments and bundled journal packages, which itself a debatable issue--they can remove some discretion from acquisitions librarians, can affect journal rankings. may well exacerbate the Jean-Claude Guedon has commented problems with impact factor. on this issue. He cited an example from OhioLINK. According to 1996 statistics from OhioLINK, 68.4 percent of all articles downloaded from its Electronic Journal Center were from Elsevier journals. Guedon estimates that Elsevier at that time only controlled 20 percent of the core scientific journals.89 Since then, Elsevier merged with Reed and acquired Academic Press and Harcourt, among many others.90 In one 18-month period, Reed Elsevier acquired 70 companies.91 In 2002, the Office of Fair Trade in the United Kingdom estimated that Reed Elsevier controlled 41 percent of the market for science and technology journals. The next largest publisher controlled a mere 5 percent.92 Clearly, Reed Elsevier and other major scientific publishers are earning large profits. In fact, Reed Elsevier's scientific publishing has become so lucrative that it disposed of holdings in the consumer publishing market. Reed Elsevier's operating margin, according to its annual reports, is not that far behind Microsoft's.93 The rate of download for Elsevier is increasing, and one would conclude that citations and impact factors are as well. In fact, it has been proven that electronic access to a journal increases its impact factor, partly because the journals are available 24 hours per day.94 Reed Elsevier's "Review of Operations and Financial Performance, Science and Medical," noted that page views in ScienceDirect increased by 67 percent from January to June 2001. And, Reed Elsevier reported profits driven by stronger subscription renewals.95

Conclusion
It is clear that some means of evaluating published research and publications is required. Judicious use of ISI bibliometric data offers some value, provided that the limitations of the ISI data are fully understood. However, the expansion of this use to other

Brian D. Cameron

121

areas, especially the evaluation of individual scientific performance, is more problematic. The implications for scholarly publishing and academia are many. Reliance on ISI data ignores the fact that research is read for many reasons. Impact factors only take into account one type of usage. It is not likely that the usage of ISI data will be abandoned. Recent initiatives to open scientific research and free it from for-profit publishers may be the best hope we have of easing our reliance on ISI data. Elements of this paper were presented at the 100th Annual Conference of the Ontario Library Association, Toronto, ON, January 30, 2003. Brian D. Cameron is a reference and systems librarian, Ryerson University Library, Toronto, ON, Canada; he may be contacted via e-mail at bcameron@ryerson.ca.

Notes
1. Eugene Garfield, "Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas," Science 122, 3159 (July 15, 1955): 108; reprinted in Essays of an Information Scientist (Philadelphia: Institute for Scientific Information Press, 1983), 6: 468, http://www.garfield.library.upenn.edu/essays/v6p468y1983.pdf (accessed October 15, 2004). Melvin Weinstock, "Citation Indexes," in Encyclopedia of Library and Information Science, ed. Allen Kent and Harold Lancour (New York: Marcel Dekker Inc., 1971), 5: 16; reprinted in Essays of an Information Scientist, 6: 548­72, http://www.garfield.library.upenn.edu/ essays/v6p548y1983.pdf (accessed October 15, 2004). Eugene Garfield, "A Unified Index to Science," in Proceedings of the International Conference on Scientific Information (Washington, D. C.: National Academy of Sciences-National Research Council, 1959), 1:461­9; reprinted in Essays of an Information Scientist (Philadelphia: Institute for Scientific Information Press, 1974­1976), 2: 674­87, http:// garfield.library.upenn.edu/essays/v2p674y1974-76.pdf (accessed October 15, 2004). Eugene Garfield, "Citation Indexes for Science," 108. Ibid., 109. Ibid. Norman P. Hummon and Patrick Doreian, "Connectivity in a Citation Network: The Development of DNA Theory," Social Networks 11, 1 (March 1989): 40, http:// www.garfield.library.upenn.edu/papers/hummondoreian1989.pdf (accessed October 15, 2004). P.L.K. Gross and E.M. Gross, "College Libraries and Chemical Education," Science 66, 1713 (October 28, 1927): 385­9. Herman H. Fussler, "Characteristics of the Research Literature Used by Chemists and Physicists in the United States," Library Quarterly 19, 1 (January 1949): 19­35; ------, "Characteristics of the Research Literature Used by Chemists and Physicists in the United States," Library Quarterly 19, 2 (April 1949): 119­43. Estelle Brodman, "Choosing Physiology Journals," Bulletin of the Medical Library Association," 32, 4 (October 1944): 479. Ibid., 479­83. Eugene Garfield, Citation Indexing: Its Theory and Application in Science, Technology, and Humanities (New York: Wiley, 1979), 63. "Interview with Eugene Garfield, Chairman Emeritus of the Institute for Scientific Information (ISI)," Cortex 37, 4 (September 2001): 577. Eugene Garfield, "Journal Impact Factor: A Brief Overview," CMAJ: Canadian Medical Association Journal 161, 8 (October 19, 1999): 979.

2.

3.

4. 5. 6. 7.

8. 9.

10. 11. 12. 13. 14.

122

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications
15. ------, "How Can Impact Factors be Improved?" BMJ 313 (August 17, 1996): 411­3. 16. ------, "Recollections of Irving H. Sher 1924­1996: Polymath/Information Scientist Extraordinaire," Journal of the American Society for Information Science and Technology 52, 14 (December 2001): 1200. 17. Per O. Seglen, "Why the Impact Factor of Journals Should Not Be Used for Evaluating Research," BMJ 314, 7079 (February 15, 1997): 499. 18. Antonio G. Valdecasas, Santiago Castroviejo, and Leslie F. Marcus, "Reliance on Citation Index Undermines the Study of Biodiversity," Nature 403, 6771 (February 17, 2000): 698; see also Frank-Thorsen Krell, "Why Impact Factors Don't Work for Taxonomy," Nature 415, 6875 (February 28, 2002): 957. 19. Seglen, 501. 20. D.P. Hamilton, "Research Papers: Who's Uncited Now," Science 251, 4989 (January 4, 1991): 25. 21. Gregor B.E. Jemec, "Impact Factor to Assess Academic Output," The Lancet 358, 9290 (October 20, 2001): 1373. 22. Raymond Coleman, "Impact Factors: Use and Abuse in Biomedical Research," The Anatomical Record (New Anat.) 257, 2 (April 15, 1999): 54. 23. J.C. Korevaar and H. F. Moed, "Validation of Bibliometric Indicators in the Field of Mathematics," Scientometrics 37, 1 (September 1996): 117­30. 24. Coleman, 54­7. 25. H.F. Moed, W.J.M. Burger, J.G. Frankfort, and A.F.J. Raan, On the Measurement of Research Performance: The Use of Bibliometric Indicators (Leiden: Science Studies Unit, LisbonInstitute, University of Leiden, 1987). 26. Jesus Rey-Rocha et al., "Some Misuses of Journal Impact Factor in Research Evaluation," Cortex 37, 4 (September 2001): 596. 27. K.S. Joseph and John Hoey, "CMAJ's Impact Factor: Room for Recalculation," CMAJ: Canadian Medical Association Journal 161, 8 (October 19, 1999): 978. 28. Lee. F. Rogers, "Impact Factor and How It Affects Academic Radiologists," 11th Annual Scientific Meeting of the Singapore Radiological Society: First Regional Workshop on Medical Writing for Radiologists (Singapore, February 1, 2002), 1, http://www.srs.org.sg/2002/ ASM/MEDICAL_WORKSHOP/Workshop/1_Feb_Friday/Session_6/impact.pdf (accessed September 24, 2004). 29. E.E. Vogel, "Impact Factor and International Collaboration in Chilean Physics: 1987­1994," Scientometrics 38, 2 (February 1997): 253­63. 30. Tobias Opthof, Ruben Coronel, and Michiel J. Janse, "Submissions, Impact Factor, Reviewer's Recommendations and Geographical Bias Within the Peer Review System (1997­2002): Focus on Germany," Cardiovascular Research 55, 2 (August 1, 2002): 215­9. 31. Per O. Seglen, "How Representative is the Journal Impact Factor?" Research Evaluation 2 (December 1992): 499. 32. Ibid., 143. 33. Ibid. 34. Ibid. 35. Clifford B. Saper, "What's in a Citation Impact Factor: A Journal By Any Other Measure . . ." The Journal of Comparative Neurology 411, 1 (July 14, 1999): 1. 36. Luis Ben'tez-Bribiesca, "The Ups and Downs of the Impact Factor: The Case of the Archives of Medical Research," Archives of Medical Research 33, 2 (March 2002): 91. 37. Theodore Eliades and Athanasios E. Athanasiou, "Impact Factor: A Review with Specific Reference to Orthodontic Journals," Journal of Orofacial Orthopedics 62, 1 (January 2001): 76. 38. Mike Sosteric, "Endowing Mediocrity: Neoliberalism, Information Technology, and the Decline of Radical Pedagogy," Radical Pedagogy 1, 1 (1999), http:// radicalpedagogy.icaap.org/content/issue1_1/sosteric.html (accessed September 24, 2004). 39. Ibid.

Brian D. Cameron
40. Eugene Garfield, "Decision Making in Library Acquisitions--Indexes or Journals," Current Contents 1 (April 9, 1968): no page number; reprinted in Essays of an Information Scientist, 6:21. 41. ------, "The Impact Factor and Using It Correctly," Der Unfallchirug 48, 2 (June 1998): 413. 42. ------, "Random Thoughts on Citationology: Its Theory and Practice," Scientometrics 43, 1 (1998): 74. 43. ------, "Citation Indexes for Science": 109. 44. ------, "How Sweet It Is--The ACS Patterson-Crane Award. Reflections on the Reward System of Science," Current Contents 30 (July 25, 1983): 9; reprinted in Essays of an Information Scientist, 6: 229­36. 45. Irving Sher and Eugene Garfield, "New Tools for Improving and Evaluating Effectiveness of Research," in Research Program Effectiveness: Proceedings of the Conference Sponsored by the Office of Naval Research, Washington, D.C., July 27­29, 1965, ed. D.M. Gilford et al. (New York: Gordon and Breach, 1966), 135­46. 46. Melvin Weinstock, "Citation Indexes," in Encyclopedia of Library and Information Science (New York: Marcel Dekker, 1971), 5: 37; reprinted in Essays of an Information Scientist, 6: 569. 47. Christine L. Borgman and Jonathan Furner, "Scholarly Communication and Bibliometrics," in Annual Review of Information Science and Technology 36 (2002): 5. 48. Anonymous, "Editorial: An Evaluation of Impact Factors and League Tables," Journal of the Royal College of Physicians of Edinburgh 32, 3 (2002): 157. 49. Hanna Kokko and William J. Sutherland, "What do Impact Factors Tell us?" Trends in Ecology and Evolution 14, 10 (October 19, 1999): 382. 50. P.J. Ell, "Impact Factor of Common Sense," Clinical Radiology 55, 6 (June 2000): 413. 51. Saper. 52. Laura Calza and Spiridione Garbisa, "Italian Professorships," Nature 374, 6522 (April 1995): 492. 53. Charles Jennings, "Citation Data: The Wrong Impact," Cortex 37, 4 (September 2001): 587. 54. Lowell L. Hargens and Howard Schuman, "Citation Counts and Social Comparisons: Scientists' Use and Evaluation of Citation Index Data," Social Science Research 19, 3 (September 1990): 205­21. 55. Eugene Garfield, "The Impact Factor and Using it Correctly": 413. 56. Penelope S. Murphy, "Journal Quality Assessment for Performance-Based Funding," Assessment and Evaluation in Higher Education 23, 1 (March 1998): 25­31. 57. Marek H. Dominiczak, "Funding Should Recognize Value of Peer Review," Nature 421, 6919 (January 9, 2003): 111. 58. Duanning Zhou, Jian Ma, and Efraim Turban, "Journal Quality Assessment: An Integrated Subjective and Objective Approach," IEEE Transactions on Engineering Management 48, 4 (November 2001): 479­90. 59. Eugene Garfield, "What Citations Tell us About Canadian Research," Canadian Journal of Library and Information Science 18, 4 (1993): 34. 60. Manfred Bonitz, "Ranking Nations and Heightened Competition in Matthew Core Journals: Two Faces of the Matthew Effect for Countries," Library Trends 50, 3 (Winter 2002): 442. 61. Ibid., 443. 62. Ibid., 450. 63. Ibid., 448. 64. Coleman, 56. 65. Ibid. 66. Seglen, 498. 67. Matthew Baylis, Michael Gravenor, Rowland Kao, "Letter," Nature 401, 6751 (September 23, 1999): 322 68. Ibid.

123

124

Trends in the Usage of ISI Bibliometric Data: Uses, Abuses, and Implications
69. Bernhard Statzner, Vincent H. Resh, and Norma G. Kobzina, "Low Impact Factors of Ecology Journals: Don't Worry," Trends in Environment and Ecology 10, 5 (May 5, 1995): 220. 70. Eugene Garfield, "The Pulling Power of Current Contents and the Reward System of Science," Current Contents 31 (August 1, 1973): 5­6. 71. Jennings, 586. 72. Kenneth M. Adams, "Impact Factors: Aiming at the Wrong Target," Cortex 37, 4 (September 2001): 601. 73. Eugene Garfield, "Fortnightly Review: How Can Impact Factors be Improved," BMJ 313, 7054 (August 17, 1996): 412. 74. Adrian Dixon, "Impact Factor: Editor's Note," Clinical Radiology 55, 6 (June 2000): 414. 75. K.S. Joseph and John Hoey, "CMAJ's Impact Factor: Room for Recalculation," CMAJ 161, 8 (October 19, 1999): 978. 76. G.H. Whitehouse, "Impact Factors: Facts and Myths," European Radiology 12, 4 (April 2002): 715. 77. David R. Harder, "Impact Factors and the Competitive Nature of Journal Publishing," AJP--Heart and Circulatory Physiology 279, 2 (August 2000): H457. 78. Ibid. 79. Richard Smith, "Journal Accused of Manipulating Impact Factor," BMJ 314, 7079 (February 15, 1997): 461. 80. Anders Hemminsson et al., "Manipulation of Impact Factors by Editors of Scientific Journals," AJR 178, 3 (March 2002): 767. 81. R. Gunzburg, M. Szpalski, and M. Aebi, "The Impact Factor: Publish, be Cited or Perish," European Spine Journal 11, supplement 1 (2002): S1; see also Ken Hyland, "Self-Citation and Self-Reference: Credibility and Promotion in Academic Publication," Journal of the American Society for Information Science and Technology 54, 3 (2003): 251­9. 82. J. Reedijk, "Sense and Nonsense of Science Citation Analyses: Comments on the Monopoly Position of ISI and Citation Inaccuracies, Risks of Possible Misuse and Biased Citation and Impact Data," New Journal of Chemistry 8 (1998): 767­70. 83. Philip Bal, "Paper Trail Reveals References Go Unread by Citing Authors," Nature 420, 6916 (December 12, 2002): 594. 84. Association of Research Libraries, "Monograph and Serials Costs in ARL Libraries, 1986­ 2002," graph 2, http://arl.org/stats/arlstat/graphs/2002/2002t2.html (accessed October 21, 2004). 85. Mary M. Case, "Igniting Change in Scholarly Communications: SPARC, Its past, Present, and Future," Advances in Librarianship 26 (October 2002): 3. 86. Ibid. 87. "Alternatives are Taking the Lead," SPARC, http://www/arl/org/sparc/resources/1001TICER/sld018.htm (accessed October 23, 2004). 88. Mary M. Case, "Capitalizing on Competition: The Economic Underpinnings of SPARC," SPARC, http://www.arl.org/sparc/announce/case040802.html (accessed October 23, 2004). 89. Jean-Claude Guedon, "In Oldenburg's Long Shadow: Librarians, Research Scientists, Publishers, and the Control of Scientific Publishing," Association of Research Libraries (2001), http://www.arl.org/arl/proceedings/138/guedon.html (accessed September 24, 2004). 90. Christopher Gasson, Who Owns Whom in British Book Publishing (London: Bookseller Publications, 2002), 30­1. 91. Priptal S. Tamber, "Is Scholarly Publishing Becoming a Monopoly?" BMC 1 (2002): 1. 92. Liz Bury, "Elsevier Cleared by OFT Report," The Bookseller 5043 (September 13, 2002): 5. 93. Brendan J. Wyly, "Competition in Scholarly Publishing: What Publisher Profits Reveal," http://www.arl.org/newsltr/200/wyly.html (accessed September 24, 2004). 94. M. Curti, V. Pistotti, G. Gabutti, and C. Klersy, "Impact Factor and Electronic Versions of Biomedical Scientific Journals," Haematologica 86, 10 (October 2001): 1015­20.

Brian D. Cameron
95. Reed Elsevier, "Interim Statement, 2001," http://www.reedelsevier.com/media/pdf/6/g/ re_interim_2001.pdf (accessed September 24, 2004); Reed Elsevier, "Financial Highlights for the Year Ended 31 December 2003," http://www.euronext.com/file/view/ 0,4245,1626_53424_133707706,00.pdf (accessed September 24, 2004).

125

