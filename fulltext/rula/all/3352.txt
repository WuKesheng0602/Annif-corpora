POSE ESTIMATION FOR ROBOTIC PERCUSSIVE RIVETING

by

Yu Lin Master of Applied Science, Mechanical Engineering Ryerson University, 2008

A dissertation presented to Ryerson University

in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Program of Aerospace Engineering

Toronto, Ontario, Canada, 2014

© Yu Lin, 2014

AUTHOR'S DECLARATION
I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

ii

ABSTRACT
Pose Estimation for Robotic Percussive Riveting Yu Lin A dissertation for the degree of Doctor of Philosophy, 2014 Department of Aerospace Engineering, Ryerson University

Recently, a robotic percussive riveting system has been developed at Ryerson University for an automation of percussive riveting process of aero-structural fastening assembly. The system consists of a robot holding a percussive riveting gun equipped with a rivet feeder, a gantry holding a working panel of aero-structure, and a position visual sensor. Prior to riveting, the robot is required to first position and then insert a rivet precisely into a hole on the panel without engaging the panel to prevent potential damage. The underlying challenges to precise insertion are various sources of system uncertainties, mainly including alignment errors among coordinate systems of the robot, panel and sensor, and relatively poor absolute positioning accuracy of the robot due to mechanical deflection, assembly clearance, and machining tolerance. For this reason, the research of relative pose estimation between the robot and panel has been carried out pertaining to these challenges. Essentially, pose estimation is proposed for robotic percussive riveting, which estimates the relative pose between two rigid bodies based on noisy visual measurements of point features on rigid bodies. Three categories can be classified, namely, static, dynamic, and robust pose estimation. Firstly, static pose estimation is the parameter estimation of static relative pose transformations among a number of frames, which solves the issue of alignment errors. Direct solutions of static relative pose estimation are derived based on least-square methods. Secondly, to tackle the issue of poor absolute positioning accuracy of the robot, dynamic relative pose estimation is proposed addressing a state estimation of relative poses during motion. Iterative extended Kalman filter method is adapted for the state estimation. Thirdly, for robustness against outliers of iii

point measurements, robust pose estimation is proposed based on an outlier diagnosis using the technique of relaxation of rigid body constraints. Indeed, outlier diagnosis is a pre-processing of point measurements, in which outliers are detected and removed prior to the relative pose estimation. Further, a decorrelation method is proposed for measurement calibration using multivariate statistical analysis to find an optimal sensorto-target configuration. As a result, each coordinate measurement is close to uncorrelated and it allows for a simple calibration.

iv

ACKNOWLEDGEMENTS
I am deeply indebted to my supervisor Dr. Fengfeng (Jeff) Xi, to whom I owe a debt of gratitude for the invaluable support, guidance and kindly encouragement throughout the past years of study. With his strong academic background and innovative ideas, he has been really helpful to me for solving many difficulties whenever I needed help. Thanks also go to my co-supervisor Dr. Vincent Chan, for his great support and sincere help during my graduate studies. Also, I would like to thank Post-Doctoral fellow Dr. Yimin Zhao for his time and all the discussion we had. Gratitude also goes to Ryerson University, National Science and Engineering Research Council (NSERC), FedDev Ontario, and Aerospace Manufacturing Technology Centre (AMTC) of National Research Council Canada (NRC) for providing adequate research funding and opportunities for my graduate studies. I would like to sincerely thank my family, especially my beloved fiancé e, Christine Xiaofang Weng, for her encouragement, support and understanding over the school years. Last but not least, thank you, Lord, Heavenly Father, for bringing me through all the difficulties of this research. Thank you for listening to my prayers and brightening my world with "the way, the true, and the life" (John 14:6). Thanks also go to my fellow brothers and sisters in Christ, for their continual encouragement.

v

TABLE OF CONTENTS
AUTHOR'S DECLARATION ........................................................................................ ii ABSTRACT ...................................................................................................................... iii ACKNOWLEDGEMENTS ............................................................................................. v TABLE OF CONTENTS ................................................................................................ vi LIST OF TABLES ........................................................................................................... xi LIST OF FIGURES ........................................................................................................ xii NOMENCLATURE ........................................................................................................ xv CHAPTER 1 INTRODUCTION ................................................................................... 1 1.1 Background ............................................................................................................. 1 1.2 Robotic Percussive Riveting .................................................................................. 4 1.3 Problem Formulation ............................................................................................. 7 1.4 Research Objective ................................................................................................. 8 1.5 Outline of Thesis ................................................................................................... 11 CHAPTER 2 LITERATURE REVIEW ..................................................................... 13 2.1 Pose Estimation..................................................................................................... 13 2.2.1 Static Pose Estimation ................................................................................... 14 2.2.2 Dynamic Pose Estimation ............................................................................. 15 2.2.3 Robust Pose Estimation ................................................................................ 16 2.2 Position-Based Visual Servoing ........................................................................... 18 CHAPTER 3 STATIC POSE ESTIMATION............................................................ 22 3.1 Formulation of Pose Estimation .......................................................................... 22 3.1.1 Cartesian Frame Formulation........................................................................ 23

vi

3.1.2 Three-Point Method ...................................................................................... 23 3.1.3 Normalized Directional Vectors ................................................................... 24 3.1.4 Covariance Matrix Method ........................................................................... 25 3.2 Least Squares Methods ........................................................................................ 26 3.3 Direct Relative Pose Estimation between Two Rigid Bodies ............................ 27 3.3.1 Motivation of Direct Relative Pose Estimation ............................................ 28 3.3.2 Problem Statement ........................................................................................ 29 3.3.3 LS solution .................................................................................................... 32 3.3.4 CTLS solution ............................................................................................... 36 3.3.5 Case Studies .................................................................................................. 38 3.3.6 Summary ....................................................................................................... 43 3.4 Pose Estimation for Multiple Frames of a Rigid-Body System ........................ 45 3.4.1 Description of Localization ........................................................................... 45 3.4.3 Localization of Robotic Riveting System ..................................................... 46 3.4.4 Verification of Localization .......................................................................... 55 3.4.5 Summary ....................................................................................................... 56 CHAPTER 4 DYNAMIC POSE ESTIMATION ....................................................... 57 4.1 Description of Dynamic Pose Estimation ........................................................... 57 4.2 Problem Statement ............................................................................................... 59 4.3 Relative Pose and Motion Estimation ................................................................. 62 4.3.1 State Space Modeling.................................................................................... 63 4.3.2 Relative Pose and Motion Estimation by IEKF ............................................ 65 4.4 Simulations and Experiments.............................................................................. 68

vii

4.4.1 Preliminary Transformation of Target 2-TCP and Target 1-Working Panel 70 4.4.2 Simulations.................................................................................................... 71 4.4.3 Experiments................................................................................................... 81 4.5 Summary ............................................................................................................... 84 CHAPTER 5 ROBUST POSE ESTIMATION .......................................................... 86 5.1 Problem Statement ............................................................................................... 86 5.2 Outlier Diagnosis .................................................................................................. 87 5.2.1 Rigid Body Constraints ................................................................................. 87 5.2.2 Outlier Diagnosis by Relaxation ................................................................... 89 5.3 Simulations and Experiments.............................................................................. 92 5.3.1 Case Study of Four Outliers .......................................................................... 93 5.3.2 Experiment Verification ................................................................................ 96 5.4 Summary ............................................................................................................... 97 CHAPTER 6 DECORRELATION METHOD FOR MEASUREMENT

CALIBRATION .............................................................................................................. 99 6.1 Description of Measurement Calibration ........................................................... 99 6.2 Decorrelation Formulations .............................................................................. 101 6.2.1 Local Decorrelation ..................................................................................... 102 6.2.2 Global Decorrelation ................................................................................... 104 6.3 Evaluation and Comparison .............................................................................. 107 6.4 Experiments and Simulations............................................................................ 107 6.5 Summary ............................................................................................................. 114 CHAPTER 7 CONCLUSIONS AND FUTURE WORKS ...................................... 115

viii

7.1 Conclusions ......................................................................................................... 115 7.2 Contributions ...................................................................................................... 116 7.3 Future Works ...................................................................................................... 117 APPENDIX A - POSITION AND ORIENTATION.................................................. 118 A.1 Position Vector ................................................................................................... 118 A.2 Rotation Matrix ................................................................................................. 119 A.3 Angle-set Representation of a Rotation ........................................................... 121 APPENDIX B - GENERAL MOTION ....................................................................... 124 B.1 General Motion of a Single Rigid Body ........................................................... 124 B.2 General Motion of Multiple Rigid Bodies ....................................................... 124 APPENDIX C - MATRIX COMPUTATIONS .......................................................... 128 C.1 The Rank of a Matrix ........................................................................................ 128 C.2 Singular Value Decomposition ......................................................................... 129 C.3 The Moore-Penrose (MP) Inverse .................................................................... 129 C.4 The Solution of Linear Equation Systems ....................................................... 130 C.5 Least Squares Method and Gauss-Markov Theorem .................................... 131 C.6 Rotation of Subspaces (Constrained Least-Squares) ..................................... 133 APPENDIX D - LEAST-SQUARE ESTIMATION OF TRANSFORMATION PARAMETERS BETWEEN TWO POINT PATTERNS......................................... 134 APPENDIX E - GENERAL FORMULATION FOR THE OUTLIER DIAGNOSIS BY RELAXATION ....................................................................................................... 136 APPENDIX F - MEASURING SYSTEM MODELING ........................................... 137 APPENDIX G - ROBOTIC PERCUSSIVE RIVETING .......................................... 139

ix

APPENDIX H - ROBUSTNESS EVALUATING EXPERIMENTS FOR NDI DATA ACQUISITION ............................................................................................................. 146 H.1 Reflection Distraction ....................................................................................... 146 H.2 Illumination Distraction ................................................................................... 147 H.3 Missing data and outliers .................................................................................. 150 APPENDIX I - JIG DESIGN FOR PANEL ............................................................... 153 APPENDIX J - PUBLICATIONS AND PATENTS .................................................. 154 REFERENCES .............................................................................................................. 156

x

LIST OF TABLES
Table 1.1: Determination of Transformations .................................................................... 8 Table 3.1: Observed Coordinates of Points of Rigid Body Targets w.r.t. the Frame of the Measurement System ....................................................................................... 39 Table 3.2: Local Coordinates of Points of Rigid Body Targets ........................................ 40 Table 3.3: Relative Rotation Matrix in Experiments ( ) ............................................. 40 Table 3.4: Simulated Coordinates of Points w.r.t. the Measurement Frame .................... 42 Table 3.5: Simulated Local Coordinates of Points w.r.t. the Body-Fixed Frames ........... 42 Table 3.6: Relative Rotation Matrix in Simulation ( ) ................................................ 42 Table 3.7: Combination of point numbers ........................................................................ 44 Table 3.8: Central coordinates of tooling balls in the sensor and panel frames. .............. 49 Table 3.9: Coordinates of the TCP in the sensor and robot base frames. ......................... 53 Table 3.10: LED coordinates of the target in the sensor and local frames. ...................... 54 Table 4.1: Local coordinates of LED points of the targets in the body-fixed frames ...... 69 Table 4.2: Local coordinates of LED points of the targets w.r.t. TCP and working panel 71 Table 4.3: D-H parameters of ABB 4400/45kg robot manipulator .................................. 77 Table 5.1: Pose Transformation Before and After the Outlier Contamination ................. 94 Table 5.2: Indicators of Compatibility - Replacement of Distance Deviation with Integers ......................................................................................................................... 95 Table 5.3: Iterations of Merit Scores for Outlier Diagnosis by Relaxation (Four Outliers) ......................................................................................................................... 95 Table 5.4: Pose Transformation Before and After the Outlier Diagnosis ......................... 96 Table 5.5: Indicators of Compatibility for 7 Markers ...................................................... 97 Table 5.6: Iterations of Merit Scores for Outlier Diagnosis by Relaxation (Two Outliers) ......................................................................................................................... 97 Table B.1: Rm and bm of different kinematic pairs [71]. ................................................ 127

xi

LIST OF FIGURES
Figure 1.1: Principle of Robotic Percussive Riveting. [18] ............................................... 5 Figure 1.2: (a) System configuration setup simulated in ABB RobotStudio. (b) Experimental setup of the robotic riveting system at EPH107, Ryerson University. ....................................................................................................... 6 Figure 1.3: System transformational modeling with two loops of transformations. One between the panel, measurement, and robot base; while another one between the robotic tooling tip, end-effector, target, measurement, and robot base. .... 7 Figure 1.4: Structure of research of pose estimation - problems and methods. ................. 9 Figure 3.1: Cartesian frame formulation. ......................................................................... 23 Figure 3.2: Transformation determinations using the three-point position data [28]. ..... 24 Figure 3.3: Two rigid body targets observed by a measurement system. ........................ 29 Figure 3.4: Experimental setup of a robotic riveting system. .......................................... 38 Figure 3.5: Two rigid body targets on the work piece and the robot end-effector. .......... 39 Figure 3.6: Three tooling balls. ........................................................................................ 48 Figure 3.7: Wireless handheld probe for digitization. ...................................................... 48 Figure 3.8: Panel localization using three tooling balls, and TCP of the rivet gun. ........ 49 Figure 3.9: (a) Digitization of tooling balls with a handheld probe; (b) the fitted sphere. ....................................................................................................................... 49 Figure 3.10: Determination of tool length from TCP to end-effector flange, (a) digitizing the surface of the flange of the end-effector; (b) digitizing the TCP of the rivet gun. ....................................................................................................... 51 Figure 3.11: (a) Screenshot of tool definition for the rivet gun in the ABB FlexPendant; (b) Pose of TCP of the rivet gun with respect to the robot base frame ( ). ....................................................................................................................... 51 Figure 3.12: Robot localization by creating a virtual target at multiple poses in workspace...................................................................................................... 52 Figure 3.13: Target 2. (a) Modeling (b) Four infrared LEDs. (c) The target on the tooling and transformation between frames of target 2 and measurement. ............... 55 Figure 3.14: Hole digitization with handheld probe. ...................................................... 55 Figure 4.1: Two rigid body targets observed by a position measurement system. .......... 60 Figure 4.2: (a) Four LEDs on the rigid body target 1. (b) Two rigid body targets attached on the robot tooling and the working panel. ................................................. 69 Figure 4.3: Relative pose estimation integrated in the position-based visual servoing of the robotic riveting system. ........................................................................... 72 Figure 4.4: Simulink model of the robot position-based visual servoing using the proposed filter of the relative pose estimation designed in MatLab. ............ 73 Figure 4.5: Position-based visual servoing in the Simulink model.................................. 74 Figure 4.6: Plant of the robot in Simulink model. ........................................................... 75 Figure 4.7: The Simulink model of the optoelectronic CMM to simulate the point measurements. ............................................................................................... 77 Figure 4.8: Schematic diagrams of the robot poses in the simulation. (a) The initial position. (b) The desired position.................................................................. 78 Figure 4.9: Simulation results of dynamic pose estimation. (a) Relative translational estimation results. (b) Relative orientation estimation results. The blue solid xii

lines represent the actual relative poses, and the red dashed lines represent the estimated poses........................................................................................ 79 Figure 4.10: Norm of the error of relative pose estimation during simulation. (a) Linear scale of norm of error values. (b) Log scale of norm of error values. .......... 80 Figure 4.11: Initial position of the robot with joint angles of q0 = [0 0 0 0 10°0]. ......... 81 Figure 4.12: Final position of the robot with joint angles of qd = [15°-20°30°0 15°0]. ....................................................................................................................... 82 Figure 4.13: Experimental linear trajectory comparison. (a) Relative translational estimation results. (b) Relative orientation estimation results. The blue solid lines are the nominal trajectory and the red dashed lines are the estimated trajectory. ....................................................................................................... 83 Figure 4.14: Converging results of norms of the errors of relative pose estimation in the experiment. (a) Linear scale of norm of errors. (b) Log scale of norm of errors. ............................................................................................................ 84 Figure 5.1: Motion of the cubic rigid body. ..................................................................... 87 Figure 5.2: Point distances under the rigid body constraints. (a) one, or two, or three distance constraints are not sufficient for unique 3D positioning; (b) the minimum number of points is 5 for unique 3D positioning; (c) four distance constraints for point p1; (d) an outlier example............................................. 89 Figure 5.3: Experimental system set-up. ......................................................................... 92 Figure 5.4: (a) 7 Markers in the 3D space; (b) target of the cubic rigid body. ................ 93 Figure 5.5: Two poses of the cubic rigid body. ................................................................ 93 Figure 5.6: Outliers occurred at markers 1, 2, 6, and 7; (a) Pose transformation determined with four outliers; (b) Pose estimation from the remaining three markers 3-5 after removing the outliers. ....................................................... 95 Figure 5.7: (a) Merit scores during 7 iterations; (b) Signs obtained from normalized merit scores, "-1" indicate outliers (markers 1, 2, 6, and 7), and "+1" indicate the reliable data (markers 3, 4, and 5). ................................................................ 96 Figure 5.8: Markers for simulating outliers and the original markers. ............................ 97 Figure 6.1: 500 repetitive readings of marker_1s' constant 3D position at pose_1. ..... 108 Figure 6.2: Standard deviation (STD) of marker's positions. ....................................... 109 Figure 6.3: Mean absolute correlation coefficients of 5 poses for 7 markers. .............. 109 Figure 6.4: Comparison of ellipsoids of 1, 2, 3 of the original (a) and the decorrelated data (b). ....................................................................................110 Figure 6.5: (a) Local optimal decorrelated configurations of the NDI Optotrak system (in red) with respect to the PDF ellipsoids of 7 markers, the original NDI configuration is in cyan; (b) mean and STD of measurements before and after the local decorrelation.......................................................................... 111 Figure 6.6: The global decorrelated configuration of NDI for the entire set of markers (in red), when the marker data Xi are mutually uncorrelated. ......................112 Figure 6.7: Results when the data of the 7 markers are mutually correlated, with a larger size of ellipsoid due to larger eigenvalues. ..................................................112 Figure 6.8: Local and global data decorrelation comparisons, with various noisy levels from ±10% to ±30%. "Local_1" to "Local_7" mean the seven local data decorrelation methods at each markers. "Global_UC" represents the global data decorrelation when mutually uncorrelated; "Global_C" indicates the xiii

global data decorrelation when mutually correlated. ...................................113 Figure A.1: Multiple modules system for a serial robot manipulator. ............................118 Figure A.2: Position vector p [71]. ..................................................................................119 Figure B.1: General motion of a single module [71]. .................................................... 124 Figure B.2: Vector method for a multi-module system [71] .......................................... 125 Figure F.1: Principle of Optoelectronic Coordinate Measurement Machines [88]. ....... 137 Figure G.1: Rivet guns and C-frame riveter. ................................................................. 140 Figure G.2: Robotic Percussive Riveting System [3]. ................................................... 142 Figure G.3: Robot Riveted Samples [3]......................................................................... 142 Figure G.4: Flowchart of Process of Robotic Percussive Riveting System. .................. 143 Figure G.5: Graphical user interface development for robotic riveting system. ........... 144 Figure G.6: Interfaces for acquisition and localization. ................................................. 145 Figure H.1: IR marker's reflection on an aluminum board and a painted metal part. ... 147 Figure H.2: (a) The NDI Optotrak system observes the IR LED lights of the remote instead, when the markers are not observable due to away from the camera sensing direction. (b) Demonstration of Infrared LED remote's distraction during the data acquisition: the remote was pressed down and placed beside a marker (marker 7). ...................................................................................... 148 Figure H.3: The positions of 7 markers under the distraction of the remote: missing data at marker 7; disturbances at markers 3-6; no effect at markers 1 and 2. The blue, green and red lines represent the x, y and z coordinates. ................... 149 Figure H.4: (a) Original set-ups with 7 markers attached to the cubic target; (b) a way of creating an outlying marker by detaching the marker from the mounted case. ..................................................................................................................... 151 Figure H.5: (a) With four outlying markers, the pose estimation failed even when there are three remaining markers; (b) once four outliers were manually removed, the pose estimation of the rigid body was restored immediately. ............... 152

xiv

NOMENCLATURE
Symbol x, y, z , ,  p R t T U, ,V v D n b b' q  I J g X A B c  s Definition x, y or z coordinate position pitch, roll, and yaw angles position vectors rotation matrix translational vector homogeneous transformation matrix singular value decomposition normalized directional vector observation matrix formed by v number of points frame - Cartesian coordinate system body vectors local body vectors motion parameters (s and ) rotation angles identity matrix Jacobian matrix geometry of a robot (bs and Rs) state of dynamic pose estimation point coordinates of target 1 point coordinates of target 2 compatibility of relaxation residual merit scores Units m rad (or degree if specified) m m m m rad (or degree if specified) m and rad m and rad m m -

Subscript i n tcp r m p tip t min max

Definition ith point total number of points tool central point robot base measurement working panel the tip of robot target minimum maximum

xv

CHAPTER 1 INTRODUCTION
1.1 Background
In aerospace industry, high precision is necessary due to manufacturing of airplanes for their improved characteristics about safety, fuel consumption, noise pollution reduction, and comfort for passengers. In aerospace applications ranging from material handling to fastening and assembly, precise positioning plays a crucial role. Robots offer the prospect of removing much of the tedious, unattractive, and in some cases dangerous manufacturing tasks, greatly enhancing the working environment. But, most of today's industrial robots do not have good three dimensional (3D) absolute positioning capacity, though their repeatability following taught positions is high enough. As in most of aerospace manufacturing environment, the complex working parts may have several thousands of positions and trajectories, at which a robot stops or passes. It is impossible to teach the robot at each position. As well, the varieties and the forms of parts may not be always rigid. For flexible fabrication with high precision, adaptive metrologyguidance robotic systems are increasingly demanded in aerospace manufacturing. Riveting is one of the major joining methods used in aircraft assembly. Manual riveting operations are tedious, repetitious, prone to error, and likely causing health and ergonomic problems [1]. Currently, automated riveting systems are being developed to replace manual riveting operations, such as using dual robots to provide panel holding and feeding functions for conventional squeezing riveting machines [2], as shown in Figure 1.1. More recently, a novel robotic percussive riveting system has been developed in Ryerson University for percussive riveting automation [3]. The system adopts a robot 1

to hold and move a percussive riveting gun equipped with a rivet feeder, and a gantry to hold a working panel and to move a bucking bar. There are two main separating steps in the robotic percussive riveting process. One is called hole-drillings in which all the holes are first drilled based on a planned riveting path and another one is called rivet-in-hole insertion or called peg-in-hole insertion along the same path. Prior to the riveting process, rivet-in-hole insertion requires a robot to first position and then insert a rivet precisely into a hole on the working panel without engaging the panel to prevent potential damage. Practically, there always exist undesired errors, namely system uncertainties which significantly influence the positioning accuracy and the stability of the robotic riveting system. The uncertainties can be briefly grouped into two parts, which are the intrinsic parameters such as robot positioning errors due to mechanical deflection, assembly clearance, machining tolerance and thermal expansion, and the extrinsic parameters such as alignment errors, tooling calibration errors, and external disturbances including vibration, drill bit wear, rivet feeder positioning error, etc. In general, the tooling calibration errors can be reduced through well calibration process, but robot positioning errors always exist in a robotic riveting system. Generally, the rivet holes are dilled with a tolerance of approximately 0.2 mm [4]. The performance of the adopted ABB 4400/45 robot can only assure unidirectional pose repeatability of 0.07 mm, linear path accuracy of 0.8 - 1.3 mm and linear path repeatability of 0.25 - 0.4 mm at rated load and 1m/s velocity on a inclined ISO test plane according to ISO 9283 [5]. Apparently, the robot path repeatability would not be able to guarantee a successful rivet insertion from the tip of the rivet gun to the inside of the hole. As a result, in the hole-drilling step of the robotic riveting process, even though the holes are drilled according to a planned path,

2

they are still deviated due to the presence of the system uncertainties. Unsuccessful rivet insertion would cause damage to sheet metal skins or to the tooling system. A great deal of research has been done on peg-in-hole insertion [6-11]. In [6], the geometric analysis and force/moment analysis of three-dimensional multiple-peg insertion are presented. Because of the difficulty and complexity in analyzing threedimensional peg-in-hole insertion, most analyses of this class of assembly tasks were done by simplifying the problems into two dimensions [7, 8]. Surges [9] analyzed the wedging in three-dimensional rectangular peg insertion. A hybrid force-position strategies using active and passive mechanism were designed in three-dimensional insertion [10]. In [11], a compliant mechanism was designed for axis symmetric part insertion to passively compensate for position and orientation uncertainties during an assembly process. However, the approaches on three-dimensional peg-in-hole insertions in assembly tasks depend on the geometric features of assembly objects or the interaction forces between pegs and holes, thus it cannot be used in the rivet-in-hole insertions of the robotic riveting process. Therefore, a robust and efficient control scheme should be explored for rivet-inhole insertions in robotic riveting system. One feasible and cost-effective way is to use robot visual servoing (RVS) method to achieve high precision positioning. RVS systems constitute a class of opto-mechatronic systems in which measurement data are integrated into the robot control system to enhance the robot control performance [12]. RVS systems can be classified into two main categories [13]: position-based visual servoing (PBVS) [14] and image-based visual servoing (IBVS) [15]. IBVS is only stable locally and may suffer from Jacobian singularity and local minima [16]. It usually induces the unpredictable and sometimes 3

undesired 3D trajectories. In contrast, PBVS is a model-based type of visual servoing and known to be global stable. It utilizes the pose (position and orientation) of object of interest with respect to the camera, therefore, sometimes referred to pose-based visual servoing. However, since it requires a 3D model of a target, the main drawback is reliance upon calibration and usually it has no control of the image plane and may easily loose the visual features that are used to estimate a pose [17]. In PBVS, first the estimation of the pose of the target with respect to the camera is performed, and this information is used to produce the appropriate control signal to track the target. To do so, pose estimation serves as an essential step of estimating the pose of object of interest with respect to the camera based on visual measurements. However, when multiple objects of interest are tracked, relative poses between these objects are demanded in the visual servoing. In this research, unlike the conventional way of estimating the pose of each object of interest with respect to the camera, relative pose estimation is proposed to directly estimate the relative pose between rigid body objects. Specifically for the purpose of robot precise positioning and rivet insertion control, the online state estimations of the relative pose and motion are desired between the tool center point (TCP) of the robot tooling and the working panel. For this reason, a metrology system of a position visual sensor is introduced to guide the robot and the research of relative pose estimation between the robot and panel has been carried out pertaining to these challenges.

1.2 Robotic Percussive Riveting
The robotic percussive riveting system is a new application for aircraft assembly

4

automation. The prototype of the system is located at EPH 107, Ryerson University. Figure 1.1 shows the principle of robotic percussive riveting. It includes a 6-DOF industrial robot that replaces the first worker for holding/moving a percussive rivet gun; a 5-axis computer numerical control (CNC) gantry system that replaces the second worker for holding/moving a bucking bar [3]. A working panel of sheet metals is loaded on the jig of the gantry for riveting operations. The tooling of riveting gun is attached on the end-effector of the robot for riveting assembly of aero-structural parts on the working panel. A rivet is located at the TCP of the robot tooling. The details of the hardware and software development are provided in Appendix G.

Figure 1.1: Principle of Robotic Percussive Riveting. [18]

For the need of precise positioning via metrology guidance, a position sensing system was adopted for position-based visual servoing in the robotic riveting system, as shown in Figure 1.2. The chosen position sensing system is a portable optical coordinate measurement machine (CMM), whose working principle is basically a camera-based triangulation system with 3 linear charge-coupled device (CCD) cameras, resulting in 1

5

horizontal and 2 vertical angle measurements [19]. The optical CMM is employed to measure the 3D coordinates of multiple feature points on the robotic tooling of riveting gun and the working panel on the gantry. Based on the measured point positions, research of relative pose estimation is to be carried out. Figure 1.2 (b) demonstrates the experimental setup of the robotic riveting system, including a robot of ABB IRB4400/45 and an optical CMM of 3D Creator FP7000 from Boulder Innovation Group (BIG).
Gantry Rivet Gun Position Sensing System

Working Panel Industrial Robot

(a)

Rivet gun BIG 3D Creator FP 7000

Working panel

ABB IRB 4400 robot

(b) Figure 1.2: (a) System configuration setup simulated in ABB RobotStudio. (b) Experimental setup of the robotic riveting system at EPH107, Ryerson University.

6

1.3 Problem Formulation
The complete system transformational modeling is provided by assigning a number of Cartesian coordinate frames to the robotic riveting system, as shown in Figure 1.3. The frames are defined as follows: : Frame of working panel : Frame of measurement : Frame of robot base : Frame of rivet hole : Frame of TCP at the tip of a rivet : Frame of robot end-effector : Frame of target 1 on the panel : Frame of target 2 on the tooling

Rivet Hole

Fh Fp

Tt1h Ft1

Target 1

Panel Target 2

Tmt1
Measurement

Eq. (1.4)

Ttcph Tmt2 Ftcp

Ft2 Tt2
e

Tmp

Eq. (1.1)

Trp

Fe Tetcp
Eq. (1.2)

Fm
Rivet TCP

Trtcp Tmr
Robot

Fr
Figure 1.3: System transformational modeling with two loops of transformations. One between the panel, measurement, and robot base; while another one between the robotic tooling tip, end-effector, target, measurement, and robot base.

7

Figure 1.3 shows two loops of transformations. The first one describes the transformation between frames of the panel, measurement, and robot base, expressed in Eq. (1.1); the second one describes the transformations between frames of the robotic TCP, endeffector, target 2, measurement, and robot base, expressed in Eq. (1.2). (1.1) (1.2) where represents a homogeneous transformation matrix from a x frame to a y frame;

subscripts p, m, r, tcp, e, and t2 stand for the panel frame, measurement frame, robot base frame, TCP frame, end-effector frame, and target 2 frame, respectively. Table 1.1 shows the determination of these transformations in Eqs. (1.1) and (1.2). are determined by localization; , , , and and

is obtained by tooling calibration.

are solved by state estimation based on point measurements on targets 1 and 2. Table 1.1: Determination of Transformations Transformation Localization        Calibration State Estimation

1.4 Research Objective
The main research objective is to improve the positioning accuracy of rivet-in-hole insertion for the robotic percussive riveting system. The future successful promotion of the robotic percussive riveting system depends on the precise positioning of rivet-in-hole

8

insertion. The research approach is to first analyze the sources of error or uncertainty, and then develop methodologies pertaining to these challenges. In general, there are three main sources of uncertainty, namely, localization, positionbased visual servoing, and measurement, as illustrated in Figure 1.4. Localization is to transfer the coordinates of the rivet spots to those in the robot frame after proper alignment, which leads to the research of static pose estimation in Chapter 3. Positionbased visual servoing is to control the robotic tooling tip to reach each rivet spot precisely based on state estimation of tip pose from visual sensing, which leads to the research of dynamic pose estimation in Chapter 4. Measurement accuracy can be improved through outlier diagnosis and measurement calibration, which leads to the research of robust pose estimation in Chapter 5, and a decorrelation method for measurement calibration in Chapter 6.
Problems Localization Methods Static Pose Estimation (Chapter 3)

Position-Based Visual Servoing

Dynamic Pose Estimation (Chapter 4)

Outlier Diagnosis

Robust Pose Estimation (Chapter 5)

Measurement Calibration

Decorrelation Method (Chapter 6)

Figure 1.4: Structure of research of pose estimation - problems and methods.

9

Clearly, localization is a prior step to provide a desired tip pose for the position-based visual servoing. Outlier diagnosis and measurement calibration both contribute to provide reliable and accurate point measurements for localization and position-based visual servoing. For localization, the key is to calculate the static relative pose between the panel and robot using Eq. (1.1). Multiple points attached on the working panel can be measured to estimate the pose transformation of the panel with respect to the measurement frame, denoted by . Likewise, the method of static pose estimation can be applied to find the . Then, can be

pose of robot base with respect to the measurement, denoted by

calculated conventionally by Eq. (1.1). However, the research reported in Chapter 3 aims at a direct solution of from these point measurements.

Upon the determination of

, the local coordinates of the rivet spots in the panel frame,

denoted by p', can be transferred to those in the robot base frame by Eq. (1.3), based on which the robot can be controlled to reach these desired spots using position-based visual servoing. (1.3) For position-based visual servoing, the key is the state estimation of tip pose from visual sensing using Eq. (1.2). To do so, a target is attached on the robotic tooling as a tracking reference for dynamic pose estimation of robotic tooling tip. Note that a target is a rigid body with multiple observable point features, in particular, artificial markers. Clearly, we have to dynamically estimate the pose of target with respect to the measurement frame based on point measurements, represented by in Eq. (1.2). is the pose from the

10

tooling TCP to the robot end-effector, which can be determined from tooling calibration. is the pose from the robot end-effector to the target 2, which can be calculated by the static pose estimation. However, for flexible manufacturing, panel frame is assumed to be mobile. In other words, may vary during robot operations. Thus, direct state estimation is demanded

for relative pose between rivet TCP frame on the tooling and rivet hole frame on the panel during motion, denoted as with provided state estimation of . Conventionally, and can be calculated by Eq. (1.4)

. In Chapter 4, the reported research of using point measurements

dynamic pose estimation aims at direct state estimation of of targets 1 and 2 on the tooling and panel.

(1.4)

1.5 Outline of Thesis
The remaining thesis is organized as follows. Chapter 2 provides literature review on previous researches on the position-based visual servoing and pose estimation, including static, dynamic, and robust pose estimation. Chapter 3 first summarizes a number of formulation of static pose estimation. Direct solutions of relative pose are derived using least-square methods. Pose estimation is then applied in the localization of the robotic riveting system. Chapter 4 investigates the method of dynamic pose estimation, which addresses the state estimation of relative poses between two rigid bodies during motion using Iterative Extended Kalman filter method. Chapter 5 provides the robust pose estimation for robustness against outliers of point measurements. A relaxation method is proposed for outlier diagnosis using constraints of

11

rigid body. Chapter 6 proposes a data decorrelation method based on multivariate statistical analysis to preprocess the 3D coordinate measurements and find an optimal measurement configuration for calibration. Chapter 7 summarizes the conclusions, contributions, and future work of the research .

12

CHAPTER 2 LITERATURE REVIEW
This chapter describes a literature review on previous researches that have been done on position-based visual servoing and pose estimation, including static, dynamic, and robust pose estimation.

2.1 Pose Estimation
A pose of a rigid body includes a position and an orientation, which describe six degrees of freedom (DOF) in 3D space with respect to a reference frame. Pose estimation for rigid bodies plays an important role in robotics, computer vision, and various positioning applications, such as visual servoing for robotics [14, 21-23], biomechanical motion [24], guidance for aircraft assembly automation [3], spacecraft docking and rendezvous [25], and satellite capture [26, 27], etc. The conventional 3D-to-3D pose estimation deals with one 3D rigid body with respect to an observing system, i.e., the pose transformation after a motion of one body. Estimation is the process of inferring the value of a quantity of interest from indirect, inaccurate and uncertain observations [20]. In general, the variable that is to be estimated can be classified into two categories, namely, a parameter ­ a time-invariant quantity of a static system and the state of a dynamic system, which involves in time according to a stochastic equation. Consequently, two classes of estimation can be classified, namely, parameter estimation and state estimation. As for pose estimation, parameter estimation is referred to the static pose estimation, and state estimation is referred to the dynamic pose estimation. Further, robust pose estimation adds robustness to pose estimation, dealing

13

with pose estimation from outliers contaminated point measurements. 2.2.1 Static Pose Estimation Static pose estimation is the parameter estimation of poses, which is to estimate the static poses based on noisy measurements of point features on rigid bodies. The closed-form solutions for static pose estimation are based on a maximum likelihood criterion or orthogonally constrained total least-square (CTLS) goodness-of-fit criterion when assuming an independent Gaussian noise modeling with zero mean and known variance. There are normally two forms of solutions, namely, batch and recursive LS estimators [20]. The former estimates pose parameters by processing the entire measurement data simultaneously. The latter deals with ongoing measurements and estimation, adopted for sequential rather than batch processing. Nevertheless, they are only suitable for static systems or constant pose estimation, where there is no motion during estimation. Further, due to the nonlinearity, the close-form CTLS solutions of static pose estimation tend to decouple the rotational and translational parameters and normally estimate the rotation first and translation later [28, 29]. There is a very extensive bibliography on the topic of pose estimation from corresponded 3D point sets in many disciplines, such as photogrammetry, computer vision, robotics, and psychometrics. Different terminologies have been defined, namely, absolute orientation [30], registration [31], rigid body motion [32], and orthogonal Procrustes problem [33]. A number of efficient close-form solutions for this CTLS problem have been found and compared; see the surveys by Wrobel [34], Eggert et al. [35], and Williams et al. [31]. The solutions differ in terms of orientation representations and

14

mathematical derivation. Some applied Singular Values Decomposition (SVD) or polar decomposition based on rotation matrix representation [36, 37]. While others applied the eigensystem analysis of matrices based on unit or dual quaternion representation of rotation [30, 38, 39]. In particular, the SVD approach had been further investigated and improved [40-44]. Kanatani [49] stated the theoretical equivalent of the polar decomposition and SVD based methods. Eggert et al. [35] compared these four solutions and concluded no difference in robustness against noise data. Essentially, these solutions are optimal in the least-squares sense under the statistical assumptions that the Gaussian noise on the point measurements is isotropic (equal distribution in all directions) and homogeneous (equal for all points in space) [23]. However, due to the nature of 3D sensors, point measurement error distributions are usually anisotropic (direction dependent) and heteroscedastic or inhomogeneous (point dependent) [46]. Ohta and Kanatani [46] and Matei and Meer [47] both tackled the problem under the general noise conditions with renormalization and multivariate errors-in-variances regression, respectively. 2.2.2 Dynamic Pose Estimation Kalman filter (KF) based state estimation provides a computational tractable solution for dynamic pose estimation [21]. Kalman filtering technique provides a way to estimate sequentially the state of a dynamic system using a sequence of noisy measurements made on the system. It not only considers kinematic motion of system model of rigid bodies but also estimates both rotational and translational parameters together as a state vector. Kalman filter techniques consist of two models: system transition model and observation

15

model. Basically, the system transition model describes the evolution of the state with time; while the observation model maps the state estimate to noisy measurements. When the transition or/and observation models are nonlinear, linearization is applied during filtering. Although Kalman filter was originally derived for a linear problem, the extended Kalman filter (EKF) algorithm was introduced to provide a near-optimal solution for these nonlinear problems [26]. In terms of linearization errors, sufficiently high sampling rate enforces the accuracy of the linearization over the sampling period. However, EKF-based solutions might easily diverge under fast and nonlinear trajectory dynamics, for example, quick changes of the pose, even with a relatively high sampling rate [20]. To solve this issue, iterative extended Kalman filter (IEKF) has been developed to improve the linearization by taking into account the measurements iteratively. Recently, adaptive extended Kalman filter (AEKF) has been investigated for noise covariance adaptability due to the rising robustness requirements [20, 21]. The requirements are mainly caused by non-constant target moving speeds and accelerations, and the measurement vulnerability of traditional planar CCD cameras when illumination condition changes, etc. 2.2.3 Robust Pose Estimation Visual features of a rigid body are observed in two ways, namely, passive and active mode. The passive mode means that illumination provides only for the visualization. The active mode means that artificial illumination provides geometrical constraints. The natural features include edges, corners, colors, textures, etc. When natural features are not accurate or reliable, it leads to the employment of artificial illumination, such as infrared

16

LED markers. A practical problem arises when measurements are contaminated with gross errors, called outliers. Outliers are anomalous observations lying outside the overall error distribution pattern. In reality, 3D measurements are vulnerable in a cluttered, dynamic, and unpredictable environment. Incorrect correspondences may cause outliers due to distractions from reflections, lighting variation, background colours and textures, and even algorithms of feature matching. Besides, sensing failure or invisibility may cause missing data due to obstruction or field-of-view constraints. Missing data is considered and treated as an extreme case of outliers. Some examples are studied in Appendix H. The foregoing reviewed closed-form solutions for pose estimation are all based on a LS goodness-of-fit criterion. Unfortunately, LS based minimization has been well recognized as highly sensitive to outliers, since it tends to accommodate all data with sum of squares of residuals. In this respect, robust estimation has been investigated in the fields of statistics and computer vision over the last 35 years [48, 49]. Rousseeuw et al. [50] and Hodge et al. [51] have reviewed two ways of solution: diagnostics and accommodation. The former detects and removes or replaces outliers prior to further processing. The Mahalanobis distance (MD) approach has been a classical method widely used in multivariate outlier detection [52]. The latter replaces the LS criterion with less sensitive ones to accommodate all data and withstand the effect of outliers, such as L1 regression [53], L norm [54], least median of squares (LMedS), M-estimators, etc. [50]. However, robust estimators are not necessarily the only or even the best techniques that can be used to solve the problems caused by outliers in all contexts, since specialized heuristics for handling occasional outliers appear in specific contexts [48]. 17

With these statistical techniques, researchers have attempted to solve the outlier problem in the field of pose estimation. Siegel et al. [55, 56] first considered the 2D and 3D rigid transformation for a robust fit using the repeated median algorithm in biometrics. Zhang [57] and Pennec et al. [58] attempted to refine the point matching and registration by thresholding data against a MD criterion. Zhuang and Huang [59] also tackled the robust registration problem in an iterative scheme of M-estimator. Boulanger et al. [60] employed an LMedS norm for a robust correspondence estimation between computeraided design (CAD) models and measured points. Rosin [61] also applied the LMedS approach to determine the best minimum subset of point matches for pose estimation. Kumar and Hanson [62] compared the LMedS and M-estimator approaches in camera pose estimation using iterative techniques. Most recently, Enqvist et al. [54] studied the geometric conditions of the L norm to handle outliers in camera pose estimation. However, all these techniques worked in their specific contexts and assumptions and the fraction of outliers were usual low, and a robust and efficient solution of outlier diagnosis has not been provided to preprocess measurements prior to pose estimation of rigid body motions.

2.2 Position-Based Visual Servoing
Visual servoing describes a class of closed-loop feedback control algorithms in robotics for which the control error is defined in terms of visual measurements [63]. It has diverse applications, including industrial automation, assistance in medical operations, control of ground vehicles, airborne vehicles, and robotic grasping. A comprehensive tutorial and review can be found in [64], and a review of more recent developments was discussed in

18

[65]. Basically, the relationship between the robot and camera is often described as moving camera (eye-in-hand) or fixed-camera at the kinematic level. In the former approach, one or more cameras are mounted on the end-effector of a robot and several targets are fixed in the workspace. The goal is to control the view of the camera with respect to targets. In the fixed setup, cameras are fixed and the targets are mounted on the end-effector and work-piece. The camera and end-effector are considered as working at opposing ends of a kinematic chain, and pose of the end-effector is controlled independently of the camera view [63]. In terms of control architecture, two types of techniques were first introduced by [66]: position-based and image-based visual servoing. In position-based visual servoing, the control error is calculated after reconstructing the pose of the end-effector from visual measurements. Joint angles are then driven by the error between the observed and desired pose. Conversely, the control error in image-based visual servoing is formulated directly as the difference between the observed and desired location of features (such as points and edges) on the image plane. Both methods have some disadvantage and advantages. The image-based approach relies on the image Jacobian (or interaction matrix), which transforms an image-space error to a pose or joint-space error. The main drawback is the singularities issue in the image Jacobian and it does not provide an explicit control of pose, which can lead to inefficient and unpredictable trajectories in Cartesian space. On the other hand, position-based visual servoing requires explicit reconstruction of the robot and target pose, and it leads to predictable trajectories and allows simple path planners to be directly incorporated into the controller. The position-based visual servoing is implemented in this thesis.

19

Visual servoing schemes can be further classified as direct visual servo1 or dynamic lookand-move, depending on whether the control law directly generates joint-level inputs or Cartesian set-points for robot's joint-level controllers. For several reasons [64], almost all recent practical systems as well as the scheme proposed in this proposal adopt the dynamic look-and-move approach. One more significant distinction should be made between endpoint open-loop (EOL) and endpoint closed-loop (ECL) [64]. An ECL controller observes both end-effector and targets to determine the control error, while an EOL controller observes only the target. In the latter case, the relative position of the robot is controlled using a known, fixed camera to robot end-effector (hand-eye) transformation. This transformation is established by the hand-eye calibration, which has a significant effect on the accuracy of EOL control; while the hand-eye calibration is not necessary for ECL control, since the pose of the endeffector is measured directly [63]. Moreover, it is always possible to transform the EOL control to ECL control by simply adding the observation of the end-effector. Thus, theoretically, the ECL system appears to be preferable to the EOL system. However, the ECL control requires the visibility of end-effector through the whole process of operation, and it may not be satisfied in the complex working conditions. There are two levels of control dealing with robots, namely, joint level and workspace level. Joint level control applies methodology of inverse kinematics in the control of joint motion of robot manipulators. For current industrial robot manipulators, such as the ABB robot used in the robotic riveting system, joint-level control is already commercially
1

The terminology was introduced by Hutchinson et al. [64] to avoid confusion, since "visual servo", Sanderson and Weiss used in 1980 [66], has come to be accepted as a generic term for any type of visual control of a robotic system.

20

available and thus it is not the focus of the research. Therefore, the control is targeted on the higher level above the joint level - the workspace level control. The workspace level control deals with the control of trajectories or more simply paths of robots without concern of time in the workspace that sends the commands of desired poses to the joint level control. The workspace level control is just analogue to the jogging mode of robots, where users use a teach pendant to jog robot incrementally in either joint or workspace level, such as linear or oriented modes in an ABB robot. In some circumstances, the workspace level robotic control is considered as the dynamic trajectory planning due to the fact that there is no direct control of joints involved and it only dynamically plans the trajectories of robots during motion.

21

CHAPTER 3 STATIC POSE ESTIMATION
This chapter presents the direct solution of static relative pose estimation between two rigid bodies based on four sets of corresponding point measurements using least-square methods. Conventionally, static pose estimation is targeted for an object with respect to a camera. Instead, the goal here is to find the direct static relative pose estimation between two rigid body objects, as described in Eq. (1.1). First, several practiced formulations of solutions are summarized. Second, three levels of least-square methods are discussed for solving the static pose estimation. Furthermore, direct relative static pose estimation is derived for determining the relative pose transformation between two rigid bodies, and compared with the indirect method. As an application, the relative pose estimation is employed in localization of the robotic riveting system.

3.1 Formulation of Pose Estimation
Four commonly used formulations are summarized here for pose estimation, namely, Cartesian frame formulation, three point method, normalized directional vectors, and covariance matrix method. The first and second ones are based on three points. Specifically, first one establishes an orthonormalized frame, and the second one simply formulates non-orthogonal linear systems. The third and fourth ones can be applied to more than three points to enhance estimation accuracy. The adopted formulation is normalized directional vectors method, since normalization reduces the effect of point distribution.

22

3.1.1 Cartesian Frame Formulation As shown in Fig. 3.1, based on three points ( , , and ), a body-attached Cartesian

coordinate system can be formulated using Gram-Schmidt orthonormalization procedure [67], i.e., (3.1) where , , and (3.2)

However, an orthogonal coordinate system is not necessary for determining a rotation. Furthermore, the method does not consider the measurement noise associated with the three points. Therefore, the following sections provide a simpler way by establishing nonorthogonal linear systems based on least-square methods.
p3 m n p2 l

p1

Figure 3.1: Cartesian frame formulation. 3.1.2 Three-Point Method A target with three non-collinear points, whose coordinates can be measured, is utilized to determine the pose transform. The three-point method estimates the pose changes through the initial and final position measurements of three targets, such as light emitting diode (LED) markers. As shown in Figure 3.2, three-point target; while , , and , , and are the initial positions of the

are the final ones measured by certain devices. 23

pf3 l f2 pf2 l f3 pf1 l f1 R t

po2 lo1 po1 lo2 lo3 po3

Figure 3.2: Transformation determinations using the three-point position data [28]. The initial and final three line vectors formed by the three-point target are (3.3) (3.4) Then, the rotation matrix from the initial pose to the final pose can be formulated by [28] (3.5) where (3.6) (3.7) 3.1.3 Normalized Directional Vectors In practice, normally, more than three points are employed. The redundancy of point features helps improve the estimation accuracy in the presence of occlusions. When more than three points are utilized for robustness, normalized line vectors can be constructed as directional vectors for each pair of the points i and j. Then, the rotation relating two corresponded directional vectors can be expressed as [29] (3.8)

24

where (3.9) (3.10) where n is the number of points. calculated by and (3.11) , and are the normalized vector from point to ,

3.1.4 Covariance Matrix Method The covariance method is to adopt statistical characteristics of point patterns to formulate the similarity transformation. The mapping equation system of the rotation matrix can be formulated by (3.12) where (3.13a) (3.13b) and represent the mean value of two point sets and and , (3.14)

Thus, the covariance matrix of two point patterns can be formulated as [40] (3.15) The solution based on the singular value decomposition of the above covariance matrix of the point data is given as a theorem in Appendix D.

25

3.2 Least Squares Methods
When considering the measuring noise in the coordinate data of these points, least square methods should be applied to estimate the optimal values for R in Equations (3.5), (3.8) and (3.12). As a result, three types of least square methods are summarized based on the method of normalized directional vectors in Eq. (3.8):  Least squares (LS) ­ Assuming noisy perturbations only on Df side. , minDf2 From Appendix C.5, the LS solution of R is (3.17) where  is the pseudo-inverse of Do, as explained in Appendix C.3. (3.16)

Total least squares (TLS) ­ Assuming measurement errors exist in both Do and Df. R(Do + Do)  Df + Df, min(DoF + DfF) The result is given in [28]. (3.18)



Constrained total least square methods (CTLS) ­ Assuming measurement errors exist in both Do and Df and subject to the orthogonality of R. R(Do + Do)  Df + Df, min(DoF + DfF), subject to (3.19)

It is shown in Goryn and Hein [42] that the above problem yields a standard orthogonal Procrustes problem in Golub and Van Loan [68] and Appendix C.6, of which the intention is to find an orthogonal matrix R, which most nearly transforms Do to Df in the least-squares sense. The solution is (3.20) 26

where the orthogonal

matrices U and V are given in terms of the SVD of

: (3.21)

This SVD method for determining R has the same form as the one presented in Arun et. al. [36]. To always obtain a rotation matrix (and not a reflection matrix), the modification proposed in Umeyama [40] should be used. As given in Appendix D, the modified solution is

(3.22)

Upon the calculation of R, the translational vector ti for each point can be obtained by (3.23) Then, the vector t for the rigid body can be computed by minimizing (3.24) By taking a derivative with respect to t and setting the derivative to 0, it yields (3.25) Clearly, the optimum solution of t is the mean of translations of all point vectors.

3.3 Direct Relative Pose Estimation between Two Rigid Bodies
This section presents the relative pose estimation of two rigid bodies based on four sets of corresponding point measurements using least-square methods, including LS and CTLS. Two approaches of relative pose estimation are developed and compared, namely, conventional (indirect) and strict (direct) least-square methods. The former is to first obtain individual least-square pose transformation of each rigid body from its two sets of point measurements, based on which the relative pose transformation is then calculated 27

indirectly; while the latter is to formulate a direct transform mapping equation of point measurements for the relative pose transformation, based on which the least-square methods are directly applied to solve the relative pose transformation. The comparison has found and proved the inequality of indirect and direct ordinary LS solutions, and the equivalence of indirect and direct CTLS solutions. Thus, it is safe and acceptable to continue using the conventional indirect CTLS approach of relative pose estimation. Lastly, a case study of a robotic riveting system is provided to validate the result, aiming to estimate the optimal relative pose transformation between a robot end-effector and a work piece. 3.3.1 Motivation of Direct Relative Pose Estimation The estimation of pose transformation of a rigid body from two sets of corresponding point measurements is a fundamental and well-studied problem, as given in literature study in Section 2.2.1. However, the research of the relative pose transformation between two rigid bodies from four sets of point measurements have not yet received much attention. Conventionally, the relative pose transformation between two rigid bodies is indirectly determined by first estimating the pose transformation of each rigid body from its two sets of corresponding point measurements and then multiplying the inverse of one of them with another one, as given in Eq. (1.1). The arising question is whether the indirect solution from the conventional approach is still optimal or not? Therefore, this section aims to derive a strict (direct) least-square solution of relative pose estimation as oppose to the conventional (indirect) approach and then compare them to verify the differences.

28

3.3.2 Problem Statement As shown in Figure 3.3, two rigid body targets with a certain number of point features are observed by a measurement system. Based on the observed point data, the underlying problem is to estimate the relative pose transformation between these two rigid bodies.

Target 2

Point features Measurement Point features

Target 1

Figure 3.3: Two rigid body targets observed by a measurement system. Thus, the following problem can be stated: Let and be two sets of

corresponding non-coplanar points for a rigid body (Body 1) in m-dimensional space; and be another two sets of

corresponding non-coplanar points for another rigid body (Body 2) in m-dimensional space. Note that the 3-dimensional cases, i.e., Body 1 and Body 2, respectively. and . n and k are the number of points on

are the observed coordinate data of points with ; and are the local

respect to a common measurement frame, denoted as

coordinates of points with respective to the body-fixed frames of Body 1 and Body 2, denoted as and , respectively. The objective is to find the relative pose with respective to frame , by minimizing the variance of

transformation of frame

29

estimation errors caused by the Gaussian noises of the data of , ,

and

. Suppose

the Gaussian noises of point measurements are isotropic (equal distribution in all directions) and homogeneous (equal for all points in space). The relative pose transformation can be defined as a matrix, i.e., (3.26) where and are the orthonormal homogeneous transformation

rotation matrix and translation vector, respectively. Scaling factor is set to be one and therefore not considered here. Conventionally, from Eq. (1.1), can be solved by a transform equation as (3.27) where (3.28) (3.29) are homogeneous transformations of frames respect to frame , respectively; and with respect to and with

are the orthonormal , respectively. with

rotation matrix and translation vector of and respect to Based on frame

are the orthonormal rotation matrix and translation vector of , respectively. and , the transform mappings of and from frames and

to

can be described in Eqs. (3.30) and (3.31), respectively. 30

(3.30) (3.31) To solve for and , the rotation mapping equations of body 1 and body 2 can be

formulated using the normalized directional vectors method in Eq. (3.8) as (3.32) (3.33) where i.e., and and , respectively; are based on the corresponding points of body 1, and are based on the

corresponding points of body 2, i.e.,

and , respectively. It is obvious that

, provided that the points of rigid bodies are well corresponded and non-coplanar. However, a direct mapping equation of associated with the observations , , and

is much more appreciated than the conventional solution in Eq. (3.27), in the sense of directly estimating parameters from noisy observations. Therefore, the transform mapping of from its description in frame as (3.34) where is the transform mapping from to , defined as (3.35) which can be characterized in the following transform mapping of from to , to a description in frame can be

described in terms of

31

(3.36) Theoretically, is the inverse of in Eq. (3.28), i.e., (3.37)

Thus, the rotation mapping equations for and (3.39) by multiplying from Eqs. (3.34) and (3.36).

and

can be formulated in Eqs. (3.38) and

with both sides of Eq. (3.33) or decoupling

(3.38) (3.39) where (3.40)

Based on the above conventional and strict formulations of relative pose transformation, the LS and CTLS methods are applied to solve the relative rotation matrix. Also, the conventional and strict solutions of are compared. The objective is to investigate the

differences and find out the reasons behind them. 3.3.3 LS solution Without the error assumption of point data and theoretical solutions of and , the

can be determined in Eqs. (3.41) and (3.42) from the

aforementioned rotation mapping equations in Eqs. (3.32) and (3.33). (3.41) (3.42)

32

Thus, the theoretical solution of

can be derived based on Eq. (3.27) as (3.43)

When considering the independent Gaussian errors in Eqs. (3.32) and (3.33), i.e., and i.e., and , and the number of points is more than m, , the LS solutions of and are given as (3.44) (3.45) where and are the Moore-Penrose (M-P) inverses of and ,

respectively. Since ,

, i.e., have full row rank, we have
.

Thus, the conventional (indirect) solution of

can be obtained based on

and

as

.

(3.46)

However, the conventional (indirect) solution might not be the LS solution for Therefore, the following is to directly derive the strict LS solution of

.

based on its

direct rotation mapping equation in Eq. (3.38) and compare it with the conventional solution in Eq. (3.46). Clearly, the strict LS solution of can be solved by (3.47)

33

From the rotation mapping equation of

in Eq. (3.39), we have (3.48)

where

is the M-P inverse of

.

Substituting Eq. (3.48) into Eq. (3.47), it yields the strict (direct) LS solution of (3.49)

Clearly, it is observed that the strict LS solution in Eq. (3.49) can be quickly achieved by replacing the inverses of and of the theoretical solution in Eq. (3.43) with their . More importantly, the

M-P inverses. Thus, Eq. (3.49) provides a general solution of

differences are observed for the conventional (indirect) and strict (direct) LS solutions of in Eqs. (3.46) and (3.49). Specifically, the inequality of the conventional and strict LS solutions, i.e., , can be proved by applying the following lemma, i.e.,

. This implies that a LS solution of a rotation matrix loses its orthonormal property, i.e., . Therefore, the CTLS solution of

relative pose transformation considering the orthonormal constraint is discussed in the next section. Lemma 1: Let . and and and . , ,

are the Moore-Penrose inverses of and can be

based on their SVD, respectively. An inequality about .

concluded that

Proof of Lemma 1: Suppose

, i.e. .

, then (3.50)

34

Computing the product of

and

reveals (3.51) (3.52)

where

and

includes the singular values of

and

, i.e.,

and

(3.53)

and

(3.54)

(3.55) where denoted by and are orthonormal bases for the row space of , denoted by , respectively. ,

and the nullspace of

Hence, Eq. (3.56) can be obtained from Eqs. (3.50) and (3.51). (3.56) yields that (3.57) which is conflicted with the property of Thus, we prove that (3.58) This concludes the lemma. 35 in Eq. (3.52).

3.3.4 CTLS solution When considering the independent Gaussian errors in both sides of the aforementioned rotation mapping equations in Eqs. (3.32) and (3.33), i.e., and , and the orthogonality of and and , the constrained

total least square (CTLS) solutions of

are given by Eq. (3.22) as

(3.59)

(3.60)

where the orthogonal matrices and , i.e.,

and

,

and

are produced in terms of the SVD of

(3.61) (3.62)

Thus, the conventional (indirect) CTLS solution of substituting the and in Eqs. (3.59) and (3.60) as

in Eq. (3.27) can be obtained by

(3.63)

Next is to directly derive the strict CTLS solution from the rotation mapping equation of in Eq. (3.38). When considering errors in both 36 and and the constraint that the

rotation matrix of

is orthonormal, the problem can be described as follows: , subject to , in Eq. (3.39), the CTLS solution of is given as , (3.64)

From the mapping equation of

(3.65)

where the SVD of

can be obtained based on Eq. (3.61) as (3.66)

Thus, the strict (direct) CTLS solution of

can be determined by

(3.67)

based on the SVD of

by substituting Eqs. (3.62) and (3.65), i.e., (3.68)

where

(3.69)

(3.70) Comparing the conventional (indirect) and strict (direct) CTLS solutions in Eq. (3.63) and (3.67), the equivalence of them can be clearly found due to the fact that the CTLS solutions of rotation matrix maintains the orthogonality, i.e., .

37

Hence, it is safe and acceptable to employ the conventional CTLS approach of relative pose estimation. It indicates that any approach that enforces all required constraints and that is based on the same cost function, must lead to the same result. For verification, a case study of a robotic riveting system is presented in the following section. 3.3.5 Case Studies As shown in Figure 3.4, a robotic riveting system consists of a robot end-effector, a work piece, and a measurement system. In this case, represents the relative pose

transformation between the robot end-effector and the work piece to be estimated for the purpose of precise rivet insertion. represents the pose transformation of the robot endrepresents the pose transformation

effector with respect to the measurement system;

of the work piece with respect to the common measurement system. The underlying problem becomes finding the optimal relative pose transformation of the work piece with respect to the robot end-effector based on the measurements of points on both bodies.

Figure 3.4: Experimental setup of a robotic riveting system. To represent the points on rigid bodies, rigid body targets are attached and fixed on the robot end-effector and the work piece at predefined locations, as shown in Figure 3.5. 38

Each target has four observable points at the ends of four frames. The observed coordinates of points of rigid body targets ( and ) with respect to the frame of the measurement system are given in Table 3.1. The local coordinates of the points ( and

) with respect to the body-fixed frames were pre-measured and collected in Table 3.2. Therefore, the problem of finding the relative pose transformation between the robot endeffector and the work piece becomes obtaining the optimal relative pose transformation between these two rigid body targets.

Robot End-Effector Rigid Body Targets

Work Piece

Figure 3.5: Two rigid body targets on the work piece and the robot end-effector. Table 3.1: Observed Coordinates of Points of Rigid Body Targets w.r.t. the Frame of the Measurement System
Observed Coordinates of Points (unit: m) Bodies Point 1 x Robot end-effector y z x Work piece y z -0.298104 0.106871 -1.678396 0.176478 0.267341 -1.911722 Point 2 -0.254750 0.123163 -1.774297 0.101090 0.308396 -1.974734 Point 3 -0.378223 0.070411 -1.738408 0.238220 0.296363 -1.993495 Point 4 -0.261440 0.008391 -1.696042 0.188659 0.370121 -1.886204

39

Table 3.2: Local Coordinates of Points of Rigid Body Targets
Local Coordinates of Points on Bodies (unit: m) Bodies Point 1 x Robot end-effector y z x Work piece y z -0.000005 0.001074 0.052548 -0.000208 0.000818 0.052497 Point 2 0.000236 0.079929 -0.019033 0.000279 0.079873 -0.018844 Point 3 -0.069540 -0.040248 -0.016784 -0.069691 -0.040136 -0.017044 Point 4 0.069309 -0.040756 -0.016731 0.069620 -0.040555 -0.016610

Based on the observed and local coordinates of points in Tables 3.1 and 3.2, the relative rotation matrix can be determined based on the conventional and strict LS solutions of in Eqs. (3.46) and (3.49), as well as the conventional and strict CTLS solutions in Eqs. (3.63) and (3.67). As shown in Table 3.3, the difference can be noticed between the conventional and strict LS solutions and no difference for CTLS solutions can be observed. The difference of conventional and strict LS solutions can be calculated as . (3.71)

Table 3.3: Relative Rotation Matrix in Experiments (
Relative Rotation Matrix Methods Conventional (Indirect) Solutions ( -0.30576275 LS -0.16915305 0.93696618 -0.30575458 CTLS -0.16937745 0.93692338 -0.77385220 -0.52902878 -0.34821710 -0.77388833 -0.52902079 -0.34818652 )

)
)

Strict (Direct) Solutions ( -0.30576276 -0.16915303 0.93696616 -0.30575458 -0.16937745 0.93692338 -0.77385219 -0.52902878 -0.34821709 -0.77388833 -0.52902079 -0.34818652

0.55468233 -0.83143468 0.03047248 0.55462689 -0.83153369 0.03067129

0.55468232 -0.83143467 0.03047247 0.55462689 -0.83153369 0.03067129

Also, the LS solutions lost the orthogonality of rotation matrix, since the determinant of is about 0.999 and the error of orthogonality is obtained as

40

.

(3.72)

In order to check the accuracy of the LS and CTLS solutions of

, a simulation based

on the experimental data is carried out by adding Gaussian noises to the nominal coordinates of points assuming the calculated CTLS solutions of pose transformation are nominal. The nominal pose transformations of solutions of , , and as , and are given by the CTLS

(3.73)

(3.74)

.

(3.75)

Thus, the contaminated coordinates of points with respect to the measurement system can be simulated by Eqs. (3.76) and (3.77). Tables 3.4 and 3.5 demonstrates the simulated coordinates of points with Gaussian noises, as opposed to the experimental data in Tables 3.1 and 3.2. (3.76) (3.77) where the Gaussian noises of point measurements are assumed to be isotropic and homogeneous and subject to . (3.78)

41

Table 3.4: Simulated Coordinates of Points w.r.t. the Measurement Frame
Simulated Coordinates of Points (unit: m) Bodies Point 1 x Robot end-effector y z x Work piece y z -0.299487 0.105938 -1.679709 0.178792 0.266327 -1.912906 Point 2 -0.255640 0.122051 -1.776592 0.098922 0.306103 -1.973721 Point 3 -0.379557 0.070346 -1.741395 0.240034 0.295649 -1.992492 Point 4 -0.261125 0.008379 -1.696640 0.189335 0.371958 -1.886938

Table 3.5: Simulated Local Coordinates of Points w.r.t. the Body-Fixed Frames
Simulated Local Coordinates of Points (unit: m) Bodies Point 1 x Robot end-effector y z x Work piece y z -0.001320 0.000658 0.053773 -0.000114 0.000440 0.051014 Point 2 0.000193 0.080512 -0.020040 0.000236 0.080834 -0.017106 Point 3 -0.069476 -0.039648 -0.018145 -0.070122 -0.041764 -0.016877 Point 4 0.069657 -0.040938 -0.017671 0.069996 -0.040782 -0.017759

Based on the simulated observed coordinates of local coordinates of and

and

in Table 3.4 and the simulated ) can be

in Table 3.5, the relative rotation matrices (

determined in Table 3.6. Table 3.6: Relative Rotation Matrix in Simulation (
Relative Rotation Matrix Methods Conventional (Indirect) Solutions ( -0.31229 LS -0.16128 0.94755 -0.30035 CTLS -0.17525 0.93759 -0.76069 -0.54042 -0.34769 -0.77032 -0.53511 -0.34679 0.56700 -0.83075 0.00274 0.56249 -0.82640 0.02572 ) Strict (Direct) Solutions ( -0.31216 -0.16132 0.94746 -0.30035 -0.17525 0.93759 -0.76065 -0.54043 -0.34769 -0.77032 -0.53511 -0.34679 ) 0.56688 -0.83069 0.00280 0.56249 -0.82640 0.02572

)

Compared with the nominal rotation matrix (

) in

, the accuracy of LS and CTLS

42

solutions of

in Table 3.6 can be evaluated by (3.79)

where .

As a result, the errors of the conventional (indirect) and strict (direct) LS solutions are different, i.e., 0.0330 and 0.0328, respectively. The errors of the conventional (indirect) and strict (direct) CTLS solutions are equivalent, which is 0.0107. Apparently, the CTLS solutions has the best accuracy among these solutions besides the fact that it also maintains the orthogonality of a rotation matrix. Therefore, both the simulation and experiments have demonstrated the inequality of conventional and strict LS solutions, and the equivalence of conventional and strict CTLS solutions. It indicates that the relative pose transformation should be solved by either the conventional or the strict CTLS solutions. 3.3.6 Summary In this section, the conventional (indirect) and strict (direct) closed-form solutions of the least-square relative pose estimation were presented. To find the relative pose transformation between two rigid bodies from four sets of corresponding point measurements, two approaches have been developed and compared, namely, conventional (indirect) and strict (direct) approaches. For each approach, the ordinary least-square (LS) and constrained total least square (CTLS) have been applied to solve the optimal solutions of the relative pose transformation. From the comparison, the

43

inequality of conventional and strict LS solutions, and the equivalence of conventional and strict CTLS solutions have been theoretically proved and verified by both simulations and experiments. The comparison indicates that the direct solution with constraints lead to identical results of the conventional solution. However, without constraints, the direct solution yields better accuracy compared with the conventional solution. The reason is that CTLS method takes into account the orthogonality of a rotation matrix, i.e., the inverse equals to its transpose. Hence, the relative pose transformation between two rigid bodies should be solved by either the conventional or the strict CTLS solutions. It is safe and acceptable to employ the conventional CTLS approach of relative pose estimation. Moreover, the number of observed points on the end-effector and the work piece might have different influences on the estimation accuracy of . For example, it might be true

that points on the robot end-effector have more influence that those on the work piece. Therefore, an analysis of influence of number of points on the relative pose estimation accuracy may need to be investigated. Also, a more interesting question would be what is the optimal combination of point numbers to have the most accurate result and yet still cost-effective? Table 3.7 demonstrates an example of the combination of point numbers. The objective is to find an optimal combination of numbers of points if existing. Eggert et. al. [35] stated that as the number of points grows, the error in the computed pose transformation approaches a value dependent on the noise level. Table 3.7: Combination of point numbers
n/k 3 4 5 3 (3,3) (4,3) (5,3) 4 (3,4) (4,4) (5,4) 5 (3,5) (4,5) (5,5)

44

3.4 Pose Estimation for Multiple Frames of a Rigid-Body System
This section presents pose estimation for multiple frames of a rigid-body system. For localization of a robotic riveting system, pose transformations need to be estimated between a number of coordinate frames. Such a robotic system consists of both single and articulated rigid bodies. The large numbers and different types of frames add complexity into the procedure of localization. For this reason, the approach is designed to generate corresponding point measurements with respect to each pair of two frames, based on which pose transformation between these two frames can be estimated based on a constrained total least-square method. Lastly, experiments are provided to validate the effective implementation of pose estimation among various frames of a robotic riveting system. 3.4.1 Description of Localization The localization is to find the location of an object in an environment, specifically pose transformation of a body-fixed frame with respect to a reference frame, including a rotation matrix and a translational vector. When a sensing system is utilized for localization, the underlying problem is to find the pose transformation with respect to the sensor's frame. The parameters of pose transformation can be estimated with a CTLS criterion from redundant measurements. More specifically for a position measuring system, redundant measurements are spatial coordinates of multiple observable point features, which are known as targets rigidly attached on the bodies. More specifically for a position sensing system, measurements are spatial coordinates of multiple points, which are known as targets rigidly attached on the bodies.

45

In this section, the methodology of pose estimation is implemented in the localization of a robotic percussive riveting system. The localization needs to be performed for the robot holding a rivet gun and a working panel on the gantry, i.e., and in Eq. (1.1). By

employing a position measuring system, the goal is to estimate the pose transformations between frames of the robot base, TCP of the rivet gun, the working panel, and the measuring system based on point measurements. Once established, the online measurements of the target on the robot tooling and the working panel could be transformed into the robot's frame using Eq. (1.2). Also, the coordinates of points of interests on the work piece could be transformed into the robot's frame for manipulation purpose using Eq. (1.3). It is obvious that the robotic riveting system consists of single and articulated rigid bodies. Single rigid bodies include the working panel, the position measuring system, and rigid body targets; while the articulated rigid body indicates the robot manipulator. Thus, two categories of frames are classified, namely, frames of single rigid bodies with multiple observable point features, and frames of articulated rigid bodies with only one point, i.e., the TCP of the robot. The large numbers and different categories of frames add complexity into the procedure of localization. Basically, the approach is to generate several corresponding point measurements with respect to each pair of two frames, based on which pose transformation between these two frames can be estimated based on the CTLS method using the SVD solution. 3.4.3 Localization of Robotic Riveting System Prior to the pose estimation, two sets of corresponding points need to be generated for

46

frames of both single and articulated rigid-bodies. For a single rigid body with multiple points, the generating approach is straightforward. One set is the local coordinates of points with respect to the body-fixed frame, while another set is the measured coordinates with respect to the sensing frame. For an articulated rigid-body system with only one point, the approach is to move the robot to multiple poses and then measure the coordinate of the TCP. The purpose is to create multiple point measurements at these locations as one set of points, while the corresponding local coordinates of the point are also recorded as another set. In this sense, a virtual target with multiple points is created. Thus, based on the methodology of point generation and the relative pose estimation in the previous section, the details of the implementation in the robotic riveting system are developed as follows, including the working panel, robot, and target localization. 3.4.3.1 Working Panel Localization The working panel localization is to determine the pose transformation of the frame of the working panel with respect to the frame of the position measuring system, denoted by . Three tooling balls with known geometry (1/2"± 0.0002") are installed at three

predefined locations of the working panel, as shown in Figure 3.6. This is a case of a single rigid body with multiple points. Thus, spheral surface digitization of the tooling balls can be conducted with a wireless handheld probe and the position measuring system, as shown in Figure 3.7. There are multiple LEDs on the probe, which provide a reference to calculate the 3D coordinate of tip of the probe. Then, the techniques of sphere fitting can be applied to estimate the central coordinates of the tooling balls with respect to the position measuring system. Based on these two sets of point coordinates of tooling balls and transformation equation in Eq. (3.80), the aforementioned CTLS

47

solution of the pose estimation can be applied to find the optimal pose transformation between the frame of the working panel and the frame of the position measurement system. (3.80)

Figure 3.6: Three tooling balls.

Figure 3.7: Wireless handheld probe for digitization.

Figure 3.8 shows the three tooling balls on the corners of the working panel and the TCP of the rivet gun. The surface digitization of tooling balls has been done with a handheld probe by touching the multiple locations of the surface, as shown in Figure 3.9(a). Sphere fitting was then employed to estimate the central coordinates of the tooling balls, as shown in Figure 3.9(b).

48

Ball 3
TCP of the rivet gun

z y x

Ball 2 p'i pi X

Ball 1

Three tooling balls

Y

Z

Figure 3.8: Panel localization using three tooling balls, and TCP of the rivet gun.

Tooling balls

-1820 -1825 -1830

Handheld probe

-1835 -1840 400 390 360 380 370 340 350 370

(a)

(b)

Figure 3.9: (a) Digitization of tooling balls with a handheld probe; (b) the fitted sphere. Table 3.8: Central coordinates of tooling balls in the sensor and panel frames.
- Measured coordinates in the sensor frame (mm) X Tooling ball 1 Tooling ball 2 Tooling ball 3 357.97 368.958 -120.915 Y 388.437 3.683 -5.52 Z -1830.798 -2379.214 -2382.621

- Coordinates in the working panel frame (mm)
X 670 0 0 Y 0 0 0 Z 0 0 490

By comparing the actual dimensions, the diameter errors of the fitted spheres for three tooling balls were [0.0494 0.0023 0.0488]T mm. The errors of the horizontal and vertical distance between three tooling balls were 0.0129 mm and 0.0290 mm. It proves the precision of the sensing system. Therefore, the fitted central coordinates and the local

49

coordinates of tooling balls were gathered and shown in Table 3.8. Based on these two sets of coordinates, the pose transformation from the measuring system frame to the working panel frame can be calculated by the CTLS techniques,

.

3.4.3.2 Robot Localization For robot localization, the goal is to find the relative pose transformation between the robot base and the working panel, based on which the local coordinates of rivet holes on the working panel from the rivet pattern planning can be transformed to the robot base frame for robotic path planning. To do so, the pose transformation from the robot to the measuring system has to be determined first, denoted by transformation from the panel to the robot, denoted by As the first step of estimating . Then, the relative pose

, can be calculated by Eq. (1.1).

, tooling geometric parameters have to be determined for

robotic TCP calculation, as shown in Figure 3.10. Specifically, the least-square techniques of plane fitting of end-effector flange and calculation of Euclidean distance from the TCP to the plane can be applied to find the tooling geometric parameter, i.e., the distance from the TCP to the flange surface of the robot end-effector by assuming no coaxiality error of the TCP with respect to the end-effector. Without the assumption, the coaxial errors can be found based on cylinder surface fitting. More specifically, principle component analysis (PCA) can be adopted as a solution of the plane fitting when finding the normal vector of the plane. The result was 369.0433 mm. Note that there was an offset of 2.5 mm considering the radius of the probe ball.

50

(a)

(b)

Figure 3.10: Determination of tool length from TCP to end-effector flange, (a) digitizing the surface of the flange of the end-effector; (b) digitizing the TCP of the rivet gun. Later, the length value was inputted into ABB robot's tool database for tool length compensation, as shown in Figure 3.11(a). In our case, the rivet gun tooling length of 369.043mm was along the z axis of the end-effector frame and assumed that the tooling was coaxial with the robot end-effector. The assumption was reasonable based on the design of the tooling.

(a)

(b)

Figure 3.11: (a) Screenshot of tool definition for the rivet gun in the ABB FlexPendant; (b) Pose of TCP of the rivet gun with respect to the robot base frame ( ).

The second step is to create a virtual target at the robot TCP of multiple poses in the workspace, as shown in Figure 3.12. Therefore, two sets of corresponding point sets can be generated for this articulated rigid-body system. The approach is to measure the

51

coordinates of the TCP by the handheld probe and the position measuring system at various locations when the robot is jogged in the workspace, denoted as . In the

meantime, the coordinates of the TCP with respect to the frame of the robot base are recorded at these poses from the robot controller or the ABB FlexPendant, as shown in Figure 3.11(b), denoted as . denotes various joint angles of the robot. (3.81)
Pose 2 TCP Pose 1 Pose 3 Pose i VirtualTarget

pi

3 1 2 i p(i) Tm
Robot Fr
r

Fm

Figure 3.12: Robot localization by creating a virtual target at multiple poses in workspace. Similarly, the aforementioned CTLS solution of the pose transformation can be applied to solve for an optimal estimation of pose transformation of from Eq. (3.81). It should be

noted that at least three locations are required and the more numbers of locations spanning in the workspace the less bias of estimation. The selection of locations should be carefully considered in terms of well representing the working volume, such as the riveting operating volume with respect to the working panel. These two sets of data are shown in Table 3.9. Based on them, the pose transformation from the measuring system frame to the robot base frame can be determined by the CTLS techniques as,

52

.

Finally, the pose transformation matrix from the panel to the robot can be calculated as

.

Table 3.9: Coordinates of the TCP in the sensor and robot base frames.
- Measured TCP coordinates in the sensor frame (mm) X Pose 1 Pose 2 Pose 3 Pose 4 Pose 5 Pose 6 Pose 7 Pose 8 Pose 9 Pose 10 Pose 11 Pose 12 Pose 13 69.825 -87.23 214.587 337.598 -8.611 105.71 251.214 83.427 48.965 193.107 158.138 127.778 25.521 Y -316.292 -334.008 126.688 7.229 -84.016 -103.454 -163.125 118.864 -105.784 -48.501 114.124 -50.006 -114.313 Z -2059.047 -2146.817 -1706.057 -1973.685 -1964.318 -2023.266 -2138.181 -1903.497 -2110.403 -2070.071 -1912.339 -2208.214 -1998.223 X 1197.4 1232.1 1362.3 1417.1 1334.6 1352.4 1369.9 1467.3 1399.6 1424.5 1468.7 1501.2 1328.9

- TCP in the robot base frame (mm)
Y 339.2 418.6 -200.4 90.3 127.8 189 319.9 -36.1 261.6 197.9 -24.2 311.5 173.5 Z 1307.2 1465.7 1152.6 1035 1382.3 1269.2 1125.6 1287.6 1327.4 1182.2 1213.1 1249.6 1348.8

3.4.3.3 Target Localization A rigid body target with observable point features is rigidly attached on the tooling of the robot to provide a tracking reference. Target localization is to find the location of the target 2 with respect to the end-effector of the robot. The target is usually equipped with multiple passive or active markers, such as reflective tapes and LEDs. This is also a case of a single rigid body with multiple points. Similarly, from these two sets of local coordinates and the measured coordinates of the points of the target 2, a pose transformation from the target to the measuring system, denoted by , can be estimated

53

based on the aforementioned CTLS technique of the pose estimation based on . Thus, the pose of the target 2 with respect to the end-effector can be derived by . (3.83) (3.82)

For target localization, a target with four infrared LEDs called a dynamic rigid frame (DRF) is rigidly attached on the tooling, as shown in Figure 3.13. The local coordinates and the measured coordinates of the four LEDs of the target are shown in Table 3.10. Similarly, the pose transformation from the DRF target to the end-effector of the robot can be calculated by the CTLS techniques,

.

Table 3.10: LED coordinates of the target in the sensor and local frames.
- Measured coordinates in the sensor frame (mm) X LED 1 LED 2 LED 3 LED 4 40.38 84.73 -39.63 76.02 Y -243.43 -228.00 -279.13 -342.36 Z -1423.66 -1519.27 -1484.19 -1440.63

- Coordinates in the target frame (mm)
X -0.0051 0.2362 -69.54.4 69.3093 Y 1.0743 79.9293 -40.2479 -40.7557 Z 52.5481 -19.0332 -16.7839 -16.7310

1.35

4 1 2

4 Infrared LEDs 1

1.3

1.25

1.2

3

1.15 0.2 0.1 0 0.8 0.85 0.9

2
0.95 1

3

(a)

(b)

54

Robot Tooling x

z

Target 2 p't2,i y pt2,i X

Target Y

Z

(c) Figure 3.13: Target 2. (a) Modeling (b) Four infrared LEDs. (c) The target on the tooling and transformation between frames of target 2 and measurement. 3.4.4 Verification of Localization For verification of the accuracy of the localization, a procedure was designed as follows. First, a random rivet spot was chosen on the working panel and then digitized the spot with the handheld probe, as depicted in Fig. 3.14. The coordinates were denoted as .

Handheld probe

h

d r

Rivet hole

Panel

Figure 3.14: Hole digitization with handheld probe. Second, the digitized coordinates of the spot were transformed into the robot base frame by . (3.84)

Third, the transformed coordinate was defined as a robot target and programmed in a

55

motion command in the ABB robot. Lastly, the command was executed and checked if the robot TCP was actually positioned at the rivet spot. Then, this procedure was repeated for several random spots on the working panel to ensure the results. It was found that the robot was capable of reaching the digitized rivet spots after the proper localization with an accuracy of 1~2 mm, approximately, which depends on the robot absolute positioning accuracy and localization accuracy. Also, the techniques of the localization with the position sensing system along with the digitization using a probe can be integrated as a robot programming-by-demonstration to replace the conventional programming-by-teaching. The work in this area is ongoing to further analyze the accuracy of estimation and the influence of the number of points on the accuracy. 3.4.5 Summary The pose estimation for multiple frames of a rigid-body system is presented in this section. Two categories of frames are classified, namely, frames of single rigid bodies with multiple observable point features, and frames of articulated rigid bodies with one point. The approach is to generate several corresponding point measurements with respect to each pair of two frames, based on which pose transformation between these two frames can be estimated based on a CTLS method using the SVD. Localization using a virtual target with multiple points is proposed for articulated body systems. The methodology has been implemented for the localization of the robotic riveting system. Pose transformations were estimated between frames at robot base, TCP, working panel, rigid body target, and a position measuring system. Clearly, the method can be extended to other rigid-body systems with single rigid bodies and articulated rigid bodies.

56

CHAPTER 4 DYNAMIC POSE ESTIMATION
This chapter addresses a novel method of dynamically estimating relative pose between two rigid bodies during motion for position-based visual servoing. The conventional way is to estimate pose of each body individually relative to measurement. The proposed approach is to dynamically estimate relative pose of two bodies simultaneously. For robotic riveting, the objective is for dynamic pose estimation between the robot TCP and working panel, as given in Eq. (1.4). For this reason, when directly assigning the relative pose and motion as a state estimate, the observation model is formulated in such a way that the state estimate is mapped to the observed point sets of two bodies. Since this observation model is nonlinear, an analytical expression of linearization is derived and state estimation by iterative extended Kalman filters (IEKF) is adapted to reduce the linearization errors. The proposed approach is implemented in simulations of positionbased visual servoing and validated experimentally on a robotic riveting system for aircraft automated assembly.

4.1 Description of Dynamic Pose Estimation
A relative pose transformation is defined between two rigid body's frames. Instead of tracking each body individually, the work presented in this section addresses the state estimation of relative poses between two rigid bodies simultaneously during motion based on noisy geometric measurements. For measurements, geometric features of rigid bodies can be observed such as points, lines, planes, etc. Among them, point features are the most effective due to easier implementation for processing. In order to measure 3D coordinates of point features in high frame rates for real-time control, a portable optical

57

CMM is utilized by tracking targets of synchronized active LED point markers attached on rigid bodies [19, 69]. It avoids conventional time-consuming processing of computer vision in planar CCD cameras, such as image formation, processing, feature extraction, matching, and registration, etc. In visual servoing, the existing relevant research [21, 22] mainly focused on pose estimation of a rigid body relative to a camera, regardless of configurations of cameras. When a camera is attached to an end-effector of a robot, they provide estimation relative to the camera or the end-effector [14]. When cameras are fixed as standalone devices, they provide estimation relative to cameras. Due to the large size of an optical CMM and the requirement of good robotic tooling accessibility [3], the CMM needs to be fixed outside of the robot's operational space. Specifically for aircraft robotic structural assembly, when one target of point markers is fixed on the robotic tooling and another one on the working aero-structural panel, the underlying problem becomes to estimate the state of relative pose and motion between the robotic tooling and the working panel simultaneously based on point measurements of targets. Therefore, Kalman filtering technique is applied to estimate sequentially the state of a dynamic system using a sequence of noisy point measurements from the optical CMM. It captures both rotational and translational parameters together as a state vector. When directly assigning the relative pose and motion as a state estimate, the observation model is formulated in such a way that the state estimate is implicitly mapped to the observed 3D coordinates of points on the bodies. Since this observation model is nonlinear, an analytical expression of linearization is derived. As mentioned in Section 2.2.2, the recent development of AEKF mainly targets noise covariance adaptability due to changing 58

velocities and accelerations of tracked objects, and the measurement vulnerability of traditional planar CCD cameras when illumination condition changes, etc [21, 22]. However, there is no such need for an optical CMM using active targets and synchronized acquisition, which has the advantage of being not sensitive to external light conditions [70]. Furthermore, the velocities and accelerations of industrial robot are normally kept to be constant. Therefore, the paper adopts IEKF for the state estimation of relative pose from point measurements provided by an optical CMM. In this section, a method of relative pose estimation from four sets of corresponding points based on IEKF is presented during the motion of two rigid bodies. The objective is to simultaneously estimate the relative pose between two rigid bodies using point measurements provided by an optical CMM. By formulating the observation model to map the state of relative pose to the measurements of points, this method applies IEKF as a state estimator. The effectiveness of this method is demonstrated through a case study of a robotic riveting system for aircraft automated assembly.

4.2 Problem Statement
As shown in Figure 4.1, two rigid body targets with certain numbers of point features are observed by a position measurement system of optoelectronic CMM during motion. Based on the observed points, the underlying problem is to dynamically estimate the relative pose and motion between these two rigid bodies. Thus, the problem can be stated mathematically as follows: Let and be two

59

sets of corresponding non-coplanar points for a rigid body (Target 1) in m-dimensional space at a discrete time step of k; and

be another two sets of corresponding non-coplanar points for another rigid body (Target 2) in m-dimensional space at discrete time of k. Note that we focus on 3-dimensional cases, i.e., and 2, respectively. and . n and l are the numbers of points on Targets 1 are the observed sample k of the coordinates of points ; and are the local

with respect to a common measurement frame, denoted as

coordinates of points with respective to the body-fixed frames of Targets 1 and 2, denoted as and , respectively.

Target 2

Point features Measurement Point features

Target 1

Figure 4.1: Two rigid body targets observed by a position measurement system. The objective is to find the state estimate of relative pose transformation of frame with respective to frame and its derivative with respect to time during motion, by

minimizing the variance of estimation errors caused by the Gaussian noises of the point data of and . Suppose the Gaussian noises of point measurements are isotropic

(equal distribution in all directions) and homogeneous (equal for all points in space). The relative pose transformation can be defined as a homogeneous transformation matrix, i.e. 60

, where , and

(4.1) are

orthonormal rotation matrix and translation vector, respectively. As for the orientation representation, Euler angles with a rotation sequence of Z-Y-X axes are adapted instead of the rotation matrix. Thus, the relative pose transformation in Eq. (4.1) can be represented as a relative pose vector, , where , then let , ; (4.2) are the Euler angles for in , i.e., (4.3)

be a matrix function defined on a set of ,

where

(4.4)

(4.5)

.

(4.6)

Thus, from Eq. (4.1) and (4.2), let on a set of in , i.e., . To directly map the relative pose transformation of Target 2, the transform mapping of local points of

be a matrix function defined

(4.7) to the measured points ( ) of to a

from its description in frame

61

description in frame

can be described in terms of ,

as (4.8)

where

is the transform mapping from

to

, defined as , (4.9)

which can be characterized in the following transform mapping of the measured points ( ) of Target 1 from to , . (4.10)

Based on the problem statement, the following is to investigate the methodology of dynamically estimating the relative pose and motion from these four corresponding point sets during motion.

4.3 Relative Pose and Motion Estimation
First, the problem is further modeled as discrete time-varying non-linear systems in state space domain, and the relative pose vector and its time derivative are both captured as a state vector. Essentially, the nearly constant velocity model is assumed for the state transition during motion. Moreover, the observation model is formulated to directly map the state to the point measurements. Second, the IEKF is applied to find the suboptimal estimation of the state based on the non-linear systems. An analytical expression of linearization is derived for the non-linear observation model and the iterative scheme of IEKF minimizes the linearization errors.

62

4.3.1 State Space Modeling When the relative pose vector in Eq. (4.2) and its derivative with respect to time are chosen as the state variables, the problem can be described as discrete time-varying nonlinear stochastic systems in state space domain, i.e., (4.11) , where is the state variables, defined as , and transition matrix. is the time derivative of in Eq. (4.2), (4.13) is a state (4.12)

is a control input with a control gain of is a is a non-linear . The

, detailed in the following section of implementation. measurement vector associated with measured points of B, and observation mapping model from the state of unpredictable disturbances of and

to the measurement of

are assumed to be zeroand

mean white Gaussian noises and mutually independent. Furthermore, are the noise gains for the disturbances of and , respectively.

As for the state transition model in Eq. (4.11), a nearly constant velocity model is assumed during motion. Thus, the state transition matrix can be defined as , where (4.14)

is the sampling time interval. As mentioned, the acceleration is modeled as a , where ,

zero-mean white Gaussian noise, i.e.,

63

, when i  j. Thus, the corresponding noise gain is given by [20] , and the covariance of can be calculated as , where , and , , given by , . (4.17) (4.16) (4.15)

For the observation model in Eq. (4.12), a nonlinear mapping from the state of the relative pose and motion to the measurements of B can be formulated in Eq. (4.18) and (4.19) based on the transform mapping equation in Eq. (4.7) and (4.8).

(4.18)

.

(4.19)

Further, the noise of the measurement is

, i.e.,

, depending

on the precision of the position measuring system. Based on the assumption of isotropic and homogeneous noise, the covariance of is denoted as , where the noise gain is given by . (4.21) (4.20)

64

4.3.2 Relative Pose and Motion Estimation by IEKF Based on the state space model in Eq. (4.11) and (4.12), the IEKF is applied to iteratively compute the mean of the posterior estimate of the state in Eq. (4.22) and then determine its corresponding covariance in Eq. (4.23) upon the convergence of the mean. Eq. (4.22)(4.28) are iterative until successive state estimates converge according to

suitable criteria [20]. The number of iteration depends on the termination criteria. The final results of is taken as the updated state estimate of . Essentially, the

scheme of the IEKF removes the effects of observation model nonlinearity.

(4.22) , where and (4.23)

are the mean and covariance of the prior estimate of

the state, predicted as (4.24) . In Eq. (4.22) and (4.23), (4.25)

is the Kalman gain using minimum mean-square error

as the performance criterion, of which one form is given by , is the measurement residual calculated from Eq. (4.18) and (4.19), i.e. , and is the linear approximation of in Eq. (4.19), derived as (4.27) (4.26)

65

Specifically, from the definition of

in Eq. (4.2) and the derivative of the pose

transformation matrix in Lemma 2, it yields

where . (4.30)

Lemma 2: A homogeneous matrix of pose transformation is defined as be a matrix function defined on a set of . Then, the partial derivative of T in with , i.e., respect

. Let

to

can be derived as

Proof of lemma 2: From the definition of R in Eq. (4.3),

can be reformulated as

66

.

(4.32)

Since

, and

is the angular velocity tensor defined as , (4.33)

where

is the skew-symmetric matrix of

, defined as

,

(4.34)

and the Jacobian of angular velocity in Lemma 3, the differential of R can be derived as . Then, the derivative of R with respect to is (4.35)

Thus, the partial derivative of Eq. (4.32) is derived as

where

Lemma 3: For the rotation sequence of Z-Y-X, the angular velocity can be expressed as [71] , (4.40)

67

where velocity,

is the Jacobian that maps the rate of change of the Euler angles to the angular

.

(4.41)

Proof of Lemma 3: If the rotation sequence is Z-Y-X, then the angular velocity can be expressed as a combination of an individual rotation axis multiplied by the time rate of change of the angles about the axis, . (4.42)

Thus, from Eq. (4.4)-(4.6), the angular velocity can be reformulated in matrix form as

.

(4.43)

4.4 Simulations and Experiments
Based on the aforementioned formulation of the relative pose and motion estimation using IEKF for robot positioning control, a case study of an aircraft robotic riveting system is presented to validate the effectiveness and performance of the methodology. As mentioned, the purpose is to calculate the state estimations of the relative pose and motion between the TCP of the robot tooling and the working panel. Thus, two rigid body targets with four infrared LEDs are rigidly attached on the robot tooling and the working panel serving as references for relative pose and motion estimation. As shown in Figure 4.2, targets 2 and 1 are attached on the robot tooling and the working panel, respectively. The local coordinates of the four LED points of the targets with respect to the body-fixed

68

frames are shown in Table 4.1. Based on the local and measured coordinates of the points of the targets, the relative pose and motion between the TCP of the robot tooling and the working panel are to be estimated dynamically using the aforementioned method.
LED4

x1 z1
LED3 LED1

y1
LED2

(a)
Rigid Body Target 2 y2 z2 x2 Robotic Tooling ytcp TCP ztcp xtcp zp yp xp Working Panel z1 y1 x1 Rigid Body Target 1

(b) Figure 4.2: (a) Four LEDs on the rigid body target 1. (b) Two rigid body targets attached on the robot tooling and the working panel. Table 4.1: Local coordinates of LED points of the targets in the body-fixed frames
- Target 1 (mm) X LED 1 LED 2 -0.2075 0.2794 Y 0.8182 79.8727 Z 52.4971 -18.8438 X -0.0051 0.2362 - Target 2 (mm) Y 1.0743 79.9293 Z 52.5481 -19.0332

LED 3 -69.6914 -40.1362 -17.0435 -69.5404 -40.2479 -16.7839 LED 4 69.6195 -40.5548 -16.6097 69.3093 -40.7557 -16.7310

69

4.4.1 Preliminary Transformation of Target 2-TCP and Target 1-Working Panel To estimate relative pose of TCP for the rivet insertion control, the local coordinates of target 2 in Table 4.1 need to be transformed to the frame of the TCP. To do so, the pose transformation from the optoelectronic CMM system to the robot base is determined as

,

and the pose transformation from the frame of the target 2 to the end-effector of the robot is

.

Since the offset of the robot tooling from the end-effector to the TCP is 369.04mm along the z axis of the end-effector, the pose transformation from the frame of target 2 to that of the TCP is

,

based on which the local coordinates of points of target 2 with respect to the frame of TCP are calculated by . The results are listed in Table 4.2.

Similarly, since the target 1 is attached on working panel, the local coordinates of target 1 need to be transformed to the frame of the working panel. First, the pose transformation

70

from the optoelectronic CMM system to the working panel is obtained as

.

Then, the pose transformation from the frame of target 1 to that of the working panel is determined as

,

based on which the local coordinates of points of target 1 with respect to the frame of the working panel are determined by , as shown in Table 4.2.

In the following, simulations and experiments are presented to verify the convergence and accuracy of the proposed relative pose estimation by IEKF. Table 4.2: Local coordinates of LED points of the targets w.r.t. TCP and working panel
a' - Target 1 (mm) w.r.t. the panel X LED 1 LED 2 LED 3 LED 4 Y Z 537.22 -50.56 184.28 510.46 20.10 259.32 485.94 19.46 122.57 616.93 18.76 169.99 b' - Target 2 (mm) w.r.t. TCP X 55.67 99.12 -26.38 87.75 Y -217.60 -146.18 -149.94 -146.54 Z -265.77 -199.79 -259.39 -338.40

4.4.2 Simulations A simulation of a robotic position-based visual servoing is carried out for the robotic riveting system using the proposed filter of relative pose estimation, as illustrated in Figure 4.3. The controller was designed using a simple proportional control law based on the error of

71

estimated relative pose compared with the desired one, denoted as e(k). Note that the estimated Euler angles are changed back into the rotation matrix as outputs of the state estimator, since the calculation of relative pose error in the controller is based on the rotation matrix. In this case, the control input is the velocity of the robot TCP with respect to the frame of end-effector or TCP, denoted by . Thus, to map the control input to

the state of the relative pose estimation, the control gain in the transition model in Eq. (4.11) can be designed as

(4.44)

(4.45) where is the rotation matrix of the TCP frame with respect to the frame of the is the rotation matrix from the CMM to the panel. is the rotation

working panel.

matrix from the base of the robot to the CMM.

is the rotation matrix of end-

effector with respect to the base of the robot at the time step of k.

X(k)d X(k)

e(k)

Position-Based Visual Servoing Controller

u(k)

Robotic Riveting System

Relative Pose Estimation by IEKF

u(k) A(k) B(k)

Optical CMM

Figure 4.3: Relative pose estimation integrated in the position-based visual servoing of the robotic riveting system. The Simulink model of the position-based visual servoing was designed in MatLab 7.12 72

based on the Robotics Toolbox 9.4 [72], as shown in Figure 4.4. It consists of a controller, a plant of an ABB robot, a vision of optoelectronic CMM, and the relative pose estimation block.

Figure 4.4: Simulink model of the robot position-based visual servoing using the proposed filter of the relative pose estimation designed in MatLab.

As shown in Figure 4.5, the control error is calculated by comparing the estimated values of relative pose transformation of the TCP with respect to the working panel, denoted as 73

, and the desired values, denoted as

, i.e., (4.46)

where

and

are the orientation and translational errors of the TCP, respectively.

Note that the error is calculated with respect to the frame of the TCP.

est: T_ptcp 2

P aTb bTa aTb aTc bTc T P u theta u theta cVc

1 v

tcpTpanel 1 Tpr 3 Ref: T_rtcp*
aTb aTc bTc

tcpTtcp*

Pose2Putheta2

PBcontrol

panelTtcp*

1 s integral

In2

Out1

2 u

Subsystem

Figure 4.5: Position-based visual servoing in the Simulink model. Then, the error of orientation is converted to the representation of the angle-axis parameters by (4.47) (4.48) where error of . is a unit direction vector representing the equivalent axis of the rotation is the rotation angle about the axis with the right-hand rule. In doing so,

the control law of the proportional gain can be applied using Eq. (4.49) to calculate the velocity of the TCP as an input of the robot plant, as shown in Figure 4.6. (4.49)

74

where

is the proportional gain with a tuned value of 0.5.
joint angles 0.26 -0.35 0.52 -0.00 0.79 0.00
In2 Out1

2 dq

derivative ABB4400 q n J jacobn ijacob1 J J -1 J Ji ABB4400 Matrix Multiply 1 s Rate controlled robot axes q T 3 q 1 X fkine simout To Workspace Interpreted MATLAB Fcn cond() 6.339 q manip jac condition plot ABB4400

1 dX

Figure 4.6: Plant of the robot in Simulink model. In the robot plant in Figure 4.6, the control input of TCP velocity is converted to the robot joint space using the robot Jacobian mapping as (4.50) where angles in is the Jacobian matrix function defined on a set of joint , and is the number of the joints of the robot. Basically, to the velocity of the TCP, derived by

maps the robot joint angle rates

where

is the z-axial vector of the ith revolute joint of the robot with respect to the is the

frame of TCP, assuming the joint rotation is about z axis of the joint.

75

position vector from the ith robot joint to the TCP. Then, the pose of robot TCP can be calculated based on the robot forward kinematics from integration of the joint angle rates,

, where

(4.52)

is the robot's geometric or kinematic parameters, including the local . Specifically, is the body vector

body vectors of the links of the robot, denoted as from the end-effector to the TCP. and

are the rotation matrix and is the is the rotation

the position vector of the robot TCP with respect to the robot base. rotation matrix of the ith joint with respect to the matrix of the ith joint with respect to the robot base. th joint.

The Simulink block of the optoelectronic CMM for simulating the point target measurements is illustrated in Figure 4.7. The simulated point measurements are generated by (4.53) . (4.54)

For simulation of robot positioning control, the initial and desired robot joint angles are set as and ,

respectively. The schematic diagrams of initial and desired robot positions are illustrated in Figure 4.8. The D-H parameters of the ABB 4400/45kg robot manipulator are listed in Table 4.3.

76

Tce_act 1 cTr 2 rTe
aTb aTc bTc aTb aP bP cPt

1 cP2

cTe

P2_camera 3 eP2

5 pTc 4 P1_local

aTb bTa

aTb aP

2 cP1

cTp

bP

P1_camera

Figure 4.7: The Simulink model of the optoelectronic CMM to simulate the point measurements. Table 4.3: D-H parameters of ABB 4400/45kg robot manipulator
theta (rad) d (mm) a (mm) alpha (rad) Link 1 0 680 200 Link 2 0 890 0 Link 3 0 0 150 Link 4 0 880 0 Link 5 0 0 0 Link 6 140 0 0

Then, based on the robot forward kinematics, the desired pose of TCP with respect to the robot base is obtained as

.

Thus, the desired relative pose of TCP with respect to the working panel can be given as

.

77

(a)

(b) Figure 4.8: Schematic diagrams of the robot poses in the simulation. (a) The initial position. (b) The desired position. For the filter setup of relative pose estimation, the tuned noise covariance matrices are set based on the noise variances of and
, instead of

null values. The process and measurement noise variances are obtained from the

acceleration range of the robot motion and offline sampling of the optical CMM device, respectively. For simplicity, the initial state value and covariance are both set to be zero. 78

To evaluate the estimation accuracy, the estimated relative poses of the TCP with respect to the working panel are compared with the actual ones calculated from the robot forward kinematics. The trajectories of the actual and estimated relative translational and orientation are illustrated in Figure 4.9(a) and (b), respectively.
Translational Estimation Results 1000
x (mm)

500 0 0 2 4 6 8 10 12 Time (s)

Actual relative position Estimated relative position 14 16 18 20

0
y (mm)

-200 -400 0 2 4 6 8 10 12 Time (s) 14 16 18 20

800 600 400 200

z (mm)

0

2

4

6

8

10 12 Time (s)

14

16

18

20

(a)
Orientation Estimation Results

 (radius)

2 0 -2 0 2 4 6 8 10 12 Time (s) 14 16 18 20 Actual relative orientation Estimated relative orientation

 (radius)

0.2 0 -0.2 0 2 4 6 8 10 12 Time (s) 14 16 18 20

 (radius)

3 2 1 0 0 2 4 6 8 10 12 Time (s) 14 16 18 20

(b) Figure 4.9: Simulation results of dynamic pose estimation. (a) Relative translational estimation results. (b) Relative orientation estimation results. The blue solid lines represent the actual relative poses, and the red dashed lines represent the estimated poses.

79

As shown in Figure 4.10, the norm of error of relative pose estimation converges sharply from over 250 mm to 1 mm at about 0.8 s, and then continues to decline steadily to the required accuracy of 0.1 mm at about 2.5 s. Note that the total simulation time is 20 s and the fix-step size of sampling period is 0.04 s. In the next section, the experiment is provided to further verify the filter with the same system setup and filter initialization.

250

200

Norm of errors: ||e||2 (mm)

150

100

50

0 0 2 4 6 8 10 12 Time (s) 14 16 18 20

(a)
10
2

Norm of errors: ||e||2 (mm)

10

0

10

-2

10

-4

0

2

4

6

8

10 12 Time (s)

14

16

18

20

(b) Figure 4.10: Norm of the error of relative pose estimation during simulation. (a) Linear scale of norm of error values. (b) Log scale of norm of error values.

80

4.4.3 Experiments To further verify the performance of the proposed method, experiments are carried out on the robotic riveting system under the equivalent system setup and IEKF initialization. Note that experiments focus on the relative pose estimation alone without the experimental implementation in the robot position-based visual servoing. First, the points of targets are measured and stored while the robot moves along the predefined linear trajectory with average velocities of 25 mm/s and 0.0154 rad/s. The initial and final positions of the robot with the working panel are depicted in Figures 4.11 and 4.12, respectively. Then, the method is applied offline to process the collected point measurements. Lastly, the estimated and nominal relative poses are compared and the norm of the error is calculated accordingly.

Figure 4.11: Initial position of the robot with joint angles of q0 = [0 0 0 0 10°0].

81

Figure 4.12: Final position of the robot with joint angles of qd = [15°-20°30°0 15°0]. As illustrated in Figure 4.13, it is observed that the performance of the relative pose estimation ensured the convergence to the nominal trajectories. In Figure 4.14, the norms of errors of the relative pose estimation converge dramatically from approximately 800 mm to 25 mm at about 0.8 s, to 9 mm at about 2.5 s, and then decline continuously before settling down stably to around 6.9 mm after about 14.3 s at the end when the robot stops. The bump around 1 s is normal as overshooting happens at the beginning of filtering, mainly due to zero initial value of the state. The final converging errors are 1.7 mm, 3.6 mm, -3.6 mm, -0.6e-3 rad, -3.8e-3 rad, and -1.8e-3 rad. The errors are mainly contributed by the poor robot absolute positioning accuracy caused by the systematic errors, such as deflection due to flexibility and loading, assembly errors due to backlash, and manufacturing errors due to machining tolerances, etc. [73]. That explains the necessity of the proposed dynamic pose estimation. The approach is capable of directly estimating the positioning errors of the TCP with respect to the working panel during motion. Based on the approach, further research of calibration-based iterative learning control (ILC) is being developed to reduce the estimated errors [97, 98]. Also, it could be extended for a 82

wide range of robotic positioning applications for intelligent manufacturing. Further, no control input was enrolled in the experiment as opposed to the simulation with the control input of the TCP velocity, which obviously benefited better motion prediction.
Translational Estimation Results 1000

x (mm)

500 0 0 2 4 6 8 10 Time (s)

Nominal relative position Estimated relative position 12 14 16 18

0

y (mm)

-200 -400 0 2 4 6 8 10 Time (s) 12 14 16 18

800

z (mm)

600 400 200 0 2 4 6 8 10 Time (s) 12 14 16 18

(a)
Orientation Estimation Results

 (radius)

1 0 -1 -2 0 2 4 6 8 10 Time (s) Nominal relative orientation Estimated relative orientation 12 14 16 18

 (radius)

0.4 0.2 0 -0.2 0 2 4 6 8 10 Time (s) 12 14 16 18

 (radius)

2.5 2 1.5 1 0 2 4 6 8 10 Time (s) 12 14 16 18

(b) Figure 4.13: Experimental linear trajectory comparison. (a) Relative translational estimation results. (b) Relative orientation estimation results. The blue solid lines are the nominal trajectory and the red dashed lines are the estimated trajectory.

83

1

Norm of errors: ||e||2 (m)

0.8

0.6

0.4

0.2

0

0

2

4

6

8 10 Time (s)

12

14

16

18

(a)
10
0

Norm of errors: ||e||2 (m)

10

-1

10

-2

10

-3

0

2

4

6

8 10 Time (s)

12

14

16

18

(b) Figure 4.14: Converging results of norms of the errors of relative pose estimation in the experiment. (a) Linear scale of norm of errors. (b) Log scale of norm of errors.

4.5 Summary
In this section, the state estimation of relative pose from point measurements is proposed between two rigid bodies during motion using iterative extended Kalman filter (IEKF). Unlike the conventional way of tracking one rigid body at a time with respect to cameras, the method tracks two rigid bodies simultaneously from point measurements provided by

84

an optical CMM. To do so, when the relative pose and motion are defined as a state vector, state-space discrete time-varying non-linear stochastic systems are modeled consisting of state transition and observation models. Specifically, the observation model nonlinearly maps the point measurements to the state of relative pose and motion. To tackle the state estimation, IEKF is adopted to minimize the linearization errors. The performance of the proposed method has ensured the convergence of the estimation through the verifications of the simulations and experiments on a robotic riveting system. Note that the proposed filter for relative pose estimation merely provides estimated feedback of relative pose errors for the control of visual servoing. Thus, the control of visual servoing is not the focus in this research. The simulation with PD control merely demonstrated the performance of the proposed filter. The experiment showed the filter could dynamically estimate the relative pose errors, which potentially could be used as feedback for control loop. With the proposed method of relative pose estimation, further research has been aiming at the control method of calibration based iterative learning control (ILC) for robotic riveting [97, 98]. The method of calibration based ILC is to reduce the systematic errors that were estimated in the experiment, while the method of relative pose estimation in this research just guarantees the estimation accuracy.

85

CHAPTER 5 ROBUST POSE ESTIMATION
In this chapter, a novel outlier diagnosis method is proposed for robust pose estimation of rigid body motions from outlier contaminated 3D point measurements. Due to incorrect correspondences in a cluttered measuring environment, observed point data are contaminated by outliers, which are unusual gross errors that lie out of an overall error distribution. Standard least-squares methods for pose estimation are highly sensitive to outliers. For this reason, an outlier diagnosis method is developed to preprocess measured point data prior to pose estimation. This diagnosis method detects and removes outliers based on a relaxation method with rigid body constraints of a rigid body. Simulations and experiments prove the effectiveness and advantages of high breakdown point and ease of implementation.

5.1 Problem Statement
As illustrated in Figure 5.1, the cubic structure represents a rigid body and its corners represent attached markers from which 3D coordinate data are measured. Note that and are the 3D coordinates of the ith marker measured at two different poses. Based on , the target is to optimally estimate the pose transformation from frame {B'} to . The solution of was given in Chapter 1. and , static pose

and

{B}, denoted by

However, considering outliers that occur in the measurements

estimation cannot give a robust solution due to the nature of least square fitting criterion. Therefore, an outlier diagnosis of and is proposed and conducted in order to detect

and remove the outliers before pose estimation.

86

Rigid Body Motion

5

6 3
z'

pi
2

B' p TB i p i  Rp  t i

Point markers

5 6
Cubic Rigid Body

4
y'

4

1
x'

1 7
x'

3
z'

2

{B}

7
B TG

Z o X

B' TG

y' {B'}- Local Reference Frame

Y {G}- Global Reference Frame

Figure 5.1: Motion of the cubic rigid body.

5.2 Outlier Diagnosis
During rigid body motion, point measurements must obey rigid body constraints. Based on these rigid body constraints, an outlier diagnosis is developed by utilizing a relaxation method. The principle is to replace rigid body constraints with values into an objective function of outlier diagnosis so as to exact a penalty on the objective if the constraints are not satisfied. 5.2.1 Rigid Body Constraints The rigid body constraint means that distances between points must not change when the rigid body is moved. Thus, the Euclidean distances calculated between point measurements should remain statistically constant or equivalent to the nominal distances when the model of the rigid body is known. Assuming the model is given, the absolute residuals between the calculated and nominal point distances can be determined by

87

 ij  l m ij  lo ij   , for 1  i, j  n, i  j
where

(5.1)

l m ij  p i  p j  p i  p j  p i  p j 
T





12

(5.2)

where

and

are measurements of the ith and jth marker, lmij and loij are the measured and , respectively. The deviation ij must be within

and nominal distances between

the requirement of  to satisfy the rigid body constraints. The value of  is determined by the error model of the distance calculation due to the sensor accuracy of the measuring system. If the model of the rigid body is unknown, the model-free approach can be developed by building the reference distances from a "teaching-by-showing" process. The process is to show the rigid body to the measuring system such that the point distances can be learned and stored as references. Thus, the deviation of the measured distances from the references can still be monitored by using (5.1). Based on the distance deviations, we can find out the candidate outliers. Large deviations out of  indicate the violation of rigid body constraints and the existing of outliers. Thresholding of these errors by  is based on the deviation of point measurement distribution. However, the procedure needs to be robust in order to provide a reliable measure for the recognition of outliers. Geometrically, at least four distances from one point to four other noncollinear and non-coplanar points are required to yield a unique solution of the point's 3D position, as shown in Figure 5.2. Figure 5.2(a) demonstrates that if there are only three or less measured distances identical to the nominal values, it may implicitly embrace outliers. In particular, there may be an outlier of reflection occurred when only three of distance constraints are satisfied. Figure 5.2(b) shows that at least five points are needed since four distance constraints of l12, l13, l14, l15 are capable of 88

locating the unique 3D position of point p1 in Figure 5.2(c). An example of a detected outlier is provided in Figure 5.2(d). Thus, though four points targets were used in static and dynamic pose estimation in Chapters 3 and 4, targets with minimum five points are recommended for the concern of robustness against outliers using rigid body constraints.

p2 p2 l12 p1 p1 p2 l12 l12 p3 l13 l14 p4 p1

p3 l13

(a)
Minimum #: 5

1 2 3 4 5 n

1 2 3 4 5 ... 0 l12 l13 l14 l15 ... 0 l23 l24 l25 ... 0 l34 l35 ... 0 l45 ... 0

n l1n l2n l3n l4n l(n-1)n 0

l12 p2 p1 l15 p3 l13 p5 p4 l14 p2

outlier p3 p1 p'1

p4 p5

... ...

...

Figure 5.2: Point distances under the rigid body constraints. (a) one, or two, or three distance constraints are not sufficient for unique 3D positioning; (b) the minimum number of points is 5 for unique 3D positioning; (c) four distance constraints for point p1; (d) an outlier example. 5.2.2 Outlier Diagnosis by Relaxation Relaxation techniques iteratively assign values to mutually constrained objects in such a way as to ensure that the values remain consistent [74, 75]. In our case, objects are markers of a rigid body mutually constrained by the rigid body constraints defined in (5.1). The mechanism of outlier diagnosis using relaxation techniques begins by replacing the rigid body constraints with compatibility indicators. For 1  i, j  n, a

...

(b)

(c)

(d)

89

compatibility indicator, denoted by ci,j, is defined by
 1, when the rigid body constriant between p i and p j is satisfied    0, no comment nor judgment made  1, when the rigid body constriant between p and p is not satisfied i j 

ci , j

(5.3)

Essentially, ci,j indicates the compatibility of the rigid body constraints between markers pi and pj. In particular, the compatibility for each marker pi itself is defined by ci,i = 1. A merit score (si) is defined and assigned to each marker pi according to how well all other markers pj are compatible with the rigid body constraints between pj and pi when pi is not an outlier. Higher merit scores indicate better compatibility of constraints and less likelihood of outliers, and vice versa. Subsequent iterations use the product of the merit scores sjk-1 of pj and the compatibility of the constraints ci,j in (5.3) to assign a new merit score sik to the marker pi. Marker pj with higher merit score and better compatibility contributes more positive support to the assumption that marker pi is not an outlier, and vice versa. The quantity of total support from all other markers after several iterations provides a robust measure for outlier diagnosis. Therefore, the outlier diagnosis by relaxation involves iterative, "parallel" adjustment of merit scores for each marker on the basis of local outlier evidence, expressed by [29]
1 sik  sik 1   s k j ci , j  n 1 j 1 1  sik 1ci ,i   s k j ci , j  n 1 j 1 1   s k j ci , j  n j 1

(5.4)

The initial values of merit scores si0 can be assigned in a variety of ways. It was chosen to

90

use the number of satisfied rigid body constraints as an initialization, since a priori knowledge of outlier is unknown. Essentially, the original problem of finding outliers among marker measurements becomes to iteratively calculate the merit scores in (5.4) starting from the initial values si0. Here are the procedures developed for the outlier diagnosis by relaxation [29]: i. Calculate the compatibility of rigid body constraints ci,j using (5.1) and (5.3); ii. Assign the initial values of merit scores for each point si0; iii. Iteratively compute the merit scores sik with sik-1 and ci,j using (5.4); iv. Detect and remove the outliers according to the final merit scores. For quantifying the robustness of an estimator, a breakdown point had been defined to measure the fraction of data that can be contaminated without breaking down the estimator [76]. For example, the LS estimator has a breakdown point of 0% since one single outlier can affect the estimation significantly. Compared with other referred robust estimators, such as M-estimator or LMedS method, the advantage of using the proposed outlier diagnosis by relaxation is higher breakdown point and ease of implementation. This method has a breakdown point of more than 0.5, providing that at least three marker matches remain for pose estimation after removing the outliers. Additionally, this method can be easily implemented as long as the observed markers are rigidly moved. Furthermore, the method can be extended to multiple sets of markers that are attached to a number of different rigid bodies. Outlier diagnosis can be carried out simultaneously for these multiple sets of markers. As a downside of the method, the computational complexity for the compatibility calculation in (5.3) is O(n2). However, it is a one-time job prior to the whole iterative procedure of the outlier diagnosis. For the iterations of the diagnosis, the complexity is linear about the number of markers O(n). Nevertheless, the 91

computational cost is always under control since the number of markers is usually userdefined and limited.

5.3 Simulations and Experiments
As shown in Figure 5.3, the experimental system located at the Aerospace Manufacturing Technology Centre (AMTC), is consisted of a Motoman UPJ robot, an Optotrak 3020 system from NDI Inc., and a cubic rigid body target. To validate the proposed robust pose estimation, the cubic rigid body target was attached to the end-effector of the robot and moved by the robot within the workspace. 7 markers were attached to the 7 corners of the cubic rigid body, as illustrated in Figure 5.4.
Markers (Infrared LED) Target ­ cubic rigid body NDI Optotrak 3020 system Motoman UPJ robot 3 Trackers (line CCD sensors)

Figure 5.3: Experimental system set-up. The positions of these markers were measured by the Optotrak system, which is also an optical CMM. With synchronized active markers and three line imaging sensors, the optical CMM systems provide efficient triangulation that enables high frame rates of 3D data delivery. The repetitive position measurements of these markers were recorded for 5 seconds at a frame rate of 100Hz at the two different poses of the robot, as shown in 92

Figure 5.5. Note that marker_1 represents the number one marker, and pose_1 represents the first pose of the cubic rigid body.

(a)

(b)

Figure 5.4: (a) 7 Markers in the 3D space; (b) target of the cubic rigid body.
Pose_2 of the cubic rigid body

Pose_1 of the cubic rigid body

Figure 5.5: Two poses of the cubic rigid body. 5.3.1 Case Study of Four Outliers As an ultimate case study, four outliers were added at marker 1, 2, 6, and 7 of pose_1. Thus, the percentage of outlier is four out of seven, i.e., about 57.1%. These outliers were generated randomly from a mixture error model with Gaussian and uniform noise components, defined as

  (1   ) N (0, 2 )  U (a, b)

(5.5)

where N(0, 2) is a Gaussian noise, with zero mean and a standard deviation of , U(a, b) 93

is a uniform noise, with a bound between a and b, and  is the proportion of outlier in the total noise. Since the data acquired from the experiments had Gaussian noise already, we chose a uniform noise model alone for outlier simulations. From the outlier contaminated data, the pose transformation from pose_1 to pose_2 was estimated, denoted by T_outlier and listed in Table 5.1. To demonstrate the influence of these four outliers, the cube at pose_1 was transformed to pose_2 by the calculated T_outlier. As shown in Figure 5.6(a), the deviation of two cubes indicates the error of pose estimation due to the outliers. Table 5.1: Pose Transformation Before and After the Outlier Contamination
Pose Transformation (T) 0.9975 0.0006 0.0701 0 T_actual -0.0007 -0.0701 1 -0.0011 0.001 0.9975 0 0 -215.554 -3.1334 3.6284 1 0.9989 0.0156 0.0444 0 T_outlier -0.0157 -0.0444 0.9999 -0.0007 -0 0.999 0 0 -161.798 -1.9748 6.1705 1

As mentioned before, the threshold  in Eq. (5.1) depends on the accuracy of the measuring system. In our case, the standard deviations of the Optotrak system are about  = [3e-3, 3e-3, 2e-2]T mm at x, y, and z axis, respectively. Thus, the three-sigma rule leads to a threshold of Euclidean distance deviation, determined by  = 2[(3)T (3)]1/2 = 0.1227mm. Therefore, the compatibility indicators cij can be calculated by Eq. (5.1) and (5.3), as listed in Table 5.2. By counting the number of satisfied constraints, the initial merit scores si0 of 7 markers are 1, 1, 3, 3, 3, 1, and 1, respectively. Table 5.3 and Figure 5.7(a) show the merit scores of all markers after relaxation with 7 iterations. As a result, the outliers (markers 1, 2, 6, and 7) can be detected clearly from the negative sign "-1" of the normalized merit scores in Figure 5.7(b). After removing the outliers from the measurements, the data of markers 3, 4, and 5 are kept for the pose estimation. As listed in Table 5.4, T_removed represents the pose

94

transformation after removing the outliers. The satisfied results are illustrated in Figure 5.6(b), showing two cubic rigid bodies placed together using T_removed. Essentially, this case study has proved the effectiveness of the method and a breakdown point of more than 0.5, providing that at least three markers remain for the pose estimation after the outlier diagnosis.
Markers 3, 4, and 5
Z

5 4 1

Z

Marker 6
-2200

5 4

-2200

-2300

6 3

-2300

6 3 1 2

-2400

-2400

-2500 200

7 2
0 Y -200 -200 -100 X 0

Marker 1
100

-2500 200

7
100 Y 0 -100 -100 0 X 100 200

Marker 7

Marker 2

-200

-200

(a)

(b)

Figure 5.6: Outliers occurred at markers 1, 2, 6, and 7; (a) Pose transformation determined with four outliers; (b) Pose estimation from the remaining three markers 3-5 after removing the outliers. Table 5.2: Indicators of Compatibility - Replacement of Distance Deviation with Integers
Point 1 Point 2 Point 3 Point 4 Point 5 Point 6 Point 7 Indicators of compatibility for rigid body constraints (ci,j) Point 1 Point 2 Point 3 Point 4 Point 5 Point 6 Point 7 1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 1 1 1 -1 -1 -1 -1 1 1 1 -1 -1 -1 -1 1 1 1 -1 -1 -1 -1 -1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 1

Table 5.3: Iterations of Merit Scores for Outlier Diagnosis by Relaxation (Four Outliers)
Iterations 0 - initial 1 2 3 4 5 6 7 Point 1 1 -11 7 -191 -65 -3503 -4673 -67727 Point 2 1 -11 7 -191 -65 -3503 -4673 -67727 Merit scores for point i (si) Point 3 Point 4 Point 5 3 3 3 5 5 5 59 59 59 149 149 149 1211 1211 1211 3893 3893 3893 25691 25691 25691 95765 95765 95765 Point 6 1 -11 7 -191 -65 -3503 -4673 -67727 Point 7 1 -11 7 -191 -65 -3503 -4673 -67727

95

0 -1 1 0 1 5 x 10 2 3 4 5 6 7

marker 1

1

Merit Scores of Points During Iterations (s k ) i 5 x 10

Signs of Merit Scores During Iterations "+1" reliable data; "-1" outlier data 2 0 -2 2 0 -2 2 0 -2 2 0 -2 2 0 -2 2 0 -2 2 0 -2 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7

sk 1

0 -1 10 0 1 4 x 10 2 3 4 5 6 7

5 0 10 0 1 4 x 10 2 3 4 5 6 7

5 0 10 0 1 4 x 10 2 3 4 5 6 7

5 0 1 0 5 1 x 10 2 3 4 5 6 7

0 -1 1 0 1 5 x 10 2 3 4 5 6 7

0 -1 0 1 2 3 4 5 6 7

(a)

marker 7

sk 7

marker 6

sk 6

marker 5

sk 5

marker 4

sk 4

marker 3

sk 3

marker 2

sk 2

(b)

Figure 5.7: (a) Merit scores during 7 iterations; (b) Signs obtained from normalized merit scores, "-1" indicate outliers (markers 1, 2, 6, and 7), and "+1" indicate the reliable data (markers 3, 4, and 5). Table 5.4: Pose Transformation Before and After the Outlier Diagnosis
0.9956 -0.0284 0.0898 0 T_outlier 0.0335 -0.088 0.9979 0.058 -0.0553 0.9944 0 0 -262.963 134.8958 -25.5177 1 0.9975 0.0007 0.0701 0 T_removed -0.0007 -0.0701 1 -0.001 0.0009 0.9975 0 0 -215.3592 -2.9641 3.6335 1

5.3.2 Experiment Verification Two additional markers were attached adjacent to the original markers (markers 3 & 7) for simulating the occurrence of outliers, as shown in Figure 5.8. The measurements of these markers were treated as those of the original markers. As an experimental

96

verification, the outlier diagnosis was applied and markers 3 & 7 were identified, as shown in Tables 5.5 and 5.6.

Marker 7

Markers for simulating outliers Original markers

Marker 3

Figure 5.8: Markers for simulating outliers and the original markers. Table 5.5: Indicators of Compatibility for 7 Markers
Point 1 Point 2 Point 3 Point 4 Point 5 Point 6 Point 7 Indicators of compatibility for rigid body constraints (ci,j) Point 1 Point 2 Point 3 Point 4 Point 5 Point 6 Point 7 1 1 -1 1 1 1 -1 1 1 -1 1 1 1 -1 -1 -1 1 -1 -1 -1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 1 -1 -1 -1 -1 -1 -1 -1 1 Merit scores for point i (si) Point 3 Point 4 Point 5 1 5 5 -25 23 23 -115 165 165 -825 1055 1055

Table 5.6: Iterations of Merit Scores for Outlier Diagnosis by Relaxation (Two Outliers)
Iterations 0 - initial 1 2 3 Point 1 5 23 165 1055 Point 2 5 23 165 1055 Point 6 5 23 165 1055 Point 7 1 -25 -115 -825

5.4 Summary
For robust pose estimation from outlier contaminated 3D marker measurements, a novel outlier diagnosis method has been developed using a relaxation of rigid body constraints. Instead of accommodating outliers in a pose estimator, the presented outlier diagnosis

97

method preprocesses measured data first to detect and remove outliers prior to further pose estimation. The rigid body constraint indicates that marker distances of a rigid body should remain statistically constant after motion. The constraint is chosen as the compatibility criteria of marker measurements. First, the diagnosis procedure by relaxation begins with a replacement of rigid body constraints with integers. Second, a merit score is defined for each marker to represent the possibility of being outliers. Third, an iterative calculation of merit scores is carried out based on the compatibility criteria. Fourth, outliers are clearly distinguished with negative diverge merit scores after the relaxation. Essentially, relaxation techniques iteratively assign merit scores to mutually constrained markers. It involves iterative, "parallel" adjustment of merit scores for each marker on the basis of local outlier evidence. Finally, a pose transformation is estimated from the remaining marker measurements after removing the detected outliers. This novel method has advantages of high breakdown point and ease of implementation. Case studies have proved the effectiveness of the method and a breakdown point of more than 0.5, providing that at least three marker measurement matches remain for pose estimation after removing the outliers. Moreover, this method can be easily implemented as long as the observed markers are rigidly moved. Nevertheless, it provides an easy-touse competitive alternative for robust pose estimation of rigid body motion and offers a simple outlier detection mechanism that can potentially be applied to many other areas.

98

CHAPTER 6 DECORRELATION METHOD FOR MEASUREMENT CALIBRATION
In this chapter, a novel data decorrelation method is proposed for measurement calibration of a portable optical CMM measuring system. The measured 3D data suffer from correlations due to the principle of the triangulation, as described in Appendix F. This nature causes complexity when a calibration procedure is carried out. For this reason, a data decorrelation method based on multivariate statistical analysis is proposed to preprocess the 3D coordinate measurements and find an optimal configuration of CMM, in such a way that each component of the coordinate can be calibrated individually during calibration process. Consequently, the calibration procedure can be significantly simplified in a less time and memory consuming manner. The proposed method includes the local and global data decorrelation. Evaluation and comparisons of these two approaches have been analyzed with a conclusion on the better performance of the global data decorrelation. The optimal decorrelated configuration based on the results has been identified for the measuring system.

6.1 Description of Measurement Calibration
Photogrammetry is the measurement of three-dimensional object coordinates from stereo photographic pictures [77]. The discipline started in the 1880s, but more recently the advent of digital imaging has dramatically changed the accessibility and application areas of photogrammetry [78]. The nature of digital images makes it well suited for real-time applications. Originally used for the measurement of buildings, and later adopted for aerial mapping until 1970s, now the applications encompass a wide range of engineering,

99

medicine, geology, archaeology. Line sensors based photogrammetric measuring systems are recently wide used in optical 3D coordinate measuring machines, especially in modern industry, for its ease of automated integration. With synchronized active targets (also called structured lighting [79]), efficient triangulation with cylindrical lenses and line imaging sensors as well as embedded circuits enables high frame rates of 3D data delivery. However, the 3D data suffer from correlations due to the computation algorithm of triangulation, as explained in Appendix F. Basically, the 3D object coordinates are computed by a spatial intersection of fan beams from the image points through the camera perspective centre into the object space [80]. Thus, each component of the coordinate from the intersection mutually depends on each other's calculation. For the measuring system, calibration is necessary in order to ensure the accurate 3D measurement. In a calibration procedure, a rigid body target needs to be fabricated as a reference object. A number of point luminous markers, such as infrared LED, are attached to the rigid body. The target needs to be displaced at multiple positions with different depths in the 3D measurement volume. Based on the readings from these markers, calibration could be achieved by either identifying the modeled parameters or building a lookup table. The former method with a physical model has been investigated by many researchers [81], [82], [83], which may undergo linearization errors, correlations, and possibility of unmodeled systematic errors. The latter method using a lookup table is straightforward. It stores the ground truth discrete data and needs interpolation among the consecutive values.

100

However, the existence of correlation causes complexity when the lookup table is being built. The table needs to be 3D that costs enormous memory throughout the measurement volume, and speed could be sacrificed during the data indexing, interpolation, and transferring. Assuming the data could be decorrelated and each component of the coordinate could be treated individually, one dimensional lookup table would qualify for storing each axis [69]. For this reason, a data decorrelation method is presented based on multivariate statistical analysis to preprocess the 3D measurements, in such a way that the calibration procedure can be significantly simplified in a less time and memory consuming manner. The network configuration has been defined to represent the geometric arrangement of cameras and targets [79], [84]. A number of literatures have demonstrated the impact of imaging geometry on precision [81]. In this article, the relative configuration of sensors regardless of target is defined as the relative network configuration. For some measuring systems, the relative network configurations are fixed as the whole sensors are packaged into one single unit. In this case, based on the data decorrelation results, the optimal decorrelated configuration of the measuring system with targets can be identified. By setting up with this configuration, the measuring system is capable of providing more accurate data than other configurations. The stability under this configuration is studied as well.

6.2 Decorrelation Formulations
Both local and global data decorrelation are defined and formulated. The two methods employ the eigen-decomposition to explore the structure of the variance-covariance

101

matrix of the measurement data. The difference between the two is the way how the variance-covariance matrix is formulated. The local method is formulated using each individual marker data, while the global method is formulated using a linear combination of the entire set of marker data. Two approaches are evaluated and compared. 6.2.1 Local Decorrelation Assuming that the 3D position measurement of markers complies with a normal distribution, the probability distribution function (PDF) of this multivariate normal distribution can be expressed as [85]
f  X   2 
 p 2

det C 



1 2

  X   T C 1  X      exp   2  

(6.1)

where X = (x, y, z)T; p is the number of variables, in this case, p = 3;  is the 3× 1 column vector of means; and C is the 3× 3 covariance matrix of X. This distribution can be noted as X ~
p(,

C), with the mean vector and variance-covariance matrix expressed as
   x  y  z T
 xy x y 2 y  yz y z  xz x z    yz y z   z2  

(6.2)

2 2  x Cov( x, y ) Cov( x, z )    x    2 C  Cov( x, y ) y Cov( y, z )    xy x y  Cov( x, z ) Cov( y, z )   z2      xz x z

(6.3)

where xy, xz, yz are the correlation coefficients among x, y, and z axes and detailed as
  E[x  x  ]  E ( x )  x
2 x 2 2 2

x 
n

2 i

 x  
i

2

n2

(6.4)

Covx, y   Ex  x  y  y   E ( xy )  x y 

 x y    x  y
i i i

i

(6.5)

n

n

2

 x, y 

Covx, y 

 x y



E ( xy )  x y [ E ( x )  x 2 ][ E ( y 2 )  y 2 ]
2

(6.6)

102

From the eigen-decomposition, C can be factorized as [10]
C  VDV
1

(6.7)

where the columns of V are the eigenvectors of C, i.e., V = (v1 v2 v3); D is a diagonal matrix, with eigenvalues as its diagonal elements, i.e., D = diag(1, 2, 3). C is a symmetric positive definite matrix, which has an orthonormal basis of eigenvectors. Therefore, V is an orthogonal matrix, and (6.7) becomes
C  VDV T

(6.8)

By substituting (6.8) into the exponential part of (6.1), it yields

 X   T C 1  X      X   T VDV T 1  X   
 V T X  V T  D 1 V T X  V T    X     D
T 1





T

 X    





(6.9)

where (6.10) From (6.10), X' is the new set of coordinate values from converting X by VT. Equation (6.9) can be viewed as an ellipsoid defining the surface of constant density of distribution
d 2   X    C 1  X      X     D 1  X    
T T

v 


T 1

T x  v1 1

 2  2 z '  3 x' 1 2  y' 2

1

  v
2

T 2

T y  v2 2

2

  v
2

T 3

T z  v2 3



2

3

(6.11)

1



2



3

where d is a real constant scalar, VT is considered as the rotation matrix, rotating the axes of the ellipsoid to be aligned with the principle x, y, and z axes. Note that no scaling happens during the rotational transformation, since VT is orthogonal. Refer to (6.9), the distribution of the obtained data X' can be noted as X' ~
3(',

D), whose covariance

103

matrix is a diagonal matrix without any cross-correlations. Also, the principle axial directions of ellipsoids correspond to the eigenvectors; the principle lengths are proportional to the eigenvalues. In the light of the relative rotation between sensors and markers, we could transform the coordinate frame of sensors instead of markers' coordinates. The new frame of the measuring system, denoted as e', can be described by [69]
eX   eX

(6.12)

where
e  v1 v3  , and e  e x

v2



ey

1 0 0  ez   0 1 0  .  0 0 1  



With the obtained eigenvectors or the rotation matrix, the sensors could be placed in such an optimal configuration that the 3D coordinate measurements of markers would not suffer from the correlations. The coordinate frames of sensors are aligned with the principle directions of PDF ellipsoids of markers, and the coordinates can be treated independently. For multiple markers, however, since the normal distributions and variance-covariance matrices for each one of them are different, the eigenvalues and eigenvectors are different accordingly. In other words, the optimal configurations for different markers are different as well. 6.2.2 Global Decorrelation As mentioned before, the optimal decorrelated configurations of the sensors vary with different markers. A common optimal sensor configuration is more preferable for the entire set of markers. Therefore, the entire set of marker data could be combined together

104

to form a global covariance matrix. Then, the eigen-decomposition in (6.8) can be applied to find the global eigenvectors for the decorrelation in (6.10). This kind of decorrelation is regarded as the global data decorrelation method, while the above method for each individual marker is the local data decorrelation method. Consider a linear combination of the coordinate vectors of markers [86],

 x1   x2   xn        Y  a1 X 1  a 2 X 2    a n X n  a1  y1   a 2  y 2     a n  y n  z  z  z   1  2  n
where the ith marker's vector is Xi = (xi, yi, zi)T, with a normal distribution of = 3, as detailed in (6.1)­(6.3); and n is the number of markers.
p(i,

(6.13)

Ci), p

When combining the vectors as a column vector, X = (X1,...,Xn)T, the combination in (6.13) can be expressed as
 X1    X  a2 I  an I  pnp  2     X   n  np1

Y p1  AX  a1 I

(6.14)

where A = (a1I, a2I, ... , anI), and I is the 3× 3 identity matrix. The covariance or correlations of the combination Y are considered as the global covariance or correlations; while the local covariance or correlations are defined for Xi. The np× 1 column vector X has a multivariate normal distribution of
np(,

C), where   1  2   n T and
 Cov X 1 , X n   Cov X 2 , X n        Cn 

C1 Cov X 1 , X 2   Cov X , X  C2 1 2 C        Cov X , X Cov X 1 n 2, Xn  

(6.15)

Then, the distribution of the combination Y can be determined by

p(A,

ACAT), i.e.,

105

p(  a i  i ,
i 1

n

 a a Cov( X , X ) ), where Cov(Xi, Xi) = Ci.
n n i j i j i 1 j 1

Statistically, assuming these vectors Xi are mutually uncorrelated, i.e., Cov(Xi, Xj) = 0, Equation (6.15) becomes
C1 0 0 C 2 C    0 0  0  0       Cn 

(6.16)

Then the distribution of Y becomes

p(

 a  ,  (a
i i i 1 i 1

n

n

2 i

C i ) ).

Specifically, when ai  1 n , i
1 n

= 1,...,n, or the vectors are uniformly combined, the distribution becomes
1 n  Ci n i 1

p(


i 1

n

i

,

). In this case, the components of the global covariance matrix are the mean values

of those of each marker. Substituting with (6.3), it yields

 2  xi   i 1 1 n 1 n Cov(Y )   C i   Cov( xi , y i ) n i 1 n  i n1   Cov( xi , z i )  i 1
n

  n  2  yi    i 1  n n 2  Cov ( z , y )    i i zi i 1 i 1  

(6.17)

where
m 1 n 2 1  m 2 2   xi  x1i  x1     xni  xn    n i 1 m  n  i 1 i 1







   

(6.18)

and m is the number of measurements for each marker. Applying the eigen-decomposition, the global covariance matrix in (6.17) can be diagonalized and the eigenvectors V can be obtained. Also, the global correlation coefficients of x and y for the combination can be calculated by 106

 ( x, y ) 

Cov( x, y )


x n



 Cov( xi , yi ) n
n i 1

y


n i 1 i i

2 xi

n
n


n i 1 i n

 n
n

 Cov( x , y )
n i i i 1 n

2 yi

 
2 xi i 1 i i


n i 1

2 yi



 E ( x , y )  x y   E ( x , y )   x y
i i i 1

(6.19)

 
n 2 xi i 1


n i 1



i

i 1

i 1

2 yi

 
2 xi i 1


n i 1

2 yi

From (6.6) & (6.19), when (x,y) = 0, or

 E( x , y )  x y
i i i

i

, it does not necessarily mean

(xi,yi) = 0, or E( xi , yi )  xi yi ; but not vice versa.

6.3 Evaluation and Comparison
For a comparison, the local and global data decorrelation methods are evaluated in terms of the root mean square (RMS) values of the local correlations coefficients of X, described as
CorrCoef _ RMS 

  
N N 2 ij i 1 j 1

(6.20)
1

where ij is the markers' local correlation coefficients, with {i, j = n × p}.

| 1  i, j  N, i  j, N

6.4 Experiments and Simulations
As an experimental validation, the data decorrelation of the 3D position readings from the Optotrak 3020 system located at AMTC was conducted with the proposed method. As shown in Figure 5.3 & 5.4, the experiment system consisted of a Motoman UPJ robot, the Optotrak system, and a target of a cubic rigid body, which was designed to form a cubic structure. In the experiment, 7 markers (infrared LED) were attached to the rigid body.

107

The locations are the 7 corners of the cube. The Optotrak device has three line CCD sensors packaged into one single unit. The relative network configuration is fixed and arranged in a way for the spatial intersection. The maximum frame rate of the system is 3500/(n+1), about 437 Hz in this case (n = 7).

Figure 6.1: 500 repetitive readings of marker_1s' constant 3D position at pose_1. The repetitive position measurements of these markers were recorded for 5 seconds at a frame rate of 100Hz at the five different poses of the robot. Figure 6.1 shows the 500 measurements of marker_1 at pose_1. From these data, variance and correlation of coordinates have been calculated, with the maximum amounts of variance at z axis (the viewing or depth direction) and correlation at y and z axes. Figure 6.2 shows the standard deviations and the average values on x, y, and z axes are 0.003, 0.0025, and 0.0151 mm, respectively. Clearly, the value of deviation on z axis is about 5 - 6 times of those on x and y axes, since z value is calculated upon x and y values and the errors are accumulated. The average values of the correlation coefficients of xy, xz, and yz axes are 0.2431, 0.3274, and 0.5443, respectively. Wing-shaped curve of the correlations has been found as shown in Figure 6.3. All of these indicate that 3D point measurement noises are

108

usually anisotropic (direction dependent) and heteroscedastic or inhomogeneous (point dependent), due to the nature of 3D sensors.

Figure 6.2: Standard deviation (STD) of marker's positions.

^j  x, y ^j  x, z ^j  y,z

Figure 6.3: Mean absolute correlation coefficients of 5 poses for 7 markers. Figure 6.4(a) and (b) show the comparison of ellipsoids of 1, 2, 3 of the original and the decorrelated data. As mentioned, the principle axial directions and lengths of ellipsoids are defined by the data's covariance matrix. In Figure 6.5(a), the local decorrelated configurations of NDI sensors are shown in red, with respect to the corresponding ellipsoids of the markers. The original configuration is shown in cyan. Note that there are slightly differences of NDI configurations for each marker according to the different eigenvectors. Figure 6.5(b) shows the mean and standard deviation values before and after the local data decorrelation.

109

(a)

(b) Figure 6.4: Comparison of ellipsoids of 1, 2, 3 of the original (a) and the decorrelated data (b).

110

(a)

(b) Figure 6.5: (a) Local optimal decorrelated configurations of the NDI Optotrak system (in red) with respect to the PDF ellipsoids of 7 markers, the original NDI configuration is in cyan; (b) mean and STD of measurements before and after the local decorrelation. The global data decorrelation is carried out with the assumption of whether the measurements of markers are mutually uncorrelated or not, as shown in Figure 6.6. It is shown that when mutually correlated in Figure 6.7, the ellipsoid has a larger size than that

111

of Figure 6.6, due to the larger eigenvalues when the covariance matrix accounts for the mutual correlations.

Figure 6.6: The global decorrelated configuration of NDI for the entire set of markers (in red), when the marker data Xi are mutually uncorrelated.

Figure 6.7: Results when the data of the 7 markers are mutually correlated, with a larger size of ellipsoid due to larger eigenvalues.

112

0.6 RMS of Local Correlation Coefficients 0.58 0.56 0.54 0.52 0.5 0.48 0.46 0.44 0.42 0.4
Local_1 Local_2 Local_3 Local_4 Local_5 Local_6 Local_7 Global_UC Global_C

Decorrelation methods Original Noised (+30%) Decorrelated Noised (-30%) Noised (+10%) Noised (-20%) Noised (+20%) Noised (-10%)

Figure 6.8: Local and global data decorrelation comparisons, with various noisy levels from ± 10% to ± 30%. "Local_1" to "Local_7" mean the seven local data decorrelation methods at each markers. "Global_UC" represents the global data decorrelation when mutually uncorrelated; "Global_C" indicates the global data decorrelation when mutually correlated. Figure 6.8 shows the RMS values for the local and global data decorrelation methods. Various noisy levels, ± 10%, ± 20%, and ± 30% of the sensor configurations are applied for the sensitivity evaluating purpose. In Figure 6.8, "Local_1" to "Local_7" represent the local data decorrelation methods at the 1st to the 7th markers, respectively. Thus, there are totally 7 local decorrelation methods. "Global_UC" stands for the global decorrelation when Xi are mutually uncorrelated; while "Global_C" stands for the global decorrelation when Xi are mutually correlated. Without noise, the RMS values for the 7 local and 2 global methods are 0.462, 0.4291, 0.4602, 0.5519, 0.4181, 0.4498, 0.5861, 0.4151, and 0.4105, respectively. Clearly, the global decorrelation methods have lower values compared with the local methods. The global method when Xi are mutually correlated has

113

the lowest value, corresponding to the fact of existence of the mutual correlations of Xi. Also, noises have impact for all of methods, especially for some of the local methods, such as local_1 and local_2. As a conclusion, the global decorrelation method when Xi are mutually correlated is the best solution to perform the data decorrelation and obtain the optimal decorrelated sensor configuration.

6.5 Summary
For eliminating the correlations of the multiple markers' readings, the local and global data decorrelation methods have been presented based on the multivariate statistical analysis. For both methods, the eigen-decomposition is employed to explore the structure of the variance-covariance matrix and then the eigenvectors are assigned as the rotation matrix to align the principle axial directions of the ellipsoids, which are the surfaces of constant density of distribution. The difference is how to formulate the variancecovariance matrix. The local method formulates it from the each individual marker data; while the global method formulates it from the linear combination of the entire set of markers data. Evaluation and comparison under various noisy levels has proved a better performance of the global data decorrelation method in terms of the root mean squares values of the local correlation coefficients of markers. The RMS value has been reduced from 0.5525 to 0.4105, with a percentage of about 25.7%. The optimal decorrelated configuration for the measuring system has been identified based on the results. In such a configuration, the calibration of 3D measurements could be simplified by using one dimensional lookup tables.

114

CHAPTER 7 CONCLUSIONS AND FUTURE WORKS
7.1 Conclusions
In this dissertation, pose estimation has been studied for position-based visual servoing with an application in robotic percussive riveting. The challenges of accuracy in positionbased visual servoing include localization errors, poor absolute positioning accuracy of the robot, and measurement errors. For this reason, three categories of pose estimation have been investigated pertaining to these challenges, namely, static, dynamic, and robust pose estimation. In static pose estimation, direct solutions of relative pose between two rigid bodies estimated from noisy point measurements are derived based on least-square methods. The inequality of conventional and direct LS solutions, and the equivalence of conventional and direct CTLS solutions have been found. Also, the results of static pose estimation are applied in localization of robotic riveting. In dynamic pose estimation, state estimation of relative pose between two rigid bodies have been developed based on iterative extended Kalman filter. Measurement errors have been tackled by outlier diagnosis for robust pose estimation, and a decorrelation method for measurement calibration. For measurement calibration, a decorrelation method is proposed for finding an optimal measurement configuration at which calibration can be conducted for each axis of measurement frame with less effect of correlation. The methods of pose estimation have been validated with simulations and experiments of a robotic riveting system.

115

7.2 Contributions
 Static, dynamic, robust pose estimation were systematically studies for precise rivetin-hoe insertion of the developed robotic percussive riveting system.  Different formulations of static pose estimation from points have been summarized, namely, Cartesian frame formulation, three point method, normalized directional vectors, and covariance matrix method. Normalized directional vectors was chosen due to no effect of point distribution.  Direct solutions of static relative pose estimation based on least-square methods have been derived with and without the constraint of orthogonality for rotation matrix. The direct solution with constraint has been proved to be identical to the conventional indirect method. However, the direct solution without the constraint has been proved to be different from the conventional solution.  Static pose estimation have been extended for localization of multiple frames of rigid body systems, including articulated rigid bodies, i.e., robot manipulators. A virtual target was proposed to align the robot frame and measurement frame. The virtual target consists of multiple points of the TCP of the robot at different poses within the workspace.  Dynamic relative pose estimation has been shown to address a state estimation of relative poses between two rigid body during motion. The method tackles the issue of poor absolute positioning accuracy of the robot by estimating relative pose between robot TCP and working panel, which has been verified in simulation and experiments.  For robustness against outliers of point measurements, an outlier diagnosis is shown based on the relaxation of rigid body constraints of points. This novel method has advantages of high breakdown point and ease of implementation. The method has been verified by both simulation and experiments.  A decorrelation method is proposed for measurement calibration using multivariate statistical analysis to find a optimal sensor-to-target configuration. As a result, each 116

coordinate measurement is close to uncorrelated and it allows a simple calibration using one dimensional lookup table.

7.3 Future Works
In static pose estimation, four formulations may be compared more thoroughly in terms of estimation accuracy. Also, the effect of number of points on static pose estimation accuracy may be of interest to be evaluated. For the considered relative pose problem, the direct formulation does not offer any redundancy in the provided input that might be exploited further in order to obtain an even optimal solution in future. In dynamic pose estimation, the IEKF based algorithm was based on known shape of the targets, i.e., availability of A' and B', homogeneous noise, assumption that the targets are within field of view and not features occlusion happens. As a future work, the algorithm may be extended to provide a general solution to the relative pose state estimation. In addition, more experimental data may need to carried out to ensure the more reliable performance of the proposed algorithm, by considering longer trials with more challenging motion sequences. Also, a comparison of this work with respect to the single rigid body pose estimation algorithms might be of interest to be evaluated. For the decorrelation method for measurement calibration, although real measurement data were used in the decorrelation experiment, it would be interesting to actually build lookup tables after the measurement system is reoriented at the global optimal configuration. To fully integrate the proposed methodologies, software development of an overall interface needs to be further carried out with details that can be found in Appendix G. 117

APPENDIX A - POSITION AND ORIENTATION
In this section, the position vector and rotation matrix are introduced to represent the pose of each module.
Module n Tip Joint n-1

Module n-1 Module i Module 2 Module 1 Joint 1 Joint 2 Joint i Joint n

Base frame

Figure A.1: Multiple modules system for a serial robot manipulator.

A.1 Position Vector
The position of a point in space is represented with respect to a coordinate frame using a vector. In general, the vector components in the Cartesian coordinate are expressed as p = [p1, p2, p3]T (A.1)

A position vector, as shown in Figure A.2, can be expressed in terms of the frame axes in linear combination as p=p1e1+ p1e1+ p1e1 or p=Ep (A.2a) (A.2b)

where E = [e1, e2, e3]. In the Cartesian coordinate system, E = [x, y, z], and x, y, z are the unit vectors along x, y, and z axes relatively, that is, x = [1, 0, 0] T, y = [0, 1, 0]T, z = [0, 0, 1]T.

118

e3'

e3

p

e2' e1 e1' e2

Figure A.2: Position vector p [71].

A.2 Rotation Matrix
A rotation matrix represents a linear transformation between two coordinate frames. In Figure A.2, position vectors can be expressed either in frame {e1, e2, e3} (Equation A.2a) or frame {e'1, e'2, e'3}, p = p'1e'1+ p'1e'1+ p'1e'1 or p=E'p' (A.3a) (A.3b)

Since Equations (A.2) and (A.3) represent the same position vector, so E'p' = Ep It leads to p = Rp' where R is the rotation matrix given as R = ET ·E' (A.6) (A.5) (A.4)

Since ei is orthogonal to ej, then ei· ej = ij = 1 when i=j; ei· ej = 0 when ij. Hence, E is orthogonal, and E-1 = ET. R is in fact defined by the dot product of two unit vectors, i.e., the direction cosine. It is

119

also called the tensor product, defined as

 e1  e '1 e1  e '2 R  (E  E ')   e2  e '1 e 2  e '2  e3  e '1 e3  e '2

e1  e '3  e 2  e '3   e3  e '3  

(A.7)

Reversing the order of Equation (A.4), Equation (A.5) becomes p' = R'p where R' = E'T · E Obviously R' = RT = R-1 (A.10) (A.9) (A.8)

Hence, R is orthogonal, and such that all columns are mutually orthogonal and have unit magnitude. In fact, it is proper orthogonal, meaning det(R) = 1. It is clear that the nine elements of a rotation matrix are not all independent. Six dependencies or constraints between the elements can be easily found from a given rotation matrix, R   X Y Z :

X 1 Y 1 Z 1
(A.11a)

XY  0 XZ  0 YZ  0
As a result, three independent parameters representation is developed in the following section in order to express the rotation matrix conveniently, using the angle-set convention. 120 (A.11b)

A.3 Angle-set Representation of a Rotation
There are basically two methods to describe the orientation of a frame relative to a reference frame by angle-set conventions. According to Craig [94], one is Euler angles, and one is fixed angles. In the former representation, each rotation is performed about an axis of the moving coordinate system rather than one of the fixed reference frame. For the latter one, each of the three rotations takes place about an axis in the fixed reference frame. For each method, 12 sets of conventions are employed according to different sequence of rotation about the X-Y-Z axes. One of them is introduced in details in this section, and the rest can be found in [94]. Usually, there is no particular reason to favor one convention over another, but various authors adopt different ones [94]. The following PRY angles (or X-Y-Z Euler angles) are applied in this thesis. (Tait) Bryan Angles (Pitch Roll Yaw, PRY) [or X-Y-Z Euler angles] In terms of pitch, roll, and yaw angle (PRY) [71], the three individual rotation matrices can be given as:

R x 

0 0  1  = 0 cos x -sin x     0 sin  cos x x 

(A.12)

R y 

 cos y 0 sin y    = 0 1 0  -sin y 0 cos y   

(A.13)

121

R z 

cos z -sin z 0  =  sin z cos z 0  0 1  0 

(A.14)

Then, the resulting rotation matrix in the global reference frame is given as:

R = R  x  R   y  R  z 
If the order is reversed, it will become the rotation matrix in the local frame

(A.15)

RT = RT  z  RT   y  RT   x 
Expanding Equation (A.15) leads to

(A.16)

 c y c z -c ys z s y    R =  s x s y c z +c x s z -s x s ys z +c x c z -s x c y  -c x s y c z +s x s z c x s ys z +s x c z c x c y   

(A.17)

When given three PRY angles, Equation (A.17) can be used to compute the rotation matrix directly. For the reverse problem, some elements in the Equation (A.17) are selected to determine the PRY angles for a given a rotation matrix, for example:
2 2 12  y = cos-1  r23  +  r33         

 x = cos-1  r33 cos y 
z = cos-1  r11 cos y 

(A.18)

where r23 = -sin(x)cos(y), r33 = cos(x)cos(y), r11 = cos(y)cos(z). Different selection of elements from the given rotation matrix leads to different method for a solution. Bai and Teo [95] developed another solution using atan2(y, x), a twoargument arctangent function that uses the signs of both x and y to identify the quadrant

122

in which the resulting angle lies:
 x = atan2  -r23 , r33 

 y = atan2(r13 , -r23sin( x ) + r33cos( x ))
z = atan2  r21cos( x ) + r31sin( x ), r22cos( x ) + r32sin(x ) 

(A.19)

However, none of them is capable of solving this inverse problem with a unique solution from a rotation matrix in all four quadrants, which is called the quadrant sensitivity problem. Equation (A.18) is only valid for 0  y  90, 0  x  180, and 0  z  180; while Equation (A.19) is valid when y lies in the first and fourth quadrants. In other words, if y is located in the second and third quadrants, the values of cos(y) would become negative. As a result, the two elements r23 = -sin(x)cos(y) and r33 = cos(x)cos(y) in Equation (A.19) change signs, and the values of x determined from atan2(r23, r33) are no longer true. So by modifying Equation (A.19),

cos y = r232 + r332 = (-s x c y ) 2 + (c x c y ) 2 = c y 2
then y can also be determined by

(A.20)

 y = atan2 (r13 , r232 + r332 )

(A.21)

Even though a second solution exists, by using the positive square root in Equation (A.20) for y, we always can compute the single solution for which -90  y  90, making it a one-by-one mapping orientation representation. Attention should also be paid to the names pitch, roll and yaw angles, since they are often given to other related but different angle-set conventions; for instance, it is referred to the X-Y-Z fixed angles in Craig [94].

123

APPENDIX B - GENERAL MOTION
This section presents the forward kinematic modeling of a serial robot with rigid bodies. The topic to compute the position and orientation of the end-effector relative to the base coordinate as a function of the joint variables.

B.1 General Motion of a Single Rigid Body
As shown in Figure B.1, the general motion of a single rigid body is the combination of rotation and translation, and the position vector p is p = Rb' + h = h + Rb' (B.1)

where h is the vector of translation, b' is the body vector in local coordinate system, and R is the rotation matrix.

h Rb' b ' p h

Rb'

b '

Rotation first Translation second Translation first Rotation second

Figure B.1: General motion of a single module [71]. Clearly, Equation (B.1) is communicative, meaning the order of rotation and translation can be reversed, which can also be found in Figure B.1. When h is null, it becomes pure rotation.

B.2 General Motion of Multiple Rigid Bodies
Figure B.2 shows the vector method used to compute the position of a multi-module 124

system.
bn-1 Jn Jn-1 bi Ji b2 b1 J1 b0 J2 pi p n+1 Tip bn

Base frame

Figure B.2: Vector method for a multi-module system [71] The position of each joint from 1, 2 to i can be expressed respectively as: Joint 1 Joint 2 Joint 3 ...... Joint i pi = b0 + R01 (b'1 +...+ Ri-1 i b'i) = pi-1 + R0 i-1 b'i-1 (B.2) p1 = b0 p2 = b0 + R01b'1 = p1 + R01b'1 p3 = b0 + R01 (b'1+R12b'2) = p1 + R01 R12 b'2

where Ri-1 i defines the rotation between two coordinate systems attached to two adjacent modules i-1 and i; b'i is the local body vector, representing the translation between two coordinate systems, or defining the ith joint to the (i+1)th joint in the ith local coordinate frame. Clearly, Equation (B.2) is a recursive method for computing the position of a multimodule system. Similarly, the recursive method of computing rotation can be given as R0i = R01R12...Ri-1i = R0i-1Ri-1i 125 (B.3)

Hence, in general, the pose (position and orientation) of the end-effector of a n-module system can be expressed as [71] Position
p n 1   R 0i b 'i   bi
i 0 i 0 n n

(B.4)

Orientation

R 0 n   R ( j 1) j
j 1

n

(B.5)

As for a robot system, it usually has the default home configuration or initial configuration setup. Therefore, it should be noted that all the employed parameters here may have static part and motion part. The static part is according to the initial configuration setup, and the motion part represents the movement of each joint. R = R s Rm b = bs + bm (B.6) (B.7)

where Rs and bs are initial configuration setup, which are the geometric parameters need to identify; and Rm and bm are related to active joints, i.e., motors. The static part can be further expressed by the PRY angles rotation as Rs = RxRyRz bs = bxx + byy + bzz (B.8) (B.9)

where Rx, Ry, and Rz are the rotation about x, y and z axis of the configuration setup; bx, by, and bz are the translation along the x, y, and z axe of the configuration setup. In terms of different kinematic pairs, they may be expressed differently according to joint movements, as shown in Table B.1. As for robotics, usually only revolute and prismatic joints are considered in reality. In Table B.1, R(z), R(y), and R(x) are the rotation about z, y, and x axis of the joint respectively; while sz is the translation along z axis of the

126

joint. Conventionally, the first physical rotation or translation axis of a joint should be defined as axis z, second as y, and last as x. It should be noted this convention is totally different from the sequence of PRY angel set, which is used to represent orientation instead of a rotation matrix. Table B.1: Rm and bm of different kinematic pairs [71]. Joint Revolute Prismatic Cylinder Universal Spherical Rm R(z) R(0) = 1 R(z) R(z)R(y) R(z) R(y) R(x) bm 0 sz sz 0 0

127

APPENDIX C - MATRIX COMPUTATIONS
C.1 The Rank of a Matrix
A set of vectors . If Let A be an is said to be linearly independent if implies all

are not linearly independent, they are said to be linearly dependent. matrix. The column rank of A is the maximum number of linearly

independent columns it contains. The row rank of A is the maximum number of linearly independent rows it contains. It may be shown that the column rank of A is equal to its row rank. Hence the concept of rank is unambiguous. We denote the rank of A by [96] (C.1) It is clearly that (C.2) If If , we say that A has full row rank. , we say that A has full column rank.

We have the following important properties: (C.3) (C.4) , if B is square of full rank (C.5) (C.6) The column space of A, denoted , is the set of vectors . (C.7)

128

C.2 Singular Value Decomposition
Let A be a real m × n matrix with that , an n × r matrix V such that . Then there exists an m × r matrix U such and an r × r diagonal matrix

with positive diagonal elements, such that [96] . (C.8)

C.3 The Moore-Penrose (MP) Inverse
The Moore-Penrose inverse was introduced by Moore (1920, 1935) and rediscovered by Penrose (1955). There is a huge literature on generalized inverses or the Pseudo-Inverse, of which the MP inverse is one particularly useful example due to its uniqueness with minimum Frobenius norm [68]. Definition An matrix is the MP inverse of a real (i) (iii) The MP inverse of Theorem 1 For each , exists and is unique. The unique M-P inverse of , It is the unique minimal Frobenius norm solution: Some Properties (C.10) , if (C.11) . is (C.9) is denoted as . (ii) (iv) matrix if

129

, if , if

(C.12) (C.13)

C.4 The Solution of Linear Equation Systems
An important property of the Moore-Penrose inverse is that it enables us to find explicit solutions of a system of linear equations [96]. Theorem 2 The general solution of the homogeneous equation , where q is an arbitrary vector of appropriate order. The solution is unique and is and only if is (C.14) , if

. If the solution is not unique, then there exist an infinite number of

solutions given by (C.14). Theorem 3 A necessary and sufficient condition for the vector equation that , in which case the general solution is , where q is an arbitrary vector of appropriate order. If there exists at least one solution, we say that the system system is consistent for every b if and only if solution is unique if and only if singular and the unique solution is . 130 . Clearly if is consistent. The (C.16) (C.15) to have a solution is

. If the system is consistent, its , then A is non-

C.5 Least Squares Method and Gauss-Markov Theorem
Theorem 4 (Least Squares Method) Let A be a given m × n matrix, and b a given n × 1 vector. Then [96] (C.17) for every in , with equality if and only if (C.18) for some q in .

Note. From Appendix C.3, the solution using M-P inverse of A has the minimum Euclidean norm among all solutions, given by , In the special case where A has full column rank n, we have a unique vector exists which minimizes . The vector is called the least squares solution and (C.19) and hence over all x, namely (C.20) is called the least squares is is not

approximation to b. Thus consistent or not. If consistent, then

is the `best' choice for x whether the equation is consistent, then is the solution; if

is the least squares solution.

Theorem 5 (Gauss-Markov) Consider the standard linear regression model , where A has full column rank n and the disturbances 131 , i.e. (C.21) are uncorrelated, i.e.

.

(C.22)

The best (minimum variance) affine unbiased estimator of exists if, and only if, in which case (C.23) with variance matrix . (C.24)

Note. Compared with Theorem 4, In other words, the best linear unbiased estimator for b is the least squares solution of . Note that the method of least squares, however, is

a purely deterministic method which has to do with approximation, not with estimation. Example: Least squares solution of Consider the equation of , where , , ,

. According to Theorem 4, the least squares solution of R is given by (C.25) Proof. The least-square errors of   can be formulated as (C.26)

By taking the first derivative and equal to zero, we can find the estimated matrix that minimizes the sum of square errors.
 

(C.27)

Hence, the LS solution of R is (C.28) where is the pseudo-inverse of Do in C.12.

132

C.6 Rotation of Subspaces (Constrained Least-Squares)
Theorem 6: Suppose are two sets of data matrix obtained

by performing two measurements or experiments. m is the size of each data or dimension of the subspace and n is the number of measurements. orthogonal Procrustes problem, which is the possibility that explored by solving the following problem: [68] (C.29) where V and U represent the SVD of , i.e. is the solution for the can be rotated into is

Proof. Recall that the Frobenius matrix norm of as . It follows that if

is the trace of a matrix is orthogonal, then

, denoted

(C.30)

Thus, is equivalent to the problem of maximizing SVD of this matrix, then

. If

is the

(C.31)

Clearly, the upper bound is attained by setting the minimum value of error norm is

for then

. Obviously,

(C.32)

133

APPENDIX D - LEAST-SQUARE ESTIMATION OF TRANSFORMATION PARAMETERS BETWEEN TWO POINT PATTERNS
Umeyama [40] refined Arun et. al [36]'s SVD solution and proved the following theorem for estimating the similarity transformation parameters between two point patterns. Theorem: Let and be corresponding point

patterns in m-dimensional space. The minimum value of the mean squared error (D.1) of these two point patterns with respect to the similarity transformation parameters (R: rotation, t: translation, and c: scaling) is given as follows: (D.2) where (D.3) (D.4) (D.5) (D.6) (D.7) and let a singular value decomposition of ), and be (

134

(D.8)

is a covariance matrix of X and Y, and

and

are mean vectors of X and Y, and

are variances around the mean vectors of X and Y, respectively.

When uniquely as follows:

, the optimum transformation parameters are determined

(D.9) (D.10) (D.11)

135

APPENDIX E - GENERAL FORMULATION FOR THE OUTLIER DIAGNOSIS BY RELAXATION
A function to calculate the total support of non-outlier provided to pi by all markers pj [87]

sik 
where

1 n 1  j c( ij ), s k  j n j 1





(E.1)

s ik - Merit score for ith marker at kth iteration
1 sk - Merit score for jth marker at (k-1)th iteration j 1  j c( ij ), s k j  - A function to calculate the support of non-outlier given to pi by pj, by
1 combining the merit scores of pj ( s k ) and the compatibility c( ij ) . These two factors j

1 can be combined in various ways. It can be a product, e.g.,  j  c( ij )  s k j , or to use 1 their minimum, i.e.,  j  min c( ij ), s k , or etc. j





c( ij ) - A function to calculate the compatibility between pi and pj. It can be a continuous
2 ) , or a function of the error of the Euclidean distance  ij , for example, c( ij )  1 (1   ij

discrete function with integers by thresholding, c(ij) = {-1, 0, 1}

 ij - The error of Euclidean distance between the nominal value and the measured value,
defined by the rigid body constraints

136

APPENDIX F - MEASURING SYSTEM MODELING
The measuring system is a line-sensor based Optical CMM. The principle of measuring system is illustrated in Figure F.1. The CMM has three line CCD sensors, represented by sensor 1, 2 and 3. At each CCD sensor, there is a projection plane perpendicular to the line sensor that pass through the point feature, the focal and the image point, denoted by 1,  2, and  3. The intersects of these three projection planes establish the triangulation of the point of interest, whose coordinates are represented as p(x, y, z). The modeling basically solves the coordinates based on the triangulation of planes 1,  2, and  3.

l1 Sensor 1 Marker's observed position l4 Inclined l1 1 l2 l5 z New observed position when error occurred in sersor 1 x l4 l3 -Z y 2 l6 l3 Sensor 3 Y 3 X l1 l2 Sensor 2

z'
z - depth

Figure F.1: Principle of Optoelectronic Coordinate Measurement Machines [88]. Here are equations of three planes, as shown in Figure 3:

1 : a1 x  b1 y  c1 z  d1  0

(F.1) (F.2) (F.3)

 2 : a2 x  b2 y  c2 z  d 2  0

3 : a3 x  b3 y  c3 z  d3  0

137

  Since planes 1 and 2 are parallel to the X axis and its normal vectors n1 and n2 are
vertical to the X axis, then a1 and a2 are zero. When calculating the coordinate values of a marker, y and z can be determined first by eqs. (F.1) and (F.2), yields [88]

l5 : b1 y  c1 z  d1  0 l6 : b2 y  c2 z  d 2  0
or

(F.4)

 b1 b  2

c1  y    d1         c2   z    d 2 

(F.5)

And substituting y and z values into eq. (F.3), x value can be obtained. Therefore, each component of the coordinates from the calculation mutually depends on each other's calculation. As a result, the coordinates are correlated and require a decorrelation procedure during the calibration of the measurement system prior to the pose estimation.

138

APPENDIX G - ROBOTIC PERCUSSIVE RIVETING
Riveting and welding represent two primary joining methods for the assembly of structural components that require strong joint strength. Compared with welding mainly a fusion method, riveting a mechanical method generates no thermal deformation, hence widely used for joining high thermal conductive materials such as aluminum sheet metals used in aircraft assembly [1]. There are hundred thousands of rivets in a regional aircraft and millions in a large continental aircraft. Overall, the operation of aircraft assembly is divided into three stages: subcomponent assembly, component assembly, and line assembly. The subcomponent assembly is the first step to construct the base components for four major sections, namely, fuselage, wing, cockpit and empennage. The component assembly is the middle step to join the subcomponents to form an individual major section. The line assembly is the last step to assemble a whole aircraft by connecting the four major sections together. The current riveting process in aerospace manufacturing entails a mix of manual riveting, semi-automated riveting, and automated riveting. The semi-automated and automated riveting machines are widely used in North America and Europe, but only limited to component assembly, such as wing skin panels and fuselage skin panels. Subcomponent assembly and line assembly are still conducted manually. The labor incurred producing these subassemblies/assemblies amounts to as much as fifty percent of the total cost. Manual riveting operations are tedious, repetitious, prone to error, and likely causing health and ergonomic problems [1]. In principle, there are two riveting methods, the first called squeezing (or one-shot)

139

riveting, where a large upsetting force is applied to deform a rivet instantly. This method requires a large riveter operating under high pressure beyond the yield strength of aluminum rivets in a range over 500 lb force. As shown in Figure G.1 (a), this type of riveter is made of either a hydraulic cylinder or an electromagnetic piston, very heavy, bulky and usually needing a lift-assisted device if used for manual operation. The automated and semi-automated riveting machines employ this type of riveter; hence they are gigantic and only limited to riveting large, simple and relatively flat components. The second method is called percussive (or hammering) riveting, where a small impulsive force is applied to deform a rivet accumulatively by a series of hits. As shown in Figure G.1 (b), this method uses a rivet gun in size of a regular hand-held power tool, very compact and light, operating under much lower pressure in a range less than 100 psi, very safe and energy efficient. Manual riveting employs this principle.

(a)

(b)

(c)

Figure G.1: Rivet guns and C-frame riveter.

Research on robotic riveting has been mainly centering on squeezing riveting that utilizes heavy-duty industrial robots of large size (> 100 Kg payload). In the automotive industry, squeezing robotic riveting systems have been fully developed and commercialized for joining metal parts. This technology is called robotic self-piercing riveting, in which a C-

140

frame tooling, as shown in Figure G.1 (c), is designed to have a squeezing riveter mounted on one end as a punch and the other end serving as a hitting base [89]. This system has been widely used for automotive chassis assembly. The application of robotic technology in aerospace manufacturing has been significantly slower than that in automotive manufacturing [90]. Though not commercially available, squeezing robotic riveting systems have been researched in the past by Boeing [91] and recently by EADS in Germany affiliated with AirBus [92]. In addition, a robotic system has been implemented at Bombardier in Montreal that uses two giant Kuka robots to hold large panels that are riveted on a C-frame squeezing riveting machine [2]. The research results presented in [3] and this dissertation have been applied to implement our robotic percussive riveting system. Figure G.2 shows the physical system involving three controllers for three subsystems: robot, tooling and bucking bar gantry. All controllers are integrated, with the robot controller being the main one for synchronization. The entire riveting process is automated through synchronization. Furthermore, the choice of a gantry system instead of a second robot allows it to serve as a jig for mounting sheet metals. A complete riveting control sequence has been created in Figure G.4, starting from position the gun  position the bucking bar  insert rive  extend the bucking bar  riveting  retract the bucking bar  retract the gun  move to the next spot. This sequence has been successfully tested and implemented to perform percussive riveting on sheet metal and composite panels [93], as shown in Figure G.3. The software development is undergoing to integrate the methodologies of the proposed pose estimation, as shown in Figures G.5 and G.6. The interface is developed as an addin button in ABB RobotStudio, which is a software of simulation, offline and online 141

programming developed by ABB Robotics, Sweden. After performing the localization based on static pose estimation, the transformation results can be typed into the input interfaces, including vision w.r.t. robot, and vision w.r.t. panel, as shown in Figure G.6. The methods of dynamic and robust pose estimation are being programming into the interface as part of robot path guidance, as shown in Figure G.5 (b). The decorrelation method is being programming into the acquisition interface, as shown in Figure G.6(a).

Robotic percussive riveting cell Controller

CNC bucking bar gantry - Controller

Tooling system - Controller

Figure G.2: Robotic Percussive Riveting System [3].

(a) front - metal

(b) side - metal

(c) back - metal

(d) composite

Figure G.3: Robot Riveted Samples [3].

142

Figure G.4: Flowchart of Process of Robotic Percussive Riveting System.

143

(a) Interface developed as an Add-In button of ABB RobotStudio.

(b) Interface of vision guidance.

Figure G.5: Graphical user interface development for robotic riveting system.

144

(a) Acquisition interface of the Optical CMM (BIG 3D Creator).

(b) Input interfaces for localization.

Figure G.6: Interfaces for acquisition and localization.

145

APPENDIX H - ROBUSTNESS EVALUATING EXPERIMENTS FOR NDI DATA ACQUISITION
In order to evaluate the robustness of the NDI Optotrak 3020 system, various distractions are considered in the experiments: reflection, illumination (white, laser, infrared), missing data and creating outliers.

H.1 Reflection Distraction
As depicted in Figure H.1(a), aluminum boards and painted metal parts are tested to prove the existence of reflection of infrared (IR) markers. The reflection on the aluminum board can be clearly noticed in Figure H.1(b). The reflected IR could be misinterpreted as a real marker and its coordinate value could be calculated by the photogrammetry system. However, when the aluminum board is far enough, the reflection seems to be weakened until disappearance. Besides, it depends on the reflecting characteristics of the material. In any circumstances, the reflection issue should be brought to our attentions during metrology integrations in aerospace manufacturing and assembly fields.

Aluminum board Painted metal part

(a)

146

IR marker's reflection on the aluminum board

(b)

Figure H.1: IR marker's reflection on an aluminum board and a painted metal part.

H.2 Illumination Distraction
 White/Laser lights Basically, no distraction caused by white/laser lights was discovered from the experiments, since the wave length of the laser tested is far beyond the IR markers'.  Infrared Light Distraction In Figure H.2(a), when the markers are out of sight of NDI cameras, emitting infrared lights from an ordinary TV remote are observed and calculated instead. In Figure H.2(b), during the data acquisition, the remote is pressed down and placed beside one of the markers (marker 7). The distractions of the IR remote can be quantified based on the acquired data, as shown in Figure H.3. Note that the blue, green and red lines represent the x, y and z coordinates. Clearly, the remote causes missing data at the closest marker (marker 7) and disturbance at the nearby markers (markers 3-6). It may not affect a few

147

markers that are far from the remote (markers 1&2). The reason behind this phenomenon could be the image extracking algorithm for the line CCD sensors of the NDI cameras. Most importantly, the observed outliers of infrared lights are not carefully filtered due to the lack of recognition of the characteristics of the infrared marker, such as the wave length. As a suggestion, infrared light distraction should be avoided in the metrology environment.

Ordinary TV remote (a)

Marker 7

Infrared LED in the remote

(b)

Figure H.2: (a) The NDI Optotrak system observes the IR LED lights of the remote instead, when the markers are not observable due to away from the camera sensing direction. (b) Demonstration of Infrared LED remote's distraction during the data acquisition: the remote was pressed down and placed beside a marker (marker 7).

148

x,y,z Coordiantes of 7 markers under distraction of the IR remote

P1 (mm)

1000 0 -1000 500 0 100

blue: x green: y 200 300 400 500 red: z 600 700 800 900 1000

P2 (mm)

0 -500 2000 0 100 200 300 400 500 600 700 800 900 1000

P3 (mm)

0 -2000 0 500 100 200 300 400 500 600 700 800 900 1000

P4 (mm)

0 -500 500

0

100

200

300

400

500

600

700

800

900

1000

P5 (mm)

0 -500 500

0

100

200

300

400

500

600

700

800

900

1000

P6 (mm)

0 -500 500

0

100

200

300

400

500

600

700

800

900

1000

P7(mm)

0 -500

0

100

200

300 400 500 600 700 Time sequence (100Hz/10sec)

800

900

1000

Figure H.3: The positions of 7 markers under the distraction of the remote: missing data at marker 7; disturbances at markers 3-6; no effect at markers 1 and 2. The blue, green and red lines represent the x, y and z coordinates.

149

H.3 Missing data and outliers
 Missing data The existence of missing data was achieved by obstructing the visibility of markers, such as swinging an arm between the markers and the camera. The results have shown that the missing data doesn't affect the pose estimation of the rigid body as long as there are at least 3 visible markers remaining for the acquisition and the further pose estimation.  Outliers As a definition, the outlier is the gross measurement error that lies outside of the overall distribution of errors. In our case, it could be caused by the false correspondence of markers due to reflection, lighting distraction, sensor failures, or even the matching algorithm, etc. In order to demonstrate the occurrence of outliers, a marker is detached from the targetmounted case and put at a distance from the original position. Figure H.4(a) shows the original set-ups of 7 markers attached to the cubic target. Figure H.4(b) depicts the way of creating an outlier from the target-mounted case. From the experiments, it is found that the pose estimation is highly sensitive to the outliers due to the nature of least-square methods that tend to accommodate the squares of all residuals. First, the outliers at least degrade the accuracy of pose estimation. Second, when the outliers are dominant and severe it leads to the failure of the pose estimation algorithm of the NDI system. As illustrated in Figure H.5, four outliers broke down the pose estimation even when three markers remained and the pose estimation was 150

restored immediately when these outlying markers were manually removed by holding them to ensure the invisibility to NDI cameras. This has proved that the outliers have significant influence on the NDI pose estimation solutions, since these solutions are based on the least-square methods. Therefore, we should endeavor to avoid this outlier caused catastrophic failure event by providing a more robust pose estimation algorithm.

(a)

Original marker

Outlying marker

(b)

Figure H.4: (a) Original set-ups with 7 markers attached to the cubic target; (b) a way of creating an outlying marker by detaching the marker from the mounted case.

151

Three remaining markers

Four outlying markers (a)

Manually removed outlying markers (b)

Figure H.5: (a) With four outlying markers, the pose estimation failed even when there are three remaining markers; (b) once four outliers were manually removed, the pose estimation of the rigid body was restored immediately.

152

APPENDIX I - JIG DESIGN FOR PANEL

153

APPENDIX J - PUBLICATIONS AND PATENTS
Journal Papers
1. Lin, Y., Xi, F.F., Guo, S., Klim, G., and Chan, V. (2014). Robotic Riveting of Composite Materials. Canadian Aeronautics and Space Journal. CASJ-2014-0005. Paper under review. 2. Lin, Y., Tu, X.W., Xi, F.F., & Chan, V. (2013). Robust Pose Estimation with an Outlier Diagnosis Based on a Relaxation of Rigid Body Constraints. ASME Journal of Dynamic Systems, Measurement, and Control, 135, 014502. 3. Lin, Y., Xi, F.F., Mohamed, R.P., & Tu, X.W. (2010). Calibration of Modular Reconfigurable Robots Based on a Hybrid Search Method. ASME Journal of Manufacturing Science and Engineering, 132, 061002. 4. Xi, F.F., Lin, Y., & Tu, X.W. (2013). Framework on robotic percussive riveting for aircraft assembly automation. Advances in Manufacturing, 1-11. 5. Zhao, Y.M., Lin, Y., Xi, F.F., and Guo, S. (2014). Calibration-based Iterative Learning Control for Path Tracking of Industrial Robots, IEEE Transactions on Industrial Electronics, 14-TIE-0555, Paper under review. 6. Mohamed, R., Xi, F.F., & Lin, Y. (2014). A Combinatorial Search Method for the QuasiStatic Payload Capacity of Serial Modular Reconfigurable Robots. Mechanism and Machine Theory. Submitted. 7. Song, T., Xi, F.F, Guo, S., Ming, Z. & Lin, Y. (2014). A Comparison Study of Algorithms for Surface Normal Determination Based on Point Cloud Data, Precision Engineering. In press.

Conference Papers 154

8. Lin, Y., Xi, F.F., Klim G., Dakdouk D., Zeng C. & Chan, V. (2013, April). Robotic Riveting of Composite Materials. In Aerospace Manufacturing Technologies Symposium, CASI 60th Aeronautics Conference and AGM on, Toronto, Canada. 9. Lin, Y., Tu, X.W., Xi, F.F., & Perron, C. (2010, July). A data decorrelation method for 3D position measurements from a line sensors based photogrammetric measuring system. In Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME International Conference on (pp. 90-95), Montreal, Canada. 10. Zhao, Y.M., Xi, F.F., Guo, S., Lin, Y. (2014, June). Calibration-based Iterative Learning Control for Path Tracking of Robotic Riveting System. In The 11th World Congress on Intelligent Control and Automation (WCICA 2014), Shenyang, China. 11. Helal, M., Xi, F.F., & Lin, Y. (2013, Sept.). Robotic Tooling Self-Calibration. In 2013 SAE AeroTech Congress & Exhibition (13ATC-0225), Montreal, Canada.

Patents
12. Xi, F., Lin, Y., Dakdouk, D., Helal, M., & East, B., 2013, Automated Percussive Riveting System, WO Patent WO/2013/152440.

Technical Reports
13. Lin, Y., and Tu, X.W., Robust Pose Estimation with an outlier diagnosis based on a relaxation of rigid body constraints, Aerospace Manufacturing Technology Centre (AMTC), National Research Council Canada (NRC), LTR-AMTC-2010-0070, 7/23/2010. 14. Lin, Y., Tu, X.W., and Perron, C. Data Analysis of NDI Readings, Aerospace Manufacturing Technology Centre (AMTC), National Research Council Canada (NRC), LTR-AMTC-20100019, 2/2/2010.

155

REFERENCES
[1] Jr, Flake C. Campbell, Manufacturing technology for aerospace structural materials. Elsevier, 2006. [2] B. Monsarrat, E. Lavoie, G. Cote, M. De Montigny, et al. "High Performance Robotized Assembly System for Challenger 300 Business Jet Nose Fuse Panels," AeroTech 2007, Los Angeles, CA, USA. [3] F. Xi, Y. Lin, and, X.W. Tu, "Framework on robotic percussive riveting for aircraft assembly automation." Advances in Manufacturing: 1-11, 2013. [4] F. Xi, Materials and Manufacturing (AER507). Ryerson University. [5] ABB Robotics. Product Specification IRB 4400. 3HAC 8770-1. pp. 47. [6] Y. Q. Fei and X. F. Zhao, "An assembly process modeling and analysis for robotic multiple peg-in-hole," Int. Journal of Intelligent and Robotic Systems, vol. 36, pp. 175-189, 2003. [7] T. Arai, et al, "Hole research planning for peg-in-hole problem," Manufacturing Systems, vol. 26, no. 2, pp.119-124, 1997. [8] K. Sathirakul and R. H. Sturges, "Jamming conditions for multiple peg-in-hole assemblies," Robotica, vol. 16, pp. 329-345, 1998. [9] R. H. Sturges and S. Laowattana, "Virtual wedging in three-dimensioal peg insertion tasks," Journal of Mechanical Design. vol. 118, pp. 99-105, 1996. [10] D. R. Strip, "Insertion using geometric analysis and hybrid force-position control: Method and Analysis," in Proc. of IEEE Int. Conf. on Robotics and Automation, pp. 1744-1751, 1988. [11] D. E. Whitney, "Quasi-static assembly of compliantly supported rigid parts," ASME Journal of Dynamic Systems Control, vol. 104, pp. 65-77, 1982. [12] Janabi-Sharifi, L. F. Deng, and W. J. Wilson, "Comparison of basic visual servoing methods," IEEE/ASME Trans. on Mechatronics, vol. 16, no. 5, pp. 967-983, 2011. [13] L. E. Weiss, A. C. Sanderson, and C. P. Neuman, "Dynamic sensor based control of robots with visual feedback," IEEE Journal of Robot and Automation, vol. RA-3, no. 5, pp. 404­417, Oct. 1987. [14] W. J. Wilson, C. C. W. Hulls, and G. S. Bell, "Relative end-effector control using

156

Cartesian position-based visual servoing," IEEE Trans on Robotics and Automation, vol. 12, no. 5, pp. 684­696, Oct. 1996. [15] B. Espiau, F. Chaumette, and P. Rives, "A new approach to visual servoing in robotics," IEEE Trans. on Robotics and Automation, vol. 8, no. 3, pp. 313­326, Jun. 1992. [16] F. Chaumettee, "Potential problems of stability and convergence in image -based and position-based visual servoing," in The Confluence of Measurement and Control, LNCIC Series, No 237, 1998. [17] O. Kermorgant, and F. Chaumette, "Combining IBVS and PBVS to ensure the visibility constraint," in IEEE/RSJ Int. Conf. on the Intelligent Robots and Systems, pp. 2849-2854, 2011. [18] Y. Li, F. Xi, and K. Behdinan, "Modeling and Simulation of Percussive Riveting for Robotic Automation", ASME Journal of Computational and Nonlinear Dynamics, Vol. 5, No. 2, 021011. 2010. [19] Cuypers, Wim, Nick Van Gestel, AndréVoet, J-P. Kruth, J. Mingneau, and P. Bleys. "Optical measurement techniques for mobile and large-scale dimensional metrology." Optics and Lasers in Engineering 47, no. 3 (2009): 292-300. [20] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with Applications to Tracking and Navigation, Wiley, 2001. [21] F. Janabi-Sharifi, and M. Marey, "A Kalman-Filter-Based Method for Pose Estimation in Visual Servoing," IEEE Trans. Robotics, vol. 26, no. 5, pp. 939-947, 2010. [22] V. Lippiello, B. Siciliano, and L. Villani, "Adaptive extended Kalman filtering for visual motion estimation of 3D objects," Control Engineering Practice, 15, pp. 123134, 2007. [23] S. Jeon, M. Tomizuka, and T. Katou, 2009, "Kinematic Kalman Filter (KKF) for Robot End-Effector Sensing," ASME J. Dyn. Sys., Meas., Control, vol. 131, p. 021010. [24] K. Halvorsen, T. Soderstrom, V. Stokes, and H. Lanshammar, "Using an Extended Kalman Filter for Rigid Body Pose Estimation," ASME J. Biomechanical Engineering, vol. 127, pp. 475-483, 2005.

157

[25] W. F. Xu, Y. Liu, B. Liang, Y.S. Xu, and W.Y. Qiang, "Autonomous Path Planning and Experiment Study of Free-floating Space Robot for Target Capturing," J. Intell. Robot Syst., 51, pp. 303-331, 2008. [26] F. Aghili, and K. Parsa, "Motion and Parameter Estimation of Space Objects Using Laser-Vision Data," J. of Guidance, Control, and Dynamics, vol. 32, no. 2, pp. 537549, 2009. [27] D.J. McTavish, R. Schumacher, and G. Okouneva, "Kalman filtering for dynamic pose and relative motion estimation in orbit," Can. Aeronaut. Space J., vol. 53, no. 3/4, pp. 95-105, 2007. [28] F. Xi, D. Nancoo, and G. Knopf, "Total lease-squares methods for active view registration of three-dimensional line laser scanning data," ASME Journal of Dynamic Systems, Measurement, and Control, Vol. 127, March 2005, pp. 50-56. [29] Y. Lin, X. Tu, F. Xi, and V. Chan, "Robust Pose Estimation with an Outlier Diagnosis Based on a Relaxation of Rigid Body Constraints", ASME Journal of Dynamic Systems, Measurement and Control, 2013. [30] B. K. P. Horn, "Close-form solution of absolute orientation using unit quarternions," J. Optical Society of America A, vol. 4, 1987, pp. 629-642. [31] J. A. Williams, M. Bennamoun, and S. Latham, "Multiple view 3D registration: a review and a new technique," In Proc. IEEE Int. Conf. Systems, Man and Cybernetics (SMC `99), vol. 3, 1999, pp. 497-502. [32] J. Angeles, "Automatic computation of the screw parameters of rigid-body motions. part 1: finitely-separated positions," ASME J. Dynamic Systems, Measurements, and Control, vol. 108, 1986, pp. 32-38. [33] P. H. Schonemann, "A generalized solution of the orthogonal Procrustes problem," Psychometrika, vol. 31, no. 1, 1966, pp. 1-10. [34] B. P. Wrobel, "Minimum solutions for orientation," in Calibration and Orientation of Cameras in Computer Vision, Gruen, and Huang Eds., Springer, 2001, ch. 2, pp. 3335. [35] D. W. Eggert, A. Lorusso, and R. B. Fisher, "Estimating 3-D rigid body transformations: a comparison of four major algorithms," Machine Vision and Applications, vol. 9, 1997, pp. 272-290.

158

[36] K. S. Arun, T. S. Huang, and S. D. Blostein, "Least-squares fitting of two 3-D point sets," IEEE Trans. Pattern Analysis Machine Intelligence, vol. 9(5), 1987, PP. 698700. [37] B. K. P. Horn, H. M. Hilder, and S. Negahdaripour, "Close-form solution of absolute orientation using orthonormal matrices," J. Optical Society of America A, vol. 5, 1988, pp. 1127-1135. [38] O. D. Faugeras, and M. Hebert, "The representation, recognition, and locating of 3D objects," Int. J. Robotics Research, vol. 5, no. 3, 1986, pp. 27-52. [39] M. W. Walker, and L. Shao, "Estimating 3-D location parameters using dual number quaternions," CVGIP: Image Understanding, vol. 54, no. 3, 1991, pp. 358-367. [40] S. Umeyama, "Least-squares estimation of transformation parameters between two point patterns," IEEE Trans. Pattern Analysis Machine intelligent, vol. 13, 1991, pp. 376-380. [41] K. Kanatani, "Analysis of 3-D rotation fitting," IEEE Trans. Pattern Analysis Machine intelligent, vol. 16, no. 5, 1994, pp. 543-549. [42] D. Goryn, S. Hein, "On the estimation of rigid body rotation from noisy data," IEEE Trans. Pattern Analysis Machine intelligent, vol. 17, 1995, pp. 1219-1220. [43] J. A. Ramos, and E. I. Verriest, "Total least squares fitting of two point sets in m-D," in Proc. IEEE Conf. Decision & Control, 1997, pp. 5048-5053. [44] G. Wen, D. Zhu, S. Xia, and Z. Wang, "Total least squares fitting of point sets in mD," in Proc. Computer Graphics International (CGI'05), 2005, pp. 82-86. [45] K. Kanatani, Geometric Computation for Machine Vision, Oxford Science Publications, 1993, pp. 112-114. [46] N. Ohta, and K. Kanatani, "Optimal estimation of three-dimensional rotation and reliability evaluation," in Computer Vision ­ ECCV'98, B. Neumann and H. Burkhardt Eds., Springer, 1998, pp. 175-187. [47] B. Matei, and P. Meer, "Optimal rigid motion estimation and performance evaluation with bootstrap," in Proc. CVPR'99, June 1999. [48] C. V. Stewart, "Robust parameters estimation in computer vision," SIAM Review, vol. 41, no. 3, 1999, pp. 513-537. [49] P. Meer, D. Mintz, and A. Rosenfeld, "Robust regression methods for computer

159

vision: a review," Int. J. of Computer Vision, vol. 6, no. 1, 1991, pp. 59-70. [50] P. J. Rousseeuw, and A. M. Leroy, Robust Regression and Outlier Detection, Wiley, New York, 1987. [51] V. J. Hodge, and J. Austin, "A survey of outlier detection methodologies," Artificial Intelligence Review, vol. 22, 2004, pp. 85-126. [52] P. Filzmoser, "A multivariate outlier detection method," in Proc. of the Seventh Int. Conf. on Computer Data Analysis and Modeling, vol. 1, 2004, pp. 18-22. [53] N. T. Trendafilov, "On the 1 Procrustes problem," Future Generation Computer Systems, vol. 19, 2003, pp. 1177-1186. [54] O. Enqvist, and F. Kahl, "Robust optimal pose estimation," D. Forsyth, P. Torr, and A. Zisserman (Eds.): ECCV 2008, Part I, LNCS 5302, Springer-Verlag Berlin Heidelberg, 2008, pp. 141-153. [55] A. F. Siegel, R. H. Benson, "A robust comparison of biological shapes," Biometrics, vol. 38, 1982, pp. 341-350. [56] A. F. Siegel, J. R. Pinkerton, "Robust comparison of three-dimensional shapes with an application to protein molecule configurations," Technical Report #217, series 2, Department of Statistics, Princeton University, 1982. [57] Z. Zhang, "Iterative point matching for registration of freeform curves and surfaces," Int. J. of Computer Vision, 13(2), 1994, pp. 119-152. [58] X. Pennec, and J. P. Thirion, "A framework for uncertainty and validation of 3-D registration methods based on points and frames," Int. J. of Computer Vision, 25(3), 1997, pp. 203-229. [59] X. H. Zhuang, and Y. Huang, "Robust 3D pose estimation," IEEE Trans. PAMI, 16(8), 1994, pp. 818-824. [60] P. Boulanger, V. Moron, and T. Redarce, "High-speed and non-contact validation of rapid prototyping parts," in Proc. SPIE Rapid Product Development Technologies, June 1996, pp. 46-90. [61] P. L. Rosin, "Robust pose estimation," IEEE Trans. Sys., Man, and Cybernetics-part B: Cybernetics, vol. 29, no. 2, April 1999, pp. 297-303. [62] R. Kumar, and A. R. Hanson, "Robust methods for estimating pose and a sensitivity analysis," CVGIP: Image Understanding, vol. 60, no. 3, November 1994, pp. 313-

160

342. [63] G. Taylor, L. Kleeman, "Visual perception and robotic manipulation", Springer Tracts in Advanced Robotics, Volume 26, 2006. [64] S. Hutchinson, G. D. Hager, and P. I. Corke, "A tutorial on visual servo control", IEEE Transactions on Robotics and Automation, 12(5): 651-670, 1996. [65] D. Kragic and H. I. Christensen, "Survey on visual servoing for manipulation", Technical Report ISRN KTH/NA/P-02/01-SE, KTH, 2002. [66] A. C. Sanderson, L. E. Weiss, "Image-based visual servo control using relational graph error signals", In proc. IEEE Conference on Cybernetics and Society, pages 1074-1077, 1980. [67] C. D. Meyer, Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics, 2000. [68] G. H. Golub, C. F. Van Loan, Matrix Computations, JHU Press, 1996. [69] Y. Lin, X.W. Tu, C. Perron, and F. F. Xi, "A data decorrelation method for 3D position measurements from a line sensors based photogrammetric measuring system," In Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME

International Conference on (pp. 90-95), 2010. [70] A. Nubiola, M. Slamani, A. Joubair, and I. A. Bonev, "Comparison of two calibration methods for a small industrial robot based on an optical CMM and a laser tracker", Robotica, pp 1 ­ 20, 2013. [71] F. Xi, "Computational dynamics (ME8138) lecture notes", Ryerson Univ., Toronto, ON. Canada, c2007. [72] P. I. Corke, "Robotics, Vision & Control", Springer 2011, ISBN 978-3-642-20143-1. [73] Y. Lin, F. F. Xi, R. P. Mohamed, and X. W. Tu, "Calibration of Modular Reconfigurable Robots Based on a Hybrid Search Method," ASME Journal of Manufacturing Science and Engineering, 132, 061002, 2010. [74] G. S. Cox, and G. De Jager, "A Survey of Pattern Matching Techniques and a New Approach to Point Pattern Recognition," Proc. IEEE South African Symp. on Comm. and Signal Processing, pp. 243-248, 1992. [75] S. Ranade, and A. Rosenfeld, "Point Pattern Matching by Relaxation," Pattern Recognition, vol. 12, pp. 269-275, 1980.

161

[76] P. J. Rousseeuw, and A. M. Leroy, Robust Regression and Outlier Detection, Wiley, New York, 1987. [77] J. C. Kent, N. Trigui, W. C. Choi, Y. G. Guezennec, and R. S. Brodkey, "Photogrammetric calibration for improved three-dimensional particle tracking velocimetry (3-D PTV)," in Proc. SPIE Optical Diagnostics in Fluid and Thermal Flow, vol. 2005, pp. 400­412. [78] C. Ogleby, "Photogrammetry," in The Focal Encyclopedia of Photography, 4th ed., 2007, pp. 583­584. [79] A. Gruen, "Fundamentals of videogrammetry ­ a review," J. Human Movement Science, vol. 16, 1997, pp. 155­187. [80] H-G, Maas, "Concepts of real-time photogrammetry," J. Human Movement Science, vol. 16, 1997, pp. 189­199. [81] H. A. Beyer, "Geometric and radiometric analysis of a CCD-camera based photogrammetric close-range system," Ph.D. dissertation, Institute of Geodesy and Photogrammetry, ETH Zurich, Switzerland, 1992. [82] A. Gruen, and T. S. Huang, Calibration and orientation of Cameras in Computer Vision, Springer, 2001, pp. 113­169. [83] T. Luhmann, S. Robson, S. Kyle, and I. Harley, Close Range Photogrammetry, Caithness: Whittles Publishing, 2006, pp.116­122. [84] C. S. Fraser, and S. Cronk, "A h ybrid measurement approach for close-range photogrammetry," ISPRS J. Photogrammetry and Remote Sensing, vol. 64, 2009, pp. 328­333. [85] R.A. Johnson, and D.W. Wichern, Applied Multivariate Statistical Analysis, 5th ed., Prentice Hall, 2002, pp. 426­476. [86] C. D. Meyer, Matrix Analysis and Applied Linear Algebra, Philadelphia, PA: SIAM, 2000, ch. 7. [87] Y. Lin, and X.W. Tu, "Robust Pose Estimation with an outlier diagnosis based on a relaxation of rigid body constraints," LTR-AMTC-2010-0070, 7/23/2010. [88] Y. Lin, X.W. Tu, and C. Perron, "Data Analysis of NDI Readings," LTR-AMTC2010-0019, 2/2/2010. [89] J. Mortimer, "Jaguar uses X350 car to pioneer use of self-piercing rivets," Industrial

162

Robot: An International Journal 28, no. 3: 192-198, 2001 [90] B. Morey, "Robot Seeks Its Role in Aerospace," Manufacturing Engineering, Vol. 139, No. 4, 2007. [91] J. Inman, B. Carbrey, R. Calawa, J. Hartmann, et al, "Flexible Development System for Automated Aircraft Assembly." Paper presented at SAE Aerospace Automated Fastening Conference & Exposition, Bellevue, WA, USA, October, 1996. [92] R. Kleebaur, "Where Precision Counts Above All Else," The High Flyer, No. 2, 2006. [93] Y. Lin, F.F. Xi, G. Klim, D. Dakdouk, C. Zeng, and V. Chan, "Robotic Riveting of Composite Materials," In Aerospace Manufacturing Technologies Symposium, CASI 60th Aeronautics Conference and AGM on, Toronto, Canada. 2013. [94] J. J. Craig, Introduction to Robotics: Mechanics and Control, Third Edition, Pearson Education, 2005. [95] S. Bai, and M. Y. Teo, "Kinematic calibration and pose measurement of a medical parallel manipulator by optical position sensors," Journal of Robotic Systems, Wiley, 2003, pp. 202-209. [96] J. R. Magnus, and H. Neudecker. Matrix Differential Calculus with Applications in Statistics and Econometrics (revised ed.) Wiley. 1999. [97] Y.M. Zhao, Y. Lin, F.F. Xi, and S. Guo. "Calibration-based Iterative Learning Control for Path Tracking of Industrial Robots," IEEE Transactions on Industrial Electronics, 14-TIE-0555, 2014, Paper under review. [98] Y.M. Zhao, F.F. Xi, S. Guo, Y. Lin, "Calibration-based Iterative Learning Control for Path Tracking of Robotic Riveting System," In The 11th World Congress on Intelligent Control and Automation (WCICA 2014), Shenyang, China.

163

