PRIVACY-PRESERVING PUBLIC AUDITING WITH DATA DEDUPLICATION IN CLOUD COMPUTING

by

Naelah Abdulrahman Alkhojandi Bachelor of Computer Science, Umm Al-Qura University, 2005

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science

Toronto, Ontario, Canada, 2015 c Naelah Abdulrahman Alkhojandi 2015

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

iii

PRIVACY-PRESERVING PUBLIC AUDITING WITH DATA DEDUPLICATION IN CLOUD COMPUTING Master of Science 2015 Naelah Abdulrahman Alkhojandi Computer Science Ryerson University

Abstract
Storage represents one of the most commonly used cloud services. Data integrity and storage efficiency are two key requirements when storing users' data. Public auditability, where users can employ a Third Part Auditor (TPA) to ensure data integrity, and efficient data deduplication which can be used to eliminate duplicate data and their corresponding authentication tags before sending the data to the cloud, offer possible solutions to address these requirements. In this thesis, we propose a privacypreserving public auditing scheme with data deduplication. We also present an extension of our proposed scheme that enables the TPA to perform multiple auditing tasks at the same time. Our analytical and experimental results show the efficiency of the batch auditing by reducing the number of pairing operations need for the auditing. Then, we extend our work to support user revocation where one of the users wants to leave the enterprise.

v

Acknowledgements
All praises and glory is due to almighty God for blessing and strengthening me in every single moment through this amazing opportunity and great journey of knowledge. My great thanks and deepest gratitude to my academic supervisor Prof. Ali Miri for his guidance and persistent support. I appreciate his generosity to make time for my questions at any matter. I also appreciate his wise contribution and encouragement in publishing a paper on the Symposium of Foundation and Practice of Security which held between 3rd and 5th of November 2014 in Montreal, Canada. This would not be possible without his dedication in perfecting my research skills and his invaluable insights. Furthermore, I would like to thank the Ministry of Higher Education in Saudi Arabia for sponsoring my complete journey of studies. As well great thanks to the Saudi Cultural Bureau in Ottawa for their continuous support, guidance, personal following and financial coverage. In addition, my gratitude to the computer science committee for reviewing my research and their feedbacks. Moreover, big thanks to the administrative staff and technical support for their tremendous cooperation and assistance. I would like to thank my colleagues whom contributed a huge inspirations and guidance. No thanks would be enough to my parents, sister, parents in law, and my husband for their endless love, continuous support, and prayers

vii

Dedication
Dad: Abdulrahman & Mom: Aatika Sister: Naeemah & Niece: Ghazal My supportive husband: Hassan Bazaid & My expected babies 

ix

Contents
Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii v ix

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv 1 Introduction 1.1 Cloud Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 1.1.2 1.1.3 1.1.4 1.1.5 1.2 1.2.1 1.2.2 1.2.3 1.2.4 1.2.5 1.3 1.3.1 Cloud Computing Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cloud Computing Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cloud Computing Service Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cloud Computing Deployment Models . . . . . . . . . . . . . . . . . . . . . . . . . Cloud Computing Security Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Deduplication Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 3 3 3 3 4 4 4 4 5 5 5 6 7 9 9 9

Data Deduplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Deduplication Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . How Data Deduplication works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Deduplication Benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Deduplication Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Boneh, Lynn, and Shacham (BLS) signature . . . . . . . . . . . . . . . . . . . . .

Security Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Literature Survey 2.1 Data Auditing in Cloud Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 2.1.2 2.1.3 2.2 2.3 Data Auditing Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Data Auditing Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Data Auditing Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

Public auditing with data deduplication Schemes . . . . . . . . . . . . . . . . . . . . . . . 19 Aggregate Signatures vs. Multisignatures . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.3.1 GDH Multisignature Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 xi

3 Design and Implementation 25 3.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.1.1 System Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.2 3.3 3.1.2 Threat Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Design Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scheme Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Public Auditing in Cloud with Data Deduplication Scheme . . . . . . 3.3.2 Privacy-Preserving Public Auditing in Cloud with Data Deduplication Support for Batch Auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Batch Auditing of Case 1: Two users have the same file . . . . . . . . 3.4.2 Batch Auditing of Case 2: Two users have some identical blocks . . . . . . . . . . . . . . . . . . . . . . . Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 27 27 27 28 31 31 32

3.4

3.5

User Revocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 37

4 Evaluation 4.1 4.2 4.3

Security Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.2.1 Cost Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Batch Auditing Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

5 Conclusions and Future Work 47 5.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 5.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 References 51

xii

List of Tables
2.1 4.1 4.2 4.3 Comparative survey of various aggregate signature and multisignature schemes . . . . . . 23 Notation of cryptographic operations, (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 The computation time of (Mediator, CS, and TPA) when 100 blocks are checked. Times in seconds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 The computation time of (the Mediator, the CS, and the TPA) when the same file is checked in each auditing task. However, the number of sampled blocks is increasing for each task. Times in seconds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.4 Comparison of the computational cost between the Single and Batch auditing tasks in the Audit phase. Where K = number of tasks, and L = consists of the number of subgroup of users in each task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Computation time in seconds for mathematical operations in pairing groups . . . . . . . . 45 Comparison of the computationa time in seconds between the Single and Batch auditing tasks in the Audit phase. Where K = number of tasks, and L = consists of the number of subgroup of users in each task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 4.7 Comparison between the number of pairing operations in the Single and Batch auditing tasks, where K = number of task, and L = the number of subgroup of users in each task 45

4.5 4.6

xiii

List of Figures
1.1 2.1 2.2 3.1 3.2 3.3 3.4 3.5 3.6 4.1 4.2 Data Deduplication Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

The system model of data owner auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 The system model of third party auditing . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 The architecture of the proposed scheme . . . Case 1: Two users have the same file . . . . . Case 2: Two users have some identical blocks Batch Auditing of Case 1 . . . . . . . . . . . Batch Auditing of Case 2 . . . . . . . . . . . User Revocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 28 30 32 33 34

Comparison of the time consumed by the Mediator, the CS, and the TPA when 100 blocks are checked of each file in each auditing task. Each file is owned by two users . . . . . . . 42 Comparison of the time consumed by (the Mediator, the CS, and the TPA) when the number of sampled blocks is increased in each auditing task of the same file "7.ppt". The file is owned by two users . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 Comparison of the computational time in the RHS of the verification equations between the Single and Batch auditing tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

4.3

xv

Chapter 1

Introduction
One of the most important services in cloud computing is data storage, which allows users to store their data in the cloud. Although cloud storage offers many advantages, it also introduces new security challenges in data integrity and availability. To verify the integrity of data stored in the cloud and to save computational resources of cloud users, it is important to enable auditing services, including those done on behalf of users by the TPA that can check the integrity of their data(Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). To increase storage efficiency, storage providers often identify and remove redundant data and keep only one copy of each file (file-level deduplication) or block (block-level deduplication). Data deduplication may occur before the data is transmitted to the cloud (client-side deduplication) or after it is transmitted (server-side deduplication) (Harnik et al.(2010)Harnik, Pinkas, and Shulman-Peleg). Standard server-side deduplication, in particular those associated with Cloud Service Providers (CSPs), requires full access to the content of users' data, which may limit the types of information that can be stored by these types of services. In particular, storing sensitive organizational data may not be appropriate. Furthermore, although deduplication can be used with single user data, it has been reported (Soghoian(2011)) that an average of 60% of data can be deduplicated for individual users by using a cross-user deduplication technique that identifies redundant data among different users. In this thesis, we use the results from (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou) and (Boldyreva(2003)) to propose a privacy-preserving public auditing protocol, which enables cross-user data deduplication achieving both data integrity and storage efficiency. At enterprise level, it is more likely that users have identical documents such as email attachments. So, cross-user data deduplication is provided by our scheme before outsourcing the data to the cloud. Moreover, our scheme eliminates not only the duplicated data but also the signatures that are used for checking data integrity. To ensure publicly verifiable data deduplication, our protocol uses an enterprise level mediator who has two main tasks to perform. Its first task is to do a client-side deduplication to eliminate duplicated blocks, so the amount of uploaded data and the bandwidth used between the enterprise and the CSP are both reduced. Since users should have the ability to check the integrity of their own stored data, the mediator's second task helps with calculating aggregated signatures that can be used by the TPA to perform its task. 1

1.1. CLOUD COMPUTING

CHAPTER 1. INTRODUCTION

In our protocol, we only assume that the TPA and the CSP are semi-trusted, and we will prove that the TPA would not learn the content of users' data while performing its tasks. Our analytical and experimental results show the efficiency of our proposed protocol. Then, we extend our work to support user revocation where one of the users wants to leave the enterprise. The signatures signed under his private key need to be re-signed by another user due to security reasons. Our extended work allows the cloud server to re-sign the blocks of the revoked user.

Thesis Outline: The rest of this chapter provides a brief introduction of cloud computing including its definition, characteristics, service models, deployment models, and security issues. It also provides an overview of the concept of data deduplication technique including definition, ratio, benefits, and types. Then, we explain some notations and preliminaries which are used in some of the literature and the proposed scheme. Chapter 2 Literature Survey: To give a better understanding of our proposed scheme, this chapter firstly explains auditing, which means checking the integrity of the data stored in the cloud. It includes auditing types, several important requirements in designing auditing protocols, and auditing techniques that have been proposed in the literature. Secondly, an extensive survey of Proof of Storage (POS) techniques is provided including definitions, algorithms, features, and drawbacks. Next, we present some schemes related to public auditing in the cloud with data deduplication. We include definitions, algorithms, features, and drawbacks. Finally, we provide a brief comparative survey between aggregate signature and multisignature in order to choose which scheme best fits the needs of our proposal. Chapter 3 Design and Implementation: This chapter describes in detail our proposed schemes that achieve both data integrity and storage efficiency. It includes an overview of our problem statement, the system model, and the threat model. It also describes the design goals of the proposed scheme. Then a detailed description of our scheme is provided. We present an extension to support batch auditing where the TPA performs many auditing tasks from different users at the same time. Another extension is presented to support user revocation. Chapter 4 Evaluation: This chapter presents a detailed analysis of the security and the performance of our proposed schemes. It also shows the efficiency of the batch auditing technique which is seen in the fact that the number of pairing operations in the batch audit is less than if each task were performed as an individually. Chapter 5 Conclusions: This Chapter presents our conclusion and some potential directions for future work.

1.1

Cloud Computing

Cloud Computing is one of the most inspiring concepts of the IT industry. It has gained attention due to its offered resources and services to enterprises and consumers. 2

CHAPTER 1. INTRODUCTION

1.1. CLOUD COMPUTING

1.1.1

Cloud Computing Definition

The most accurate definition of cloud computing is given by the National Institute of Standard and Technology (NIST). "Cloud Computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction" (Mell and Grance(2011)).

1.1.2

Cloud Computing Characteristics

According to NIST (Mell and Grance(2011)), there are five essential characteristics of cloud computing, which are: 1. On-demand self-service: A consumer can provision computing capabilities by himself with no need for any human interaction with any service provider. 2. Broad network access: Cloud capabilities are available through standard internet-enabled devices such as desktops, laptops, smart-phones, and tablet devices. 3. Resource pooling: Resource pooling assigns computing resources such as storage, processing, memory, and network bandwidth to multiple customers dynamically, based on consumer demand. 4. Rapid elasticity: A consumer can increase or decrease the capabilities of cloud services at any time based on demand. 5. Measured service: A consumer is charged based on the amount of usage of services (e.g., storage, processing, bandwidth, and so on). This is known as pay-as-you-go or pay-per-use.

1.1.3

Cloud Computing Service Models

There are three service models (Mell and Grance(2011)): · Software as a Service (SaaS): The consumer can use application software in the cloud. · Platform as a Service (PaaS): The consumer can use platform of their choice to run their applications on the cloud. · Infrastructure as a Service (IaaS): The consumer can access the underlying cloud infrastructure for their use.

1.1.4

Cloud Computing Deployment Models

There are four models (Mell and Grance(2011)): 1. Private Cloud: The cloud infrastructure is provided to a single corporation. It may be possessed, administrated and operated by the same corporation, a third party or by both, and it may exist inside or outside the premises (corporation's building). 3

1.2. DATA DEDUPLICATION

CHAPTER 1. INTRODUCTION

2. Community Cloud: Cloud infrastructure is provided to a specific group of consumers from corporations with the same demand and objectives. It may be owned, administered, and operated by one or more corporations, a third party or both, and it may exist inside or outside the premises. 3. Public Cloud: Cloud infrastructure is provided to the general public. It may be owned, administered, and operated by any corporation, and it exists inside the premises of the cloud provider. 4. Hybrid Cloud: Cloud infrastructure is a combination of two or more different cloud infrastructures (private, community, or public).

1.1.5

Cloud Computing Security Issues

Cloud storage is one of the most commonly used application services in the cloud that provides data storage and business access services externally (Li et al.(2011)Li, Xu, Li, and Zhang). However, the security of the data stored in the cloud of increasing concerned. The consumer gains several advantages from storing his data in the cloud such as availability, reliability, efficient retrieval, data sharing, and avoiding maintenance costs (Kamara and Lauter(2010)). On the other hand, cloud storage also has several disadvantages such as network outage, incompatible networking protocols, and unauthorized access. Briefly, the main security concerns are (Delettre et al.(2011)Delettre, Boudaoud, and Riveill): · Confidentiality of stored data. · Privacy. · Availability of services and/or data. · Integrity of services and/or data. · Loss control of services and/or data.

1.2
1.2.1

Data Deduplication
Data Deduplication Definition

Data Deduplication is a technique for eliminating the redundant data and storing only one copy of the duplicate data. Pointers are created to provide a link from the duplicate data to the unique copy (Tate et al.(2012)Tate, Beck, Hugo Ibarra, Kumaravel, and Miklas)

1.2.2

Data Deduplication Ratio

The data duplication ratio refers to the relationship between the size of data sent to be deduplicated and the size of the space used to store it, see Figure1.1. 4

CHAPTER 1. INTRODUCTION Ratio =
Bytes In Bytes Out

1.2. DATA DEDUPLICATION

Figure 1.1: Data Deduplication Ratio

1.2.3

How Data Deduplication works

According to (Rivera(2009)), · The data is broken up into segments. · An identifier is created for each segment. · Identifying the duplicated data by comparing the data segment identifiers · Only one copy of each duplicated segment is stored. · The duplicated segments are not stored and pointers are created for them.

1.2.4

Data Deduplication Benefits

According to (Rivera(2009)), there are several benefits of data deduplication which are: · Increased storage efficiency. · Increased network efficiency. · Decreased hardware costs. · Decreased backup costs. · Decreased costs for business continuity/disaster recovery.

1.2.5

Data Deduplication Types

The data deduplication can be categorized depending to various aspects (Rivera(2009); Harnik et al.(2010)Harnik, Pinkas, and Shulman-Peleg; Singh(2009)): · Location/site: where the deduplication occurs. 1. Source-based Deduplication: acts on the data at the source (client side) before it is transmitted to the data storage. 2. Target-based Deduplication: acts on the data at the target (data storage device or service). · Time: When the deduplication occurs. 1. In-line Deduplication: acts as the data is written to the data storage system 2. Post-process Deduplication: acts after the data is written to the data storage system 5

1.3. SECURITY ASSUMPTIONS

CHAPTER 1. INTRODUCTION

· Method: How the deduplication is performed. 1. Hash-based Deduplication: A hash algorithm such as SHA3 is used to create an identifier for the segment. 2. Delta-based Deduplication: the data is sored in the form of differences from a standard copy. · Granularity: How the data is broken down/divided. 1. File-level Deduplication: is applied on full (complete) file. Only one copy of each file is stored. 2. Block-level Deduplication: is applied on a sub-file (a file is broken up into blocks). Only one copy of each block is stored. · Alignment: How the data is arranged. 1. Fixed Length Block: The file is divided into fixed-length blocks 2. Variable Length Block: The file is divided into variable-length blocks.

1.3

Security Assumptions

In cryptography, we have two security models: Random Oracle Model: The random oracle is a random function "black box" where all the parties are responded with a random values from its output range (Ananth and Bhaskar(2013)). Standard Model: The standard model of computation where it depends on the standard complexity theoretic assumption (Bellare and Rogaway(1993)). Security proofs of many given protocols is based on intractability of problems considered to be hard to solve. The problem considered in most discrete logarithm based protocol is the Diffie-Hellman problem and one of its variants. Two are such variants are Decision Diffie-Hellman (DDH) and Computational Diffie-Hellman (CDH). Consider a multiplicative cyclic group G, of prime order p and with a generator g .
a b c Definition 1. Decision Diffie-Hellman (DDH): Given a, b, c  Z p , and (g, g , g , g ), it is difficult

to determine whether c = ab.
a b Definition 2. Computational Diffie-Hellman (CDH): a, b  Z p , and (g, g , g ), it is difficult to

compute g ab without knowing a or b. Any (g, g a , g b , g c ) or (g, g a , g b ) satisfying the stated problems, respectively are referred to as DiffieHellman (DH) tuple. The short signatures used in the proposed scheme in this thesis are based on the Weil pairing in (Boneh et al.()Boneh, Lynn, and Shacham), and the aggregate, verifiable signature in (Boneh et al.(2003)Boneh, Gentry, Lynn, and Shacham). The security assumption in these papers are based on Gap Diffie-Hellman (GDH) in which the CDH problem is considered hard, but the DDH problem is considered easy to solve. 6

CHAPTER 1. INTRODUCTION

1.3. SECURITY ASSUMPTIONS

Next, we give the definition of bilinear maps, the short signature scheme in (Boneh et al.()Boneh, Lynn, and Shacham), and the aggregate signature of (Boneh et al.(2003)Boneh, Gentry, Lynn, and Shacham). This closely follows the discussions in these two papers, and the reader is referred to those papers for more details. Definition 3. Bilinear Maps: Let G1 , G2 and GT be multiplicative cyclic groups of prime order p. g1 is a generator of G1 and g2 is a generator of G2 . A bilinear map is a map G1 × G2  GT with the following properties (Boneh et al.()Boneh, Lynn, and Shacham): · Bilinear: for all u  G1 , v  G2 and a, b  Zp , e(ua , v b ) = e(u, v )ab . · Non-degenerate: e(g1 , g2 ) = 1. It is easy to extend the DH problem over these types of maps. When G1 = G2 , they are referred to as co-DDH, co-CDH, and co-GDH. The next two signature schemes are proved to be secure in a the Random Oracle Model, based on the co-GDH assumption.

1.3.1

Boneh, Lynn, and Shacham (BLS) signature

The scheme is proposed by Boneh et. al. (Boneh et al.()Boneh, Lynn, and Shacham). Definition 4. Let (G1 , G2 ) be a co-Gap Diffie-Hellman group pair where |G1 | = |G2 | = p. A signature  is an element of G1 . H : {0, 1}  G1 is a hash function (random oracle). The scheme has three algorithms: KeyGen: This algorithm produces the secret and the public key used by the user for the signature generation, and the public key used by the signature verifier. Select at random x  Zp and x . The secret key used is x and the public key is v  G2 . compute v  g2 Sign: This algorithm will generate the signature for a message M  {0, 1} , using the secret key x. Compute h = H (M )  G1 . The signature will be  = hx  G1 . Verify: This algorithm will allow the verifier to determine whether the signature is genuine. Compute h = H (M )  G1 . Verify that (g2 , v, h,  ) is a valid co-Diffie-Hellman tuple. Aggregate Signature: This scheme makes it possible to aggregate multiple signatures from distinct users on distinct messages into a single short signature (Boneh et al.(2003)Boneh, Gentry, Lynn, and Shacham). It has two additional algorithms: Aggregation, and Aggregate Verification. Suppose that we need to generate an aggregated signature for messaged Mi  {0, 1} , and a subset of all users U , where |U | = k , and i = 1, · · · , k . Aggregation: This algorithm will generate the aggregated signature for messages Mi and users ui with secret keys xi and public keys vi  G2 . Compute the signature i  G1 for each distinct message Mi . The aggregate signature  =
k i=1

i . 7

1.3. SECURITY ASSUMPTIONS

CHAPTER 1. INTRODUCTION

Aggregate Verification: This algorithm will allow the verifier to determine whether the aggregated signature is genuine. Compute h = H (M )  G1 . To do this, the verifier checks the following: · Checks to ensure that all Mi are distinct, and · Computes hi = H (Mi ) for i = 1, · · · , k , and checks to see if e(, g2 ) =
k i=1

e(hi , vi ).

8

Chapter 2

Literature Survey
In this chapter, we explain auditing, which means checking the integrity of the data stored in the cloud: types, important requirements, and techniques. We give an extensive survey of those techniques: definitions, algorithms, features ,and drawbacks. Next we present related work of public auditing in the cloud with data deduplication: definitions, algorithms, features, and drawbacks. Finally, we provide a brief comparison between aggregate signature and multisignature in order to choose which one is more suitable to our proposed scheme.

2.1

Data Auditing in Cloud Computing

One of the most significant services of Cloud Computing is data storage which allows users to move their data to the cloud. Although cloud storage has many advantages, it also introduces new security challenges as data may be accessed by unauthorized users or lost in the cloud. We need to check the integrity of our stored data in the cloud for two reasons: Cloud services are still suffering outages and security breaches and CSPs may delete some of the data to obtain storage space. Due to the limited computing resources of cloud users and the large size of outsourced data, enabling public auditing is the best solution (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou).

2.1.1

Data Auditing Types

According to (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song) and (Yang and Jia(2012)), there are two types of auditing of cloud storage: 1. Data Owner Auditing: also known as Private Verifiability/Auditing, when the data owner verifies the integrity of the data stored in the cloud server. The system model consists of two entities: the data owner and the cloud server, as shown in Figure 2.1. 2. Third Party Auditing: also known as Public Verifiability/Auditing, when the TPA, instead of the data owner, verifies the integrity of the data stored in the cloud server. The system model consists 9

2.1. DATA AUDITING IN CLOUD COMPUTING

CHAPTER 2. LITERATURE SURVEY

Figure 2.1: The system model of data owner auditing

Figure 2.2: The system model of third party auditing of three entities: the data owner, the cloud server, and the TPA, as shown in Figure 2.2.

2.1.2

Data Auditing Requirements

To construct an efficient auditing protocol, there are several requirements must be considered according to (Yang and Jia(2012)),(Worku et al.(2012)Worku, Ting, and Zhi-Guang), and (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou): · Storage correctness / Unforgeability: ensures that the cloud cannot cheat and pass the auditing process without storing the data properly. · Low storage cost: the additional storage needs for the auditing should be low. That depends on: type of metadata (e.g MAC, signature, tag, etc.), size of metadata , and where to store the metadata (auditor side or server side which is preferred). · Low communication cost: the amount of communication between the parties needs for the auditing should be low. · Low computation cost: the complexity of the computation needs for the auditing should be low. The computation cost on the data owner depends on pre-prossessing the data (e.g. error correcting code) and calculating the metadata. On the other hand, the computation cost on the auditor and the server depends on the computation of auditing process. · Unbounded number of audits: the number of challenging the server should be unlimited. If the number is limited, the data owner needs to download the data from the server and re-computes 10

CHAPTER 2. LITERATURE SURVEY

2.1. DATA AUDITING IN CLOUD COMPUTING

the metadata. The server may cheat by responding to the challenge with a previous respond if the same challenge is issued before. · Recoverability: the ability to recover the data if failure occurred. It depends on applying error correcting code or erasure code to the data before it is sent to the server. · Public auditing: enables the TPA to check the integrity of the stored data in the server on behalf of the user. · Batch auditing: allows the TPA to perform multiple auditing tasks at the same time from different users. · Blockless verification: the auditor should not retrieve any data blocks from the server during the auditing process. · Stateless verification: the TPA does not need to maintain and update state between the auditing process. In case of failure, it is difficult to maintain state (Shacham and Waters(2008)). · Privacy-preserving: ensures that the TPA cannot learn anything about the content of the stored data during the auditing process. · Dynamic data: the scheme should support dynamic data operations such as data modification, insertion and deletion. The dynamic operating cost should be low.

2.1.3

Data Auditing Techniques

In this chapter, we give a review on the techniques that used to check the data integrity at untrusted server. We explain Proof of Storage protocols such as Remote Data Integrity Checking (RIC), Provable Data Possession (PDP), and Proof of Retrievability (POR). Some of theses protocols related to private verifiability and the others related to public verifiability. There are three categories of data auditing (Yang and Jia(2012)): 1. Message Authentication Code (MAC)-based methods. 2. RSA-based Homomorphic methods. · RSA-based Homomorphic hash value. · RSA-based Homomorphic tag. 3. Boneh-Lynn-Shacham signature (BLS)-based Homomorphic methods. MAC-Based Schemes The simple way is the data owner computes a Message Authentication Code (MAC) of his file and sends the file to untrusted server and keeps both the computed MAC and the secret key. Whenever he wants to check the integrity of the file, he retrieves the file and recomputes the MAC to compare it with his 11

2.1. DATA AUDITING IN CLOUD COMPUTING

CHAPTER 2. LITERATURE SURVEY

stored version. This way has a severe drawback which is downloading the file every time he needs to check its verification. Another way is the data owner divides the file into blocks and computes a MAC for each block. He sends both the file and the MACs to the server and keeps only the secret key. The user may share the secret key with the TPA. Later, the owner or TPA can retrieve from the server a random number of data blocks with their MACs and check the data integrity by comparing the fresh MACs with the stored ones. This way has two drawbacks: the communication cost is linear with the size of sampled blocks and the TPA needs to know the content of the blocks for the verification(Zeng(2008); Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). To avoid retrieving the data blocks from the server, the data owner selects randomly multiple secret keys and computes multiple MACs for the file. He sends the the file to the server and sends the MACs and the keys to the TPA. Later, the owner or TPA sends every time one of the keys to the server and asks for a fresh MAC. Although this way preserve the privacy of owner's data, it has two drawbacks: bounded number of challenges because the total number of challenges depends on the total number of MAC secret keys, and the owner or TPA needs to maintain and update state between auditing process (Zeng(2008); Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). Provable Data Possession (PDP) PDP is a technique/protocol that allows the user who stores his data at untrusted server to check if the server indeed retains the data without retrieving it by the verifier and without accessing the whole data by the server (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song). Protocol Overview: PDP protocol allows the user who wants to outsource his file into untrusted storage server to check if the server possess the file. The user pre-processes the file, which consists of n blocks, and generates a metadata. Then, he sends the file with the metadata to the server and may delete the local copy. At a later time, the user checks if the server retains the file by issuing a challenge to the server who computes a proof and sends it back to the user. Finally, the user verifies the response without retrieving the file blocks (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song). PDP schemes provide probabilistic and deterministic guarantees. The probabilistic guarantee utilizes a sampling technique which means the server generates the proof of data possession by accessing a random set of blocks. On the other hand, in the deterministic guarantee, all the blocks are accessed by the server(Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song). The concept of Homomorphic Verifiable Tag (HVT)/Homomorphic Linear Authenticator (HLA) is introduced to be as the building block for the PDP schemes. According to (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou), HVTs/HLAs are unforgeable verification metadata used to check the integrity of the data blocks. The HVTs/HLAs can be aggregated to verify a linear combination of the individual data blocks. The HVTs/HLAs have another property which is Blockless verification, means the user verifies if the server retains the file without accessing or retrieving the file blocks. 12

CHAPTER 2. LITERATURE SURVEY

2.1. DATA AUDITING IN CLOUD COMPUTING

Definition 5 (Provable Data Possession (PDP) Scheme). A PDP scheme (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song) has four polynomial-time algorithms (KeyGen, TagBlock, GenProof, and CheckProof), and it consists of two phases: Setup (includes the first two algorithms) and Challenge (includes the last two algorithms). KeyGen (1k )  (pk, sk ) is a key generation algorithm that is run by the user. It takes a security parameter k as input, and returns a pair of public and secret keys (pk, sk). TagBlock (pk, sk, m)  Tm is a metadata generation algorithm that is run by the user. It takes a public key pk , a secret key sk , and a file block m as input, and returns the verification metadata Tm . GenProof(pk, F, chal, )  V is a proof of possession algorithm that is run by the server. It takes which is the

a public key pk , a collection of blocks F , a challenge chal, and a collection of

verification metadata corresponding to the blocks in F . It returns a proof of possession V . CheckProof (pk, sk, chal, V )  {success, f ailure} is run by the verifier to validate the proof. It takes a public key pk , a secret key sk , a challenge chal, and a proof of possession V . The authors of (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song) present two PDP constructions: the first scheme is Sampling Provable Data Possession (S-PDP) with strong data possession guarantee. The second scheme is Efficient PDP (E-PDP) with better efficiency and weaker guarantee. S-PDP Scheme Details: Let k, ,  be security parameters. Let N = pq be an RSA modulus with p and q prime numbers. Let g be a generator of QRN which is the set of quadratic residues modulo N .
 Let Z N be a multiplicative cyclic group. Let h{0, 1}  QRN be a secure deterministic hash-and-encode function that maps strings uniformly to QRN . Let H be a cryptographic hash function. let f be a

pseudo-random function (PRF), let  be a pseudo-random permutation (PRP). · Setup: ­ KeyGen (1k ): pk = (N, g ) and sk = (e, d, v ), such that ed  1 mod (p - 1)(q - 1), e is a large R secret prime such that e >  and d > , v  - {0, 1}k . ­ TagBlock (pk, sk, m, i): For each block of F = {m1 , · · · , mn }, compute a tag Ti,m = (h(Wi ) × g m )d , where Wi = v ||i ­ The user sends the file F and the tags T to the server. · Challenge: ­ The user chooses randomly two keys k1 for  , k2 for f , c is number of blocks to be checked, and gs = g s mod N . Then, he sends chal = {c, k1 , k2 , gs } to the server. 13 mod N

2.1. DATA AUDITING IN CLOUD COMPUTING

CHAPTER 2. LITERATURE SURVEY

­ GenProof (pk, F = (m1 , · · · , mn ), chal, = (T1,m , · · · , Tn,m )): For 1  j  c : the server computes the indices: ij = k1 (j ), and the coefficients: aj = fk2 (j ). Then, he computes c aj c T = j =1 Tj,m and  = H (gs j=1 mj aj mod N ). The server sends V = (T, ) to the user. ­ CheckProof (pk, sk, chal, V ): The user computes ij = k1 (j ), aj = fk2 (j ),  = mod N . Then, he checks H ( s mod N ) =  The only difference between the two schemes is that the E-PDP provides guarantee of the sum of the data blocks not each individual block as in S-PDP. Thus, all the coefficients aj are equal to 1. The PDP scheme can be modified to offer Public Verifiability property, which allows anyone, not only the data owner, to verify the correctness of the stored data (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song). Features of the scheme: PDP scheme provides less access to the file blocks, less computation on the server, less communication between the user and the server, and unbounded number of challenges with constant amount of data (Ateniese et al.(2007)Ateniese, Burns, Curtmola, Herring, Kissner, Peterson, and Song). Drawbacks of the scheme: Since the HLAs are based on RSA, that makes the HLAs relatively long. Although the PDP scheme provides Public Verifiability, it does not support privacy-preserving and Batch auditing properties. Proof of Retrievability (POR) POR is a protocol that allows the user who stores his data at untrusted server to check if the server indeed retains the data in which the user can retrieve the entire data (Juels and Kaliski Jr.(2007)). Protocol Overview: POR protocol allows the user who wants to outsource his file into untrusted storage server to check if the server possess the file. The user encrypts the file and inserts random values which are called sentinels. In addition, Error-Correcting Code (ECC) is applied to the file to recover a small corruption. Then, he sends the file to the server and may deletes the local copy. At a later time, the user challenges the server by asking to return specific sentinel values. The server computes a proof and sends it back to the user. Finally, the user verifies the response without retrieving the file blocks. POR scheme provides a probabilistic guarantee (Juels and Kaliski Jr.(2007)). Definition 6 (Proof of Retrievability (POR)). A POR scheme (Juels and Kaliski Jr.(2007)) has six algorithms (keygen, encode, extract, challenge, respond, and verify), and it consists of two phases: Setup and Verification. Sentinel-based POR Scheme Details: · Setup phase: a secret key is generated by keygen. encode function has four steps: the file F is divided into k blocks. For each block an ECC is applied, so that yields a file F . Then, a symmetric 14
?
c j =1

Te h ( W i j ) aj

CHAPTER 2. LITERATURE SURVEY

2.1. DATA AUDITING IN CLOUD COMPUTING

key encryption is applied to F , that yields file F . Next, sentinels are created and appended to F , that yields file F . Finally, a pseudorandom permutation (PRP) is applied to F , that yields file F which is sent to the server. · Verification phase: the user runs challenge to generate q positions for different sentinels and sends them to the server. The server sends back the values of the corresponding sentinels as a respond to the user who runs verify to check if the server returned correct values. Then, the file F is recovered by extract function. Features of the scheme: The file is recovered by applying ECC. The communication and computation costs of the scheme are low (Juels and Kaliski Jr.(2007)). Drawbacks of the scheme: POR scheme allows bounded number of challenges because the total

number of challenges depends on the total number of sentinels (Juels and Kaliski Jr.(2007)). Compact Proofs of Retrievability-2008 In this paper (Shacham and Waters(2008)), the authors present two schemes that rely on the Homomorphic Authenticators (HAs). The first scheme is based on pseudorandom functions (PRFs), is secure in the standard model, and offers private verifiability. The second scheme is based on Boneh-Lynn-Shacham (BLS) signature, is secure in the random oracle model, and offers public verifiability. Definition 7 (Compact Proof of Retrievability (POR)). A Compact POR scheme (Shacham and Waters(2008)) has four algorithms (Kg, St, P and V ). PRF-based POR Scheme Details: Let f : {0, 1} × Kprf  Zp be a PRF. Kg(): Select randomly symmetric encryption key and MAC key. sk will be (kenc , kmac ) and there is no pk . St(sk, M ): Pre-process the file M by applying the erasure code, that yields a file M . Then, the file M is divided into n blocks. Each block is divided into s sectors, {mij }1in,1j s . Choose a PRF key kprf and s random numbers 1 , · · · , s  Zp . Let 0 = n Enckenc (kprf 1 ··· s ), the file tag  = 0
s j =1

M ACkmac (0 ). Compute the authenticator for each block i as

i = fkprf (i) + j mij . The processed file M  is {mij } together with {i }. The file M  is stored on the server along with the file tag  . V (pk, sk,  ): kmac is used to verify the MAC on  . kenc is used to decrypt the encrypted portions and recover n, kprf , and(1 , · · · , s ). Pick a random l­element subset I of the set [1, n]. For each i  I , select a random element vi . Then, send Q = {(i, vi )} to the prover. Check the prover's response 1 , · · · , s and  via =
(i,vi )Q ? s

vi fkprf (i) +
j =1

j j

15

2.1. DATA AUDITING IN CLOUD COMPUTING

CHAPTER 2. LITERATURE SURVEY
(i,vi )Q i vi

P (pk, , M  ): Compute j =

(i,vi )Q

vi mij for 1  j  s,  =

BLS-based POR Scheme Details: Let e : G × G  GT be a bilinear map, g be agenerator of G, and H : {0, 1}  G be the BLS hash which is treated as a random oracle (Shacham and Waters(2008)). Kg(): Generate a random signing key pair (spk, ssk ). Select a random   Zp and compute v  g  . sk will be (, ssk ) and pk will be (v, spk ). St(sk, M ): Pre-process the file M by applying the erasure code, that yields a file M . Then, the file M is divided into n blocks. Each block is divided into s sectors, {mij }1in,1j s . Choose a random file name name. Choose s random elements u1 , · · · , us  G. Let 0 = name n u1 · · · us , the file tag  = 0 SSigssk (0 ). Compute the authenticator for each block i as
s

i = (H (name

i) ×
j =1

uj

mij 

)

The processed file M  is {mij } together with {i }. The file M  is stored on the server along with the file tag  . V (pk, sk,  ): spk is used to verify the signature on  and recover name, n, and (u1 , · · · , us ). Pick a random l­ element subset I of the set [1, n]. For each i  I , select a random element vi . Then, send Q = {(i, vi )} to the prover. Check the prover's response 1 , · · · , s and  via e(, g ) = e(
(i,vi )Q ? s

H (name

i ) vi ×
j =1

uj j , v )



P (pk, , M  ): Compute j =

(i,vi )Q

vi mij for 1  j  s,  =

(i,vi )Q

i vi

Features of the scheme: The server's response is short due to the homomorphic properties that aggregate the proof into one authenticator value (Shacham and Waters(2008)). Drawbacks of the scheme: By sending the server's response, which consists of the linear combination of the sampled blocks, to the verifier, the user's data may leak to the verifier. Hence, the public verification scheme does not support privacy-preserving property (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). Privacy-Preserving Schemes In these papers (Shah et al.(2008)Shah, Swaminathan, and Baker; Shah et al.(2007)Shah, Baker, Mogul, and Swaminathan), the proposed protocol allows the TPA to check the integrity of the stored data and assist in returning the data intact to the user. The protocol supports privacy-preservation property, so the TPA cannot learn the content of user's data. 16

CHAPTER 2. LITERATURE SURVEY

2.1. DATA AUDITING IN CLOUD COMPUTING

Protocol Overview: The user who wants to store his data to untrusted server allows the TPA to verify the stored data without revealing the data content to the auditor. The user encrypts the data with a secret key and sends both of them to the server. He also sends the encrypted data with a keycommitment, which fixes a value for the key without revealing the key, to the TPA. The TPA can check if the server has intact both the encrypted data and encryption key without learn any information about the key or the data. He also assists in returning the key and encrypted data to the user in a privacy manner (Shah et al.(2008)Shah, Swaminathan, and Baker; Shah et al.(2007)Shah, Baker, Mogul, and Swaminathan). Definition 8 (Privacy-preserving PDP scheme). The scheme consists of three phases: initialization, audit, and extraction (Shah et al.(2008)Shah, Swaminathan, and Baker; Shah et al.(2007)Shah, Baker, Mogul, and Swaminathan). Privacy-Preserving PDP Scheme Details: · The user encrypts the data EK (M ) with the secret key K , then sends both of them to the server. The user sends EK (M ) with a key-commitment g K to the TPA. Upon the agreement between the parties, the server sends the key-commitment g K , and a hash of the encrypted data H (EK (M )). The TPA checks whether both the user and the server agree on a common key and encrypted data. · To verify the encrypted data, the TPA generates n random numbers R1 , · · · , Rn and computes L ~ 1, · · · , H ~ n , where H ~ i = HM AC (Ri , EK (M )). The TPA keeps the pairs hashes H ~ 1 ) , · · · , ( Rn , H ~ n )} and deletes the encrypted data. The TPA chooses randomly L = {(R1 , H ~ j ) from L and now L = L \ {(Rj , H ~ j )}, and sends it to the server as a challenge. The ( Rj , H ~ s = HM AC (Rj , EK (M )) and sends it to the TPA. The TPA server computes the respond H ~s = H ~j . checks if H · To verify the encryption key, the TPA selects randomly  , computes g  , and sends it to the server. The server computes Ws = g K and sends it to the TPA who computes Wa = (g K ) and checks if Wa = Ws . · To extract the encrypted data, the server sends EK (M ) to the TPA who checks whether his local copy of the encrypted data matches what he receives. If so, he sends the EK (M ) to the user. · To extract the encrypted key, a trusted fourth party generates a random secret key R and sends it to the server and user. The trusted fourth party also sends a secret-commitment key g R to the TPA. The server sends blinded version of the key Bs = K + R to the TPA who checks the blinded-key using the key-commitment and secret-commitment as g Bs = g K g R = g K +R and sends Bs to the user. The user computes Bs - R = K and recovers the original key. Features of the scheme: The scheme allows public auditing of the user's data and the encryption key while supports privacy-preservation property. Moreover, the protocol can extract the digital contents from the server and deliver it to the user (Shah et al.(2008)Shah, Swaminathan, and Baker; Shah et al.(2007)Shah, Baker, Mogul, and Swaminathan). 17

2.1. DATA AUDITING IN CLOUD COMPUTING

CHAPTER 2. LITERATURE SURVEY

Drawbacks of the scheme: It allows bounded number of challenges because the total number of challenges depends on the number of list values L. The TPA needs to maintain and update state between the auditing process, so the scheme does not support stateless verification (Shah et al.(2008)Shah, Swaminathan, and Baker; Shah et al.(2007)Shah, Baker, Mogul, and Swaminathan). The authors of (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou), propose a secure cloud storage system supporting privacy-preserving public auditing. Beside this, they provide an efficient TPA with multiple auditing tasks in a batch manner. In their proposed protocol, they utilize public key based homomorphic linear authenticator (HLA), which is based on BLS-based POR Scheme, with random masking. The proposed protocol guarantees that the TPA could not learn any knowledge about the stored data content (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). Definition 9 (Privacy-preserving POR scheme). The scheme consists of four algorithms: KeyGen, SigGen, GenProof, and VerifyProof. It consists of two phases: Setup (includes the first two algorithms) and Audit (includes the last two algorithms) (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). Privacy-Preserving POR Scheme Details: Let G1 , G2 , and GT be multiplicative cyclic groups of

prime order p, and e : G1 × G2  GT be a bilinear map. Let g be a generator of G2 . H (·) is a secure map-to-point hash function H :{0, 1}  G1 , and h(·) is a hash function h: GT  Zp . · Setup phase: ­ KeyGen: The user chooses a random signing key (spk, ssk ). He also selects a random x  Zp , computes v = g x , and selects a random u  G1 . The user's secret parameter is sk = (x, ssk ), and public parameters are pk = (spk, v, g, u, e(u, v )). ­ SigGen: Given a data file F = {mi }. The user computes the signature of each block as i = (H (Wi ) × umi )x  G1 , where Wi = name i. name  Zp is chosen randomly as the file ID. The file tag is computed as t = name SSigssk (name) Then, the user sends {F = {mi }, {i }, t} to the server and deletes the local copy. · Audit phase: ­ TPA retrieves the file tag t and verifies the signature. If it is true, he recovers name. TPA can check the integrity of files/blocks on behalf of the users by sending chal = {(i, i )} to the cloud server, where i  I = {s1 , · · · , sc } for set of blocks [1, n] and i is a random value. ­ GenProof: The cloud server selects r  Zp , and computes R = e(u, v )r  GT . CS computes the linear combination of the sampled blocks as  = iI i mi , and blind it with r as i  = r +  , where  = h(R). CS also computes an aggregated signature  = iI i . Then, CS sends {, , R} to the TPA. ­ VerifyProof: The TPA computes  = h(R), and verifies {, , R} via: R × e(  , g ) = e((
i I ?

( H ( Wi )  i )  × u  , v )

18

2.2. PUBLIC AUDITING WITH DATA DEDUPLICATION SCHEMES CHAPTER 2. LITERATURE SURVEY Features of the scheme: The scheme is efficient in terms of providing privacy-preserving public auditing and supporting batch auditing from different users. There is constant communication overhead for the server's response due to applying HLA technique (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou).

2.2

Public auditing with data deduplication Schemes

In this paper (Zheng and Xu(2012)), the authors propose Proof of Storage with Deduplication (POSD) scheme. The scheme provides secure and efficient cloud storage. To improve the security of cloud storage, PDP and POR are introduced for checking the verification of the stored data. To improve the efficiency of cloud storage, Proof of Ownership (POW), data deduplication is introduced to eliminate the duplicated data thus reduce communication and storage overhead. The deduplication is done on the cloud server. To enable that, the data will be stored in the cloud in plaintext form. Protocol Overview: A user sends his file with its tag to the cloud. Then the user claims that he has a file in the cloud, which is already sent by another user. The cloud plays the role of the auditor (challenge-response protocol) and sends a challenge to the user. Then the user computes a response and sends it back to the cloud. The latter verifies it (Zheng and Xu(2012)). Definition 10 (POSD scheme). The scheme consists of four algorithms: Keygen, Upload, AuditInt, and Dedup. "POSD = PDP/POR + POW" (Zheng and Xu(2012)). POSD Scheme Details:


Let G and GT are cyclic groups of prime order q , and e : G × G  GT be

a bilinear map. H1 : {0, 1}  G and H2 : {0, 1}  Zq hash functions. The file F is divided into n blocks and m symbols in Zq , Fi = (Fi1 , · · · , Fim ). f id is the file ID. · Keygen: generates key pairs. Select randomly v1 and v2 from Z p , p is another prime. Se-s -s  lect randomly sj 1 and sj 2 from Zq for 1  j  m. Set zj = v1 j1 v2 j2 mod p for 1  j  m. Let g be a generator of G. Select u randomly from G and w randomly from Z q, zg = g w . The verification key pairs set to be P Kint = {q, p, g, u, v1 , v2 , z1 , · · · , zm , zg }, and SKint = {(s11 , s12 ), · · · , (sm1 , sm2 ), w}. The deduplication key set to be P Kdup = P Kint , and SKdup = null. · Upload: For each block Fi , the user selects randomly ri1 and ri2 from Z q . Then, computes xi = m m ri1 ri2 v1 v2 mod p, yi1 = ri1 + j =1 Fij sj 1 mod q , yi2 = ri2 + j =1 Fij sj 2 mod q , ti = (H1 (f id

) × uH2 (xi ) )w . The user sends (f id, F, T agint ) to the server, where T agint = {(xi , yi1 , yi2 , ti )}1in . The server sets T agdup = T agint .

· AuditInt: the auditor or the user verifies the integrity of the file F . The auditor chooses randomly c elements I = {1 , · · · , c } set from [1, n]. He also chooses randomly  = {1 , · · · , c }, and sends chal = (I,  ) to the server. The server computes for 1  j  m, j = iI i Fij mod q , Y1 =
i I

i yi1 mod q , Y2 =

i I

i yi2 mod q , and T =

i i I ti .

The server sends

19

2.2. PUBLIC AUDITING WITH DATA DEDUPLICATION SCHEMES CHAPTER 2. LITERATURE SURVEY the response as resp = ({j }1j m , {xi }iI , Y1 , Y2 , T ) to the auditor. Then, the auditor computes j m Y1 Y2 i X = i I x  i)i , and verifies X = v1 v2 mod p, e(T, g ) = i , W = iI H1 (f id j =1 zj  i H 2 (x i ) i  I e( W u , zg ). · Dedup: the user claims that he has a file that is already sent to the server by another user. The server sends a challenge chal = (I,  ) to the user. The user computes for 1  j  m, j = iI i Fij mod q , and sends it as resp = ({i }1im ) to the server. The server computes Y1 = T =
i i  I ti . i I

i yi1 mod q , Y2 =

i I

i yi2 mod q , W =
Y1 Y2 v1 v2 j m j =1 zj

i I

H1 (f id

i) i , X =
iI

The server verifies X =

mod p, e(T, g ) = e(W u

i i I x i ,  i H2 (x i )

and

, zg )

Features of the scheme: Xu(2012)).

The scheme allows public auditing with proof of ownership (Zheng and

Drawbacks of the scheme: The scheme does not support dynamic data(Zheng and Xu(2012)). In this paper (Yuan and Yu(2013)), The author propose a Public and Constant cost storage integrity Auditing scheme with secure Deduplication (PCAD) that provides secure and efficient cloud storage. To ensure data integrity, PDP and POR techniques are used. To improve storage efficiency, POW is used to eliminate the duplicated data. The scheme is based on techniques including polynomial-based authentication tags and homomorphic linear authenticators. The deduplication is done on the cloud server side on both the files and their corresponding authentication tags by aggregating the tags of the same file from different owners. The system model of this scheme has four entities: Trust Authority (TA), Data Owner, Cloud Server and User/TPA (Yuan and Yu(2013)). Protocol Overview: A user sends his file with its tag to the cloud. TPA can check the integrity of the user's file by challenge-response protocol. When another user wants to send the same file to the cloud, the latter has to check if this user has indeed the same file (or if he is also the owner of the file) by sending a challenge. Then, the user computes a proof and sends it back to the cloud. The cloud verifies it. If the verification equation holds, the user becomes an owner of the file (Yuan and Yu(2013)). Definition 11 (PCAD scheme). The scheme consists of six algorithms: KeyGen, Setup, Challenge, Prove, Verify and Deduplication (Yuan and Yu(2013)). POSD Scheme Details: Let H (·) be a hash function, G be a multiplicative cyclic group of prime order q , g be a generator of G, and u  G. F is the erasure coded file divided into n blocks with s elements: {mij }1in,0j s-1 . fa(x) is a polynomial with coefficient vector a = (a0 , a1 , · · · , as-1 ) (Yuan and Yu(2013)). · KeyGen: TA selects random number   Z q which is the master key, and generates public key j +1 of the system {g  }s . The data owner generates a signing key pair (spk, ssk ). The owner also j =0 j +1  selects random number  Zq , and computes k = g , v = g  . P K = {q, k, v, spk, u, {g  }s j =0 } , SK = { , ssk }, and M K = {}. 20

2.2. PUBLIC AUDITING WITH DATA DEDUPLICATION SCHEMES CHAPTER 2. LITERATURE SURVEY · Setup: Given F , the data owner selects randomly a file name name  Z q , and generates a file tag  = name n Signssk (name n). For each block mi , the owner generates an auj +2 f () s- 1 thentication tag as i = (uH (name i) × j =0 g mij  ) = (uH (name i) × g i ) , where i = {0, 0, mi,0 , mi,1 , · · · , mi,s-1 }. The data owner sends (F , , {i }) to the cloud server. · Challenge: A user retrieves the file tag  and verifies the signature. If it is true, he recovers name and n. The user chooses k ­elements subset K of [1, n], and two random numbers r  Z q . Then, he sends chal = {K, r} to the server. · Prove: The cloud generates {pi = ri mod q }, i  K , and y = fA (r), where A = {0, 0, iK pi mi,0 , j s+1 pi · · · , iK pi mi,s-1 }. The cloud generates  = j =2 (g  )wj = g fw () , computes  = iK i , and sends them as P rf = {, , y } to the user. · Verify: The user computes  = e(, vk -r ) = e(, g ) × e(k -y , g ).
i K

pi H (name

i) and  = u . He verifies via e(, k ) ×

· Deduplication: A user claims that he has a file F and wants to send it to the cloud that is already sent by another user. The cloud chooses d­elements subset D of [1, n], and sends D to the user who sends back the corresponding blocks. The cloud computes  = iD i ,  = iD uH (name i) , j Bj s+1 and  = e( j =2 (g  ) , k ) = e(g fB () , k ) where B = (0, 0, iD mi,0 , · · · , iD mi,s-1 ). Then the server verifies via e( , k ) ×  = e( , g ). · Auditing after Deduplication: If the file F is owned by multiple owners ownerw where w is the number of owners and the original owner is owner0 , the cloud has to aggregate the authentication tags in order to audit the file (Yuan and Yu(2013)).
+1 ­ A new owner ownerw generates his public and secret key as P Kw = {q, kw , vw , spkw , u, {g  }s j =0 } , SKw = { w , sskw }. He also generates the file tag as w = name n Signsskw (name n) j +2 s- 1 and authentication tag for each block as wi = (uH (name i) × j =0 g mij  ) w = (uH (name i) ×
j

g

f (  )
i

) w , where i = {0, 0, mi,0 , mi,1 , · · · , mi,s-1 }. The cloud aggregates tags for each blocks
W

from multiple owners as i = w=0 wi . When ownert wants to check the integrity of F , i K kw he sends chal = {K, r} to the cloud who computes  = i , k = , w  W/t, and v vw , w  W/t. The server sends P rf = {, , y, k , v } to the user who verifies it via e(, k ) × e(, v vt k -r ) = e(, g ) × e(k -y , g ), where k = k kt . Features of the scheme: The scheme allows public auditing with proof of ownership. The scheme has constant communication and computational cost on the user side. It also allows auditing after deduplication, and batch auditing (Yuan and Yu(2013)). Drawbacks of the scheme: The scheme does not support privacy-preservation property. It also does not consider deduplication on block-level. 21

2.3. AGGREGATE SIGNATURES VS. MULTISIGNATURES CHAPTER 2. LITERATURE SURVEY

2.3

Aggregate Signatures vs. Multisignatures

According to (Boneh et al.(2003)Boneh, Gentry, Lynn, and Shacham), aggregate signature is a a digital signature that can be aggregated. Suppose we have n signatures on n different messages from n different users, these signatures can be aggregated into single signatures. Multisignature is related to the aggregated signature, however all the users sign the same message and convince a verifier that each user signed the message. We give a briefly comparative survey of various aggregate signatures and multisignatures, see Table2.1, in order to choose what the suitable one that fits our needs.

2.3.1

GDH Multisignature Scheme

A multisignature scheme enables subgroup of users to sign a document in which a verifier can check if each user in the group participate in signing. Unlike aggregate signature, multisignature enables us to aggregate signatures of the same message (Boldyreva(2003)). The proposed scheme in (Boldyreva(2003)) enables us to decide the composition of the subgroup of users after the aggregation of the signatures. It also has one round of communication between the users. The signing protocol of the scheme is noninteractive (Boldyreva(2003)). Theses features fulfill our requirements to build our proposed scheme. Definition 12 (MGS scheme). The scheme consists of three algorithms: MK, MS, and MV (Boldyreva(2003)). MGS Scheme Details: Let G be a GDH group and I be the global information that consists of (g is a generator of G, p = |G|, and H : {0, 1}  G is a hash function (random oracle)). Let U = {U1 , · · · , Un } be a group of users.
xi MK: Each users Ui selects xi  Z p and computes y = g . ski = xi and pki = (p, g, H, y )

MS: Any user Uj  U wants to participate in signing M , computes and broadcasts j = H (M )xj . Let L = {Ui1 , · · · , Uil } be a subgroup of users who contributed to the signing. Let J = {i1, · · · , il}be the indices of the users. The designated signer D (implemented by any player) who knows the signer of each signature computes  = j J (j ) and outputs T = (M, L,  ) MV: The verifier takes T and the public keys of L, and computes pkL = outputs VDDH (g, pkL , H (M ),  ).
j J (pkj )

=

j J ( g

xj

) and

22

Table 2.1: Comparative survey of various aggregate signature and multisignature schemes
Interactive scheme No No Random Oracle The scheme is based on BLS and bilinear map. It works in any group where the Decision Diffie-Hellman problem (DDH) is easy, but the Computational DiffieHellman problem (CDH) is hard. The scheme is based on BLS The order of the signer is required Security Model Assumption Notes

Scheme name

1

BGLS (Boneh et al.(2003)Boneh, Gentry, Lynn, and Shacham)

The scheme must be on Distinct Messages.

2

MGS (Boldyreva(2003))

No

No

Random Oracle

2.3. AGGREGATE SIGNATURES VS. MULTISIGNATURES CHAPTER 2. LITERATURE SURVEY

23
No No Standard The scheme is based on Waters Signature scheme (Waters(2005)) No Yes Random Oracle The scheme is based on MGS and bilinear map. The CDH is hard relative to its associated bilinear-group generator G. bilinear map, computational Diffie-Hellman(CDH) is hard. Decision DiffieHellman (DDH) is easy. yes Yes Random Oracle

The subset of signers should not be known in advance. It requers one round of communication for the scheme generation protocol. The signature length and verification time is independent of the size of the subgroup and is almost the same as for the base signature scheme.

3

LOSSW (Lu et al.(2006)Lu, Ostrovsky, Sahai, Shacham, and Waters)

4

OMS (Boldyreva et al.(2007)Boldyreva, Gentry, O'Neill, and Yum)

The scheme has constant size (constant-size keys)

5

AS-CPO (Wen and Ma(2008))

The scheme requires only two pairing operations in verification which is independent of the number of signers. The scheme requires no MapToPoint hash function which requires more computation overhead than ordinary cryptography hash function.

Chapter 3

Design and Implementation
In this chapter, we describe in details our proposed schemes that achieve both data integrity and storage efficiency. We present an extension to support batch auditing and user revocation.

3.1

Problem Statement

Figure 3.1 depicts a typical setting for our proposed protocol. X-enterprise users want to send their data files to the cloud server. Later, they can check the correctness of their stored data by allowing the TPA to verify the data integrity. But before sending the data to the cloud, the users calculate the signature of the data for the integrity verifications. Then, they send them to the mediator. The latter calculates the hash value of the data and compares them to identify the duplicated data. Thus, the mediator has two jobs: first, eliminating the duplicated blocks before sending the data to the cloud, so we reduce the amount of the uploaded data. Hence, that minimizes the bandwidth between the enterprise and the cloud server. Second, aggregating the signature of the duplicated data. Later, the TPA can check the integrity of the stored data on behalf of the users or the mediator. The users can contact the cloud for downloading their stored files. In addition, they can directly contact the TPA to issue auditing processes.

3.1.1

System Model

The architecture of the proposed scheme includes a cloud storage service with four entities. · Cloud User (U): who wants to store his data in cloud storage server. · Cloud Server (CS): is a data storage service with huge storage capacity and computational resources provided by a Cloud Service Provider (CSP). · Third Party Auditor (TPA): who can verify the integrity of U's data upon request and on their behalf. 25

3.1. PROBLEM STATEMENT

CHAPTER 3. DESIGN AND IMPLEMENTATION

Figure 3.1: The architecture of the proposed scheme · Mediator: who performs block-level deduplication on users' data before they are sent to the CSP, and also computes an aggregate signature for the duplicated blocks instead of sending multiple signatures with each block.

3.1.2

Threat Model

· Mediator: is trusted and allowed to see the content of the blocks and their signatures, but it is prohibited from knowing the private keys of the users, so it cannot generate a valid signature on behalf of any user. · TPA: is semi-trusted and allowed to check the integrity of the blocks on behalf of the users/mediator, but it is prohibited from seeing the content of the blocks. · CS: is semi-trusted and allowed to see the content of the blocks and their signatures, but it is required to follow the steps needed for the auditing process.

Protocol Overview:
Users have files that are to be sent to the cloud. Each user divides his file into n blocks and computes the signature of each block for the integrity verification using his private and public keys. Then, the users send their files (blocks) with the signatures to the mediator. There is no interaction between the users during the signing process. The mediator calculates the hash value of each block and compares them to identify duplicated blocks. Then, it calculates the aggregated signature of the duplicated blocks utilizing the multisignature scheme of (Boldyreva(2003)). So, Instead of sending one (identical) block with multiple signatures from multiple users, the mediator sends the aggregated signature with the block and deletes the local copy. The metadata of the deduplication process is stored locally and in the cloud. 26

CHAPTER 3. DESIGN AND IMPLEMENTATION

3.2. DESIGN GOALS

The TPA checks the data integrity on behalf of the users or the mediator upon request. To do so, it sends a challenge message to the cloud server to make sure that the cloud has retained the data properly. To generate that challenge, the TPA picks random c-element subset of set [1, n]. For each element, the TPA chooses a random value. The challenge message specifies the positions of the blocks required to be checked. The cloud server generates a proof of data storage correctness and sends it to the TPA who verifies the proof using a verification equation.

3.2

Design Goals

1. Public auditability: enables the TPA to check the integrity of the data stored in the cloud on behalf of the user/mediator. 2. Storage correctness: ensures that the cloud cannot cheat and pass the auditing process without having stored the data intact. 3. Client-side deduplication at block-level: allows the Mediator to eliminate the duplicated blocks before sending the data to the cloud. 4. Privacy-preserving: ensures that the TPA cannot learn anything about content of the stored data during the auditing process. 5. Lightweight: provides the scheme with low communication and computational costs. 6. Batch auditing: allows the TPA to perform multiple auditing tasks at the same time from different users. We design schemes that may match these goals. For example, the first scheme provides public auditabiltiy, storage correctness, and client-side deduplication. The second scheme provides goal 1 to goal 5. The last scheme involves all the goals.

3.3
3.3.1

Scheme Details
Public Auditing in Cloud with Data Deduplication Scheme

· Let G1 , G2 , and GT be multiplicative cyclic groups of prime order p, and e : G1 × G2  GT be a bilinear map. Let g be a generator of G2 . H (·) is a secure map-to-point hash function H :{0, 1}  G1 . · We define the users as U = {u1 , · · · , um }. User ui has files Fi . The blocks are denoted by mi,j where 1  i  k and 1  j  n, for some k and n. · Setup phase: 27

3.3. SCHEME DETAILS

CHAPTER 3. DESIGN AND IMPLEMENTATION

Figure 3.2: Case 1: Two users have the same file ­ KeyGen: Each user ui selects a random xi  Zp , computes yi = g xi , and selects a random ui  G1 . The user ui 's public and private keys (ski , pki ) are set to be (xi , (yi , g, ui )) ­ SigGen: Each user computes the signature of each block of his file as i,j = (H (mi,j ) × m ui i,j )xi  G1 . Then each user ui sends (Fi = {mi,j }, {i,j }) to the Mediator. ­ Dedup: The deduplication is done by the Mediator at the block-level. The mediator calculates the hash value of each block and compares them to identify the duplicated ones. Then, he calculates the aggregated signatures of the duplicated blocks as j = i,j . After that, he sends (F = {mj }, {j }, L) to the cloud, where L= the subgroup of users. · Audit phase: ­ The TPA can check the integrity of files/blocks on behalf of the users/mediator by sending chal = {(s, vs )} to the cloud server s  I = {s1 , · · · , sc } of set of blocks [1, n]. ­ GenProof: the cloud server computes  = {, , {H (ms )}} to the TPA.
s I

vs ms , and  =

s I

vs s . Then, he sends

­ VerifyProof: The TPA verifies {, , {H (ms )}} via: e(, g ) =
i L ?

e((
s I

( H ( ms ) v s ) × u  i , yi )

(3.1)

Note: TPA should not know (ms ), thus he couldn't calculate the hash value of it. So, the cloud server has to send {{}, , {H (ms )}} to the TPA. The following steps show the correctness of the equation: e(, g ) = e( =
i L

(
s I i L

s x i vs (H (ms ) × um i ) ) , g)

e(
s I

xi s vs (H (ms ) × um i ) ,g )

=
i L

e((
s I

( H ( ms ) v s ) × u  i , yi )

3.3.2

Privacy-Preserving Public Auditing in Cloud with Data Deduplication Scheme

For the sake of clarity, we begin with the case where two users have identical files. Then, we extend this case to where the two users have identical blocks of (possibly different) files. 28

CHAPTER 3. DESIGN AND IMPLEMENTATION Case 1: Two users have the same file

3.3. SCHEME DETAILS

Suppose we have two users who have the same file (same blocks) as described in Figure 3.2. With no interaction, they send their files and signatures to the mediator who performs a block-level deduplication on the the files and aggregates the signatures of the duplicated blocks. Then, the mediator sends the files and the signatures after the deduplication process to the cloud. Assuming that User 1 asks the TPA to check the correctness of his file which is owned by another User 2, we have the following: · Let G1 , G2 , and GT be multiplicative cyclic groups of prime order p, and e : G1 × G2  GT be a bilinear map. Let g be a generator of G2 . H (·) is a secure map-to-point hash function H :{0, 1}  G1 , and h(·) is a hash function h: GT  Zp . We define the users as U = {u1 , · · · , um }. User ui has files Fi . The blocks are denoted by mi,j where 1  i  k and 1  j  n, for some k and n. · Setup phase: ­ KeyGen: Each user ui selects a random xi  Zp , computes yi = g xi , and selects a random ui  G1 . The user ui 's public and private keys (ski , pki ) are set to be (xi , (yi , g, ui , e(ui , yi ))). ­ SigGen: Each user computes the signature of each block of his file as i,j = (H (mi,j ) × m ui i,j )xi  G1 . Then, each user ui sends (Fi = {mi,j }, {i,j }) to the Mediator. ­ Dedup: The deduplication is done by the Mediator at the block-level. The mediator calculates the hash value of each block and compares them to identify the duplicated ones. Then, it calculates the aggregated signatures of the duplicated blocks as j = i,j . For example,
A xi  A = i  L ( H ( mA ) × u m and so on to the end of the file. After that, the mediator sends i ) (F = {mj }, {j }, L) to the cloud, where L= the subgroup of users.

· Audit phase: ­ TPA can check the integrity of files/blocks on behalf of the users/mediator. TPA sends chal = {(s, vs )} to the cloud server s  I = {s1 , · · · , sc } for set of blocks [1, n]. ­ GenProof: The cloud server selects r  Zp , and computes Ri = e(ui , yi )r  GT . CS also computes R = R1 × ... × Rm , L = y1 · · · ym , and  = h(R L). CS computes as well vs  = sI vs ms ,  = r +  , and  = sI s . Then, CS sends {, , {H (ms )}, R, L} to the TPA. ­ VerifyProof: The TPA computes  = h(R L), and verifies {, , {H (ms )}, R, L} via: R × e(   , g ) =
i L ?

e((
s I

( H ( ms ) v s )  × u  i , yi )

(3.2)

29

3.3. SCHEME DETAILS

CHAPTER 3. DESIGN AND IMPLEMENTATION

Figure 3.3: Case 2: Two users have some identical blocks The following steps show the correctness of the equation: R × e(   , g ) =
i L

e(ui , yi )r × e((
s I

vs  s ) , g)

=
i L

e(ui , yi ) × e(
r

(
s  I i L

s x i vs  ((H (ms ) × um i ) ) ) , g)

=
i L

e(ui , yi ) ×
r i L

e(
s I vs 

s ms  ( H ( ms ) v s × u v ) , yi ) i

=
i L

e(
s I

  +r (H (ms ) ) × ui , yi )

=
i L

e((
s I

(H (ms )vs ) × u i , yi )

Case 2: Two users have some identical blocks Suppose we have two users who have some identical blocks as described in Figure 3.3. With no interaction, they send their files and signatures to the mediator who performs the block-level deduplication on the the files and aggregates the signatures of the duplicated blocks. Then, the mediator sends the files and the signatures after the deduplication process to the cloud. Assuming that User 1 asks the TPA to check the correctness of his file F1 = {A, B, C } which some of his blocks are owned by another User 2, F2 = {A, C }, we have the following: · Setup phase: KeyGen, SigGen, and Dedup as in the previous case. The mediator calculates the aggregated signatures of the duplicated blocks as j = i,j . For example, A = iL (H (mA ) × mB x1 A xi um since this block is owned only by User 1, and C = iL (H (mC )× i ) ,  B = ( H ( mB ) × u 1 ) mC x i ui ) . · Audit phase: ­ TPA can check the integrity of files/blocks on behalf of the users/mediator by sending chal = {(s, vs )} to the cloud server s  I = {s1 , · · · , sc } for the set of blocks [1, n]. ­ GenProof: The cloud server selects r  Zp , and computes Ri = e(ui , yi )r  GT . CS also computes R = R1 × · · · × Rm , L = y1 · · · ym , and  = h(R L). CS computes as well i = sI vs ms , i = r + i for each user ui , and  = {{i }, , {H (ms )}, R, L} to the TPA. 30
s I vs s . Then CS sends

CHAPTER 3. DESIGN AND IMPLEMENTATION

3.4. SUPPORT FOR BATCH AUDITING

­ VerifyProof: The TPA computes  = h(R L), and verifies {{i }, , {H (ms )}, R, L} via: R × e(   , g ) =
i L ?

e((
s I

i (H (ms )vs ) × u i , yi )

(3.3)

The following steps show the correctness of the equation: R × e(   , g ) =
i L

e(ui , yi )r × e((
s I

vs  s ) , g)
s x i vs  (H (ms ) × um i ) ) ) , g)

=
i L

e(ui , yi ) × e((
r s I

=
i L

e(ui , yi ) ×
r i L

e(
s I vs 

s ms  (H (ms )vs ) ) × uv , yi ) i

=
i L

e(
s I

(H (ms ) ) × ui i

  +r

, yi )

=
i L

e((
s I

i ( H ( ms ) v s )  × u  i , yi )

3.4

Support for Batch Auditing

The TPA can handle numerous auditing tasks from different users. It may be inefficient to treat them as an individual tasks rather than batch them together and audit at the same time. Suppose we have K auditing tasks on K distinct files from different users. To achieve the batch auditing, the K verification equations are aggregated (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou).

3.4.1

Batch Auditing of Case 1: Two users have the same file

Suppose there are L users who want to check K distinct files by delegating K auditing tasks. Each auditing task consists of two users who have the same file as described in Figure 3.4. User 1 delegates auditing Task 1 (k = 1) on file F 1 which is owned also by User 2, User 3 delegates auditing Task 2 (k = 2) on file F 2 which is owned also by User 4, and User 5 delegates auditing Task 3 (k = 3) on file F 3 which is owned also by User 6. Each auditing task has two users, so Lk = 2 for each task k . In General, each user k has a file Fk = {mk,1 , · · · , mk,n }, which is also owned by another user uk,i , where k  {1, · · · , K } and i  {1, · · · , L}. Setup phase: Each user uk,i selects a random xk,i  Zp , computes yk,i = g xk,i , and selects a random uk,i  G1 . Denote the secret key and the public key of each user uk,i is (skk,i , pkk,i ) = (xk,i , (yk,i , g, uk,i , e(uk,i , yk,i ))). Then, each user computes the signature of each block of his file as mk,j xk,i k,j = (H (mk,j ) × uk,i )  G1 . Then, he sends (Fk,i = {mk,j }, {k,j }) to the Mediator. The Mediator calculates the hash value of each block and compares them to identify the duplicates. Then, it calculates the aggregated signatures of the duplicated blocks as k,j = k,i,j . After that, the mediator sends (Fk = {mk,j }, {k,j }, Lk ) to the cloud, where L= the subgroup of users. Audit phase: TPA sends the audit challenge chal = {(s, vs )} to the cloud server for auditing data files 31

3.4. SUPPORT FOR BATCH AUDITING

CHAPTER 3. DESIGN AND IMPLEMENTATION

Figure 3.4: Batch Auditing of Case 1 of all K users. Upon receiving chal, cloud server selects r  Zp and computes Rk,i = e(uk,i , yk,i )r  GT . Cloud server also computes R = R1,1 × · · · × RK,m ,and L = y1,1 · · · yK,m . Then, he computes  = vs h(R L). The cloud generates the proof as follows: k = sI vs mk,s , k = r + k , and k = sI k,s . Then, the cloud sends {{k }, {k }, {H (mk,s )}, R, {Lk }} to the TPA. To verify the response, the TPA computes  = h(R L). Next, he verifies {{k }, {k }, {H (mk,s )}, R, {Lk }} via:
K

R × e(
k=1

 k , g) =

?

K

(
k=1 iL

e((
s I

k (H (mk,s )vs ) × u k,i , yk,i ))

(3.4)

The following steps show the correctness of the equation:
K K  k , g ) = Rk,i × · · · × RK,m × k=1 K k=1  e ( k , g)

R × e(

=
K

(
k=1 iL

 Rk,i × e(k , g ))

=
K

(
k=1 iL

 e(uk,i , yk,i )r × e(k , g ))

=

(
k=1 iL

e((
s I

k (H (mk,s )vs ) × u k,i , yk,i ))

3.4.2

Batch Auditing of Case 2: Two users have some identical blocks

Suppose there are L users who want to check K distinct files by delegating K auditing tasks. Each auditing task consists of two users who share some blocks as described in Figure 3.5. User 1 delegates auditing Task 1 on file F 1 which has some identical blocks from F 4 which is owned by User 4, User 2 delegates auditing Task 2 on file F 2 which has some identical blocks from F 5 which is owned by User 5, and User 3 delegates auditing Task 3 on file F 3 which has some identical blocks from F 6 which is owned by User 6. Each auditing task has two users, so Lk = 2 for each task k . In General, each user k 32

CHAPTER 3. DESIGN AND IMPLEMENTATION

3.4. SUPPORT FOR BATCH AUDITING

Figure 3.5: Batch Auditing of Case 2 has a file Fk = {mk,1 , · · · , mk,n }, and some of these blocks are also owned by another user uk,i , where k  {1, · · · , K } and i  {1, · · · , L}. Setup phase: As in the previous one. Audit phase: TPA sends the audit challenge chal = {(s, vs )} to the cloud server for auditing data files of all K users. Upon receiving chal, cloud server selects r  Zp and computes Rk,i = e(uk,i , yk,i )r  GT . Cloud server also computes R = R1,1 × · · · × RK,m , and L = y1,1 · · · yK,m . Then, he computes  = h(R L). The cloud generates the proof as follows: computes the linear combination of sampled blocks for each user in L in each task in K as k,i =
vs sI k,s s I

vs mk,s , k,i = r + k,i . Then, he computes k =

for each task in K . Next, the cloud sends {{k,i }, {k }, {H (mk,s )}, R, {Lk }} to the TPA. To verify the response, the TPA computes  = h(R L). Then, he verifies {{k,i }, {k }, {H (mk,s )}, R, {Lk }} via: R × e(
k=1 K  k , g) = ? K

(
k=1 iL

e((
s I

k,i (H (mk,s )vs ) × uk,i , yk,i ))



(3.5)

The following steps show the correctness of the equation:
K K  k , g ) = Rk,i × · · · × RK,m × k=1 K k=1  e( k , g)

R × e(

=
K

(
k=1 iL

 Rk,i × e(k , g ))

=
K

(
k=1 iL

 e(uk,i , yk,i )r × e(k , g ))

=

(
k=1 iL

e((
s I

k,i (H (mk,s )vs ) × uk,i , yk,i ))



33

3.5. USER REVOCATION

CHAPTER 3. DESIGN AND IMPLEMENTATION

(a) Before User 2 is revoked, block D is signed by User 2

(b) After User 2 is revoked, block D is signed by Mediator

Figure 3.6: User Revocation

3.5

User Revocation

Inside the Enterprise, the users work as a group. If any user wants to leave the group (or enterprise), the blocks that signed by the revoked user need to be re-signed by another user which is in our architecture the mediator. The simple solution is the mediator has to compute a signature of the block under his private key. Then, he computes a new aggregate signature of the block and sends it to the cloud as an updated version of the stored one as new = old × M ed /U2 . Then, he has to send new L to the cloud as an update. This way has a major drawback which is the mediator has to download the block from the cloud in order to compute the signature. By utilizing the proxy re-signature proposed by (Wang et al.(2013b)Wang, Li, and Li), we allow the cloud to convert signatures of the blocks of the revoked user into Mediator's signature while preserving the privacy of the secret keys of both the revoked user and the mediator. See Figure 3.6, the block D is signed by User 2 U2 . After the user revocation, the same block is signed by Mediator M ed. Since the cloud has only the aggregated signature of block D which consists of the signatures of User 1 U1 and of User 2 U2 , Proxy re-signature scheme cannot be directly adopted. The solution is: First, the cloud has to remove the signature of the revoked user from the aggregated signature he has as Dtemp = D /DU 2 . Second, using the proxy re-signature scheme, the cloud converts the signature of the block into the mediator signature. Finally, he aggregates the result with the new signature of the block as Dnew = Dtemp × DM ed . Proxy Re-signature Scheme: · In the Setup phase: the mediator as well generates his public and private keys. (skM ed , pkM ed ) are set to be (xM ed , (yM ed , g, uM ed , e(uM ed , yM ed ))). 34

CHAPTER 3. DESIGN AND IMPLEMENTATION

3.5. USER REVOCATION

· The mediator sends the signature of the block of the revoked user to the cloud as DU 2 . · The cloud generates a re-signing key as follows: he selects a random value r  Zp and sends it to the user U2 who computes and sends r/xU2 to the mediator. The mediator computes and sends r × xM ed /xU2 to the cloud. Finally, the cloud recovers the re-signing key xM ed /xU2 . · The cloud removes the signature of user U2 from the aggregated signature ha has as Dtemp = D /DU 2 Next, he converts the signature of U2 into the signature of the mediator as
D × (uM ed /u2 )mD )xM ed /xU2 DM ed = (H (mD ) × um 2

xM ed D DM ed = (H (mD ) × um M ed )

Finally, he computes the new aggregated signature of block D as Dnew = Dtemp × DM ed · The cloud updates L which is the subgroup of users who participate in signing block D. Notes: · Assuming that the mediator keeps a copy of all the signatures of all the blocks but not the blocks themselves.
D · By this formula DM ed = (H (mD ) × um × (uM ed /u2 )mD )xM ed /xU2 , the cloud converts not only 2 the private keys x but also the values u, which are public parameters, since each user has different

values.

35

Chapter 4

Evaluation
In this chapter, we present a detailed analysis of the security and the performance of our proposed schemes. Then, we show the efficiency of the batch auditing technique.

4.1

Security Analysis

The security guarantees of our proposed schemes are based on analyzing (a) the storage correctness and (b) the privacy preservation of these schemes (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). For the storage correctness, we have to provide proofs that the cloud server can only compute the correct reply to the TPA inquiry if the integrity of the data stored on the cloud is preserved. Such computations will be based on file or block contents in cases 1 and 2. So, although duplicate blocks or files are removed in our scheme, the reliance on the content implies that the security proofs given in (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou) and (Shacham and Waters(2008)) are directly applicable. In particular, we refer the reader to section 4 of (Shacham and Waters(2008)) where different cases for the cloud server behaviour acting as an adversary are discussed. Similar arguments for the proof of the privacy-preserving property of our schemes can be made. That is, the proof generated by the cloud server and sent to the TPA is based on the data content. Hence, if the TPA can learn about the user's data content based on the proof generated by the cloud server, it can also do the same for non-duplicated data violating the proof of theorems 2 and 3 in (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou).

4.2

Performance Analysis

The experiment was conducted using C code on a Linux system with an Intel Core i3 processor running at 2.13 GHz, 4 GB of RAM, and a 5400 RPM Toshiba 320 GB Serial ATA drive with a 8 MB buffer. We also used Pairing-Based Cryptography (PBC) library version 0.5.13, and MNT elliptic curve with base field size 159 bits and the embedding degree 6. All experimental results represent the mean of 20 trails. 37

4.2. PERFORMANCE ANALYSIS

CHAPTER 4. EVALUATION

Table 4.1: Notation of cryptographic operations, (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou)
Hasht G Addt G M ultt G Expt G( ) m-M ultExp1 G( )
t P airG 1 ,G2 t m-M ultP airG 1 ,G2

Hash t values into the group G t additions in group G t multiplications in group G t exponentiations g ai , for g  G, |ai | = t m-term exponentiations
m i=1 m i=1

g ai

t pairings e(ui , gi ), where ui  G1 , gi  G2 t m-term pairings e (ui , g i )

4.2.1

Cost Analysis

We estimate the computational cost in terms of basic cryptographic operations (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou) (see Table 4.1 for notations). User Computation: · Each user computes the signature of each block of his file as j = (H (mi,j ) × ui sends them to the mediator.
mi,j xi

)

 G1 , then

1 1 · The corresponding computation cost is j = Hash1 G1 + ExpG1 (|p|) + M ultG1 (|p|), for each block

· |j | = |p| =160 bits Mediator Computation: · Calculating the hash value of each block to identify the duplicated ones (sha-1 = 160 bits) · Then, aggregating the signature of the duplicated blocks as  = computation cost is  = M ultt G1 (t times). j , so the corresponding

· The size of the sending data (Bandwidth) depends on the rate of the deduplication. CS Computation: · CS computes  = · CS computes  =
s I s I c vs ms  Zp , so the corresponding computation cost is  = M ultc Zp + AddZp vs s  G1 , so the corresponding computation cost is  = c­M ultExp1 G1 ( | v i | )

· The size of {,  } is independent of the number of sampled blocks c, that (HLA) helps achieve constant communication overhead (Bandwidth). TPA Computaion: Verification with No Privacy-Preserving
1 · TPA computes e(, g ), so the corresponding computation cost is P airG 1 ,G 2

38

CHAPTER 4. EVALUATION

4.2. PERFORMANCE ANALYSIS

· TPA computes iL e(( sI (H (ms )vs ) × u i , yi ), so the corresponding computation cost is 1 L L 1 L(c­M ultExpG1 (|vi |)) + ExpG1 + M ultG1 + L­M ultP airG 1 ,G 2 CS Computation: with Privacy-Preserving, Case 1 · CS selects r  Zp , |p| = 160 bits · CS computes for each user Ri = e(ui , yi )r  GT , so the corresponding computation cost is Ri = Exp1 GT (|p|), and the size of Ri close to 960 bits · CS computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · CS computes  =
s I c vs ms , so the corresponding computation cost is  = M ultc Zp + AddZp

1 · CS computes  = r +  , so the corresponding computation cost is  = Add1 Zp + M ultZp

· CS computes  =

s I

vs s , so the corresponding computation cost is  = c­M ultExp1 G1 ( |v i |)

TPA Computation: Verification with Privacy-Preserving, Case 1 · TPA computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp
1 · TPA computes R × e(  , g ), so the corresponding computation cost is Exp1 G1 (|p|) + P airG1 ,G2 + M ult1 GT

· TPA computes iL e(( sI (H (ms )vs ) × u i , yi ), so the corresponding computation cost is 1 2L L 1 (c­M ultExpG1 (|vi |)) + ExpG1 + M ultG1 + L­M ultP airG 1 ,G 2 CS Computation: with Privacy-Preserving, Case 2 · CS selects r  Zp , |p| = 160 bits · CS computes for each user Ri = e(ui , yi )r  GT , so the corresponding computation cost is Ri = Exp1 GT (|p|), and the size of Ri close to 960 bits · CS computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · CS computes for each user: i =
c M ultc Zp + AddZp s I

vs ms , so the corresponding computation cost is i =

· CS computes for each user: i = r + i , so the corresponding computation cost is i = Add1 Zp + M ult1 Zp · CS computes  =
s I vs s , so the corresponding computation cost is  = c­M ultExp1 G1 ( | v i |)

39

4.2. PERFORMANCE ANALYSIS

CHAPTER 4. EVALUATION

TPA Computation: Verification with Privacy-Preserving, Case 2 · TPA computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp
1 · TPA computes R × e(  , g ), so the corresponding computation cost is Exp1 G1 (|p|) + P airG1 ,G2 + M ult1 GT
i · TPA computes iL e(( sI (H (ms )vs ) × u i , yi ), so the corresponding computation cost is 2L L 1 L(c­M ultExp1 G1 (|vi |)) + ExpG1 + M ultG1 + L­ M ultP airG1 ,G2

CS Computation: Batch auditing of Case 1 · CS selects r  Zp , |p| = 160 bits · CS computes for each user Rk,i = e(uk,i , yk,i )r  GT , so the corresponding computation cost is Rk,i = Exp1 GT (|p|), and the size of Ri close to 960 bits · CS computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · CS computes k =
s I c vs mk,s , so the corresponding computation cost is k = M ultc Zp + AddZp

1 · CS computes k = r + k , so the corresponding computation cost is k = Add1 Zp + M ultZp

· CS computes k =

s I

vs k,s , so the corresponding computation cost is k = c­M ultExp1 G1 ( |v i |)

TPA Computation: Batch auditing of Case 1 · TPA computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · TPA computes R × e( 1 P airG + M ult1 GT 1 ,G 2
K K k=1  k , g ), so the corresponding computation cost is K ­M ultExp1 G 1 ( | p| ) +

k · TPA computes k=1 ( iL e(( sI (H (mk,s )vs ) × u k,i , yk,i )), so the corresponding computation K -1 2L L 1 cost is K (c­M ultExp1 G1 (|vi |)) + ExpG1 + M ultG1 + L­ M ultP airG1 ,G2 + M ultGT

CS Computation: Batch auditing of Case 2 · CS selects r  Zp , |p| = 160 bits · CS computes for each user Rk,i = e(uk,i , yk,i )r  GT , so the corresponding computation cost is Rk,i = Exp1 GT (|p|), and the size of Ri close to 960 bits · CS computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · CS computes for each user in each task: k,i = c cost is k,i = M ultc Zp + AddZp
1 is k,i = Add1 Zp + M ultZp s I

vs mk,s , so the corresponding computation

· CS computes for each user in each task: k,i = r + k,i , so the corresponding computation cost

40

CHAPTER 4. EVALUATION · CS computes k =
s I

4.2. PERFORMANCE ANALYSIS

vs k,s , so the corresponding computation cost is k = c­M ultExp1 G1 ( |v i |)

TPA Computation: Batch auditing of Case 2 · TPA computes  = h(R L), so the corresponding computation cost is  = Hash1 Zp · TPA computes R × e( 1 P airG + M ult1 GT 1 ,G 2
K K k=1  k , g ), so the corresponding computation cost is K ­M ultExp1 G 1 ( | p| ) +

k,i · TPA computes k=1 ( iL e(( sI (H (mk,s )vs ) × uk,i , yk,i )), so the corresponding computation -1 1 2L L 1 cost is L(c­M ultExpG1 (|vi |)) + ExpG1 + M ultG1 + L­M ultP airG + M ultK GT 1 ,G 2



Experiment 1: Suppose we have two users who have the same files, case 1 for simplicity, and we want to check 100 blocks. Each auditing task consists of one file (e.g. "coding.pdf" with size of 513.4 KB and 125 blocks). This file is owned by two users. Table 4.2 shows the computation time of Mediator, CS, and TPA in seconds when the TPA checks 100 blocks of the file. The CS computed time is the time of generating the proof ,while the TPA time is the time of verifying the proof. In Figure 4.1, the chart shows that the computation time of (Mediator, CS, and TPA) is increased by increasing the number of the blocks of the file. However, the computation time of checking 100 blocks of the file consumed by the TPA is slightly larger than the computation time by CS. Table 4.2: The computation time of (Mediator, CS, and TPA) when 100 blocks are checked. Times in seconds File name size Number of blocks 125 415 1181 1536 1725 Mediator computation time in seconds 0.17 1.66 12.99 21.89 26.86 CS computation time in seconds 0.30 0.49 0.96 1.15 1.27 TPA computation time in seconds 0.32 0.53 1.05 1.27 1.39

coding.pdf 1.pdf 4.pdf 6.pdf 7.ppt

513.4 KB 1.7 MB 4.8 MB 6.3 MB 7.1 MB

Experiment 2:

Suppose we have two users who have the same files, case 1 for simplicity, and we

want to check the same file with different number of sampled blocks in each task. Each auditing task consists of one file (e.g. "7.ppt" with 1725 blocks). This file is owned by two users. Table 4.3 shows the computation time of Mediator, CS, and TPA in seconds when the TPA checks the file but every time he increases the number of sampled blocks. The computation time consumed by the Mediator is almost the same in each task since he checks the same file (same number of blocks). The CS computed time is the time of generating the proof ,while the TPA time is the time of verifying the proof. In Figure 4.2, the chart shows that the computation time of the CS and TPA is increased by increasing the number 41

4.3. BATCH AUDITING EFFICIENCY

CHAPTER 4. EVALUATION

Figure 4.1: Comparison of the time consumed by the Mediator, the CS, and the TPA when 100 blocks are checked of each file in each auditing task. Each file is owned by two users

of the sampled blocks. However, the computation time consumed by the CS is slightly larger than the TPA computation time. Table 4.3: The computation time of (the Mediator, the CS, and the TPA) when the same file is checked in each auditing task. However, the number of sampled blocks is increasing for each task. Times in seconds Number of sampled block 300 460 1000 1500 1725 Mediator computation time in seconds 27.05 27.11 26.68 27.21 27.57 CS computation time in seconds 3.74 5.64 11.70 17.84 20.74 TPA computation time in seconds 3.71 5.49 11.08 16.83 19.44

4.3

Batch Auditing Efficiency

Due to the batch auditing process, the TPA can perform multiple auditing task in the same time. First, we compare the computational cost between the single auditing task and batch auditing task in order to test whether the batch auditing process is more efficient than multiple process of the single task. Table 4.4 shows the computational cost of the Audit phase of the single and Batch auditing in Case 1, for simplicity. Precisely, we compare the computational cost of the verification equations of both ways 42

CHAPTER 4. EVALUATION

4.3. BATCH AUDITING EFFICIENCY

Figure 4.2: Comparison of the time consumed by (the Mediator, the CS, and the TPA) when the number of sampled blocks is increased in each auditing task of the same file "7.ppt". The file is owned by two users (Equations: 3.2 and 3.4). The HashZp operation is computed more in the single task than in the batch one. However, it is negligible and we may ignore it. There are more pairing operations in the Right Hand Side (RHS) of the single verification equation than in the Batch auditing equation. Moreover, the Left Hand Side (LHS) of both ways is almost the same except the batch auditing has M ultGT . The author of (Martin(2013)) computed the time needed for the mathematical operations in pairing groups using CHARM benchmarking suite. Table 4.5 shows the computation time in seconds of multiplication, exponentiations, and pairing over type D elliptic curve. We use the table to compute the computational cost of the equations above and transfer Table 4.4 to computed times in Table 4.6. The latter table shows that the computation time of the LHS of both methods is almost the same. On the other hand, the the computation time of the RHS of the batch auditing when K > 1 is less than computation time of the single task, see Figure 4.3 In summary, the batch auditing process reduces the required number of pairing operations, the most expensive operation according to Table 4.5, from (L + 1) which would be required to run the single user case K times or ((L1 + L2 + · · · + LK -1 + LK ) + K ), to ((L1 + L2 + · · · + LK -1 + LK ) + 1) thus reducing the computational costs (Wang et al.(2013a)Wang, Chow, Wang, Ren, and Lou). For example, Table 4.7 shows that the number of pairing operations in batch auditing is less than if each task in performed as individual one.

43

Table 4.4: Comparison of the computational cost between the Single and Batch auditing tasks in the Audit phase. Where K = number of tasks, and L = consists of the number of subgroup of users in each task Single auditing task  K=1,L=2 K=1,L=3 K = 2 , L = (2,2) K = 3 , L = (2,3,2) Hash1 Zp Hash1 Zp Hash2 Zp Hash3 Zp RHS
1 Exp1 G1 + P air + 1 M ultGT 1 Exp1 G1 + P air + 1 M ultGT 2 Exp2 G1 + P air + 2 M ultGT 3 Exp3 G1 + P air + 3 M ultGT 4 Exp4 G1 + P air + 4 M ultGT

LHS
4 2 c­M ultExp1 G1 + ExpG1 + M ultG1 + 1 2­M ultP airGT 6 3 c­M ultExp1 G1 + ExpG1 + M ultG1 + 1 3­M ultP airGT 8 4 2(c­M ultExp1 G1 ) + ExpG1 + M ultG1 + 1 2­M ultP airGT

3(c­M ultExp1 + Exp1 4G1 G1 ) 7 2 + 2­M ultP airG M ultG1 T 1 3­M ultP airGT 4(c­M ultExp1 + Exp2 0G1 G1 ) 1 2 + 2­M ultP airG M ult 0G1 T 2 3­M ultP airGT LHS

+ + + +

K = 4 , L = (3,3,2,2)

Hash4 Zp

Batch auditing task  K=1,L=2 K=1,L=3 K = 2 , L = (2,2) K = 3 , L = (2,3,2) Hash11 Zp Hash11 Zp Hash11 Zp Hash11 Zp RHS
1 Exp1 G1 + P airGT + 1 M ultGT 1 Exp1 G1 + P airGT + 1 M ultGT 4 2 c­M ultExp1 G1 + ExpG1 + M ultG1 + 1 2­M ultP airGT 6 3 c­M ultExp1 G1 + ExpG1 + M ultG1 + 1 3­M ultP airGT 8 4 2(c­M ultExp1 G1 ) + ExpG1 + M ultG1 + 1 1 2­M ultP airGT + M ultGT

2­M ultExp1 + G1 1 1 + M ult P airG GT T 3­M ultExp1 + G1 1 1 + M ult P airG GT T 4­M ultExp1 + G1 1 1 + M ult P airG GT T

3(Hashc + c­M ultExp1 + G1 G1 ) 7 2 + M ult + 2­ M ultP air Exp14 G1 G1 GT + 1 2 + M ult 3­M ultP airG GT T
20 10 4(c­M ultExp1 G1 ) + ExpG1 + M ultG1 + 2 2 2­M ultP airGT + 3­M ultP airGT + M ult3 GT

K = 4 , L = (3,3,2,2)

Hash11 Zp

Table 4.5: Computation time in seconds for mathematical operations in pairing groups MultG1 MultG2 MultGT ExpG1 ExpG2 ExpGT Pairing
2.558 × 10-6 2.360 × 10-5 6.992 × 10-6 5.895 × 10-5 5.314 × 10-3 1.100 × 10-3 3.798 × 10-3

Table 4.6: Comparison of the computationa time in seconds between the Single and Batch auditing tasks in the Audit phase. Where K = number of tasks, and L = consists of the number of subgroup of users in each task Computation time of single auditing task in seconds RHS K=1,L=2 K=1,L=3 K = 2 , L = (2,2) K = 3 , L = (2,3,2) K = 4 , L = (3,3,2,2)
3.864 × 10
-3

Compuation time of batch auditing task in seconds RHS LHS
-3

LHS
7.851 × 10
-3

3.864 × 10

7.851 × 10-3 1.178 × 10-2 1.571 × 10-2 2.749 × 10-2 3.928 × 10-2

3.864 × 10-3 7.728 × 10-3 1.159 × 10-2 1.546 × 10-2

1.178 × 10-2 1.570 × 10-2 2.748 × 10-2 3.925 × 10-2

3.864 × 10-3 3.928 × 10-3 3.990 × 10-3 4.051 × 10-3

Table 4.7: Comparison between the number of pairing operations in the Single and Batch auditing tasks, where K = number of task, and L = the number of subgroup of users in each task Single Auditing Task K=1, L=2 K=1, L=3 K=1, L=5 K=3, L=(2,2,2) K=4, L=(3,2,3,2) K=6, L=(5,3,5,5,3,5) K=10, L=(5,5,5,5,5,5,5,5,5,5) 3 4 6 9 14 36 60 Batch Auditing Task 3 4 6 7 11 27 51

Figure 4.3: Comparison of the computational time in the RHS of the verification equations between the Single and Batch auditing tasks

Chapter 5

Conclusions and Future Work
In this chapter, we present our conclusion and some potential directions for future work.

5.1

Conclusions

Cloud data storage is one of the most commonly used services in the Cloud Computing paradigm. This service allows users to store their data remotely in the cloud and to access it from anywhere at any time. Data integrity and storage efficiency are two key requirements of outsourcing data to the cloud. To verify the integrity of the data stored in the cloud considering the large size of that data and the limited computing resources of users, it is important to enable public auditing services which is the focus of this thesis. To increase storage efficiency, data deduplication is applied to eliminate redundant copies of the data before sending it to the cloud. We proposed a privacy-preserving public auditing scheme with data deduplication that achieves both data integrity and storage efficiency. We listed several requirements that need to be considered to construct an efficient auditing protocol. Our scheme covered most of the auditing requirements listed in 2.1.2 excepting recoverability and dynamic data operations. We can apply error correcting code to the file before it is send to the cloud, so the scheme can recover the entire data if failure occurred. The scheme also covers cross-user client side deduplication on block level that is achieved by the mediator who computes the hash value of each block to identify the duplicate data and aggregates multi-user signatures for duplicate blocks of data. So, the proposed scheme eliminates not only the duplicate data but also the signatures that used for data verification. By that, we reduce the amount of the uploaded data to the cloud storage and minimize the bandwidth between the enterprise and the cloud. Then, We presented an extension to support batch auditing when the TPA performs many auditing tasks from different users at the same time. Another extension was also presented to support user revocation utilizing proxy re-signature scheme to allow the cloud to convert the signatures of the revoked user into mediator signatures while preserving the privacy of the private keys of both the revoked user and the mediator. By the proposed scheme, we allow the cloud to convert not only the private key of 47

5.2. FUTURE WORK

CHAPTER 5. CONCLUSIONS AND FUTURE WORK

the revoked user but also one of the public parameter of that user, since each user has different private keys and public parameters. We presented a detailed analysis of the security and the performance of our proposed schemes. We also showed the efficiency of the batch auditing technique which was demonstrated in terms of the number of pairing operations in the batch auditing, which was shown to be less than if each task is performed as an individual one. In addition, the computation time in the RHS of the verification equations done by the TPA of the batch auditing is less than the computation time of the single audit. So, our protocol is computationally lightweight, and is proven secure in the random oracle model. There are some limitations of our proposed scheme which are: First, the data is static and we can solve that by considering dynamic data operations including data insertion, deletion, and multiplication. Second, the number of pairing operations, which is the most expensive one, needed for TPA verification is dependent of the number of users in each auditing task. Third, the data is not encrypted since we assume that the cloud is semi-trusted.

5.2

Future Work

The data in the cloud is not only being accessed by the user but also updated from time to time. We are planning to allow our proposed scheme to support dynamic data operations including: data insertion, deletion, and modification. In addition, we will allow the TPA to perform the verification process with constant number of pairing operations, which is the most expensive operation, which is independent of the number of users. Moreover, in terms of data deduplication in the cloud, Proof of Ownership (POW) schemes have been proposed. We want to expand our work to include POW to verify the ownership of duplicate data.

48

Bibliography
C. Wang, S. S. Chow, Q. Wang, K. Ren, and W. Lou, "Privacy-preserving public auditing for secure cloud storage," Computers, IEEE Transactions, pp. 362­375, Feb 2013. D. Harnik, B. Pinkas, and A. Shulman-Peleg, "Side channels in cloud services: Deduplication in cloud storage," Security Privacy, IEEE, vol. 8, pp. 40­47, Nov 2010. C. Soghoian. (2011, April) How dropbox sacrifices user privacy for cost savings. Accessed November 23, 2014. [Online]. Available: http://paranoia.dubfire.net/2011/04/ how-dropbox-sacrifices-user-privacy-for.html A. Boldyreva, "Threshold signatures, multisignatures and blind signatures based on the gap-diffie-hellman-group signature scheme," in Proceedings of the 6th International Workshop on Theory and Practice in Public Key Cryptography: Public Key Cryptography, ser. PKC '03. London, UK, UK: Springer-Verlag, 2003, pp. 31­46. [Online]. Available: http://dl.acm.org/citation.cfm?id=648120.747061 P. Mell and T. Grance, "The NIST definition of cloud computing," 2011. [Online]. Available: http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf L. Li, L. Xu, J. Li, and C. Zhang, "Study on the third-party audit in cloud storage service," pp. 220­227, 2011. [Online]. Available: http://dx.doi.org/10.1109/CSC.2011.6138525 S. Kamara and K. Lauter, "Cryptographic cloud storage," pp. 136­149, 2010. [Online]. Available: http://dl.acm.org/citation.cfm?id=1894863.1894876 C. Delettre, K. Boudaoud, and M. Riveill, "Cloud computing, security and data concealment," in Proceedings of the 16th IEEE Symposium on Computers and Communications, ISCC 2011, Kerkyra, Corfu, Greece, June 28 - July 1, 2011. IEEE, 2011, pp. 424­431. [Online]. Available: http://dx.doi.org/10.1109/ISCC.2011.5983874 J. Tate, P. Beck, H. Hugo Ibarra, S. Kumaravel, and L. Miklas. (2012, November) Introduction to storage area networks and system networking. An IBM Redbooks publication. [Online]. Available: http://www.redbooks.ibm.com/redbooks/pdfs/sg245470.pdf T. Rivera. (2009) Understanding data deduplication. Storage Networking Industry Association (SNIA). Accessed November 23, 2014. [Online]. Available: http://www.snia.org/sites/default/education/tutorials/2009/fall/data/ThomasRivera 49

BIBLIOGRAPHY

BIBLIOGRAPHY

UnderstandingDeduplication A Tutorial Understanding Dedupe 9-15-09.pdf J. Singh. (2009, 09) Understanding data deduplication. Druva. Accessed November 23, 2014. [Online]. Available: http://www.druva.com/blog/understanding-data-deduplication/#. UxqW4YVnibE P. Ananth and R. Bhaskar, "Non observability in the random oracle model," in Provable Security, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2013, vol. 8209, pp. 86­103. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-41227-1 5 M. Bellare and P. Rogaway, "Random oracles are practical: A paradigm for designing efficient protocols," in Proceedings of the 1st ACM Conference on Computer and Communications Security, ser. CCS '93. New York, NY, USA: ACM, 1993, pp. 62­73. [Online]. Available: http://doi.acm.org/10.1145/168588.168596 D. Boneh, B. Lynn, and H. Shacham, "Short signatures from the weil pairing," in Journal of Cryptology. Springer-Verlag. D. Boneh, C. Gentry, B. Lynn, and H. Shacham, "Aggregate and verifiably encrypted signatures from bilinear maps," in Proceedings of the 22Nd International Conference on Theory and Applications of Cryptographic Techniques, ser. EUROCRYPT'03. Berlin, Heidelberg: Springer-Verlag, 2003, pp. 416­432. [Online]. Available: http://dl.acm.org/citation.cfm?id= 1766171.1766207 G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner, Z. Peterson, and D. Song, "Provable data possession at untrusted stores," in Proceedings of the 14th ACM Conference on Computer and Communications Security, ser. CCS '07. New York, NY, USA: ACM, 2007, pp. 598­609. [Online]. Available: http://doi.acm.org/10.1145/1315245.1315318 K. Yang and X. Jia, "Data storage auditing service in cloud computing: challenges, methods and opportunities," World Wide Web, pp. 409­428, 2012. [Online]. Available: http://dx.doi.org/10.1007/s11280-011-0138-0 S. G. Worku, Z. Ting, and Q. Zhi-Guang, "Survey on cloud data integrity proof techniques," in Information Security (Asia JCIS), 2012 Seventh Asia Joint Conference on, Aug 2012, pp. 85­91. H. Shacham and B. Waters, "Compact proofs of retrievability," in Proceedings of the 14th International Conference on the Theory and Application of Cryptology and Information Security: Advances in Cryptology, ser. ASIACRYPT '08. Berlin, Heidelberg: Springer-Verlag, 2008, pp. 90­107. [Online]. Available: http://dx.doi.org/10.1007/978-3-540-89255-7 7 K. Zeng, "Publicly verifiable remote data integrity," in Information and Communications Security, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2008, pp. 419­434. [Online]. Available: http://dx.doi.org/10.1007/978-3-540-88625-9 28 A. Juels and B. S. Kaliski Jr., "Pors: proofs of retrievability for large files," in Proceedings of the 14th ACM conference on Computer and communications security, ser. CCS '07. New York, NY, USA: ACM, 2007, pp. 584­597. [Online]. Available: http://doi.acm.org/10.1145/1315245.1315317 50

BIBLIOGRAPHY

BIBLIOGRAPHY

M. A. Shah, R. Swaminathan, and M. Baker, "Privacy-preserving audit and extraction of digital contents," IACR Cryptology ePrint Archive, 2008. [Online]. Available: https: //eprint.iacr.org/2008/186.pdf M. A. Shah, M. Baker, J. C. Mogul, and R. Swaminathan, "Auditing to keep online storage services honest," in Proceedings of the 11th USENIX Workshop on Hot Topics in Operating Systems, ser. HOTOS'07. Berkeley, CA, USA: USENIX Association, 2007, pp. 1­6. [Online]. Available: http://dl.acm.org/citation.cfm?id=1361397.1361408 Q. Zheng and S. Xu, "Secure and efficient proof of storage with deduplication," in Proceedings of the second ACM conference on Data and Application Security and Privacy, ser. CODASPY '12. New York, NY, USA: ACM, 2012, pp. 1­12. [Online]. Available: http://doi.acm.org/10.1145/2133601.2133603 J. Yuan and S. Yu, "Secure and constant cost public cloud storage auditing with deduplication," IACR Cryptology ePrint Archive, pp. 149­149, 2013. S. Lu, R. Ostrovsky, A. Sahai, H. Shacham, and B. Waters, "Sequential aggregate signatures and multisignatures without random oracles," pp. 465­485, 2006. [Online]. Available: http://dx.doi.org/10.1007/11761679 28 B. Waters, "Efficient identity-based encryption without random oracles," in Advances in Cryptology, EUROCRYPT 2005, ser. Lecture Notes in Computer Science. Springer Berlin Heidelberg, 2005, pp. 114­127. [Online]. Available: http://dx.doi.org/10.1007/11426639 7 A. Boldyreva, C. Gentry, A. O'Neill, and D. H. Yum, "Ordered multisignatures and identity-based sequential aggregate signatures, with applications to secure routing," in Proceedings of the 14th ACM Conference on Computer and Communications Security, ser. CCS '07. New York, NY, USA: ACM, 2007, pp. 276­285. [Online]. Available: http://doi.acm.org/10.1145/1315245.1315280 Y. Wen and J. Ma, "An aggregate signature scheme with constant pairing operations," in Proceedings of the 2008 International Conference on Computer Science and Software Engineering, ser. CSSE '08. Washington, DC, USA: IEEE Computer Society, 2008, pp. 830­833. [Online]. Available: http://dx.doi.org/10.1109/CSSE.2008.941 B. Wang, B. Li, and H. Li, "Public auditing for shared data with efficient user revocation in the cloud," in INFOCOM, 2013 Proceedings IEEE, April 2013, pp. 2904­2912. R. F. Martin, "Group selection and key management strategies for ciphertext-policy attribute-based encryption," Master's thesis, Rochester Institute of Technology, New York, August 2013. [Online]. Available: www.cs.rit.edu/rfm6038/Thesis.pdf

51

