ROBUST ROBOTIC VISUAL SERVOING FOR UNCERTAIN SYSTEMS

by Akbar Assa Master of Science in Electrical Engineering, Amirkabir University of Technology, 2009 Bachelor of Science in Electrical Engineering, University of Tehran, 2006

A dissertation presented to Ryerson University in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Program of Mechanical and Industrial Engineering

Toronto, Ontario, Canada, 2015 Â© Akbar Assa 2015

Author's Declaration

I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my dissertation may be made electronically available to the public.               

ii

Abstract
ROBUST ROBOTIC VISUAL SERVOING FOR UNCERTAIN SYSTEMS
Doctor of Philosophy, 2015 Akbar Assa Department of Mechanical and Industrial Engineering, Ryerson University The control of robotic manipulators in unstructured environments is a challenging task. Exploiting the camera images for that purpose, known as visual servoing, offers an interesting solution to the problem. Classic visual servoing techniques were the first attempts towards this goal. However, these methods suffered from system's shortcomings such as ones imposed by the limited camera's field of view and the robot's reachability. Numerous approaches were proposed to overcome these limitations. Nevertheless, most of these techniques assumed full knowledge about the system and did not account for uncertainties. Uncertainties in visual servoing systems are introduced by multiple sources such as camera image noise and robot parameters. The lack of knowledge about system's parameters may lead to reduced accuracy or even total failure. Adaptive techniques were introduced previously to cope with this matter. However, those techniques were usually useful for deterministic uncertainties (e.g., camera calibration errors). Alternately, robust methods were employed to improve the performance of the system under uncertainties. Yet, those methods were usually conservative and more concerned with the stability of the system, rather than its accuracy. This work proposes three steps towards robust and accurate visual servoing. First, the pose estimation algorithm, used in many visual servoing systems, is revised by introducing novel sensor fusion techniques. Multiple fusion algorithms at different levels of estimation are introduced to enhance the accuracy and robustness of the estimation against system uncertainties. Second, a novel uncertainty estimation technique is presented to approximate the level of uncertainties induced by image noise at different levels of the system. A general approach is used for that matter which has applicability over a wide range of controllers. Finally, multiple constraint-aware and robust controllers with improved stability and numerical feasibility are iii

proposed to enhance the performance of the visual servoing systems in presence of uncertainties. The developed uncertainty model is exploited in robust control design. The effectiveness of each proposed technique is verified through numerous simulations and experiments. As it is expected, the proposed methods are capable of handling the uncertainties and enhancing the accuracy, while accounting for system constraints.

iv

Acknowledgements

First, I would like to thank my supervisor, Dr. Farrokh Janabi-Sharifi, who made this work possible. I am profoundly grateful to him for his support during my Ph.D. studies. His dedication to the research inspired me to work hard, while his great management skills taught me valuable lessons for life. It was through his guidance that I found my way in the research and was able to improve my skills in various areas. I am also thankful to him for financially supporting my work. I would also like to thank my supervisory committee members, Dr. Soosan Beheshti, Dr. Guanjun Liu, Dr. Vincent Chan, and Dr. Siyuan He, who helped me improve my thesis with their constructive feedback. I am also grateful to my internal, Dr. Hekmat Alighanbari, and my external examiner, Dr. Seth Hutchinson, for their time and consideration. I am grateful to the staff of the department of Mechanical and Industrial engineering, who helped me throughout my Ph.D. studies. I especially thank Devin Ostrom for his help and collaboration in preparation of required equipment. I am very grateful to Dr. Aleksandar Vakanski and Troy MacAvelia for the support and friendship they offered during my Ph.D. studies. They helped me a lot to adapt to my new position and overcome problems. I am also thankful to RMAL students who welcomed me with open arms and offered me their friendship. Last but not least, I would like to express my sincere appreciation to my family from the bottom of my heart, for their love and constant support. Without them, it would be impossible to finish this work. I am very thankful to my parents, Masoud and Fatemeh, who motivated me for this work and always supported me through all these years. My deepest gratitude goes to my wife, Negar, who offered me her utter love and always stood by my side. I would also like to thank my parents-in-law, Faramarz and Batoul, for their support and kindness. Finally, I am thankful to my sister Solmaz, my brother Manoochehr, and my brothers-in-law, Faraz and Christiaan, for their support and help.

v

Dedication
To my wife, Negar, and my parents, Masoud and Fatemeh, for their utter love and sincere support.

vi

Table of Contents

Author's Declaration .................................................................................................................... ii Abstract......................................................................................................................................... iii Acknowledgements ....................................................................................................................... v Dedication ..................................................................................................................................... vi List of Tables ................................................................................................................................. x List of Figures ............................................................................................................................... xi Nomenclature ............................................................................................................................. xvi Abbreviations ........................................................................................................................... xxiv 1 Introduction ............................................................................................................................ 1 1.1 Overview........................................................................................................................... 1 1.2 Motivation and Approach ................................................................................................. 5 1.3 Robotics Visual Servoing Structures ................................................................................ 8 1.3.1 IBVS ................................................................................................................. 8 1.3.2 PBVS .............................................................................................................. 10 1.3.3 HVS ................................................................................................................ 10 1.4 Contributions .................................................................................................................. 11 1.5 Thesis Structure .............................................................................................................. 12 2 Pose Estimation .................................................................................................................... 14 2.1 Introduction..................................................................................................................... 14 2.2 Literature Survey ............................................................................................................ 15 2.3 System Configuration ..................................................................................................... 17 2.4 Kalman Filter-Based Methods ........................................................................................ 20 2.4.1 Linear Kalman Filter ...................................................................................... 20 2.4.2 Nonlinear Kalman Filter ................................................................................ 22 2.4.3 Adaptive Schemes .......................................................................................... 27 2.4.4 Iterative Schemes ........................................................................................... 31 2.4.5 Iterative and Adaptive Schemes ..................................................................... 32 2.5 Gauss-Newton Method ................................................................................................... 36 vii

2.6 Virtual Visual Servoing .................................................................................................. 39 2.7 Comparison of Optimization-Based Methods ................................................................ 41 2.8 Summary ......................................................................................................................... 46 3 Sensor Fusion ....................................................................................................................... 47 3.1 Introduction..................................................................................................................... 47 3.2 Literature Survey ............................................................................................................ 48 3.3 Centralized Sensor Fusion .............................................................................................. 51 3.3.1 Centralized Kalman Filter-Based Fusion ....................................................... 51 3.3.2 Centralized VVS Fusion ................................................................................ 52 3.4 Decentralized Sensor Fusion .......................................................................................... 53 3.4.1 Decentralized Kalman Filter-Based Fusion ................................................... 53 3.4.2 Error Covariance Computation ...................................................................... 55 3.4.3 Decentralized VVS Fusion ............................................................................. 58 3.5 Pre-Processing Fusion .................................................................................................... 59 3.6 Simulation and Experimental results .............................................................................. 62 3.6.1 Centralized Fusion ......................................................................................... 63 3.6.2 Decentralized Fusion ...................................................................................... 83 3.6.3 Pre-Processing Fusion .................................................................................... 90 3.7 Summary ......................................................................................................................... 97 4 Uncertainty Modeling in Visual Servoing Systems ........................................................... 99 4.1 Introduction..................................................................................................................... 99 4.2 Literature Survey .......................................................................................................... 100 4.3 Uncertainty Modeling in SISO Systems ....................................................................... 101 4.3.1 Open-Loop Uncertainty Propagation ........................................................... 101 4.3.2 Closed-Loop Uncertainty Propagation ......................................................... 102 4.4 Error Modeling in Visual Servoing ............................................................................. 110 4.4.1 Open-Loop Approach................................................................................... 110 4.4.2 Closed-Loop Approach ................................................................................ 114 4.5 Simulation Results ........................................................................................................ 121 4.6 Summary ....................................................................................................................... 128 5 Robust and Constrained Visual Servo Control ............................................................... 130 viii

5.1 Introduction................................................................................................................... 130 5.2 Literature Survey .......................................................................................................... 131 5.3 Predictive Controllers for Constrained Visual Servoing .............................................. 133 5.3.1 Preliminaries ................................................................................................ 133 5.3.2 Image-Based Predictive Control .................................................................. 137 5.3.3 Position-Based Predictive Control ............................................................... 138 5.3.4 Hybrid Predictive Control ............................................................................ 139 5.3.5 System Constraints ....................................................................................... 141 5.4 Two-Stage Robust Control Design ............................................................................... 143 5.4.1 Decoupled Controller ................................................................................... 143 5.4.2 Robust Control with Online Predictive Controller....................................... 148 5.4.3 Robust Control with Offline Predictive Controller ...................................... 151 5.5 Simulation and Experimental Results ........................................................................... 154 5.5.1 Constrained Predictive Controllers .............................................................. 154 5.5.2 Decoupled Controller ................................................................................... 164 5.5.3 Robust Controller with Online Predictive Controller .................................. 178 5.5.4 Robust Controller with Offline Predictive Controller .................................. 184 5.6 Summary ....................................................................................................................... 189 6 Conclusions ......................................................................................................................... 190 6.1 Summary of the Thesis ................................................................................................. 190 6.2 Contributions ................................................................................................................ 191 6.3 Future Works ................................................................................................................ 192 Appendices .......................................................................................................................... 194 A. Velocity Transformation Matrix ............................................................................... 194 B. Iterative Pose Estimation ........................................................................................... 196 References ........................................................................................................................... 199

ix

List of Tables

3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9

First experiment: Iterative schemes in comparison with zero noise EKF. ......................... 65 Second experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of fine tuning. ......................................................................................................... 68 Third experiment: IAUKF fusion in comparison with UKF fusion, in case of fine tuning. 70 Fourth experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of noise mismatch. ................................................................................................... 72 Eighth experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of EIH calibration error (%2 focal length error). .................................................... 77 Ninth experiment: Pose estimation time of the different algorithms. ................................. 78 Eleventh experiment: The results of the proposed VVS fusion in comparison with those of Gauss-Newton and EKF. ..................................................................................................... 83 Thirteenth experiment: Decentralized fusion accuracy compared to that of the centralized, EKF-based, and Gauss-Newton-based fusion. .................................................................... 89 Estimation times of VVS fusion methods per estimation compared with that of EKF-based and Gauss-Newton-based fusion for 2 and 4 cameras......................................................... 90

3.10 Decentralized EKF-based fusion results compared to those of monocular EIH camera. ... 96

x

List of Figures
1.1 1.2 1.3 2.1 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 The block diagram of an IBVS system. ................................................................................ 3 The block diagram of a PBVS system................................................................................... 3 The block diagram of an HVS system................................................................................... 3 System configuration: The eye-in-hand (mobile) cameras have a direct view of the target object, while the eye-to-hand (fixed) cameras have the auxiliary object in their FOV....... 18 Decentralized sensor fusion block diagram. ........................................................................ 54 Experimental setup used for sensor fusion. ......................................................................... 63 First experiment: Pose estimation error of EKF with zero measurement noise, GaussNewton, and IEKF. .............................................................................................................. 64 Second experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion and ground truth, in case of tuned filters...... 66 Second experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of finely tuned filters. ........................................ 67 Third experiment: Pose estimation error of IAUKF fusion in comparison with UKF fusion, in case of finely tuned filters. .............................................................................................. 69 Fourth experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of noise mismatch.............................................. 71 Fifth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of initialization maladjustment. ......................... 73 Sixth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of sampling mismatch. ...................................... 74 3.10 Seventh experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of occlusion. .................................... 76 3.11 The experiment configuration of experiment 10. ................................................................ 78 3.12 Tenth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion and ground truth, in case of altered camera configuration. ............................................................................................................................................. 79 3.13 Tenth experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of altered camera configuration......................... 80 xi

3.14 Eleventh experiment: Centralized VVS pose estimation errors versus Gauss-Newton and EKF pose estimation errors. ................................................................................................ 82 3.15 First simulation: Error covariance estimation. .................................................................... 84 3.16 Second simulation: Decentralized EKF-based fusion compared with monocular pose estimators............................................................................................................................. 85 3.17 Third simulation: Decentralized EKF-based fusion compared with monocular pose estimators in case of faulty local estimation........................................................................ 86 3.18 Twelfth experiment: Decentralized EKF-based fusion in practice compared with monocular pose estimators. ................................................................................................. 87 3.19 Thirteenth experiment: Decentralized VVS pose estimation errors versus centralized VVS pose estimation error. .......................................................................................................... 88 3.20 Fourth simulation: Monocular pose estimation with an eye-in-hand camera. .................... 91 3.21 Fifth simulation: Decentralized EKF-based fusion for pose estimation with half level noise for ETH camera. .................................................................................................................. 93 3.22 Fifth simulation: Decentralized EKF-based fusion for pose estimation with equal camera noise levels. ......................................................................................................................... 94 3.23 Fifth simulation: Decentralized EKF-based fusion for pose estimation with double level noise for ETH camera.......................................................................................................... 95 3.24 Fusion with tuned weights pose estimation versus fusion with equal weights pose estimation results ................................................................................................................ 97 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 A general SISO control system. ........................................................................................ 103 First simulation: Process error covariance using a proportional controller. ...................... 122 First simulation: Process error covariance using proportional derivative (second-order) controller............................................................................................................................ 123 Second simulation: Simulated IBVS method in image space. .......................................... 124 Second simulation: Simulated IBVS method in Cartesian space. ..................................... 125 Second simulation: Camera velocities in simulated IBVS method. .................................. 126 Second simulation: Image features error covariance in comparison with its estimation. . 127 Second simulation: Camera velocity error covariance in comparison with its estimation.128 Second simulation: Camera pose error covariance in comparison with its estimation. .... 128

xii

5.1 5.2 5.3 5.4 5.5

First simulation: Visual servoing via image-based MPC with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy........................... 156 First simulation: Visual servoing via position-based MPC with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy................ 157 First simulation: Visual servoing via first hybrid controller with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy................ 158 First simulation: Visual servoing via second hybrid controller, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. .......................................... 159 Second simulation: Camera velocities of different MPC controllers without velocity penalties, (a) image-based controller, (b) position-based controller, (c) first hybrid controller, and (d) second hybrid controller. ..................................................................... 160

5.6

Third simulation: Visual servoing via image-based MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. ............................................................................................................................... 161

5.7

Third simulation: Visual servoing via position-based MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. ............................................................................................................................... 162

5.8 5.9

Third simulation: Visual servoing via first hybrid MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. . 163 Third simulation: Visual servoing via second hybrid MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. ............................................................................................................................... 164

5.10 Fourth simulation: The mean and covariance of single feature, using a traditional proportional controller with (a)   0.5 , (b)   1 , and (c)   2 . .................................. 165 5.11 Fifth simulation: The mean and covariance of single feature, using the proposed decouple controller with constant online gain ( 2  0.5 ) and variable offline gain, (a) 1  0.5 , (b)

1  1 , and (c) 1  2 . ....................................................................................................... 166
5.12 Sixth simulation: The mean and covariance of single feature, using the proposed decouple controller with constant offline gain ( 1  0.5 ) and variable offline gain: (a) 2  0.5 , (b)

2  1 , and (c) 2  2 . ...................................................................................................... 167
xiii

5.13 Seventh simulation: The statistical measurements of visually servoed system, using the proposed controller with different offline controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. ....................................................................... 169 5.14 Eigth simulation: The statistical measurements of visually servoed system, using the proposed controller with multiple online controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. ....................................................................... 171 5.15 The experimental setup...................................................................................................... 172 5.16 First experiment: The statistical measurements of visually servoed system, using the proposed controller with different offline controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. ....................................................................... 174 5.17 Second experiment: The statistical measurements of visually servoed system, using the proposed controller with two online controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. .............................................................................. 177 5.18 Ninth simulation: Visual servoing via two-stage controller with IBVS offline controller and online MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy time derivative. ...................................................................................................... 179 5.19 Tenth simulation: Visual servoing via two-stage controller with HVS offline controller and online MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy time derivative. ...................................................................................................... 180 5.20 Eleventh simulation: The norm of the position error covariance for the proposed constrained and unconstrained two-stage controller with online MPC versus a simple proportional IBVS controller............................................................................................. 181 5.21 Twelfth simulation: The norm of the position error covariance for the proposed constrained and unconstrained two-stage controller with different control weightings. ...................... 182 5.22 Third experiment: The norm of the position error covariance for, (a) two-stage controller with online MPC, and (b) a simple proportional IBVS controller. ................................... 183 5.23 Fourth experiment: The norm of the position error covariance for the two-stage controller with online MPC, (a)   103 I , and (b)   105 I . ........................................................ 183 5.24 Fifth experiment: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, and (c) Camera velocities....................................................... 184

xiv

5.25 Thirteenth simulation: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy................ 185 5.26 Fourteenth simulation: Two dimensional view of Constraint visual servoing in the presence of uncertainties with, (a) uncompensated controller, and (b) the proposed controller. ..... 187 5.27 Sixth experiment: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, and (c) Camera velocities....................................................... 188

xv

Nomenclature
ai
c First row of rotation matrix Ro
c Second row of rotation matrix Ro c Third row of rotation matrix Ro

aj
ak

Ak
c c*
c

Matrix representing the dynamics of the system Camera frame Desired camera frame Known Constant Covariance between x and s Coordinate frame of ith EIH camera Coordinate frame of jth ETH camera Error covariance matrix prediction Covariance of transferred sigma points Covariance of transferred sigma points and posterior sigma points Posterior error covariance matrix Image information from camera i Fused data Equivalent data from camera j Autocorrelation of the system measurements Coordinate frame of robot's end-effector Image features servoing error Pose servoing error Hybrid servoing error Gauss-Newton error to be minimized Translation servoing error

cov( x, s)
Cmi

Cf j

Ck|k 1
C y ,k C , y ,k

Ck
di dh

 d j

Dk e es ep eh eg et , k

xvi

eR1 ,k eR2 ,k ez , k eG eH1 , k eH 2 ,k eG ,k
exp  
E  

Orientation servoing error based on angle and axis representation Orientation servoing error based on Euler angles representation Depth servoing error General servoing error Hybrid servoing error including depth error Hybrid servoing error including full position error General servoing error Exponential function Expectation function Autocorrelation of the system innovation term Error covariance function at present time Error covariance function at present time Error covariance function of state derivatives at present time Error covariance function of state and its derivatives at present time Steady state of the error covariance function f c Function that maps the coordinates of point i to its image plane coordinates Function that maps the information from all cameras to the desired pose

Ek fc f c1 fc 2 fc3 f ss
C ) fi ( P O ,i

f  
F ( )

Nonlinear function which maps the camera pose to image feature vector Measurement mapping matrix Matrix of error covariance of image features Quality measure of the ith feature point Image features mean Image features covariance Function which maps the system output to system input Transfer function mapping the image error to pose error
xvii

Fk
Fcs ,k
gi

g m () g c ()
g  

GP

Gt G Gv G Gc ,k
 G
h  
c Ho

Transfer function mapping the rotation error to position error Transfer function mapping the camera/object rotation error to orientation error Transfer function mapping the pose error to camera velocity error Transfer function mapping the orientation error to angle and axis error Matrix of error covariance between image features and pose Transfer function mapping the camera/world rotation error to orientation error Function which maps the system input to system output Transformation matrix between the object and camera frame in camera frame Arbitrary scalar Identity matrix 66 identity matrix Arbitrary scalar Image Jacobian (interaction matrix) Translational part of the Jacobian matrix of the vector s  Rotational part of the Jacobian matrix of the vector s  Gauss-Newton Jacobian Pose Jacobian Position Jacobian Angle Jacobian Depth Jacobian Offline image Jacobian General offline Jacobian Robot Jacobian Time step Proportional control gain Derivative control gain xviii

i I I6 j J s ,k
J s ,v

J s, J J P ,k J p ,i J ,k J z ,k
J s ,k J G ,k

JR k kP kD

Kk l L Ls ,k L p ,k LH ,k LH1 , k LH 2 ,k m M M c,k n
n

Kalman filter gain Total number of the cameras Camera pose minimization cost function Cost function of image-based control Cost function of position-based control Cost function of hybrid control Cost function of hybrid control with eH1 , k servoing error Cost function of hybrid control with eH 2 ,k servoing error Deterministic error caused by calibration Intermediate matrix relating the image features time derivation and pose changes Matrix of error covariance of pose Number of selected feature points Size of the state Number of samples in adaptation window Control horizon Object frame Terms of order n or higher Target object frame Axillary object on the robot frame
C Image plane projection of point P O ,i

N Nc o

O(x n )
OT OR
C pO ,i

C P O ,i

Vector of homogenous coordinates of ith point of the object in the camera frame Vector of homogenous coordinates of point i in the object frame Process noise Robot joint position Minimum allowable value for robot joints Maximum allowable value for robot joints Covariance of the process noise xix

O P i

qk
qr qr ,min qr ,max

Qk

rk
c Ro

Measurement noise Rotation matrix between the object and camera frame in camera frame Vector of current image features Vector of desired image features Vector of current image features and their depths logarithm Vector of desired image features and their depths logarithm Minimum allowable value for image features Maximum allowable value for image features Offline image features Skew symmetric matrix Translation vector between the object and camera frame in camera frame Translational velocity of the camera in camera frame Transformation matrix relating the Euler angles to the angular velocity
C First element of the image coordinates of pO ,i

s

s*
s

s* smin smax sk
S  
c to

c t
T ()
C uO ,i

U
C vO ,i

Set of optimal camera velocities
C Second element of the image coordinates of pO ,i

Vc
Vc ,k

Camera velocity in camera frame Offline camera velocity World coordinate frame Weighting scalar Wiener-Levy process Weighting matrix First element of the translation vector
c Unnormalized first row of rotation matrix Ro

w

wk
wt
Wk
x

Xi Xj y

c Unnormalized second row of rotation matrix Ro

Second element of the translation vector

xx

^ i Y k
^ Y k

Transferred ith sigma points Mean of transferred sigma points Third element of the translation vector

z

   
06



Matrix Moore-Penrose pseudo inverse function The ith element of the bracketed vector

i
1

Matrix inverse function Estimated value 66 matrix of zeros Arbitrary coordinate frame Arbitrary coordinate frame Pose error weighting matrix Translational error weighting matrix Hybrid error weighting matrix Depth weighting matrix Hybrid error including depth error weighting matrix Hybrid error including full position error weighting matrix General weighting matrix Image error weighting matrix Quality threshold Covariance of the measurement noise Sampling time The difference between current and estimated features Error caused by uncertainty Temporary scalar used in Dementhon's pose estimation method Camera velocity noise Image noise xxi

 ^




p t H z

 H1 H2 G s


k
t
s



V s

i
  

Innovation term Roll angle Angle value in angle and axis representation of the rotation matrix Rcc Controller gain Offline controller gain Online controller gain Set of acceptable camera velocities Known Constant Vector of noisy measurements Vector form of the rotation matrix, R13 Function that converts matrices to vectors Rotational velocity of the camera in camera frame Axis vector in angle and axis representation of the rotation matrix Rcc Standard deviation of error Error covariance of x Matrix of feature error covariance Image features constraint threshold Camera velocity constraint threshold Camera position constraint threshold Translational part of camera velocity Camera velocity weighting matrix Vector of Euler angles Pitch angle Vector of unknown noise parameters Yaw angle Arbitrary positive definite matrix Vector of pose parameters xxii
* *

1 2





 13
  

c




x
 s ,k

s V P
cT,k


 







^ k |k 1 
i ^ k|k  1

State prediction
ith prior sigma points ith posterior sigma points

^ k i 
i k

State vector after i iterations Velocity transformation matrix between end-effector and the camera

c e

xxiii

Abbreviations
ARMA CAD DOF EIH EKF ETH FOV GPC GPS HPC HVS IAEKF IAUKF IBPC IBVS IEKF i.i.d. IMU LMI LQG LQR LTV MIMO MPC P PBPC PBVS PD PF Auto-Regressive Moving Average Computer Aided Design Degree Of Freedom Eye-In-Hand Extended Kalman Filter Eye-To-Hand Field Of View Generalized Predictive Control Global Positioning System Hybrid Predictive Control Hybrid Visual Servoing Iterative Adaptive Extended Kalman Filter Iterative Adaptive Unscented Kalman Filter Image-Based Predictive Control Image-Based Visual Servoing Iterative Extended Kalman Filter Independent and Identically Distributed Inertial Measuring Unit Linear Matrix Inequality Linear Quadratic Gaussian Linear Quadratic Regulator Linear Time-Varying Multi-Input Multi-Output Model Predictive Control Proportional Pose-Based Predictive Control Pose-Based Visual Servoing Proportional-Derivative Particle Filter xxiv

PnP POS POSIT RCONPC RCOFPC RVS SISO SLAM UAV UKF VVS

Perspective n-Points Pose from Orthography and Scaling POS with ITerations Robust Control with Online Predictive Controller Robust Control with Offline Predictive Controller Robotic Visual Servoing Single-Input Single-Output Simultaneous Localization And Mapping Unmanned Aerial Vehicle Unscented Kalman Filter Virtual Visual Servoing

xxv

Chapter 1 Introduction
1.1 Overview
The fast movement towards globalization has made the competition in most markets tighter than ever. The exceeding needs for reduction in production costs and time has turned automated manufacturing into an important asset to many companies. Industrial robots and particularly manipulators play a major role in manufacturing automation. These manipulators facilitate fast and accurate production, while reducing high costs. Many applications such as welding, painting, cutting, and assembling have benefited from the exploitation of industrial manipulators and the demands for these approaches in new applications are ever-increasing. In order to produce the required quality, manipulators usually require a highly-structured workspace. In many applications, the tool (e.g., welding gun, spray nozzle, etc.) is mounted on the robot's end-effector and is brought to the desired location with respect to the object of interest to carry out the required task. Therefore, the relative pose (i.e., position and orientation) of the object with respect to the tool should be known a priori. Structuring the workspace is usually very demanding for each task and requires a few days to several months, depending on the size of the workspace and the number of robots involved. In addition, the kinematics of the robot might not be accurate enough (due to system uncertainties and nonlinearities such as backlash and slippage) to be used for such purposes. In summary, conventional robotic manipulation is time-consuming, expensive and requires major revisions and programming efforts for each task. Vision-guided robot control, also known as robotic visual servoing (RVS), offers an attractive remedy to this problem. In this technique, the images of the camera taken from the object of interest are exploited to provide the system with adequate information about the current pose of the robot with respect to the object. Contrary to previous trends in which images are used in the planning level (also known as "look-and-move"), RVS involves use of images directly in the 1

execution control level. Two major groups of RVS schemes were introduced previously [1.2]. In the first group, the difference between the current and desired images (usually that between the image features) is directly exploited for servoing purposes. Therefore, this structure is known as image-based visual servoing (IBVS). In IBVS systems, the interest points of each image (i.e., image features) are extracted (denoted by s) and compared with those of the desired image (denoted by s * ). A controller is employed to reduce the image difference. The image space control action is then mapped to the camera velocity through the image Jacobian matrix (or interaction matrix). The robot is then moved based on this velocity. The process is repeated till the robot gets to its desired pose. Figure 1.1 demonstrates the block diagram of an IBVS system. Alternately, each image from the camera might be used to acquire the relative pose of the object with respect to the robot. The difference between desired and current poses (denoted by H oc and
c Ho , respectively) is then passed to the control unit for robot control. As a result, this structure is
*

called pose-based visual servoing (PBVS). The relative pose of the object with respect to camera is estimated for each image by using the image features. Various pose estimation algorithms are available for that purpose, which will be introduced in the coming chapters. Figure 1.2 exhibits the block diagram of a PBVS system. The properties of the aforementioned RVS structures were thoroughly investigated by the previous works [1.3, 1.4]. It was shown that IBVS has a good robustness against robot or camera uncertainties. These methods are simple to implement and does not require a model of the object. However, the robot trajectory of these methods might not be feasible, as a result of controlling in 2D space. In addition, the methods suffer from possible systems local minima and singularity of image Jacobian. Moreover, the visibility of the object is not guaranteed. On the other hand, PBVS systems offer smooth robot trajectories in Cartesian space and have global stability. Nevertheless, the pose estimations used in these methods are sensitive to the uncertainties of the system and usually require a 3D model of the object. Likewise, the object is likely to leave the camera's field of view (FOV), which leads to system failure. Hybrid visual servoing (HVS) schemes were introduced to alleviate the aforementioned shortcomings of IBVS and PBVS [1.5]. By sharing the control action between the image and Cartesian space, i.e. decomposing of position and orientation control, these methods were able to successfully guide the robot towards

2

the goal, providing global stability. However, the satisfaction of all system constraints was not guaranteed. Figure 1.3 depicts a block diagram view of this scheme.

s*




es

Vc

 q
q

s
Figure 1.1 The block diagram of an IBVS system.

c Ho

*

ep

Vc

 q
q

c Ho

s

Figure 1.2 The block diagram of a PBVS system.

s*




e

c

c Ho

*

e
c Ho

c

 q
q

s

Figure 1.3 The block diagram of an HVS system.

3

Despite their impressive performance in theory, RVS systems may not be as successful in practical cases. As a matter of fact, RVS systems usually suffer from system's uncertainties which lead them to accuracy reduction or even total failure. Multiple factors are responsible for system's uncertainties. One source of uncertainties in the system is improper calibration of the camera or the robot. These uncertainties remain the same throughout the servoing and are therefore deterministic. Image noise and feature extraction algorithms could be counted as other sources of uncertainties. Examples include noise or uncertainty due to the the angle of the view, lighting, etc. [1.6]. These uncertainties change throughout the servoing and have a stochastic nature. Adaptive techniques were previously introduced to compensate for uncertainties of the system (e.g., [1.7]). However, these methods are more suitable for deterministic uncertainties. Alternatively, robust systems were presented to relieve the effects of uncertainties (e.g., [1.8]); yet, most of these systems targeted specific uncertainties in the system and were mainly concerned with the stability of the system, and not its accuracy. In short, robustness of visual servoing techniques is critical to the successful integration of RVS techniques into the practice, and there are still open problems before achieving a reliable RVS approach for industry. This work presents novel approaches for robust RVS to deal with the system uncertainties.
First, sensor fusion is introduced as an effective remedy to accurately estimate the pose of the

object in spite of system uncertainties, since most RVS approaches require full or partial pose estimation during servoing. Multiple sensor fusion algorithms are introduced for fast and accurate pose estimation. For this matter, fusion at different levels of pose estimation is practiced and the results are compared to highlight the strengths and weaknesses of each algorithm. The wide range of provided fusion methods enables us to select the most efficient algorithm, considering the available resources and required characteristics.
Second, in order to simulate and predict the performance of RVS techniques, a good model of

uncertainties and errors is needed to be developed. Such model can also serve as the basis of a robust control system design. In this thesis, a novel uncertainty modeling for closed-loop RVS is developed to accurately model the propagated noise in different levels of the system. For this matter, the effect of image noise on different parts of different visual servoing systems is investigated and a proper general model is proposed. The proposed approach considers the

4

discrete-time and closed-loop nature of the visual servoing system and therefore leads to high accuracy noise estimation. This model is then exploited for robust control of the system.
Ultimately, several robust controllers are introduced for RVS as the main goal of this work, in

further attempts of improving the system accuracy and incorporating the system constraints. For that matter, in this thesis, several controllers are developed which are capable of dealing with the system constraints and expand the stability region of the system. In addition to that, other controllers are proposed to minimize the system uncertainties and provide robust performance. Robust control is made possible by introducing a novel two-stage controller structure which increases the speed of the system and predicts the outcome of the system. It is shown that this control structure is capable of minimizing the effect of uncertainties and compensate for the remaining errors effectively. Numerous simulations and experiments are conducted to certify the effectiveness of the proposed methods. While the simulations are important to verify the proposed systems in theory, experimentation is necessary to emphasize the applicability of these methods in real world tasks. It is shown that the proposed methods are capable of improving the accuracy of the system, while surviving undesired situations such as partial occlusion. In addition, these methods enable the system to fulfill the constraints in spite of system uncertainties.

1.2 Motivation and Approach
Pose estimation plays a crucial role in many of visual servoing systems as they require full pose (PBVS) or partial pose (IBVS and HVS) for servoing. The success of these visual servoing methods highly depends on the accuracy of the estimated pose. Most of the previously proposed pose estimation methods provide accurate estimations of the pose in the absence of system uncertainties (e.g., inaccurate object model, camera calibration error, etc.). However, the performance of these methods declines drastically once the uncertainties of the system are increased. Therefore, robust and accurate pose estimation is of great importance. Traditionally, monocular vision is used for that purpose. The camera may be installed on the robot's end-effector and move with it, or alternatively set apart from the robot at a known pose with respect to the object. The former configuration is known as eye-in-hand (EIH), while the 5

latter is usually referred to as the eye-to-hand (ETH) [1.9]. The EIH cameras have the advantage of higher accuracy as they can get very close to the object of interest; however, their FOV become limited for the very same reason. The reverse is true for ETH cameras. These cameras usually provide a large FOV which usually include the robot and the object of interest, while their accuracy is comparatively low. In order to benefit from both of these configurations, multicamera systems were introduced. Multi-camera systems were used in many of previous works; yet in most of those works the data from the cameras were complementary rather than redundant. Therefore, their information could not be used to improve the overall robustness and accuracy. Sensor fusion provides a tool to combine the data from multiple cameras synergistically to enhance the accuracy of the systems. A few sensor fusion works were available for improving the accuracy of pose estimation. However, those methods had limited accuracy and applications. A detailed survey of sensor fusion methods is provided in the coming chapter. As a part of robust RVS, this work presents a comprehensive design tool for pose estimation through sensor fusion. Sensor fusion is carried over three different levels of pose estimation. First, fusion at measurement level is discussed. Novel methods are proposed for measurement fusion, which are shown to have higher accuracy and degrees of robustness compared to their previous rivals. These methods process all the data from different cameras simultaneously, which result in a high-accuracy output. As a result, they are computationally expensive. Sensor fusion at the state level is presented as an alternative. These methods have reduced accuracy compared to measurement fusion techniques; however, they are more efficient computationally. This difference becomes more significant as the number of cameras increases. Finally, a fusion technique at the pre-processing stage is introduced. Unlike the previous methods, this scheme may be practiced with any pose estimation method. Once again, the outcome of this fusion technique is shown to be more accurate than monocular systems, but not as accurate as measurement fusion techniques. Despite their usefulness, the proposed fusion techniques can only reduce the effect of uncertainty in the system and are not able to fully resolve this issue. In addition, these techniques do not address the limitations of the system (e.g., those caused by the camera's FOV or robot's 6

reachability). These problems can be handled at the control level but before that, an accurate model of uncertainty seems to be necessary. This model helps to estimate the impact of uncertainties at different levels of the system which in turn facilitates the treatment of errors through robust controllers. Without this model, the control actions taken against the possible uncertainties might be conservative. Current models of uncertainty in RVS are either conservative [1.10] or restricted to a particular controller [1.11]. In order to resolve this issue, a novel methodology is proposed to estimate the effect of uncertainties at different levels of RVSs. This methodology enables us to calculate the error covariance of various signals in the system, assuming known error covariance of the image features. The main advantage of this approach is its practicality over a wide range of controllers. In addition, the discrete-time and closed-loop nature of the system is taken into account which makes the method more realistic compared to the previous modeling attempts. It is noteworthy that only the effect of image noise is considered in this work and other sources of uncertainties are left for future works. Once the uncertainty model is complete, control strategies may be taken to address the aforementioned problems of visual servoing systems. As it was discussed before, RVS systems suffer from several limitations. One important limitation of these systems is their inability to address the constraints, imposed by the structure of the system. Cameras usually have a limited FOV. This means that each image can capture only a part of the environment, usually the part that is directly in front of the camera. This limitation is a burden to RVS systems as the object of the interest may fall out of camera's FOV. Moreover, the robots used for servoing have limited reachability and motion. The length of robot's links and robot's joint limits set a boundary for the space which the robot can reach. The acceleration of the robot is usually limited by the payload and the dynamics of the robot. The velocity of robots is usually restricted for safety reasons. In summary, the system constraints are mainly imposed by the camera's limited FOV and robot's restricted reachability and motion, which are considered as the system's limitation in this work. Another major shortcoming of RVS systems is their sensitivity to system uncertainties. In particular, image noise contributes to the errors of camera velocity, which in turn affects the 7

accuracy of robot's trajectory. As a result, the robot may experience irradic motions or the system may fail if the constraints are violated. Therefore, it is important to minimize the effect of image noise on the system and compensate for any undesirable influence. This work proposes a two-level controller for this purpose. In this proposed technique, an offline controller is employed to predict the trajectories of the system and its camera velocities in the absence of any uncertainties. Next, an online controller is engaged to guide the robot as close as possible to the predicted trajectories, using the pre-calculated velocities as a part. Model predictive control (MPC) strategies are then taken to guarantee the satisfaction of system constraints. Proper changes had been made to these systems to enhance their feasibility and stability. It has been shown that the proposed control technique have the capability to minimize the error covariance of the system, while maintaining the global stability by taking the constraints of the system into consideration. Moreover, it is shown that the method can be benefited from the developed error modeling technique to predict the violation of constraints at offline stage and avoid them accordingly. As a result, the system is capable of handling the uncertainties and constraints very well.

1.3 Robotic Visual Servoing Structures
In this section, the main basic RVS techniques are introduced. This introduction helps the reader to better understand how the RVS systems work. For that matter, IBVS, PBVS, and HVS systems are briefly presented as follows.

1.3.1 IBVS
In IBVS systems, a controller is applied to guide the image features towards their desired locations. For this purpose, the image features are extracted at each time step and compared with those of the desired image (taken prior to the servoing). The servoing error is formed as the difference between these features, i.e.,
es  s*  s ,

(1.1)

where s and s * are the current and desire image feature vectors, respectively. One simple way to reduce this error exponentially to zero is to employ a proportional controller, i.e., 8

s   es  0 , e

(1.2)

where  is the controller gain. It is known that the image features are a nonlinear function of object pose with respect to the camera, formulated as follows,
sk  F (k ) ,

(1.3)

where k is the vector of pose parameters. Having the time derivative of both sides of (1.3) yields,
k  s F (k ) k .   k

(1.4)

The time derivative of the pose is related to the camera velocity. Consequently, (1.4) may be rewritten as follow,
k  J kVc , k , s

(1.5)

where Vc , k is the camera velocity in camera frame and J s ,k is the image Jacobian (or interaction matrix) at time step k, respectively. Using (1.5) along with (1.1) and (1.2) results in,

*    s*  s  , J s ,kVc,k  s
which yields,

(1.6)

*   J s,k  s*  s  . Vc,k  J s,k s

(1.7)

Here the pseudo-inverse of the Jacobian matrix is denoted by J s,k . The camera velocity is recomputed at each time step to find the new camera velocity. The camera velocity is transferred to robot's angular joint velocities in turn to move the robot accordingly. As it can be seen from (1.7), the camera velocity will be zero if the feature error happens to be in the inverse Jacobian's null space. In addition, singularities of the image Jacobian may render the inverse of the Jacobian used in (1.7) impossible. Therefore, local minima and Jacobian singularities are some of the main shortcomings of IBVS systems. Moreover, the stability of these systems are guaranteed only locally as the Jacobian matrix is a local linearization between the Cartesian and image spaces. Furthermore, the camera trajectories created by (1.7) might not be feasible for the robot due to its limited workspace.

9

1.3.2 PBVS
In these systems, the image features extracted from camera images are exploited to estimate the relative pose of the object with respect to the camera. This pose is denoted by the transformation matrix between the object and camera frame,
c  Ro H  0 c o c  to , 1

(1.8)

c c and t o are the rotation matrix and translation vector between the object and camera where Ro

frames, respectively. Once the desired pose ( H oc ) is estimated, the pose error is formed as a function of the current and the desired pose. Various pose errors may be formed for this purpose. In this section, transformation between the current and desired camera poses is exploited for that purpose, i.e.,
c H cc  H o  Hoc  .
* *

*

1

(1.9)

The pose error is then defined as a vector including the translation vector and the angle and axis representation of the rotation matrix obtained in (1.9) [1.12],
*

ep  [tcc

 ] .

(1.10)

Once again, the proportional controller similar to (1.2) is employed to reduce the error to zero exponentially. It can be shown that the camera velocity is calculated as follows [1.2],

Vc,k

  Rc* tcc c   

*

 . 

(1.11)

Such controller guides the camera to its desire pose through a straight line in Cartesian space, which is desirable. This method offers global stability assuming perfect pose calculation [1.2]. On the other hand, the features may leave the camera FOV at any time. Moreover, the pose information used for this control scheme is only an estimate of the real value and sensitive to uncertainties.

1.3.3 HVS
In order to combine the advantages of IBVS and PBVS systems, HVS systems were introduced which relayed the control partially to image space and partially to Cartesian space. For that 10

matter, the image feature used by IBVS were extended to include the logarithm of depth for each feature and the angle and axis of the rotation between the current and desired camera frames is added to the error, similar to PBVS,
* eh    s  s   ,

(1.12)

where s  and s* are the vectors of current and desire image feature plus their depth logarithm. Using the aforementioned proportional controller to reduce this error to zero yields [1.5],

Vc ,k

  J s,v ( s*  s  J s,  )   ,   

(1.13)

where J s ,v and J s, are the translational and rotational parts of the Jacobian matrix which relates the time derivative of the extended features to the camera velocity,

  k s  J s , v

J s,   Vc,k .

(1.14)

Exploiting the camera velocity introduced in (1.13) improves the stability of the system and smoothness of the system trajectories, both in image and Cartesian spaces. However, none of the constraints of the system is yet guaranteed.

1.4 Contributions
The contributions of this work are listed as follows,
        

Introduction and development of iterative adaptive extended Kalman filter (IAEKF) for centralized sensor fusion. Introduction and development of virtual visual servoing (VVS), its relation with GaussNewton method, and the proof of its superiority. Development of decentralized fusion techniques such as extended Kalman filter (EKF) and virtual visual servoing (VVS) for pose estimation. Formulation of pose estimation error for ETH cameras. Introduction and development of a pre-processing fusion technique for pose estimation. Establishment of a closed-loop error modeling methodology for visual servoing systems. Introduction and development of the two-stage control structure for robust RVS. Development of novel model predictive controllers for constrained RVS. Integration of control and uncertainty estimation for robust constraint handling. 11

1.5 Thesis Structure
The steps towards robust visual servoing are presented in consecutive chapters as follows.


The pose estimation techniques are the focus of Chapter 2. First, Kalman filter is introduced as one of the most important pose estimation techniques which can be easiliy modified for sensor fusion. The tuning problems of this method are investigated and proper solutions such as adaptive methods, iterative techniques, and iterative adaptive schemes are discussed in details. In addition, Virtual Visual Servoing (VVS) and GaussNewton methods are introduced as alternative optimization-based solutions. The close relation of these methods is investigated towards the end of this chapter.



Chapter 3 is dedicated to sensor fusion applications for robust and accurate pose estimation which is used in RVSs. The proposed sensor fusion techniques are discussed in three groups as centralized, decentralized, and pre-processing fusion schemes. It is shown that the centralized fusion techniques have the highest accuracy, while they are computationally expensive. Two novel schemes are introduced for that purpose. Next, decentralized fusion techniques are proposed as alternatives to centralized methods. The accuracy of these systems is lower than the centralized algorithms; however, they bring the advantages of reduced computational cost and straight-forward fault detection and isolation. Two methods are proposed for that reason. Finally, a pre-processing fusion technique is presented as an alternative method which could be used with any pose estimation technique. The effectiveness of each proposed techniques is demonstrated through numerous simulations and experiments.



The focus of Chapter 4 is on error modeling in RVS. A novel closed-loop error modeling strategy is proposed. Generality and simplicity are the main advantages of this method. Through such technique, the error covariance of different signals in visual servoing, including the camera velocities and robot pose, is estimated. The accuracy of the predictions is then validated through Monte Carlo simulations. These methods are then used for robust control design, introduced in the subsequent chapter.



The robust control design is discussed in Chapter 5. The goal of this chapter is to introduce novel controllers to improve the performance of RVS. First, model predictive 12

controllers with global stability are introduced for constrained visual servoing. Then, a novel two-stage controller is proposed for accurate and robust control in RVS. The first stage of this controller allows predictions to be made for the system without the uncertainties. Next, the prepared reference trajectories are exploited to reduce the effect of uncertainties in action. Model predictive controllers are exploited to protect the system against possible constraint violations. In addition, the error model of the system is employed to predict possible violations of constraints and avoid them accordingly. It is shown that the methods are efficient in handling system's constraints and uncertainties, while providing global stability.


The summary of the thesis is presented in Chapter 6. The contributions of the works are highlighted in this chapter, and the directions for possible future works are presented.

13

Chapter 2 Pose Estimation
2.1 Introduction
Acquiring the relative position and orientation (pose) of an object with respect to a camera by means of the camera images is known as 3D object pose estimation which plays an important role in many applications including visual servoing. For this purpose, the interest regions of the object (i.e., image features) from camera images are extracted and used along with the 3D model of the object and camera calibration information. Since the projection on the camera image plane includes only 2D data from the 3D scene, the relative pose of the object could be estimated only up to certain accuracy. The accuracy of estimations depends on many parameters such as camera calibration and image noise. Lack of information about the system may result in inferior pose estimations. Traditionally, monocular (i.e., single camera) vision was exploited for this purpose which was prone to camera calibration errors, image noise, and partial occlusion. As a remedy, multicamera systems with sensor fusion techniques were introduced. Since sensor fusion techniques used for pose estimation are built up on monocular pose estimation methods, the structure of these methods need to be investigated first. The sensor fusion technique will be discussed afterwards. This chapter reviews the pose estimation methods that may be used for sensor fusion which will be discussed later in Chapter 3. Three major optimal techniques are introduced for this purpose. First, the Kalman filter-based methods are introduced as the most important group of pose estimation algorithms. The advantages of these methods include temporal noise filtering, recursive implementation, and expedition of dynamic windowing for feature selection, which have made these techniques very popular. Novel iterative and adaptive techniques are introduced to improve the robustness of these methods against the uncertainties of the system. Next, the 14

Gauss-Newton method is introduced as one of the most practiced optimization-based pose estimation techniques which can provide the RVS system with accurate estimations of the pose. This method is closely related to Levenberg-Marquardt and bundle adjustment techniques. In addition to that, VVS is introduced as an accurate method for pose estimation. In this technique, the pose of the object is acquired through virtually servoing the camera to a known pose. Next, the close relation between the VVS and Gauss-Newton method is proven and the superiority of VVS over the Gauss-Newton method in terms of accuracy and speed are discussed in details. The introduced methods will be used in sensor fusion which will be discussed in the coming chapter. This chapter is organized as follows. First a survey over the previous literature on pose estimation is presented in Sec. 2.2. Next, the configuration of the system is discussed in Sec. 2.3. Kalman filter-based pose estimation methods are the focus of Sec. 2.4. The Gauss-Newton and VVS methods are introduced in Sec. 2.5 and 2.6, respectively. The relation between these two is demonstrated in Sec. 2.7. Finally, the chapter is concluded in Sec. 2.8.

2.2 Literature Survey
The problem of 3D object pose estimation through camera images, also known as external camera calibration, is an old though fundamental problem in many vision related fields such as computer vision and photogrammetry [2.1]. Pose estimation was applied to numerous domains, such as virtual reality [2.2], 3D reconstruction [2.3], object recognition [2.4], 3D tracking [2.5], visual servoing [2.6] and simultaneous localization and mapping (SLAM) [2.7]. The pose of an object may be acquired through other sensors such as Inertial Measuring Units (IMUs) and Global Positioning System (GPS) sensors as well. However, the former is only accurate for short periods of time [2.8] and the latter is less accurate and reliable [2.9]. Vision is usually a better option since it can provide rich information, which if processed properly, can lead to accurate and dependable results. Moreover, vision sensor is able to provide the relative pose of the object with respect to the sensor, which is beneficial for many applications such as visual servoing. Vast variety of methods has been introduced for pose estimation through camera images. Linear methods based on perspective n-points (PnP) were early responses to the problem [2.10, 2.11]. A 15

thorough review of these methods can be found in [2.12]. More recently, linear solutions based on polynomial constraints [2.1] and linear equations solved through singular value decomposition [2.13] have been proposed. These methods are fast and do not need initialization; however they usually suffer from image noise. Closed-form solution such as those in [2.14] and [2.15] were introduced for simple but fast estimations. In addition, numerical and iterative solutions were also introduced for simple pose estimation [2.16]; nevertheless, they were also shown to be sensitive to image noise. Nonlinear optimization-based pose estimation algorithms based on Gauss-Newton or Levenberg-Marquardt [2.17] and least-square estimators [2.18, 2.19] have shown to be relatively accurate in presence of image noise. These algorithms iteratively approach the correct pose from an initial guess using the task-function Jacobian. Recently, a new nonlinear scheme based on visual servoing control systems was introduced to solve the same problem [2.20, 2.21]. In this method, a virtual camera is servoed from an initial known pose to the current (i.e., desired) pose using the image features. Later, the method was extended to be robust against outliers [2.22]. The main drawback of nonlinear optimization-based algorithms was their stability. It was shown that these methods were only locally stable and therefore the final estimation was guaranteed to be accurate only if the initial pose guess was sufficiently close to the current one [2.20]. However, this problem is not very concerning for visual servoing systems, since consequent poses of the robot are close to each other due to high sampling rates of the camera(s). Kalman filter-based pose estimation offered another solution to the problem. Since the problem of pose estimation is nonlinear, extended Kalman filter (EKF) should be exploited. Early attempts on this scheme can be found in [2.23] and were later applied to robotic field for grasping [2.24] and visual servoing [2.25]. One big disadvantage of these approaches was their challenging tuning and initialization problems. The effectiveness of these methods dramatically decreased, once the filter was out of tune. Adaptive techniques were introduced as a remedy to filter tuning problem [2.26-2.29]; yet, these methods were prone to system initialization. Alternately, iterative methods were proposed to reduce the error caused by system linearization. It was shown that these methods reduced the sensitivity of the system to tuning and initialization problems [2.30]. Lately, a cooperative iterative and adaptive scheme was introduced which had the capacity to adapt to the changes of the system while following the system's fast dynamics [2.31].

16

This chapter reviews the advancements on pose estimation techniques and propose the iterative adaptive unscented Kalman filter as a powerful tool for robust and accurate pose estimation. The introduced pose estimation techniques are then exploited in the coming chapter.

2.3 System Configuration
A multi-camera robotic cell is considered in this work. Two types of camera configurations are employed for this purpose. Much of the previous work on multi-camera pose estimation assumes the relative pose between the cameras to be known a priori (e.g., [2.35-2.37]). Though this is a good assumption in many practical cases, it may not be viable in all scenarios. For instance, the relative pose of a camera mounted on an Unmanned Aerial Vehicle (UAV) may not be available. Moreover, the calibration between different cameras is a time-consuming procedure (especially for a large number of cameras) and is prone to errors. In this work, the EIH cameras have a direct look at the target object (OT) and the ETH cameras have the target object and the object on the robot end-effector (OR) in their FOV. Figure 2.1 demonstrates this configuration. Relevant coordinate frames are considered for the cameras, objects, and robot end-effector.

17

Figure 2.1 System configuration: The eye-in-hand (mobile) cameras have a direct view of the target object, while the eye-to-hand (fixed) cameras have the auxiliary object in their FOV. The relative pose between  and  coordinate frames is expressed by the transformation matrix,
 H . The transformation matrix is composed of rotation matrix  R ( )

(where

           is the vector of Euler angles with roll, pitch, and yaw elements) and the
  translation vector ( t   x  y   z  ) as follows,
T

T

  R  t     H , 01Ã3 1 

(2.1)

where 01Ã3 is a row vector with 3 zero elements. The relative pose of the target object with respect to the robot end-effector is desirable and is obtained separately for EIH cameras, 18

Cmi e e HO  HCm HO , T i T

(2.2)

and ETH cameras,
e e HO  HO HOR j T R



Cf



1

HOT j .

Cf

(2.3)

Here Cmi , Cf j , OT , OR , and e denote the coordinate frames of ith EIH camera, jth ETH camera, target object, end-effector object and end-effector, respectively. A few of these transformation
e e matrices are obtained through calibration procedures (e.g., HCmi and H OR ), while others are

calculated directly or indirectly through camera images. The coordinates of a point of interest of the object in a camera frame is calculated using the transformation matrix,
C  H CP O P O ,i O i ,

(2.4)

 O   xO where P i  i

yiO

ziO 1  is the vector of homogenous coordinates of point i in the object
T

T

frame, which is known a priori from the Computer Aided Design (CAD) model of the object, and

 C   xC P O ,i  O ,i

C yO ,i

C zO 1 ,i  is the vector of homogenous coordinates of the same point in the

camera frame. The pinhole camera model is considered for the image projection. Point features are exploited
 C is projected onto the camera image since they are simple to extract. A point in camera frame P O ,i

plane, i.e.,

p

C O ,i

 u

C O ,i

C  xO C  v   fi ( PO,i )   C,i z  O ,i C O ,i

C  yO ,i C  , zO ,i 

(2.5)

C  C , u C and vC are its image coordinates. where pO ,i is the image plane projection of point P O ,i O ,i O,i

 C ) is a Without the loss of generality, the focal length in (2.5) is assumed to be unity. Here fi ( P O ,i

nonlinear function that maps the homogeneous coordinates of point i to its image plane 19

coordinates. The image plane projection of each point is obtained from image feature points using camera intrinsic parameters, which are provided by the camera calibration procedure. The image plane coordinates of the feature points are exploited to estimate the object pose afterward. It is worth mentioning that the extracted image features are noisy. In this work, the image noise is assumed to be a zero-mean Gaussian variable. In this chapter, pose estimation methods used for sensor fusion are discussed. The main focus of the chapter is on Kalman filter-based methods, which have several advantages over other pose estimation methods such as recursive implementation, capability to be used for sensor fusion and image feature windowing, and temporal filtering [2.64]. In addition, virtual visual servoing and Gauss-Newton methods are presented as alternatives. The latter methods are optimisation-based methods and are closely related as shown in the sequel.

2.4 Kalman Filter-Based Methods
Kalman filter-based techniques have been commonly used for pose estimation applications. As the pose estimation problem is nonlinear, extensions of Kalman filters for nonlinear systems, namely EKF and UKF are employed. This section briefly introduces the linear Kalman filter structure, followed by pose estimation methods through employing EKF and UKF. These methods are then robustified against uncertainties in estimation via novel iterative and adaptive techniques as follows.

2.4.1 Linear Kalman Filter
Kalman filter in general is a sequential procedure to optimally estimate the state of the system,

 , through noisy measurements,  , in a linear system. For this purpose, a linear model of
system dynamics and measurements is exploited. At each time step, the current state is assumed to be related to the previous state with some uncertainties. That is,

k 1  Akk  qk ,

(2.6)

20

where Ak represents the dynamics of the system and qk is a zero mean Gaussian noise with the covariance of Qk . In addition to that, the state of the system is related to system measurements through the measurement mapping matrix, Fk , written as,

Yk  Fkk  rk ,
where

(2.7)

^k denotes the state rk is a zero mean Gaussian noise with the covariance of k . Here 

estimation. Equations (2.6) and (2.7) form the foundation of the optimal estimation through Kalman filtering, which is outlined as follows. First the initial value of the system state is estimated (usually through other methods, or directly through measurements). Moreover, the error covariance of this initial estimation is approximated,

^0  (0) ,  C0  C (0) ,

(2.8) (2.9)

where Ck denotes the error covariance matrix at time step k. Next, at each prediction step, the state and error covariance of the state are predicted based on the dynamics of the system, described in (2.5).
^ k |k 1  Ak  ^ k 1 , 
C k |k 1  Ak C k 1 Ak T  Qk .

(2.10) (2.11)

^ k |k 1 and Ck|k 1 are the predictions of the state estimation and error covariance matrix, Here,  respectively. Once the measurements are available, the state of the system and the associated error covariance are adjusted as follows, ^k   ^ k |k 1  K k ( k  Fk  ^ k |k 1 ) , 
T Ck  ( I  K k Fk )Ck |k 1 ( I  K k Fk )T  K k  k K k ,

(2.12) (2.13)

21

where,
K k  Ck |k 1 FkT ( Fk Ck |k 1 FkT   k ) 1 ,

(2.14)

and I is the identity matrix. It is worth mentioning that if the measurements are accurate (i.e.,

k  0 ), (2.12) and (2.14) yield,
^ k  Fk 1 k .  (2.15)

On the other hand, noisy measurements cause the magnitude of the Kalman gain to be small and therefore, ^k   ^ k |k 1 .  (2.16)

It can readily be noted that (2.6) and (2.7) model a system with linear dynamics and linear measurement functions. Since the measurement function in the pose estimation problem is nonlinear, the formulation of the filter requires proper modifications, which is discussed in the sequel.

2.4.2 Nonlinear Kalman Filter
The application of Kalman filters can be extended to systems with nonlinear dynamics and/or measurement functions. Two types of filters were introduced for this purpose. In EKF, the state of the system is estimated by linearizing the system around the current state, while sigma points are exploited in UKF to serve this purpose. These methods are explained for pose estimation as follows.
Extended Kalman Filter

Since the camera projection model is nonlinear, the extended Kalman filter (EKF) is considered as a base for pose estimation. The system state vector entails parameters of the relative pose of the target object with respect to the robot's end-effector and its velocity,
e k   tO e O e e T , O t  OT  T

T

T

(2.17)

22

where  k is the state vector at time step k. The system motion and measurement models are as follows,
 k  A k 1  q k ,  k  F ( k )  rk ,

(2.18) (2.19)

where A is the system dynamics matrix,  k and F denote the measurement vector and modeling function, rk is the measurement noise, and qk represents the process noise. Assuming a constant velocity model, the matrix A is defined as follows,

I A 6 06

tI 6  , I6  

(2.20)

where I6 is a 66 identity matrix, 06 is a 66 matrix of zeros , and t is the sampling time of the system. The camera measurement vector is defined as,
C C C C k   uO,1 vO,1 . . uO,n vO,n  ,

(2.21)

where n is the number of selected feature points for pose estimation. The measurement modeling function is described as,
F (k )   f1 (k ) . . f n (k )  ,

(2.22)

where,

 ^C ^ C )   xO,i fi (k )  fi ( P O ,i C z  ^O,i

C  ^O y ,i  C . ^O,i  z

(2.23)

^ C is the estimation of point i coordinates from the object (i.e., estimation of P  C ). The Here P O ,i O ,i

EKF is formulated as follows. First in the prediction phase the system state and its error covariance are predicted based on the dynamics of the system,

23

^ k |k 1  A ^ k 1 , 

(2.24) (2.25)

Ck|k 1  ACk 1 AT  Q ,

^ k |k 1 and  ^ k 1 are a priori and a posteriori state estimation vectors respectively. where  Moreover, C k 1 and Ck |k 1 denote a priori and a posteriori estimation-error covariance matrices, respectively. The process noise matrix is denoted by Q and is defined as follows,

Q  E qk qkT  ,

(2.26)

 is the expectation function. Next, the state vector and error covariance are updated, where E 

based on the new measurements, in the estimation phase.

Fk 

f n ( )  F ( )  f ( )  1 . . ,        ^k|k 1   ^k|k 1
K k  Ck |k 1 Fk T  k  Fk Ck |k 1 FkT  ,
1

(2.27)

(2.28)
(2.29) (2.30)

^k   ^ k |k 1  K k  k  F ( ^ k |k 1 )  , 

Ck   I12  K k Fk  Ck |k 1  I12  K k Fk   K k  k K kT .
T

Here I12 is a 1212 identity matrix, Fk is the Jacobian matrix, K k is the Kalman gain, and  k is the measurement noise covariance. It should be noted that the differentiability of the image mapping function ( f ) certifies the differentiability of the measurement modeling function (F). Each element of the Jacobian matrix is calculated as follows,

 C ) P C fi ( ) fi ( P O ,i O ,i  . C   PO,i 
The first term of the right hand side of (2.31) is derived as follows,

(2.31)

24

 1 C  C )  zO f i ( P ,i  O ,i  C  PO ,i  0  

0 1 C zO ,i

C  xO ,i

z

C O ,i

C  yO ,i C zO ,i

 0  , 0  

(2.32)

whereas the calculation of the second term of (2.31) is followed separately for EIH and ETH cameras. Recalling from the previous section, the 3D Cartesian parameters of the jth point on target object in the ith EIH camera frame is calculated as follows,

 Cmi  H Cmi H e P  OT P OT , j e OT j .
Therefore, in case of EIH cameras,

(2.33)

 j P OT ,i 

Cm

H

Cmi e

e  OT H O P T i



,

(2.34)

and for ETH cameras,

 Cf j  H Cf j H e P OR , j OT OT





1

e  OR HO Pj , R

(2.35)

 j P OR ,i
Cf



H

Cfi OT

e 1 e  OR  HO HOR Pi T





.

(2.36)

e  OR parameters are known a priori and the Derivation of (2.36) is straightforward since H OR and P i e matrix HOT is a function of  .

Unscented Kalman Filter

The adaptation to nonlinearities of the system may also be performed through the unscented transformations [2.48]. UKF estimates the system's state and its error covariance by means of data sampling. First the sampled data, known as sigma points, are selected,
i  nCk 1  , ^ k|k ^  1  k 1 



i

(2.37)

25

where n  is the size of the state,



nCk 1


T

i ^ k|k nCk 1  nCk 1 , and  1 are the sigma points.



Next, the sigma points are passed through the system's dynamics to form the state and its error covariance predictions,
^ k |k 1   1 2 n  i ^ k |k 1 ,  2n i 1

(2.38)

Ck |k 1 

1 2 n i T ^ k|k ^ ^ i ^ {(  1  k |k 1 )(k |k 1  k |k 1 ) }  Qk .  2n i 1

(2.39)

Similar procedure is taken to calculate the state and it error covariance estimations,

^ ki   ^ k|k 1   nCk |k 1  , 



i

(2.40)

^ i  f  ^ ki , Y k i
2 n ^  1 Y ^ i , Y k k 2n i 1

 

(2.41)

(2.42)

C y ,k 

1 2 n ^  i ^ ^  i ^ T  (Yk  Yk )(Yk  Yk )   k , 2n i 1 1 2 n  i ^ i  Y ^ )T , ^k   ^ k |k 1 )(Y (  k k 2n i 1

(2.43)

C , y ,k 

(2.44)

which result in,
K k  Cx , y ,k C y ,k 1 ,

(2.45) (2.46) (2.47)

^k   ^ k |k 1  K k ( k  F ( ^ k |k 1 )) , 
T Ck  Ck |k 1  K k C y , k K k .

26

It was shown that UKF results in more accurate estimation for nonlinear systems when compared to EKF [2.69]. Yet, this algorithm also approximates the nonlinear system and it provides suboptimal estimates of the real system. It is worth mentioning that Kalman filters were designed under system's zero-mean Gaussian noise assumption. If the noise of the system is nonGuassian, Kalman filter remains the optimal linear estimator. However, particle filters (PFs) may work better in this case. Yet, these filters impose high computational burdens on the system and may not be desirable, especially for real-time applications. For such cases, UKF offers a balance between low computational costs of Kalman filters and high performance of particle filters. In this chapter, the accuracy of UKF is enhanced even further through the proposed IAUKF, which is explained in the sequel. The optimality (sub-optimality) of Kalman filters (EKF and UKF) is directly related to proper estimations of noise covariance matrices,  k and Qk , tuning of which is non-trivial. Moreover, these parameters usually vary during the estimation procedure, making the initial adjustments less effective. Adaptive schemes were proposed as a remedy to these problems, which is explained in the sequel.

2.4.3 Adaptive Schemes
Different schemes were presented to deal with the lack of knowledge about system noise statistics. These methods were mainly categorized into multiple model systems and innovationbased adaptive techniques [2.70]. The former was based on operation of multiple Kalman filters with different parameters in parallel. The closest filter output to true value was chosen as the state estimation. Since these techniques usually need a huge bank of filters and yet the optimality of the system is not guaranteed, they are not discussed in this chapter. Alternatively different adaptive schemes are introduced as follows.
Correlation-based Algorithms

In these methods, the unknown noise parameters are computed based on autocorrelation of the system measurements,

Dk  E iikT ,
27





(2.48)

or autocorrelation of the system innovation term,
T Ek  E  , i i k





(2.49)

where i   i  F (i ) is the innovation term of the system. The aforementioned autocorrelations are estimated over a number of sample set,

1 N ^ Dk   ii  k T , N i  k 1
N ^  1  vv T , E k i i k N i k 1

(2.50)

(2.51)

where N is the number of samples. The noise matrices are then calculated based on the relationship of the autocorrelation and noise matrices,
^  FCF T , ^ D  0

(2.52) (2.53)

^  C  ACAT , Q

where C is the steady-state error covariance and is computed based on the estimates in (2.50) or (2.51). As it can be inferred, the relationship is usually based on the steady-state Kalman filter, which requires the system to be linear and stationary [2.71]. Also these methods usually perform less efficiently compared to maximum likelihood schemes [2.71], which are explained next.

Maximum Likelihood Algorithms

In these algorithms, the unknown parameters of the system are found by maximizing their likelihood,

  arg max L(  ) ,


(2.54)

28

where  is the vector of unknown noise parameters and L(  )  log p   i ,.., 1  is its likelihood function. The maximization procedure often involves iterative schemes, increasing the computational expense of the system. This problem can be solved by introducing the covariance matching algorithms.
Covariance Matching Algorithms

These methods compute the noise matrices based on system residuals [2.72]. The differences between state prediction and estimation, and also actual measurements and predicted measurements, are recorded as an estimation of process and measurement noise respectively, ^k  k  k |k 1 , q
^k   k  F ( k ) . r

(2.55) (2.56)

On the other hand, it can be shown that,
T ^k , q ^k )  Qk  Ck  Ak Ck 1 Ak , cov(q

(2.57) (2.58)

^k , r ^k )   k  Fk Ck FkT . cov(r

Calculating the noise residues from (2.55), (2.56) and employing them in (2.57), (2.58) yields,

^   N

1 N 1 T ^i  r  r ^i  r   r  N  1 i 1 N

 FC F
i 1 i i

N

T

i

,

(2.59)

^  Q N
where,

1 N 1 N T ^ ^ q  q q  q  ACi 1 AT  Ci ,      i i N  1 i 1 N i 1

(2.60)

r

1 N

 r^ ,
i 1 i

N

(2.61)

29

1 N ^i . q  q N i 1

(2.62)

^ are the estimates of measurement and process noise covariance, respectively. ^ and Q Here  N N

One main advantage of this form of estimation is its computation that can be performed sequentially as follows,

^N  r  r ^N  r  r FN CN FN T ^  ^    , N N 1 N 1 N ^ N  q  q ^N  q  q ACN 1 AT  CN ^ Q ^ Q   . N N 1 N 1 N
T

T

(2.63)

(2.64)

This form of adaptation reduces the computational cost by almost N times (for this part) as iterating for N loops, shown in (2.59) and (2.60), is no longer required. It is worth mentioning that since the linearized modeling matrices (i.e., Ak and Fk ) are not calculated explicitly in UKF, (2.57) and (2.58) cannot be employed directly to estimate the noise parameters. It can be shown that [2.69],
C k |k 1  QK  Ak C k 1 AK T ,

(2.65) (2.66)

Cy,k  Fk Ck FkT ,

which are used in conjunction with (2.57) and (2.58) to approximate the noise covariance matrices as follows,

^   N

1 N 1 N T ^ ^ r  r r  r      i  C y ,i , i N  1 i 1 N i 1

(2.67)

^  Q N

1 N 1 N T ^ C . ^ ^ Ci|i 1  Q   qi  q  qi  q   N  i i N  1 i 1 i 1

(2.68)

In adaptive methods, the covariance of residues is approximated over a window of past measurements. The size of this window (N) is usually chosen empirically [2.71]. Large window 30

size offers a more accurate approximation at the cost of reduced flexibility to system dynamicity, while small window size may lead to system instability. It should be mentioned that despite their good performance, the adaptive methods are sensitive to initialization and camera calibration. Moreover, these schemes could not adapt to very fast changes in the system as the linearization made by EKF or UKF become less valid. To make the system more robust against fast dynamics of the system and its parameter errors, iterative schemes are introduced.

2.4.4 Iterative Schemes
The linearization method, used in EKF and UKF, is based on local system approximation. As a result, the estimations may deviate from the original states if changes in the system state are considerable. To alleviate this limitation, iterative schemes are applied through which, the state

^k ) are recalculated iteratively, using the states from the ^ k|k 1 and  prediction and estimation (i.e., 
last iteration in linearization. With enough number of iterations, the output of the system becomes steady, i.e.,
i i 1 , k  k

(2.69)

i is the state vector at time step k after i iterations. Using (2.69) in (2.29) and (2.46) where k

yields,
i F k  k .

 

(2.70)

Similar results are achieved by increasing the process noise (Q) or reducing the measurement noise (  ) in linear KFs, but not EKFs (nor UKFs). The benefit of iterative schemes is two-fold. First, these methods approximate the nonlinear functions closely, depending on the number of iterations. This property allows the system to follow highly dynamic systems. More importantly, these schemes alleviate the sensitivity of the system on the error covariance matrices (as was reported in [2.58]). Therefore, the system follows the measurements closely and does not diverge, in case of high error covariance mismatches. Higher dependency of system estimations on the measurements is rewarding when an accurate model of system's dynamics is not available 31

(e.g., sensor data estimation). However, this benefit comes at the price of higher sensitivity to measurement noise. Moreover, iterations usually lead to high computational costs, which is not desirable for real-time systems. In such systems, the number of iterations should be selected as a compromise between the accuracy and time allowance. Equation (2.70) may be interpreted as a nonlinear optimization solution to find the pose of the system. As an example, Gauss-Newton and VVS methods may result in similar outcomes as are explained in the sequel.

2.4.5 Iterative and Adaptive Schemes
Previously, adaptive and iterative techniques were discussed separately. Despite their advantages, each of these approaches has their shortcomings. As was mentioned before, adaptive methods are vulnerable to system uncertainties such as initialization, while the iterative methods suffer from the lack of information about the system noise. This section reviews the previously introduced IAEKF and proposes IAUKF as a novel method under the same methodology.
Iterative Adaptive Extended Kalman Filter (IAEKF)

Iterative and adaptive schemes are employed in synergy for robust and accurate pose estimation [2.64]. The adaptive algorithm adjusts the noise parameters in the filter, while the iterative scheme reduces the estimation error caused by any maladaptation. Though various adaptive schemes might be used for filter tuning, this chapter focuses on the covariance matching method described before. It is noteworthy that the combination of these two techniques has to be conveyed carefully, not to make the system slow or unstable. For this reason, the adaptation of noise covariance matrices is performed out of the iteration loop. For a faster adaptation, only measurements of N previous samples are considered. Moreover, a fading-memory approach is taken to give higher weights ( wk ) to more recent readings and lower weights (1  wk ) to initial samples. Finally, the covariance matrices are checked for positive definiteness. The number of iterations is an open parameter in the system, which augments the system with additional flexibility. This parameter may be regulated by the dynamicity of the system or the 32

convergence of the states. Alternately, the number of iterations may also be selected subject to system's computational resources since extra iterations are performed at the cost of increased computational time. Yet, thanks to recent advancements in computing systems, such problem is deemed as insignificant. The pseudocode of IAEKF with fixed number of iterations is summarized as follows: ______________________________________________ Pseudocode 1: IAEKF Pseudocode Initialize 0 and C0 . FOR k =1:M

k |k 1  Ak k 1
C k |k 1  Ak C k 1 Ak T  Qk
1 k  k |k 1

FOR j =1: 
Fk j  F    j
k

K kj  C k |k 1 Fk jT ( Fk j C k |k 1 Fk jT   k ) 1

kj 1  kj  K kj ( k  F (kj ))
END FOR
C k  ( I  K k Fk )C k , k 1 ( I  K k Fk )T  K k  k K kT
 1 qk  k1  k |k 1

33

1 N qk   ql N l  k  N 1
rk   k  F (k1 )

rk 

1 N

l  k  N 1



N

rl
wk

Qk 1  Qk  

(q N 1

wk

k

 qk )(qk  qk )T  ( qk  N  qk )( qk  N  qk )T  ( qk  qk  N )( qk  qk  N )T

N 1  T   T Ck  Ak Ck 1 Ak  Ak  N Ck  N 1 Ak  N  Ck  N N



 k 1   k  
END FOR

(r  r )(r  r ) N 1
k k k k

T

 (rk  N  rk )(rk  N  rk )T  (rk  rk  N )(rk  rk  N )T

N 1  Fk Ck Fk T  Fk N Ck  N Fk N T N





______________________________________________
Iterative Adaptive Unscented Kalman Filter (IAUKF)

Iterative UKFs were previously proposed to improve nonlinear estimation; however, IAUKF is proposed for the first time in this work. As in IAEKF, the state prediction and estimation are done through an iterative scheme, while the noise parameters are tuned based on most recent measurements. The iterations minimize the effect of system nonlinearities on the filter, while the noise adaptation adjusts the parameters of the system exploiting a fading memory scheme. The cooperative iterative and adaptive schemes work similar to IAEKF, benefiting the system with extra accuracy and robustness to system uncertainties. The pseudocode of IAUKF with fixed number of iterations is summarized below. ______________________________________________ Pseudocode 2: IAUKF Pseudocode 34

Initialize 0 and C0 . FOR k =1:M

k|k 1 

1 n   Ak (k 1    nCk 1  l ) 2n l 1

1 n Ck |k 1  nCk 1  nCk 1  ( Ak (k |k 1   )  k |k 1 ) ( Ak (k |k 1   )  k |k 1 )T  Q      l l 2n l 1
1 k  k |k 1





FOR j =1: 

ykj 

1 n  F (kj     nCk ,k 1  l ) 2n l 1 1 n nCk |k 1  nCk |k 1  ( F (kj   )  ykj ) ( F (kj   )  ykj )T   k      l l 2n l 1

C yj,k 







j x, y ,k

1 n nCk 1  nCk |k 1  ( Ak (k |k 1   )  k |k 1 ) ( F (kj   )  ykj )T       l l 2n l 1





Kkj  Cxj, y ,k Cyj,k 1

kj 1  kj  K kj (k  F (kj ))
END FOR
 T Ck  Ck ,k 1  KkCy ,k Kk

qk  k1  k |k 1

qk 

1 N  ql N l  k  N 1
35

rk   k  F (k1 )

rk 

1 N

l  k  N 1



N

rl
wk

Qk 1  Qk  

(q N 1
wk

k

 qk )(qk  qk )T  ( qk  N  qk )( qk  N  qk )T  ( qk  qk  N )( qk  qk  N )T

N 1  Ck  Ck ,k 1  Qk  Ck  N ,k  N 1  Qk  N  Ck  N  N



 k 1   k  

(r  r )(r  r ) N 1
k k k k

T

 (rk  N  rk )(rk  N  rk )T  (rk  rk  N )(rk  rk  N )T

N 1  C y ,k  C y,k  N T N





END FOR ______________________________________________

2.5 Gauss-Newton Method
Another approach to solve the pose estimation problem is to find a solution that totally relies on the system measurements. The goal is then to find the state vector  k such that (2.70) holds. In this method, pose estimation problem is treated as a non-linear least-square minimization problem. All features from the cameras are gathered in a single vector such as s,
c c c c s u c v1 u2 u2 . . uc j uj  .  1

(2.71)

In addition, a set of features is calculated based on pose estimation,
x ^c ^ 1 s ^c z   1
c ^1 y c ^1 z c ^2 x c ^2 z c ^c x ^2 y j c . . c ^2 ^j z z

 ^c y j , ^c z j  

(2.72)

^c  x  ^c y ^c ^c where P j j z j  is calculated as follows,  j

36

ci o ^ci ^c  R ^o P Pj  to . j

(2.73)

Here, the estimated values are denoted by a "hat". The pose parameters used in (2.73) are calculated based on estimated pose of the target object with respect to the end-effector and known transformation which were mentioned in (2.2) and (2.3). The cost function is defined as the difference between current image features and estimated image features,

eg  s ,
where,

2

(2.74)

^s . s  s
One way to minimize this cost function is to employ a simple proportional control law,
  s . s

(2.75)

(2.76)

where  is the control gain. This control law leads to an exponential decrease in the cost function. The changes in the cost function are related to the changes of the pose,
e  J  ^ s ^ s  o ,
T

(2.77)

^ s e e  to oeT  , J  and is calculated as follows for a set of feature coordinates where o e T  T  ^o 
T

i  (i.e.,  u ci vc j  ),  j

J

1  c ^j z    0  

0 1 ^c z j

 
^c z j

 

   e ^o ). D( T c  ^j  y 2 ^c z  j 
2

 ^c x j

(2.78)

e is a pose dependent matrix which is computed differently for eye-in-hand cameras, Here, D ( o )
T

37

e e T e o e  ^ e T R ^cm ^o ^o ^o , ) D( S R Pj T    Rcmi i  
T T T T





(2.79)

and eye-to-hand cameras,
e ^e T ^o ) D (   Rcfi 
T

e e  ^e T S R ^o ^oe  t ^oe T  ^o R Pjo  t  . cfi
R R R T T







(2.80)

Using (2.76) and (2.77), the time update of pose is calculated,
e  ^o    J   s .
T

(2.81)

This time update is used to estimate the pose through integration from an initial guess,
e e ^o ^e ^  , k 1  o , k  o ,
T T T

(2.82)

which leads to the same estimation of the pose as calculate by Lowe in [2.17]. The inverse of the J matrix may be calculated through Moore-Penrose pseudo-inverse (which leads to the GaussNewton solution) or damped least-square inverse (which is equivalent to Levenberg-Marquardt solution). Generally, the latter is preferred since it was proven to have stronger stability; however, assuming sufficiently closed initial guess and avoidance of singularity of the image Jacobian matrix (due to high number of features from multiple cameras), the Gauss-Newton provides the system with high accuracy estimation of the pose. It is also worth mentioning that the measurement error s and also the update equation (2.82) are very similar to innovation term and update equation in EKF-based pose estimation methods. In fact, it can be shown that if the measurements of the system are fully trusted (i.e. the measurement error covariance is equal to an all zero matrix), the Iterative EKF (IEKF) approach is equivalent to Gauss-Newton solution to the problem. However, the Gauss-Newton approach has the advantage of lower computation time compared to IEKF, while preserving the same or better accuracy. The major drawback of this method is sensitivity to measurement noise, which is not accounted for.

38

2.6 Virtual Visual Servoing
Image-based visual servoing was employed in previous works to obtain the object relative pose [2.20]. A virtual camera was servoed through an initial known pose to a pose that minimizes the image space error, s . The reached pose was considered as an estimation of the current pose. The same idea is proposed in this work for fast and accurate fusion of the cameras. Given an initial pose for the virtual manipulator, the measurements of each camera from object of interest are simulated using,

^c p j 

x ^c j

^ z   j

c

 ^c y j  c , ^j  z 

(2.83)

^ jc   x  ^c y ^c ^c where virtual point P z j j  is calculated through known transformations between  j

objects and cameras,
cmi o ^ jcmi  R ^o P Pj  t^ocmi ,
T T T

(2.84) (2.85)

cf i o ^ jcfi  R ^o P Pj  t^ocfi .
R R R

e ) is It worth mentioning that, since the relative pose between object and the end-effector ( Ho
T

assumed to be known during virtual servoing, an estimation of all pose parameters used in (2.2) and (2.3) is virtually available at each time step. The velocity of the virtual camera is related to the changes in its image features,
i  J s ,iVci , s
t c  i  is  ci   

(2.86)

where Vci  

the velocity of ith camera in its own frame, s i is the time derivative of the
T

J T . . J s,i,nT  feature vector of ith camera ( si ), and J s,i   , where n is the number of  s,i ,1 
features detected in the ith camera and,

39

J s ,i . j

 1  c z ^ ji     0  

0

i ^c u j i ^c z j

i ^ ci ^c u j v j

i ^c 1  u j

 

2

i ^c 1 v j i ^c j ci 1  v i ^c ^ z z j j

 

2

i ^ci ^c u j v j

 .  i ^c u j   

i  ^c v j



(2.87)

Here tc and  c are the translational and angular velocity of the ith camera in its own frame,
i
i

respectively. Now the virtual end-effector should be moved, exploiting the calculated velocities of all cameras. The velocity of each camera is transferable to the equivalent end-effector velocity, i.e.,
i Vci  c e Vei ,

(2.88)

where Vei  

t e  i  is  ei   

the velocity of the virtual end-effector in its own frame coordinates, based on

the velocity of the ith camera. It is straight-forward to realize this velocity transformation for the eye-in-hand virtual cameras, since they are assumed to be fixed to the robot,
cmi R ^e     0 cmi  ^e ^ecmi R S t 

i cm e

 

^ecmi R

,  

(2.89)

where S t^ecmi is the skew-symmetric matrix of the vector t^ecmi , defined as follows,
 t x    0    S t y     t z   t    t  z   y t z 0 tx ty   t x  . 0  

 

(2.90)

On the other hand, the equivalent velocity of the end-effector should be computed in case of eyeto-hand virtual cameras. Similar velocity transformation matrix has been derived, with a sign difference,
 i cf e  
cfi cfi  ^e ^e R S t^ecfi R  . cfi ^    Re  0 

 

(2.91)

40

The details of the calculations are forwarded to the Appendix. The velocity of the virtual endeffector is calculated based on IBVS control law as follows,
i Vei    J s,i c e  si .



(2.92)

Once the velocity of the virtual end-effector is achieved, the pose parameters updates are calculated as follows,  e  S ( ) R e , R o e o
e e e  S ( e )to , to t

(2.93) (2.94)

and the pose parameters are updated through integration. In order to speed up the algorithm even further, the use of Jacobian matrix calculated at the desired pose is proposed for calculation of (2.87). For this matter, the desired feature points are available (i.e., currant feature measurements) and the initial depth is acceptable as estimation of depth at desired location since the initial guess is sufficiently close to the desired pose. In the next step, the velocity of different cameras should be fused into a single velocity, guiding the virtual robot towards the current pose accurately. Two different approaches are discussed for fusion of different virtual camera velocities, which are explained in the sequel.

2.7 Comparison of Optimization-Based Algorithms
In this part, it will be shown that the pose estimation through VVS leads to the same solution as Gauss-Newton. The velocity of the virtual end-effector is connected to the time derivation of the pose,
e  ^o , Ve  G
T

(2.95)

where,

41

0   I  I S (t ^oeT )   I     G   ^oe )    0  T 0 (    I 0   T   

^oe )  ^oe )T (  S (t . ^oe )  T (
T T T

(2.96)

 

^oe ) is the transformation matrix that relates the time derivation of the Euler angles to the Here T (
T

angular velocity,
e e e o  T (o )o .
T T T

(2.97)

It is worth mentioning that while (2.95) relates the velocity of the virtual end-effector, Ve , to the
e  ^o , this conversion does not happen in VVS algorithm. Instead, time derivation of the pose, 

(2.93, 2.94) are exploited to calculate the pose update parameters. By comparing (2.95) with (2.93, 2.94), one can notice that the position is updated similarly, while the orientation update is different. Using (2.86), (2.88) and (2.89) the relation between image features time derivation and pose changes is calculated as follows,
e  ^o ,   M s
T

(2.98)

where,
i M  J s ,i  c eG.

(2.99)

Image Jacobian Js,i, used in (2.87), can be rewritten as,
 1  c z ^ ji     0  
i  ^c x j

0 1 i ^c z j

J s ,i , j

 
i ^c z j

 z^ 
ci j

i ^c y j

    I  2  
2

^ jci  . S P  

 

(2.100)

Using (2.89), (2.95), and (2.99) it can be shown that for eye-in-hand cameras,

42

 1  c z ^ ji  M    0  

0 1 i ^c z j

   

    ^ ci Re ci   ^j  y 2 i  ^c z j 
2 i zc j

i  ^c x j

^oe )  . ^eci S ( R ^ce Pjci  t^ce  t^oe )T ( R i i 
T T



(2.101)

The skew-symmetric matrix in (2.101) can be calculated as,
e ^ci ^ce Pjci  t ^ e ^ ci o1 ^ci  t ^c ^ce  t^oe1  S  ^ce  R ^ce  S R t t  Rci Ro1 Pj  t o1 i i i i o1 i  e ^ ci o1 e ^ci ^c ^ce t^ci  R ^c ^ e Po1 . S R R P R t S R o1 j i o1 j i o1 i o1











 









(2.102)

Replacing (2.102) in (2.101) will give the same result as was calculated by (2.78) and (2.79). In case of eye-to-hand cameras,
 1  c z ^ ji  M    0  
i  ^c x j

0 1 i ^c z j

   

    ^ ci  Re ci   ^j  y 2 i  ^c z j 
2 i ^c z j

^oe )  . ^eci S ( R ^ce Pjci  t^ce  t^oe )T ( R i i 
T T



(2.103)

The skew-symmetric matrix in (2.103) is rewritten as,
e ci ^c ^ e ^ ci o ^oci  t^ce  t^oe  ^oe )  S  S (R P  t^cei  t  Rci Ro Pj  t  i j i
R T

^ e ^ ci o ^ e ^oci  S  Rci Ro Pj  Rci t
R



R





R

 ^  t^   t
R

R



T

e ci

e oT

 S 




R T

e o ^o R Pj  t^oe  t^oe ,
R R



(2.104)

which makes the matrix M similar to its counterpart in Gauss-Newton method, described in (2.78) and (2.80). It is very important to know that these two approaches are basically equivalent; however, their minor differences separate them from each other in terms of accuracy and speed. The first major difference between the two algorithms comes from the variation in the formation of the Jacobian matrix in these two methods. The Jacobian matrix in VVS is composed of two parts, i.e., image
i Jacobian Li which is dependent of virtual features, and the velocity transformation matrix  c e

43

which is depending only on virtual pose parameters. Therefore, the Jacobian matrix is calculated quickly via a single multiplication of the image Jacobian of all features by the velocity transformation matrix. On the other hand, the Jacobian matrix used in the Gauss-Newton method is composed of two matrices that are both dependent on the feature, as was shown in (2.78-2.80). As a result, the Jacobian is formed by concatenating the multiplications shown in (2.78) for each set of features. In programs that are optimized for matrix operations (such as MATLABÂ®), the former implementation is performed faster, despite their similarity in the hypothetical computational costs. The second major deviation occurs in the orientation updates of these methods. The VVS algorithm exploits the virtual end-effector velocity, Ve , to update the rotation matrix as was described in (2.93). On the other hand, the Gauss-Newton algorithm calculates the Euler angles at each iteration using (2.95), and then revert them back to update the rotation matrix, used in (2.79-2.80). The transformation of end-effector angular velocity to Euler angles and then back to rotation matrix causes an additional operation which may increase the system uncertainty. In order to follow this matter more closely, the error propagation from the end-effector angular velocity to the rotation matrix is examined for each of these two methods. The methodology of error propagation presented in [2.73] is exploited for this purpose. By employing (2.95), the
 e ) in the VVS method is found as, noise of the rotation matrix update ( R oT
e , R  S ( ) R  o
e oT e T

(2.105)

e where  R  e and  e are the noises of Ro and  e , respectively. After the integration, the resultant
oT

noise of the rotation matrix in the VVS fusion method is calculated as,

 R  S ( ) Roe t .
e oT e T

(2.106)

In case of the Gauss-Newton method, the noise of the end-effector angular velocity is initially transferred to the Euler angles and is calculated using (2.97), i.e.,
e 1 n e  T (o ) n e t . T
oT

(2.107)

44

The noise of the rotation matrix is found as,
 R R R R   R  R z y x  z y Rx    Rz R y Rx ,
e oT e oT e oT e oT

(2.108)

e where R z , R y , R x are the rotation matrix components related to Euler angles (i.e., Ro  Rz Ry Rx ), T

and

 ,  , 
e oT e oT

e oT

e e e are the noise of the Euler angles  o , o , and  o , respectively. T T T

Reformulating (2.108) results in,

R  S   
e oT



Rz 

e Rz R y      e RoT ,
oT



(2.109)

where,
e e  o    0  o 0 0   ,   
T

T

e o 0  ,     T

0 0 .

(2.110)

It can be shown that,
e o  T e  Rz Ry     T (oT )  0   0

  

Rz 

0 e 0

oT

0   0 . e  o  T 

(2.111)

Using (2.107-2.111), the rotation matrix noise may be formulated as,
e  o T e   S (T (oT )  0   0

R

e oT

0  e 0

oT

0   e 1 e 0  T (o )  e ) Ro t . T T e   oT  

(2.112)

Comparing (2.112) with (2.106), it is obvious that noise of the rotation matrix in Gauss-Newton method is a function of changes in orientation, which is a direct function of the convergence speed of the algorithm. High rates of change in orientation may lead to high noise projection in the estimated pose. It is worth mentioning that the noise in the orientation affects the translation as well, through matrix G, described in (2.96). Therefore, it may be concluded that the VVS method is more robust to noise, compared to the Gauss-Newton method. 45

2.8 Summary
Accurate pose estimation of the object is a crucial part of many RVS systems. Various techniques were introduced previously to address this concern. However, only a few of these methods are suitable for sensor fusion. In addition, the accuracy and robustness of these methods vary from one to another. This chapter was devoted to robust and accurate pose estimation techniques which had application in sensor fusion. For this purpose, Kalman filter-based methods were introduced as the most important pose estimation technique with several advantages. The shortcomings of these methods were alleviated through the introduction of iterative and adaptive techniques. In particular, IAEKF and IAUKF were proposed for robust and accurate pose estimation. In addition, well-known Gauss-Newton method for pose estimation was developed. Moreover, VVS for pose estimation was reformulated as an accurate optimization-based technique. It was shown that this method outperforms Gauss-Newton approach in terms of accuracy and speed. The developed pose estimation techniques are to be used for sensor fusion in the coming chapter. The performance assessment of these techniques is therefore referred to the next chapter.

46

Chapter 3 Sensor Fusion
3.1 Introduction 
As it was mentioned ealier, robust and accurate pose estimation is required by many RVS systems. The performance of these RVS systems is dependent on the accuracy of the estimated pose. Usually, single camera systems are exploited for this purpose; however, the performance of monocular vision is limited. Multi-camera systems are proposed as a remedy to this problem. Then, sensor fusion techniques are employed to combine multiple sensor data synergistically, enhancing the richness of the outcome data. Therefore, the goal is to use the data from multiple cameras to reach a highly accurate and robust estimation of the object pose. Previously EKFbased central fusion and bundle adjustment were introduced as effective methods for sensor fusion to obtain accurate pose estimations. However, these methods were prone to system parameter uncertainty and measurement noise. Moreover, the data from cameras were fused without discrimination, which could lead to inferior results. This chapter proposes three fusion structures for pose estimation, namely centralized, decentralized, and pre-processing fusions. Centralized fusion offers high accuracy at the price of increased computation time. Two novel centralized fusion techniques, namely Iterative Adaptive Extended Kalman Filter (IAEKF) and VVS are introduced for this matter and their performance was shown to be superior compared to their previously introduced competitors. On the other hand, decentralized fusion provides the system with a faster estimation by sacrificing the accuracy partially. This work proposes two decentralized fusion methods based on extended Kalman filter (EKF) and VVS for pose estimation. Finally, a pre-processing fusion technique is discussed which enables the system to fuse the image information from multiple cameras prior to their processing. The main advantage of this method is its independency from the pose estimation technique involved, which makes it suitable for any available pose estimation method.

47

3.2 Literature Survey
Multi-camera configurations were previously proposed to overcome the deficiencies of a single camera. These systems were capable of enhancing the overall accuracy and robustness of the estimation and also increasing the total FOV. Multiple cameras could benefit different situations. In most scenarios, all of the cameras had the target object in their field-of-view. The transformations between the individual cameras were known through pre-calibration of the system (for fixed cameras) or forward kinematics of calibrated robots (for end-mounted cameras). The visibility of the object (in at least one of the cameras) was crucial for such schemes. In addition, the common presence of errors in object modeling (e.g., object CAD modeling errors) impaired the overall estimation significantly. Only few works have addressed this problem so far. For instance, multiple eye-in-hand cameras with different FOVs were employed for visual servoing in [3.1]. In addition, a recent work has benefited from using several eye-in-hand/eye-to-hand cameras to control the robot more accurately [3.2]. Moreover, multiple fixed cameras were used in [3.3] for accurate visual servoing. A combination of eye-in-hand and eye-to-hand cameras was exploited to overcome occlusion using epipolar geometry-based visual servoing [3.4]. A multi camera system was utilized to servo a robot and a stereo rig simultaneously [3.5]. Several eye-in-hand cameras were used for accurate visual servoing in [3.1]. Also control level fusion of multiple eye-in-hand/eye to-hand cameras was implemented for robust servoing [3.6]. In addition, multi-camera configurations were previously employed for 3D reconstruction [3.7] and object recognition [3.8]. However, none of these works focused on the pose estimation problem. Multiple camera pose estimation schemes were traditionally based on stereo vision technique [3.9], thus facing several issues such as system calibration and camera correspondence problems. In order to fully utilize the data from each camera and improve the estimation robustness, data fusion techniques were introduced. A full realisation of potential capacity given by multiple sensors was achieved by techniques known as sensor fusion methods. These methods were generally categorized into centralized and decentralized fusion algorithms. Centralized fusion methods, also known as measurement fusion, took all measurement data into a central unit and estimated the pose based on all available measurements. A well-known example of this method (i.e., bundle adjustment) was proposed in [3.10], where a weighted least square approach was taken to acquire the pose estimation. A 48

similar method based on Extended Kalman Filter (EKF) was also introduced in [3.11, 3.12] for the same purpose. The advantage of centralized fusion methods was the minimal loss of information, whereas their computational cost grew drastically as the number of sensors increases. Moreover, they were prone to faulty measurements or outliers since they combine all data from every sensor without discrimination. Decentralized fusion methods, on the other hand, relied on local estimators that each processed the data from a single sensor. The outcome of all estimators were then fused together to form the optimal/suboptimal estimation. These methods were generally less accurate than centralized methods, yet the fused information was more accurate than any local estimation [3.13]. In return, they were computationally lighter and could survive false measurements by isolating the corresponding estimator [3.14]. Aside from the aforementioned fusion structures, the fusion techniques also played a major role in the success of the overall fusion. Various strategies were employed for sensor fusion, among which Kalman filtering remained one of the most popular methods [3.14-3.17]. Kalman filter was a well-known sequential estimation method which could provide optimal results under certain conditions. The main advantages of Kalman filtering were its optimality and simplicity, which made it appealing for real-time applications. Due to the nonlinearities of many measurement systems, extended Kalman filter (EKF) was developed and exploited for fusion in many applications such as odometry [3.18], tracking [3.19], navigation [3.20, 3.21] and pose estimation [3.22]. However, EKF performance was degraded drastically when the system noise conditions were unknown. Moreover, EKF was sensitive to high dynamics of the system, since it assumed a linearized model of the system which was valid only locally. Several remedies were proposed to address the shortcomings of EKF-based estimators. Unscented Kalman filter (UKF) was introduced to increase the level of accuracy in linearization [3.23] and was exploited for sensor fusion in many applications such as navigation [3.24] and cooperative driving [3.25]. As was discussed by [3.26] and [3.27], EKF had the advantage of lower computational complexity, whereas UKF was more accurate and did not require an explicit Jacobian calculation. Yet, UKF could not fully address the aforementioned limitations. Adaptive techniques were introduced to tune the filter noise parameters and were applied both to EKF 49

[3.28-3.31] and UKF [3.32, 3.33]. The adaptations were mainly performed based on previous measurements of the system, which led to low convergence rates. Besides, these methods were unable to cope with the initialization errors. Iterative schemes were proposed to compensate the linearization error in EKF [3.34-3.36] and UKF [3.37, 3.38] estimators. However, these methods placed overconfidence on the measurements. In addition, an adaptive iterated Kalman filter was briefly introduced in [3.39]; however, the method did not find wide applications. Recently, a novel technique based on iterative and adaptive schemes was adopted for pose estimation [3.40]. The method was shown to have superior performance, compared to other Kalman filter-based estimators. Nonetheless, the method was applied only for monocular pose estimation. In spite of the aforementioned advances in adaptive and nonlinear Kalman filtering, only few of these techniques were applied to sensor fusion. A number of works proposed adaptive EKF [3.41, 3.42] and UKF [3.43] for specific sensor fusion applications. However, these methods suffered from the aforementioned shortcomings. In addition, a self-tuning approach based on the Auto-Regressive Moving Average (ARMA) innovation model was introduced for linear sensor fusion [3.44]; however the method was not extendable to nonlinear systems. Furthermore, similar to other adaptive techniques, the convergence rate of this method was low; thus the approach was more suitable for linear systems with constant error covariance. In this chapter, sensor fusion techniques for the proposed pose estimation methods are discussed. Three possible fusion structures, namely centralized, decentralized, and pre-processing schemes are introduced. The centralized methods fuse the measurements together and use the fused data as the input to the pose estimating unit, while the decentralized methods estimate the pose parameters separately and then fuse their estimated states. As a result, they are also known as measurement and state fusion algorithms, respectively. Pre-processing techniques combine data from all cameras before processing and send it to the pose estimation unit as a single unified source of data. Each of these schemes is discussed separately as follows.

50

3.3 Centralized Sensor Fusion 
3.3.1 Centralized Kalman Filter-Based Fusion
To fuse the measurement data in Kalman filter-based methods (e.g. IAEKF), the data from each source is merged with others into a large measurement vector. The measurement data from different cameras are first concatenated into a single measurement vector,

k   1,k

. . l ,k  ,

(3.1)

where  k is the overall measurement vector, i,k is the measurement from the ith camera (either EIH or ETH), and l is the total number of the cameras. The measurement modeling function is then modified as,
F  k    F1  k  . . Fl  k  ,

(3.2)

where Fi  k  is the measurement modeling function, described in (2.22), for the ith camera. Moreover, the measurement noise covariance is altered as follows,  k  diag 1,k . . l ,k  , (3.3)

where diag   is the diagonal matrix with the elements of the contained matrices as diagonal elements, and i ,k is the measurement noise covariance matrix of the ith camera. In addition, a selective fusion scheme is engaged to enhance the accuracy and robustness of the system towards feature's impairments such as occlusion, illumination and point of view changes. A quality measure is exploited to evaluate the reliability of the extracted features [3.45]. If the quality of an extracted feature drops below a threshold value (e.g., due to partial occlusion), the feature will be eliminated from the list of features employed in fusion algorithm. To serve this purpose, a weighting matrix is introduced,
Wk  diag w1 . . wnl  ,

(3.4)

51

wi   

 0 0 gi     1 1 gi  

,

(3.5)

where Wk is the weighting matrix, wi is the weight associated to the ith feature point, g i is the quality measure of the ith feature point, and  is the predefined quality threshold. The gain matrix in (2.28) and (2.45) are then modified to exclude the undesirable features from the pose estimation method, i.e.,

K k  Ck |k 1 Fk T  k  Fk Ck |k 1 FkT  Wk ,
1

(3.6) (3.7)

K k  Cx , y ,k C y ,k 1Wk .

3.3.2 Centralized VVS Fusion
In this method, VVS is selected as the central pose estimation unit, where the virtual eye-in-hand cameras move with a virtual manipulator, while the virtual eye-to-hand cameras observe the motion of the virtual auxiliary object attached to the virtual manipulator. The velocity of the virtual manipulator is calculated at each time step based on equivalent velocity of all virtual cameras. For this purpose, all features are gathered in one vector and use the IBVS control law to calculate overall velocity of the virtual end-effector through centralized fusion. The vector of all features is related to the velocity of the virtual end-effector as follows,
s   1
cl  c1  l  . . s    J s,1e . . J s,l e  Ve , T T

(3.8)

which yields,
l   1 Ve    J s ,1 c . . J s ,l  c  s1 . .  sn  . e e



T



 



(3.9)

t   Here Ve   e  is the overall velocity of the virtual end-effector in its own coordinate frame and  e   

 



is the pseudo-inverse function. Once the overall velocity is calculated, the pose is updated as

before through (2.93) and (2.94). As was proven earlier, Gauss-Newton method is very similar to 52

VVS method. As a result, their fusion techniques are also similar and therefore are not discussed here. It should be mentioned that the size of the overall Jacobian matrix in (3.9) increases with the number of cameras, which in turn can prolong the inversion calculation. Decentralized VVS fusion is a way to alleviate this difficulty, which is explained in the sequel.

3.4 Decentralized Sensor Fusion
In this section, decentralized fusion schemes are introduced. In these schemes, data from EIH and ETH cameras is exploited to estimate the pose for each camera separately. The estimated poses are then fused based on their pose error covariance. From this point of view, using EKF for pose estimation is advantageous, since it provides the estimated pose along with its error covariance. To accommodate visual servoing requirements, the relative pose of the object with respect to the robot's end-effector is estimated. This choice of pose allows some of the needed calibrations in previous works (i.e., robot and ETH camera external calibration) to be relaxed. The object's pose (and its error covariance) can directly be obtained for EIH cameras, whereas in case of the ETH cameras the relative pose of the object should be combined with the relative pose of the end-effector with respect to the camera.

3.4.1 Decentralized Kalman Filter-Based Fusion
A decentralized sensor fusion scheme is selected for accurate and robust pose estimation, while preserving the computational cost. The pose of the target object with respect to robot's endeffector is estimated separately using ETH and EIH camera information. These estimations are next checked for fault detection. If any of the EKF methods are detected as faulty, the estimation involving the erroneous pose will be eliminated from fusion step. A fault detection based on EKF innovation term is used for this purpose [3.46]. A decentralized fusion layer is exploited to fuse the pose estimation into a more accurate output. Figure 3.1 demonstrates the block diagram of this fusion scheme. The fusion is based on maximum likelihood criterion developed by [3.13]. The fused estimation is calculated as follows,

^ k |k  Wi ^ k |k ,i , 
i 1

l

(3.10)

53

^ k 1|k 1 

z 1  1, k ^ k |k 1 

^ k |k  ^ k |k 

^ k 1|k 1 

z

1

 2,k
^ k |k 
3,k

^ k |k 

^ k 1|k 1 

z

1

^ k |k 1 

Figure 3.1 Decentralized sensor fusion block diagram. ^ k |k is the fused estimation,  ^ k |k ,i is ith local estimation where l is the number of local estimators,  and W i is its fusion weighting matrix and is defined as,
 l  Wi  Ci   C j 1  ,  j 1 
1 1

(3.11)

where C i is the error covariance of ith estimation, and the superscript "-1" indicates the inverse of the matrix. In fusion algorithm that exploits (3.10) and (3.11), the two fusing estimations are assumed to be independent. The error covariance of the final fused pose is equal to,
 l  C f    C j 1  ,  j 1 
1

(3.12)

which is less than any of the error covariance matrices [3.13]. The error covariance of the estimation error using EIH camera data is directly obtained from EKF; however the error covariance of the pose estimated using ETH camera information is not 54

available and should be estimated separately. The estimation of error covariance using the EKF pose estimators is discussed next.

3.4.2 Error Covariance Computation
As it can be seen from (2.3), the estimated pose through ETH requires three poses to be combined. The error covariance of the estimated pose is impacted by each of the comprising pose estimations and their error. This section illustrates how to estimate the error covariance of a pose, composed of two dependent pose estimations. The error covariance is discussed in two separated parts, the angular error covariance and the translational error covariance. Each of these covariance matrices are estimated for a combined transformation matrix,
3 H13  H 2 H12 ,

(3.13)

3 where H12 and H2 represent two known pose estimations with known error covariance. Finally the

error covariance of an inverse transformation is estimated, since (2.3) necessitates this computation. It should be noted that the coordinate frames 1-3, used in this section, are symbolic and can be replaced with any coordinate frames in this work. The angular error of the pose H13 is defined as,

^3 , 3  13   1
1

(3.14)

^3 are true and estimated Euler angles related to where  represents the estimation error, 13 and  1

R13 . This error is related to the rotation matrix error,

   J1313 ,
3 1

(3.15)

where  13 is the vector form of the rotation matrix, R13 ,

 13   ( R13 ) .

(3.16)

55

Here operator     converts matrices to vectors. The Jacobian matrix J13 relates the error of the rotation matrix (  r ) and Euler angles' error ( 13 ) and is calculated as follows,
3 1

 R 3 R 3 R 3  J13    ( 13 )  ( 13 )  ( 13 )  . 1  1   1

(3.17)

The rotation error is calculable from the two estimations' errors,
3  R   R R12  R2 R .
3 1 3 2 2 1

(3.18)

The error of rotations is then related using (3.15),

   K1   K 2  ,
3 1 3 2 2 1

(3.19)

where,
3 3 3  R2  R2 R2 2 2 K1    ( 3 R1 )  ( 3 R1 )  ( 3 R12 )  ,  2  2   2 

(3.20)

2 2 2  3 R1 3 R1 3 R1  )  ( R2 )  ( R2 ) . K 2    ( R2 12 12  12  

(3.21)

The angular error covariance is calculated using (3.15) and (3.19),

E   3   3T  J13 K1 E   3   3 T K1T J13T  J13 K 2 E   2   2 T K 2T J13T .
1 1 2 2 1 1













(3.22)

The translational error is defined as follows,

 t  t13  t^13 ,
3 1

(3.23)

where t13 and t^13 are the true and estimated translation. This error is calculated as follows,
3  t   R t12  R2 t  t .
3 1 3 2 2 1 3 2

(3.24)

56

The equation can be rewritten as follows,
3 t3  K33  R2  t 2   t3 ,

1

2

1

2

(3.25)

where,
 R 3 K 3   23 t12   2
3 R2 t2 3 1  2 3  R2 t2 . 3 1   2 

(3.26)

Therefore the covariance of the translational error is computed as:
3 3T E  t 3  t 3 T  K 3 E   3   3 T K 3T  R2 E  t 2  t 2 T R2  E  t3  t3 T
1



 K 3 E   3  t 3 T  E  t 3   3 T K3T .
2 2 2 2



1



 



2

2







1

1





2

2



(3.27)

Computation of the desire pose using ETH camera involves the inverse relative pose of the endeffector with respect to the ETH camera. Therefore, the error covariance of this inverse pose
1  H12 should be calculated. The angular error of the inverse pose H 2

 

1

estimation is defined as

follows, 1 1  J12  2 , J2
2 1

(3.28)

where,
1T 1T 1T  R2  R2 R2 1  . ) J 2   ( 1 )  ( 1 )  ( 1   2  2   2 

(3.29)

Hence, the angular error covariance of the inverse pose is calculated as follows,

1 J12 E  2  2 T J12T J 2 1T . E 1 1 T  J 2
2 2 1 1









(3.30)

The translational error of the inverse pose is defined as follows,

 t   R T t12  R12T  t   K 4   R12T  t ,
1 2 2 1 2 1 2 1 2 1

(3.31)

57

where,
 R 2T K 4   1 2 t12  1 R12T 2 t1 12 R12T 2  t1  .  12 

(3.32)

So the translational error covariance is computed as follows,

    R E    K
2 2

E  t1  t1  K 4 E   2   2 T K 4T  R12T E  t 2  t 2 T R12  K 4 E   2  t 2 T R12
1 1 1 1 1 1











2T 1

T

T 4

.

(3.33)

2 12 t1

3.4.3 Decentralized VVS Fusion
Another method for decentralized fusion is through VVS. For this matter, the equivalent endeffector velocities are calculated separately for each camera using (2.86) and (2.88),
i Vei   J s,i c si , e







(3.34)

and use a decentralized fusion technique to reconstruct the overall velocity of the virtual camera. Here  is a gain factor. For this purpose, a weighted averaging operator is employed,

Ve  WiVei ,
i 1

m

(3.35)

where Wi is the associated weight for the ith camera. The weights may be adjusted to give more weights to more accurate sensors (e.g., eye-in-hand cameras), or to isolate possible faulty sensors (e.g., those that have encountered occlusion), similar to Kalman-based methods. If no prior knowledge about the sensors is available, the weight might be selected to be equal. The accuracy of decentralized fusion techniques are generally lower than centralized ones, however they are faster and they also ease the sensor fault detection and separation [3.14], which makes this technique appealing.

58

3.5 Pre-Processing Fusion
A common way for sensor fusion is to centrally fuse the data by concatenating all data into a single vector and feed the new data into the processing (e.g., pose estimation) unit. Since the characteristics of the new data have changed, the processing unit should be modified to deal with the newly formed data. This modification may be demanding and different for every individual processing method. In addition, the size of the data increases with the number of sensors, which usually result in exponential rise in computational costs. Another solution is to fuse all the data into a single element and pass the fused data to the processing unit. This is especially beneficial since the size of the data is preserved and the processing unit is needless of any modifications; however this method is usually practical for data of the same type. A brief discussion on this matter can be found in [3.15]. In this section, a novel framework for pre-processing sensor fusion is proposed. For this matter, the observations from different cameras are transformed into equivalent data, seen by a virtual camera which is located at the desired place (i.e., robot end-effector). A weighted averaging operator is used as the fusion unit. The fused data is then processed by a pose estimation algorithm. Since an estimation of the object depth is required for data transfer, an iterative algorithm (i.e., Dementhon algorithm proposed in [2.16]) is considered for pose estimation. The main advantage of the proposed method is its applicability to a large group of pose estimation methods, since it is almost independent of the estimation algorithm.
C  e Let di   pC . . pO ( d i ) be the ,l  be the image information from camera i, and  o ,i  f i   O,1
i i
T

desired pose calculated based on camera i information. Here f i() is the pose estimating function. Then the goal is to find
e o  f   d1 ,.., d n  ,
T

(3.36)

where f   is the fusing/estimating function that maps the information from all cameras to the desired pose. Fusion of multiple data sources might be done at different stages of a system. In this section, the focus is on the fusion of the data before getting processed for pose estimation (i.e. pre-processing fusion). For this purpose, a virtual camera with same frame coordinates as 59

robot end-effector is assumed. The fusion of mapped data (to this virtual camera) is considered for pose estimation,
f  d1 ,.., d n   f h  d h  ,

(3.37)

where,
d h  h  d1 ,.., d n  .

(3.38)

One should realize that fused data, d h has the same form as any di , therefore it neither increases the computation, nor changes the structure of the pose estimator. To form the fused data, each set of information from a camera is first mapped to equivalent data from virtual camera h, and is fused next by means of a weighted averaging operator,
 , dh  W j d j
j 1 n

(3.39)

 . If the where Wj is the weight matrix associated to equivalent data from camera j, denoted by d j

error covariance of the transferred data from each camera, denoted by Ci , is known, the weights are calculated as follows,
  Ci1   

Wi



 1 C j  j 1 
n

1

.

(3.40)

It can be shown that the error covariance of such fused data is equal to,
   

Ch

 j 1

n

1 C j

  

1

,

(3.41)

which is smaller than any other error covariance,
 C h  Ci  i   1, n  .

(3.42)

60

The proof of this fact is available in [3.13]. If Ci is not available, the weights will be assumed equal. It also worth mentioning that the weights are normalized,
Wj 1.  j 1
n

(3.43)

The weights might also be used to exclude the corrupted/noisy data from the fusion process. Now to calculate the equivalent data for each camera, the relation between the presentations of a single feature point in two different frames, namely the camera i and the virtual camera, is discussed. From (2.4) one can write,
e ci e Poe, j  Rc P  tc . i o, j i

(3.44)

e    p e o . . p Assuming d i ,i ,l  , the equivalent data for each camera is calculated as follows  o,i ,1

using (2.4),
j e  ci p   ci e  o,i , j 1  z e Rci  po, j 1  z e , j j

z ci

te

(3.45)

ci where Rcei and tcei are assumed to be known through calibration, and po , j is available through
e i feature detection on image from camera j; however z c j and z j are not available and needed to be

estimated. Since (3.45) provides the system with 3 linear equations for 2 unknown parameters of
e i the equivalent data, one of the unknown parameters (i.e., z c j or z j ) can be calculated from the

other. The depth of features in each camera frame is generally unknown, but the depth of features in end-effector frame can be approximated by the previous calculated pose.

^e  ^ e oo ^e z , j  to , j  0 0 1 Ro P





(3.46)

e ^o where R and t^oe are the approximations of the current pose, obtained from the previous

estimated pose. This approximation is valid in visual servoing, since the consequent poses are close due to camera's high frame rate. Having z ej estimated, (3.45) yields, 61

z ck j 

 ci ^ci z j  0 0 1 tck 0 
ci ck 0 1  Rck p j

.

(3.47)

The approximation used in (3.47) can be improved if an iterative technique is considered for pose estimation. Then, the pose obtained at each iteration is used as a pose approximation in next iteration calculation of (3.47).

3.6 Simulation and Experimental Results
In order to verify the effectiveness of the proposed sensor fusion methods for pose estimation, numerous simulations and experiments are conducted. The accuracy of each method is the main concern of the tests. The simulations are performed in MATLABÂ® 2011b from Mathworks (Natick, MA, USA). The system configuration for the simulations and experiments are similar to the system which was explained in Sec. 2.2.

3.6.1 Centralized Fusion
First, the performance of the centralized IAEKF and IAUKF fusion is put into test and compared with that of the EKF-based fusion, UKF-based fusion and monocular IAEKF pose estimators. For this matter, an experimental setup composed of a 5-DOF CRS robot from CRSRobotics (Burlington, ON, Canada), an EIH Firefly Point Grey camera (Richmond, BC, Canada), a similar camera as an ETH camera, an accurate optical tracker from NDI (Waterloo, ON, Canada), the target and auxiliary objects are exploited. The image size of each camera is 640 by 480 pixels. The cameras work at 60 frames per second and are calibrated prior to the experiments. The location of the EIH camera coincides with that of the robot's end-effector. The target object and robot's end-effector object each has four circular features (black dots) with known distance between the features. The exploited optical tracker is very accurate and capable of locating a set of infrared markers with accuracy of 0.1 mm with a frequency of up to 4500 targets per second. Since the accuracy of this measurement device is higher than the camera, its measurements have been selected as the ground truth. The infrared markers are installed on the robot and the target object and their positions are known with respect to the target object and robot's end-effector. The robot is a 5 DOF anthropomorphic arm. Each joint of the robot can move as fast as 210 62

degrees per second, with a maximum acceleration of 498 degrees per second squared. The experimental setup is shown in Figure 3.2. The setup and the frame coordinates are the same as the setup shown in Figure 2.1.

Figure 3.2 Experimental setup used for sensor fusion. In the first experiment, the similarity of the proposed iterative methods is put into test. The performance of IEKF is compared with those of the EKF that have zero measurement noise and the Gauss-Newton algorithm. The chosen number of iterations suffices for (2.69) to hold. The same number of iterations is considered in the inverse-Jacobian algorithm. Figure 3.3 shows the error of these methods and Table 3.1 summarizes the results. The first column of Figure 3.3 entails the translational errors, while the second column demonstrates the errors of orientation (represented in Euler angles). Apparently all of the methods result in very similar estimation error, as was predicted. In fact, the results from IEKF and Gauss-Newton are almost identical, 63

while EKF with zero measurement noise is slightly different (since it represents the GaussNewton method with no iterations). The results suggest the applicability of alternative methods in case of highly dynamic systems.

6 4 2 0 0 5 10

1

phierr (deg)

EKF Gauss-Newton IEKF

xerr (mm)

0.5

0

0

5

10

Time (s) thetaerr (deg)
3 1.5 1 0.5 0 0

Time (s)

yerr (mm)

2 1 0 0 5 10

5

10

Time (s)
3 2 1 0 0 5 10 1

Time (s) psierr (deg)

zerr (mm)

0.5

0

Time (s)

0

5

10

Time (s)
Newton, and IEKF.

Figure 3.3 First experiment: Pose estimation error of EKF with zero measurement noise, Gauss-

64

Max Error Mean Error

EKF Gauss-Newton IEKF EKF Gauss-Newton IEKF EKF Gauss-Newton IEKF

x (mm) 4.8 4.8 4.8 1.1 1.1 1.1 1 1 1

y (mm) 2.8 2.8 2.8 0.6 0.6 0.6 0.5 0.5 0.5

z (mm) 2 2 2 0.4 0.4 0.4 0.4 0.4 0.4

 (deg) 0.6 0.6 0.6 0.1 0.1 0.1 0.1 0.1 0.1

 (deg) 1.4 1.3 1.3 0.2 0.2 0.2 0.2 0.2 0.2

 (deg) 0.7 0.7 0.7 0.2 0.2 0.2 0.2 0.1 0.1

Std

Table 3.1 First experiment: Iterative schemes in comparison with zero noise EKF. In the second experiment, the accuracy of the proposed IAEKF fusion technique is measured and compared to similar methods (namely EKF fusion and monocular IAEKF) and the reference pose. For this purpose, the noise covariance matrices are tuned initially based on offline measurements. Figure 3.4 demonstrates the output of pose estimation methods in comparison to ground truth. Figure 3.5 magnifies the error of each estimation technique. The statistics of this error is summarized in Table 3.2. Since the trajectories of the pose are dynamic, EKF fusion cannot estimate the pose accurately, even when the filter is tuned initially. On the contrary, IAEKF methods compensate for changes in the system noise variations. The mean of estimation error of the proposed fusion method is generally lower than the other two methods. This is because IAEKF fusion tunes the noise parameters automatically, is more robust to changes in system dynamics, and also benefits the data from different sources.

65

IAEKFFUSION 100 20 EKFFUSION IAEKFEIH Ground Truth

phi(deg)
0 5 10

x(mm)

10 0 -10 0 5

0

-100

10

Time (s)
60 10

Time (s) theta(deg)

y(mm)

40 20 0 0 5 10

0 -10 -20 0 5 10

Time (s)
250 20

Time (s)

psi(deg)
0 5 10

z(mm)

10 0 -10 0 5 10

200

150

Time (s)

Time (s)

Figure 3.4 Second experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion and ground truth, in case of tuned filters.

66

IAEKFFUSION 6 4 2 0 0 5 10 10 IAEKFEIH EKFFUSION 5

phierr(deg)

xerr(mm)

0

0

5

10

Time (s)
3 2 1 0 0 5 10 6

Time (s) thetaerr(deg)

yerr(mm)

4 2 0 0 5 10

Time (s)
15 10 5 0 0 5 10 3

Time (s) psierr(deg)

zerr(mm)

2 1 0 0 5 10

Time (s)

Time (s)

Figure 3.5 Second experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of finely tuned filters.

67

x y z   (mm) (mm) (mm) (deg) (deg)

 (deg)

IAEKF 4.8 2.8 2.6 0.6 1.6 0.8 Fusion Max IAEKF 4.9 2.8 2.5 0.6 2.4 2.5 Error EIH EKF 5.1 2.4 11.6 7.7 5.8 1.7 Fusion IAEKF 1.1 0.6 0.4 0.1 0.2 0.2 Fusion Mean IAEKF 1.2 0.6 0.4 0.1 0.4 0.6 Error EIH EKF 1.2 0.5 2 0.9 1.1 0.2 Fusion IAEKF 0.9 0.5 0.4 0.1 0.2 0.1 Fusion IAEKF Std 1 0.5 0.4 0.1 0.3 0.6 EIH EKF 1 0.4 2.2 1.3 1.1 0.2 Fusion Table 3.2 Second experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of fine tuning. In the third experiment, the accuracy of the IAUKF fusion is tested and compared to that of UKF fusion. Similar to previous experiment, the both filters are initially tuned and initialized properly. The same trajectories are taken to test both methods. The results of this comparison are shown in Figure 3.6 and the result summary is depicted in Table 3.3. As it can be seen, the proposed IAUKF performs better that UKF in terms of accuracy. This fact is also reflected in the mean of error in Table 3.3.

68

6 ex (m) 4 2 0

x 10

-3

0.15 e (rad) 0.1 0.05 0 0 5 10 Time (s)

UKF IAUKF

0 x 10
-3

5 10 Time (s)

15



15

3 ey (m) 2 1 0

0.15 e (rad) 0 5 10 Time (s) 15 0.1 0.05 0 0 5 10 Time (s) 15

0.015 e (rad) 0 5 10 Time (s) 15 ez (m) 0.01 0.005 0

 

0.04 0.02 0

0

5 10 Time (s)

15

Figure 3.6 Third experiment: Pose estimation error of IAUKF fusion in comparison with UKF fusion, in case of finely tuned filters.

69

x y z (mm) (mm) (mm)

 (rad)

 (rad)

 (rad)

IAUKF 4.8 2.8 1.9 0.008 0.022 0.011 Fusion Max Error UKF 5.0 2.8 14.2 0.142 0.146 0.038 Fusion IAUKF 0.8 0.5 0.3 0.001 0.004 0.002 Mean Fusion Error UKF 0.9 0.4 1.8 0.011 0.018 0.003 Fusion IAUKF 0.9 0.4 0.3 0.001 0.003 0.002 Fusion Std UKF 1.0 0.4 2.5 0.020 0.027 0.004 Fusion Table 3.3 Third experiment: IAUKF fusion in comparison with UKF fusion, in case of fine tuning. In the fourth experiment, the process noise covariance of the filters is reduced 100 times to verify the effect of filter tuning in the proposed method and its counterparts. The error of IAEKF fusion pose estimation is compared with EKF fusion and IAEKF based on EIH camera in Figure 3.7. The statistics of these errors are summarized in Table 3.4. As it was expected, the performance of EKF fusion algorithm decreases significantly, while the IAEKF-based methods remain almost without change. This fact can be inferred from large errors of the EKF fusion method. This experiment magnifies the role of filter tuning in Kalman-filter based pose estimation methods. As mentioned before, adaptive and robust schemes limit the undesirable effects of any mistuning.

70

IAEKFFUSION 20 15 IAEKFEIH EKFFUSION

phierr(deg)
0 5 10

xerr(mm)

10 5 0 0 5

10

0

10

Time (s)
20 10 15 10 5 0 0 5 10

Time (s) thetaerr(deg)

yerr(mm)

5

0

0

5

10

Time (s)
40 6

Time (s) psierr(deg)

zerr(mm)

4 2 0 0 5 10

20

0

0

5

10

Time (s)

Time (s)

Figure 3.7 Fourth experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of noise mismatch.

71

x y z   (mm) (mm) (mm) (deg) (deg)

 (deg)

IAEKF 4.8 2.7 2.2 0.6 1.5 0.7 Fusion Max IAEKF 4.9 2.8 2.6 0.8 4.7 5 Error EIH EKF 17.8 16.3 32.2 12.6 9.6 4.4 Fusion IAEKF 1.1 0.6 0.4 0.1 0.2 0.2 Fusion Mean IAEKF 1.1 0.6 0.4 0.1 0.6 0.6 Error EIH EKF 3.7 2.5 7.8 3.2 3.6 1.1 Fusion IAEKF 1 0.5 0.4 0.1 0.2 0.1 Fusion IAEKF Std 1 0.5 0.4 0.1 0.7 0.7 EIH EKF 3.9 3.1 6.4 2.8 2.2 1 Fusion Table 3.4 Fourth experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of noise mismatch. In experiment 5, the sensitivity of the proposed method to initial state adjustment is compared to two previously discussed pose estimation methods. For this matter, the initial position states are misadjusted by 0.2 meters in each direction and the initial orientation states are misadjusted by almost 28 degrees (0.5 rad) for each Euler angle. The results are magnified in Figure 3.8. As it can be seen from the results, the IAEKF fusion algorithm converges towards the true value faster than the two others, though with an overshoot. This property assures the system to be minimally affected by erroneous initial estimations.

72

IAEKFFUSION 200 100 EKFFUSION IAEKFEIH Ground Truth

phi(deg)
0 0.5 1

x(mm)

50 0 -50 0 0.5

100

0

1

Time (s)
200 50

Time (s) theta(deg)

y(mm)

100 0 -100 0 0.5 1

0 -50 -100 0 0.5 1

Time (s)
600 20

Time (s)

z(mm)

400 200 0 0 0.5 1

psi(deg)

0 -20 -40 0 0.5 1

Time (s)

Time (s)

Figure 3.8 Fifth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of initialization maladjustment. In experiment 6, the pose estimation algorithms are tested under a different sampling rate for the same motion as previous experiments to test the effect of the sampling rate change on the estimations. Various factors such as change of the camera might cause variations in the sampling rate of the system. The sampling frequency is selected 5 times lower than original sampling frequency (Ts=50 ms). The proposed IAEKF fusion algorithm is compared in performance to 73

IAEKF based on EIH camera and EKF fusion algorithm under the new sampling time condition. Figure 3.9 shows the error results for these methods. The performance of the IAEKF fusion is superior to that of the two other methods in this case as the errors of this method is much less than the others. Sampling time changes particularly affect the EKF method, which is sensitive to changes of system parameters.

IAEKFFUSION 100 10 EKFFUSION IAEKFEIH 0 Ground Truth

0

-100

phi(deg)
0 5 10

x(mm)

-10

0

5

10

Time (s)
60 10

Time (s) theta(deg)
5 0 -5 -10 0 5 10

y(mm)

40 20 0 0 5 10

Time (s)
250 20

Time (s)

psi(deg)
0 5 10

z(mm)

10 0 -10 0 5 10

200

150

Time (s)

Time (s)

Figure 3.9 Sixth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of sampling mismatch. 74

In experiment 7, the robustness of the proposed estimation method is investigated for feature occlusion. To serve this purpose, two of the features in EIH camera are occluded for 1 second. The IAEKF fusion outcome of feature loss is shown in Figure 3.10 in comparison to two previously mentioned estimation methods. As it was expected, the IAEKF, based on EIH, diverges, since there are not enough feature points to follow the pose changes. Also the EKF fusion method is lost during the occlusion, however both return to the correct values once the features are visible again. The best performance belongs to IAEKF fusion which remains almost unchanged, thanks to the weighting system which isolates the faulty features from the measurement vector.

75

IAEKFFUSION 100 200 EKFFUSION IAEKFEIH Ground Truth

phi(deg)
0 5 10

x(mm)

100 0 -100 0 5

0

-100

10

Time (s)
60 100

Time (s) theta(deg)
50 0 -50 -100 0 5 10

y(mm)

40 20 0 0 5 10

Time (s)
300 100

Time (s)

z(mm)

200 100 0 0 5 10

psi(deg)

0 -100 -200 0 5 10

Time (s)

Time (s)

Figure 3.10 Seventh experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of occlusion. In order to test the superiority of data fusion over estimations using single data sources, the focal length of EIH camera is misadjusted by %2 in experiment 8. The error statistical parameters of this experiment are summarized in Table 3.5. As it can be concluded, fusion methods have superior performance especially in the estimated depth as its errors are less than the other methods. The focal length error of EIH camera is compensated by ETH camera data. Moreover, 76

IAEKF fusion shows better performance than EKF fusion. x y z   (mm) (mm) (mm) (deg) (deg)  (deg)

IAEKF 6.2 2.5 3.2 1.3 2 0.5 Fusion Max IAEKF 4.9 2.8 6.7 0.6 2.3 2.7 Error EIH EKF 6.5 2.7 14.1 7.8 5.3 1.8 Fusion IAEKF 1.8 0.6 0.7 0.2 0.6 0.1 Fusion Mean IAEKF 1.2 0.6 4.5 0.1 0.4 0.7 Error EIH EKF 1.8 0.5 2.8 0.9 1.6 0.2 Fusion IAEKF 1.2 0.5 0.7 0.2 0.4 0.1 Fusion IAEKF Std 1 0.5 0.6 0.1 0.3 0.7 EIH EKF 1.4 0.4 2.8 1.3 1.3 0.3 Fusion Table 3.5 Eighth experiment: IAEKF fusion in comparison with EKF fusion and monocular IAEKF, in case of EIH calibration error (%2 focal length error). The speed of the proposed IAEKF and IAUKF central fusion algorithm is compared with its counterparts in the 9th experiment. The estimation time is calculated for each method using a Core i3 2.2 GHz laptop with 4GB RAM. The results are shown in Table 3.6. As it was expected, iterative methods (i.e., IAEKF-based estimations) are more time consuming. Moreover, the amount of data processed for fusion techniques makes these methods slower compared to monocular camera-based algorithms. It is also noteworthy that IAUKF is considerably slower than the other methods. However, the imposed computational cost is usually manageable in today's fast systems and is not considered as a major problem.

77

Method IAUKF Fusion IAEKF Fusion IAEKF EIH UKF Fusion

CPU Time per estimation (ms) 131.9 33 15.9 3.5

EKF Fusion 1.8 Table 3.6 Ninth experiment: Pose estimation time of the different algorithms. In experiment 10, the efficiency of the proposed method is verified under an altered camera configuration. For this purpose, experiment 2 is repeated, having the cameras relocated in the workspace. The location of cameras and the robot are shown in Figure 3.11. Figure 3.12 shows the result of the proposed pose estimation versus its competitors. The errors of these methods are demonstrated in Figure 3.13. As it can be inferred, the proposed method provides a more stable and accurate pose estimation compared to the two others, regardless of the pose of the cameras.

Figure 3.11 The experiment configuration of experiment 10.

78

IAEKFFUSION 200 20 EKFFUSION IAEKFEIH Ground Truth

phi(deg)
0 5 10

x(mm)

10 0 -10 0 5

0

-200

10

Time (s)
100 20

Time (s) theta(deg)

y(mm)

50 0 -50 0 5 10

10 0 -10 0 5 10

Time (s)
450 20

Time (s)

psi(deg)
0 5 10

z(mm)

10 0 -10 0 5 10

400

350

Time (s)

Time (s)

Figure 3.12 Tenth experiment: Pose estimation output of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion and ground truth, in case of altered camera configuration.

79

IAEKFFUSION 6 4 2 0 0 5 10 15 IAEKFEIH EKFFUSION

phierr(deg)

xerr(mm)

10 5 0 0 5

10

Time (s)
15 10 5 0 0 5 10 10

Time (s) thetaerr(deg)

yerr(mm)

5

0

0

5

10

Time (s)
15 10 5 0 0 5 10 10

Time (s) psierr(deg)

zerr(mm)

5

0

0

5

10

Time (s)

Time (s)

Figure 3.13 Tenth experiment: Pose estimation error of IAEKF fusion in comparison with monocular IAEKF (IAEKFEIH), EKF fusion, in case of altered camera configuration. Next, the proposed VVS fusion method is compared to its rival methods, namely Gauss-Newton optimization-based and EKF algorithms through an experiment. All algorithms are initialized through POSIT algorithm proposed by [2.16]. The POSIT method approximates the pose initially through geometric projection of the features on the camera plane, and then makes the estimation accurate through iterations. At each step, the estimated pose of previous step is utilized as the 80

initial pose. Since the initial poses are close to final estimations, a total number of three iterations are considered for each of the algorithms. Accuracy and time efficiency are the criteria of this comparison. In the 11th experiment, the accuracy of centrally fused virtual visual servoing is compared with that of Gauss-Newton optimization and EKF methods. A trajectory composed of three different movements is considered as a test bench. The estimation error of each algorithm during robot operation is highlighted in Figure 3.14. As it was expected, the estimations resulted from VVS and Gauss-Newton optimizations are very close. However, VVS shows to be more robust to image noise, which makes it more attractive. The same property was reported by [2.20] in the case of a single camera. Both of these methods outperform EKF in terms of accuracy, as it can be inferred from Figure 3.14. The error statistics are briefed in Table 3.6. As it can been seen from the table, VVS performs slightly better than Gauss-Newton optimization, and much better than EKF method. The difference between VVS and Gauss-Newton optimization is more significant in case of translational parameters (first three columns), while EKF performance is comparable only in case of x and y directions.

81

6 4 2 0 0 5 10

4

phierr(deg)

Centralized VVS Gauss-Newton EKF

xerr(mm)

2

0

0

5

10

Time (s) thetaerr(deg)
3 3 2 1 0 0

Time (s)

yerr(mm)

2 1 0 0 5 10

5

10

Time (s) psierr(deg) zerr(mm)
4 2 0 0.6 0.4 0.2 0 0

Time (s)

0

5

10

5

10

Time (s)

Time (s)

Figure 3.14 Eleventh experiment: Centralized VVS pose estimation errors versus Gauss-Newton and EKF pose estimation errors.

82

x Centralized Max Error VVS GaussNewton EKF Centralized Mean Error VVS GaussNewton EKF Centralized VVS Std GaussNewton EKF

y

z







(mm) (mm) (mm) (deg) (deg) (deg) 3.6 4.6 5.4 1 1.1 1.2 0.7 0.9 1 2.1 2.3 2.7 0.5 0.6 0.6 0.4 0.4 0.5 1.7 2.3 4.2 0.3 0.4 0.9 0.3 0.4 0.8 0.5 0.7 3.1 0.1 0.1 0.6 0.1 0.1 0.6 1.3 1.4 2.7 0.2 0.3 0.6 0.2 0.2 0.5 0.6 1.2 0.7 0.2 0.2 0.2 0.1 0.1 0.2

Table 3.7 Eleventh experiment: The results of the proposed VVS fusion in comparison with those of Gauss-Newton and EKF.

3.6.2 Decentralized Fusion
In order to verify the effectiveness of the proposed decentralized EKF-based method, three different simulations and one experiment have been performed. The first simulation puts the idea of error covariance calculation into test. For that matter, the estimations of relative pose of the end-effector and the relative pose of the target object with respect to ETH camera are combined to acquire the relative pose of the object with respect to the end-effector. The error covariance of the combined estimation is calculated during a visual servoing robot maneuver by Monte Carlo simulation, using 100 of different samples and is compared with the estimated error covariance computed previously. The random Gaussian noise with zero mean and 0.001 pixel standard deviation is used for this purpose. Figure 3.15 shows the response of this two error covariance

83

estimations. As it can be seen, the estimated error covariance is matching very closely the true error covariance. Slight mismatches are present as a result of approximations made by this work.

Figure 3.15 First simulation: Error covariance estimation. The second simulation verifies the effectiveness of the decentralized fusion and analyses the accuracy of this method. For this purpose, the results of a decentralized fusion that exploits the pose estimations from ETH and EIH cameras are verified. The results of this method are brought in Figure 3.16, comparing them with the poses estimated by each camera. The estimation made by data from ETH is generally noisier, since two estimations are used to form this pose. Though, in some of the estimations (e.g., ) EIH provides less accurate estimation and the fusion inclines to the data provided by ETH.

84

Figure 3.16 Second simulation: Decentralized EKF-based fusion compared with monocular pose estimators. The third simulation demonstrates the power of decentralized fusion method in the case of fault occurrence. In this simulation, two feature points of the object is occluded from second 40 to second 50. Figure 3.17 shows how the fused pose survives this problem by cutting the faulty data off the fusion layer. The estimation based on EIH camera become unreliable after the occlusion, while the fusion estimation is the same as estimation based on ETH camera, since only two cameras has been used.

85

Figure 3.17 Third simulation: Decentralized EKF-based fusion compared with monocular pose estimators in case of faulty local estimation. Finally, an experiment has been conducted to demonstrate the effectiveness of the proposed method in practice. The experimental setup used for this matter is the same as before. The pose of the object is estimated from each camera separately and is compared to the results from the proposed fusion method. Each Kalman filter is tuned through several experiments. Figure 3.18 shows the results of this comparison. It can be inferred that the result from the fusion technique follows the reality closely, while the other estimations have been deviated slightly in some cases. However, the fusion algorithm is twice slower than the single-camera estimators.

86

Figure 3.18 Twelfth experiment: Decentralized EKF-based fusion in practice compared with monocular pose estimators. Next, the accuracy of the proposed decentralized VVS fusion algorithms is put into test through an experiment. Similar trajectories as the experiment for the centralized fusion are used for this purpose. Pre-computed Jacobian, calculated based on measured features, is used for the decentralized fusion. The estimation error of each method is shown in Figure 3.19. The error of the decentralized fusion method seems to be larger, as it was expected. A summary of the important statistical points of the estimation errors can be found in Table 3.8. Judging by the mean value of the estimation errors, the decentralized fusion algorithm is almost as accurate as the centralized method. In some cases it shows equal or even better estimation compared to Gauss-Newton method.

87

4 2 0 0 5 10

phierr(deg)

xerr(mm)

6

1

Centralized VVS Decentralized VVS

0.5

0

0

5

10

Time (s) thetaerr(deg)
3 1.5 1 0.5 0 0

Time (s)

yerr(mm)

2 1 0 0 5 10

5

10

Time (s)
2 1.5

Time (s) psierr(deg)

zerr(mm)

1 0.5 0 0 5 10

1

0

0

5

10

Time (s)
VVS pose estimation error.

Time (s)

Figure 3.19 Thirteenth experiment: Decentralized VVS pose estimation errors versus centralized

88

x y z    (mm) (mm) (mm) (deg) (deg) (deg) Centralized 3.6 2.1 1.7 0.5 1.3 0.6 VVS Gauss4.6 2.3 2.3 0.7 1.4 1.2 Max Newton Error EKF 5.4 2.7 4.2 3.1 2.7 0.7 Decentralized 6.6 2.2 2 0.7 1.4 1.2 VVS Centralized 1 0.5 0.3 0.1 0.2 0.2 VVS Gauss1.1 0.6 0.4 0.1 0.3 0.2 Mean Newton Error EKF 1.2 0.6 0.9 0.6 0.6 0.2 Decentralized 1.6 0.5 0.4 0.1 0.3 0.3 VVS Centralized 0.7 0.4 0.3 0.1 0.2 0.1 VVS Gauss0.9 0.4 0.4 0.1 0.2 0.1 Newton Std EKF 1 0.5 0.8 0.6 0.5 0.2 Decentralized 1.2 0.4 0.4 0.1 0.2 0.3 VVS Table 3.8 Thirteenth experiment: Decentralized fusion accuracy compared to that of the centralized, EKF-based, and Gauss-Newton-based fusion. In an experiment, the time efficiency of VVS fusion methods is measured. A laptop with Core i3 2.2 GHz CPU and 4GB of RAM is used for these time measurements. The time of estimation for a single posture is found as an average of pose estimation time over the whole trajectory. The experiment was conducted for a setup with 2 and 4 cameras. The results of these measurements are shown in Table 3.9. The VVS estimation methods are shown to be faster than their counterpart (i.e., the Gauss-Newton). The reason lies within the efficient formulation of the Jacobian matrix in VVS algorithms. Moreover, the decentralized method has shown to be slightly faster than the centralized method. However, the speed improvement of this method, compared to the centralized VVS, is not significant. In fact, the calculation of Jacobian matrix accounts for most of the computational time and the reversion of the Jacobian matrix plays a less significant role. Yet, the time increase in the VVS methods is reported to be small compared to other methods. As it was expected, EKF performs fastest of all, since it is not an iterative algorithm. 89

Method

Estimation Time (ms) 2 Cameras 4 Cameras

Centralized 1.36 1.67 VVS Gauss1.76 2.58 Newton EKF 0.96 1.24 Decentralized 1.29 1.6 VVS Table 3.9 Estimation times of VVS fusion methods per estimation compared with that of EKFbased and Gauss-Newton-based fusion for 2 and 4 cameras.

3.6.3 Pre-Processing Fusion
A set of simulations are conducted to justify the proposed pre-processing fusion method. An eyein-hand camera and an eye-to-hand camera are considered in the simulations. The focal lengths of the cameras are assumed to be 1000 pixels. The end-effector coordinate frame coincides with those of eye-in-hand camera. The eye-in-hand robot moves with a known trajectory, while the eye-to-hand camera and the object are assumed to be stationary in the simulation environment. Initially, the object of interest is located in front of the eye-in-hand camera within distance of 1 meter, parallel to the camera plane. The object consists of four non-coplanar points. The eye-tohand camera is located a meter away from the eye-in-hand camera, having the object in its FOV. After 3 seconds, the eye-in-hand camera is moved to a new pose and becomes stable after three seconds. An inverse Jacobian scheme is exploited to move the camera to its new location. The iterative pose estimation method is implemented. The number of iterations is set to 10. The fusion is not performed in the first iteration of the first estimate, since the depth information is not available. In the fourth simulation, the pose is estimated counting only on the eye-in-hand camera. Additive Gaussian noise with standard deviation of 1 pixel is considered for features of this camera. The noise covariance is counted as the noise level of the camera data. Figure 3.20 shows the estimated pose compared to ground truth. As it was expected, this method shows high sensitivity to the measurement noise. This is especially noticeable in case of orientation angles and depth estimation. Another reason behind high errors is the long distance of the camera from the object. The estimation algorithm will perform better once the camera is closer to the object. 90

400 200 0 0 400 5

Eye-in-Hand Ground Truth

phi(deg)

0 -10 -20

x(mm)

0

5

Time (s)
20

Time (s) theta(deg)

y(mm)

200 0 0 5

0 -20 -40 0 5

Time (s)
1400 1200 1000 0 5 20

Time (s) psi(deg)

z(mm)

0 -20 0 5

Time (s)

Time (s)

Figure 3.20 Fourth simulation: Monocular pose estimation with an eye-in-hand camera. In order to alleviate the noise sensitivity, the proposed fusion scheme is employed in fifth simlation. The data from ETH camera is fused with data from EIH camera. Three different noise values are considered for the eye-to-hand camera data. This noise is assumed to be additive Gaussian and has the standard deviation of 0.5, 1, and 2 pixels for different simulations. The weights of fusion are equal, based on the assumption of no prior knowledge about the noise 91

values of the camera. Figures 3.21-3.23 show the results of these fusions. As it can be seen from the figures, the higher the noise level gets, the worse the estimation result will become. This is mainly due to the maladjustment of the weights in the fusion process. By having the same weights for all sensors, the noisy sensors might dominate the less noisy sensors. As it was mentioned before, one solution to that problem is to detect the highly noisy data and exclude them from the fusion data (robust to outliers). The other way is to adjust the fusion weights properly. Table 3.10 briefs the important statistics of the fusion estimation error for different levels of noise, in addition to monocular estimation error. As it was expected, data fusion of cameras with lower and even equal noise level results in improved accuracy of pose estimation. One should realize that once the noise levels are equal, the weight tuning results in equal weights, which is the default value for the weights. If the weights are not tuned, the noisy sensor may corrupt the overall fusion data, which is the case for fusion of eye-to-hand data with noise level twice as the eye-in-hand camera data.

92

400 200 0 0 400 5

5

Fusion(0.5X) Ground Truth

phi(deg)

x(mm)

0 -5 -10 -15 0 5

Time (s)
10

Time (s) theta(deg)
0 -10 -20 -30 0 5

y(mm)

200 0 0 5

Time (s)
1400 1200 1000 0 5

Time (s) psi(deg)
0 -10 -20

z(mm)

0

5

Time (s)

Time (s)

Figure 3.21 Fifth simulation: Decentralized EKF-based fusion for pose estimation with half level noise for ETH camera.

93

400

10

Fusion (1X) Ground Truth

x(mm)

200 0 -200 0 2 4 6 8

phi(deg)

0 -10 -20 0 2 4 6 8

Time (s)
400 50

Time (s) theta(deg)

y(mm)

200 0 -200 0 2 4 6 8

0

-50

0

2

4

6

8

Time (s)
1400 20

Time (s) psi(deg)

z(mm)

1200 1000 800 0 2 4 6 8

0 -20 -40 0 2 4 6 8

Time (s)

Time (s)

Figure 3.22 Fifth simulation: Decentralized EKF-based fusion for pose estimation with equal camera noise levels.

94

400 200 0 0 400 200 0 0 1400 5 5

20

Fusion (2X) Ground Truth

phi(deg)

x(mm)

0 -20 0 50 5

Time (s) theta(deg)

Time (s)

y(mm)

0 -50 0 20 5

Time (s) psi(deg)

Time (s)

z(mm)

1200 1000 0 5

0 -20 -40 0 5

Time (s)

Time (s)

Figure 3.23 Fifth simulation: Decentralized EKF-based fusion for pose estimation with double level noise for ETH camera.

95

x Eye-in-hand Max Error Fusion(0.5x) Fusion(1x) Fusion(2x) Eye-in-hand Mean Error Fusion(0.5) Fusion(1) Fusion(2) Eye-in-hand Std Fusion(0.5) Fusion(1) Fusion(2) 26.7 7.3 24.9 60.1 6.1 1.7 3.5 8.4 7.3 1.8 4 10.5

y

z

 8.7 1.8 11.2 27 2.1 0.8 1.6 3.5 1.8 0.7 1.6 3.6

 19.7 4.6 31.9 57.5 6.4 3.6 7.1 13.4 4.6 2.9 5.8 10.7

 19.9 4.6 12.3 25.2 6.4 1.9 3.7 7.3 4.6 1.4 2.8 5.6

(mm) (mm) (mm) (deg) (deg) (deg) 27.2 125.4 7.6 32.7 30.8 126.2 70.3 293.9 6.3 1.7 3.2 7.4 7.6 2.2 4.8 11.6 35.5 8.3 16.8 42.5 32.7 9.1 20.2 49

Table 3.10 Decentralized EKF-based fusion results compared to those of monocular EIH camera. In the next simulation, the importance of weight tuning for fusion is highlighted. For this purpose, the case of fusion of two cameras with different noise levels is considered again; however the weights are adjusted based on the noise covariance as was described in (3.40). The results of this fusion compared to equally weighted fusion are shown in Figure 3.24. As it was expected, tuning the weights increased the performance of the system significantly and prevented the noisy data from dominating the overall output of the fusion scheme.

96

400 200 0 0 400 200 0 0 1400 5 5

20

Fusion (Not tuned) Fusion (Tuned) Ground Truth

phi(deg)

x(mm)

0

-20

0

5

Time (s)
50

Time (s) theta(deg)

y(mm)

0 -50 0 20 5

Time (s) psi(deg)

Time (s)

z(mm)

1200 1000 0 5

0 -20 -40 0 5

Time (s)

Time (s)

Figure 3.24 Fusion with tuned weights pose estimation versus fusion with equal weights pose estimation results.

3.7 Summary
Accurate and robust pose estimation plays a key role in visual servoing systems. Traditionally, monocular vision was exploited for this purpose; however, the accuracy of a single camera is 97

limited and its measurements are prone to outliers and occlusions. Sensor fusion of a multicamera system was proposed to enhance both the accuracy and robustness of the pose estimation. Three fusion structures were introduced for this matter. Centralized fusion algorithms enabled the system to enhance the accuracy of the system. Two centralized methods, namely IAEKF and VVS were introduced, which had the capacity to increase the robustness of the system to parameter uncertainties and occlusion in addition to the accuracy. The major drawback of these systems was their increasing computation time with the number of cameras. Decentralized fusion algorithms were proposed to alleviate this problem. These methods were relatively less accurate compared to their centralized fusion counterparts; however, they performed comparatively faster. This speed difference was more tangible once the number of sensors is significant. In addition, these schemes were capable of fault isolation, which was difficult in centralized fusion algorithms. Finally, a pre-processing fusion scheme was introduced which was independent of the pose estimation algorithm, unlike the centralized fusion techniques. This fusion method was shown to be effective once the noise level of each camera is known in advance. Despite the advantages of multi-camera systems and fusion techniques, these methods impose higher costs to the system. Moreover, the overall processing time of these systems increases (almost linearly) with the number of sensors. However, the price to be paid for the achievements in increased accuracy and robustness seems to be minimal, since the efficiency of the visual servoing systems directly depends on the performance of pose estimation system. In addition, multi-camera systems add to the flexibility of the system by adding new degrees of freedom. The system designer may decide on the number of sensors and fusion algorithms based on the application, requirements, and available system hardware.

98

Chapter 4 Uncertainty Modeling in Visual Servoing Systems
4.1 Introduction
Integration of visual servoing systems into real-life applications mainly depends on their robustness to system uncertainties. While the robustness in the sense of system's stability has been well studied, the accuracy of system during the servoing (i.e., system trajectories) has not been thoroughly investigated. In order to enhance the accuracy of the system, its behavior under uncertain conditions must be investigated. Therefore, uncertainty modeling remains a key step towards the development of accurate systems. This chapter briefly reviews previous uncertainty modeling methods for visual servoing and their shortcomings, which led to the introduction of a novel closed-loop error modeling approach developed by following a probabilistic methodology. The modeling strategy is established based on a single-input single-output (SISO) system which accounts for discrete-time nature of the systems. Subsequently, the derived method is applied to visual servoing systems. Unlike the previous works, the proposed method is expandable to various types of controllers used for servoing purposes. In addition, it basically presents a straight-forward way to calculate the error covariance function over time, which is used in control loops. Thus, simplicity and generality of the proposed method are its main advantages over the previously proposed methods. Moreover, this method models the system uncertainties more accurately by considering the discrete-time characteristic of the system. The proposed uncertainty modeling methods are verified through Monte Carlo simulations, since the assumptions of this work (e.g., noise distribution) are hard to be realized in experimental setups. The proposed modeling method is exploited for control purposes, in the following chapter. This chapter is organized as follows. In Sec. 4.2 a brief review of previous works on uncertainty modeling in visual servoing systems is presented. The open-loop and closed-loop uncertainty modeling methods based on probabilistic error propagation are explained in Sec. 4.3. The application of uncertainty models in visual servoing systems are brought in Sec. 4.4. The 99

simulation results and discussion are provided in Sec. 4.5. A summary of this chapter in Sec. 4.6 concludes the chapter.

4.2 Literature Survey
In visual servoing scenarios, image data is usually assumed to be accurate, which might not hold in practice. As a matter of fact, the uncertainties in intensity function, digitalization, and image processing levels [4.1] all contribute to image uncertainties. Moreover, the system uncertainties and modeling errors affect the robot's end-effector pose, which would lead to the inaccurate positioning and even task failure. The focus of this chapter is on uncertainties introduced by image features. Two different methodologies have been developed in previous works to model the uncertainties in visual servoing systems, namely the boundary and the probabilistic methods. On one hand, the boundary method [4.2, 4.3] offered the upper and lower boundaries of camera displacement error using an eye-in-hand camera, assuming the level of image noise boundaries to be known. This method was later employed for feature selection [4.4] and global path planning [4.5]. While these works have proven their usefulness, they do not usually consider the servoing method used for visual servoing, i.e., only the final pose error is discussed. They assume that the exact pose of the object is known, which could only be estimated using visual data. Besides, this approach only provides conservative bounds of the pose error given a known image error bound [4.6]. Moreover, this method is time consuming and computationally expensive. On the other hand, the approach of probabilistic error analysis was proposed based on error covariance propagation through different components of a visual servoing system. This method was first employed in [4.7, 4.8] to analyze the error in PBVS and hybrid visual servoing (HVS) [4.9], in an open-loop fashion. The method divided these visual servoing systems into three components, i.e., pose estimation, servoing, and control. Subsequently, the image noise covariance was propagated through the linearized model of each part, similar to [4.10]. This work was recently extended to entail IBVS method and the closed-loop nature of visual servoing systems [4.11], since the previous works failed to address this important characteristic. The continuous-time Ornstein-Uhlenbeck process was exploited to model a proportional feedback controller of a simple linear system. The characteristics of this process were explored in IBVS 100

and PBVS systems, and simulations were run for verification. It was shown that only steady-state error covariance is obtainable for PBVS systems through this method. Moreover, the simulation results had some discrepancies between the theory and simulations, which was left for future research [4.11]. In this work, the closed-loop visual servoing method was considered as a time continuous system, which does not hold for visual servoing systems as the vision sensor data is acquired in discrete-time intervals. Moreover, the chosen stochastic process, (i.e., OrnsteinUhlenbeck process) is limited to the proportional control law only and does not apply to other controllers. This chapter presents a novel uncertainty modeling method for visual servoing systems that overcomes the shortcomings of the previous methods. In the sequel, the modeling methods are discussed in open-loop and closed-loop systems.

4.3 Uncertainty Modeling in SISO Systems
Probabilistic modeling of uncertainties propagated through a system is the topic of this section. The previous works on open-loop error modeling for a SISO system are briefly reviewed and a superior closed-loop model is proposed to address the shortcomings of these works as follows.

4.3.1 Open-Loop Uncertainty Propagation
This approach is based on first-order linear approximation of the system. A SISO system expressed by a function which relates its input and output is assumed,
y  h( x) ,

(4.1)

where x, y, and h represent the system's input, output, and the mapping function, respectively. The first-order approximation based on Taylor series suggests,

y  y0 

dh( x) .( x  x0 ) . dx x  x0

(4.2)

Assuming the system error as a deviation from true value, one can write, ^  y y, x ^  x  x, y (4.3)

101

where the noisy signals are denoted by "hat", the noise values are denoted by  in x and y directions and are assumed to be zero-mean random variables. Then (4.3) could be rewritten as,
^ y y dh( x) dx ^) , .( x  x
^ xx

(4.4)

which is equal to,

y  x

dh( x ) dx

.
^ x x

(4.5)

If the covariance of the input error (  x ) is known, the covariance of the output error could be approximated as follows,
2 2   dh( x) 2    dh( x)  2 2  dh( x )  y  E  y   E   x   E  x       x   ,   dx x  x  dx x  x dx x  x ^  ^  ^      2

(4.6)

where  x is the input error covariance,  y is the output error covariance and E    is the expectation function. Equation (4.6) relates the covariance of the input and output of a system and is the basis of error propagation in the probabilistic approach. While the open-loop modeling discussed above provides a useful tool for estimating the uncertainties of the system, it may not be useful for closed-loop systems since the feedback loop of the system has not been accounted for. A novel closed-loop uncertainty modeling is proposed to address this shortcoming as follows.

4.3.2 Closed-Loop Uncertainty Propagation
In this subsection, the uncertainty propagation in a closed-loop SISO control system is considered. The developed method is extendable to multi-input multi-output (MIMO) systems such as visual servoing. Figure 4.1 shows such a control system, where g () is the sensor transfer function and h () represents the system's controller and plant transfer functions. As it can be seen, the following relations are considered for input and output of each subsystem,

102

^, ex  x*  x

(4.7) (4.8) (4.9) (4.10)

y  h(ex ) ,
x  g ( y) ,

^  x x . x

^ where ex is the closed-loop error, x* is the system reference input, y is the system output, and x

is the feedback signal with uncertainty, x is the feedback's real value and  x is the feedback noise, which is assumed to have zero mean and known covariance. The noise sequence is assumed to be independent and identically distributed (i.i.d.). The goal of the system is to minimize the closed-loop control error, ex . Different controllers are usually employed for this purpose. This work investigates the uncertainty propagation in case of two most well-known controllers, namely proportional (P) and proportional-derivative (PD), which also have become very popular in visual servoing systems.

x* 


ex
^ x
 

h  

y

x
x

g  

Figure 4.1 A general SISO control system.
Proportional Controller

The closed-loop uncertainty propagation in a system with a proportional controller was previously followed by exploiting the solution of a stochastic process [4.8]. This process, known as Ornstein-Uhlenbeck process, is closely related to closed-loop control using simple proportional control law. The process is defined as follows,

dxt   ( xt   )dt   dwt ,
103

(4.11)

where wt is a Wiener-Levy process with unity variance,  ,  , and  are known constants. If initial point is known ( x0  c ), the mean and covariance of this process is solved as follows,

E     ( c   ) e   t ,  xt x0  c
Cov ( xs , xt x0  c) 

(4.12)

 2   s t e  e   ( s t ) , 2





(4.13)

and the variance is obtained by coinciding the times,
Cov ( xt , xt x0  c) 

2 1  e 2t  . 2

(4.14)

Despite the usefulness of this approach for uncertainty modeling in systems with proportional controllers, it cannot be extended to other types of controllers. Moreover, the discrete-time nature of the system is not considered. Therefore, a new uncertainty modeling approach is proposed to overcome these shortcomings in the sequel. In a system with proportional controller, the error is decreased to zero exponentially using a proportional control law,
 x , k   ex , k  0 , e

(4.15)

where  is the controller gain. Using (4.7) and (4.10), it can be shown that,
   ( xk  x* )   x  x * . x

(4.16)

*  0 ). The process x In this work, the system reference input was assumed to be constant ( x

could be approximated by integration over time,
, xk 1  xk  tx

(4.17)

where xk is the values of process x at time step k and  t is the time increment. Injecting (4.16) into (4.17) yields, 104

xk 1  xk  t  ( xk  x*   x )  1  t  xk  tx*  t x .
The covariance of the process is defined as,





(4.18)

cov( xk 1 , xk 1 )  E  xk 1  E ( xk 1 )  .
2





(4.19)

By assuming the reference input to be error free, the covariance of the process is calculated,
cov( xk 1 , xk 1 )  E E
k

1  t  x  tx  t  1  t  E x   tx    E  1  t   x  E  x    t  
* x *
2 k 2 k k



1  t  xk  tx*  t x  E 1  t  xk  tx*  t x 
x

2


(4.20)

 1  t  E
2

 x  E x     t  E   
2

2

2

k

k

x

 1  t  cov( xk , xk )   t   2 .

2

2

Here  2 is the covariance of the estimation error. It is worth mentioning that the current noise is independent of xk . Now if the covariance of process x is modeled with a time varying function, it can be shown that, cov( xk , xk )  f c (t ) . The time derivation of (4.21) results in, (4.21)

f c   cov( xk , xk )    t t  E   xk  E  xk   t

 E

  x  E x  
2
k k



2

  E    x  E  x     2  x  E  x  
k k

t

k  E  x k    k ).  2E   xk  E  xk    x   2 cov( xk , x

 

k

k

t

 

(4.22)

The covariance of next time step is calculated as follows using (4.17),

105

cov( xk 1 , xk 1 )  E
k k

 x  tx  E x  tx    Ex    ,  E  x  E  x     2tE  x  E  x    x f   Ex      f (t )  t E  x t  O(t ) t
2 k k 2 k k 2 2 c

(4.23)

2

c

which is in agreement with Tylor expansion of function f. Here O(t 2 ) denotes terms of order
t 2 or higher and are assumed to be negligible. Then (4.15) may be rewritten as,
fc  f c 2 t  1   t  f (t )   2 t 2 2 t

(4.24)

which results in,
f c  2   2 t f c (t )   2 t 2 . t





(4.25)

The solution to this differential equation is as follows,

f c (t ) 

  2 t  2  t    t 2     1 e  . (2  t )  

(4.26)

It is interesting to note that this result is close to the covariance expected by using OrnsteinUhlenbeck process (as was discussed in [4.11]) when time increment is small,

f c (t )  t

 2
2

1  e2t  .

(4.27)

However, the model shown in (4.26) is more accurate than the previous model of [4.11] in discrete-time systems as it accounts for the sampling time of the system. The steady-state error covariance could also be calculated by assuming, cov( X  , X  )  cov( X  , X  )  f ss . Using this assumption with (4.25) will result in, (4.28)

106

f ss 
which can also be obtained from (4.26).
Second-order controller

t 2 , (2  t )

(4.29)

In a system with second-order controller, also known as PD controller, the error is reduced to zero through the following control law,

x ,k  k D e x ,k  k P ex ,k  0 , e

(4.30)

where kP and k D are controller gains. Then using (4.7) and (4.10), the following equation is found,
 k  x  * )  k P ( xk  x* )  k p  x   x  kD ( x x* .

(4.31)

 are calculated as follows, Similar to before, the processes X and X

k 1  x k  tx  , x k 1 . xk 1  xk  tx

(4.32) (4.33)

Applying the same methodology as before and taking (4.31-4.33) into consideration yields,

*  t 2  k  k p t 2 x , xk 1  1  kP t 2 xk  kP t 2 x*  kD t 2 x x*  t 1  kD t  x
k 1  1  k D t  x k  k P txk  k P tx*  k D tx *  tx *  k p t x , x
which yield (assuming the reference input to be constant and error-free),





(4.34) (4.35)

107

 2 2 * k  k p t 2 x  1  k P t xk  k P t x  t 1  k D t  x  cov( xk 1 , xk 1 )  E  k  k p t 2 x   E 1  k P t 2 xk  k P t 2 x*  t 1  k D t  x  











    

2

  E  1  k P t 2   1  k P t 2



   x  E x   t 1 k t   x  E x   k t    
2
2 k k D k k p x

      

(4.36)





2

k , x k ) cov( xk , xk )  t 2 (1  k D t )2 cov( x

k )  k P 2 t 4 2 ,  2t (1  k D t ) 1  k P t 2 cov( xk , x
 1  k t x  k  kP txk  kP tx*  k p t x D   k 1 , x k 1 )  E  cov( x k  k P txk  k P tx*  k p t x  E 1  k D t  x       
2









E

1 k t   x  E x   k t  x  E x   k t  
2

    

(4.37)

D

k

k

P

k

k

p

x

k , x k )   k P t  cov( xk , xk )  (1  k D t ) 2 cov( x k )  k P 2 t 2 2 .  2k P t (1  k D t ) cov( xk , x
2  1  k t x   k  k P txk  k P tx*  k p t x   D     k 1 )  E  cov( xk 1 , x  *   E 1  k t  x k  k P txk  k P tx  k p t x    D     1  k t 2  x  E  x    t 1  k t   x k  E  x k    k p t 2 x P k k D   E  1  k D t   x k  E  x k    k P t  xk  E  xk    k p t x 

2












  

k , x k )  k P t 1  k P t 2 cov( xk , xk )  t 1  k D t  cov( x
2



k )  k P 2 t 3 2 .  1  k D t  1  2k P t 2 cov( xk , x Now assuming, f c1 (t )  cov( xk , xk ) ,
k , x k ) , f c 2 (t )  cov( x k ), f c 3 (t )  cov( xk , x





 

(4.38)



(4.39) (4.40) (4.41)

108

and using (4.22), (4.36)-(4.38) are rewritten as follows,

f c1 

f c1 t  1  k P t 2 t





2

f c1  t 2 (1  k D t )2 f c 2

 2t (1  k D t ) 1  k P t
fc 2 



2

f

(4.42)
4 2

c3

 k P t  ,

2

f c 2 2 t  (1  k D t ) 2 f c 2   k P t  f c1  2k P t (1  k D t ) f c 3  k P 2 t 2 2 . t
fc3 
2 f c 3 t  k P t 1  k P t 2 f c1  t 1  k D t  f 2 t

(4.43)





 1  k D t  1  2k P t



2

f

(4.44)

c3

 k P t  .

2

3 2

The covariance of process X is obtained through solving (4.42)-(4.44) as follows,
 P Dt  2  k D t  k P t 2   2 k 1  (t )  1 e D  .

2k k

f c1

4k D

 

 

(4.45)

It is worth mentioning that such function cannot be calculated through the Ornstein-Uhlenbeck process, discussed in [4.11]. The steady-state error covariance is calculated through (4.42)-(4.44) by assuming,
 f c1  f c 2  f c 3    0, t t t

(4.46)

which yields,

cov( X , X ) ss

2  k D t  kP t 2   4k D

.

(4.47)

The error covariance of closed-loop systems with other controllers is calculated similarly. In the next section the application of the proposed error modeling in visual servoing systems is presented.

109

4.4 Error Modeling in Visual Servoing
The focus of this section is the uncertainty propagation analysis in visual servoing applications. Similar to previous section, the open-loop error modeling is discussed first, followed by the novel closed-loop error modeling developed for classic visual servoing systems, namely IBVS and PBVS.

4.4.1 Open-Loop Approach
This subsection follows the open-loop error propagation in visual servoing systems. First, the image error propagation to camera velocities in an IBVS system is discussed. Next, the propagation of image noise through three different parts of the PBVS system, namely pose estimation, servoing, and proportional control, is investigated. The HVS analysis is similar and could be found in [4.7].
Image-Based Visual Servoing

In image-based visual servoing (IBVS) systems, the image error is directly propagated to the camera velocity. The camera velocity is directly calculated as follows,
^  ( s  s* ) . Vc   J s

(4.48)

^  is the approximate pseudo-inverse of the image Jacobian matrix, J, s and s * are the Here, J

current and desired image features vectors, respectively. Assuming the final feature points to be free of noise, one can define camera velocity noise as follows,

V 
c

Vc s, s

(4.49)

where  V and  s are the camera velocity noise and image noise respectively and,
^ Vc J ^ ,   s ( s  s* )   J s s s

(4.50)

110

In calculation of (4.50), the derivation of inverse Jacobian with respect to image features is calculated as follows,
J s  J   J s s J s . s s

(4.51)

It is interesting to note that when the camera is reaching its desired location, the first term in (4.50) will be negligible, simplifying the error as follows,
^  .  V   J s s

(4.52)

It is noteworthy that the image noise is assumed to be a zero-mean Gaussian random variable.
Position-Based Visual Servoing

Pose estimation is the first part of PBVS to be investigated, which takes image feature points as input and gives the object pose as its output. The optimization-based pose estimation methods are based on minimizing the image plane error,
2 2 c o c c o c          Ro Pi  to Ro Pi  to     L    ui  c o c 1    vi  c o c 2   ,      i 1   Ro Pi  to  3   Ro Pi  to  3         n

(4.53)

where ui and vi are the image plane coordinates of ith feature point, Pi o is the ith point in object
c frame, Roc is the rotation matrix between the object and camera frames, to is the translation from

the object frame to the camera frame, n is the number of feature points, and i is the ith element of the bracketed vector. The error is shown as a function of minimal pose representation (translation plus minimal orientation representation such as Euler angles),
c L  L ( s ,  o ),

(4.54)

c is the minimal pose representation. Then s is the where s is the vector of image features, and o c is its output. The error function is minimized by putting the input of the pose estimation and o

gradient equal to zero. The input and output are assumed to have additional error as before, i.e., 111

^  s  s , . s
c c ^o   o   .
c o

(4.55) (4.56)

Then, the gradient of error is written as,
c c c c ^o ^o ^o ^,  ^,  ^,  ) L ( s )  2 L ( s ) ) L ( s, o  2 L ( s     c . s c c c 2 c o ^o ^o ^ o  o s 

(4.57)

Since the gradient should be equal to zero both for real and estimated gradients, (4.57) is simplified to,
c c ^o ^o ^,  ^,  ) )  2 L ( s  2 L ( s s  c  0 , c 2 c o ^ o s ^o ^  

(4.58)

which yields,
c c   2 L ( s ^o ^o ^,  ^,  )   2 L ( s )  s  GP s .    c 2 c ^o ^ o s ^     1



c o

(4.59)

Then using (4.53), the covariance of the estimated pose is obtainable. The goal in PBVS algorithms is to minimize the pose error between current and desired camera poses. This error is defined by forming homogenous transformation between the current and desired camera poses as follows,

 Rcc* H  0
c c*

tcc*  , 1

(4.60)

where H cc* is the homogenous transformation between the desired camera frame ( c* ) and the current camera frame ( c ), Rcc* and tcc* are relevant rotation matrix and translation vector. In (4.60), H cc* is calculated as follows,

112

 Rc t c   Ro* c Hcc*  Ho Hco*   o o   c  0 1 0

c o tco*   Ro Rc*    1  0

c o c  Ro tc*  to . 1 

(4.61)

The object frame is denoted by "o". A proportional control law is assumed to be used,

 t c*  Vc    c  .   

(4.62)

Here,  is the angle and axis representation of Rcc* and Vc is the camera velocity. It is assumed that the desired pose is error free. Next, the error propagations for rotation and translation are followed separately.

 t   R tco   t  Gt     t .
c c* c o

*

c o

c o

c o

(4.63)

The error of  t is directly obtained from the pose estimation error propagation, while the error
c o

signal  R can be calculated from the error of minimum representation of orientation (Euler
c o

angles) as follows,

 
c o

roc roc roc      c  G   c , c c c o c o o o oc o  o

(4.64)

c where  c is the error of vector representation of rotation matrix, denoted by  o . The Euler
o

angles (roll, pitch, and yaw) are denoted by  ,  , and  , respectively. The error of orientation is calculated as follows,

   G   ,
c c* c o

(4.65)

and then propagated to the angle and axis form [4.7],

   G   .
c c*

(4.66)

Ultimately, the error propagated to camera velocity is obtained as follows, 113

 v   

  t c*   Gt   oc   toc  I c         3 G G     0     oc     

Gt G    toc       Gv oc . G G G     c   o

(4.67)

4.4.2 Closed Loop Approach
Image-based Visual Servoing

In IBVS systems, the error is formed based on current and desired image feature locations. The error is then reduced to zero using a suitable control law, such as a proportional control law. For this matter, at each time step the camera velocity is chosen as,
^k  s * ) , Vc , k    J s , k  ( s

(4.68)

where s* is the vector of desired image features and, ^k  sk   s , k , s (4.69)

is the vector of image features at time step k. The image features at the next time step are updated based on the given camera velocity, sk 1  sk  tJ s ,kVc ,k . Then the covariance of image features in the next time step is calculated as, (4.70)

114

cov( sk 1 , sk 1 )  E  sk 1  E sk 1   sk 1  E sk 1 



T




  I n  tJ s ,k J s , k   sk  tJ s ,k J s , k  s*  tJ s ,k J s ,k  s , k        E  I  tJ J   s  tJ J  s*  tJ J    n s ,k k k s ,k s ,k s ,k s ,k s ,k     E T     I n  tJ s ,k J s , k   sk  tJ s ,k J s , k  s*  tJ s ,k J s ,k  s , k      E  I n  tJ s ,k J s , k   sk  tJ s ,k J s , k  s*  tJ s ,k J s ,k  s , k     

 



  I n  tJ s ,k J s , k    sk  E sk    tJ s ,k J s ,k  s , k   E   I n  tJ s ,k J s , k    sk  E sk    tJ s ,k J s ,k  s , k    I n  tJ s , k J s ,k   cov( sk , sk )  I n  tJ s , k J s ,k     2 t 2 J s ,k J s ,k  s ,k J s , k J s ,k  ,
T

 

 

  T  

(4.71)

where,

 s ,k  diag  s1 2 . .  sn 2 ,
is the matrix of feature error covariance. Now assuming,
cov( sk , sk )  J s ,k J s ,k  Fcs , k J s ,k J s , k  ,





(4.72)

(4.73)

F   cov( sk 1 , sk 1 )  J s ,k J s ,k   Fcs ,k  cs ,k t  J s ,k J s ,k  . t  
One can rewrite (4.71) as follows, F   J s ,k J s ,k   Fcs ,k  cs , k t  J s , k J s , k    2 t 2 J s ,k J s ,k  s , k J s ,k J s ,k  t    1  2t   t
2 2

(4.74)

J

(4.75)


s ,k

J s , k Fcs , k J s ,k J s ,k ,



which yields, Fcs , k t   2 t  s ,k    2 t  2  Fcs , k . (4.76)

115

Similar to (4.25), (4.76) is solved as follows, Fcs ,k 
  2 t  2 t    t    e 1     s ,k . (2  t )  

(4.77)

Replacing (4.77) in (4.73) results in, cov( sk , sk ) 
  2t  2 t    t      e  1   J s ,k J s ,k  s ,k J s ,k J s ,k . (2  t )  

(4.78)

The pose of the camera is updates as follows,

cw,k 1  cw,k  tJ P ,kVc ,k ,
where,
 Rcw  0 0  , T Rcw 
1

(4.79)

J p ,k

(4.80)

 ). and T ( ) is the transformation between the Euler angles and angular velocity (i.e.,   T ( )
Replacing (4.68) in (4.79) yields,
^k  s* ) .  cw, k 1   cw, k   tJ P , k J s , k  ( s

(4.81)

The error covariance of pose in next time step is calculated as follows,

116

cov(cw,k 1 , cw,k 1 )  E cw, k 1  E cw,k 1 cw, k 1  E cw, k 1





  w  tJ J  ( s ^k  s* )  E cw,k  tJ P ,k J s ,k  ( s ^k  s* ) c ,k P ,k s ,k   E  cw,k  tJ P , k J s ,k  ( s ^k  s* )  E cw,k  tJ P ,k J s ,k  ( s ^k  s* )   cw,k  E cw, k   tJ P ,k J s ,k   sk  E sk    tJ P , k J s ,k    E  cw,k  E cw, k   tJ P ,k J s ,k   sk  E sk    tJ P , k J s ,k    cov(cw, k , cw, k )   2 t 2 J P , k J s ,k  cov( sk , sk ) J s ,k T J P ,k T   2 t 2 J P , k J s ,k  s ,k J s , k T J P , k T  tJ P ,k J s , k  cov( s, cw,k ) t cov(cw,k , s ) J s ,k T J P ,k T .

 
 


T

 

               
T s T s

(4.82)

The last two terms in (4.82) are obtained as follows,

cov( sk 1 , cw, k 1 )  E  sk 1  E sk 1  cw,k 1  E cw,k 1






T

  I n  tJ s ,k J s , k   sk  tJ s , k J s ,k  s*  tJ s , k J s ,k  s , k        E  I  tJ J   s  tJ J  s*  tJ J    n s ,k s ,k k s,k s ,k s,k s ,k s ,k  E     T  cw, k  tJ P ,k J s , k  ( s ^k  s* )  ^k  s* )  E cw, k  tJ P ,k J s , k  ( s     I n  tJ s ,k J s , k    sk  E sk    tJ s ,k J s , k  s ,k     E T  cw,k  E cw,k   tJ P ,k J s , k   sk  E sk    tJ P ,k J s ,k  s   




 



 



(4.83)



  I n  tJ s ,k J s , k   cov( sk , cw,k )  2 t 2 J s , k J s ,k  s ,k J s , k T J P , k T .

t  I n  tJ s , k J s ,k   cov( sk , sk ) J s , k T J P , k T

Now assuming,
cov( sk , cw,k )  J s , k J s , k Gc ,k J s ,k T J P ,k T ,

(4.84)

the covariance of pose and image features is calculated as follows,

117

G   J s , k J s ,k   Gc ,k  c , k t  J s ,k T J P , k T   I n  tJ s ,k J s , k   J s ,k J s ,k Gc , k J s ,k T J P ,k T t     2 t  2  t  2 t 2 1  t       T T    1  e  J s ,k J s ,k  s ,k J s ,k J P ,k (2  t )   2 2  T T   t J s ,k J s , k  s ,k J s ,k J P ,k , (4.85) which yields,

Gc ,k 

Gc , k t



  2 t  2 t     3t 2   2 t  2   1  e   s ,k   t  s ,k . (2  t )  

(4.86)

Then the sought covariance is found by solving (4.86), cov( sk , cw, k ) 
  2 t  2 t    t   T T   1  e  J s ,k J s ,k  s ,k J s ,k J P ,k . (2  t )  

(4.87)

It is easy to show that,
  2 t  2 t    t   T T   cov( , sk )  cov( sk ,  )  1  e  J P ,k J s , k  s ,k J s , k J s , k (4.88) (2  t )   w c,k w T c,k

Now replacing (4.78), (4.87), and (4.88) in (4.82) results in,
cov(cw,k 1 , cw, k 1 )  cov(cw, k , cw, k ) 
  2 t  2  t     3t 3  T  T   1  e  J P ,k J s ,k  s ,k J s ,k J P ,k (2  t )     2 t  2  t     2 t 2  T  T   1  e  J P ,k J k  s ,k J k J P ,k (2  t )  

  2 t 2 J P , k J k  s , k J k T J P ,k T  

(4.89)

  2 t  2  t     2 t 2  T  T   1  e  J P ,k J s ,k  s ,k J s ,k J P ,k (2  t )  

 cov( ,  )   t e
w c ,k w c ,k 2

  2 t  2  t  2   

J P ,k J s ,k  s ,k J s ,k T J P ,k T .

Similar to previous cases, (4.89) is solved by assuming,

118

cov(cw, k , cw,k )  J P , k J s ,k  M c , k J s ,k T J P , k T ,

(4.90)

as follows,
M c , k t   2 te
  2 t  2   t   

 s ,k ,

(4.91)

which is solved through integration, knowing that M c ,0  0 ,
  2 t  2  t    t  (1  e ) s , k . 2   t

M c ,k 

(4.92)

Therefore, the pose error covariance is formulated as follows,
cov(cw, k , cw,k ) 
  2 t  2   t    t  (1  e ) J P , k J s , k   s , k J s , k T J P , k T . 2   t

(4.93)

The velocity error covariance is found through (4.68), i.e.,

  J  ( s ^k  s* )  E  J s ,k  ( s ^k  s* ) s ,k  cov(Vc ,k , Vc ,k )  E    J s , k  ( s ^k  s* )  E  J s ,k  ( s ^k  s* )   E  J s ,k   sk  E sk     J s ,k  s ,k



 

 

 

  T  
 k

   J  s
s ,k

 E sk     J s ,k  s ,k


T

(4.94)

  2 J s ,k  cov( sk , sk ) J s ,k T   2 J s ,k  s ,k J s ,k T ,
and replacing (4.78) in (4.94),
  2 t  2 t   3 t       cov(Vc ,k ,Vc , k )   1  e   2  J s ,k  s , k J s ,k T .    (2  t )     

(4.95)

Position-based Visual Servoing

In PBVS, the camera velocity is obtained based on pose difference between current and desire pose, as was stated in (4.62). The pose is then updated based on the obtained camera velocity, similar to (4.79). If the pose is defined as follows, 119

 tc c

*

  , 

(4.96)

then the desired pose is equal to zero (i.e.,  *  0 ). This pose is updated with the camera velocity,
^ k  (1  t )k   t  , k 1  k   t
k

where,

   Gv    G v G P  s ,
k c o

(4.97)

similar to (4.67). The covariance of the pose is calculated as follows,
cov(k 1 , k 1 )  E k 1  E k 1  k 1  E k 1 



T


 
  T  

 (1  t )  t  E (1  t )  t k k k k   E  (1  t )k  t k  E (1  t )k  t k   E (1  t ) k  E k    t k



 

 

(4.98)

  (1  t ) 

k

 E k    t k


T

 (1  t ) 2 cov(k , k )   2 t 2Gv GP  s ,k GPT GvT ,

which is solved similar to (4.71), i.e,
  2 t  2 t    t  T T   cov(k , k )  1  e  Gv GP  s , k GP Gv . (2  t )  

(4.99)

Finally, it is worth mentioning that error covariance of other pose definitions (e.g., cw ) is easily obtainable from the error of the pose defined in (4.96),
  ,    G  G    G  G
c o w c

(4.100) (4.101)

 t  Rcw  .
w c
*

c tc

*

120

4.5 Simulation Results
The efficiency of the proposed methods for error covariance calculation is verified through multiple simulations. These simulations have been carried under MATLABÂ® 2011b environment from Mathworks (Natick, MA, USA). The simulations are based on Monte Carlo simulations with large number of samples (10,000 samples). In these simulations, the image noise covariance is assumed to be known. The closed-loop error covariance is calculated based on this image noise covariance. In the first simulation, the validity of the proposed method for a closed-loop SISO system is evaluated. Two systems with proportional and second-order controllers are simulated by adding Gaussian noise to the input process (x) with the covariance of 0.01. A time step of 1 ms is considered for these systems. The resultant error covariance of the process is compared with the predictions, formulated in this chapter. Figures 4.2 and 4.3 show the results of these comparisons. As it can be seen, the predictions closely match the simulation results in the case of proportional controller, while the predictions from [4.11] are only approximating the results. The results also match closely for the second-order system, where the previous method was not able to make any predictions. It should be noted that the stability of the controllers are not of concern in this work and they are assumed to be stable (by selecting proper gains).

121

6

x 10

-6

5

4 X covariance

3

Actual uncertainty covariance Discrete-time uncertainty covariance estimation Continuous-time uncertainty covariance estimation

2

1

0 0

0.1

0.2

0.3

0.4

0.5 0.6 Time (s)

0.7

0.8

0.9

1

Figure 4.2 First simulation: Process error covariance using a proportional controller.

122

6

x 10

-7

5 Actual uncertainty covariance Discrete-time uncertainty covariance estimation

4 X covariance

3

2

1

0

0

1

2

3

4

5 6 Time (ms)

7

8

9

10

Figure 4.3 First simulation: Process error covariance using proportional derivative (secondorder) controller. In the second simulation, a closed-loop IBVS is tested to verify the efficiency of the proposed method for an IBVS system. An IBVS system with proportional controller is simulated in MATLABÂ® 2011b environment. The camera is servoed from a distance to half a meter above the object of interest. Four coplanar feature points have been exploited for this purpose. Figures 4.44.6 show the simulation results in image space, in Cartesian space, and for camera velocities. In image space, u and v are the image coordinates in pixel. The initial image is shown by dashed lines and the initial points are shown by circles. The final feature points are shown by crosses. The trajectories of the feature points are shown by solid lines. In Cartesian space, the orientations of the camera are shown by their frame coordinates for its initial and final poses and the camera trajectory is shown by a solid curve. The closed-loop image features error covariance is then obtained based on (4.78) and are compared to those from the simulations. The diagonal elements 123

of the error covariance matrices are selected for sake of brevity. The result of this comparison is shown in Figure 4.7. As it is apparent, there is a close match between the simulations and the predictions throughout the simulation period. Similarly, the error covariance of the camera poses with respect to the object and the error covariance of the camera velocities are calculated and compared with their predictions through (4.93) and (4.95). Once again, the diagonal elements of the error covariance matrices are chosen. The outcomes of these comparisons are presented in Figures 4.8 and 4.9. Once again, the close estimation of the error covariance for camera pose and velocity is observed.

200

150 v (pixel) 100 50

100

150 u (pixel)

200

250

Figure 4.4 Second simulation: Simulated IBVS method in image space.

124

-0.6 z (m) -0.4 -0.2 0 0.1 0 -0.1 y (m) -0.2 0 -0.1 x (m) 0.1

Figure 4.5 Second simulation: Simulated IBVS method in Cartesian space.

125

0.8 Vx (m/s) 0.6 Camera Velocity 0.4 0.2 0 -0.2 0 Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

1

2 Time (s)

3

4

Figure 4.6 Second simulation: Camera velocities in simulated IBVS method.

126

2 1.5 Cov(u1) 1 0.5

x 10

-8

2 1.5 Cov(v1) 1 0.5

x 10

-8

Actual Error Covariance Error Covariance Estimation

0 0 2 1.5 Cov(u2) 1 0.5 0 0 x 10
-8

1

2 time (s)

3

4

0 0 2 1.5 Cov(v2) 1 0.5 0 0 x 10
-8

1

2 time (s)

3

4

1

2 time (s)

3

4

1

2 time (s)

3

4

2 1.5 Cov(u3) 1 0.5

x 10

-8

2 1.5 Cov(v3) 1 0.5

x 10

-8

0 0 2 1.5 Cov(u4) 1 0.5 0 0 x 10
-8

1

2 time (s)

3

4

0 0 2 1.5 Cov(v4) 1 0.5 0 0 x 10
-8

1

2 time (s)

3

4

1

2 time (s)

3

4

1

2 time (s)

3

4

Figure 4.7 Second simulation: Image features error covariance in comparison with its estimation.

127

8 6 cov(Vx)

x 10

-3

8 6 cov(Vy) 4 2 2 time (s) 4

x 10

-3

1 cov(Vz)

x 10

-3

Actual Error Covariance Error Covariance Estimation

4 2 0 0

0.5

0 0 0.03

2 time (s)

4

0 0 8 6 cov(V) 4 2 0 0 x 10
-4

2 time (s)

4

0.03 cov(V) cov(V) 2 time (s) 4 0.02 0.01 0 0

0.02 0.01 0 0

2 time (s)

4

2 time (s)

4

Figure 4.8 Second simulation: Camera velocity error covariance in comparison with its estimation.
1.5 1 0.5 0 0 8 6 cov() 4 2 0 0 2 time (s) 4 cov() x 10
-7

x 10

-5

1.5 1 0.5

x 10

-5

6 4 2

x 10

-7

Actual Error Covariance Error Covariance Estimation

cov(x)

cov(y)

2 time (s)

4

0 0 4 3 2 1 0 0 x 10
-5

cov(z) 2 time (s) 4

0 0 4 3 cov( ) 2 1 x 10
-5

2 time (s)

4

2 time (s)

4

0 0

2 time (s)

4

Figure 4.9 Second simulation: Camera pose error covariance in comparison with its estimation.

4.6 Summary
In this chapter, the uncertainty modeling in visual servoing systems was developed and a novel method was introduced to model the closed-loop uncertainty in visual servoing systems. The proposed method accounted for discrete-time nature of visual servoing systems and could entail more control laws than previous works. The effectiveness of the method was then proven by 128

simulations. The method was applied to IBVS and PBVS systems and the error covariance of image features, camera velocities, and camera pose were found. The simulation results were presented to certify the accuracy of error covariance estimations. As was shown, there was a good match between the actual error covariance and its estimation. The development of uncertainty models can help the designer of visual servoing systems to have a good understanding of the effect of uncertainties on different parts of the system and improve the systems accuracy and robustness with respect to this knowledge. In next chapter, robust controllers based on the developed error modeling are presented to enhance the accuracy of the system and add to its robustness.

129

Chapter 5 Robust and Constrained Visual Servo Control
5.1 Introduction
The practicality of any RVS approach for real world applications highly depends on its robust and accurate performance in presence of system uncertainties and constraints. The limitations imposed by the robot or the camera are usually known a priori and therefore can be avoided in RVS systems. However, system uncertainties are usually unknown and cannot be measured. Thus, robust schemes are required to alleviate the effect of uncertainties on system performance. Previously, path planning methods and image-based predictive control (IBPC) schemes were proposed to address the constraints in RVS systems. However, IBPC had only local stability and its numerical feasibility was not certified. Moreover, those methods were prone to system uncertainties. Robust methods were previously proposed as a remedy to this problem. Yet, most of those robust methods focused on robust convergence of the system and accuracy of the system was not considered. In addition, those approaches were either deterministic and/or conservative (e.g., path planning), or negligent of system constraints (e.g., robust controllers). In this chapter, a set of controllers are introduced to address the aforementioned shortcomings. In particular, position-based predictive control (PBPC) and hybrid predictive control (HPC) methods are proposed as alternative solutions to IBPC. These controllers can manage the system constraints properly, while providing the system with global stability and flexibility in the design. The feasibility of the proposed controllers is improved by minimizing their computational cost and the subsequent stability issues are compensated through proper modifications of the cost function and the constraints. In addition, a two-step control scheme is proposed for robust control. By using this scheme, the rate of convergence is decoupled from uncertainty measures (namely mean and covariance of the error). Thus unlike previous controllers, the effect of uncertainties may be minimized without affecting the convergence rate. This methodology is then exploited in conjunction with the developed uncertainty propagation 130

model and predictive controllers to robustly guide the robot towards its destination, without any constraint violation. The chapter is organized as follows. First, a survey of previous works is presented in Sec. 5.2 and most related works are reviewed. The design of PBPC and HPC are explained in Sec. 5.3. The two-stage robust control design is developed in Sec. 5.4 and is exploited towards robust and constraint-aware controllers with model predictive structures. The simulation and experimental results are brought after in Sec. 5.5 and the chapter is concluded with a discussion over the theoretical and experimental results in Sec. 5.6.

5.2 Literature Survey
Successful RVS necessitates effective constraint handling in the system. Various methods were previously proposed to deal with the constraints. One effective way to manage the system's constraints was to employ path planning techniques [5.1-5.3]. These methods formed image space constraint-aware trajectories prior to servoing, which were then tracked closely to reach the desired pose. Global stability was obtained through this method. Several techniques were exploited to achieve the constraint-aware trajectories. Potential field method, once popular in collision-free path planning, was employed for path planning [5.1]. Optimization-based techniques were alternatively proposed to generate robust trajectories [5.3]. Nevertheless, such techniques could not cope with the uncertainties of the system. On a separate line, constrained controllers were employed to deal with the system limitations during the servoing. Specifically, model predictive controllers (MPC) were employed for this matter. These controllers were capable of system output optimization, while considering the limitations of the system. Model predictive control was previously employed for visual servoing. One of the early applications of MPC for visual servoing was proposed in [5.4], where a generalized predictive control (GPC) form of MPC was employed in image space. The method was numerically stable and implementable for real-time applications, while providing optimal results. Yet, this work was not concerned with constraint handling. Later, the camera and robot constraints were considered in the optimisation of the MPC to address the shortcomings of the classical IBVS [5.5]. A more general constrained image-based MPC was introduced in [5.6] with a different cost 131

function. It was shown that the proposed MPC was capable of handling the constraints, yet its stability and numerical feasibility were questionable. Similar image-based MPCs were employed by other works such as [5.7]. The reference trajectory used in MPC was modified in [5.8] to ensure the stability of the controller. In addition, a collision-free visual servoing was practiced via MPC in [5.9], which used collision-free planning techniques (such as probabilistic road map) for this matter. This technique was later used as a part of robot programming scheme in cluttered workspaces [5.10]. Moreover, a MPC which exploited image moments was proposed to improve the speed and stability of image-based MPC controllers in visual servoing systems. Similarly, a quasi min-max MPC controller was developed for visual servoing which modeled the system as a polytopic linear time-varying (LTV) system [5.11]. This controller could handle the system constraints by constrained optimizations carried through LMI. Yet, many of these controllers suffered from local stability and numerical feasibility problems. Moreover, the effect of system uncertainties was not considered in any of these works. Robust RVS was followed in many of the previous works as a remedy to problems caused by system uncertainties. Many of these works focused on uncertain parameters of camera's calibration and robot's dynamics. An adaptive technique was employed to estimate robot's parameters online [5.12]. A robust control scheme was proposed by [5.13] and its stability in presence of camera/robot's uncertainties was proven via Lyapunov method. In addition, robustness with respect to calibration errors in terms of stability and error boundness was achieved in [5.14]. Separate compensators were designed in [5.15] for robot and camera uncertain parameters. A quaternion-based visual servoing was proposed by [5.16] to provide robustness against camera calibration parameters. Moreover, a controller robust to robot's dynamics parameter uncertainties was proposed for a 2 DOF robot in [5.17]. Robot's parametric uncertainties were considered in [5.18]. The robustness to visual servoing parametric uncertainties was also provided in [5.19] through LMI optimisation. Time-varying uncertainties of the robot was considered in [5.20] via an adaptive controller, which used a function approximation technique. A PBVS scheme robust to camera parameter uncertainties was proposed in [5.21] which had the benefit of global stability. In addition to that, various control schemes were proposed to counter the effects of uncertainties in the system. Sliding-mode control techniques were practiced in a couple of works including [5.22-5.25]. Robust filter-based techniques were employed to reduce the uncertainties of the system [5.26, 5.27]. Optimization 132

techniques based on LMI were also introduced in [5.28, 5.29]. The inherent robustness of LQG and optimal controllers were exploited in [5.30], [5.31]. Yet, most of these methods were not concerned with the constraints of the system. In addition, the accuracy of the system was usually not the main concern. This work proposed several control schemes that can handle the constraints of the system properly, offer global stability and improved numerical feasibility, and are robust against the uncertainties of the system. The details of these controllers are presented in the sequel.

5.3 Predictive Controllers for Constrained Visual Servoing
The design methodology of robust and constrained visual servo controllers is the topic of this section. The required knowledge of the system is presented first, followed by constrained and robust control schemes for visual servoing.

5.3.1 Preliminaries
In this subsection, the structure of the system, the definitions and assumptions required by the proposed control designs are discussed as follows.
System Configuration

Similar to previous chapters, this chapter assumes a system composed of a six degree-of-freedom manipulator and a camera which is mounted on the end-effector of the manipulator (i.e., eye-inhand camera configuration). The camera initially has the object of interest in its FOV. The camera images of the object are processed during the servoing and the image features are extracted. In this thesis, point image features are of interest. The image plane coordinates of image points are used as image features,

sk   u1,k

v1,k

. . un,k

vn,k   ,

T

(5.1)

where n is the number of point features, sk is the vector of image features at time k and,

133

 ui ,k

c  Xo ,i , k  vi ,k  f  c    Z o ,i , k

Yoc,i ,k  , c Zo ,i , k  

(5.2)

are the image coordinates of ith point of the object in the camera frame (i.e.,
c Poc,i ,k    X o ,i , k

Yoc,i ,k

c  Zo ,i , k  ), projected on the image plane. In (5.2), the camera's focal length is

denoted by f. Image features are indirectly connected to the relative pose of the object with respect to the camera,
c o c Poc,i ,k  Ro ,k P i  to , k ,

(5.3)

c c where Ro ,k , and to , k are the rotation and translation between the object and camera frame, o o respectively. Here, Pi o    X i Yi

th Zio   is the i point of the object in object frame and is

supposed to be known through the object's geometry. Both image features and pose parameters may be used for servoing, as is explained in the sequel.
System Servoing Errors

The image features may be directly used for control purposes (e.g., in IBVS methods). In such scenarios, the image features are compared with the features at the desired pose,
* * * * s*   u1 v1 . . un vn   ,

T

(5.4)

and the image space error at time step k is formed as,
es , k  sk  s* .

(5.5)

It is assumed that the robot will reach its desired pose once this error is reduced to zero. Yet, this assumption may not always hold due to local minima of the system. Alternatively, image features may be used to estimate the pose of the object with respect to the camera, which is represented by the transformation matrix between the object and camera frame,

134

c  Ro c ,k  Ho  ,k  0

c  to ,k . 1

(5.6)

The pose of the object with respect to the camera may be estimated accurately through iterative methods (as was discussed in chapter 2). The pose error is then defined based on the transformation between the current and the desired camera frames,
c H cc,k  H o  Hoc,k  .
* *

1

(5.7)

Three different pose errors are defined based on this transformation. The translation error is defined as follows, et ,k  tcc,k ,
* *

(5.8)

where tcc, k is the translation between the current and the desired camera frame. As for the orientation error, two different choices are taken. In the first orientation error, the angle and axis representation of the rotation matrix between the current and desired camera frames ( Rcc,k ) is exploited, eR1 ,k   cc,kcc, k ,
* * * * *

*

(5.9)

where cc, k and  cc,k are the angle and axis related to the Rcc,k , respectively. The second orientation error is composed of the Euler angles, representing the rotation matrix, Rcc,k ,
eR2 , k  cc, k   cc, k 
* *

*

 cc,k  cc, k  ,
* *

T



(5.10)
*

where cc,k ,  cc,k , and  cc,k are the roll, pitch, and yaw angles related to the rotation matrix, Rcc,k . The third error is defined as the logarithm of depth difference for each feature point between the current and desired camera frames,

*

*

*

135

c c* c c* ez , k   log Z o ,1, k  log Z o ,1 . . log Z o ,i , n  log Z o , n  ,

(5.11)

where Z oc,*i is the depth of ith point of the object in the desired camera frame ( Poc,* i ). As it is apparent, the camera will reach its desired pose once the first and second introduced pose errors are reduced to zero. The third pose error may be used as a complement to enhance the stability of the system [5.32]. In the sequel, it will be shown how a combination of image space and pose errors may be used in visual servoing to make the robot reach its desired pose.
System Dynamics

Each time the controller generates the camera velocity, the robot is moved in compliance. As a result, the pose of the object with respect to the camera and subsequently the pose errors are updated as, et ,k 1  et ,k  tRcc, kc ,k , eR ,k 1  eR ,k  tJ ,k c ,k , ez ,k 1  ez ,k  tJ z ,kVc ,k .
T
*

(5.12) (5.13) (5.14)

T T Here, Vc ,k   c ,k  c ,k   is the camera velocity in camera frame,  t is the sampling time, J ,k

and J z ,k are the angle and depth Jacobian, defined in [5.32]. Similarly, the features error is updated as follows, es ,k 1  es , k  tJ s ,kVc , k , (5.15)

where J s ,k is the image Jacobian, defined in [5.33]. The error update formulation, shown in (5.12) to (5.15), is used in constraint optimization of predictive controllers, which are the focus of the coming subsections.

136

The control goal in visual servoing is to reduce the error of the system, whether in image space or Cartesian space, to zero. Traditionally, a simple proportional controller was employed to guarantee exponential decree of the error,

G   eG , e

(5.16)

where eG is the general error term and  is the controller gain. However such controller could not handle the robot/camera constraints due to its simplicity. The design of MPC to handle the constraints is the topic of this subsection. It will be shown that the errors introduced in the previous subsection are reduced to zero without violation of the constraints. Image-based, position-based, and hybrid controllers are discussed separately and the constraints of the system are explained afterwards.

5.3.2 Image-Based Predictive Control
This controller was introduced in [5.6] and is composed of an optimization problem solver which finds the appropriate control to minimize the cost function defined as follows,
Ls ,k   es ,k iT  s es , k i ,
i 1 Nc

(5.17)

where Bs is the image error weighting matrix and N c is the control horizon. As was discussed in [5.6], the choice of N c is not trivial. Large values for N c result in increase of computation time, which makes this controller not suitable for real-time and fast applications, such as visual servoing. On the other hand, small values of N c cause discontinuities in the control and instability. In this work, N c is chosen as unity to minimize the computational time. Then the problem of control discontinuities caused by this selection is solved by modifying the cost function as,
Ls ,k  es ,k 1T  s es , k 1  Vc ,k T Vc ,k ,

(5.18)

where Vc ,k and  are the camera velocity and its related weighting matrix, respectively. This choice of cost function reduces the discontinuities of the control as it penalizes large control 137

actions. The stability of the system is ensured through an additional constraint, which is explained in the sequel. The control actions are found by minimizing the cost, subject to system constraints,
Vc ,k  arg min Ls ,k
Vc ,k

(5.19)
sk  S Vc ,k  ,

s.t. es , k 1  es ,k  tJ s , kVc ,k

where S is the set of admissible image features,  is the set of acceptable camera velocities, and J s ,k is the image Jacobian at time step k, respectively. As can be seen, the first constraint of the system is the linearized model of the system, while the other conditions set boundaries on future features and system controls. The velocity of the camera is then found by solving this optimization problem and is applied to the robot's end-effector.

5.3.3 Position-Based Predictive Control
This controller, proposed by this work, is similar to its image-based counterpart in optimization and the constraints. In this controller, the reduction of the position error in the presence of constraints is considered. As a result, the cost function is defined as,
L p , k  e p , k 1T  p e p , k 1  Vc ,k T Vc , k ,

(5.20)

where  p is the error weighting matrix. The position error is defined as,
T ep,k   et ,k T  . eR 2 ,k  T

(5.21)

The control action is found similar to image-based MPC,
Vc , k  arg min L p ,k
Vc ,k

(5.22)
sk  S Vc ,k   ,

s.t. e p , k 1  e p ,k  tJ p ,kVc ,k

where J p ,i is the position Jacobian [5.34] at time step i and is defined as,

138

0   I3 J p ,i   . c* 1 0 T ( )    c  
*

(5.23)

Here I3 is the 3 by 3 identity matrix and T (cc ) is the transformation matrix between angular velocities and time derivatives of Euler angles,
c . cc  T (cc ) c
* * *

(5.24)

As can be inferred from (5.20) and (5.22), this is a MPC with unity control horizon, which has set penalties on the control to prevent discontinuities in the output. Setting the control horizon to unity has the advantage of minimal computational cost, as was discussed before. The velocity of the camera is obtained by solving (5.22) online and is then applied to the robot for servoing. The constraints of the system are explained in the sequel.

5.3.4 Hybrid Predictive Control
Image-based predictive controllers were proposed previously to reduce the image space error, subject to system constraints. However, image-based algorithms were prone to local stability as was discussed in [5.6]. Hybrid predictive controllers are proposed to solve this problem. In such controllers, a suitable combination of image space and pose errors are selected as the error of the system and is reduced to zero using the optimization techniques. For this reason, the cost function of the system is defined as follows,
T T LH ,k  eH , k 1 H eH , k 1  Vc , k Vc , k ,

(5.25)

where eH ,k 1 is the hybrid error at time step k+1, Vc , k is the camera error at time step k, and  H is the error weighting matrix. Such cost function set penalties on current camera velocity and the error of next time step, which is related to the camera velocity through (5.12) to (5.15). A suitable selection of control weighting matrix (  ) is important as it affect the smoothness and convergence of the system. It is also worth mentioning that the weighting matrices in (5.25) are in close relationship as they determine the contribution of error and control in the cost function. The camera velocity is found by minimizing the cost function, subject to system constraints, 139

Vc , k  arg min LH , k
Vc ,k

s.t

sk 1  S Vc , k  ,

(5.26)

where S and  are the sets of permissible image features and camera velocities, respectively. The choice of cost function in this minimization makes this controller equivalent to a model predictive controller with unity control horizon. Short control horizon in this controller helps its online solution to be feasible. As a result, the stability of this controller is not guaranteed. Moreover, the controller may result in severe control discontinuities as a result of fast control actions. This work proposes an additional constraint to guarantee the stability of the system, while introducing the penalties on control action (i.e., the camera velocities) to prevent erratic control decisions, caused by short control horizon. The control penalties are explained in (5.25), whereas the stability constraint is discussed later, along with the other constraints. The hybrid error selection for the cost function has an important impact on both image space and Cartesian trajectories. This work proposes two hybrid controllers with different error definitions for cost minimization. The image error is common between these hybrid errors, while the pose errors are selective. The first hybrid error is defined similar to hybrid visual servoing controller proposed in [5.32],
T eH1 ,k   es ,k T eR ,k

 eT z ,k  .

T

(5.27)

This selection of hybrid error renders the orientation control to orientation error, while the translational motion is directed by the image and depth error minimization. As a result, both image space and Cartesian space trajectories are smooth. The error weighting matrix is chosen as follows,

 H1

s  0  0

0 R 0

0 0 , z  

(5.28)

which changes the cost function in (5.25) to,
T T T T LH1 ,k  es , k 1 s es , k 1  eR , k 1 R eR , k 1  ez , k 1 z ez , k 1  Vc , k Vc , k .

(5.29)

140

It is worth mentioning that an equal weighting between different error in (5.29) results in a balanced motion, while priorities might be given to a specific error, depending on the servoing scenario. Alternatively, the second hybrid error is defined as,
T eH 2 ,k   es ,k T eR ,k

etT,k   ,

T

(5.30)

which is a combination of image and pose errors, each being used in IBVS and PBVS, respectively. The weighting matrix is selected as,

H2

 s  0  0

0 R 0

0 0 , t  

(5.31)

which results in a cost function as follows,
T T T T LH 2 , k  es , k 1 s es , k 1  eR , k 1 R eR , k 1  et , k 1t et , k 1  Vc , k Vc , k .

(5.32)

As it can be inferred from (5.32), this choice of hybrid error provides an interesting tool to compromise between the error reduction in image space and Cartesian space. The controller will became purely image-based by choosing  R  t  0 , while a totally position-based control is experienced by selecting s  0 . Therefore, these controllers are subsets of the second hybrid controller. It is worth mentioning that the weighting matrices may be selected to minimize the chance of constraint violation, which is handled by the predictive controller. The system constraints are explained as follows.

5.3.5 System Constraints
The constraints of the system are categorized into three groups and are discussed as follows. In order to have a continuous feedback from the camera, the image features of the object should be visible at all times. This means that the image features should not leave a bounded area in the image space. Such constraint is considered in the admissible image feature's set, 141

S  s | smin  s  smax  ,

(5.33)

where smin and smax are the minimum and maximum allowable values for image features. The workspace of the robot is limited and the velocity at which it can move is also restricted. Therefore the velocity of the camera should be limited so that the optimization solution results in feasible robot motion. These constraints are incorporated in the acceptable camera velocity set as follows,
r ,min  J R  RcbVc  q r ,max   V | qr ,min  qr  tJ R  RcbVc  qr ,max  ,   V | q

(5.34)

 r are the robot's joint angles and velocities, respectively. The robot Jacobian is where qr and q

denoted by J R and the rotation between camera and robot's base is shown by Rcb .The first set in (5.34) is based on the limitation of joint velocities and the second set implies the joint limits. Alternately, the set of admissible camera velocities may be defined as,   Vc , k | Vc ,min  Vc ,k  Vc ,max   Vc | min  co,k  tRco,k J p ,kVc , k  max  . The new set definition is based on camera position and velocity limits. In order to enforce the stability of the system, the energy of the error signal is bound by the previous energy of the system to ensure the convergence,
eG ,k 1T eG , k 1  eG , k T eG ,k ,

(5.35)

(5.36)

where  is a positive definite matrix. Such constraint may be set by amending the velocity constraint,

V | q V | V

r ,min  J R  RcbVc  q r ,max     V | q
r ,min T

c

T T T J pT eG ,k  eG , k J pVc  tV J p J pVc  0 ,

 qr  tJ R  RcbVc  qr ,max  

(5.37)

where the stability constraint is included as the third set or alternatively, 142

V | P  P  tR J V  P   V | V J Pe  e PJV  tV J PJV  0.
c min o c ,k k o c ,k k c,k max T T T k T T

  Vc ,k | Vc ,min  Vc ,k  Vc ,max  

(5.38)

The error term in (5.37) and (5.38) may be selected as the pose error ( e p , k ) or either of the hybrid errors ( eH ,k ). Enforcing the complementary stability constraint causes the optimal solver to reduce the error constantly and therefore the convergence of the system is guaranteed. The design of the constrained MPC controllers is now complete. The effectiveness of these methods is verified through multiple simulations in Sec. 5.5.

5.4 Two-Stage Robust Control Design
Robust control schemes are the focus of this section. A two-stage control scheme is introduced for this purpose. First, it will be shown that this controller has the ability of decoupling the mean and covariance of the propagated error from the convergence rate. Hence, the mean and covariance of the errors may be reduced without sacrificing the convergence rate. Second, the discussed MPC is employed at either of control stages to enable the system of constraint management. As a result, two robust schemes are proposed and the unique properties of each are highlighted.

5.4.1 Decoupled Controller
It was shown in chapter 4 that the traditional single degree of freedom (DOF) P type controller is not sufficient to reduce the propagated error without sacrificing the speed. As a remedy, a two degree of freedom (DOF) controller is proposed in this section that minimizes the rate of error growth, while maintaining the servoing convergence rate. This controller decouples the adjustment of the mean and the covariance of image features through a two stage control scheme. An offline controller is employed to maintain the convergence of the system, while an online ancillary controller is introduced to minimize the effects of propagated uncertainties. As a result, the online controller is needless of depth estimation and inverse Jacobian calculation, leading to a faster controller. A novel control structure is proposed to decouple the dependency of mean and covariance of the features. For this purpose, a two DOF controller is proposed. In this scheme, an 143

offline controller is employed as the base for visual servoing. Initially the feature trajectories are formed offline through a proportional controller, starting from the initial features ( s0  s0 ),
k  1 ( sk  s* ) , s k , sk 1  sk  ts

(5.39) (5.40)

k are the offline features and their time derivatives. where 1 is the offline controller gain, sk and s

The important feature of this offline controller is its deterministic nature. This character comes as a consequence of offline computation, based on initial and desired features. This may be considered as an open-loop controller. The features are then updated based on (5.39) and (5.40), sk 1  (1  1t ) sk  1ts* . (5.41)

Next, an online (closed-loop) controller is proposed to keep the features close to the determined trajectory. For this purpose, the time derivative of online features is calculated as,
  2 ( sk  sk ) , k  s s

(5.42)

where 2 is the online controller gain. As can be seen from (5.42), the feature update has two parts to it. The first part is the pre-computed feature update from the aforementioned open-loop controller. The second part is similar to typical IBVS controller having the pre-computed features ( sk ) as its desired features. By employing the integration on the feature vector,
^ ,  sk 1  sk  ts k

(5.43)

the trajectory of the features are updated online as follows,
sk 1  (1  2 t ) sk  (2  1 )tsk  1ts*  tm  2 t s ,k ,

(5.44)

where m is the deterministic error caused by calibration and is assumed to be constant for simplicity. It is worth mentioning that a choice of 1  2 reduces the controller to the conventional proportional controller, discussed in previous section. Having (5.44) ready, the 144

mean and covariance of features are calculated in the sequel. It should be reminded that the precomputed features ( sk ) are deterministic and therefore have zero covariance. The mean of features are calculated by applying the expectation function to (5.44),  sk 1  (1  2 t ) sk   (2  1 )t  sk   1ts*  tm. The mean of the pre-computed features is computed as follows, (5.45)

 sk   s0  s* exp(1k t )  s* .
Now the mean of features is found with the same approach. It is assumed that, g m (k t )   sk  ,  sk 1  g m (k t )  which lead to, g m  2 g m (k t )  2 s*  m  (2  1 ) s0  s* exp(1k t ) . t g m t , t





(5.46)

(5.47)

(5.48)







(5.49)

The solution to this partial differential equation is given by,
g m ( k t )  s0  s* exp(1k t )  s* 





m

2

1  exp(2 k t )  .

(5.50)

The achieved mean is very similar to mean of the simple proportional controller, calculated in chapter 3. The only difference is the separate control of the convergence rate and the steady-state error by the offline and online gains, respectively. By definition, the covariance of the features are expressed as,
g c (k t )  cov sk  ,

(5.51)

145

cov sk 1  g c ( k t )  By exploiting (5.44), (5.51), and (5.52), one can show,
g c (k t ) 

g c t . t

(5.52)

g c 2 t  1  2 t  g c (k t )  2 2 t 2 2 , t

(5.53)

which has a solution as follows,
 1  exp  2   2 t  k t 2 2 g c ( k t )    2  2 t 



   t
 
2

2

.

(5.54)

As can be seen, the convergence rate and final value of the covariance of the features in the new controller depends only on the gain of the online controller. Therefore, the proposed controller has the advantage of reducing the covariance and the rate of its growth, while keeping the original convergence rate of the mean trajectories. Based on the proposed scheme, an IBVS controller has been developed. In such a system, the velocity of the camera is related to the time derivation of features via image Jacobian,
k  J s , kVc , k , s

(5.55)

where J s ,k is the image Jacobian discussed in [5.33]. Then the offline camera velocity may be calculated as,
Vc , k  1 J s ,k  ( sk  s* ) .

(5.56)

Here J s ,k  is the inverse Jacobian of the offline controller, J s,k .The feature trajectories ( sk ) and their respective Jacobian ( J s,k ) are recorded offline. In the next step, the online controller is employed and the camera velocity is found as,
Vc ,k  Vc ,k  2 J s ,k  ( sk  sk ) .

(5.57)

146

The first part of this controller guides the robot towards the desired position based on offline calculations and the second part works as an ancillary IBVS controller with sk as its desired features, assuring the current features to follow the pre-computed feature trajectories closely. As was shown in [5.33], for such controller the Jacobian of desired features may be used alternatively and hence (5.57) can be reformulated as,
Vc ,k  Vc ,k  2 J s ,k  ( sk  sk ) .

(5.58)

The proposed controller in (5.56) has several advantages over the conventional controller. First, this controller has two degrees of freedom. As was shown previously, such structure allows the decoupling of the rate of convergence from the uncertainty control. Second, the Jacobian used in (5.58) is pre-computed and is needless of online depth estimation. Third, a big portion of the velocity is computed online and therefore current and computed features are very close. As a result the online gain ( 2 ) may be selected sufficiently small. Finally, since most components of such controller are pre-computed and readily available at servoing, the control will perform faster compared to conventional controller. It is worth computing the mean and covariance of the camera velocity through the new controller. The mean of the velocity is obtained through (5.50), (5.57), and (5.58) as follows,
 Vc , k  J s ,k  1 s*  s0 exp(1k t )  J s , k  m 1  exp( 2 k t )  .

 

 





(5.59)

Similarly, the covariance is computed as follows,
 1  exp  22  2 2 t  k t cov Vc ,k     2  2 t 



 
 

3

2

t 2  J s ,k T J s ,k  .
1

(5.60)

As can be seen from (5.60), increasing the online gain will increase the covariance of the camera velocity and consequently the uncertainty on robot position will rise. Therefore, it is important to select the online gain as small as possible, but not too small so that the steady-state error of features remains reasonable. The effectiveness of this proposed method is verified through simulations in Sec 5.5. 147

As it was shown in the previous chapter, the uncertainty affects the visual feature trajectories, which in turn propagates to the robot's trajectory. To alleviate this issue, a two-stage control scheme is proposed. The controller is composed of an offline controller, through which the trajectories of camera velocity, visual features, and their associated image Jacobian are estimated, and an online controller which minimizes the uncertainties by relying on the estimated trajectories. The constraint handling is possible in either of these steps. Based on this fact, two robust predictive controllers are proposed. In the first robust controller, MPC is employed as the online controller. Such scheme has the advantage of online constraint handling and uncertainty minimization at the price of increased computational cost. The global stability of this controller is guaranteed similar to path planning schemes (e.g., [5.1]). The second robust controller exploits the MPC as its offline controller, rendering the online control to a simple proportional controller. This scheme has the advantage of guaranteed constraint management, even in the presence of uncertainties. Moreover, the choice of offline MPC reduces the computational cost dramatically. The stability of the system is guaranteed, similar to the proposed predictive controllers in Sec. 5.3. The control scheme as explained as follows.

5.4.2 Robust Control with Online Predictive Controller (RCONPC)
As was discussed, this controller has two stages. At the offline stage, the control problem is solved assuming no uncertainties. Therefore the model of the system is reduced to,
eG ,k 1  eG ,k  tJ G ,kVc ,k ,

(5.61)

where the estimated values are denoted by an "over-bar". The problem is initiated by setting,
e0  s0  s * ,

(5.62)

and is solved by selecting an appropriate controller (e.g., Vc , k    J G , k  eG , k ). Since this problem is solved offline, the parameters such as object's pose and features' depths are readily available at each time step. Two different error sets may be selected to calculate the offline camera velocity. In the first set, the error of each image feature is selected as the servoing error,
eG ,k  es ,k ,

(5.63)

148

In the second set, a combination of pose and image feature errors is selected, similar to hybrid controller discussed in Sec. 5.3, i.e.,
T eG ,k   es ,k T eR ,k

 eT z ,k  .

T

(5.64)

The choice of the first set of features has the advantage of simplicity, while the second set ensures smooth Cartesian trajectory and global stability [5.32]. Choosing only the point features simplifies the overall servoing scheme. This is because these features are easy to extract and do not require any post-calculations (as is the case for depth and rotation angle). Therefore, regardless of the feature choice, only the trajectories of the point features and their associated Jacobian are recorded. In addition to that, the velocity profile of the offline controller is recorded and exploited in the online controller. Once the estimated trajectories of the system are available, an online controller is employed to track the estimated trajectories. An optimal controller is designed by setting a cost function and minimizing the cost,
Vc ,k  arg min  (eG ,k i  eG ,k i )G (eG , k i  eG ,k i )T ,
i 1 Nc

(5.65)

where  G is the weighting matrix and N c is the control horizon. The controller is a typical model predictive control (MPC) adopted for VS, e.g., the one proposed in [5.6], with the desired features being replaced by the offline controller trajectories. As it can be inferred from (5.65), the covariance of the features is minimized if the mean of the features is estimated by their offline trajectories. The control horizon is directly related with the computation time of controllers and therefore cannot be large for real-time applications. For this purpose, the control horizon is chosen to be minimal (i.e., equal to one). As a result, the acquired velocities and feature trajectories might face discontinuities that would cause erratic robot motions, as was pointed out in [5.6]. In order to alleviate this problem, the cost function is modified as follows
T Vc ,k  arg min   (eG ,k 1  eG ,k 1 ) G (eG , k 1  eG ,k 1 )  (Vc ,k  Vc ,k )  (Vc , k  Vc , k )  . T

(5.66)

149

The new cost function penalizes the control deviations from the offline control and therefore minimizes the discontinuity in velocities. The choice of  in the new cost function must be done cautiously, as small values in  cause the system to be subjected to unwanted vibrations, while large values may prevent the system to converge. A reasonable design will start with higher values for  and reduce it gradually to let the system converge. In the absence of constraints, the control may be calculated analytically similar to a linear quadratic regulator (LQR) as follows,
T 2 T Vc,k  (  t 2 J s ,k T s J s ,k )1  tJ s ,k s (es,k  es ,k )  (  t J s.k s J s,k )Vc,k  .

(5.67)

The image Jacobian is not accessible directly and may only be estimated. Here, we approximate this Jacobian with its offline counterpart, J s , k . Then, (5.67) may be rewritten as follows,
Vc , k  Vc ,k  t (   t 2 J s ,k T  s J s , k ) 1 J s , k T s (es ,k  es ,k ) .

(5.68)

As can be inferred from (5.68), the unconstrained online control is composed of the offline control and a damped-least-square controller that regulates the features along their pre-computed trajectories. The use of offline Jacobian has several advantages. First, this Jacobian is not affected by the noise and therefore does not amplify the propagated noise level. Second, as this Jacobian replaces the real-time Jacobian, there is no need for online Jacobian computation which requires partial pose estimation. Finally, by having most part of (5.68) pre-computed, this choice of Jacobian speeds up the control calculation. Alternatively, the minimization problem may be solved as a constrained optimization problem to entail the restrictions of the system such as camera's visibility and robot's reachability constraints, similar to [5.6], i.e.,
T Vc ,k  arg min   (es , k 1  es , k 1 )  s (es ,k 1  es ,k 1 )  (Vc ,k  Vc ,k )  (Vc , k  Vc , k )  T

s.t. sk  S Vc ,k   ,

(5.69)

where S and  are the sets of admissible image features and velocity trajectories, respectively. This empowers the proposed controller to be capable of minimizing the uncertainties, while 150

handling the constraints. The unconstrained analytical solution is used as an initial guess to speed up the optimization procedure. The stability of the offline controller was proven in the previous works [5.33]. While the stability of the point features is only local, adding the depth and rotation to the features results in global stability. Therefore, the feature trajectories in offline phase will end in desired features. As for the online controller, the stability may be followed separately for constrained and unconstrained problems. In case of unconstrained system, the analytical solution is available for the controller and the stability is proven using the energy of the error as the positive definite Lyapunov function,

T 1 es , k  es , k   es , k  es ,k  ,  2

(5.70)

The derivate of this function is computed as follows,

   e  e T J V  J V .  s ,k s ,k s ,k c , k s ,k c , k
By using (5.68) in (5.71) and assuming J s,k  J s ,k , it can be shown that,





(5.71)

  t  e  e T  J (  t 2 J T  J )1 J T   (e  e ),  s ,k s ,k s ,k s s,k s,k s  s ,k s ,k  s,k

(5.72)

which is always negative as  s and  are positive definite symmetric matrices. One way to assure that the Jacobian matrices are the same is to run the offline controller at each time step and find the velocity and the trajectory for the coming step.

5.4.3 Robust Control with Offline Predictive Controller (RCOFPC)
Prior to servoing, an error-free model of the system is developed as a reference, i.e.,
sk 1  sk  tJ kVc ,k ,

(5.73)

151

where the error-free values are denoted by the "over-bar". The error-free feature trajectories are decided by selecting a proper camera velocity profile, which guides the features towards the desires ones. In order to reduce the uncertainties of the system, the actual features are guided as close as possible to the error-free features. For this matter, the error between the actual and errorfree features is to be minimized. This difference of features is shown by, ^k  sk . es ,k  s Then it can be shown that, (5.74)

s ,k , es ,k 1  es ,k  te
where,
s , k  J s , kVc , k  J s , kVc , k . e

(5.75)

(5.76)

The difference of features may reduce to zero exponentially by choosing,

s ,k   es ,k . e
The velocity of the camera is then found by employing (5.77) in (5.76),
Vc ,k  J s ,k  ( J s ,kVc , k   es ,k ) .

(5.77)

(5.78)

The image Jacobian may be estimated by the reference image Jacobian (i.e., J s ,k  J s ,k ), which reduces the control to,
Vc , k  Vc ,k   J s ,k es , k .

(5.79)

It can be inferred from (5.77) that the proposed camera velocity is composed of two parts. The first part is a pre-computed camera velocity which is the velocity reference. The second part is a regulatory controller that minimizes the deviation of features from the reference ones. It is worth mentioning that since the error-free components of (5.79) are computed prior to servoing, the computation of camera velocity through (5.79) is very fast in action. The computation of the 152

reference velocities ( Vc ,k ), image Jacobians ( J s,k ), and image features ( sk ) are explained in the sequel. In order to handle the constraints of the system, a MPC controller is developed. Since this controller is based on the error-free model of the system, it has been introduced to the reference system shown in (5.70). The cost of the system is defined as,

Ls    sk i  s
i 1

Nc

*

  s
s

T

k i

 s*   VcT , k  i Vc , k  i .

(5.80)

Here  s and  are the features and velocity weighting matrices and N c is the control horizon, which is selected as one to minimize the computational costs. As a result, the system may face velocity discontinuities, which is alleviated by selecting proper weighting matrices. Alternatively, the control horizon may be selected larger at the cost of extra computational time. The control actions of the system is predicted over the control horizon by minimizing the cost function, subject to system constraints, i.e.,
U  arg min Ls
Vc ,k 1 ,..,Vc ,k  N

s.t. sk  i  S ,

Vc ,k  i  ,

(5.81)

where S and  are the admissible features and camera velocities sets, respectively. The first element of U is selected as the camera velocity and is applied to the reference model (5.73) to form the features of the next step. The process is repeated until the reference image features are close enough to the desired features. Next, the trajectories of camera velocity, image features and their relevant image Jacobian are recorded to be used for servoing through (5.79). The conditions defined in (5.33) through (5.38) prevent the system from constraint violation. However, the constraints may not be met if the system is uncertain. One way to alleviate this problem is to tighten the constraints by setting new limits including,
S  sk | smin   s  sk  smax   s  ,

(5.82)

153

V

  Vc ,k | Vc ,min   V  Vc , k  Vc ,max   V  
c

| min   P  co, k  tRco,k J p ,kVc ,k  max   P  ,

(5.83)

where  s ,  V , and  P are the constraint thresholds of image features, camera velocity, and camera position, respectively. A proper selection of the uncertainty thresholds is important, since they may cause the system to be conservative or inadequate to handle the constraints. In this work, the uncertainty thresholds are selected as three times of the uncertainties standard deviation,

 s  3 s ,  V  3 V ,  P  3 P ,

(5.84)

where  s ,  V , and  P are the standard deviations of the uncertainties related to image features, camera velocity and camera position, respectively. Assuming the noise to be Gaussian, this choice of thresholds ensures satisfactory constraint handling for %99.7 of times. However, the standard deviations of the uncertainties are required for this matter, which is available from Chapter 3.

5.5 Simulation and Experimental Results
In this section, numerous simulations and experiments are conducted to demonstrate the effectiveness of the proposed methods for constraint and uncrtainty handling. Testing the system through simulations is performed under controlled situations (e.g., known noise parameters, system dynamics, etc.), while experimental results confirm the practicality of the proposed methods under actual system conditions. In the simulations, an object with four planar feature points, located on the vertices of a square that has a side of 10 cm, is considered. The camera is servoed from a distant pose to half a meter above the object. The sampling time is 30 milliseconds (  t  0.03 seconds) and the error weighting matrices are selected as the identity matrix. The simulations are carried under MatlabÂ® 2011b software from Mathworks (Natick, MA, USA).
5.5.1 Constrained Predictive Controllers

In this subsection, the proposed predictive controllers (i.e., PBPC and HPC) are verified and compared with previously proposed image-based predictive controller. In the first simulation, the 154

constraint handling property of the hybrid and position-based predictive controllers with the proposed cost functions, in which the control actions are penalized, are verified and compared with that of image-based predictive controller. For this purpose, the image features are bounded between 125 and 175 pixels in u direction and between 75 and 125 pixels in v direction. The position of the camera is restricted by 0.3 meters from the object in x and y direction and between 0.5 and 0.6 meters away from the object in z direction in the object frame. The camera velocities are limited to 0.3 (m/s or rad/s). The control weighting matrix is selected hundred times smaller than the identity matrix (   102 I 6 ). The result of servoing with these controllers in image space and Cartesian space, along with camera velocities and system error energy are shown in Figures 5.1-5.4. The initial and final images of the object are demonstrated in image space by red dashed and blue dot-dashed lines, respectively. The image and Cartesian boundaries are depicted by black and blue dotted lines, respectively. The image space and Cartesian space results confirm the ability of all proposed controller to successfully handle the constraints of the system. The trajectories in both spaces are relatively smooth and all controllers are capable of guiding the robot to its desired pose in spite of the constraints in image and Cartesian space. The camera velocities, resulted from each of these controllers, are smooth despite the minimal selection of the control horizon. This smoothness is due to the penalties set on them in the cost function. In addition, the gradual decrease of error energy was depicted in all controllers and therefore the stability of the systems is maintained. The terminal constraint set on the camera velocities has a major role in this steady reduction of the error. It is worth mentioning that the image-based predictive controller has the longest convergence rate as a result of velocity penalizing, while the second hybrid method has the smoothest trajectories of all.

155

130 120 110 v (pixel)
z (m)

-0.8 -0.6 -0.4 -0.2 0

100 90 80 70 120

0.2 0

0.2 -0.2 0 -0.2 x (m)

130

140

150 u (pixel)

160

170

180

y (m)

(a)
0.2
0.14 0.12 Energy Derivative 0.1 0.08 0.06 0.04 0.02 0 0 10

(b)

0.1 Camera Velocity

0 V (m/s)
x y

-0.1

V (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

-0.2

-0.3 0

10

20 Time (s)

30

40

20 Time (s)

30

40

(c)

(d)

Figure 5.1 First simulation: Visual servoing via image-based MPC with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

156

130 120 110 v (pixel)
z (m) -0.8 -0.6 -0.4 -0.2 0

100 90 80 70 120

0.2 0 -0.2 0 -0.2 x (m)

0.2

130

140

150 u (pixel)

160

170

180

y (m)

(a)
0.3

(b)
3

0.2

2.5
Camera Velocity 0.1

Energy Derivative

0 Vx (m/s) -0.1 -0.2 -0.3 -0.4 0 2 4 Time (s) 6 Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

2 1.5 1 0.5 0 0

8

10

2

4 Time (s)

6

8

10

(c)

(d)

Figure 5.2 First simulation: Visual servoing via position-based MPC with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

157

130 120 110 v (pixel)

-0.8 -0.6 z (m) -0.4 -0.2

100 90

0
80 70 120

0.2 0
130 140 150 u (pixel) 160 170 180

0.2 -0.2 0 -0.2 x (m)

y (m)

(a)
0.3 0.2

(b)
3 2.5

Camera Velocity

0.1

Energy Derivative

0 -0.1 -0.2 -0.3 -0.4 0 2 4 Time (s) 6 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

2 1.5 1 0.5 0 0

8

10

2

4 Time (s)

6

8

10

(c)

(d)

Figure 5.3 First simulation: Visual servoing via first hybrid controller with velocity penalties, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

158

130 120 110 v (pixel)

-0.8 -0.6 z (m) -0.4 -0.2

100 90

0
80 70 120

0.2 0
130 140 150 u (pixel) 160 170 180

0.2 -0.2 0 -0.2 x (m)

y (m)

(a)
0.3 0.2

(b)
3 2.5

Camera Velocity

Energy Derivative

0.1 0 -0.1 -0.2 -0.3 -0.4 0 2 4 Time (s) 6

2 1.5 1 0.5 0 0

Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

8

10

2

4 Time (s)

6

8

10

(c)

(d)

Figure 5.4 First simulation: Visual servoing via second hybrid controller, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. In the second simulation, the role of velocity penalizing in the cost function is investigated. The goal is to show how removing the velocity penalty affects the system trajectories. For this matter, the proposed controllers are tested without any velocity penalties (i.e.,   0 ) and their camera velocities are compared with those of image-based MPC. The results of this comparison are demonstrated in Figure 5.5. As it can be seen, all controllers are affected by the short control horizon. Control discontinuities are caused as a matter of numerical optimization and seem to be more severe in case of image-based predictive control. Position-based predictive controller

159

seems to be the least affected. It is also worth mentioning that all controllers have converged at a higher rate, compared to their velocity penalized counterparts.
0.3 0.2 0.1 Camera Velocity
Camera Velocity 0.3 0.2 0.1 0 -0.1 -0.2 -0.3 -0.4 0 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

0 -0.1 -0.2 -0.3 -0.4 0 V (m/s)
x y z

V (m/s) V (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

1

2

3 Time (s)

4

5

6

1

2

3 Time (s)

4

5

6

(a)
0.3 0.2 0.1 Camera Velocity 0 -0.1 -0.2 -0.3 -0.4 0 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  z (rad/s)  y (rad/s)

(b)
0.3 0.2 0.1 Camera Velocity 0 -0.1 -0.2 -0.3 -0.4 0 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

1

2

3 Time (s)

4

5

6

1

2

3 Time (s)

4

5

6

(c)

(d)

Figure 5.5 Second simulation: Camera velocities of different MPC controllers without velocity penalties, (a) image-based controller, (b) position-based controller, (c) first hybrid controller, and (d) second hybrid controller. In the third simulation, the superiority of the proposed position-based and hybrid MPCs over their image-based competitor in terms of stability is put into test. For this matter, the camera is rotated 180 degrees around the camera's Z direction from its desired orientation, while its 160

position is kept the same as the desired one. This particular pose selection causes IBVS systems to retract the camera to infinity, which will lead to system failure. All controllers are engaged separately to bring the camera back to its desired pose and their results are compared through Figures 5.6 to 5.9. As it was expected, the image-based MPC is unable of converging to the desire pose, while the position-based and hybrid MPCs converge with minimal translational move. It should be noted that the robot motion in Z direction comes as a result of constraint handling. This simulation confirms the improvement of system's stability in the proposed controllers, compared to image-based MPC.

130 120 110 v (pixel)
z (m) -0.8 -0.6 -0.4 -0.2

100 90

0

80 70 120

0.2 0 -0.2 0 -0.2 x (m)

0.2

130

140

150 u (pixel)

160

170

180

y (m)

(a)
0.05 0

(b)
0.32 0.31 Energy Derivative
V (m/s)
x

Camera Velocity

-0.05 -0.1 -0.15 -0.2 -0.25 -0.3 0 2 4 Time (s) 6

Vy (m/s) V (m/s)
z

0.3 0.29 0.28 0.27 0.26 0

 x (rad/s)  y (rad/s)  z (rad/s)

8

10

2

4 Time (s)

6

8

10

(c)

(d)

Figure 5.6 Third simulation: Visual servoing via image-based MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. 161

130 120 110 v (pixel) z (m) 100 90 0 80 70 120 0.2 0 130 140 150 u (pixel) 160 170 180 y (m) -0.2 -0.2 0 x (m) 0.2 -0.8 -0.6 -0.4 -0.2

(a)
0.3 0.25 0.2 Camera Velocity 0.15 0.1 0.05 0 -0.05 -0.1 0 5 10 Time (s) 15 20 V (m/s)
x y

(b)
10
V (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

8 Energy Derivative 6 4 2 0 0

Vz (m/s)

5

10 Time (s)

15

20

(c)

(d)

Figure 5.7 Third simulation: Visual servoing via position-based MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

162

130 120 110 v (pixel)

-0.8 -0.6 z (m) -0.4 -0.2

100 90

0
80 70 120

0.2 0
130 140 150 u (pixel) 160 170 180

0.2 -0.2 0 -0.2 x (m)

y (m)

(a)
0.3 0.2

(b)
12 10 Energy Derivative 8 6 4 2 0 0

Camera Velocity

0.1 0 -0.1 -0.2 -0.3 -0.4 0 5 10 Time (s) 15

Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

20

5

10 Time (s)

15

20

(c)

(d)

Figure 5.8 Third simulation: Visual servoing via first hybrid MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

163

130 120 110 v (pixel) z (m) 100 90 0 80 70 120 0.2 0 130 140 150 u (pixel) 160 170 180 y (m) -0.2 -0.2 0 x (m) 0.2 -0.8 -0.6 -0.4 -0.2

(a)
0.3 0.2

(b)
12 10

Camera Velocity

Energy Derivative

0.1 0 -0.1 -0.2 -0.3 -0.4 0 5 10 Time (s) 15 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

8 6 4 2 0 0

20

5

10 Time (s)

15

20

(c)

(d)

Figure 5.9 Third simulation: Visual servoing via second hybrid MPC with a half turn around the camera axis, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy.

5.5.2 Decoupled Controller
In this subsection, the performance of the proposed decoupled controller is verified through multiple simulations and experiments. In the simulation 4, the effect of controller gains over image features is investigated through a Monte Carlo simulation with 1000 samples. A single feature system is chosen for this purpose. The initial value of the feature is selected as 10. The mean and the covariance of feature error are both selected to be 0.1. The traditional single DOF proportional controller is employed to control the feature towards the desired value, which is selected as zero. The mean and covariance of the feature are shown in Figure 5.10 for three 164

different gain values. As it can be inferred, the convergence rate, steady-state error, and error covariance are all changing with the gain.
x 10
-5

10 Covariance Mean (mm) Measured Estimated 5

3 2 1 0

Measured Estimated 0 x 10
-5

0

0

10 Time (s)

20

(a)
6 Covariance 4 2 0

10 Time (s)

20

10 Mean (mm) Measured Estimated 5

Measured Estimated 0 x 10
-4

0

0

10 Time (s)

20

(b)
1.5 Covariance 1 0.5 0

10 Time (s)

20

10 Mean (mm) Measured Estimated 5

Measured Estimated 0 10 Time (s) 20

0

0

10 Time (s)

20

(c)

Figure 5.10 Fourth simulation: The mean and covariance of single feature, using a traditional proportional controller with (a)   0.5 , (b)   1 , and (c)   2 . Next in simulation 5, the proposed decoupled controller is employed for the same purpose as simulation 4. To demonstrate the role of the offline controller, the gain of this controller is altered while keeping the online gain constant ( 2  0.5 ) in the fifth simulation. The changes of mean and covariance of the feature are shown in Figure 5.11. It is clear that the rate of convergence changes with the offline control gain, while the steady-state error and error covariance are remaining unchanged due to the online gain being constant.

165

10 Covariance Mean (mm) Measured Estimated 5

3 2 1 0

x 10

-5

Measured Estimated 0 x 10
-5

0

0

10 Time (s)

20

10 Time (s)

20

10 Covariance Mean (mm) Measured Estimated 5

3 2 1 0

Measured Estimated 0 x 10
-5

0

0

10 Time (s)

20

10 Time (s)

20

10 Covariance Mean (mm) Measured Estimated 5

3 2 1 0

Measured Estimated

0

0

10 Time (s)

20

0

10 Time (s)

20

Figure 5.11 Fifth simulation: The mean and covariance of single feature, using the proposed decouple controller with constant online gain ( 2  0.5 ) and variable offline gain, (a) 1  0.5 , (b) 1  1 , and (c) 1  2 . In the sixth simulation, the mean and covariance of the feature is calculated for multiple online gains, having the offline gain constant ( 1  0.5 ). The results of this experiment are shown in Figure 5.12. It is obvious that the rate of convergence remains the same, while the steady-state error and error covariance change due to modification of the online controller gain. Though these parameters are decoupled and can be designed separately. Moreover, it can be inferred from Figures 5.10-5.12 that the mean and covariance of the feature are estimated correctly.

166

10 Covariance Mean (mm) Measured Estimated 5

3 2 1 0

x 10

-5

Measured Estimated 0 x 10
-5

0

0

10 Time (s)

20

10 Time (s)

20

10 Covariance Mean (mm) Measured Estimated 5

6 4 2 0

Measured Estimated 0 x 10
-4

0

0

10 Time (s)

20

10 Time (s)

20

10 Covariance Mean (mm) Measured Estimated 5

1.5 1 0.5 0

Measured Estimated 0 10 Time (s) 20

0

0

10 Time (s)

20

Figure 5.12 Sixth simulation: The mean and covariance of single feature, using the proposed decouple controller with constant offline gain ( 1  0.5 ) and variable offline gain: (a) 2  0.5 , (b) 2  1 , and (c) 2  2 . In the seventh simulation, the proposed controller is used in a visual servoing scenario. The desired pose of the camera-mounted robot is selected half a meter away from the object. Four point features are selected on the object and are exploited for servoing. The goal of simulations 7 and 8 is to show the decoupling effect of the proposed controller. Initially, the gain of the offline controller is changed, while the gain of the online controller is kept constant. The mean and covariance of features is shown in Figure 5.13. As it can be inferred, the convergence rate of the features changes with the offline gain. As it was predicted, the steady-state error remains unchanged, while the covariance of the features changes slightly, due to the change of Jacobian matrix. Nevertheless, the steady-state of the covariance is fixed as well. 167

0.2 0.15 0.1 0.05 0 0 50 Time (s)  (rad) x (m)

2 1.5 1 0.5 0 0 50 Time (s)

1=0.5 1=1.5 1=5

100

100

0.4 0.3 0.2 0.1 0 0 50 Time (s)  (rad) 100 y (m)

0.2 0 -0.2 -0.4 -0.6 0 50 Time (s) 100

-0.2  (rad) 0 50 Time (s) 100 -0.4 z (m) -0.6 -0.8 -1

0.2 0.1 0 -0.1 -0.2 0 50 Time (s) 100

(a)

168

x 10 2 x 1 0 0 x 10

-4

1.5 1  0.5 50 Time (s)
-4

x 10

-5

1=0.5 1=1.5 1=5

100

0

0 x 10
-3

50 Time (s)

100

3 2

1 0.5 0

1 0 0 x 10
-5

50 Time (s)

100



y

0 x 10
-3

50 Time (s)

100

4 2 0

1 0.5 0

0

50 Time (s)

100
(b)



z

0

50 Time (s)

100

Figure 5.13 Seventh simulation: The statistical measurements of visually servoed system, using the proposed controller with different offline controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. Next, the gain of the offline controller is kept constant and the online gain is varied in the eighth simulation. The results of these changes on the mean and covariance of the features are

169

demonstrated in Figure 5.14. As it was expected, the rate of convergence remains the same, while the steady-state error decreases and the error covariance increases as the online gain rises.

0.3 0.2 0.1 0 -0.1 0 50 Time (s)  (rad) x (m)

2 1.5 1 0.5 0 0 50 Time (s)

2=0.5 2=1.5 2=5

100

100

0.3 0.2 0.1 0 -0.1 0 50 Time (s)  (rad) 100 y (m)

0.2 0 -0.2 -0.4 -0.6 0 50 Time (s) 100

-0.4 -0.5 -0.6 -0.7 -0.8 0 50 Time (s)  (rad) 100 z (m)

0.2 0.1 0 -0.1 -0.2 0 50 Time (s) 100

(a)

170

1.5 1 x

x 10

-3

1.5 1  0.5

x 10

-4

2=0.5 2=1.5 2=5

0.5 0 0 x 10
-3

50 Time (s)

100

0

0 x 10
-3

50 Time (s)

100

2 1 0

6 4  2

y

0 x 10
-4

50 Time (s)

100

0

0 x 10
-3

50 Time (s)

100

6 4  2

2 z 1 0

0

50 Time (s)

100
(b)

0

0

50 Time (s)

100

Figure 5.14 Eigth simulation: The statistical measurements of visually servoed system, using the proposed controller with multiple online controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters. Next, the decoupling properties of the proposed controller are demonstrated through experiments. The experimental setup used for this matter is composed of a 6-degree of freedom 171

(DOF) robot from Denso robotics (Long beach, CA, USA), equipped with a Firefly camera from Point Grey (Richmond, BC, Canada). The robot operates in open-architecture mode, which is made possible through MATLABÂ® SIMULINKÂ® from Mathworks (Natick, MA, USA) and the QuarcÂ® control software from Quanser (Markham, ON, Canada). The camera is working with the speed of 60 fps, and the maximum velocity of the robot is 3900 mm/s, which is reachable at its end-effector. An object with four circles (as features) is used for servoing. The experimental setup is shown in Figure 5.15.

Figure 5.15 The experimental setup.

172

In the first experiment, the effect of offline gain in the proposed controller is tested. For that matter, the online gain of the controller is kept constant, while the offline gain is increased four times. The camera is servoed to almost 40 cm above the object and the experiment is repeated 10 times. The mean and covariance of the camera pose are depicted in Figure 5.16. It can be seen that the change of offline gain results in faster convergence, while the covariance of the system remains almost the same.

0.03  (rad) x (m) 0.02 0.01 0 0 20 40 60 Time (s) 80

0.4 0.3 0.2 0.1 0 0 20 40 60 Time (s) 80 1X 4X

0.22 0.215 y (m) 0.21 0.205 0.2 0 20 40 60 Time (s) 80  (rad)

0 -0.05 -0.1 -0.15 -0.2 0 20 40 60 Time (s) 80

0.2  (rad) 0 20 40 60 Time (s) 80
(a) 173

0.15 0.1 0.05 0 -0.05 0 20 40 60 Time (s) 80

z (m)

0.18 0.16

1

x 10

-6

4

x 10

-7

1X 4X 0.5 2 x  0 x 10 20
-7

0

40 60 Time (s)

80

0

0 x 10

20
-6

40 60 Time (s)

80

6 4

4

2

y

2 0 0 x 10 20
-7

 40 60 Time (s) 80

0

0 x 10

20
-6

40 60 Time (s)

80

1

3 2

0.5

 1 0 20 40 60 Time (s) 80 0 0

z

0

20

40 60 Time (s)

80

(b) Figure 5.16 First experiment: The statistical measurements of visually servoed system, using the proposed controller with different offline controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters.

174

In the second experiment, the effect of online gain on the mean and covariance of the camera pose is investigated. For that matter, the offline gain is selected as constant, while the online gain is increased four times. Similar to previous experiment, the camera is servoed to 40 cm above the object repeatedly for 10 times. The results of this experiment are shown in Figure 5.17. It can be seen from thos figure that the mean values of both controller are almost similar, while the covariance of the controller with increased online gain is higher. It is noticeable that the covariance of some degrees of freedom becomes similar for both cases once the camera reaches its destination. The reason behind this matter is the small changes in velocity, which are not followed by robot's motors.

175

0.03  (rad) x(m) 0.02 0.01 0 0 20 40 60 Time (s) 80

0.4 0.3 0.2 0.1 0 0 20 40 60 Time (s) 80 1X 4X

0.22 0.215 0.21 0.205 0.2 0 20 40 60 Time (s) 80  (rad) y(m)

0 -0.05 -0.1 -0.15 -0.2 0 20 40 60 Time (s) 80

0.2  (rad) 0 20 40 60 Time (s) 80 z(m) 0.18 0.16

0.15 0.1 0.05 0 -0.05 0 20 40 60 Time (s) 80

(a)

176

2

x 10

-6

2

x 10

-6

1X 4X

1

1

x

0

 0 x 10 20
-6

40 60 Time (s)

80

0

0 x 10

20
-5

40 60 Time (s)

80

1.5 1

1

0.5

0.5 0 0 x 10 20
-7

 40 60 Time (s) 80

y

0

0 x 10

20
-5

40 60 Time (s)

80

1

1

0.5

0.5

0

 0 20 40 60 Time (s) 80

z

0

0

20

40 60 Time (s)

80

(b) Figure 5.17 Second experiment: The statistical measurements of visually servoed system, using the proposed controller with two online controller gains, (a) mean of the pose parameters, and (b) covariance of the pose parameters.

177

5.5.3 Robust Controller with Online Predictive Controller
The simulation and experimental results of the robust controller with online MPC are presented in this subsection. In the ninth simulation, the features error is used for the offline controller to test the constraint handling capability. For this matter, the trajectory of the camera in Cartesian space is bounded. The camera was banned to move more than 10 cm in x direction, 25 cm in y direction, and 60 cm in z direction, away from the object. Moreover, the camera velocity is set to be less than 0.5 m/s for translational and 0.5 rad/s for rotational velocities. The control measurement matrix is initialized at   103 I , where I is the identity matrix, and is reduced 3 percent at each time step after 3 seconds. The results of this simulation are demonstrated in Figure 5.18. The initial and final images are shown by dashed red and dot-dashed blue lines in the image space. As it can be depicted, the propose controller is fully capable of handling the given constraints, while delivering the camera to its desired pose. The camera velocities are not facing any unusual discontinuities. In addition, the time derivative of error energy is negative throughout the servoing, which is indicative of system's stability.

178

130 120 110 v (pixel)
z (m) -0.8 -0.6 -0.4 -0.2 0

100 90 80 70 120

0.2 0 -0.2 0 -0.2 x (m)

0.2

130

140

150 u (pixel)

160

170

180

y (m)

(a)
0.2 0.1

(b)
2 0
Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

x 10

-4

Energy Derivative

Camera Velocity

0 -0.1 -0.2 -0.3 -0.4 -0.5 0 2 4 6 Time (s) 8

-2 -4 -6 -8 0

10

12

2

4

6 Time (s)

8

10

12

(c)

(d)

Figure 5.18 Ninth simulation: Visual servoing via two-stage controller with IBVS offline controller and online MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy time derivative. In simulation 10, the hybrid error ( eH1 ) is employed for the offline controller to test the constraint handling capabilities of this controller. The weighting matrices and the velocity boundaries are selected the same as the previous simulation. The image features are bounded between 125 and 175 pixels in u direction and between 70 and 130 in v direction. The results of this simulation are shown in Figure 5.19. The image boundaries are shown with the black dotted rectangle in the image space. It is clear that the image features are kept within the boundaries, while moving toward the desired features. The velocities are smoothly converging towards zero

179

without violating the limit. Moreover, the stability of the system is shown through consistent negativity of the error energy time derivative.
130 120 110 v (pixel)

-0.8 -0.6 z (m) -0.4 -0.2

100 90

0
80 70 120

0.2 0
130 140 150 u (pixel) 160 170 180

0.2 -0.2 0 -0.2 x (m)

y (m)

(a)
0.4 0.3 0.2 Camera Velocity

(b)
x 10
-4

2 0 Energy Derivative -2 -4 -6

0.1 0 -0.1 -0.2 -0.3 -0.4 -0.5 0 2 4 6 Time (s) 8 Vx (m/s) Vy (m/s) Vz (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

10

12

-8 0

2

4

6 Time (s)

8

10

12

(c)

(d)

Figure 5.19 Tenth simulation: Visual servoing via two-stage controller with HVS offline controller and online MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy time derivative. In simulation 11, the uncertainty levels of the proposed methods are compared with that of a typical IBVS to show the superiority of the proposed controller in terms of accuracy. The norm of the position error covariance is exploited as a measure of uncertainty. Two Monte Carlo simulations with 20 iterations are conducted for constrained and unconstrained visual servoing and their results are compared with that of a typical single-stage visual servoing with a gain of 180

0.8. The point features were used for the offline controller. The weighting matrices are selected as   I and   104 I . Figure 5.20 entails the results of this comparison. As it can be seen, the uncertainty levels of the camera position's trajectory with the proposed controllers are much less than that of a typical controller. It is also worth mentioning that the uncertainty level is dropped during the constraint handling, since the controller keeps the trajectories at the limits and therefore position variations are diminished.
1.4 1.2 Error Covariance Norm 1 0.8 0.6 0.4 0.2 0 0 2 4 6 Time (s) 8 10 12 x 10
-4

Constrained Unconstrained IBVS

Figure 5.20 Eleventh simulation: The norm of the position error covariance for the proposed constrained and unconstrained two-stage controller with online MPC versus a simple proportional IBVS controller. In the twelfth simulation, the effect of control weighting matrix (  ) on the uncertainty level is investigated. Three different values are selected and the norm of position error covariance is measured in an unconstrained visual servoing scenario. The results of this simulation are depicted in Figure 5.21. From this figure, it can be inferred that increasing the weights on the control results in uncertainty decrease. However, the weights cannot be selected too high, since it prevents the system to fully converge, as discussed before.

181

x 10 2 Error Covariance Norm

-4

R=10-3I R=10-4I R=10-5I

1

0 0

2

4

6 Time (s)

8

10

12

Figure 5.21 Twelfth simulation: The norm of the position error covariance for the proposed constrained and unconstrained two-stage controller with different control weightings. In order to demonstrate the applicability of the proposed control methods in real world, the controller has been tested with experimental setup shown in Figure 5.15. In experiment 3, the accuracy of the proposed controller is compared with that of classic IBVS. For this matter, the norm of the camera position in 10 repeated servoing scenarios is measured separately for the proposed controller and IBVS. Then, the covariance of these norms is computed. Figure 5.22 demonstrates the results of this experiment. As it can be seen, the covariance of the proposed controller is much less than that of IBVS.
5 4.5 4 Error Covariance Norm 3.5 3 2.5 2 1.5 1 0.5 0 0 10 20 30 40 50 Time (s) 60 70 80 90 Error Covariance Norm x 10
-9 -6

2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0

x 10

0

10

20

30

40 50 Time (s)

60

70

80

90

(a)

(b)

Figure 5.22 Third experiment: The norm of the position error covariance for, (a) two-stage controller with online MPC, and (b) a simple proportional IBVS controller. 182

In experiment 4, the effect of velocity penalizing on the accuracy of the system is demonstrated. For that matter, the camera is servoed from a distant pose to 40 cm above the object. Two controllers with   103 I and   105 I are each used 10 times to bring the camera to its desired location. The covariance of camera position norm is then calculated. Figure 5.23 shows the results of this experiment. It is obvious that higher penalities in the controller lead to decreased covariance of the robot trajectory.
5 4.5 4 Error Covariance Norm 3.5 3 2.5 2 1.5 1 0.5 0 0 20 40 Time (s) 60 80 100 Error Covariance Norm x 10
-9 -6

2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0

x 10

0

20

40 Time (s)

60

80

100

(a)

(b)

Figure 5.23 Fourth experiment: The norm of the position error covariance for the two-stage controller with online MPC, (a)   103 I , and (b)   105 I . In experiment 5, the camera is servoed from a distance to its desired pose, which is 15 cm above the object. The control weighting matrix is selected as   103 I to reduce the image noise effects. Image features servoing error is used for the offline controller. The results of this maneuver are presented in Figure 5.24. As it can be seen, camera is successfully guided towards the desired pose with a smooth motion. This is mainly due to proper selection of  matrix.

183

350 300 250 200 150 100 200 0.3 0.2 z (m) 0.1

v (pixel)

0 0.25 0.2 250 300 350 u (pixel) 400 450 500 y (m) 0.15 -0.05 0 x (m)

0.05

(a)
0.1 V (m/s) 0.08 Camera Velocity 0.06 0.04 0.02 0 -0.02 -0.04 0 20 40 60 Time (s) 80 100 120
x y z

(b)

V (m/s) V (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

(c) Figure 5.24 Fifth experiment: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, and (c) Camera velocities.

5.5.4 Robust Controller with Offline Predictive Controller
In this subsection, the effectiveness of the robust control method with offline MPC is put into test. In simulation 13, the MPC is employed initially to form the reference trajectories. The image features are not constrained; however, the camera's distance from the object is bounded. The weighting matrices, B and  , are selected to be the identity matrix and one tenth of the identity matrix, respectively. The velocity weighting matrix is gradually decreased to allow the system to fully converge. The goal of this simulation is to show the performance of this controller in presence of system constraints. The results of this simulation are depicted in Figure 184

5.25. The image trajectories are smooth and the Cartesian trajectory is bounded as expected. The camera velocities are mostly smooth and converge to zero as the camera approaches its desired pose.
130 120 110 z (m) v (pixel) -0.4 -0.2 90 80 70 120 0 0.2 0 130 140 150 u (pixel) 160 170 180 y (m) -0.2 0 -0.2 -0.1 x (m) 0.1 0.2 100 -0.8 -0.6

(a)
0.15 0.1 0.05

(b)

0.14 0.12

Camera Velocity

0.1 Error Energy
60 0 -0.05 -0.1 -0.15 -0.2 0 10 20 30 Time (s) 40 Vx (m/s) Vy (m/s) V (m/s)
z

0.08 0.06 0.04 0.02 0 0 10 20 30 Time (s) 40 50 60

 x (rad/s)  y (rad/s)  z (rad/s)

50

(c)

(d)

Figure 5.25 Thirteenth simulation: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, (c) Camera velocities, and (d) Error energy. In the fourteenth simulation, the effect of constraint tightening in presence of system uncertainty is demonstrated. First, the system with ordinary constrained MPC is exposed to the image noise. Next, the system with the proposed constraint handling is tested under the same conditions. Both systems are simulated for 100 times and their trajectories are compared. The results of this comparison are presented in Figure 5.26. In this figure, the position of the camera is shown in 185

two dimensions for better demonstration of constraint handling. The tubes encompassing the uncertain camera position trajectories are shown by the red ellipsoids and the mean of these trajectories are shown by the solid black line. The constraints are shown by dashed blue lines. As it can be seen, only the mean of trajectories of the system with ordinary MPC satisfies the constraints. In fact, many of the trajectories of such system that are included in the red tubes have violated the constraints. On the other hand, none of the trajectories of the system with the proposed controller has violated the boundaries of the system, due to correct uncertainty estimation and constraint handling.

186

0.4 0.3 0.2 0.1 y (m) 0 -0.1 -0.2 -0.3 -0.4 -0.4 -0.2 0 x (m) 0.2 0.4

(a)
0.4 0.3 0.2 0.1 y (m) 0 -0.1 -0.2 -0.3 -0.4 -0.4 -0.2 0 x (m) 0.2 0.4

(b) Figure. 5.26 Fourteenth simulation: Two dimensional view of Constraint visual servoing in the presence of uncertainties with, (a) uncompensated controller, and (b) the proposed controller. In the experiment 6, the camera is moved from its desired pose and is returned back to the desired pose through the proposed visual servoing controller. The same experimental setup used 187

for RCONPC is exploited for that matter. The camera is bound not to be higher than 0.2 meters above the object. The weighting matrices are similar to the first simulation and the proportional gain is selected as 0.1. The results of this maneuver are demonstrated in Figure 5.26. As it can be inferred, the controller is fully capable of guiding the robot to its desired pose, while handling the given constraints. The camera velocities are mostly smooth and converge to zero, similar to the simulations.
350 300 z (m) 250 200 150 100 200 0.2 0.15 0.1 0.05 0 0.25 0.2 250 300 350 u (pixel) 400 450 500 y (m) 0.15 0 0.05 x (m) 0.1 0.15

v (pixel)

-0.05

(a)
0.1

(b)

0.05 Camera Velocity 0 Vx (m/s) -0.05 V (m/s)
y z

V (m/s)
 x (rad/s)  y (rad/s)  z (rad/s)

-0.1

-0.15 0

20

40

60 80 Time (s)

100

120

(c) Figure 5.27 Sixth experiment: Visual servoing via two-stage controller with offline MPC, (a) Image space, (b) Cartesian space, and (c) Camera velocities.

188

5.6 Summary
Successful RVS requires proper constraint handling and treatment of uncertainty effects. In this chapter, several control schemes were proposed to address these concerns. Initially, a PBPC and two HPC were proposed to handle the constraints of the visual servoing while improving the stability and numerical feasibility of the system. It was shown that the proposed controllers were fully capable of handling the constraints, while their convergence was guaranteed through a supplementary constraint. The HPC with full pose error offered the smoothest trajectories of all. In addition, a novel two-DOF control structure was introduced for robust RVS. Unlike the conventional proportional controllers, this control scheme enabled the system to control the rate of convergence, without impacting the steady-state error and the error covariance of the system. The controller was needless of online depth estimation and inverse Jacobian calculation, since it relied on offline calculation for that matter. Moreover, the controller allowed the system to run faster, since most of controller components were pre-computed during the offline phase. Based on this structure two controllers were presented for robust RVS. The first controller was capable of minimizing the system uncertainties, while handling the camera/robot constraints. The second controller was proposed for constraint handling in the presence of system uncertainties. Moreover, a model of system uncertainties was developed and exploited in constraint handling to minimize the effects of uncertainties. It was shown through simulations and experiments that the proposed methods were capable of constraint handling and were robust to image noise. It is worth mentioning that unlike their image-based counterpart, the proposed MPC controllers require accurate estimations of pose from the camera images. The computation of pose may impose an extra computational burden to the system, which is supposed to be minimal. As a solution to this problem, the pose of previous time step may be used in optimization-based estimators to achieve fast pose estimations, similar to VVS explained in Chapter 2. In addition, the object of interest was supposed to be static which could limit the applicability of the proposed robust controllers. However, the same methodology may be practiced for moving objects by shifting the offline controller to the online stage to predict the next time step trajectories. Yet, this change comes at a price of increased computational cost.

189

Chapter 6 Conclusions
6.1 Summary of the Thesis
Successful RVS in unstructured environments entails apt techniques to handle the system limitations and errors caused by uncertainties. This thesis was mainly focused on robust visual servoing schemes to address such requirements. In addition to that, enhancing the accuracy of the system was of interest. Three major steps were taken towards robust and accurate RVS design. In the first phase, pose estimation was targeted as one of the most sensitive parts of most RVS systems. It was known that image uncertainties could degrade the accuracy of pose estimations, which could lead to inferior performance or even task failure. Sensor fusion techniques were proposed to alleviate this sensitivity by employing multiple cameras. A comprehensive study on sensor fusion for pose estimation was conducted under various system conditions. It was shown that the proposed centralized fusion techniques could response to system uncertainties and faults (e.g., image occlusion) better than the previous methods. Since this superior performance (in terms of accuracy and robustness) came at the price of increased computational cost of the system, decentralized fusion techniques were proposed as alternatives. These schemes were capable of lowering the computational complexity at the price of accuracy reduction. Yet, they could facilitate the fault detection and isolation. Since the centralized and decentralized fusion techniques required particular vessels for pose estimation (e.g., Kalman-based structure), they could not be used for other pose estimation methods. In order to address this shortcoming, a preprocessing fusion scheme was introduced. This scheme could offer enhanced accuracy and be engaged with any available pose estimation technique. In the second step towards robust control of RVS systems, a novel uncertainty model for RVS systems was proposed. In this model, the error covariance of different signals in the system was estimated. For that purpose, the closed-loop and discrete-time nature of the RVS systems were considered. In addition, a general methodology was offered which could be applied to a vast group of controllers. The modeling was particularly developed for first and second order 190

controllers, which were very popular in RVS systems. It was shown later that the model could approximate the uncertainties of the system closely. Finally, several robust and constrained control schemes were introduced to cope with system limitations. Initially, novel predictive controllers were proposed to enhance the stability of the system, while handling the constraints. Then, a two-stage control scheme was introduced for robust and accurate RVS. The proposed control structure had the capability of decoupling the uncertainty measures from the system's convergence rate, which could help in uncertainty minimization without sacrificing the speed. In addition, the proposed uncertainty model was exploited along with the developed predictive controllers to robustly guide the system to its destination, while avoiding the system constraints, even in the presence of system uncertainties. Moreover, the accuracy of the system was improved by minimizing the effects of uncertainties. In conclusion, the proposed robust techniques were capable of achieving reasonable results, in the presence of uncertainties mainly originated from image noise. The proposed fusion techniques can pave the way for more accurate and robust pose estimation to be used not only in visual servoing systems, but also in many other applications such as object recognition and tracking. The developed IAUKF scheme may be used for accurate estimations in many nonlinear applications. In addition, the developed uncertainty model adds extra insight to the response of RVS systems in presence of uncertainties, which in turn may lead to optimal control schemes. Last but not least, the proposed robust controllers open a new horizon in robust visual servoing by exploiting the developed uncertainty model to handle the constraints efficiently. In summary, the proposed techniques make RVS was step closer to wide applicability in industrial tasks, while offering new paths to expand the robustness of the system even further.

6.2 Contributions
The contributions of the work were many-fold. Some of the most important contributions are listed as follows.



Novel sensor fusion techniques: Three fusion structures were proposed to enhance the accuracy and robustness of pose estimation in presence of system uncertainties. Novel centralized fusion, namely IAEKF, IAUKF, and VVS were proposed for the first time 191

and their superior accuracy was demonstrated in the presence of system uncertainties. The close relation between the VVS and Gauss-Newton pose estimation methods was shown and the superiority of the former was proven. Decentralized fusion methods were proposed to reduce the computational cost of the system. A novel pre-processing fusion was introduced for the first time which could be used with any available pose estimation to enhance the accuracy of the pose estimation.



Novel uncertainty modeling for RVS systems: A novel methodology was developed to model the effect of image noise in RVS systems. Unlike the previous models, this model accounted for the closed-loop nature of the system and could be used with a wide range of controllers and systems. In this model, the RVS system was treated as a discrete-time system.



Robust and constraint-aware controller design: Multiple constrained controllers such as PBPC and HPC were developed. It was shown that these controllers provide enhanced stability and numerical feasibility, compared to previous predictive controllers. In addition, a novel two-stage control scheme was proposed to decouple the effect of image noise from the system's rate of convergence. It was shown that the effect of errors could be minimized, while maintaining the same convergence rate. Next, the constrained control design was used in conjunction with the two-stage control scheme to handle the system's constraints, while minimizing the effect of image noise on the system's accuracy. Finally the developed error model was used to robustly handle the constraints in the presence of image noise. The chance of conservatism was reduced by engaging the knowledge from the developed uncertainty model.

6.3 Future Works
The work proposed in this thesis may be expanded in many directions, some of which are listed as follow.



Parameter Tuning: Many of the proposed techniques are dependent on parameters that

need to be adjusted accordingly. While these settings may be done on a case by case basis, an autonomous system is desirable which could maximize the performance of the

192

system. This system may take the required specifications of the system and tune the parameters through an optimization technique.



Sensor Fusion Expansion: Several sensor fusion techniques were proposed in this

work. However the propose fusion techniques were limited to pose estimation. One way to enhance the performance of the system is to expand the fusion algorithms to other parts of the system. Fusion at imaging and control stage may be beneficial as the redundant information provide by multiple cameras may add to system robustness.



Uncertainty Modeling Expansion: The proposed uncertainty modeling was shown to

be very useful. However, this model only accounts for the uncertainties imposed by the image noise. In addition, the image noise was assumed to be Gaussian. It would be desirable to expand this model to entail more uncertainties from the system (e.g., camera calibration or robot dynamics). This change will enhance the applicability of RVS in actual tasks.



Predictive Control: The usefulness of predictive controllers for constraint handling was

shown in this work. However, this was just the beginning. There are different structures for predictive controllers which could benefit the RVS systems in different ways. Simpler predictive controllers may be sought to reduce the computational complexity of the optimization algorithms. In addition, different models of the system and reference trajectories may be taken to improve the accuracy or the robustness of the system.



Comprehensive Study: Several robust algorithms were proposed for various purposes.

However their careful investigation and applicability was beyond the scope of this work. The applications of the robust estimator (i.e., IAEKF and IAUKF) in other systems are yet to be investigated. Moreover, the proposed robust controller may find useful applications in other systems, where accuracy and robustness are key features.

193

Appendices A. Velocity Transformation Matrix
i The velocity transformation matrix,  c e , is calculated separately for eye-in-hand and eye-to-hand

cameras.
Eye-in-hand

The rotation between the camera and the end-effector is expressed as,
e o Rce  Ro Rc .

(A.1)

The time derivation of both sides yields,
e e o ) Rce   S ( Ro S ( o  c ) Rce .

(A.2)

A change of coordinates results in,

 co   eo ,
which is equivalent to,

(A.3)

 c  Rec e .
As for the translational velocity, one can show,
e o e . tce  Ro tc  to

(A.4)

(A.5)

A time derivative of both sides of the equation results in,
e e o e o e o ) Ro (te  tco )  Ro S ( o te  Ro tc ,

(A.6)

which can be reordered as,
e c )te . tco  teo  Rco S ( Rec o

(A.7)

194

A simple change of coordinates yields,

e  S (tec ) Rec e . tc  Rec t
One can show that (A.4) and (A.8) are equivalent to (2.89).
Eye-to-hand

(A.8)

The time derivation of (A.1) yields,
e ) Rce . S ( ce ) Rce  S ( o

(A.9)

A change of coordinates results in,

 ce   Roe eo ,
which is equivalent to,

(A.10)

 c   Rec c .
In case of translational velocity, time derivation of (A.5) yields,
e e o e o ) Ro (tc  teo )  Ro tce  S ( o te .

(A.11)

(A.12)

Once again, a change of coordinates yields,

e  S (tec ) Rec e . tc   Rec t
It is easy to show that (A.11) and (A.13) are equivalent to (2.91).

(A.13)

195

B. Iterative Pose Estimation
In this section the iterative pose method proposed by Dementhon in [2.16] is explained. This method is selected especially, since it is usually difficult to modify this method to entail measurements from multiple cameras. Pre-processing fusion is beneficial to algorithms of this type. Also, since the algorithm has an iterative fashion, it provides the fusion level with more accurate estimate of object depth. The algorithm is described as follows. Based on (2.4) and (2.5),
c  c io  to , xic  1 0 0  Ro P  c c o c zi 0 0 1  Ro Pi  to    c  c io  to . yic  0 1 0  Ro P  c  c o c zi 0 0 1  Ro Pi  to   

uic

(B.1)

vic

(B.2)

c If point P0o of the object is selected as the origin of the object frame (i.e., to  P0c ) and the rotation

matrix is rewritten as follows,
 ai    a j  ,  ak   

c Ro

(B.3)

then (B.1) and (B.2) can be presented as follows,

 c ai Pio  x0 c ui  c  ak Pio  z0

Pi

o T



c T c c  u0 Pio  X i  u0  z0  , ak Pio  i  1 c 1 z0
i

a 

T

(B.4)

vic

 c a j Pio  y0  c  ak Pio  z0

Pi

o T



a 
j

T c  v0

ak Pio c 1 z0
196

c z0

Pio   

c X j  v0 ,  i  1

T

(B.5)

where:

 i 

ak Pio c . z0

(B.6)

Therefore if i are available, linear equations (B.4) and (B.5) yield,
   Xi       
c u c    1  u0  P1o     1 1    . .    , . .       T c c  Po   ul  l  1  u0  T





(B.7)

l

  



   Xj      

 
. .

o T P 1

 Plo 

      T   



v c  1     c v   l

1  1  v0c  


 . . .  c  l  1  v0 

(B.8)

The first two rows of the rotation matrix are extracted through normalization,
ai  Xi , Xi

(B.9)

aj 

Xj . Xj

(B.10)

The third row of the rotation matrix is calculated through its orthogonality property,
ak  ai  a j ,

(B.11)

and the translation element of the pose is calculated as follows,
c c c to  P0c  z0 p0 ,

(B.12)

where,

197

c z0 



2 . ai  a j



(B.13)

If an initial value is used for i values (e.g.,  i  0 ) an approximate method for pose estimation is achieved, which is known as pose from orthography and scaling (POS). A more accurate estimation of pose is obtained by using the approximate pose gained by POS algorithm to recalculate the values of  i through (B.6), and re-estimating the pose using the new  i values. Iterating through this process leads to an accurate pose estimation method known as POS with iterations (POSIT), which is considered in this work for pose estimation.

198

References
[1.1] F. Janabi-Sharifi, "Visual Servoing: Theory and Applications," Chapter in Opto-

Mechatronic Systems Handbook, Ed. H. Cho, CRC Press, Boca Raton, FL, 2002, pp. 15-1--1524. [1.2] F. Chaumette, S. Hutchinson, "Visual servo control I: basic approaches," IEEE Robotics

and Automation Magazine, vol. 13, no. 4, pp. 82Â­90, Dec. 2006.
[1.3] F. Chaumette, "Potential problems of stability and convergence in imagebased and position-based visual servoing," in The Confluence of Vision and Control, D. Kriegman, G. D. Hager, and A. Morse, Eds. Springer-Verlag, Lecture Notes in Control and Information Sciences, 1998, vol. 237, pp. 66Â­78. [1.4] F. Janabi-Sharifi, L. Deng, W.J. Wilson, "Comparison of basic visual servoing methods,"

ASME/IEEE Transactions on Mechatronics, vol. 16, no. 5, pp. 967Â­983, Oct. 2011.
[1.5] E. Malis, F. Chaumette, S. Boudet, "2-1/2-D visual servoing," IEEE Transactions on

Robotics and Automation, vol. 15, no. 2, pp. 238Â­250, Oct. 1999.
[1.6] F. Janabi-Sharifi, M. Ficocelli, "Formulation of radiometric feasibility measures for feature selection and planning in visual servoing," IEEE Transactions on Systems, Man, and

Cybernetics: Part B, vol. 34, no. 2, pp. 978Â­987, Apr. 2004.
[1.7] J. E. Slotine, W. Li, "Adaptive manipulator control: A case study," IEEE Transactions on

Automatic Control, vol. 33, no. 11, pp.995Â­1003, Aug. 1988.
[1.8] R. Kelly, "Robust asymptotically stable visual servoing of planar robots," IEEE

Transactions on Robotics and Automatation, vol. 12, pp.759Â­766, Oct. 1996.
[1.9] S. A. Hutchinson, G. D. Hager, P. I. Corke, "A tutorial on visual servo control," IEEE

Transactions on Robotics and Automation, vol. 12, no. 5, pp. 651Â­670, Oct. 1996.

199

[1.10] G. Chesi, Y.S. Hung, "Image noise induced errors in camera positioning," IEEE

Transaction on Pattern Analysis and Machine Intelligence, vol.29, no. 8, pp. 1476Â­1480, Aug.
2007. [1.11] V. Kyrki, "Control uncertainty in image-based visual servoing," in Proceeding of IEEE

International Conference on Robotics and Automation, Kobe, Japan, pp. 1516Â­1521, May 2009.
[1.12] B. Siciliano, L. Sciavicco, L. Villani, G. Oriolo, Robotics: Modeling, Planning and

Control, Springer-Verlag: London, 2009.
[2.1] L. Quan, Z. Lan, "Linear N-point camera pose determination," IEEE Transactions on

Pattern Analysis and Machine Intelligence, vol. 21, no. 8, pp. 774-780, Aug. 1999.
[2.2] A.H. Zahraee, J.K. Paik, J. Szewczyk, G. Morel, "Toward the development of a hand-held surgical robot for laparoscopy," IEEE/ASME Transactions on Mechatronics, vol. 15, no. 6, pp. 851-863, Dec. 2010. [2.3] Q. He, C. Hu, W. Liu, N. Wei, M.Q.H. Meng, L.Liu, C. Wang, "Simple 3-D point reconstruction methods with accuracy prediction for multiocular system," IEEE/ASME

Transactions on Mechatronics, vol. 18, no. 1, pp. 366-375, Feb. 2013.
[2.4] P.J. Flynn, A.K. Jain, "BONSAI: 3D object recognition using constraint search," IEEE

Transactions on Pattern Analysis and Machine Intelligence, vol. 13, no. 10, pp. 1066-1075, Oct.
1991. [2.5] Y. Shen, D. Sun, Y.H. Liu, K. Li, "Asymptotic trajectory tracking of manipulators using uncalibrated visual feedback," IEEE/ASME Transactions on Mechatronics, vol. 8, no. 1, pp. 8798, March 2003. [2.6] W.J. Wilson, C. Williams Hulls, F. Janabi-Sharifi, "Robust Image Processing and PositionBased Visual Servoing," Chapter in Robust Vision for Vision-Based Control of Motion, Ed. M. Vincze, and G. D. Hager, IEEE Press, New York, NY, pp. 163-201, 2000.

200

[2.7] E. Mouragnon, M. Lhuillier, M. Dhome, F. Dekeyser, P. Sayd, "Monocular vision based SLAM for mobile robots," in Proceedings of IEEE Internation Conference on Pattern

Recognition (ICPR), vol. 3, pp. 1027-1031, Honk Kong, 2006.
[2.8] J.F. Vasconcelos, C. Silvestre, P. Oliveira, "A nonlinear GPS/IMU based observer for rigid body attitude and position estimation," in Proceedings of IEEE Conference on Decision and

Control, pp. 1255-1260, Cancun, Mexico, Dec. 2008.
[2.9] S. Panzieri, F. Pascucci, G. Ulivi, "An outdoor navigation system using GPS and inertial platform," IEEE/ASME Transactions on Mechatronics, vol. 7, no. 2, pp. 134-142, Jun. 2002. [2.10] M.A. Fischler, R.C. Bolles, "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography," Journal of Graphics and Image

Processing, vol. 24, no. 6, pp. 381-395, 1981.
[2.11] W. Forstner, "Reliability analysis of parameter estimation in linear models with applications to mensuration problems in computer vision," Journal of Computer Vision,

Graphics and Image Processing, vol. 40, pp. 273-310, 1987.
[2.12] R.M. Haralick, C. Lee, K. Ottenberg, M. NoÃ lle, "Analysis and solutions of the three point perspective pose estimation problem," in Proceedings of IEEE Conference on Computer

Vision and Pattern Recognition, pp. 592-598, Maui, Hawaii, USA, Jun. 1991.
[2.13] J.J. Wu, R.E. Rink, T.M. Caelli, V.G. Gourishankar, "Recovery of the 3-D location and motion of a rigid object through camera image (an extended Kalman filter approach,"

International Journal of Computer Vision, vol. 2, no. 4, pp. 373-394, Apr. 1989.
[2.14] N. Y. Chen, J. Birk, R. Kelley, "Estimating workpiece pose using the feature points method", IEEE Transactions of Automatic Control, vol. 25, no. 6, pp. 1027-1041, 1980. [2.15] P. D. Fiore, "Efficient linear solution of exterior orientation", IEEE Transactions on

Pattern Analysis and Machine Intelligence, vol. 23, no. 2, pp. 140-148, 2001.

201

[2.16] D. DeMenthon, L. S. Davis, "Exact and approximate solutions of the perspective-three point problem," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 14, no. 11, pp. 1100-1105, Nov. 1992. [2.17] D.G. Lowe, "Fitting parameterized three-dimensional models to images," IEEE

Transactions on Pattern Analysis and Machine Intelligence, vol. 13, no. 5, pp. 441-450, May
1991. [2.18] R. M. Haralick, H. Joo, C. Lee, X. Zhuang, V. G. Vaidya, M. B. Kim, "Pose estimation from corresponding point data,", IEEE Transactions on Systems, Man and Cybernetics, vol. 19, no. 6, pp. 1426-1445, 1989. [2.19] P. L. Rosin, "Robust pose estimation," IEEE Transactions on Systems, Man and

Cybernetics-Part B, vol. 29, no. 2, pp. 297-303, 1997.
[2.20] E. Marchand, F. Chaumette, "Virtual visual servoing: A framework for real-time augmented reality," in Proceedings of Computer Graphics Forum, vol. 21, no. 3, pp. 289-298, SaarebrÃ¼cken, Germany, Sep. 2002. [2.21] A.I. Comport, E. Marchand, M. Pressigout, F. Chaumette, "Real-time markerless tracking for augmented reality: the virtual visual servoing framework," IEEE Transactions on

Visualization and Computer Graphics, vol. 12, no. 4, pp. 615-628, July-Aug. 2006.
[2.22] A.I. Comport, E. Marchand, F. Chaumette, "Robust model-based tracking for robot vision", in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and

Systems, vol. 1, pp. 692-697, Sendal, Japan, Sep. 2004.
[2.23] J.J. Wu, R.E. Rink, T.M. Caelli, V.G. Gourishankar, "Recovery of the 3-D location and motion of a rigid object through camera image (an extended Kalman filter approach,"

International Journal of Computer Vision, vol. 2, no. 4, pp. 373-394, Apr. 1989.
[2.24] C. Fagerer, D. Dickmanns, E. D. Dickmanns, "Visual grasping with long delay time of a free floating object in orbit", Journal of Autonomous-Robots, vol.1, no. 1, pp. 53-68, March 1994.

202

[2.25] W.J. Wilson, C.C.W. Hulls, G.S. Bell, "Relative end-effector control using Cartesian position based visual servoing," IEEE Transactions on Robotics and Automation, vol. 12, no. 5, pp. 684-696, Oct. 1996. [2.26] L. Jetto, S. Longhi, and G. Venturini, "Development and experimental validation of an adaptive extended Kalman filter for the localization of mobile robots," IEEE Transactions on

Robotics and Automation, vol. 15, no. 2, pp. 219Â­229, Apr. 1999.
[2.27] P.S. Maybeck, R.L. Jensen, D.A. Harnly, "An adaptive extended Kalman filter for target image tracking," IEEE Transactions on Aerospace and Electronic Systems, vol. 17, no. 2, pp. 173Â­180, Mar. 1981. [2.28] V. Lippiello, B. Siciliano, L. Villani, "Adaptive extended Kalman filtering for visual motion estimation of 3D objects," Journal of Control Engineering Practice, vol. 15, no. 1, pp. 123Â­134, Jan. 2007. [2.29] Ficocelli, M., and Janabi-Sharifi, F., "Adaptive filtering for pose estimation in visual servoing," in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and

Systems, Maui, Hawaii, pp. 19-24, Oct. 2001.
[2.30] A. Shademan, F. Janabi-Sharifi, "Sensitivity analysis of EKF and iterated EKF pose estimation for position-based visual servoing," in Proceedings of IEEE Conference on Control

Applications., Toronto, Canada, pp. 755Â­780, Aug. 2005.
[2.31] F. Janabi-Sharifi, M. Marey, "A Kalman-filter-based method for pose estimation in visual servoing," IEEE Transactions on Robotics, vol. 26, no. 5, pp. 939Â­947, Oct. 2010. [3.1] E. Malis, G. Morel, F. Chaumette, "Robot control using disparate multiple sensors," The

International Journal of Robotic Research, vol. 20, no. 5, pp. 364-377, May 2001.
[3.2] O. Kermorgant, F. Chaumette, "Multi-sensor data fusion in sensor-based control: application to multi-camera visual servoing," in Proceedings of IEEE International Conference

on Robotics and Automation, pp. 4518-4523, Shanghai, China, May. 2011.

203

[3.3] D. Hershberger, R. Burridge, D. Kortenkamp, R. Simmons, "Distributed visual servoing with a roving eye," in Proceedings of IEEE/RSJ International Conference on Intelligent Robots

and Systems, vol. 1, pp. 441-447, Takamatsu, Japan, Oct. 2000.
[3.4] C. Dune, E. Marchand, C. Leroux, "One click focus with eye-in-hand/eye-to-hand coopration," in Proceedings of IEEE International Conference on Robotics and Automation, pp. 2471-2476, Roma, Italy, Apr. 2007. [3.5] L. Qiu, Q. Song, J. Lei, Y. Yu, Y. Ge, "Multi-camera-based robot visual servoing," in

Proceedings of IEEE International Conference on Mechatronics and Automation, pp. 15091514, Luoyang, China, June 2006. [3.6] N. Garcia-Aracil, C. Perez-Vidal, J.M. Sabater, R. Morales, F. J. Badesa, "Robust and cooperative image-based visual servoing system using a redundant architecture," Journal of

Sensors, vol. 11, pp. 11885-11900, Dec. 2011.
[3.7] A. Y. Mulayim, U. Yilmaz, V. Atalay, "Silhouette-based 3-D model reconstruction from multiple images," IEEE Transactions on Systems, Man and Cybernetics-Part B, vol. 33, no. 4 pp. 582-591, 2003. [3.8] F. Farshidi, S. Sirouspour, T. Kirubarajan, "Active multicamera object recognition in presence of occlusion," In Proceedings of IEEE International Conference on Intelligent Robots

and Systems, pp. 2718-2723, Edmonton, Canada, 2005.
[3.9] J. Stavnitzky, D. Capson, "Multiple camera model-based 3-D visual servo," IEEE

Transactions on Robotics and Automation, vol. 16, no. 6, pp. 732-739, 2000.
[3.10] B. Triggs, P.F. McLauchlan, R.I. Hartley, A.W. Fitzgibbon, "Bundle adjustment-a modern synthesis," Lecture Notes in Computer Science, vol. 1883, pp. 298-375, 2000. [3.11] V. Lippiello, B. Siciliano, L. Villani, "Eye-in-hand/eye-to-hand multicamera visual servoing," in Proceedings of 44th IEEE Conference on Decision and Control, and the European

Control Conference, Seville, Spain, ,pp. 5355-5359, 2005.

204

[3.12] -----, "Position-based visual servoing in industrial multirobot cells using a hybrid camera configuration", IEEE Transactions of Robotics, vol. 23, no. 1, pp. 73-86, 2007. [3.13] S.L. Sun, Z.L. Deng, "Multi-sensor optimal information fusion kalman filter," Journal of

Automatica, vol. 40, no. 6, pp. 1017-1023, 2004.
[3.14] J.B. Gao, C.J. Harris, "Some remarks on kalman filters for multisensor fusion," Journal of

Information Fusion, vol. 3, no. 3, pp. 191-201, 2002.
[3.15] R. C. Luo, C. C. Chang, and C. C. Lai, "Multisensor fusion and integration: theories, applications, and its perspectives," IEEE Sensors Journal, vol. 11, no. 12, pp. 3122Â­3138, Dec. 2011. [3.16] H. Zhao, and Z. Wang, "Motion measurement using inertial sensors, ultrasonic sensors, and magnetometers with extended Kalman filter for data fusion," IEEE Sensors Journal, vol. 12, no. 5, pp. 943Â­953, May 2012. [3.17] M. Carminati, G. Ferrari, R. Grassetti, and M. Sampietro, "Real-time data fusion and MEMS sensors fault detection in an aircraft emergency attitude unit based on Kalman filtering,"

IEEE Sensors Journal, vol. 12, no. 10, pp. 2984-2992, Oct. 2012.
[3.18] M. Hoshino, Y. Gunji, S. Oho, and K. Takano, "A Kalman filter to estimate direction for automotive navigation," in Proceedings of IEEE/SICE/RSJ International Conference on

Multisensor Fusion and Integration for Intelligent Systems, Washington, USA, pp. 145Â­150,
Dec. 1996. [3.19] G. Bleser, and D. Stricker, "Advanced tracking through efficient image processing and visual-inertial sensor fusion," Journal of Computer & Graphics, vol. 33, no. 1, pp. 59Â­72, Feb. 2009. [3.20] V. Sazdovski, and P. Silson, "Inertial navigation aided by vision-based simultaneous localization and mapping," IEEE Sensors Journal, vol. 11, no. 8, pp. 1646Â­1656, Aug. 2011.

205

[3.21] H. Zhao, and Z. Wang, "Motion measurement using inertial sensors, ultrasonic sensors, and magnetometers with extended Kalman filter for data fusion," IEEE Sensors Journal, vol. 12, no. 5, pp. 943Â­953, May. 2012. [3.22] A. Nemra, and N. Aouf, "Robust INS/GPS sensor fusion for UAV localization using SDRE nonlinear filtering," IEEE Sensors Journal, vol. 10, no. 4, pp. 789Â­798, Apr. 2010. [3.23] S. J. Julier, and J. K. Uhlmann, "A new extension of the Kalman filter to nonlinear systems," in Proceedings of SPIE, Signal Processing, Sensor Fusion, and Target Recognition VI, Orlando, Florida, USA, pp. 182Â­193, Apr. 1997. [3.24] Q. Fang, and S. X. Huang, "UKF for integrated vision and inertial sensors based on threeview geometry," IEEE Sensors Journal, vol. 13, no. 7, pp. 2711Â­2719, Jul. 2013. [3.25] W. Li, and H. Leung, "Simultaneous registration and fusion of multiple dissimilar sensors for cooperative driving," IEEE Transactions on. Intelligent Transportation Systems, vol. 5, no. 2, pp. 84Â­98, Jun. 2004. [3.26] S. J. Julier, and J. K. Uhlmann, "Unscented filtering and nonlinear estimation," in

Proceedings of the IEEE, vol. 92, no. 3, pp. 401Â­422, Mar. 2004.
[3.27] J.J. Laviola, "A comparison of unscented and extended Kalman filtering for estimating quaternion motion," in Proceedings of IEEE American Control Conference, Denver, Colorado, USA, pp. 2435Â­2440, Jun. 2003. [3.28] L. Jetto, S. Longhi, and G. Venturini, "Development and experimental validation of an adaptive extended Kalman filter for the localization of mobile robots," IEEE Transactions on

Robotics and Automation, vol. 15, no. 2, pp. 219Â­229, Apr. 1999.
[3.29] P.S. Maybeck, R.L. Jensen, D.A. Harnly, "An adaptive extended Kalman filter for target image tracking," IEEE Transactions on Aerospace and Electronic Systems, vol. 17, no. 2, pp. 173Â­180, Mar. 1981.

206

[3.30] V. Lippiello, B. Siciliano, L. Villani, "Adaptive extended Kalman filtering for visual motion estimation of 3D objects," Journal of Control Engineering Practice, vol. 15, no. 1, pp. 123Â­134, Jan. 2007. [3.31] Ficocelli, M., and Janabi-Sharifi, F., "Adaptive filtering for pose estimation in visual servoing," in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and

Systems, Maui, Hawaii, pp. 19-24, Oct. 2001.
[3.32] Q. Song, Y. He, "Adaptive unscented Kalman filter for estimation of modelling errors for helicopter," in Proceedings of IEEE International Conference on Robotics and Biomimetics, Guilin, China, pp. 2463Â­2467, Dec. 2009. [3.33] Q. Song, J. D. Han, "An adaptive UKF algorithm for the state and parameter estimations of a mobile robot," Journal of Acta Automatica Sinica, vol. 34, no. 1, pp. 72Â­79, Jan. 2008. [3.34] A. Shademan, F. Janabi-Sharifi, "Sensitivity analysis of EKF and iterated EKF pose estimation for position-based visual servoing," in Proceedings of IEEE Conference on Control

Applications., Toronto, Canada, pp. 755Â­780, Aug. 2005.
[3.35] K. Spingarn, "Passive position location estimation using the extended Kalman filter,"

IEEE Transactions on Aerospace and Electronic Systems, vol. 23, no. 4, pp. 557Â­567, Jul. 1987.
[3.36] L. Ling, E. Cheng, I.S. Burnett, "An iterated extended Kalman filter for 3D mapping via Kinect camera," in Proceedings of IEEE International Conference on Acoustics, Speech, and

Signal Processing., Vancouver, Canada, pp. 1773Â­1777, May. 2013.
[3.37] R. Zhan, J. Wan, "Iterated unscented Kalman filter for passive target tracking," IEEE

Transactions on Aerospace and Electronic Systems, vol. 43, no. 3, pp. 1155Â­1163, Jul. 2007.
[3.38] M. Zhong-Kai, S. Li-Fen, "Improvement of UKF algorithm and robustness study," in

Proceedings of IEEE International Workshop on Intelligent Systems and Applications, Wuhan,
China, pp. 1Â­4, May. 2009.

207

[3.39] Y. A. Zhang, D. Zhou, G. R. Duan, "An adaptive iterated Kalman filter," in Proceedings

of IEEE Multiconference on Computational Engineering in Systems Applications, Beijing,
China, pp. 1727-1730, Oct. 2006. [3.40] F. Janabi-Sharifi, M. Marey, "A Kalman-filter-based method for pose estimation in visual servoing," IEEE Transactions on Robotics, vol. 26, no. 5, pp. 939Â­947, Oct. 2010. [3.41] D.J. Jwo, S. H. Wang, "Adaptive fuzzy strong tracking extended Kalman filtering for GPS navigation," IEEE Sensors Journal, vol. 7, no. 5, pp. 778Â­789, May 2007. [3.42] J. Z. Sasiadek, Q. Wang, M. B. Zeremba, "Fuzzy adaptive Kalman filtering for INS/GPS data fusion," in Proceedings of IEEE Internationa Symposium on Intelligent Control, Rio Patras, Greece, pp. 181Â­186, July 2000. [3.43] R. van der Merwe, E. A. Wan, S. J. Julier, "Sigma-point Kalman filters for nonlinear estimation and sensor fusion: Applications to integrated navigation," in Proceedings of AIAA

Guidance, Navigation, and Control Conference Exhibition, Providence, RI, AIAA 2004-5120,
Aug. 2004. [3.44] Y. Gao, W. J. Jia, X. J. Sun, Z. L. Deng, "Self-tuning multisensor weighted measurement fusion Kalman filter," IEEE Transactions on Aerospace and Electronic Systems, vol. 45, no. 1, pp. 179Â­191, Jan. 2009. [3.45] F. Janabi-Sharifi, M. Ficocelli, "Formulation of radiometric feasibility measures for feature selection and planning in visual servoing," IEEE Transactions on System, Man, and

Cybernetics-Part B, vol. 34, no. 2, pp. 978-987, 2004.
[3.46] A.S. Willsky, "A survey of design method for failure detection in dynamic systems,"

Journal of Automatica, vol. 12, pp. 601-611, 1976.
[4.1] M. De Santo, C. Liguori, A. Pietrosanto, "Uncertainty characterization in image-based measurements: a preliminary discussion", IEEE Transaction on Instrumentation and

Measurement, vol.49, no. 5, pp. 1101Â­1107, 2000.

208

[4.2] G. Chesi,Y.S. Hung, "Image noise induced errors in camera positioning," IEEE

Transaction on Pattern Analysis and Machine Intelligence, vol.29, no. 8, pp. 1476Â­1480, 2007.
[4.3] G. Chesi, H.L. Yung, "Performance limitation analysis in visual servo systems: bounding the location error introduced by image points matching," in Proc. IEEE International Conference

on Robotics and Automation, Kobe, Japan, pp. 695Â­700, 2009.
[4.4] G. Chesi, "Optimal object configuration to minimize the positioning error in visual servoing," IEEE Transactions on Robotics, vol. 26, no. 3, pp. 584Â­589, 2010. [4.5] -------, "Visual servoing path-planning via homogeneous forms and LMI optimizations,"

IEEE Transactions on Robotics, vol.25, no.2, pp 281Â­291, 2009.
[4.6] -------, "LMI conditions for time-varying uncertain systems can be non-conservative,"

Journal of Automatica, vol. 47, pp. 621-624, 2011.
[4.7] V. Kyrki, D. Kragic, H.I. Christensen, "Measurement errors in visual servoing," inProc.

IEEE International Conference on Robotics and Automation, vol. 2, New Orleans, LA, USA,
2004, pp. 1861Â­1867. [4.8] V. Kyrki, D. Kragic, and H. Christensen, "Measurement errors in visual servoing,"

Robotics and Autonomous Systems, vol. 54, no. 10, pp. 815Â­827, 2006.
[4.9] E. Malis, F. Chaumette, and S. Boudet, "2-1/2-D visual servoing," IEEE Transaction on

Robotics and Automation, vol. 15, no. 2, pp. 238Â­250, Oct. 1999.
[4.10] R.M. Haralick, "Propagating covariance in computer vision," International Journal of

Pattern Recognition and Artificial Intelligence, vol.10, no.5, pp 561Â­572, 1996.
[4.11] V. Kyrki, "Control uncertainty in image-based visual servoing," In Proceeding of IEEE

Internatonal Conference on Robotics and Automation, Kobe, Japan, pp. 1516Â­1521, 2009.
[4.12] A. Chao, M. Athans, "Stability robustness to unstructured uncertainty for linear time invariant systems," The Control Handbook (Editor, W.S. Levine), CRC Press and IEEE Press, 1996. 209

[4.13] J. Raisch, B. Francis, "Modeling deterministic uncertainty," The Control Handbook (Editor, W.S. Levine), CRC Press and IEEE Press, 1996. [4.14] B. Siciliano, L. Sciavicco, L. Villani, G. Oriolo, Robotics: Modeling, Planning and

Control, Springer, Verlag, London, 2009.
[5.1] Y. Mezouar, F. Chaumette, "Path planning for robust image-based control," IEEE

Transactions on Robotics and Automation, vol. 18, no. 4, pp. 534Â­549, Aug. 2002.
[5.2] M. Kazemi, K. Gupta, M. Mehrandezh, "Global path planning for robust visual servoing in complex environments," in Proceeding of IEEE International Conference on Robotics and

Automation, Kobe, Japan, pp. 326Â­332, May, 2009.
[5.3] G. Chesi, "Visual servoing path-planning via homogeneous forms and LMI optimizations,"

IEEE Transactions on Robotics, vol.25, no.2, pp. 281Â­291, Apr. 2009.
[5.4] J.A. Ganglo, M.F. De Mathelin, "High speed visual servoing of a 6 DOF manipulator using multivariable predictive control," Journal of Advanced Robotics, vol. 17, no. 10, pp. 993Â­1021, 2003. [5.5] M. SauvÂ´ee, P. Poignet, E. Dombre, E. Courtial, "Image based visual servoing through nonlinear model predictive control," in Proceedings of 45th IEEE Conference on Decision and

Control, San Diego, California, USA, pp.1776Â­1781, Dec. 2006.
[5.6] G. Allibert, E. Courtial, F. Chaumette, "Predictive control for constrained image-based visual servoing," IEEE Transactions on Robotics, vol. 26, no. 5, pp. 933Â­939, Oct. 2010. [5.7] C. Lazar, A. Burlacu, "Visual servoing of robot manipulators using model-based predictive control," in Proceedings of IEEE International Conference on Industrial Informatics, Cardiff, Wales, pp. 690Â­695, Jun. 2009. [5.8] C. Lazar, A. Burlacu, C. Copot, "Predictive control architecture for visual servoing of robot manipulators," in Proceedings of 18th IFAC Congress, Milan, Italy, pp. 9464Â­9569, Aug. 2011.

210

[5.9] A. Chan, S. Leonard, E.A. Croft, J.J. Little, "Collision-free visual servoing of an eye-inhand manipulator via constraint-aware planning and control," in Proceedings of American

Control Conference, San Francisco, California, USA, pp. 4642Â­4648, Jun. 2011.
[5.10] T. Wang, W.Xie, G. Liu, Y. Zhao, "Quasi-min-max model predictive control for imagebased visual servoing," in Proceedings of IEEE/ASME International Conference on Advanced

Intelligent Mechatronics, Kaohsiung, Taiwan, pp. 98Â­103, July 2012.
[5.11] C. Copot, C. Lazar, A. Burlacu, "Predictive control of nonlinear visual servoing systems using image moments," IET Control Theory and Applications, vol. 6, no. 10, pp. 1486Â­1496, July 2012. [5.12] H. Wang, M. Jiang, W. Chen, Y.H. Liu, "Visual servoing of robots with uncalibrated robot and camera parameters," Journal of Mechatronics, vol. 22, no. 6, pp. 661Â­668, Sept. 2012. [5.13] R. Kelly, "Robust asymptotically stable visual servoing of planar robots," IEEE

Transations on Robotics and Automation, vol. 12, no. 5, pp. 759Â­776, Oct. 1996.
[5.14] G. Morel, P. Zanne, F. Plestan, "Robust visual servoing: bounding the task function tracking errors," IEEE Transactions on Control Systems Technology, vol. 13, no. 6, pp. 998Â­ 1009, Nov. 2005. [5.15] C.S. Kim, E.J. Mo, S.M. Han, M.S. Jie, K.W. Lee, "Robust visual servo control of robot manipulators with uncertain dynamics and camera parameters," International Journal of Control,

Automation and Systems, vol. 8, no. 2, pp.308Â­313, Apr. 2010.
[5.16] G. Hu, N. Gans, and W. Dixon, "Quaternion-based visual servo control in the presence of camera calibration error," Intertional Journal of Robust and Nonlinear Control, vol. 20, no. 5, pp. 489Â­503, Mar. 2010. [5.17] E. Zergeroglu, D.M. Dawson, M.S. de Queiroz, P. Setlur, "Robust visual-servo control of robot manipulators in the presence of uncertainty," Journal of Robotic Systems, vol. 20, no. 2, pp. 93Â­106, Feb. 2003.

211

[5.18] C.S Kim, K.W. Lee, "Image-based robust control of robot manipulators using dynamic compensator," in Proceeings of American Control Conference, Baltimore, Maryland, USA, pp. 5266Â­5271, June 2010. [5.19] M.U. Khan, I. Jan, N. Iqbal, "Robustness analysis of uncalibrated eye-in-hand visual servo system in the presence of parametric uncertainty," Industrial Robot: an International Journal, vol. 39, no. 2, pp. 154 Â­161, 2012. [5.20] M.C. Chien, A.C. Huang, "Design of a Fat-based adaptive visual servoing for robots with time varying uncertainties," International Journal of Optomechatronics, vol. 4, no. 2, pp. 93Â­ 114, Jun. 2010. [5.21] D.H. Park, J.H. Kwon, I.J. Ha, "Novel position-based visual servoing approach to robust global stability under field-of-view constraint," IEEE Transactions on Industrial Electronics, vol. 59, no. 12, pp. 4735Â­4752, Dec. 2012. [5.22] H. M. Becerra, C. Sagues, "A sliding mode control law for epipolar visual servoing of differential-drive robots," in Proceedings of IEEE/RSJ International Conference on Intelligent

Robots and Systems, Nice, France, pp. 358Â­363, Sept. 2008.
[5.23] T.R. Oliveira, A.J. Peixoto, A.C. Leite, L. Hsu," Sliding mode control of uncertain multivariable nonlinear systems applied to uncalibrated robotics visual servoing," in Proceedings

of American Control Conference, St. Louis, Missouri, USA, pp. 71Â­76, Jun. 2009.
[5.24] T.R. Oliveira, A.J. Peixoto, L. Hsu, "Sliding mode control of uncertain multivariable nonlinear systems with unknown control direction via switching and monitoring function," IEEE

Transactions on Automatic Control, vol. 55, no. 4, pp. 1028Â­1034, Apr. 2010.
[5.25] X. Liang, X. Huang, M. Wang, X. Zeng, "Adaptive task-space tracking control of robots without task-space- and joint-space-velocity measurements," IEEE Transactions on Robotics, vol. 26, no. 4, pp. 733Â­742, Aug. 2010. [5.26] M.G. Ortega, M. Vargas, F.R. Rubio, "H controller for a visual servoing system," in

Proceedings of European Control Conference, Karlsruhe, Germany, pp. 217Â­220, 1999.

212

[5.27] A.I. Comport, E. Marchand, F. Chaumette, "Statistically robust 2-D visual servoing",

IEEE Transactions on Robotics, vol. 22, no. 2, pp. 416Â­421, Apr. 2006.
[5.28] D. Bellot, P. Danes, "Handling visual servoing schemes through rational systems and LMIs," in Proceedings of IEEE Conference on Decision and Control, Orlando, Florida, USA, pp. 3601Â­3606, Dec. 2001. [5.29] D. Bellot, P. Danes, "An LMI solution to visual-based localisation as the dual of visual servoing," in Proceedings of IEEE Conference on Decision and Control, Maui, Hawaii, USA, pp. 5420Â­5425, Dec. 2003. [5.30] N. P. Papanikolopoulos, P.K. Khosla, T. Kanade, "Visual tracking of a moving target by a camera mounted on a robot: a combination of control and vision," IEEE Transactions on

Robotics and Automation, vol. 9, no. 1, pp. 14Â­35, Feb 1993.
[5.31] K. Hashimoto, T. Ebine, H. Kimura, "Visual servoing with hand-eye manipulator-optimal Control approach," IEEE Transactions on Robotics and Automation, vol. 12, no. 5, pp. 766Â­774, Oct. 1996. [5.32] E. Malis, F. Chaumette, S. Boudet, "2-1/2-D visual servoing," IEEE Transactions on

Robotics and Automation, vol. 15, no.2, pp.238Â­250, Apr. 1999.
[5.33] F. Chaumette, S. Hutchinson, "Visual servo control I: Basic approaches," IEEE Robotics

and Automation Magazine, vol. 13, no. 4, pp. 82Â­90, Dec. 2006.
[5.34] B. Siciliano, L. Sciavicco, L. Villani, G. Oriolo, Robotics: Modeling, Planning and

Control, pp.128Â­132, Springer, Verlag, London, 2009.

213

