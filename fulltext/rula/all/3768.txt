Discovering Access Holes in Disaster Rubble with Functional and Photometric Attributes

by

Christopher Kong Bachelor of Science, Ryerson, 2009

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science

Toronto, Ontario, Canada, 2015 c Christopher Kong 2015

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

iii

Discovering Access Holes in Disaster Rubble with Functional and Photometric Attributes Master of Science 2015 Christopher Kong Computer Science Ryerson University

Abstract
The collapse of buildings often result in human victims becoming trapped within rubble. This environment is dangerous for emergency first responders tasked with locating and extricating victims. Recent work in scene mapping using photometric colour and metric depth (RGB-D) data suggest the possibility of automatically identifying potential access holes into rubble interior. This capability would improve search operation by directing limited resources to be concentrated on areas where access holes might exist. This thesis presents an approach to automatically identify access holes in rubble. The investigation begins by defining access holes in terms of their functional utility, that allow for their algorithmic identification. From this definition, a set of hole-related features extracted from RGB-D imagery are proposed for detection. Experiments were conducted using data collected over a real-world disaster training facility. Empirical evaluation indicates the efficacy of the proposed system for successfully identifying potential access holes in disaster rubble scenes.

v

Acknowledgements
I express my gratitude to my supervisors Dr. Alex Ferworn, Dr. Konstantinos G. Derpanis and Dr. James Elliott Coleshill for their direction, guidance and inspiration. I thank everyone in the N-CART research lab and specifically thank Jimmy Tran for his knowledge and experience. Finally, I thank my family. I give special thanks to my wife Ping Tam, because without her love and support none of this would be possible.

vii

Contents
Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii v xi

Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii 1 Introduction 1.1 1.2 1.3 1.4 1.5 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Problem Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Outline of Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 3 4 6 6 9 9

2 Background 2.1 2.2 2.3 2.4 Disasters and Emergency Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 2.3.1 2.4.1 2.4.2 2.4.3 2.5 2.5.1 2.5.2

Urban Search and Rescue (USAR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Triage and Prioritizing Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Survival Under Rubble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Ground-based Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Aerial Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Appearance and Geometry-based Detection . . . . . . . . . . . . . . . . . . . . . . 22 Functional Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 27 Rubble, Voids and Access Holes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

Visual Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

3 Technical Approach 3.1 3.2 3.1.1

Access Hole Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Naive and Conservative Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Access Hole Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 ix

3.2.1 3.2.2 3.2.3 3.3

Depth Disparity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Access Hole Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Photometric Appearance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 35

4 Experimental Results 4.1 4.2 4.3

System Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.3.1 Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 4.3.2 4.3.3 Ablative Analysis of Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

4.4

Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 45

5 Summary and Conclusions 5.1 5.2 5.3

Summary of Findings and Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 Limitations and Restrictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 Directions for Future Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

Bibliography Glossary

49 56

x

List of Tables
4.1 4.2 4.3 Average precision (AP) for a range of target superpixel values . . . . . . . . . . . . . . . . 38 Average precision (AP) for individual feature scores . . . . . . . . . . . . . . . . . . . . . 39 Comparison of average precision (AP) for geometric features, photometric features and all features combined . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

xi

List of Figures
1.1 1.2 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 3.1 3.2 3.3 4.1 4.2 4.3 4.4 Examples of real-world building collapses and disasters . . . . . . . . . . . . . . . . . . . . A real-world building collapse in Savar, Bangladesh . . . . . . . . . . . . . . . . . . . . . . 2 5

Canada's Emergency Management Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Common types of structural collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A ground robot utilizing tracks for locomotion . . . . . . . . . . . . . . . . . . . . . . . . 18 A Hex-copter UAV outfitted with an RGB-D sensor package . . . . . . . . . . . . . . . . . 19 The Asus Xtion RGB-D sensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Sample images extracted from an RGB-D camera . . . . . . . . . . . . . . . . . . . . . . . 23 Examples of access holes in disaster rubble . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Examples of a variety of chairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Detection system architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Examples of under- and over-segmented depth images . . . . . . . . . . . . . . . . . . . . 30 Segmentation and neighbourhood discovery . . . . . . . . . . . . . . . . . . . . . . . . . . 32 The OPP reference rubble pile, located in Bolton, Ontario, Canada . . . . . . . . . . . . . 36 Precision-Recall (PR) for overall detection system . . . . . . . . . . . . . . . . . . . . . . 40 Sample output of the access hole detection approach . . . . . . . . . . . . . . . . . . . . . 42 Sample RGB and depth image pairs from the introduced dataset . . . . . . . . . . . . . . 43

xiii

Chapter 1

Introduction

1.1

Motivation

From the moment humanity started using material to construct structures for shelter, those same materials have posed dangers when they collapse with people inside. As buildings became more complex and were more densely packed, we formed cities that compounded the problem when those buildings collapsed. When buildings collapse on a scale that the aftermath is beyond the capability of local emergency services to cope, the result is an urban disaster. Disasters involving collapsed buildings in urban areas occur for a variety of reasons including natural and human-influenced. Due to the increased population density in these areas, the likelihood of humans becoming trapped (entombed) in the resultant building rubble is quite high. Examples of this type of disaster are not hard to find and include the 2001 World Trade Center collapses due to terrorist attacks in the United States (Casper and Murphy, 2003), the 2009 earthquake in Haiti (Yates and Paquette, 2011), the 2011 Tohoku earthquake and tsunami off the coast of Japan (Nakahara, 2011), the 2012 Algo Centre Mall collapse in Elliot Lake, ON caused by rust (Belanger, 2014), and the 2013 Rana Plaza collapse in Savar, Bangladesh (Alamgir et al., 2014). Each of these examples, shown in Figure 1.1, have resulted in thousands of people killed outright, severely 1

1.1. MOTIVATION

CHAPTER 1. INTRODUCTION

Figure 1.1: Examples of human-influenced and natural disasters. (from left-to-right) 2009 earthquake in Haiti, 2011 Tohoku tsunami, 2012 Algo Centre Mall collapse, debris from the 2001 World Trade Center attack, and the 2013 building collapse in Savar, Bangladesh.

injured and traumatized. In response to these events, organized teams of emergency first responders with specialized training and equipment, called Urban Search and Rescue (USAR) Task Forces (TF), are deployed to locate victims, medically stabilize and extricate victims to send them to hospitals and other second line facilities (FEMA, 2009). Victims trapped in rubble will inevitably perish if they are not found in a timely manner. When rescue personnel perform triage on a collapsed structure, they first determine areas that are likely to contain trapped victims and then form a plan to access those structures' interiors. If access holes already exist, these will be evaluated before rubble removal is considered to save time and reduce the chances of creating secondary collapses (FEMA, 2009). Two critical factors in the response planning process are the considerations of time and an understanding of what is actually happening at the scene, i.e., situational awareness. The likelihood of finding and extracting a survivor is dependent on the amount of time that passes. Anything that safely decreases the delay required to locate a survivor can lead to an increase in lives saved (Macintyre et al., 2006). A USAR incident is inevitably a dangerous and chaotic environment, often containing elements that pose threats to the safety of the responders working in it. It is important that any tools responders use decrease the search time required to find trapped survivors and also decrease the risk of injury to themselves. 2

CHAPTER 1. INTRODUCTION

1.2. PROBLEM DEFINITION

When a building collapses, the resulting rubble often contains areas where intact pieces of debris resist the crushing force of the load, and act as shelters for the area under them. These areas are called voids and are prime locations to contain survivors. One of the first actions of structural engineers is to identify likely locations of voids and target them as key areas to search for survivors. Searching voids is a dangerous task because the stability of the rubble pile is largely unknown. Typical methods of finding entry into these areas involve visually inspecting the scene for "access holes" that may lead to subsurface voids; however, interacting with the rubble pile at this point can trigger secondary collapses that put victims and responders at risk of injury or even death. This thesis proposes a novel vision-based approach that potentially extends the situational awareness of first responders and reduces the time necessary for locating these access holes in USAR environments within the domain of Computational Public Safety (CPS). This approach allows responders to perform a subset of the inspection task before they arrive at an incident, removing them from direct danger and potentially speeding up rescue efforts.

1.2

Problem Definition

The terms "hole" or "access hole" are not clearly defined within the USAR nomenclature. The challenge lies in the amorphous nature of holes (e.g., the lack of prototypical shape, depth and orientation), thus a definition is left open to the interpretation of each search team. To compound the difficulty of this problem, disaster rubble typically contains many irregularities within the rubble pile. For instance, inconsistency in the size, shape and types of the material constituting the rubble affect what is and is not considered to be a candidate entry hole. Figure 1.2 shows an example of a rubble scene with multiple access holes that can potentially be used for access to the rubble's interior. An access hole can be defined through its intended use. The goal is to locate holes that are sufficiently large to permit human entry. In this way, an access hole is defined in the context of its functional utility 3

1.3. OBJECTIVES

CHAPTER 1. INTRODUCTION

for search and rescue. This is analogous to the functional object recognition paradigm pursued in computer vision (Dickinson, 2009) that models objects, such as chairs, in terms of their function, i.e., their ability to support a human, rather than the particulars of their appearance. The instability of rubble can prevent USAR teams from safely traversing it to find access holes. This situation has led researchers to investigate ways to minimize risk to human searchers, through the use of unmanned vehicles (Murphy, 2004; Birk et al., 2011; Finn and Wright, 2012; Ferworn et al., 2011; Onosato et al., 2006). Previous work (Ferworn et al., 2011) demonstrated the ability to equip a UAV with a low-cost, off-the-shelf camera sensor that captures colour imagery with per-pixel metric depth information (i.e., an RGB-D sensor) to survey disaster scenes from a safe distance. This information can then be used to generate a scene-level model that can quickly provide first responders with important details about the structure of the rubble (Ferworn et al., 2011) and provide input to an automated hole detection system, as pursued in this thesis. In addition to the challenges posed by dealing with large amounts of visual data, traumatic events can overwhelm an individual, such as a building collapse disaster. This can lead to critical incident stress that can impair the ability of personnel to function and perform tasks involving detailed observation required for visual search (FEMA, 2009). This thesis argues that a system that automates the identification of access holes potentially reduces the cognitive load faced by response personnel.

1.3

Objectives

Current approaches for identifying access holes rely on visual inspection by first responders. If responders are precluded from entering the scene or not yet present, they must rely on imagery as their main source of information. The ability to collect data far outpaces a human's ability to deal with it. A large disaster field would quickly tax a trained human observer. Assuming a human operator inspects data collected with an RGB-D sensor, the amount of time required to review a video dataset would need to be equal 4

CHAPTER 1. INTRODUCTION

1.3. OBJECTIVES

Figure 1.2: An example of a real-world building collapse in Savar, Bangladesh. Holes into the rubble, highlighted in green, may help first responders find trapped survivors.

to or greater than the time taken to record it. While an operator is capable of manually inspecting each image for access holes, a large disaster field would render this task extremely difficult and prohibitively time consuming. The ability to assess an urban disaster scene with respect to the likely location of hidden human survivors must be done quickly to increase the chances of finding victims alive within the structures formed by the rubble. Finding and accessing victims can be a time consuming process, fraught with danger as the structures are not always stable and are subject to secondary collapse. One way of determining how to gain access to a rubble structure is through the identification of access holes into the structure. Due to the unstable nature of disaster rubble, it is better to find an existing access hole than to make a new one because of the dangers of secondary collapse. This thesis presents a novel visionbased approach for automatically detecting access holes in disaster rubble imagery without unnecessarily exposing disaster workers to the dangers of rubble structures, and potentially accelerating the structural 5

1.4. CONTRIBUTIONS triage process (see Chapter 2, Section 2.2).

CHAPTER 1. INTRODUCTION

This thesis examines the case of holes leading into subsurface voids to demonstrate the proof-ofconcept. The approach is naive by assuming an access hole possesses clearly marked boundaries and a salient depth variation from the surrounding area. In addition, the potential access hole possesses a minimum width and aspect ratio to accommodate the entry of an adult human searcher. It is conservative by limiting detections of access holes to the proposed functional definition. While the focus in the current work is on human searchers, other types of search entities are readily accommodated, such as search dogs or robots.

1.4

Contributions

In light of the previous work, this thesis makes three contributions. First, a novel definition of an access hole is presented based on a set of features derived from the functional form and photometric characteristics in collapsed structure. Second, a novel approach is developed to identify access holes in collapsed structures to be used by USAR personnel in accessing the collapsed structure. Analysis is performed on aerial imagery obtained by a UAV outfitted with an RGB-D sensor to identify candidate access holes. Third, this thesis is the first to introduce a publicly available dataset obtained from a real-world USAR training rubble pile, where access holes are manually provided as ground truth. Quantitative empirical evaluation on the introduced dataset indicates the potential of the proposed approach for successfully identifying access holes in disaster rubble scenes.

1.5

Outline of Thesis

The remainder of this thesis is structured in the following way. Chapter 2 provides background information related to this thesis, including an overview of emergency management with respect to USAR and a survey of related work on data acquisition methods and image recognition methods. Chapter 3 presents 6

CHAPTER 1. INTRODUCTION

1.5. OUTLINE OF THESIS

the proposed approach by providing an operating definition of an access hole and describes the access hole detection algorithm. Chapter 4, documents the introduced dataset used in evaluating the proposed approach, the experimental evaluation of a software implementation of the proposed approach, and the results of empirical evaluation. Finally, Chapter 5 presents a summary of this thesis, with a discussion of limitations and directions for future research.

7

Chapter 2

Background
This chapter presents background information and related work drawn from the literature. This chapter begins with an introduction to Disaster Emergency Management (Section 2.1) and Urban Search and Rescue (Section 2.2). While knowledge of these domains is not paramount to understanding the concept of access hole detection or the proposed detection system, it is important to understand the context within which the work is applied, i.e., within the field of computational public safety. Next, an introduction to data acquisition, in particular, response/rescue robotics with respect to their sensing capabilities. Finally, this chapter concludes with a review of visual object detection approaches.

2.1

Disasters and Emergency Management

In the context of emergency management, the terms hazard, emergency and disaster have very specific meanings (Lindell et al., 2006). A hazard is an event or situation with the capability of endangering human life, property, or the environment in a particular location (Lindell et al., 2006). A hazard represents the potential for damage, rather than the damage itself. An emergency can have two different meanings, depending on the particular context that it is used 9

2.1. DISASTERS AND EMERGENCY MANAGEMENT

CHAPTER 2. BACKGROUND

(Lindell et al., 2006). The first application is applied to minor emergencies, where there may only be a limited amount of casualties or property damage. The second definition refers to an impending calamitous event. This creates an emergency situation where there is very little time to respond; however, the consequences of the event are likely to be major. The situation warrants a co-ordinated response from local emergency services, such as fire, police and emergency medical services (EMS). In this thesis, emergency refers to the latter definition. A disaster is a major destructive event that cannot be managed with the resources of a single community (Lindell et al., 2006). A disaster may result in the loss of human life, destruction of property or environmental devastation. An affected community must reach out to larger jurisdictions and communities for assistance, often at the regional or national level. The immediate activity following the onset of a disaster incident is called response and is intended to initiate recovery as quickly as possible. To effectively react to disasters, response agencies plan for disasters by developing action plans that mitigate potential risks, allowing for appropriate reaction to events, and provide continuity of operations and recovery. To effectively deal with the many types of disasters requires specialized expertise and management ability. Response agencies typically employ professional emergency managers to manage the copious effort required to mount the response effort and mitigate the negative impacts of the incident (McEntire, 2007). The Emergency Management Cycle represents a strategy for minimizing risk pre-, during and postdisasters (Environment Canada, 2014). Recognizing and taking appropriate actions at each stage of the cycle allows for greater awareness, preparedness and reduction in loss of life. Preparedness can also help mitigate and prevent future disasters. Most jurisdictions recognize the following four stages in the emergency management cycle, shown in Figure 2.1:

· Preparedness focuses on learning from past disasters, and outlining strategies. Specific action plans are prepared in advance of a disaster occurrence. 10

CHAPTER 2. BACKGROUND

2.1. DISASTERS AND EMERGENCY MANAGEMENT

Mitigation

Recovery

Emergency Management

Preparedness

Response
Figure 2.1: A diagram of the Emergency Management Cycle, reproduced from Canada's Emergency Management Basics (Environment Canada, 2014).

· Response represents the realization of the mitigation and preparedness phase. It includes allocating necessary emergency services and first responders to a potential disaster to protect human life and reduce property damage.

· Recovery begins once the threat to human life has been removed. Attempts to restore an affected area to normality by focusing on rebuilding and repairing infrastructure.

· Mitigation focuses on long-term reduction or removal of risk by attempting to prevent or reduce the severity of disasters.

The term "first responder" generally refers to personnel who respond to an emergency situation first. During a physical event, such as a natural disaster or similar events, police, fire and/or EMS are immediately deployed to an affected area during the response phase. It is during this phase that the potential for loss of life is at its highest. Depending on the scale of a physical disaster, a local response may be adequate. If a disaster is large enough, specialized response teams are deployed to assist. This 11

2.2. URBAN SEARCH AND RESCUE (USAR)

CHAPTER 2. BACKGROUND

thesis proposes a novel approach with the goal of both increasing the situational awareness of these responders and reducing the time needed to formulate response plans.

2.2

Urban Search and Rescue (USAR)

In urban environments, the structural collapse of buildings may result in people becoming trapped in the resulting rubble. The term for these trapped people is "victims" or "patients". USAR involves the detection, extrication and medical stabilization of these victims and is used to describe a specialized group of skills and activities related to Search and Rescue (SAR) operations in urban centres dominated by numerous buildings in close proximity (Emergency Management Australia, 2004). This specialization requires a multi-agency response that is beyond the capability of normal emergency service organizations. Teams of first responders, often referred to as Task Forces (TFs), are typically organized as regional or national resources that are available for quick deployment when required. USAR TFs typically consist of fire, police, EMS, doctors, structural engineers, radio operators, canine handlers, and heavy equipment operators (Murnane and Fortney, 2003). When a disaster is formally declared, a USAR TF may be activated and dispatched to assist in the response to it. The components that make up a TF are:

· Search members are responsible for locating survivors.

· Rescue workers are responsible for extricating trapped survivors.

· Technical crew specialize in inspecting and reinforcing structural components, operating specialized equipment and heavy machinery, and providing communication services.

· Medical personnel provide any medical care as required (including to other members of the TF).

In Canada and the United States, TFs are classified into the following three levels based on their capability (Public Safety Canada, 2014): 12

CHAPTER 2. BACKGROUND

2.2. URBAN SEARCH AND RESCUE (USAR)

· Light USAR TFs respond to incidents within a single jurisdiction for a single operational shift (up to 12 hours). They are trained and equipped to search and stabilize within structural wood systems, light metal components and un-reinforced masonry.

· Medium USAR TFs respond within mutual aid boundaries for an operation time period of up to one day (24 hours). They are equipped and trained to search and stabilize all collapsed structures and can conduct USAR operations involving heavy timber and reinforced masonry.

· Heavy USAR (HUSAR) TFs respond to incidents nationally with an initial sustained operation period of up to ten days, with resupply every three days thereafter. They are equipped and trained to search and stabilize all collapses and include structural engineering components.

2.2.1

Triage and Prioritizing Search

Responding organizations must prioritize locations where finding and rescuing victims has the highest probability for success. This assessment of initial reconnaissance, known as triage, is performed either by local emergency response organizations or USAR TFs. Triage must be performed by qualified and experienced individuals who usually have significant structural engineering and search experience. Possible voids (further discussed in Section 2.3) or access holes into the rubble are noted during this initial inspection for use during the later search operations. This initial size-up involves traversing through the debris field and individually identifying buildings and evaluating them for priority. By automating the process of identifying access holes, as is pursued in this thesis, the search space of the debris field is reduced, freeing trained TF members to perform other tasks. Once a building has been triaged, a search plan is built around potential entry points, known victim locations, hazards, and potential egress routes. Search teams are deployed with various strategies. Canine teams (consisting of a trained USAR dog and its human handler) are employed to detect the human scent of victims trapped and hidden within rubble. The dog signals the location of victims by standing 13

2.3. RUBBLE, VOIDS AND ACCESS HOLES

CHAPTER 2. BACKGROUND

and barking in the vicinity of the strongest scent (International Fire Service Training Association, 2005). Sound-sensing devices, such as microphones, can be used to listen for trapped survivors who may be calling, breathing, tapping or otherwise producing sounds (FEMA, 2006). Recently, Ground Penetrating Radar (GPR) has demonstrated the potential for locating victims by remotely sensing heart beats (Crocco and Ferrara, 2014); however, these approaches require placing TF members in direct contact with the disaster scene hazards. Employing unmanned vehicles to remotely survey rubble, as considered in this thesis, can remove trained responders from this hostile environment. During rescue operations, survivors who can be extracted easily are assisted first. Those survivors who are trapped beneath debris require additional effort since material, such as concrete and metal, must be carefully removed without causing secondary collapses. The automatic identification of areas that can likely be accessed without further removal of debris provides TFs with access paths that can be further prioritized by search specialists. A disaster scene often exposes first responders to traumatic stimuli and contact with dead or injured casualties. Studies have shown that contact with these kinds of experiences may overwhelm the ability for a responder to cope, leading to critical incident stress (Harris et al., 2002). By remotely inspecting a disaster scene, the approach proposed in this thesis removes TF members from direct contact with these stressors. This can potentially reduce the cognitive load placed on search specialists.

2.3

Rubble, Voids and Access Holes

Rubble characterization is a difficult problem. There have been several investigations attempting to contribute solutions to this problem (Molino et al., 2007; Lombillo et al., 2013; Binda et al., 2001; Onosato et al., 2012); however, there is no universally accepted categorization method for rubble. These early attempts to characterize rubble are unlikely to lead to better operational techniques for finding trapped people faster. While characterising the semantics and physical characteristics of rubble is beyond 14

CHAPTER 2. BACKGROUND

2.3. RUBBLE, VOIDS AND ACCESS HOLES

Void Void Void Void

Void Void Void

Figure 2.2: Examples of common types of structural collapse. Clockwise from top left: A-frame collapse, Pancake collapse, V-shaped collapse and a Lean-to collapse. the scope of this thesis, a specific sub-problem of rubble characterization is addressed, namely, structures formed by its absence. Components within newly created rubble can form spaces that may provide havens where victims can temporarily survive. These spaces are known within USAR terminology as "voids" (FEMA, 2012). A common way for TFs to survey rubble for areas that house potential voids is to exploit patterns in the way buildings typically collapse. Examples of the forms that newly collapsed structures often take are (shown in Figure 2.2):

· V-shaped floor collapse occurs when the exterior walls remain intact, with the upper floors failing in the middle. Void spaces are found below the characteristic V-pattern caused by this type of collapse.

· Pancake collapse form when all the exterior walls of a building fail simultaneously, resulting in 15

2.3. RUBBLE, VOIDS AND ACCESS HOLES

CHAPTER 2. BACKGROUND

floors that stack flatly on top of each. The name is given due to the resemblance to a stack of pancakes.

· Lean-to floor collapse is the result of a single outer wall failing while leaving the other intact. The floor on the side that is no longer supported falls to form a triangular collapse pattern. This collapse pattern yields a void underneath.

· A-frame collapse occurs when floors on both sides of a centre wall fail and collapse inwards. The resulting collapse is reminiscent of two opposing lean-to collapses. This type of collapse results in the likelihood of two void spaces on either side of the centre wall.

While there are specific patterns of collapses that TFs look for when surveying a rubble pile, this approach does not consider ways of entering a collapse. This thesis presents an approach to visually identify holes in rubble that can be further examined for human insertion or probing for further reconnaissance and information gathering.

2.3.1

Survival Under Rubble

Extricating entombed survivors is a time sensitive operation. Statistics show that survival of victims within rubble becomes drastically low beyond 72 hours, a window referred to within the response community as the "golden 72 hours" (Tadokoro, 2005). Even if rescue is performed before the 72 hour mark, studies have shown that victims extricated beyond 48 hours are unlikely to survive beyond a few weeks in hospital (Murphy et al., 2008). Events that require activation of HUSAR TFs often require many hours of transit for specialists and heavy equipment to arrive on scene. This is critical time where early reconnaissance is critical for planning and decision making. Our approach allows local responders to perform preliminary information gathering that automatically detects access holes that can be transmitted to HUSAR TFs on-route and used by search specialists in decision making and planning. 16

CHAPTER 2. BACKGROUND

2.4. DATA ACQUISITION

2.4

Data Acquisition

The instability of rubble can prevent USAR teams from performing their function without compromising their own safety. These dangers have led researchers to seek alternatives that minimize risk to rescue workers. Search and rescue operation are often time critical and sensory data is useful for determining the quality of the environment and potentially assist with locating victims. Rescue robots may be able to provide benefit to operations by providing a robust platform to carry various sensory apparatus, collect data and deliver supplies to trapped victims (Murphy, 2000).

2.4.1

Ground-based Robotics

The military and law enforcement often use robots to remotely inspect and manipulate potential hazards, such as bombs and other explosive devices (Costo and Molfino, 2004). A common term used by an increasing number of robotics researchers is "response robot", a robot that is used in the response phase of the emergency management cycle. Early attempts at incorporating response robots into USAR applications utilized repurposed military and police ground robot technologies. In the earliest documented use of SAR robots, the Center for Robot-Assisted Search and Rescue (CRASAR) deployed 17 tele-operated robots to search the wreckage of the 2001 World Trade Center collapse in New York (Murphy, 2004). There have been attempts to use ground-based robots for autonomous navigation and mapping rubble interior spaces (Mobedi and Nejat, 2012); however, this approach does not attempt to locate access holes for insertion into rubble. Using ground vehicles as platforms for automated, top-down, road inspection has demonstrated some success (Sy et al., 2008). In this work, a sensor collects baseline information about level road surfaces and detects variances that translate to detected surface cracks. Since disaster rubble is often comprised of irregular shapes and materials, as opposed to level terrain, this approach is not appropriate for USAR. Work has been carried out using autonomous vehicles equipped with GPRs for detecting subsurface voids in mining operations (Wilson et al., 2009); however, this approach requires 17

2.4. DATA ACQUISITION

CHAPTER 2. BACKGROUND

Figure 2.3: An example of a Matilda ground robot utilizing tracks for locomotion (Munkeby et al., 2002).

heavy equipment, level terrain and a mobile platform traversing the area of inspection. USAR terrain is inevitably cluttered and chaotic, making effective ground traversal problematic for robots that utilize wheels or tracks for locomotion. Figure 2.3 shows an example of such a tracked robot. Research in terrain traversability has produced the concept of "negative obstacles". Negative obstacles are defined as obstacles that lie below the ground surface that return no sensor data and thus should be treated as holes to be avoided (Heckman et al., 2007). Early investigations into detecting negative obstacles analyzed ray traces of every pixel, comparing actual range values to expected ranges (determined via the position of the ground plane) to determine the difference (Matthies et al., 1995). This approach makes the assumption of a homogeneous terrain being traversed, making it unsuitable for USAR. Further work in negative obstacle detection (Sinha and Papadakis, 2013) project 3D point cloud data collected directly in front of the sensor to a 2D ground plane to detect gap contours. Detections are then further analyzed for traversability of ground robots in the USAR domain. In contrast to these 18

CHAPTER 2. BACKGROUND

2.4. DATA ACQUISITION

Figure 2.4: A Hex-copter UAV outfitted with a sensor package. Image courtesy of (N-CART, 2011).

previous works that are concerned with the avoidance of negative obstacles for terrain traversability, this thesis is interested in the suitability of these negative obstacles for insertion of trained search personnel in subsurface voids.

Ultimately, ground robots are limited in the areas they are able to successfully traverse, since the terrain composition can adversely impact locomotion (Ollero, 2004). Consequently, there are documented incidents where response robots have been abandoned during operations after becoming stuck on rubble terrain (Murphy, 2004). This limits their utility for data collection in this thesis and has motivated the use of UAVs to conduct surveying and reconnaissance tasks. 19

2.4. DATA ACQUISITION

CHAPTER 2. BACKGROUND

2.4.2

Aerial Robotics

To avoid the limitations of ground robots and to investigate areas inaccessible by a human searcher Unmanned Aerial Vehicles (UAVs) have been used to explore remote regions and collect data. Using a UAV for USAR operations allows searchers to survey areas that would not ordinarily be accessible from the ground and view the terrain in perspectives unattainable by terrestrial robots (Finn and Wright, 2012; Onosato et al., 2006). This rich information allows responders to carefully plan missions (Birk et al., 2011; Goodrich et al., 2008) and has proven extremely useful in finding victims in search and rescue operations (RCMP, 2013). A drawback with deploying UAVs is that they do not perform well in inclement conditions and cannot be easily controlled in confined spaces where the threat of collision is present. Recent work has considered UAVs for data collection in both terrain mapping and 3D scene reconstruction (Ferworn et al., 2011), shown in Figure 2.4. This previous work was used as a basis in collecting the evaluation dataset introduced in this thesis.

2.4.3

Sensors

Sensors provide information about the environment they are surveying. A variety of sensors can be employed by robots to gather information from an environment. Most pertinent to the current thesis are sensors that determine distances. These include Laser Range Finders (LRFs) that calculate the time-of-flight of a single laser pulse to be sent/reflected/received (Sedha, 2008) and sonar arrays that calculate a distance measure by emitting a high frequency pulse of sound and receiving the reflected echo (Moravec and Elfes, 1985). Currently, these sensor packages are cost prohibitive, which reduces their accessibility and availability. The large financial costs associated with outfitting and potentially losing a response robot has been one of the limiting factors for wide spread adoption of these devices. In addition, these sensors have weights in excess of the lift capacity of most small scale UAVs. Recently, affordable and compact stereo cameras have become available. Passive stereo cameras 20

CHAPTER 2. BACKGROUND

2.5. VISUAL OBJECT DETECTION

calculate depth using image pairs generated from photometric cameras placed side-by-side with a known fixed separation. Depth is encoded as the difference in perspective between the left and right camera, and is extracted by identifying a set of correspondence points between the two images. Once accurate correspondences are found, rays are intersected between the two images to triangulate the 3D positions, i.e., real world co-ordinates, (Trucco and Verri, 1998). A potential drawback of such passive stereo cameras is the need for rich surface texture which is necessary to establish correspondences between the same world points projected in each camera. Rubble created from structural collapse does not exhibit sufficient unique texture to provide accurate depth estimates. The introduction of light-weight, low-power, low-cost commercial off-the-shelf red-green-blue-depth (RGB-D) sensors (Newcombe et al., 2011; Asus Xtion, 2014) have provided solutions to replace similar but cost prohibitive sensors. RGB-D sensors differentiate themselves from passive stereo cameras by actively emitting an infrared grid on the surface that is being sensed. The grid simplifies the correspondence problem by providing texture to an infrared camera, thereby increasing the quality of the depth information obtained by the sensor. A drawback of this type of sensor is the limitation of environments where it can be deployed. In particular, strong sources of infrared light (e.g., the Sun) can wash out the infrared grid, introducing errors into the distance measurements. In this thesis, data is collected using the ASUS Xtion RGB-D sensor (Asus Xtion, 2014) shown in Figure 2.5, with sample output from the sensor shown in Figure 2.6.

2.5

Visual Object Detection

The challenge this thesis addresses is closely related to the domain of object detection, found in computer vision. The goal of object detection is to detect and localize object instances (i.e., an image pattern) within an image . An extensive body of work has accumulated centred on appearance- and geometrybased object recognition approaches (Grimson et al., 1990; Mundy, 2006; Dickinson, 2009; Andreopoulos 21

2.5. VISUAL OBJECT DETECTION

CHAPTER 2. BACKGROUND

A

B

C

Figure 2.5: An Asus Xtion RGB-D sensor used to collect data in this thesis. The sensor uses (A) an infrared emitter (B) a standard RGB colour camera and (C) an infrared camera. The infrared emitter is used in conjunction with the infrared camera to recover metric depth.

and Tsotsos, 2013).

2.5.1

Appearance and Geometry-based Detection

Appearance-based approaches map a photometric input pattern to a label of a specific object instance or class (Dalal and Triggs, 2005; Felzenszwalb et al., 2010; Lampert et al., 2008; Krizhevsky et al., 2012). To perform object detection, an input image is abstracted into a set of features. This can include low-level visual properties, such as colour or texture captured by SIFT (Lowe, 1999) or HOG (Dalal and Triggs, 2005), to more sophisticated mid-level representations such as CNN (Krizhevsky et al., 2012). Detection is achieved by comparing features of the input image to object models that are learned from sets of labeled training images (object vs. non-object). The learned models capture the variability in appearance of the object of interest (Yang et al., 2002). A number of approaches have been proposed to address the challenge of object detection. Sliding window approaches scan over patches of the input image at multiple scales and compare the learned models against each window patch. This can be computationally expensive since the search complexity is a product of the number of scales and patch locations in the image. One way to address this issue is the use of detection window proposals based on "objectness" detectors (Alexe et al., 2012). Objectness detectors produce a list of detection windows 22

CHAPTER 2. BACKGROUND

2.5. VISUAL OBJECT DETECTION

Figure 2.6: Sample imagery extracted from an ASUS Xtion RGB-D camera. A photometric image (left) with a corresponding per-pixel registered depth image that has been colourized for visualization (right).

that represent a subset of the total locations and scales that are possible. This can have the benefit of improving detection speed, but still does not address the problem that a classification window does not segment out pixels of the object from the surrounding background. Object detection using regions is one approach to addressing this concern (Fulkerson et al., 2009; Gu et al., 2009; Uijlings et al., 2013; Carreira and Sminchisescu, 2010). This type of approach has the benefit of encoding boundary and scale information into a detection (Gu et al., 2009). Most closely related to the approach used in this thesis is segmentation by superpixels. Superpixels capture region information from contiguous areas by aggregating pixels into superpixels. This approach creates accurate segmentations since the boundaries of objects tend to be respected, provided the risk of merging unrelated pixels is minimized (Fulkerson et al., 2009). The problem of 3D object recognition has been of particular interest in the fields of pattern matching, robotics and computer graphics (Tangelder and Veltkamp, 2008; Jain and Dorai, 2000; Bimbo and Pala, 2006). This approach leverages 3D models to build descriptors of the object of interest. A major advantage of these approaches is their invariance to material properties, viewpoint and illumination over appearance-based methods. Further, these approaches simplify the figure-background segmentation problem compared to appearance-based approaches. Three-dimensional recognition has experienced a 23

2.5. VISUAL OBJECT DETECTION

CHAPTER 2. BACKGROUND

Figure 2.7: Examples of the variation in size, shape and orientation of access holes in disaster rubble. This wide variation makes classification by appearance a difficult problem. revived interest in both the robotics and vision communities due to the introduction of commodity priced RGB-D sensors (Newcombe et al., 2011) and the abundant availability of three-dimensional models (Song and Xiao, 2014). An access hole lacks a standard shape, size or orientation and so no canonical definition exists to perform a matching (see Figure 2.7), making appearance- or geometry-based detection a difficult problem.

2.5.2

Functional Recognition

Most closely related to the approach proposed in this thesis are functional descriptions for object recognition (Winston et al., 1983; Stark and Bowyer, 1991; Stark et al., 2008; Grabner et al., 2011), i.e., centering the object model on what one can do with the object rather than its appearance or shape. Many object classes exhibit a large degree of variation in physical appearance. 24

CHAPTER 2. BACKGROUND

2.5. VISUAL OBJECT DETECTION

Figure 2.8: Chairs display large intra-class variation. For some of these objects, their description could be more easily provided by their function rather than their appearance or shape. In contrast to appearance-based detectors, "affordance" detectors use the characteristics of an object to imply its functional definition. For instance, chairs exhibit a large variety of shape and appearance (shown in Figure 2.8), e.g., the number of legs of a chair, while usually four, may vary. It is difficult to classify a chair based on its appearance, but cues from its function are useful in identification (i.e, "is this useful for sitting"?). In light of this, affordance based detectors have become a focus within the robotics and cognitive vision domains (Yao and Fei-Fei, 2010; Aksoy et al., 2010). This idea is adapted to develop a working definition of an access hole and use the proposed function of an access hole to classify it. In other words, rather than describing what a hole looks like, it is more productive to define its function.

25

Chapter 3

Technical Approach
3.1 Access Hole Definition

Before developing an approach that detects access holes, an operational definition is required. In this thesis, an "access hole" is defined by its potential utility as a means of accessing a collapsed structure, i.e., its function. An access hole must be deeper in the interior than the surrounding terrain. Furthermore, to be useful for USAR purposes, an access hole must be large enough to allow entry by a searcher, such as a human, dog or robot. In the remainder of this thesis, a searcher is assumed to be an adult human. This thesis has identified three attributes that characterize an access hole to perform a detection: (i) depth disparity, (ii) hole size and (iii) photometric brightness.

3.1.1

Naive and Conservative Approach

The definition of an access hole is deliberately naive in that the assumptions imply another category: non-access hole that is defined with the negative attributes of an access hole, i.e., it cannot fit a humansized entity. The assumptions are naive in the sense that there is no significant evidence to suggest that a non-access hole does not offer some form of access to a trapped human, e.g., the insertion of a camera27

3.2. ACCESS HOLE ATTRIBUTES

CHAPTER 3. TECHNICAL APPROACH

equipped search pole. The approach is conservative by deliberately limiting the potential number of visual features that might otherwise be identified as access holes. This is important in that there are many visual features in rubble that may be useful for accessing interior rubble but would require too many physical resources or be too dangerous to actually use. The intention has been to provide a useful, metrics-based definition of access holes that can be algorithmically exploited without producing results that would needlessly overwhelm any first responders.

3.2

Access Hole Attributes

The input to the proposed approach is an image pair extracted from an RGB-D sensor consisting of photometric colour (RGB) and metric depth. The two images are registered such that they have a one-to-one mapping. To perform detection, candidate regions that potentially contain access holes must be identified from the terrain surrounding it. The proposed approach first over-segments the depth input into regions, i.e., superpixels (Ren and Malik, 2003), with the purpose of isolating regions (i.e., potential holes) exhibiting depth measurement discontinuities. A superpixel is a perceptually meaningful atomic image unit that contains pixels that are similar in some image property, such as depth, colour and texture. It is implicitly assumed the constituent pixels of a superpixel belong to the same physical entity in the world. An adjacency graph is next created by identifying the neighbours of each superpixel. For each superpixel, a set of geometric and photometric feature scores is assigned, where each score represents the likelihood of a hole. Feature scores for each superpixel are aggregated to realize a final hole detection score. Figure 3.1 summarizes the data processing flow for the proposed approach to access hole detection. 28

CHAPTER 3. TECHNICAL APPROACH

3.2. ACCESS HOLE ATTRIBUTES

System Overview
Input Frames
Segmentation

Image Oversegmentation

Adjacency Graph Discovery

RGB

Depth

Score Generation
Detection Score Calculation Feature Score Generation

Detected And Localized Access Hole

Figure 3.1: Data flow for the proposed access hole detection approach. Using the input images, the depth image is over-segmented and treated as an undirected graph. For each superpixel, a set of geometric and photometric based feature scores is determined that are used to calculate a final detection score. The final output is a set of localized access holes with a tightly fitted bounding box representing a candidate detection.

3.2.1

Depth Disparity

Typically, rubble scene imagery is extremely cluttered and unstructured. A hole, the region of interest, must be isolated from the area around it along the shared boundary. Due to the heterogeneous nature of rubble, figure-ground separation (i.e., target entity versus background) of holes and rubble from photometric appearance alone is rendered difficult. Fortunately, RGB-D sensors provide an estimate of metric depth information, i.e., the underlying geometry. The depth information is exploited to partition the image into a set of superpixels along boundaries that exhibit a strong depth gradient. A publicly available superpixel algorithm is used to partition the image. An inappropriate number of partitions results in a contiguous entity (e.g., a hole) either being under-segmented or over-segmented, as shown in Figure 3.2. The assumption is made that every superpixel overlaps with at most one hole and the set of superpixel boundaries are a superset of 29

3.2. ACCESS HOLE ATTRIBUTES

CHAPTER 3. TECHNICAL APPROACH

Figure 3.2: Examples of depth images segmented using a range of target superpixels. The images show (left) an under-segmented image where the access hole is missed, (center) an image segmented with the boundaries of the potential access hole captured and (right) an over-segmented image where the potential access hole is sub-divided.

the hole boundaries; these are standard assumptions in the use of superpixels in vision applications, e.g., (Fulkerson et al., 2009; Liu et al., 2011). The absolute depth value of a region does not alone determine if a region is an access hole. An access hole by definition must be deeper than its surrounding terrain. As such, it is the depth discontinuity between adjacent regions that are important. For each superpixel, an adjacency graph is built to obtain a list of its neighbouring regions. A natural way to express the superpixel image is by an undirected graph G = (V, E ), where each vertex, vi  V , corresponds to a superpixel and the edges (vi , vj )  E , denote the set of neighbouring superpixels. Figure 3.3 shows an example of the superpixel extraction and neighbourhood discovery steps. For each superpixel, vi , its average depth is compared against all other superpixels that share an edge with it. Superpixels that correspond to a local depth maxima compared to its neighbours serve as access hole candidates for scoring. The higher the mean depth for a candidate region, the more likely it is indeed an access hole. For each superpixel, a relative depth score, Sd , is calculated. The depth threshold used for scoring is based on data collected from anatomical models (Panero and Zelnik, 1979). This threshold establishes the minimum depth a region must be from its surroundings to be a valid candidate. A linear score, between 0 and 1, is assigned for any relative depth between the minimum and maximum thresholds derived from the anatomical model. Any depth greater than the maximum 30

CHAPTER 3. TECHNICAL APPROACH

3.2. ACCESS HOLE ATTRIBUTES

threshold is assigned a score of 1 and any depth less than the minimum threshold is assigned 0.

3.2.2

Access Hole Size

An access hole must have an appropriate size for the potential entry of rescue personnel or similarly sized entities. Based on this function, two size-based attributes are computed: (i) width and (ii) aspect ratio. The width of the region is determined by fitting an ellipsoid around the superpixel from the metric values provided by the depth sensor and projecting the points to a plane. Points that lie beyond three standard deviations from the mean are filtered to exclude outliers, and then projected to a plane. An ellipse is fit to the point cluster to calculate the major and minor axes. This yields the approximate metric width and girth of a region in metric units. For a hole to be considered appropriate for insertion of a searcher, the width of the major axis and girth of the minor axis were adopted based on anatomical data of the average adult human (Panero and Zelnik, 1979). A feature score Sw is assigned, between 0 and 1, where a higher score indicates a higher likelihood of accommodating a searcher. A score of 1 is assigned to Sw if the measurement of the major or minor axis are both equal or greater than the anatomical model. If the axis measurements are 50% or below, a score of 0 is assigned. To minimize missed detections of holes due to partial occlusion or superpixel over-segmentation, a score is applied linearly between 0 and 1 for measurements greater than 50% of the anatomical model measurements. To limit the candidacy of holes that may be thin and curvilinear a score for the aspect ratio of the region is introduced. The aspect ratio score is assigned linearly by calculating the ratio of the area of a given superpixel to the area of the bounding box tightly outlining the major and minor axes. The higher the percentage occupied in the bounding box, the better the candidacy of the detected region. A score, Sr , is assigned linearly between 0 and 1 based on the percentage of the bounding box occupied by the superpixel. 31

3.2. ACCESS HOLE ATTRIBUTES

CHAPTER 3. TECHNICAL APPROACH

1 9 5 4 7 6 7 2 8 8 5 3 4 6 1 2 9

3

Figure 3.3: Outline of the processing flow for segmenting a depth image and determining the neighbours of a particular superpixel. (left) Raw depth image, (middle) superpixel segmentation with unique identifier and (right) an undirected graph generated from adjacency discovery.

3.2.3

Photometric Appearance

Examining the depth information alone does not provide sufficient discriminatory information about a hole. To account for this uncertainty photometric brightness derived from the RGB image is incorporated. It is assumed that access holes are poorly illuminated and thus appear darker in the RGB image. To capture this attribute two feature scores are introduced: i) absolute brightness and ii) relative brightness. To compute the absolute brightness intensity of a superpixel, the RGB image is converted to the YUV colour space (Black, 2009) and the average brightness from the Y-channel (i.e., the luminance) for each superpixel is calculated, where Y  [0, 1]. To determine the threshold for a valid brightness intensity value, a dataset was compiled from images collected via Google Image (Google, 2014). The dataset contains 118 images depicting collapsed buildings and rubble from disaster scenes. Holes were hand labeled and the mean brightness intensity was collected. A linear score, Sb , is assigned ranging between 0 and 1, where a higher score is assigned to regions lower than a pixel intensity threshold that was empirically determined from the training data. Since holes are typically darker than the region surrounding them, each region is also scored based on its relative brightness intensity. Using the Y-channel, the difference between the average brightness of a superpixel with the average brightness of all pixels within (directly) neighbouring superpixels 32

CHAPTER 3. TECHNICAL APPROACH

3.3. DETECTION

is calculated. A minimum threshold was empirically determined using the image training set containing the hand labeled ground truth. A linear score, Sc , is a assigned to a given superpixel between 0 and 1.

3.3

Detection

Each superpixel is assigned a final detection score, S . Higher scores indicate a stronger likelihood of a superpixel being a hole. The resulting detection score, S , is calculated as follows:

S=
Si F

wi Si + b,

(3.1)

where F = {Sd , Sw , Sr , Sb , Sc } is the set of feature scores, wi denotes the weighting given to the corresponding feature, and b is a bias term. The detection algorithm produces a list of spatial bounding boxes for each image. Each detected access hole is represented by a bounding box that tightly outlines the image region. The final output of the approach consists of the coordinates of the bounding boxes and their corresponding detection score.

33

Chapter 4

Experimental Results

4.1

System Setup

Evaluation of the proposed approach was performed on a novel rubble scene dataset, described below in Section 4.2. Throughout the evaluation, the various thresholds of the approach are fixed to the same values for all images. The minimum and maximum depth used for computing the depth score, Sd , is based on an anatomical human model (Panero and Zelnik, 1979) and is set to 200mm and 1951mm, respectively. The same anatomical model was used to set the minimum width and girth thresholds used for computing the size score, Sw , and aspect ratio score, Sr , set at 655mm and 368mm, respectively. The threshold used to compute the photometric brightness score, Sb , was empirically set to the luminance value of 0.274. Similarly, the brightness difference between a superpixel with its neighbouring regions used to compute the relative brightness score, Sc , is empirically set to the luminance value of 0.267. Due to the limited amount of data available for learning parameters, an equal weighting of wi =
1 5

is given to

each feature and the bias term, b, is set to zero. This is done to remain agnostic to features that may be stronger and avoid over-fitting to the introduced dataset. Consequently, detection scores range between 0-1. 35

4.2. DATASET

CHAPTER 4. EXPERIMENTAL RESULTS

Figure 4.1: The OPP reference rubble pile, located in Bolton, Ontario, Canada.

4.2

Dataset

The proposed access hole detection approach was evaluated on a challenging dataset containing images of a real rubble scene. Data was collected by mounting an ASUS Xtion RGB-D sensor and capture device under a UAV (shown in Figure 2.4). The Xtion outputs two images: (i) a 32-bit colour image captured at a resolution of 640 × 480 and (ii) a depth matrix of distances in metric measurement, mapped per-pixel onto the RGB image. The device captures images at a rate of 30 frames per second (FPS) with a field of view (FOV) of 58 horizontal, 45 vertical and 70 diagonal, respectively. The sensor has an optimal distance of use between 0.8 meters and 2.5 meters. Figure 2.6 shows a sample output of the ASUS Xtion, an RGB image and a registered per-pixel depth image that has been colourized for visualization purposes. Data was collected at the Reference Rubble Pile of the Ontario Provincial Police (OPP), located in Bolton, Ontario, Canada (U.C.R.T., 2013). The rubble pile is used for training purposes, and consists 36

CHAPTER 4. EXPERIMENTAL RESULTS

4.3. EVALUATION

of heterogeneous terrain comprised of concrete, metal and wood debris fields, purpose-built simulation buildings, shipping containers and partially crushed and buried vehicles (shown in Figure 4.1). Commodity RGB-D sensors, such as the Microsoft Kinect and Asus Xtion, are notoriously sensitive to external sources of infrared light (Ferworn et al., 2011). To minimize the corruption of depth estimates for the experiments, data was captured during sunrise or dusk when the influence of the Sun's infrared emissions was minimal. The dataset is comprised of 254 image pairs consisting of an RGB image and corresponding registered depth map, with an image resolution of 640 × 480. Out of this set, there are 166 RGB-D images that contain 18 unique holes that meet the definition of an access hole. Ground truth was marked by hand labeling the location of each access hole with a tight bounding box. Figure 4.4 shows a sample of the data used for evaluation. The image dataset and ground truth is publicly available at: http://ncart.scs.ryerson.ca/research/access-hole-detection.

4.3

Evaluation

To quantitatively evaluate the detection accuracy of the approach on the introduced dataset, PrecisionRecall (P-R), a standard evaluation tool in information retrieval (Rijsbergen, 1979) is used. The curve captures the trade-off between accuracy and noise as the detection threshold is varied. "Precision" denotes the number of correctly detected holes over the total number of detections and is defined as follows: P recision = TP/(TP + FP), (4.1)

where TP denotes the number of true positives (i.e., correctly detected holes) and FP denotes the number of false positives, i.e., the number of detections where no hole is present. "Recall" is the fraction of true positives that are detected rather than missed and is defined as follows:

Recall = TP/nP, 37

(4.2)

4.3. EVALUATION

CHAPTER 4. EXPERIMENTAL RESULTS number of superpixels 9 11 13 15 0.43 0.37 0.37 0.36

AP

5 0.43

7 0.43

18 0.38

20 0.36

Table 4.1: Comparison of average precision (AP) for a range of target superpixel values used to partition each image. where nP is the total number of positives present in the dataset. A detection is considered a true positive if there is a spatial overlap greater than 50% with the hand labeled ground truth. A detection is represented as a (rectilinear) bounding box that spatially outlines the candidate access hole along with the associated detection score, S .

4.3.1

Segmentation

To over-segment the depth image a publicly available superpixel segmentation algorithm is used. In particular, the Entropy Rate Superpixel (ERS) (Liu et al., 2011) algorithm is used to produce a user specified number of superpixels with roughly similar sizes and compact shapes. Other segmentation algorithms are also applicable, e.g, Mean Shift (Comaniciu and Meer, 2002) and Normalized Cuts (Shi and Malik, 2000). An inappropriate number of partitions results in a contiguous surface either being under- or over-segmented beyond the ability to register a detection. An example of this is shown in Figure 3.2. To evaluate the sensitivity of the proposed approach to the number of selected superpixel segments, the detection approach was run against the introduced dataset with ground truth, using a range of segmentation targets. To summarize the results for each P-R curve, the average precision was computed over the recall interval 0 - 1. Table 4.1 shows the average precision for the approach using a range of superpixels. Table 4.1 shows that the average precision is fairly stable at 0.43 up to 9 partitions. Beyond 9 segments the average precision begins to slowly decrease. To avoid the case of missed detections due to under- or over-segmentation, 9 segments with an average precision of .43 was selected for all further evaluations. 38

CHAPTER 4. EXPERIMENTAL RESULTS single feature scores aspect ratio brightness 0.14 0.21

4.3. EVALUATION

AP

depth 0.11

width 0.07

relative brightness 0.28

Table 4.2: Comparison of average precision (AP) for individual feature scores. combined feature scores geometric photometric combined 0.25 0.38 0.47

AP

Table 4.3: Comparison of average precision (AP) for geometric features, photometric features and all features combined.

4.3.2

Ablative Analysis of Features

The approach captures information from both photometric and geometric features. To systematically determine the relative effectiveness of each feature and their combination, the detection system is run using various combinations of the feature scores to calculate the final detection score. To summarize the results for each P-R curve, the average precision was computed over the recall interval 0 - 1. To begin, the most basic case of isolating each feature score is examined. Table 4.2 shows the average precision for each feature. When considering the results of single feature evaluation, the geometric features do not perform very well on their own. For the depth feature, this can be explained by the large height variations in rubble terrain, where a disparity in depth from the surrounding regions may not necessarily translate to a hole. The hole size features (e.g., width and aspect ratio) perform poorly as well, since there is no discriminative information beyond the human anatomical model to reject false detections. The remaining features (e.g., photometric brightness and relative brightness) outperform the geometric features. Intuitively this makes sense, since access holes are expected to exhibit poor illumination and have effectively used the geometry to pre-process the depth image by over-segmenting the image along the boundaries of contiguous areas. To improve the AP, the system is next evaluated by combining geometric-only, photometric-only and all features. Table 4.3 shows the average precision of the approach for combining the features. 39

4.3. EVALUATION

CHAPTER 4. EXPERIMENTAL RESULTS

Figure 4.2: Evaluation of the overall system for detecting holes. The Precision-Recall (PR) curve is computed across the entire introduced dataset; the number of superpixels is set to nine.

Combining geometric features results in a higher AP than any single geometric feature alone. This can be accounted for by recognizing that combining the features now permits the exclusion of false detections that satisfied a single feature's criteria but not the others. Similarly, combining photometric features boosts the AP of any individual photometric feature. Finally, by combining all the geometric and photometric features the proposed approach is able to outperform any individual feature or feature subset combination. Figure 4.2 shows the PR curves generated from this evaluation. The plot demonstrates that the PR curve for the combined features outperforms the geometric-only and photometric-only features for the same recall value. Furthermore, the combined features attain a higher recall than either geometric or photometric features alone, while maintaining a moderate precision value. 40

CHAPTER 4. EXPERIMENTAL RESULTS

4.4. DISCUSSION

4.3.3

Runtime

Experiments were performed with unoptimized code, using MATLAB version R2012b, running on a 64bit Intel Core I5 2.50GHz machine with 6GB of RAM. To detect access holes in a single RGB-D image with a resolution of 640 × 480 segmented into 9 superpixels, the system requires 9 seconds. Increasing the number of superpixels to 20 yields a runtime of 14 seconds per input image pair. Significant runtime improvements are anticipated via optimizing the code and leveraging parallel computation, e.g., a graphics processing unit (GPU). These results suggest that our experimental system could effectively process large amounts of imagery in realistic time-frames1 .

4.4

Discussion

The motivation for this system is identifying and localizing access holes for disaster scenarios, thus this thesis is interested in high recall for detections with moderate to high precision. The system performs well in this regard as it is able to detect all labeled ground truth holes with .16 precision when recall is 1, i.e., all holes in the ground truth detected. Precision is lowered by the number of false positive detections. The ultimate goal is to provide detections to response personnel that correctly identify all access holes with minimal false detections. Since a missed detection can result in the potential loss of life, a high false positive rate is accepted so not to exclude any potential access holes. Figure 4.3 shows sample detection outputs. On examining the detections it is found that a number of false positive detections occur in areas that the geometric features score high, but are not excluded through the scoring of photometric properties. Non-uniform weighting of the various feature scores via learning may ameliorate some of these issues; however, a lack of sufficient training data currently limits the ability to tune the system without overfitting to the current dataset. Ultimately, these false positives can be rejected by further visual inspection
1 The intended use for our approach is to create access hole information for first responders in-transit to a disaster scene. The realistic time-frame should be considered to be on the order of many hours, e.g., the main body of Canada Task Force 3 (Toronto HUSAR) arrived at the Algo Centre Mall Collapse in roughly 14 hours after their activation (Belanger, 2014).

41

4.4. DISCUSSION

CHAPTER 4. EXPERIMENTAL RESULTS

Figure 4.3: Sample output of the proposed access hole detection approach. (left) Input RGB image, (middle) superpixel segmentation with ground truth label given in red and (right) detected regions given in green. The first two rows show successful detections and the last row a successful detection with a false detection. This image is best viewed in colour. with minimal effort, as compared to evaluating all inputs manually.

42

Figure 4.4: A sample of RGB and depth image pairs from the introduced dataset used to evaluate the detection system. This image is best viewed in colour.

Chapter 5

Summary and Conclusions

5.1

Summary of Findings and Contributions

This thesis presented a novel approach for the automated detection of access holes in rubble scenes. Access holes represent areas of particular interest for first responders. They represent the possibility of accessing subsurface voids where live humans may be hidden. This thesis is the first to define the characteristics of an access hole through both functional and photometric attributes inherent to a valid entry point. A novel approach for identifying candidate access holes in RGB-D data was proposed, a real rubble pile dataset was introduced and an evaluation protocol to validate the approach was provided. Empirical evaluation has shown promising results for detecting access holes. There are numerous positive implications of the current contribution. First, the introduced approach can potentially reduce the need for the dangerous task of humans performing the general visual inspection of an urban disaster incident for potential areas of access. Second, the approach is able to reduce the cognitive load of response workers tasked with identifying access points through visual inspection. Third, a UAV can investigate regions beyond line of sight, meaning searching and analyzing areas that might not have been accessible before. Finally, significant reductions can be made in the search space of a 45

5.2. LIMITATIONS AND RESTRICTIONS

CHAPTER 5. SUMMARY AND CONCLUSIONS

large collapse to a manageable number of locations, thus saving time. Search and rescue operations are extremely time-critical, as life expectancy of victims under buried rubble is limited. Identifying and localizing access holes in this way makes better use of limited time. Since the detection approach is intended for planning purposes, it provides a search team advanced warning of "potential" access paths that can then by prioritized by human search specialists. The intent is to provide a means of indicating holes that can then be explored or eliminated from further consideration by the search team. These on-scene teams would then transmit the simulation to inbound TFs whose search teams and structural specialists would use the data as input to forming their plans prior to arriving at the scene.

5.2

Limitations and Restrictions

Despite having achieved the goals set for the detection system, some limitations in the approach have been identified. Current commodity RGB-D cameras, such as the Xtion, do not work well outdoors in full daylight conditions. This is a well-known challenge within the field robotics community. To simplify the stereo correspondence problem for establishing depth, a grid of IR light illuminates the scene. This technique is easily corrupted by an external IR source, notably the Sun. Sunlight will wash out the IR grid and cause the sensor to provide erroneous data across the depth map. To date, (photometric) stereo-based algorithms have not achieved the same level of depth accuracy as RGB-D sensors. The consideration of other, more sophisticated sensors is possible in the future; however, they are currently cost prohibitive and heavier than the payload capacity of our UAV. The RGB-D sensor can be replaced with a more reliable sensor provided that the input to the system remains an RGB image with a reliable, registered metric depth map. Nonetheless, this thesis has presented a novel approach and demonstrated it in the field. An alternative approach is to investigate ways of improving the depth data estimates, such as 46

CHAPTER 5. SUMMARY AND CONCLUSIONS 5.3. DIRECTIONS FOR FUTURE RESEARCH integrating the data over time rather than sampling a single frame. This approach could reduce the number of areas with missing or corrupted depth estimates. The lack of a large real-world dataset is a current limiting factor. While the dataset introduced in this thesis takes a first step, it is insufficient for providing examples of the multitude of debris configurations that rubble fields can present. Furthermore, the limited amount of data restricts tuning the weight of features when calculating the detection score, (Section 3.3). This thesis purposely remains agnostic to these weights to avoid the problem of over-fitting performance biased to our current dataset. The availability of other disaster scene datasets would allow for learning the weight parameters. Sensitivity to specific features could be tuned to provide better performance. In addition, a more diverse dataset would also provide a more thorough evaluation of the approach. Overall, as more data becomes available, improvements in the algorithm may be realized.

5.3

Directions for Future Research

Further developments can be made to the approach by improving the attributes identified in this thesis and augmenting the set with additional ones. The criteria used to identify holes can be expanded, the nomenclature around the terms "access hole" and "non-access holes" can be widened and other methods for discovering and validating them can be investigated. For instance, the characterization of pore space or "macropores" is well understood within the field of soil sciences (Cary and Hayden, 1973; Nimmo, 2004; Luo et al., 2010; Glab, 2007) but is not used in rubble terminology at all. Further investigation in this domain may yield insights that may be beneficial to USAR classification of access holes and the larger problem of rubble characterization. Since this thesis is interested in high recall when detecting access holes, false positives are expected. These can quickly be ruled out by visual inspection or through the use of corroborating ancillary data collected over multiple flights or using complementary techniques. For instance, holes in rubble tend 47

5.3. DIRECTIONS FOR FUTURE RESEARCH CHAPTER 5. SUMMARY AND CONCLUSIONS to have different thermal properties than the terrain surrounding them (Matthies and Rankin, 2003). The use of Forward Looking Infrared (FLIR) sensors to detect secondary thermal effects present around potential holes with humans inside may help reduce errors. The long-term goal of this approach is to perform on-board analysis in real-time while raw image data is being gathered by a UAV. When an access hole is detected, its geographic coordinates will be provided by the UAV and transmitted wirelessly to ground teams to flag the location for further investigation. This approach will not only reduce processing time but will improve access hole location accuracy. The intention is to include this information in a physics-aware disaster scene model (Ferworn et al., 2013), with the access hole information represented and clearly marked for searchers inside the spatially accurate simulation. From an application perspective, it should be noted that this technique of collecting hole data and rendering a scene model would ideally be used by the advance parties of any inbound TF or the local first responders at the scene1 . To date, this goal has not been realized due to the limited number of opportunities available.

1 While the approach presented in this thesis can reduce the need for visual inspection by first responders physically colocated with the incident, it should not be the impression that this system can stand alone without additional confirmatory physical inspections by expert USAR practitioners.

48

Bibliography
Aksoy, E. E., Abramov, A., Worgotter, F., and Dellen, B. (2010). Categorizing object-action relations from semantic scene graphs. In IEEE International Conference on Robotics and Automation, pages 398­405. Alamgir, M., Parvin, R., and Khan, M. A. H. (2014). Tragedy in Savar: Management of Victims in Enam Medical College Hospital. Journal of Enam Medical College, 4(1):31­35. Alexe, B., Deselaers, T., and Ferrari, V. (2012). Measuring the objectness of image windows. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(11):2189­2202. Andreopoulos, A. and Tsotsos, J. (2013). 50 years of object recognition: Directions forward. Computer Vision and Image Understanding, 117(8):827­891. Asus Xtion (2014). Xtion motion sensor for pc. http://www.asus.com/Multimedia/Xtion PRO/. Belanger, P. R. (2014). Report of the Elliot Lake Commission of Inquiry, Executive Summary. Queen's Printer for Ontario. Bimbo, A. D. and Pala, P. (2006). Content-based retrieval of 3D models. ACM Transactions on Multimedia Computing, Communications, and Applications, 2(1):20­43. Binda, L., Saisi, A., and Tiraboschi, C. (2001). Application of sonic tests to the diagnosis of damaged and repaired structures. NDT & E International, 34(2):123­138. Birk, A., Wiggerich, B., B¨ ulow, H., Pfingsthorn, M., and Schwertfeger, S. (2011). Safety, security, and rescue missions with an unmanned aerial vehicle (UAV). Journal of Intelligent & Robotic Systems, 64(1):57­76. Black, G. M. R. B. (2009). YUV color space. Communications Engineering Desk Reference, page 469. Carreira, J. and Sminchisescu, C. (2010). Constrained parametric min-cuts for automatic object segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3241­3248. Cary, J. and Hayden, C. (1973). An index for soil pore size distribution. Geoderma, 9(4):249­256. Casper, J. and Murphy, R. R. (2003). Human-robot interactions during the robot-assisted urban search and rescue response at the World Trade Center. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 33(3):367­385. Comaniciu, D. and Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603­619. Costo, S. and Molfino, R. (2004). A new robotic unit for onboard airplanes bomb disposal. In International Symposium on Robotics, pages 23­26. 49

BIBLIOGRAPHY

BIBLIOGRAPHY

Crocco, L. and Ferrara, V. (2014). A review on ground penetrating radar technology for the detection of buried or trapped victims. International Conference on Collaboration Technologies and Systems, pages 535­540. Dalal, N. and Triggs, B. (2005). Histograms of oriented gradients for human detection. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 886­893. Dickinson, S. J. (2009). Challenge of image abstraction. In Dickinson, S. J., Leonardis, A., Schiele, B., and Tarr, M. J., editors, Object categorization: Computer and human vision perspectives. Cambridge University Press. Emergency Management Australia (2004). Urban Search and Rescue Capability Guidelines for Structural Collapse Response. Emergency Management Australia. Environment Canada (2014). Emergency management basics. hurricanes/default.asp?lang=En&n=31DADDF5-1. http://www.ec.gc.ca/ouragans-

Felzenszwalb, P. F., Girshick, R. B., McAllester, D., and Ramanan, D. (2010). Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627­1645. FEMA (2006). National Urban Search & Rescue (USAR) Response System: Rescue Field Operations Guide. U.S. Department of Homeland Security. FEMA (2009). Urban Search & Rescue Structures Specialist: Field Operations Guide. U.S. Army Corps of Engineers. FEMA (2012). Sct01c structural engineering systems http://disasterengineer.org/Library/tabid/57/Default.aspx. student manual ver4.

Ferworn, A., Herman, S., Tran, J., Ufkes, A., and McDonald, R. (2013). Disaster scene reconstruction: Modeling and simulating urban building collapse rubble within a game engine. Summer Simulation Multi-Conference, 45(11). Ferworn, A., Tran, J., Ufkes, A., and D'Souza, A. (2011). Initial experiments on 3D modeling of complex disaster environments using unmanned aerial vehicles. In IEEE International Symposium on Safety, Security, and Rescue Robotics, pages 167­171. Finn, R. L. and Wright, D. (2012). Unmanned aircraft systems: Surveillance, ethics and privacy in civil applications. Computer Law & Security Review, 28(2):184­194. Fulkerson, B., Vedaldi, A., and Soatto, S. (2009). Class segmentation and object localization with superpixel neighborhoods. In IEEE International Conference on Computer Vision, pages 670­677. Glab, T. (2007). Application of image analysis for soil macropore characterization according to pore diameter. International Agrophysics, 21:61­66. Goodrich, M. A., Morse, B. S., Gerhardt, D., Cooper, J. L., Quigley, M., Adams, J. A., and Humphrey, C. (2008). Supporting wilderness search and rescue using a camera-equipped mini UAV. Journal of Field Robotics, 25(1-2):89­110. Google (2014). Google image. https://images.google.com/. Grabner, H., Gall, J., and Gool, L. V. (2011). What makes a chair a chair? In IEEE Conference on Computer Vision and Pattern Recognition, pages 1529­1536. 50

BIBLIOGRAPHY

BIBLIOGRAPHY

Grimson, W., Lozano Perez, T., and Huttenlocher, D. (1990). Object Recognition by Computer: The Role of Geometric Constraints. MIT Press. Gu, C., Lim, J. J., Arbel´ aez, P., and Malik, J. (2009). Recognition using regions. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1030­1037. Harris, M. B., Balo glu, M., and Stacks, J. R. (2002). Mental health of trauma-exposed firefighters and critical incident stress debriefing. Journal of Loss &Trauma, 7(3):223­238. Heckman, N., Lalonde, J.-F., Vandapel, N., and Hebert, M. (2007). Potential negative obstacle detection by occlusion labeling. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2168­2173. International Fire Service Training Association (2005). Urban Search and Rescue in Collapsed Structures. International Fire Service Training Association. Jain, A. K. and Dorai, C. (2000). 3D object recognition: Representation and matching. Statistics and Computing, 10(2):167­182. Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097­1105. Lampert, C. H., Blaschko, M. B., and Hofmann, T. (2008). Beyond sliding windows: Object localization by efficient subwindow search. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1­8. Lindell, M. K., Prater, C., and Perry, R. W. (2006). Introduction to Emergency Management. Wiley. Liu, M.-Y., Tuzel, O., Ramalingam, S., and Chellappa, R. (2011). Entropy rate superpixel segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2097­2104. ´ Lombillo, I., Thomas, C., Villegas, L., Fern´ andez-Alvarez, J. P., and Norambuena-Contreras, J. (2013). Mechanical characterization of rubble stone masonry walls using non and minor destructive tests. Construction and Building Materials, 43:266­277. Lowe, D. G. (1999). Object recognition from local scale-invariant features. In IEEE International Conference on Computer Vision, volume 2, pages 1150­1157. Luo, L., Lin, H., and Schmidt, J. (2010). Quantitative relationships between soil macropore characteristics and preferential flow and transport. Soil Science Society of America Journal, 74(6):1929­1937. Macintyre, A. G., Barbera, J. A., and Smith, E. R. (2006). Surviving collapsed structure entrapment after earthquakes: A time-to-rescue analysis. Prehospital and Disaster Medicine, 21(01):4­17. Matthies, L., Kelly, A., Litwin, T., and Tharp, G. (1995). Obstacle detection for unmanned ground vehicles: A progress report. In International Symposium of Robotics Research, pages 475­486. Matthies, L. and Rankin, A. (2003). Negative obstacle detection by thermal signature. In IEEE/RSJ International Conference on Intelligent Robots and Systems, volume 1, pages 906­913. McEntire, D. A. (2007). Disaster response and recovery: Strategies and tactics for resilience. Wiley. Mobedi, B. and Nejat, G. (2012). 3-D active sensing in time-critical urban search and rescue missions. IEEE/ASME Transactions on Mechatronics, 17(6):1111­1119. 51

BIBLIOGRAPHY

BIBLIOGRAPHY

Molino, V., Madhavan, R., Messina, E., Downs, A., Balakirsky, S., and Jacoff, A. (2007). Traversability metrics for rough terrain applied to repeatable test methods. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1787­1794. Moravec, H. P. and Elfes, A. (1985). High resolution maps from wide angle sonar. In IEEE International Conference on Robotics and Automation, volume 2, pages 116­121. Mundy, J. (2006). Object recognition in the geometric era: A retrospective. In Ponce, J., Hebert, M., Schmid, C., and Zisserman, A., editors, Toward Category-Level Object Recognition, volume 4170 of Lecture Notes in Computer Science. Springer Berlin Heidelberg. Munkeby, S. H., Jones, D., Bugg, G., and Smith, K. (2002). Applications for the matilda robotic platform. In AeroSense, pages 206­213. International Society for Optics and Photonics. Murnane, L. and Fortney, J. (2003). Technical rescue for structural collapse. Fire Protection Publications, Oklahoma State University. Murphy, R. (2000). Marsupial and shape-shifting robots for urban search and rescue. IEEE Intelligent Systems and Their Applications, 15(2):14­19. Murphy, R., Tadokoro, S., Nardi, D., Jacoff, A., Fiorini, P., Choset, H., and Erkmen, A. (2008). Search and rescue robotics. In Springer Handbook of Robotics, page 1152. Springer. Murphy, R. R. (2004). Trial by fire [rescue robots]. Robotics & Automation Magazine, 11(3):50­61. N-CART (2011). UAV with rgbd. http://ncart.scs.ryerson.ca/. Nakahara, S. (2011). Lessons learnt from the recent tsunami in Japan: Necessity of epidemiological evidence to strengthen community-based preparation and emergency response plans. Injury Prevention, 17(6):361­364. Newcombe, R. A., Izadi, S., Hilliges, O., Molyneaux, D., Kim, D., Davison, A. J., Kohli, P., Shotton, J., Hodges, S., and Fitzgibbon, A. (2011). Kinectfusion: Real-time dense surface mapping and tracking. In IEEE International Symposium on Mixed and Augmented Reality. Nimmo, J. (2004). Porosity and pore size distribution. Encyclopedia of Soils in the Environment, 3:295­303. Ollero, A. (2004). Control and perception techniques for aerial robotics. Annual Reviews in Control, 28(2):167­178. Onosato, M., Takemura, F., Nonami, K., Kawabata, K., Miura, K., and Nakanishi, H. (2006). Aerial robots for quick information gathering in USAR. In International Joint Conference of the Society of Instrument Control Engineers and the Institute of Control, Automation and Systems Engineers, pages 3435­3438. Onosato, M., Yamamoto, S., Kawajiri, M., and Tanaka, F. (2012). Digital gareki archives: An approach to know more about collapsed houses for supporting search and rescue activities. IEEE International Symposium on Safety, Security, and Rescue Robotics. Panero, J. and Zelnik, M. (1979). Human dimension and interior space: A source book of design reference standards. Watson-Guptill, New York. Public Safety Canada (2014). Canadian urban search and rescue (USAR) classification guide. http://www.publicsafety.gc.ca/cnt/rsrcs/pblctns/rbn-srch-rsc/index-eng.aspx. 52

BIBLIOGRAPHY

BIBLIOGRAPHY

RCMP (2013). Saskatoon RCMP search for injured driver with unmanned aerial vehicle. http://www.rcmp-grc.gc.ca/sk/news-nouvelle/video-gallery/video-pages/search-rescue-eng.htm. Ren, X. and Malik, J. (2003). Learning a classification model for segmentation. In IEEE International Conference on Computer Vision, pages 10­17. Rijsbergen, C. V. (1979). Information Retrieval. Butterworth-Heinemann, 2 edition. Sedha, R. (2008). A Textbook of Electronic Circuits. S. Chand & Company LTD. Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888­905. Sinha, A. and Papadakis, P. (2013). Mind the gap: Detection and traversability analysis of terrain gaps using LIDAR for safe robot navigation. Robotica, pages 1­17. Song, S. and Xiao, J. (2014). Sliding shapes for 3D object detection in depth images. In European Conference on Computer Vision, pages VI: 634­651. Stark, L. and Bowyer, K. (1991). Achieving generalized object recognition through reasoning about association of function to structure. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(10):1097­1104. Stark, M., Lies, P., Zillich, M., Wyatt, J. L., and Schiele, B. (2008). Functional object class detection based on learned affordance cues. In International Conference on Computer Vision Systems. Sy, N., Avila, M., Begot, S., and Bardet, J.-C. (2008). Detection of defects in road surface by a vision system. In IEEE Mediterranean Electrotechnical Conference, pages 847­851. Tadokoro, S. (2005). Special project on development of advanced robots for disaster response DDT Project. In IEEE Workshop on Advanced Robotics and its Social Impacts, pages 66­72. IEEE. Tangelder, J. W. and Veltkamp, R. C. (2008). A survey of content based 3D shape retrieval methods. Multimedia Tools and Applications, 39(3):441­471. Trucco, E. and Verri, A. (1998). Introductory techniques for 3-D computer vision, volume 201. Prentice Hall Englewood Cliffs. U.C.R.T. (2013). USAR (Urban Search and Rescue) CBRNE (Chemical, Biological, Radiological and Nuclear) Response Team (U.C.R.T.). http://www.opp.ca/ecms/index.php?id=69. Uijlings, J. R., van de Sande, K. E., Gevers, T., and Smeulders, A. W. (2013). Selective search for object recognition. International Journal of Computer Vision, 104(2):154­171. Wilson, S. S., Gurung, L., Paaso, E. A., and Wallace, J. (2009). Creation of robot for subsurface void detection. In IEEE Conference on Technologies for Homeland Security, pages 669­676. Winston, P. H., Binford, T. O., Katz, B., and Lowry, M. (1983). Learning physical descriptions from functional definitions, examples, and precedents. In Association for the Advancement of Artificial Intelligence, page 433. Yang, M.-H., Kriegman, D., and Ahuja, N. (2002). Detecting faces in images: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(1):34­58. Yao, B. and Fei-Fei, L. (2010). Modeling mutual context of object and human pose in human-object interaction activities. In IEEE Conference on Computer Vision and Pattern Recognition, pages 17­24. 53

BIBLIOGRAPHY

BIBLIOGRAPHY

Yates, D. and Paquette, S. (2011). Emergency knowledge management and social media technologies: A case study of the 2010 Haitian earthquake. International Journal of Information Management, 31(1):6­13.

54

Glossary
3D Three-dimensional CRASAR Center for Robot-Assisted Search and Rescue Disaster A major destructive event that cannot be managed with the resources of a single community Emergency An impending calamitous event EMS Emergency Medical Services FEMA Federal Emergency Management Agency FOV Field of View GPS Global Positioning System GPR Ground Penetrating RADAR Hazard The potential for damage HUSAR Heavy Urban Search and Rescue IR Infrared N-CART Network-Centric Applied Research Team OPP Ontario Provincial Police RADAR RAdio Detection And Ranging RGB-D Red-Green-Blue-Depth SAR Search and Rescue TF Task Force Triage The assessment of initial reconnaissance UAV Unmanned Aerial Vehicle USAR Urban Search And Rescue

56


