THE ROLE OF THE MIRROR NEURON SYSTEM IN BOTTOM-UP AND TOP-DOWN PERCEPTION OF HUMAN ACTION by Lucy Marjorie Joanne McGarry Bachelor of Science, University of Toronto, 2008 Master of Arts, Ryerson University, 2011 A dissertation presented to Ryerson University in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Program of Psychology

Toronto, Ontario, Canada, 2015 © Lucy McGarry, 2015

I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my dissertation may be made electronically available to the public.

ii

The role of the mirror neuron system in bottom-up and top-down perception of human action Doctor of Philosophy, 2015 Lucy Marjorie Joanne McGarry Psychology Ryerson University

Abstract
When we see or hear another person execute an action, we tend to automatically simulate that action. Evidence for this has been found at the neural level, specifically in parietal and premotor brain regions referred to collectively as the mirror neuron system (MNS), and the behavioural level, through an observer's tendency to mimic observed movements. This simulation process may play a key role in emotional understanding. It is currently unclear the extent to which the MNS is driven by bottom-up automatic recruitment of movement simulation, or by top-down (task driven) mechanisms. The present dissertation examines the role of the MNS in the bottom-up and top-down processing of action in the auditory and visual modalities, in response to emotional and neutral movements performed by humans. Study 1 used EEG to demonstrate that the MNS is affected by bottom-up manipulations of modality, and shows that the MNS is activated to a greater extent towards multi-modal versus unimodal sensory input. Study 2 employed an EEG paradigm utilizing a top-down emotion judgment manipulation. It was found that the left STG, part of the extended MNS, is affected by top-down manipulations of emotionality, but there were no areas in classical MNS that met the statistical threshold to be affected by top-down forces. Study 3 employed an fMRI

iii

paradigm combining bottom-up and top-down manipulations. It was found that the classical MNS was strongly affected by bottom-up differences in emotionality and modality, and minimally affected by the top-down manipulation. Together, the three studies presented in this dissertation support the premise that the classical mirror neuron system is primarily automatic. More research is needed to determine whether top-down manipulations can uniquely engage the MNS.

iv

Acknowledgements
This dissertation could not have been completed without the help and support of a number of key people. Thank you to Dr. Frank Russo and Dr. Todd Girard for serving on my supervisory committee, your guidance throughout the process, availability for questions and edits, and your encouragement. To Dr. Frank Russo: your guidance as my PhD advisor has been invaluable. Not only have I been inspired by your creative approach to research, but I have learned a lot from you - from research methodology to dissemination and application of results. Thanks to Dr. Jaime Pineda and Dr. Emily Cross for allowing me to visit each of your labs and providing me with valuable training in EEG and fMRI methods. Thanks to Raj Sandhu and Matt Schalles for stimulating research discussions. Thanks to Lisa Chan, Arla Good, Lisa Liskovoi, Katie Peck, and Gabe Nespoli, my peers in the SMART lab, for your friendship and support. Thanks to my research assistants Emma, Mia, Saul, Tristan, James, and Stephanie, who helped me collect what seemed to be endless amounts of data. Thanks to my dad for always believing in me, for teaching me to write, and for teaching me to be a critical thinker. Thanks to my mom for your love and support, and for always knowing the right answers. Thanks to Katie for being a supportive and inspiring sister. Thanks to Dan for your creative thoughts, delicious dinners, and infinite patience.

v

Table of Contents
Title ...................................................................................................................................... i Abstract .............................................................................................................................. iii Acknowledgements ............................................................................................................. v Table of Contents ............................................................................................................... vi List of Tables ................................................................................................................... viii List of Figures .................................................................................................................... ix List of Appendices .............................................................................................................. x Chapter I. General Introduction .......................................................................................... 1 The Mirror Neuron System ................................................................................................. 3 Measurement of the MNS ................................................................................................... 6
Single-cell recording .................................................................................................................... 6 Non-invasive methods .................................................................................................................. 7 Stimuli known to engage the MNS ............................................................................................ 17 Procedures that engage the MNS ............................................................................................... 21 The presence of an MNS throughout development.................................................................... 22

Bottom-up and top-down influences on MNS functionality............................................. 24
Research on bottom-up/top-down influences on MNS responsivity ......................................... 24 Bottom-Up Research Paradigms ................................................................................................ 28 Top-Down Research Paradigms ................................................................................................. 28 Emotionality and the MNS......................................................................................................... 29

Current Studies.................................................................................................................. 30 Chapter II. Study 1 ............................................................................................................ 33 Abstract ............................................................................................................................. 34 Introduction ....................................................................................................................... 35 Methods............................................................................................................................. 40
Participants ................................................................................................................................. 40 Experimental Design .................................................................................................................. 40 Apparatus ................................................................................................................................... 41 Stimuli ........................................................................................................................................ 41 Data Analysis ............................................................................................................................. 42

Results ............................................................................................................................... 44
Electrode space analyses ............................................................................................................ 44 Independent components analyses ............................................................................................. 46

Discussion ......................................................................................................................... 51 Conclusions ....................................................................................................................... 56 Chapter III. Study 2........................................................................................................... 57 Abstract ............................................................................................................................. 58 vi

Introduction ....................................................................................................................... 59 Methods............................................................................................................................. 65
Participants ................................................................................................................................. 65 Design ........................................................................................................................................ 65 Stimuli ........................................................................................................................................ 65 Apparatus ................................................................................................................................... 68 Procedure.................................................................................................................................... 68 Data analysis .............................................................................................................................. 69

Results ............................................................................................................................... 70
Left-Central Cluster Analysis .................................................................................................... 71 Right Precentral Gyrus Cluster .................................................................................................. 74 Other Clusters............................................................................................................................. 77 Gender Differences .................................................................................................................... 77

Discussion ......................................................................................................................... 77 Chapter IV. Study 3 .......................................................................................................... 85 Abstract ............................................................................................................................. 86 Introduction ....................................................................................................................... 88 Methods............................................................................................................................. 96
Participants ................................................................................................................................. 96 Stimuli ........................................................................................................................................ 97 Apparatus ................................................................................................................................... 98 Design ........................................................................................................................................ 99 Procedure.................................................................................................................................. 100 Localiser: (Execute  Listen)  (Execute  View) ............................................................... 102 Data analysis ............................................................................................................................ 105

Results ............................................................................................................................. 106
Behavioral analyses .................................................................................................................. 106 Mask analyses: Auditory OR visual MNS regions .................................................................. 110 The Davis Empathy Scale ........................................................................................................ 116

Discussion ....................................................................................................................... 120
Behavioral Data ........................................................................................................................ 121 Imaging data ............................................................................................................................. 122

Conclusions ..................................................................................................................... 130 Chapter V: General Discussion ....................................................................................... 133
Top-down and bottom-up findings across studies ................................................................... 136 Bottom-up findings across studies: Implications and future directions ................................... 137 Top-down findings across studies: Implications and future directions .................................... 140 The temporal gyrus, movement simulation, and emotion ........................................................ 144 Modality and Emotion.............................................................................................................. 145

Conclusions ..................................................................................................................... 146 Permissions ..................................................................................................................... 166 References ....................................................................................................................... 178 ! vii

List of Tables
Chapter III Table 1. ............................................................................................. 74 Chapter IV Table 1. ............................................................................................. 98 Table 2. ............................................................................................ 117

viii

List of Figures
Chapter I. Figure 1............................................................................................... 5 Chapter II. Figure 1. ............................................................................................ Figure 2. ............................................................................................ Figure 3. ............................................................................................ Figure 4.............................................................................................. Figure 5.............................................................................................. Chapter III. Figure 1. ............................................................................................ Figure 2. ............................................................................................ Figure 3. ............................................................................................ Figure 4. ............................................................................................ Figure 5. ............................................................................................ Figure 6. ............................................................................................ Chapter IV. Figure 1a. ......................................................................................... Figure 1b. ......................................................................................... Figure 2a. ......................................................................................... Figure 2b. ......................................................................................... Figure 4a. ......................................................................................... Figure 4b. ......................................................................................... Figure 4c(i). ...................................................................................... Figure 4c(ii). ...................................................................................... Figure 4d. ......................................................................................... Figure 4e.......................................................................................... Figure 4f. ......................................................................................... 100 103 105 105 112 113 114 114 115 116 117 68 72 73 75 76 77 45 46 47 48 50

ix

List of Appendices
Appendix A. .................................................................................................................... 147 Appendix B. .................................................................................................................... 163

x

Chapter I. General Introduction

1

A sensory-motor brain network, referred to as the mirror neuron system (MNS), mirror system (MS) or as part of the action-observation network (AON), is considered to be involved in both the execution and perception of action (Rizzolatti & Craighero, 2004; Buccino et al., 2001; Iacoboni, 2005). When a person perceives another person engaging in an action, the same pattern of neural firing is thought to occur as when that person engages in the same action themselves. This multi-functionality of the MNS has been thought to underlie the understanding of intentions behind actions (Umilta et al., 2001; Iacoboni, 2005), as well as empathy (Iacoboni & Dapretto, 2006). Since the discovery of mirror neurons in macaques in the 1990's, many empirical research studies have been conducted examining mirror neuron functionality in monkeys as well as in humans (Rizzolatti & Craighero, 2004). While it is rarely possible to safely examine neuronal activity at the single-neuron level in humans, investigations of neuronal populations using EEG, MEG, and fMRI paradigms support the premise that the MNS functions the same way in humans as in monkeys (for a review, see Rizzolatti & Craighero, 2004). Many of these studies compare conditions in which human action is observed compared to non-human movement. Numerous controls are put in place during these studies in order to ensure that stimuli are engaging and that participants are attending to the stimuli; for instance, by adding a stimulus processing task (Oberman, 2005; Oberman, Ramachandran & Pineda, 2008). Thus, it seems reasonable to ask whether these controls are in fact necessary to engage the system. In other words, does the manner in which stimuli are processed affect the degree to which the MNS is engaged? Based on the available evidence, it is currently

2

unclear to what extent engagement of the human MNS is affected by top-down versus bottom-up influences. In addition, the role of various bottom-up sources of activation in the MNS could be further elucidated. Research on stimulus modality suggests that the MNS is more responsive to visual than auditory stimuli (though both do activate the MNS) and most towards audio-visual stimuli. In addition, emotionality of actions appears to stimulate greater activation in the MNS than would normally be the case. However, there are still outstanding questions related to these types of bottom-up sources of activation. Study 1 of the current dissertation examines the effects of bottom-up manipulations of identical stimuli perceived through different sensory modalities on MNS responsiveness, using EEG measurement of the mu wave, and in the process, examines the usefulness of an EEG paradigm to assess MNS engagement. Study 2 examines the effects of top-down manipulations of judgments of identical stimuli on MNS responsiveness, using EEG measurement of the mu wave. Study 3 examines both bottomup and top-down manipulations on MNS responsiveness, using an fMRI paradigm.

The Mirror Neuron System
I will begin by reviewing the literature on the mirror neuron system. In the 1990's a system of neurons, later to be named the MNS, was discovered in the monkey brain that appeared to play an important role in the understanding of movement intention (di Pellegrino, Fadiga, Fogassi, Gallese, & Rizzolatti, 1992; Gallese, Fadiga, Fogassi & Rizzolatti, 1996; Rizzolatti, Fadiga, Gallese, & Fogassi, 1996; for a review see Rizzolatti & Craighero, 2004). In this system, a single neuron will fire in a similar manner during execution of an action and observation of the same action (di Pellegrino et al., 1992).

3

Additionally, intention-mirroring neurons will fire towards the execution and observation of different actions that convey similar intentions (Umilta et al., 2001). The same system has since been observed in humans, through multiple fMRI (Carr et al., 2003; Cross, Hamilton & Grafton, 2006), EEG (Pineda, 2005; 2008), and MEG (Hari et al., 1998) studies, as well as a single-cell recording study published in 2010 (Mukamel, Ekstrom, Kaplan, Iacoboni, & Fried, 2010). Although claims about the function of the MNS have frequently been exaggerated in the scientific literature and oversimplified by the media, a consensus is emerging that a MNS exists in humans and that it serves a similar function to that in monkeys (Molenberghs, Cunnington & Mattingly, 2010; Keysers & Gazzola, 2010; Kilner & Lemon, 2013; Mukamel et al., 2010). However, it is important to acknowledge that serious criticisms have been raised regarding the role of the MNS in action understanding, particularly in humans (Hickok, 2009; Turellaa, Piernoa, Tubaldia, & Castiello, 2009). In the current line of studies, the MNS is purported to exist in humans at the level of systems of neurons, rather than making claims about individual neurons per se. Anatomically, the human MNS consists of sets of neurons in the inferior frontal gyrus (IFG), ventral premotor cortex (vPMC), inferior parietal lobule (IPL), and precentral gyrus (PCG). Information enters the superior temporal sulcus (STS) via visual and auditory sensory areas. Activation in the STS refers to activation in the area in between its neighbouring gyri (superior temporal gyrus (STG) and medial temporal gyrus (MTG)). From the STS, information is projected to the vPMC, IFG, and IPL. These mirroring areas also project information onward to the sensorimotor cortices. The STS does not appear to possess its own mirroring properties but it does integrate auditory and

4

visual information before it is sent onward to the MNS (Keysers et al., 2003), in addition to its other functions. The extended MNS is thought to include a much wider range of areas that cooperate with the MNS to enhance empathy during perception of movement by activating areas of the brain that would be activated if one were performing the perceived movement him/herself (Pineda, 2008). These include the limbic system (Carr et al., 2003; Mukamel et al., 2010), extending through the medial temporal lobe and middle temporal gyrus (Pineda, 2008; Mukamel, 2010), as well as the STG and STS (Pineda, 2008). See Figure 1 for a visual depiction of the core MNS.

Figure 1. The core MNS. Information enters through visual and auditory streams, and affects processing in the STS, which feeds onward to parietal and frontal mirroring areas. Extended mirroring areas such as the insula and limbic system are thought to communicate with the MNS during movement perception.

5

Measurement of the MNS
Single-cell recording
MNS activation has been measured in monkeys and then humans using several different parameters. When mirror neurons were first discovered in monkeys, researchers had been employing a single-cell recording paradigm in order to measure differences between object-oriented and non-object-oriented motor representations of actions in area F5 of the monkey brain (di Pellegrino et al., 1992). They incidentally found neurons that were responsive to both the observation and the execution of movement in monkeys, a property that had not previously been observed. This was a revolutionary finding ­ the idea that a certain cell can exhibit multiple functions, functions advanced enough to pair a motor movement with a matching observed movement, as well as different movements directed at a matching goal. Single-cell recording involves inserting electrodes directly into the brain, and is therefore the most invasive procedure of those being outlined here. This makes the single-cell approach rare in humans, as there are few circumstances in which it is ethical to do this type of study. Mukamel et al. (2010) did perform such a study in humans, in epileptic patients who already had to undergo a clinical procedure involving single-cell recordings. The researchers asked participants to view hand grasping actions and facial expressions while single cells were recorded in the medial frontal and temporal parts of the brain. They found that, indeed, there were several areas possessing mirroring properties; i.e., areas that were responsive during movement observation as well as movement execution. Areas including neurons that exhibited these observation-execution matching properties were found in the supplementary motor area as well as the hippocampus. These are promising findings for the mirror neuron field, 6

although future studies that replicate and possibly extend these findings will need to be conducted when possible.

Non-invasive methods
In humans, many studies of the mirror neuron system have been conducted using functional magnetic resonance imaging (fMRI), positron emission tomography (PET), electroencephalography (EEG), magnetoencephalography (MEG) and transcranial magnetic stimulation (TMS). While these methods are not able to measure activation at the single-cell level, they can give us valuable information about the activity of groups of cells in locations of the brain considered to contain mirror neurons. Some of these methods will be elaborated upon below. fMRI. fMRI is spatially sensitive to blood-oxygenation. Changes in bloodoxygenation occurring during different experimental conditions provide an indication of the neural bases of behavior and cognition (Devlin, 2007; Logothetis, Pauls, Augath, Trinath & Oelterman, 2001). fMRI can only measure activation over groups of neurons; thus, single neurons with mirroring properties cannot be measured using fMRI. However, inferences can be made based on the activity of groups of neurons. For example, if a larger portion of the brain is activated during audio-visual perception of action than during visual perception of action alone, this could indicate that a greater number of neurons are active in that area during audio-visual action perception. In addition, if the same area is activated during the observation and perception of action, we can assume that spatially similar, if not identical, sets of neurons are activated. One limitation of fMRI is that it is difficult to know whether neurons are excitatory or inhibitory based on measuring collective blood-oxygenation levels over an area. However, when we

7

complement fMRI methods with other techniques in animal homologues or in humans, this helps to gain a more complete understanding at the neuronal level. In an early fMRI study of the MNS, Buccino et al. (2001) asked individuals to observe object- and non-object-oriented hand, mouth and foot actions while in the scanner. He found that observation of action activated the premotor cortex, an area involved in motor execution, in a somatotopically-organized fashion resembling the organization of the same area for movement execution. For instance, the hand observation area of the premotor cortex was in the same location as the hand execution area would be expected to be located. Observation of object-oriented movements also activated the posterior parietal lobe in a somatotopically-organized fashion resembling patterns observed during execution of object-oriented movements. The authors concluded that an internal replica of an action is created in these motor areas of the brain when observing someone else's movements. Similar findings have been shown in monkeys, using single-cell recordings. Kaplan & Iacoboni (2007) conducted a study measuring auditory-visual integration of the MNS using fMRI in humans. This study was an extension of a study conducted by Keysers et al. (2003) in monkeys. Keysers et al. (2003) used a single-cell recording method to show that various cells in area F5 of the monkey brain were responsive to various combinations of perceptual modalities. There were certain neurons only responsive to auditory information, and other neurons only responsive to visual information. In addition, there were neurons that were only responsive to audio-visual information and still others that were responsive to any combination of auditory and visual information. This pattern of results suggests that there would be supra-additivity

8

when observing movements audio-visually, since audio, visual, audio-visual, and uni/bimodal responsive neurons would fire during the perception of audio-visual movement, whereas only a fraction of these cells (just visual-only or just audio-only) would fire during unimodal movement. Kaplan & Iacoboni (2007) examined whether there is also supra-additivity in responsiveness towards audio-visual activity in the MNS in humans. They found that there was supra-additivity of this information in the ventral premotor cortex, which is a homologue of the monkey area F5. Since they found supraadditivity in the vPMC, they were able to conclude that there is likely a similar distribution of auditory, visual, audio-visual inclusive, and audio-visual exclusive cells present in the human homologue of the monkey area F5. Study 1 of the current dissertation involves a replication of this finding using EEG measurement of mu ERD (another method for examining MNS activation, to be elaborated upon below). Other fMRI studies corroborate single-cell evidence in monkeys, while providing additional insight about the role of the MNS in humans. For instance, Iacoboni et al. (2005) examined the role of the MNS in processing intentions behind movements. They found that actions embedded within a context (making an action look as though it was intended for drinking or for cleaning) lead to enhanced activation in the posterior part of the IFG as well as the vPMC. The authors concluded that these branches of the MNS are involved in automatic processing of the goal behind an action. This study was testing for the presence of a similar function to that previously found in monkeys (Umilta et al., 2001). In the monkey studies, there were a number of neurons in F5 that were involved in the processing of the goal behind an action (Umilta et al., 2001). These neurons only fired

9

when an observer watched or executed a goal-directed action, and were not responsive to the specific action used to reach the goal. One way to help enhance the specificity of fMRI data, and demonstrate that the tasks manipulated in the experiment actually correspond to MNS activation in a specific sample, is to conduct an MNS-localiser task in each participant. Spunt & Lieberman (2012) did this in a study where they were examining the role of the MNS in understanding the "what" (direct action understanding), the "how" (overall intention processing), and the "why" (higher-order motive processing) of action understanding. Using a localiser allows the researcher to generate a unique region-of-interest mask for each participant, thereby increasing certainty that the areas under consideration are indeed mirroring areas. In the localiser task employed by Spunt & Lieberman (2012), participants were asked to (a) observe a video clip of a person pressing buttons with their right hand; or (b) execute the same set of button presses on the button box they were holding. To create the region-of-interest mask based on the localiser, they ran a conjunction analysis to include areas of the brain that were significantly activated during observation as well as execution trials. They found that a series of MNS-associated brain regions, including the posterior IFG (pIFG), vPMC, dorsal premotor cortex (dPMC), supplementary motor area (SMA), and rostral inferior parietal lobule (rIPL) were activated during both observation and execution. Evidence from fMRI studies completed to date provides promising support for the existence of mirror neurons in humans. Use of a localiser task to identify specific areas involved in execution and observation matching helps to bolster claims made in a given

10

experiment about the MS. In study 3 of the current dissertation, a localiser mask is employed in order to help identify mirror neuron areas in our participants. EEG & MEG. Electroencephalography (EEG) and magnetoencephalography (MEG) have also been widely used to examine the mirror neuron system in humans. EEG involves placing electrodes over the scalp in a non-invasive manner, usually using a cap to which EEG sensors are placed. Electrical activity at the scalp is measured, which is indicative of neuronal firing rates. MEG uses magnetic frequencies to measure electrical activity at the scalp, and measures the same type of activity as EEG (i.e., neuronal firing rates; Barnes, Hillebrand, & Hirata, 2010). Although these methods are not as spatially sensitive as fMRI, they are much more temporally sensitive. In addition, there have been major improvements in source localization methods over the last decade, which has helped close the gap with respect to spatial sensitivity (Delorme, Sejnowski, & Makeig, 2007). There are several brain rhythms that are characteristic of different types of brain activity, measured via EEG and MEG. The alpha wave (measured at 8-12 Hz, or 8-12 cycles per second) is commonly induced during resting or sleep states, with slower waves (theta, delta) becoming prominent as a person falls into a deep sleep (Buzsaki, 2006). Beta (15-25 Hz) and gamma (> 25 Hz) waves are generally of low power and characteristic of brain activity localized to specific brain areas, while alpha tends to be higher in power and characteristic of slightly more widespread brain activity, indicating communication or synchronization between brain areas (Buzsaki, 2006). The mu wave is the brainwave most linked to MNS activity (Pineda, 2005, 2008). Mu contains components in the alpha and beta frequency range, with relatively more

11

power in alpha. The mu wave can be detected by recording from electrodes placed above the motor cortex (Pineda, 2005, 2008). The mu wave was initially considered to be a resting-state brain rhythm, with a similar function to alpha. This is because both waves are most synchronized (i.e., higher power at the level of the scalp) when eyes are closed and when the brain lacks external stimulation. However, Cohen-Seat et al. (1954) discovered that whereas the regular alpha wave, measured over the occipital cortex, is desynchronized most strongly while the eyes are open, the mu wave requires more conditions to be met in order to desynchronize. Specifically, it is only when an individual observes or executes an action that the mu wave is characteristically desynchronized. Many more studies since this discovery have supported the premise that the mu wave is indicative of mirror neuron activation (Cochin, Barthelemy, Lejeune, Roux, & Martineau, 1998; Pfurtscheller & Aranibar, 1979; Pfurtscheller, Neuper, Andrew & Edlinger, 1997; Pfurtscheller, Zalaudek & Neuper, 1998; Pfurtscheller & Lopes da Silva, 1999; for a review see Pineda, 2008). For instance, Pfurtscheller et al. (1997, 1998) conducted several studies in which they found that during the observation and execution of hand, foot, and other body movements, the mu wave becomes desynchronized compared to observation of non-human movement. Laufs et al. (2003a; 2003b) found that mu wave desynchronization is directly correlated with an increase in the BOLD response in frontal and parietal areas of the brain that have been associated with the MNS. Because of these strong associations between the mu wave and other measures of MNS activity, EEG/MEG measurement of the mu wave has become another important tool for indexing MNS activity. A benefit of EEG and MEG is that these methods are time-sensitive, which

12

means that the time-course of MNS activation can be measured more specifically using this method (Pfurtscheller & Lopes da Silva, 1999). As noted above, traditional EEG/MEG methods are not as spatially sensitive as fMRI. This is because EEG/MEG only measures summed activity of neurons from various areas of the brain as they accumulate at the scalp. Therefore, when measuring activity over the frontal cortex, for example, this activity will typically include projections from the occipital cortex, temporal cortex, and parietal cortex. Because of this, conclusions about the spatial specificity of different frequency patterns have to remain tentative until corroborated with other methods. However, in recent years, source localization procedures have become more refined and are being used more often in EEG/MEG research to identify sources of activation (e.g., Moore, Gorodinsky & Pineda, 2011). One common source localization procedure is called an independent components analysis (ICA; Delorme et al., 2007; Delorme & Makeig, 2004). This procedure involves taking account of the multivariate signal measured at each recording site (e.g., each of 32-, 64- or 128-electrodes in and EEG system) and decomposing it into component signals that are statistically independent and presumably reflective of underlying brain sources. This type of procedure is useful to the EEG/MEG community, as it allows for determination of brain areas contributing to activation at the scalp (Vigario, Sarela, Jousmaki, Hamalainen & Oja, 2000). It is particularly valuable to mu research because it allows for a new level of understanding of the neural sources of mu. Not only does this bolster the idea that that mu activity arising above the motor cortex has indeed originated from the MNS, but it also helps us to differentiate mu from occipital alpha. In addition, sometimes occipital alpha becomes mixed with mu over the motor cortex, as occipital

13

projections hit the upper part of the scalp (eg., Moore, Gorodinsky, & Pineda, 2011). This creates noise in detecting mu specifically, as alpha is desynchronized under any condition in which the eyes are open, instead of specifically action-associated conditions. With the use of ICA, alpha-associated brain components can be separated from mu based on the location of their source dipole in the brain, thus denoising mu data and allowing for a more powerful look at the conditions that lead to desynchronization of the mu wave. ICA-based source localization procedures have been shown to have a spatial sensitivity of 4 mm ­ 20 mm (Acar & Makeig, 2013), and thus may sometimes possess comparable spatial sensitivity to fMRI resolution, which is commonly placed at 3 mm in humans (Toga & Mazziota, 2002). In addition to the studies outlined above, several researchers have used the mu wave to examine intention processing and emotional processing in the MNS. Oberman et al. (2005) used mu event-related desynchronization (ERD) to demonstrate that the mu wave is less responsive in individuals with autism towards movements of other people than it is in neurotypicals. Meanwhile, execution of action activates the mu rhythm to the same extent in autistic and neurotypical individuals (Oberman et al., 2005). In a more recent study, Oberman et al. (2008) found that the mu rhythm is engaged in autism to a greater extent towards familiar than unfamiliar individuals. This suggests that MNS responsivity towards others can increase with familiarity in this group. Perry, Troje, & Bentin (2011) measured mu-ERD towards point-light walkers in various conditions, and found that there was a greater amount of mu ERD towards walkers when the viewer had to make a judgment about whether the walker was headed toward them or away from them, or whether the walker's movements were happy or sad, versus just making a

14

gender judgment of the walker. This suggests that the MNS may behave differently depending on judgment conditions. Thus, event-related desynchronization of the mu rhythm is a well-established method of measuring MNS functionality and has revealed many properties of the MNS in humans. Just as with fMRI, what we can discern from these studies is limited to identifying activity across groups of neurons instead of at the individual-neuron level, and thus in order to be certain that we are measuring MNS activity, single-cell follow-up studies are needed when possible. However, EEG/MEG measurement of the mu rhythm provides a promising route to understanding MNS functionality in a time-sensitive manner. TMS. Another common method used to examine MNS activity involves using transcranial magnetic stimulation (TMS). TMS involves sending small electrical currents into the brain, resulting in either depolarization or hyperpolarization of specific brain areas (Barker & Freeston, 2007). This method allows researchers to move beyond correlational approaches to understanding the brain by systematically disabling or enhancing activity in specific areas. When stimulation enables activation over the motor cortex, motor-evoked potentials (MEPs) can be recorded from the muscles of contralateral extremities such as those found in the hand (Rizzolatti & Craighero, 2004). The amplitude of the MEP indexes the amount of activity elicited in the corresponding part of the motor cortex. Fadiga, Fogassi, Pavesi & Rizzolatti (1995) used TMS to stimulate activation in the motor cortex during observation of object-directed arm gestures as well as objectabsent intransitive gestures. It was found that both of these types of movements lead to

15

enhanced MEP's in corresponding muscles of the observer's arm as those used to complete the movement in the performer. These findings, replicated by Maeda et al. (2002), indicate that in humans, the MNS responds to both transitive (object-oriented) and intransitive (non-object-oriented) actions, rather than only object-oriented actions like in monkeys (Rizzolatti & Craighero, 2004). Gangitano et al. (2001) also found that the time course of MEP activation during observation of movements directly mirrors the time course of the movement itself. This supports the premise that a direct simulation of motor activation is taking place when an individual processes another person's movements. When TMS is applied at a higher current, this can result in the temporary disabling of a specific area of the brain, with minimal discomfort for the participant. Keuken et al. (2011) used repeated TMS (rTMS) to disable a key area of the MNS, namely the left IFG, while participants made emotion judgments of emotional videos. As compared to participants who had a non-MNS-associated area disabled (the vertex), disabling the IFG lead to slower reaction times identifying emotions in videos, and it also completely disabled desynchronization of the 8-12 Hz mu rhythm, bolstering the claim that the mu rhythm directly measures MNS activity. This study provided further support that the MNS is involved in the perception of social information. In sum, enhancing or inhibiting brain function in a specific area using TMS is a powerful way to determine the functionality of the MNS, because the experimenter no longer has to rely solely on correlations between brain responses and experimental behaviour. Instead, the researcher can experimentally manipulate localized brain activity to examine corresponding changes in behaviour.

16

Stimuli known to engage the MNS
Biological movement. Next, I will review different types of stimuli known to engage the MNS. Generally, biological movement of conspecifics or execution of a matching movement activate the MNS most strongly (Buccino et al., 2001). It has also been found that experience executing a movement leads to the greatest degree of responsiveness in the MNS during later observation of that movement. For instance, Cross et al. (2006) asked experienced dancers to observe a video of a complex sequence of dance movements, before and after receiving training in the execution of those dance movements. Following training, MNS activation during mere observation was greater than it was before training. Similarly, Calvo-Merino et al. (2006) measured MNS activation in trained dancers, trained capoeira fighters, or novices in either discipline, and found that experts exhibited greater MNS activation during observation of movements from their discipline only. Thus, movements that are already a part of an observer's movement history are more likely to engage the MNS. When an individual observes a moving stimulus that is not human, this is not generally expected to engage the MNS. However, there are some non-human stimuli that have also been found to engage this system. For instance, researchers will sometimes use point-light animations of human action to portray dynamic movement, while controlling for the presence of static visual attributes, in order to demonstrate that responses are elicited by the movement itself and not the static attributes of a stimulus such as facial expression (Ulloa and Pineda, 2007; Perry, Troje & Bentin, 2010). There have also been several studies in which robot actions have been found to engage the MNS (Cross et al., 2012; Engel, Burke, Fiehler, Bien, & Rosler, 2008a). In most cases it is presumed that the

17

robot actions resemble human qualities enough that they come to activate the MNS as though a human were moving. However, in one case by Cross et al. (2012), it was found that robotic actions activated the MNS even more strongly than human actions. This anomaly was followed up and replicated, and the effect remained the same. Interestingly, the robotic movements elicited this greater degree of activation regardless of whether the actions were completed by a robot or a human, and the movements themselves were not easily executed by participants upon request. The authors were obviously perplexed by these findings, since generally, MNS activation is increased when a movement is familiar and when the perceiver knows how to execute that movement (Cross et al., 2006; CalvoMerino et al., 2006). The authors surmised that the relationship between familiarity with action execution and MNS activation during observation could actually be non-linear. They suggested that perhaps highly familiar actions as well as highly non-familiar actions both engage the MNS to a greater degree than somewhat familiar actions. It is also possible that the robotic movements employed in their study were highly interesting, and that this lead to a greater top-down recruitment of action areas in the brain in an attempt to neurally simulate, and thus understand, this challenging new type of movement. Object-oriented movement. Another type of movement known to elicit MNS activation, particularly in the IPL, is object-oriented movement. Based on single-cell recordings in monkeys, and fMRI studies of groups of neurons in humans, it is assumed that humans possess canonical mirror neurons in the parietal cortex that function in a similar way to those in monkeys (Rizzolatti et al., 1988; 1998). Canonical mirror neurons are engaged merely at the presentation of an object to which action can be applied (Grezes, Armony, Rowe, & Passingham, 2003). There are also mirror neurons in the IPL

18

that are especially responsive to object-directed movements (Grezes et al., 2003; Rizzolatti, 1988; 1998; Sakata, 1995), and mirror neurons in the IFG that are responsive to the goal behind an action when it is object-directed, rather than the exact action pattern itself (Umilta et al., 2001; Iacoboni et al., 2005), as mentioned above. Thus, an objectoriented movement is likely to engage the MNS to a greater degree, since it is assumed to engage individual mirror neurons involved in action perception alone, object oriented actions, the goal behind the action, and canonical mirror neurons that are responsive to the object. Different perceptual modalities. Vision and audition are the most commonly studied modalities found to engage the MNS (Keysers et al., 2003; Kaplan & Iacoboni, 2007; Gazzola et al., 2006), although there is increasing evidence for tactile engagement (e.g., Bolognini et al., 2012). Generally, activation occurs in a somatotopic manner that matches across auditory and visual domains (Buccino et al., 2004; Gazzola et al., 2006). As noted earlier, different classes of neurons in the monkey MNS respond exclusively to auditory action, visual action, or audio-visual action, while others respond to both unimodal and multimodal action (Kohler et al., 2002; Keysers et al., 2003). Thus, a multimodal representation of action would be hypothesized to activate the MNS more strongly than a unimodal representation of action. This hypothesis was confirmed via activation patterns in the vPMC in an fMRI study by Kaplan & Iacoboni (2007). Speech and song. Since area F5 in the monkey (where audio and visual mirror neurons are found) is a homolog of Broca's area in humans (where speech is produced), many researchers have theorized that the MNS could have been involved in the evolution of speech production and perception (Rizzolatti & Arbib, 1998; Kohler et al., 2002;

19

Rizzolatti, 2005; Arbib, 2006), and in addition, it has been proposed that the MNS is involved in the evolution of song (Molnar-Szakacs & Overy, 2006). In the auditory domain, speech and song, which elicit movement primarily in the vocal tract as well as on the face, have been found to engage the MNS (Fadiga, Craighero, Buccino, & Rizzolatti, 2002; Buccino, et al., 2005; Leveque & Shon, 2013; Lahav, Saltzman & Schlaug, 2007). In a study by Buccino et al. (2005), it was found that listening to sentences involving hand and foot movements activated hand and foot reactions as measured by TMS-evoked MEPs, respectively, supporting the involvement of the MNS in processing spoken representations of movement. Leveque & Shon (2013) found that listening to melodies produced vocally elicited greater mu ERD activation, indicative of greater MNS activity, as compared to melodies produced via non-human tones. Lahav et al. (2007) asked participants to reproduce piano melodies. As the melodies were learned, MNS activation increased during listening alone, supporting the premise that experience with execution of a certain movement pattern enhances MNS responsivity during observation of that pattern. Molnar-Szakacs and Overy (2006) describe the reasons by which the MNS would be stimulated during music listening ­ when we listen to music, we are lead to infer the emotional movements of the person who created the music, and we create an internal simulation of those movements. This allows us to infer what movements were conveyed, and also to feel the emotion portrayed by the music, through feedback to our own emotional systems. Emotions. Movement is an essential part of emotion. Empathy can be defined as involving an internal simulation of another person (McGarry & Russo, 2011). It has been found that during emotion perception, the MNS is activated (Carr et al., 2003; Wicker et

20

al., 2003; Grosbras & Paus, 2006; Ferri et al., 2013), perhaps in order to create an internal simulation of the person's emotional movements (Carr et al., 2003; Wicker et al., 2003). This is thought to feed back into the observer's limbic system, where emotion is processed in the brain, to allow the observer to feel how the observed person might be feeling (Carr et al., 2003; Bastiaansen, Thious, & Keysers, 2009). Thus, the MNS is considered a precursor to one component of empathy (Bastiaansen, et al., 2009; McGarry & Russo, 2011).

Procedures that engage the MNS
Different researchers employ different task methodologies during measurement of the MNS, some of which are more passive and some of which are more active. Oberman et al. (2005, 2008) asked participants to count the number of pauses in ongoing movement of stimuli, in order to ensure ongoing attention. Spunt & Lieberman (2012) asked participants to make judgments about each stimulus in their fMRI study, because they were interested in the effects of different cognitive processes on MNS activation. Iacoboni et al. (2005) asked participants to either make explicit judgments or avoid making judgments about the intentionality of movements in their fMRI study, and found that results were the same regardless of whether a top-down task was employed. Meanwhile, Enticott, Johnston, Herring, Hoy & Fitzgerald (2008) did not ask participants for any judgments during a TMS procedure measuring motor responsivity via MEPs during the perception of human actions. Similarly, Fecteau, Pascual-Leone, & Theoret (2008) asked participants to passively view stimuli during their TMS/MEP procedure. It is presently unknown whether, and under what circumstances, top-down manipulations may enhance activation in the MNS and potentially the power to detect differences

21

between one's experimental conditions. It is also difficult to compare findings across studies, since it is unknown in many instances whether the presence or absence of a topdown manipulation leads to differential processing in the MNS.

The presence of an MNS throughout development
During development, the capacity to imitate others' movements is considered a key aspect of learning (Meltzoff & Moore, 1977; Lepage & Theoret, 2007). Many have theorized that the MNS is present in the human brain from birth, and that with learning, its responsivity towards observation and during execution of actions is refined. While more research needs to be conducted (Lepage & Theoret, 2007), a few neurophysiological studies in children suggest that the MNS functions in a similar manner. Dapretto et al. (2005), who conducted a comparison study of children with and without ASD, found that typically developing school-aged children exhibited activation in the pars opercularis of the IFG during both the execution and observation of emotional facial expressions, suggesting the presence of an MNS that functions similarly to adults. They also noted that children's scores for social adjustment were negatively correlated with activation in the IFG, suggesting that a developmental role in the IFG for understanding emotions that should be further explored. As the authors hypothesized, there was decreased activation in the IFG during perception of movements in children with ASD, further supporting the developmental role of the IFG in the understanding of emotional actions of other people. A few studies have also taken place to measure the existence of a MNS in younger infants. Harai & Haraki (2005) found that 8-month-olds exhibit neural patterns suggesting recognition of motor acts as early as 8 months of age. Shimada & Hiraki

22

(2006) performed a near infrared spectroscopy (NIRS) study to directly compare sensorimotor activation in adults and 6-7-month-olds, and found an identical pattern of responsivity during both execution and observation of hand-object interactions, supporting the existence of a similar neural simulation mechanism in infants during the perception of movement as exists in adults. In addition, the authors found that the infants exhibited similar MNS responsivity to objects moving on a TV screen ­ a pattern not found in adults. They suggested that the children did not understand that these objects were moving of their own volition and were not the result of a human acting upon an object (a condition which would be expected to elicit enhanced MNS activity). The authors suggested that throughout development, children learn that object movements are not always biologically driven, and thus will refine their levels of MNS responsivity towards these types of movements, distinguishing them from biological object-oriented movements as adults do. Fecteau et al. (2004) measured activation from subdural electrodes on the scalp of a 36-month-old child, and it was observed that activity in the alpha band was desynchronized in a similar pattern to an adult during execution and observation of hand actions. Lepage and Theoret (2006) also conducted a scalp EEG study of 15 children aged 52-133 months, and demonstrated that mu amplitude was decreased at C3 and C4 electrodes, the electrodes commonly used to measure mu ERD, during action execution and observation. They found that there was no correlation between age and mu rhythm modulation in their study, as did Oberman et al. (2005) in a study comparing neurotypical and ASD children in levels of mu ERD, suggesting that MNS function is relatively stable throughout development.

23

Taken together, these findings suggest that a MNS exists in children and that it is present from early infancy. It appears that the MNS then refines itself as the brain develops, learning which actions can be neurally simulated to enhance understanding of those actions or to imitate them, and which cannot (Lepage & Theoret, 2007; Shimada & Hiraki, 2006). Further research should examine whether a mirror mechanism is present in the brain from the first time an infant observes an action, in order to determine the extent to which this mechanism is innate or forms throughout development.

Bottom-up and top-down influences on MNS functionality
It has already been suggested that certain roles of the MNS, such as its involvement in assessing higher-level intentions or emotions during observation of human action, could depend not only on bottom-up activation based on the perception of human motion, but also on top-down activation from areas specializing in socioemotional processing (Nishitani et al., 2004; Oberman et al., 2005). Top-down refers to processes that are controlled by the task, while bottom-up refers to processes that are stimulus-driven. It has also been suggested that the medial prefrontal cortex and the temporal pole modulate activation in the STS, in order to enhance attention to social aspects of stimuli (Allison et al., 2000; Castelli et al., 2002). This guiding activation could, in turn, affect the MNS through connections between the STS and the rest of the MNS (Rizzolatti, 2005); in effect producing a top-down influence on the MNS by socially-motivated higher-level brain areas.

Research on bottom-up/top-down influences on MNS responsivity
Thus far, little research has been conducted on the role of top-down influences on MNS activation. Iacoboni et al. (2005) asked participants to either make judgments about 24

the movement intention of stimuli, or did not request any judgments, and did not find a difference in MNS responsivity between these conditions, suggesting that the use of a judgment to enhance attention in one's experimental paradigm may not make a difference in MNS responsivity, and that MNS responsiveness may be driven primarily by bottomup forces. However, other lines of research on MNS responsivity during the perception of animated non-human stimuli suggests otherwise. One line of research examining the role of the MNS during top-down and bottom-up conditions relates to the perception of artificial object movements. Generally, it is held that the MNS is activated only towards animate stimuli. However, some studies of robot vs. human movements have found equivalent activation in the MNS towards each of these types of stimuli, whereas other studies have found more activation towards human movements, as traditional MNS theories would predict (Press, Gillmeister & Heyes, 2006; Engel et al., 2008a; Engel, Burke, Fiehler, Bien, & Rosler, 2008b). Press, et al. (2006) conducted a study in which human and robotic movements were compared. They found that MNS activation was greater towards human movements, even when participants' perception of animacy was not different between human and robotic actions. Thus, these authors concluded that bottom-up forces ultimately determine levels of MNS activation, regardless of conscious awareness of animacy. Conversely, Cross et al. (2012) found that robotic movements lead to greater MNS activation than natural human movements, whether performed by a human or a robot. They concluded that movements do not have to be familiar to the perceiver in order to be neurally simulated. These results are contradictory to many studies of the MNS, which have found that the MNS is preferentially responsive to actions that are within one's own

25

motor repertoire (Cross et al., 2006), and those actions that are performed by humans (Press et al., 2006; Engell et al., 2008b; Cochin et al., 1998). In order to clarify these mixed results regarding the MNS response to human and artificial movements, Engell et al. (2008b) compared top-down and bottom-up manipulations of MNS activation in response to artificial movements. Their hypothesis was that the mixed results found in other studies could be explained by whether a topdown manipulation was used during perception of artificial movements, which could have contributed to the degree of MNS responsivity. In their study, they compared differences between the stimuli (bottom-up processes) to differences between tasks in response to stimuli (top-down processes) during perception of human and robot actions. They manipulated stimuli by making human and robot movements smooth or discontinuous, or they manipulated task load by asking participants to make a judgment about the colour of the stimulus or of their own ability to simulate the action. They found greater MNS activation towards both robotic and human movements in the judgment condition invoking simulation. This finding indicates that it is not only stimulus qualities that can lead to changes in MNS activation, but also task demands. The authors concluded that during observation of artificial action, it is possible to create conditions where MNS activation is as great as during perception of true human action. Other scientists also present mixed views on the role of bottom-up and top-down processes in human motion perception. A series of studies were conducted in the late 1990's/early 2000's involving the optimal circumstances through which to perceive human gait, and whether perception of human gait was guided more through bottom-up or top-down processes. Using point-light walkers, Mather, Radford and West (1992)

26

found that human motion analysis favoured shorter ITIs between successive frames depicting gait movement, supporting the premise that low-level visual mechanisms are important for perception of human gait. Thornton, Pinto and Shiffrar (1998) found that gait could be perceived at a range of temporal display rates, favoring a combination of top-down and bottom-up mechanisms in human gait. Cavenagh, Labianca and Thornton (2001) found that variations on attentional load affected perception of a point-light walker, also suggesting that top-down processes play a role in perception of gait. These studies did not investigate the brain mechanisms involved in the perception of gait, which include the MNS (Perry, Troje & Bentin, 2010). However, the behavioral findings indicate that there would be a difference in brain activation depending on the task manipulation. Thus, the extent of involvement of the MNS during top-down or bottom-up processing of human movement remains to be elucidated. One important reason to determine the degree to which MNS activation may be driven by top-down or bottom-up processes is to distinguish between different types of MNS activation. Top-down or bottom-up activation of the MNS may depend on the context of perception ­ for instance, if a person is guided to make an emotional judgment about another person, it is likely that higher-level cognitive processes influence the MNS to be activated more strongly. However, if a person is simply passively observing a moving stimulus, with no specific motivation, the MNS might merely be activated based on low-level sensory detection of the moving stimulus. There are likely different parts of the MNS that are activated by each type of manipulation, as well, just as different parts of the MNS are involved in intentional processing (Iacoboni, 2005) and emotional processing (Carr et al., 2003) of stimuli.

27

Bottom-Up Research Paradigms
A representative study is described next, exemplifying a typical study design that investigates bottom-up activation of the MNS during the perception of human movement. Kaplan and Iacoboni (2007) conducted an fMRI study measuring the degree of MNS activation differences between stimuli that were identical, but were presented in different sensory modalities known to evoke mirror neuron system activation ­ auditory, visual, and audio-visual combined. They found that there was greater activation towards audiovisual presentation of action stimuli than towards auditory or visual stimuli alone, suggesting that bottom-up influences such as modality do have an influence on responsiveness of the MNS to presented stimuli. The use of several modalities presenting the same stimuli is just one of many potential ways that bottom-up processing in the MNS can be manipulated. One advantage of this paradigm is that the actual stimuli being presented and processed are identical, with the bottom-up manipulation only coming into play in terms of modality. In addition, this paradigm allows one to address the question of how the MNS processes biological stimuli that are presented through different or multiple modalities, and, for instance, whether the MNS is more responsive to auditory or visual stimuli.

Top-Down Research Paradigms
There are several ways to examine the role of top-down processing in MNS activation during the perception of human movement. For instance, the amount of attention paid to stimuli could be manipulated. Alternatively, participants could be asked to attend to different attributes of stimuli.

28

Moore, Gorodinsky, and Pineda (2011) investigated potential top-down effects on MNS activation by showing participants pictures of emotional faces, and asking them to either empathize or make an unrelated judgment. They found only marginal differences in MNS activation for high and low empathy judgment conditions, thereby offering only limited support for the possibility of top-down influence. While few other studies have directly compared different levels of top-down activation on MNS processing, some examples of paradigms that have elicited varying levels of top-down processing are presented next. Oberman et al. (2005; 2008) used a procedure in which participants were asked to count the number of pauses in movement of stimuli, in order to ensure ongoing attention. In Spunt and Lieberman's (2012) fMRI study, participants were asked to make judgments about each stimulus, because they were interested in the effects of different cognitive processes on MNS activation. Generally in experiments in which a top-down procedure is used, an additional task is employed with the intention being to enhance attention on the part of the listener, in order to enhance recruitment of MNS regions. It is not clear, however, whether these types of procedures are more effective than not having a top-down system in place at all, and instead capitalizing on automaticity of the system.

Emotionality and the MNS
The role of the MNS in the processing of emotion is currently up for examination in the MNS literature. In addition, one way to manipulate top-down processing of the MNS is to manipulate attention towards emotional aspects of stimuli during processing. Many theoretical papers have been published since the appearance of mirror neurons in the literature, and many have speculated that the MNS could play a role in empathy

29

(Iacoboni & Dapretto, 2006; Molnar-Szakacs & Overy, 2006; McGarry & Russo, 2011). Some empirical studies have been conducted that support this claim (Carr et al., 2003; Ferri et al., 2013; Grosbras et al., 2006). However, these studies have generally used stimulus-driven paradigms that contain stimuli that vary systematically between conditions not only on emotionality, but often also on cognitive or psychophysical differences that are characteristically part of an emotion, such as saliency or loudness (Carr et al., 2003; Ferri et al., 2013; Grosbras et al., 2006). Thus, a question remaining is whether the MNS responds in a special way to stimuli when directed towards emotional qualities of that stimuli in a top-down fashion, as opposed to attending to other nonemotional aspects of those stimuli.

Current Studies
The three studies included in this dissertation explore the roles of bottom-up and topdown manipulations on MNS responsivity towards human action. Study 1 seeks to examine bottom-up activation of the MNS. Unimodal and multimodal perception of action were compared using EEG measurement of the mu wave. Study 1 intentionally avoided asking participants to make judgments about stimuli, in order to minimize any task-driven effects on MNS responsivity. This study can be considered a replication of Kaplan and Iacoboni's (2007) study, using EEG instead of fMRI methodologies. This new method allows for an examination of the timing of the MNS response to unimodal and multimodal stimuli, while replicating findings of bottom-up effects on MNS responsivity in our own laboratory. This study also provides a validation of the premise that such effects can be measured using EEG instead of fMRI methods, as well as for the validity of ICA as a source localization tool in EEG.

30

Study 2 examined the effects of top-down judgments on MNS activation towards identical stimuli. In this study, sung intervals were presented across all conditions, so that only the judgment type changed across conditions, allowing for a measure of the influence of top-down processes on MNS activation. Specifically, participants were asked to make an emotional judgment, a structural judgment, or a non-movement-oriented judgment about pairs of melodic intervals, allowing for an examination of task condition on MNS engagement. This study also employed EEG methods to measure desynchronization of the mu rhythm as an index of MNS activity. Study 3 sought to combine top-down and bottom-up conditions in a single study. This study addressed limitations in extent of MNS activation in studies 1 and 2 by using more obviously action-oriented stimuli that are known to activate the classical MNS (hand actions) and by using fMRI to better localize areas of activation that may be involved in top-down and bottom-up manipulation of the MNS.

31

! ! Chapter II. Study 1: Audio-visual facilitation of the mu rhythm
Published in 2012, in Experimental Brain Research
McGarry, L. M., Russo, F. A., Schalles, M. D., & Pineda, J. A. (2012). Audio-visual facilitation of the mu rhythm. Experimental brain research, 218(4), 527-538.

33

Abstract
Previous studies demonstrate that perception of action presented audio-visually facilitates greater mirror neuron system (MNS) activity in humans (Kaplan and Iacoboni 2007) and non-human primates (Keysers et al. 2003) than perception of action presented unimodally. In the current study we examined whether audio-visual facilitation of the MNS can be indexed using EEG measurement of the mu rhythm. The mu rhythm is an EEG oscillation with peaks at 10 and 20 Hz that is suppressed during the execution and perception of action, and is speculated to reflect activity in the premotor and inferior parietal cortices as a result of MNS activation (Pineda 2008). Participants observed experimental stimuli unimodally (visual-alone, or audio-alone) or bimodally during randomized presentations of two hands ripping a sheet of paper, and a control video depicting a box moving up and down. Audio-visual perception of action stimuli led to greater event-related desynchrony (ERD) of the 8-13 Hz mu rhythm compared to unimodal perception of the same stimuli over the C3 electrode, as well as in a left central cluster when data were examined in source space. These results are consistent with Kaplan and Iacoboni's (2007) findings that indicate audio-visual facilitation of the MNS; our left central cluster was localized approximately 13.89 mm away from the ventral premotor cluster identified in their fMRI study, suggesting that these clusters originate from similar sources. Consistency of results in electrode space and component space support the use of ICA as a valid source localization tool. Keywords: mu rhythm, mirror neuron system, multimodal facilitation, independent components analysis

34

Introduction
The auditory components of action play an important role in action perception and understanding. For instance, the sound of approaching footsteps precedes and signals the arrival of an individual, and hearing articulatory gestures is central to perceiving speech. Recent research demonstrates that auditory perception of human actions elicits activation of the human mirror neuron system (MNS) similar to that during the visual perception of action (Gazzola, Aziz-Zadeh and Keysers 2006). In addition, audio-visual perception of action elicits enhanced MNS activation as compared to unimodal perception as measured by fMRI (Kaplan and Iacoboni 2007). In the current study, we sought to replicate findings that MNS activation is facilitated during the audio-visual perception of action, using measurement of the mu rhythm, an EEG oscillation with peaks at 10 and 20 Hz that is desynchronized during the execution and perception of action. Mirror neurons were first recorded in area F5 in the monkey premotor cortex (Di Pellegrino, Fadiga, Fogassi, Gallese and Rizzolatti 1996; Gallese, Fadiga, Fogassi and Rizzolatti 1996; Rizzolatti, Fadiga, Gallese and Fogassi 1996). Single cell recordings in monkeys suggest that action execution stimulates firing of the same neurons as perception of another individual (monkey or human) performing the same action, or a different action with the same end goal (di Pellegrino et al. 1992; Gallese et al. 1996; Rizzolatti et al. 1996a). Evidence for mirror neurons in humans has been demonstrated using EEG, TMS, neuroimaging, and single-cell recordings in a combination of parietal, prefrontal, and premotor areas (Cohen-Seat, Gastaut, Faure and Heuyer 1954; Pineda 2005; 2008; Fadiga, Fogassi, Pavesi, and Rizzolatti, 1995; Maeda, Kleiner-Fisman, and Pascual-Leone, 2002; Buccino et al. 2001; Grafton et al. 1996; Iacoboni et al. 1999, 2001; Rizzolatti et al. 1996b;

35

Mukamel et al. 2010; for a review see Rizzolatti and Craighero 2004), demonstrating that MNS areas are activated during execution and perception of goal directed action (Fadiga, Craighero, and Olivier 2005; Maeda et al. 2002), as well as towards different actions with similar intentions (Rizzolatti, Fogassi and Gallese 2001). More recently, Mukamel et al. (2010) used single-cell recordings in medial frontal and temporal cortices in humans, and demonstrated single cells that fired towards the execution and perception of action (Mukamel et al., 2010). In terms of anatomical connectivity, research in the primate brain demonstrates connections between discrete superior temporal regions and two distinct regions of the frontal lobes, namely the caudal dorsolateral prefrontal cortex, and rostral and ventral prefrontal areas (Romanski et al. 1999). These frontal areas overlap with area F5, which is thought to be a homolog of Broca's area (Romanski et al. 1999). In humans, similar pathways connecting posterior superior temporal gyrus and dorsolateral prefrontal cortex are reported by Frey, Campball, Pike and Petrides (2008) using Diffusion Fiber Tractography. The perception of audio-visual (AV) action elicits supra-additive responses in areas comprising the MNS in humans, such as the STS and ventral premotor cortex (Kaplan and Iacoboni 2007; Keysers et al. 2003). In Kaplan and Iacoboni's (2007) fMRI study, videos were presented to participants depicting two hands ripping a sheet of paper, or a control video of a box moving up and down, accompanied by beeping sounds. For the experimental stimuli only, AV facilitation of action observation was observed in the ventral premotor cortex, on the border of areas 44, 6, 3a, and 3b, at Talaraich coordinates

36

(-64, 0, 18). In the current study, we sought to replicate these findings and extend them to the realm of EEG mu suppression measurement. There is a rich literature on AV integration and its contributions to perceptual sensitivity that may be relevant to questions about AV facilitation in the MNS. This literature demonstrates that multi-modal processing results in enhanced perception of unimodal information (Driver 1996), earlier neural responses to multi-modal information (Welch, DutionHurt and Warren 1986; Giard and Perronnet 1999), and in some cases supra-additive neural responding to multi-modal versus unimodal information (Stein and Meredith 1993). Stein and Meredith (1993) demonstrated in single cell recordings of the superior colliculus that spatially and temporally congruent multimodal cues enhance responding in a multiplicative way, as compared to responses to either mode alone. When these responses are spatially separated, the response is suppressed. This suggests that multimodal relationships between stimuli affect stimulus processing at the unimodal level. It appears that this information then feeds back to influence how the unimodal stimuli are perceived. Although there are differences in functionality and connectivity between neurons in the superior colliculus and those in the MNS, it is possible that they respond in a similar manner to multimodal cues. In the current study, we expected that AV action stimuli might elicit earlier and more powerful responses compared to audio or visual unimodal action stimuli in the MNS, perhaps even in an additive or multiplicative way. Activity in the MNS is hypothesized to be reflected in the mu rhythm, which is measured through scalp electroencephalography (EEG), and becomes desynchronized during the perception and execution of action (Cohen-Seat et al., 1954; Gastaut and Bert

37

1954, Cochin, Barthelemy, Roux and Martineau 1999; Pineda 2005; 2008). Mu suppression is thought to reflect mirror neuron activity, specifically downstream regulation of the motor cortex via parietal and prefrontal mirroring areas (Pineda 2005; 2008). Measured over central electrodes, with frequency peaks at around 10 Hz and 20 Hz, these rhythms are suppressed in healthy individuals during visual or auditory observation of action, as well as performance of action (Pineda 2008). The alpha component (8-13 Hz) of mu is thought to reflect MNS activation in the postcentral somatosensory cortex, with the beta band (15-25 Hz) reflecting slightly anterior motor activity (Pfurtscheller, Neuper, Andrew and Edliner 1997; Pfurtscheller, Neuper and Krausz 2000; Hari et al. 1998; Pineda 2008). Increased BOLD signal in frontal and parietal areas has been correlated with suppression of the alpha EEG response (Laufs et al. 2003a; Laufs et al. 2003b), as well as marginally with the beta response (Laufs et al. 2003b). These negative correlations occurred in areas such as the frontal and parietal cortex, areas containing mirror neurons (Rizzolatti and Craighero 2004). Because these frequency bands overlap with mu, this supports the premise that the mu response is linked to activity in these regions. Consistent with this, Keuken et al. (2011) recently showed that using transcranial magnetic stimulation to disrupt activity in the inferior frontal gyrus directly affects the modulation of mu rhythms over sensorimotor cortex. These findings are consistent with the hypothesis that mu rhythm suppression during action observation reflects downstream modulation of activity in motor neurons in the primary motor cortex by mirror neurons in the inferior frontal gyrus that are involved in action planning (Pineda, 2005).

38

Similar to other measurements of activation in the MNS (Rizzolatti and Craighero 2004), the mu rhythm is modulated during perception of human movement but not object movement (Cochin, Barthelemy, Lejeune, Roux and Martineau 1998). Cochin et al. (1998) found that viewing human movements led to suppression of the mu rhythm in alpha and beta frequency ranges, whereas viewing object movements did not. Also like other measurements of the MNS (Rizzolatti and Craighero, 2004), the mu rhythm is more sensitive to movements such as a hand operating on an object, rather than non-objectoriented human movement (Muthukumaraswamy, Johnson and McNair 2004). In the current study, we examined whether AV action compared to unimodal action would facilitate event-related desynchronization (ERD; Pfurtscheller and Aranibar 1979; Pfurtscheller 1992; Neuper, Wortz and Pfurtscheller 2006) of the mu rhythm. ERD corresponds to mu suppression and reflects a decreasing post-stimulus change in power in the mu band. Subjects viewed action and non-action audio-only (A), visualonly (V), and audio-visual (AV) congruent stimuli while their EEG mu rhythms were measured. We expected that mu suppression would be greatest during the perception of AV action stimuli. These results would suggest that the MNS responds selectively to action-related AV stimuli, and that this facilitation can be captured through measurement of mu ERD. These findings will allow us to conduct future studies concerning AV facilitation of the MNS in healthy adults as well as patient groups using EEG technology, which is a non-invasive, temporally sensitive, and cost-effective measurement tool.

39

Methods
Participants
Thirty-four undergraduate students (22 females) from the University of California, San Diego with a mean age of 21.0 (SD = +3.2) and an average of 14.6 years of education (SD = +2.0) participated in the study for course credit. Exclusion criteria included the presence of psychiatric or neurological disorders. One participant was excluded from analyses for psychiatric reasons. The study was reviewed and approved by the University of California, San Diego Human Research Protections Program.

Experimental Design
All participants were involved in the same one-hour experiment. The independent variable was the stimulus type: Action A, V, or AV, control A, V, or AV, or self-performed action. The dependent variable of interest was the amount of mu suppression during perception of multimodal and unimodal action stimuli. Mu suppression during action perception conditions was also compared with control conditions, as well as with mu suppression during action execution. Participants were seated in a comfortable chair 60 cm from the computer screen with speakers located on either side of the screen. Each video was repeated 20 times in random order. Perception trials were preceded by a 5-second black fixation cross on a white screen, while action execution trials were preceded by a 5-second green cross to signal that they should pick up a sheet of paper, followed by an 8-second red cross during the action execution trial to signal that they should begin ripping.

40

Apparatus
EEG Recording. EEG was recorded using a Neuroscan Synamps system and an electro-cap. Nineteen electrodes were placed, following the International 10-20 electrode placement method, at: FP1, FP2, F3, Fz, F4, F7, F8, C3, Cz, C4, P3, Pz, P4, T3, T4, T5, T6, O1, O2. One electrooculogram (EOG) electrode was placed below the eye, and reference electrodes were placed behind each ear (mastoids). Impedance was set to below 10 k. All electrodes were amplified by a factor of 1000x and sampled at 500 Hz. Online bandpass filtering was set at 0.3­100 Hz (half-amplitude, 3 dB/octave roll-off). The experiment was conducted in a sound-attenuated chamber.

Stimuli
Stimuli consisting of short-duration videos were presented using NeuroBehavioral Presentation software run on a separate PC computer located outside the chamber that was connected to a monitor within the chamber. Sound was presented via speakers located on either side of the computer screen, at a consistent volume. Participants were asked to observe a series of 8-second videos in which 2 hands were depicted ripping a sheet of paper 6 times, with a 5-second ITI. The stimuli were identical to those used in Kaplan and Iacoboni's (2007) fMRI study and provided courtesy of the authors. Videos were either presented audio-visually (AV action condition), visual only (V action condition), or audio only (A action condition). Control stimuli consisted of a blinking square accompanied by a pure tone (261.7 Hz) that ranged in duration between 200 and 500 msec, with onset of beeps aligned with the onset of rips. Control stimuli were presented in audio-visual (AV control), visual (V control), and audio (A control) conditions. An additional condition was included in which participants were asked to 41

pick up a sheet of paper resting on the arm of the chair they were sitting in during the intertrial interval (ITI), and then rip the sheet of paper 6 times during the following 8 second trial.

Data Analysis
Using the EEGLab Matlab toolbox (DeLorme and Makeig 2004), channels were located using Montreal Neurological Institute (MNI) coordinates, and data were referenced to the average. Data were bandpass filtered (3-30 Hz) and organized into trials. Data were initially examined in electrode space. Additionally, in order to isolate mu-specific components, a source localization procedure was performed on the data. It was found that multiple brain sources were contributing to activity at the central electrodes. Because of this, data were also examined using independent components analysis (ICA; DeLorme and Makeig 2004), isolating brain components instead of electrodes. Component decomposition was performed on the entire epoched dataset using a second order blind identification (SOBI) algorithm, with the number of correlation matrices set to 50. SOBI is a blind-source separation (BSS) technique that exploits the second-order statistics of the measurements to compute an estimate of the mixing matrix (full details can be found in Belouchrani, Abed-Meraim and Cardoso 1997; also see Moore, Gorodnitsky and Pineda 2012). Dipoles were then fitted to each component, and trials containing amplitudes greater than 75 or less than -75 mV were rejected. An average of 17% of trials were deleted overall. For the action condition, an average of 36% of trials had to be rejected due to excessive artifact, probably due to movement artifact from the gross movements involved in the procedure, compared to only 14.5% in the remaining conditions. The remaining

42

trials elicited event related spectral perturbations (ERSPs) that appeared largely synchronized, which is opposite to the trend that is normally observed in the mu band while participants are engaged in action. However, it is important to note that procedures employed in previous studies have involved much smaller movements (e.g., finger tapping) than was required in the current study. When the data were examined in electrode space, looking at differences between conditions over central electrodes C3 and C4, patterns appeared comparable to those observed in component space. Components were manually inspected for each participant and any components contributing eye movement artifact or other muscular artifact were deleted. Components were clustered using the k-means procedure and outliers were excluded based on three standard deviations. Eight clusters were formed. For each cluster, ERSPs were compared across the alpha range of mu (8-13 Hz) as well as the beta mu component (17-23 Hz), for the time range containing the first two rips (0-2500 ms). This narrower time range, and narrower beta frequency range (as compared to 15-25 Hz) was chosen to enhance power in the face of multiple comparisons across each time point and frequency point in the ERSP comparison. It was also expected that earlier rips would elicit greater activity in mirroring areas before habituation occurred. ERSPs depict deviations in spectral power relative to baseline, specifically ERDs (post-stimulus decreasing change in power) and ERSs (post-stimulus increasing change in power) and allow for comparison at multiple time points and frequency points within a time/frequency band (described in Moore et al. 2011). The purpose of examining data at multiple points across a time and frequency window is to capture differences between conditions that may occur at one point in time or frequency

43

but not another. If time and frequency are collapsed, this may cancel out some important differences between conditions, due to oscillating synchronization and desynchronization of activity. All ERSP results were corrected using the Benjamini and Hochberg false discovery rate correction (Benjamini and Hochberg, 2005) with an alpha value of .05. For each cluster a one-way analysis of variance (ANOVA) was conducted with condition (A action, V action, AV action, A control, V control, AV control, action execution) as the within-subjects factor. Following this, pairwise comparisons were conducted on each action trial versus its respective control (i.e. A action versus A control, V action versus V control, and so on), as well as for unimodal versus multimodal action conditions, and unimodal versus multimodal control conditions.

Results
Corresponding significant multimodal facilitation was found towards perception of audio-visual action, but not audio-visual control stimuli, in the left central C3 electrode, as well as in a left central ICA cluster.

Electrode space analyses
Audio-visual facilitation of action was examined at each electrode site, for the 813 Hz and 17-23 Hz ranges. Event-related spectral perturbations (ERSPs) were compared. The one-way ANOVA was significant for a main effect of condition as depicted in Figure 11, across both frequency ranges. Pairwise comparisons demonstrated that there was audio-visual facilitation of mu ERD versus audio-alone as well as visualalone at C3 and C4 electrode sites for 8-13 Hz, but not 17-23 Hz (see Figure 2). Both A !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
One addition has been made to this version of the manuscript that was not included in the published version. In this version, all figures include the scale of power on the z-axis for each ERSP graph, in dB.
1

44

and V action stimuli generated greater ERD than control at the C3 location only, while AV action stimuli generated greater ERD than control at C3 and C4 locations, indicating that action facilitation of mu occurred at the C4 site when audio and visual presentation of action occurred together.

Figure 1. ANOVAs for all conditions, 8 ­ 25 Hz, for central electrodes (1a: C3; 1b: C4). On the ERSP graphs, warmer colours indicate synchronization and cooler colours indicate desynchronization. The y-axis indicates frequency and the x-axis indicates time. Graph on far right: Areas that are significant for a main effect of condition are maroon in colour, with a Benjimini Hochberg FDR correction for multiple comparisons at an alpha level of .05. V: visual condition, A: auditory condition, AV: audio-visual action condition, CV: control visual condition, CA: control auditory condition, CAV: control audiovisual condition, ACT: action condition. Each ANOVA was significant for a main effect of condition.

! ! !

45

Figure 2. Pairwise comparisons, 8-13 Hz, for central electrodes (2a: C3; 2b: C4). Unimodal and bimodal conditions were compared to their respective control conditions, as well as to each other. On the ERSP graphs, warmer colours indicate synchronization and cooler colours indicate desynchronization. The y-axis indicates frequency and the x-axis indicates time. Graph to the right of each pair of ERSP graphs: Areas that are significant for a main effect of condition are maroon in colour, with a Benjimini Hochberg FDR correction for multiple comparisons at an alpha level of .05. V: visual condition, A: auditory condition, AV: audio-visual action condition, CV: control visual condition, CA: control auditory condition, CAV: control audiovisual condition. For the C3 electrode, there was audiovisual facilitation in the 8-13 Hz range, and all action conditions generated greater ERD than control conditions. For C4, there was audio-visual facilitation of action, and only the audio-visual action condition generated greater mu ERD than control conditions.

Independent components analyses
Clusters. Eight clusters were formed. Clusters originating from central sources with frequency peaks in the mu range were selected for further examination. In addition, one occipital cluster generated audio-visual facilitation of suppression in the 8-13 Hz range, so this cluster was examined further to ensure that it was representative of alpha and not mu. For each of these clusters, the one-way ANOVA was significant for a main effect of condition at time and frequency points spanning 8-25 Hz (see Figure 3 for ANOVA results for the two centrally generated clusters). A left central-parietal cluster generated multimodal action facilitation of mu ERD in the 8-13 Hz range, and a midcentral cluster generated multimodal facilitation of mu ERD versus audio but not compared to visual action stimuli, in the 8-13 Hz and 17-23 Hz ranges. A right central

46

cluster that was also formed in the clustering analysis yielded no significant differences between ERSPs.

Figure 3. ANOVAs for each condition for the left central-parietal cluster (3a), and mid-central cluster (3b), 8-25 Hz. On the ERSP graphs, warmer colours indicate synchronization and cooler colours indicate desynchronization. V: visual condition, A: auditory condition, AV: audio-visual action condition, CV: control visual condition, CA: control auditory condition, CAV: control audiovisual condition, ACT: action condition. The y-axis indicates frequency and the x-axis indicates time. Graph on far right: Areas that are significant for a main effect of condition are maroon in colour, with a Benjimini Hochberg FDR correction for multiple comparisons at an alpha level of .05. All ANOVAs were significant for a main effect of condition.

! Left central-parietal cluster. The left central-parietal cluster with mean Talairach coordinates at (-57, -12, 18) (see Figure 4) generated greater mu ERD in the 813 Hz range but not the 17-23 Hz range during perception of AV action versus V action or A action, demonstrating bimodal facilitation of the mu rhythm. In addition, this cluster generated greater mu ERD for V action as compared to V control, A action versus A control, and AV action versus AV control stimuli in the 8-13 Hz range, as well as V action compared to V control and AV action versus AV control in the 17-23 Hz range. Please see Figure 4 for visual depictions of these comparisons and their significance. A pairwise

47

comparison of ERSPs for multimodal versus unimodal control conditions yielded significant differences between A and AV, but not between V and AV conditions.

Figure 4a. Pairwise comparisons between action and control conditions; left central-parietal cluster. ERSP comparisons are depicted for each action condition versus its respective control condition. On the ERSP graphs, warmer colours indicate synchronization and cooler colours indicate desynchronization. The y-axis indicates frequency and the x-axis indicates time. Graph to the right of each pair of ERSP graphs: Areas that are significant for a main effect of condition are maroon in colour, with a Benjimini Hochberg FDR correction for multiple comparisons at an alpha level of .05. V: visual condition, A: auditory condition, AV: audio-visual action condition, CV: control visual condition, CA: control auditory condition, CAV: control audiovisual condition. Each of the action conditions elicited greater desynchronization along the 813 Hz and 17-23 Hz mu range than their respective control conditions, except for audio which generated greater desynchronization for action versus control at the 8-13 Hz range but not the 17-23 Hz range. Figure 4b. Pairwise comparisons between multimodal and unimodal conditions; left central-parietal cluster. ERSP comparisons are depicted for the AV action condition versus A and V action conditions. Audio-visual facilitation was found versus both audio and visual action conditions in the 8-13 Hz range, and versus the audio condition in the 17-23 Hz range. Figure 4c. Mean dipole location for left central-pareital cluster. Talairach coordinates: -57, -12, 18.

48

Middle-central cluster. A middle central cluster with mean Talairach coordinates at (-9, -2, 13) (see Figure 5) generated greater mu ERD in the 8-13 Hz range and 17-23 Hz range during the perception of AV action as compared to A but not V action stimuli. In addition, greater mu ERD was found in the 8-13 Hz and 17-23 Hz ranges for V action as compared to V control, A action compared to A control, and AV action compared to AV control stimuli. See Figure 5 for visual depictions of these comparisons and their significance. There was no multi-modal facilitation of mu ERD for control stimuli.

49

Figure 5a. Pairwise comparisons between action and control conditions; mid-central cluster. ERSP comparisons are depicted for each action condition versus its respective control condition. On the ERSP graphs, warmer colours indicate synchronization and cooler colours indicate desynchronization. The y-axis indicates frequency and the x-axis indicates time. Graph to the right of each pair of ERSP graphs: Areas that are significant for a main effect of condition are maroon in colour, with a Benjimini Hochberg FDR correction for multiple comparisons at an alpha level of .05. V: visual condition, A: auditory condition, AV: audio-visual action condition, CV: control visual condition, CA: control auditory condition, CAV: control audiovisual condition. Each of the action conditions elicited greater desynchronization along the 813 Hz mu range than their respective control conditions. Figure 5b. Pairwise comparisons between multimodal and unimodal conditions; mid-central cluster. ERSP comparisons are depicted for the AV action condition versus the A condition at 8-13 Hz and 17-23 Hz. The AV action condition elicited greater desynchronization along the 8-13 Hz and 17-23 Hz mu ranges than the A condition. There were no significant differences between AV and V conditions. Figure 5c. Mean dipole location for mid-central cluster. Talairach coordinates: -9, -2, 13.

Left Occipital cluster. A left occipital cluster with mean Talairach coordinates at (-44, -62, 8) was also examined. AV action stimuli elicited greater suppression in the 8-

50

13 Hz alpha range than A or V action stimuli, but AV control stimuli did not elicit multimodal facilitation of alpha suppression compared to control stimuli. ERSPs generated from this cluster showed no differentiation between action and control conditions. There were also no differences across these conditions in a right occipital cluster that was also formed in the clustering analysis.

Discussion
Our results indicate that mu ERD, an index of MNS activity, is enhanced by the multi-modal presentation of action associated with the left central-parietal area. This cluster generated more desynchronized activity in the 8-13 Hz component of the mu band during perception of audio-visual action compared to either visual or audio action, and each action condition generated greater ERD than control conditions. Similar results were obtained under examination of the C3 electrode in electrode space, an electrode that is also located in the left-central area, where mu suppression towards action stimuli is commonly found. The mean of this cluster is strikingly close to the ventral premotor area of the MNS, identified in Kaplan and Iacoboni's (2007) fMRI study as generating facilitated activation during the perception of multi-modal action. The ventral premotor cortex identified in Kaplan and Iacoboni's (2007) study is approximately 13.89 mm away from the mean of this cluster, suggesting that these clusters could originate from the same source. The control audio-visual condition also generated more ERD than the audio but not the visual condition in this cluster (and not in electrode space for C3). This may indicate that the audio-visual facilitation that occurs in the ventral premotor cortex is due

51

to an additive combination of audio-visual facilitation and action perception, rather than audio-visual facilitation being necessarily action related. A middle-central cluster generated greater mu ERD in the alpha and beta mu bands during perception of visual, audio, and audio-visual action relative to control conditions, and also generated greater ERD of audio-visual versus audio action but not visual action stimuli. Rather than multi-modal facilitation, it is likely that the greater contribution of visual action to mu ERD in this cluster was responsible for the enhanced ERD in both visual and audio-visual conditions. There was no enhanced ERD of mu for control AV versus A or V stimuli, supporting the idea that enhanced ERD occurred primarily for action stimuli in this cluster. An occipital cluster generated greater alpha ERD during perception of audiovisual versus visual but not auditory stimuli. However, this cluster did not differentiate between action and non-action conditions. This suggests that this cluster was generating occipital alpha activity. Changes in the occipital alpha band towards multi-modal stimuli are likely to be related to changes in visual attention elicited by AV stimulation (Sauseng et al. 2005), whereas mu is specifically desynchronized during the perception and execution of biological motion. Occipital alpha did not show similar facilitation of AV versus A or V control stimuli. Perhaps this is because the control stimuli were not as visually complex as the experimental stimuli. Interestingly, while audio action stimuli generated greater ERD of the mu band than audio control stimuli in both the left central and middle central clusters, visual action stimuli appeared to generate greater ERD than audio in both instances, though this was not significant at any point in the time window after the FDR correction for multiple

52

comparisons. This could mean that while audio action information contributes to mu desynchronization, desynchronization of the mu rhythm relies more on visual information than audio information. There may also be stimuli such as speech or singing where the auditory signal is more informative about movement intention than the visual signal. However, there are other possible reasons why visual stimuli elicited greater mu suppression than audio in the current study. Visual stimuli were present onscreen for the entirety of each 8-second video. However, audio information was only available while the ripping occurred, and went away while the hands in the video prepared to create an additional rip. The pervasiveness of the visual stimuli could explain the enhanced ERD. In addition, the presence of visual stimuli always preceded sound during audio-visual trials. Follow-up studies will equate the timing of visual and audio stimulus presentation, and experiment with other types of stimuli, in order to account for these potential factors. Future studies should also explore whether the MNS relies more on auditory information when visual stimuli are degraded. The inverse effectiveness rule of multisensory integration suggests that the likelihood of multisensory integration increases as the ambiguity of its unimodal constituents increases (Meredith and Stein 1986). There is considerable behavioral evidence in support of this rule (e.g., Alais and Burr 2004; Walden et al. 1977), so it is reasonable to expect that when information from the visual modality is degraded, participants may rely more heavily on audio information, and there might be more audio-visual integration observed. Previous research has demonstrated that multi-modal stimulus attributes are perceived earlier and influence perception of unimodal stimulus attributes. In Driver's (1996) study, it was shown that multi-modal cues are processed before unimodal cues are

53

fully processed. In this case, it led viewers to perceive the spatial location multi-modally before they would have processed the spatial location of the auditory source, and this influenced their perception of the auditory spatial location. The colour of a food item has been shown to affect the gustatory experience of flavor intensity (DuBose Cardello and Maller 1980), the magnitude of a singer's head movement has been shown to influence the auditory experience of sung interval size (Thompson, Russo and Livingstone 2010), and the sound of parchment on the skin influences the tactile experience of texture (Jousmäki and Hari 1998). It has been suggested that the time course of cross modal perception occurs earlier than that of unimodal perception, serving to influence processing of unimodal stimulus properties (Welch et al. 1986). This is also observed for the experience of orthogonal multi-modal cues. Welch et al. (1986) demonstrated that auditory beeps influenced perception of the rate of unrelated visual flickers. Giard and Perronnet (1999) found an early event-related potential (ERP) signal whose pattern of responding suggests that an auditory cue, even if it is unrelated to the visual cue, can affect visual processing at an early stage. In terms of processing in the MNS, we might expect that AV processing will also occur earlier than A or V alone. Future studies should examine this by equating onset time of audio and visual stimuli to be able to compare latency of mu suppression in unimodal and bimodal conditions. Our findings complement recent research, which has also suggested the presence of AV neurons in the MNS that generate supra-additive responding during perception of AV congruent stimuli, as opposed to A, V, or AV-incongruent stimuli (Barraclough et al. 2005; Keysers et al. 2003; Kaplan and Iacoboni 2007). This facilitation occurs for action-related stimuli only (Barraclough et al. 2005; Kaplan and Iacoboni 2007). AV facilitation has

54

previously been measured using single-cell recordings in monkeys (Barraclough et al. 2005, Keysers et al. 2003; Kohler et al. 2002), as well as fMRI studies in humans (Kaplan and Iacoboni 2007). AV facilitation occurs only when audio-visual stimuli represent spatially and temporally congruent, goal-directed action. This implies that mirror neurons not only respond selectively to sets of actions serving a single intention, but that they serve to single out matching goal-directed actions from multiple modalities (Keysers et al. 2003). Research suggests that AV integration of action and non-action stimuli occurs first in the STS (Barraclough et al. 2005; Calvert, Kammel and Brammer 2000; Keysers et al. 2003; Skipper, Nussbaum and Small 2005), and the current study supports findings that AV action stimuli are further processed in the left ventral premotor cortex (Keysers et al. 2003; Kaplan and Iacoboni 2007). Keysers et al. (2003) used single-cell recordings in monkeys to measure MNS activity during the perception of AV action. They found AV mirror neurons in the ventral premotor cortex of the monkey. These neurons fire whether action is performed, heard, or seen. Of 33 single cells that they measured in this area, 22 showed both visual and auditory selectivity, and 8 of these 22 neurons demonstrated additive or supra-additive responding to AV action, as compared to A and V action alone. This evidence supports findings that that AV integration continues to occur within the mirror neuron system as well as in the STS (Kaplan and Iacoboni 2007). Future studies should examine whether incongruent audio-visual action stimuli generate a comparable degree of multi-modal facilitation as compared to congruent action stimuli. Keysers et al. (2003) found that individual audio-visual neurons in monkeys discriminate in firing towards congruent versus incongruent action stimuli. It is

55

possible that measurement in humans using fMRI or EEG could be more difficult to detect differences in brain responsivity to congruent versus incongruent audio-visual actions, since these devices measure summed activity of individual neurons. However, it is possible that overall activity will be greater during congruent stimuli than incongruent, since it is likely that a greater overall number of neurons will desynchronize during congruent than incongruent audio-visual action perception.

Conclusions
In the current study we have identified a left central-parietal cluster, possibly associated with the ventral premotor cortex, that generates greater ERD in the 8-13 Hz frequency range during perception of audio-visual action as compared to audio action, visual action, and non-action stimuli. Similar results were found when data was examined over the C3 electrode in channel space, bolstering the results and also bolstering accuracy of the ICA method. These results are consistent with findings in fMRI literature in humans (Kaplan and Iacoboni 2007) and single-cell recordings in monkeys (Keysers et al. 2003) that indicate supra-additive responding of MNS activity the ventral premotor cortex during multi-modal action perception. In addition, these findings demonstrate that this multi-modal action facilitation can be detected using EEG, allowing for time-sensitive and cost-effective data collection. Future studies should examine multi-modal facilitation of mu oscillations in autistic populations, in order to learn whether multi-modal integration impairments (Le Bel, Pineda and Sharma 2009) are related to dysfunctional AV facilitation of the MNS, and to aid development of MNSrelated training and therapy for autism.

56

Chapter III. Study 2: The role of the extended MNS in emotional and nonemotional judgments of human song
Published in 2014, in Cognitive, Affective, and Behavioral Neuroscience
McGarry, L.M., Pineda, J.A., and Russo, F.A. (in press). The role of the extended MNS in emotional and nonemotional judgments of human song. Cognitive, Affective, and Behavioral Neuroscience.

57

Abstract
In the current study we examined the involvement of the extended mirror neuron system (MNS), i.e., areas that have a strong functional connection to the core system itself, during emotional and non-emotional judgments about human song. We presented participants with audio-visual recordings of sung melodic intervals (two-tone sequences) and manipulated emotion and pitch judgments while keeping stimuli identical. Mu eventrelated desynchronization (ERD) was measured as an index of MNS activity, and a source localization procedure was performed on the data to isolate brain sources contributing to this ERD. We found that emotional judgments of human song lead to greater amounts of ERD than pitch distance judgments (non-emotional), as well as control judgments related to the singer's hair, or pitch distance judgments about a synthetic tone sequence. Our findings support and expand recent research suggesting that the extended MNS is involved to a greater extent during emotional versus non-emotional perception of human action.

Keywords: Mirror neuron system, mu event-related desynchronization, emotion perception, action perception, superior temporal gyrus

58

Introduction
Emotion is rooted in motion (Casasanto and Dijkstra, 2010). When joyful, individuals may jump and exclaim, gesturing in a way that reveals their emotion. When sad, they may exhibit a stooped posture and lowering of the larynx, releasing pharyngeal constriction, leading to a sobbing vocal quality (Harris, 1998). When observing another person's emotions, it is common to remark that individuals are "moved" with compassion or pity. Such remarks are viewed by more than one theorist as more than just metaphor; rather, when we feel empathy we do so through a process of simulating movement, neurally, via specialized mirroring mechanisms (Rizzolatti & Craighero, 2004; MolnarSzakacs & Overy, 2006; McGarry & Russo, 2011), and peripherally, through facial mimicry (Dimberg, 2000; Riskind & Gotay, 1982; Chartrand & Bargh, 1999) in a kind of embodiment of another person. In the current study, we used a task-driven design in which participants were asked to make emotional or non-emotional judgments of identical stimuli (sung melodic intervals) in order to examine the neural underpinnings of top-down emotional attention during the perception of human movement. There are likely a number of different neural mechanisms of embodiment during the perception of human action, but a reasonable candidate is the mirror neuron system (MNS). This is a system of visuomotor neurons in the frontal and parietal regions of the human and nonhuman primate brain that appears responsible for the execution and perception of meaningful action (di Pellegrino, Fadiga, Fogassi, Gallese, & Rizzolatti, 1992; Gallese, Fadiga, Fogassi & Rizzolatti, 1996; Rizzolatti, Fadiga, Gallese, & Fogassi, 1996; for a review see Rizzolatti & Craighero, 2004) and is hypothesized to be linked to simulation of the emotional actions of others (Chartrand & Bargh, 1999; Carr et al., 2003;

59

McGarry & Russo, 2011; Aragón et al., 2013). Neurons in the MNS will fire during one's own execution of an action, as well as during perception of the same action by other agents (di Pellegrino et al., 1992). Additionally, these same neurons will fire during the execution and perception of different actions that convey similar intentions (Umilta et al., 2001). Over the past two decades, numerous studies have further characterized the MNS in humans through functional Magnetic Resonance Imaging (fMRI; Carr et al., 2003; Cross, Hamilton & Grafton, 2006; Iacoboni et al., 2005), electroencephalography (EEG; Pineda, 2005; 2008; McGarry, Russo, Schalles & Pineda, 2012), and magnetoencephalography (MEG; Hari et al., 1998) studies, as well as a single-cell recording in a clinical population (Mukamel, Ekstrom, Kaplan, Iacoboni, & Fried, 2010). Accumulating functional evidence suggests that while there are differences between primates, the MNS serves overlapping functions in humans with those that have been identified in monkeys (Mukamel et al., 2010; Keysers & Gazzola, 2009). The MNS does not function in isolation and receives inputs from a variety of regions and sends outputs to an equal variety of regions, including the sensorimotor cortex. Hence, the notion of an extended MNS is defined to include a much wider range of areas that are functionally connected with the classical MNS to instantiate embodiment, imitation, and empathy during perception of meaningful movement (Pineda, 2008). These extra-mirroring regions likely include the insula (Carr et al., 2003; Mukamel et al., 2010), as well as the medial and superior temporal gyrus (Pineda, 2008; Mukamel, 2010). From an information processing perspective, signals are assumed to enter the MNS at the superior temporal sulcus (STS) via visual and auditory sensory streams (Keysers et al., 2003; Barraclough et al., 2005). The STS has been implicated in the processing of

60

biological motion, face processing, and social attention (Redcay, 2008; Iidaka, 2012). It is less certain whether the STS itself possesses mirror neurons (although see Molenberghs et al., 2010; Iacoboni et al., 2001) but it does integrate auditory and visual information before it is sent on to the ventral premotor cortex (Keysers et al., 2003). There have been some findings implicating the STS as a mirroring region, as it shows enhanced activation during imitation of actions, in similar areas as those activated during the observation of actions (Iacoboni et al., 2001). Iacoboni et al. (2001) believe that the STS is involved in creating a visual sensory copy of an imitated action, through receipt of signals from parietal and frontal mirroring areas, rather than being a mirroring region in its own right. The STS has also been reported to show people-selectivity, with enhanced responsivity to faces and voices as opposed to other types of auditory and visual stimuli (Watson et al., 2014). That information is then relayed to mirror neuron areas, including the precentral gyrus (PCG), containing the ventral premotor cortex, the posterior part of the inferior frontal gyrus (IFG), specifically the operculum, and the inferior parietal lobule (IPL), particularly the supramarginal gyrus (Rizzolatti & Craighero, 2004; Buccino et al., 2001). In humans, EEG measurement of MNS activation involves indexing event-related desynchronization (ERD) to human action. This index is temporally sensitive and considered to be the result of fronto-parietal mirroring activity on the coordination of neurons in the sensorimotor cortex (Pineda 2005; 2008). Mu ERD is enhanced (i.e., greater desynchronization) during the visual and auditory observation or performance of biological movement in healthy individuals (Pineda, 2005). Moreover, during observation, mu ERD is only enhanced by movements performed by other humans and not by object movements (Cochin, Barthelemy, Lejeune, Roux, & Martineau, 1998).

61

Several studies support the premise that emotional systems stimulate classical mirroring systems to be activated more strongly during emotional versus non-emotional perception of action (Wicker et al., 2003; Enticott et al., 2008; Zaki, Weber, Bolger & Ochsner, 2009; Pineda & Hecht, 2009). This could be because emotional actions reliably tend to be more salient than neutral actions (Talmi, Luk, McGarry & Moscovitch, 2007), and therefore more interesting to simulate. It could also be because emotional actions are more evolutionarily important to simulate (McGarry & Russo, 2011). Other studies suggest that social mirroring is adaptive in order to meet the demands of a varied social environment (Aragón et al., 2013). Previous studies have used primarily visual stimuli in their examination of the role of the MNS in emotion perception (Carr et al., 2003; Wicker et al., 2003; Enticott et al., 2008; Zaki et al., 2009; Pineda & Hecht, 2009; Perry et al., 2010). However, it has also been suggested that the MNS is involved in the perception and production of human speech and song (Warren et al., 2006; Overy & Molnar-Szakacs, 2009). For instance, Lévêque and Schön (2013) demonstrated that mu-ERD was greater during perception of human vocal melodies as opposed to non-vocal melodies. Warren et al. (2006) have previously examined a similar question using human vocalizations, and found MNS enhancement towards emotional versus neutral vocalizations in the left IFG and presupplementary motor area (pSMA). However, this type of stimulus possesses some potential confounds characteristic of stimulus-driven studies. In particular, emotional vocalizations could contain systematic differences in low-level auditory features, such as loudness or pitch quality, that contribute to any observed neural differences between emotional and neutral vocalizations. While these low-level differences are considered

62

necessary components of emotional sounds (Scherer & Oshinsky, 1977), it becomes difficult to determine which difference may have lead to changes in reactions to the stimuli, or whether it is the essence of the emotion itself that contributes. The current study addressed this potential confound by equating stimuli across task conditions, and manipulating task conditions instead of stimulus conditions. Similarly, previous studies using visual stimuli that consider the MNS in the context of emotional judgments have typically done so using distinct stimuli for emotional versus non-emotional judgments. There are some unavoidable issues with this type of design. First, there tend to be systematic differences between emotional and neutral stimuli, such as speed of movement, movement contour, and saliency of movement that could contribute to any differences between conditions (Scherer & Oshinsky, 1977; Talmi, Luk, McGarry & Moscovitch, 2007), and this is arguably what makes certain stimuli emotional. It becomes very difficult in these cases to pinpoint the specific cause of differences between emotional stimuli, as these low-level differences may contribute to perceptual differences, but do not necessarily make up the complete essence of an emotion. One potential solution is to include a large number of emotional and neutral stimuli that vary on these low-level qualities. However, it is still likely that emotional stimuli will be louder, more variable in pitch, and so on. This has motivated our task-driven design, in which we used identical stimuli between conditions. In the current study, we examined the role of the MNS in the top-down perception of emotion in human movement. Specifically, we looked at the role of the MNS in emotional versus structural judgments of human song. We sought to clarify the level of activation as well as the source of activation during emotion-related judgments as

63

opposed to intention-related judgments that do not involve emotion (i.e., pitch distance). These judgment conditions were compared with each other, as well as with two control conditions, in order to control for the role of movement-related judgments (by asking participants to judge whether the singer's hair was up or down), or the presence of biological movement (by requesting pitch distance judgments of a moving oval accompanied by beeping sounds). In addition, we used a source localization procedure in order to determine which areas of the MNS contribute more activity to certain types of judgments than others. The stimuli chosen were melodic intervals (two-note sequences), namely the perfect fifth, tritone, major third, and minor third. These intervals vary both in emotionality (positive or negative; Thompson, Russo & Quinto, 2008; Schellenberg & Trehub, 1994) and structure (small or large interval size; Russo & Thompson, 2005; Russo, Sandstrom & Maksimowski, 2011), allowing for different judgment conditions to apply equally well to the same stimuli. We indexed MNS activity using mu ERD (Pfurtscheller & Lopes da Silva, 1999; Pineda 2005; 2008) during the perception of dynamic audio-visual presentations of human song. In order to enhance mu ERD power, all stimuli were presented audiovisually (McGarry, et al., 2012). Across all judgment conditions, our audio-visual stimuli consisted of recordings of the aforementioned melodic intervals. Keeping the stimuli consistent across the judgment conditions allowed us to attribute differences in the neural response to the task alone. We predicted greater mu ERD during emotion judgments as compared to pitch distance judgments, indicating that the MNS is involved to a greater extent during task-driven emotional vs. non-emotional perception. We also predicted that the hair judgment would elicit less activation than the movement-oriented emotion and

64

pitch distance judgments, due to a lack of movement-oriented attention, and that this would be followed by a lack of activation in the control task, due to lack of biological movement.

Methods
Participants
Twenty healthy adults (17 females, 3 males) with no psychiatric or neurological disorders were tested. This sample size was chosen following the work of Moore et al. (2011) who used a similar data analysis method (independent components analysis) to successfully find mu ERD during an emotion-related task. The average age (standard deviation) was 21.85 (3.63), and the average number of years of education was 14.75 (4.34). All but two participants were right-handed. Participants were fully consented and debriefed. Our research protocol was approved by the Research Ethics Board at Ryerson University.

Design
The study followed a within-subjects design. The within-subjects variable was the type of judgment made during test: emotional, structural, hair, or control. The dependent variable was the mu ERD occurring during the observation of sung melodic intervals.

Stimuli
Stimuli consisted of audio-visual recordings of sung melodic intervals spanning one of four pitch distances (see Figure 1). Each video was 3000 ms long. Examples of the four pitch distances were produced by each of two female actors. The four intervals represent a perfect crossing of size and emotional valence. The minor third and the tritone 65

are reliably rated as conveying negative emotion, while the major third and perfect fifth are reliably rated as conveying positive emotion (Thompson, et al., 2008; Schellenberg & Trehub, 1994). The perfect fifth and the tritone also have a greater pitch distance between notes than the major and minor third. Prior research has demonstrated that the relative size of these intervals can be scaled accurately regardless of musical training (Russo & Thompson, 2005; Russo, et al., 2011). Stimuli were identical for the emotion, pitch distance, and hair judgment control conditions. The non-biological control stimuli consistent of beeping sounds accompanied by a moving oval. Beeps were matched to sung stimuli for average pitch and consisted of two simple (pure) tones at the same average pitch as the experimental stimuli. The moving circle was controlled in order to ensure that the magnitude of movement was equivalent to that observed in the human song stimuli. This control was accomplished by making the circle equal in size to the head of the actor producing the intervals, starting movement at the same start point as the actor, and ending movement at the same endpoint as the actor. Stimuli were piloted prior to testing. The chosen stimuli were equivalent with regard to the difficulty of the emotional or non-emotional judgment (i.e., pilot testing confirmed that there was no difference in accuracy or reaction times between task conditions).

66

Figure 1

Figure 1. Stimuli and Procedure. Stimuli. Stimuli consisted of audio-visual recordings of sung melodic intervals spanning one of four pitch distances. The four intervals represent a perfect crossing of size and emotional valence. The minor third and the tritone are reliably rated as conveying negative emotion, while the major third and perfect fifth are reliably rated as conveying positive emotion. The perfect fifth and the tritone also have a greater pitch distance between notes than the major and minor third. Two actors were featured, one whose hair was tied up, and one whose hair was down at her shoulders. Procedure. In the structural judgment block, participants were asked to judge whether the sung interval is small or large (1 = small, 2 = large). In the emotion judgment block, participants were asked to judge whether the sung interval is positive or negative (1 = positive, 2 = negative). In the hair judgment block, participants were asked to indicate whether the singer's hair is up or down after each trial (1 = hair up, 2 = hair down). In the control block, participants were asked to judge the pitch distance of non-biological stimuli consisting of a moving circle and auditory beeps.

67

Apparatus
Data were collected using a 64-channel Biosemi ActiveTwo EEG system at a sample rate of 512 Hz. Electrodes were placed using the international 10-20 system layout. All electrodes were of interest, as a source localization procedure was planned on the data.

Procedure
Upon arrival at the lab, participants provided informed consent to participate. The EEG cap was attached. Participants were tested in a sound-attenuated chamber, and situated 60 cm from the computer screen. Speakers were located within the computer monitor and stimuli were presented at a consistent volume. Participants viewed 4 blocks of 12 videos depicting the 4 types of sung intervals, each video 3000 ms long, with each trial followed by the button-press judgment screen. Order was counterbalanced across participants and stimuli were blocked by judgment type. In the structural judgment block, participants were asked to judge whether the sung interval is small or large (1 = small, 2 = large). In the emotion judgment block, participants were asked to judge whether the sung interval is positive or negative (1 = positive, 2 = negative). In the hair judgment block, participants were asked to indicate whether the singer's hair is up or down after each trial (1 = hair up, 2 = hair down). This condition was added in order to determine whether simply attending to movement-related aspects in the other two conditions, versus non-movement related aspects of the stimuli in this condition, would lead to greater MNS activation. In the control block, participants were asked to judge the pitch distance of non-biological stimuli consisting of a moving circle and auditory beeps.

68

Data analysis
EEG data were analyzed using EEGlab, an open-source Matlab program (Delorme & Makieg, 2004). Electrode data were subjected to a source-localization procedure in order to isolate specific brain sources contributing to electrode activity at the scalp. Specifically, we wished to isolate mu sources. It was found that multiple sources were contributing to activity at central electrodes, where mu is typically measured. Therefore, data were examined in source space. Fronto-parietal and parieto-central sources were isolated for further analysis, because these are the areas of the brain considered to generate mu activity. Sources were examined for the amplitude of activity taking place in the 10 and 20 Hz ranges, which are characteristic of the mu rhythm: alpha (8-13 Hz) and beta (15-25 Hz). First, data for each participant were imported into EEGlab and referenced to the average level of activity from all electrodes. Channels were identified according to a BESA head model and 8 facial muscle reference channels were deleted. Data were selected to only include trials and to exclude inter-trial intervals (ITIs). Activity that occurred during a 1000 ms pre-stimulus baseline was subtracted from each 3000 ms trial. Next, data were filtered from 3-30 Hz. Following this step, trials with artifact greater than 150 mV on 33% of electrodes were excluded. An independent components analysis (ICA) was then run on the data to separate data into 64 brain sources. The Adjust plugin for EEGlab was used to automate the exclusion of ICA sources that represented eyeblink, muscle activity, or other types of general artifact, based on defined criteria (Mognon, Jovicich, Bruzzone & Buiatti, 2011). Following this, a course dipole fitting procedure was applied (see the EEGlab Wiki for more information on the course dipole fitting

69

procedure, and other data analysis steps mentioned here; DeLorme & Makeig, 2013), and data were saved separately according to condition. Data for each participant from each condition were then saved in an EEG "Study" file, and component clustering was performed. Component clustering took place using the K-means procedure and excluding outliers greater than 3 SD from the mean. Components included in a cluster were those with less than 40% residual variance. Ten clusters were formed.

Results
Of the 10 clusters that were formed, 3 were located in brain areas that are theoretically consistent with the MNS, in the right pre-central gyrus (considered part of the classical MNS; Rizzolatti & Craighero, 2004; Buccino et al., 2001) and left superior temporal gyrus and insula (each considered part of the extended MNS; Pineda, 2008). Next, we examined event-related spectral perturbations (ERSPs) for each cluster. ERSPs are three-dimensional displays of power across time on the x-axis, and frequency on the y-axis. Power is indicated by the presence of warmer colours, while desynchronization is indicated by cooler colours. We were interested in the power in the alpha and beta bands of mu that was desynchronized during perception of action. Two of these clusters showed ERSPs yielding the expected pattern (emotional judgment > non-emotional judgment, as signified by a greater amount of desynchronization, or `cooler' coloured activity, in the 813 Hz range across the time series of interest); however, this pattern only reached significance for the left-central cluster (detailed below). There were no significant differences between conditions for any other clusters.

70

Left-Central Cluster Analysis
The left-central cluster that yielded the expected pattern of activity across conditions had Talairach coordinates at (-55, -15, 5), in the left STG (left cerebrum, temporal lobe; see Figure 2). This is only 13.49 mm away from the ventral premotor cortex, an area of the MNS that we also found to exhibit greater activity towards audiovisual perception of emotion in a previous study (McGarry et al., 2012), suggesting that the source is related. We analysed ERSPs for each condition within this cluster, across alpha and beta bands of mu. An ANOVA comparing all four conditions (emotion, pitch distance, hair judgment, and control pitch distance) was significant in many places across the time/frequency window (see Figure 3a), in the alpha band only. A follow-up pairwise comparison of the emotion and control conditions was significant as well (see Figure 3b). However, a pairwise comparison of the emotion judgment and the nonemotion judgment was not significant. Figure 2.

Figure 2. In this figure, a manual tracing of the left STG (part of the extended MNS) is depicted in blue, while the significant mean dipole location is depicted as a red dot. This cluster exhibited patterns of activation consistent with our predictions.

71

Figure 3. a. Event-related spectral perturbations for the cluster originating from the left STG. In the 8-13 Hz alpha frequency band, the emotion judgment condition lead to greater mu ERD (desynchronization is depicted in cooler colours) compared to the non-emotion judgment condition, the hair, and the nonbiological control conditions. This pattern was significant. A series of F-tests were performed for several time and frequency combinations across the ERSP window, with an FDR correction for multiple comparisons, and areas that are maroon in colour are considered significant at the p < .05 level. b. Eventrelated spectral perturbations for the emotion condition versus the non-biological control condition, originating from the left STG cluster. A series of pairwise comparisons were performed for various time and frequency combinations across the ERSP window, and are depicted in the box on the right with an FDR correction. Significant comparisons are maroon in colour. The emotion condition was found to lead to significantly greater mu ERD than the non-biological control condition. This was the only significant pairwise comparison for this cluster.

72

Due to the multiple comparisons that need to be made when comparing multiple time/frequency points across a window of interest, power is reduced by statistical corrections. As an alternative, we used the Matlab Statistics Toolbox to complete a secondary analysis based on averaged power across time and frequency windows. This secondary approach revealed that the pitch distance judgment activity was not different from hair control activity, but that both pitch distance and hair control conditions had marginally greater mu ERD than the non-biological control condition (see Figure 4), F (3,236) = 2.47, p = .06. Although this ANOVA was only marginally significant, followup t-tests were conducted between conditions of interest to see what differences might have been driving this trend. It was found that the emotion condition generated significantly more mu ERD than the control condition; t(118) = 2.2759, p = .025. No other differences reached significance when examined across the entire time/frequency window (see Table 1).

73

Figure 4. Patterns of mu ERD originating from the left STG cluster, collapsed across the 8-13 Hz frequency window, and collapsed across the 500-1500 ms time window of interest. This simpler graph illustrates the greater level of desynchronization towards the emotion task condition as opposed to other conditions. Table 1.

Table 1. All statistical results, collapsed across time and frequency, for each cluster of interest.

Right Precentral Gyrus Cluster
A right-sided cluster with Talairach coordinates at (35, -16, 43) in the right PCG (right cerebrum, frontal lobe), while not significant, demonstrated trends in activity in the alpha band that resembled those of the left STG cluster (see Figure 5; Table 1). Mu desychronization in the alpha range was greatest for the emotion condition, followed by 74

the pitch distance condition, the hair control condition, then the non-biological control condition (Figures 6a, 6b). These trends can be easily observed when data are collapsed across time and frequency bands in the alpha range (see Figure 6b). See Table 1 for all results.

Figure 5

Figure 5. Cluster originating in the right PCG, which is considered to be part of the classical MNS. This cluster exhibited non-significant trends consistent with our predictions. Areas coloured in blue are manual tracings of the right PCG. The mean dipole location of the cluster contributing the pattern of activation found in the current study is depicted as a red dot.

75

Figure 6a.

Figure 6. a. Event-related frequency perturbations originating from the right PCG cluster, for each condition, across the 8-13 Hz alpha frequency band, a component of mu. Visual examination of these ERSPs indicates that there is greater desynchronization during the emotional task condition, followed by the non-emotional action intention task condition, followed by the two control conditions. While these differences are not significant, they suggest a trend in the predicted direction that may originate in the classical MNS. b. mu ERD collapsed across the 8-13 Hz frequency band and the 500-1500 ms time band of interest. This simpler graph illustrates that there is marginally greatest mu ERD in the emotion judgment condition (indicated by a lower mean value), followed by the non-emotional intention judgment condition, followed by the control conditions; this is a non-significant pattern that is consistent with our predictions.

76

Other Clusters
While we have sought to make conditions as identical as possible by manipulating task instead of stimuli, there are still high-level differences that vary systematically between judgment conditions that could have contributed to differences in mu desynchronization. For instance, emotional judgments could be considered more engaging or arousing than non-emotional judgments, which would lead to desynchronization in other brain regions in addition to MNS-related regions. For this reason, we were careful to examine any differences between occipital brain sources, which could be reflective of enhanced visual attention (Sauseng et al. 2005). There were no differences across judgment conditions for occipital brain sources. There were also no differences in any other brain sources, suggesting that our judgment manipulation did not cause unwanted systematic differences in attention/arousal between conditions.

Gender Differences
Performers in videos were both female. Most of our participants were also female (17 female, 3 male). There is some evidence that females are more expressive than males (Cheng et al., 2009), and that they have a more developed MNS, corresponding with a greater level of empathy (Cheng et al., 2009). Thus, it is possible that effects reported here were inflated in our mostly female sample. However, patterns of activation between emotional and non-emotional conditions have not generally been found to change across sexes in prior research.

Discussion
In the current study, we found enhanced mu ERD during emotion judgments vs. non-emotional judgments of sung stimuli, localized to the left STG. While the STG is 77

considered to be part of the extended MNS, there is uncertainty about whether it possesses mirroring properties per se (although see Molenberghs et al., 2012; Iacoboni et al., 2001). There was marginal support for a comparable pattern of activation found in the right PCG, an area believed to contain mirror neurons (Iacoboni et al., 2005). The current results support recent research suggesting that the extended MNS plays a distinctive role in the perception of emotional information, and further advances our understanding by demonstrating that this enhancement can occur under task-driven conditions. This finding supports the premise that it is the task-driven emotional attributes of the stimuli, rather than individual low-level systematic differences contributing to perception of emotional versus neutral stimuli, that lead to enhanced MNS responsivity during the perception of emotional action in this study. As predicted, there was a trend towards a step-function in terms of mu ERD during different judgment conditions. Pitch distance and hair judgments led to marginally greater mu ERD than the non-biological control condition, and emotion judgments led to significantly greater mu ERD than control conditions. Pitch distance and emotion judgments were marginally different from each other. These data indicate that the greatest amount of extended MNS activity in the left STG occurs during an emotionrelated judgment, as opposed to a different type of movement-intention-related judgment, or a non-movement-related judgment. Because we were asking participants to make emotional judgments, the lateralization of our STG finding is somewhat surprising. Many studies have demonstrated a right-sided bias for emotional processing in the brain (Schwartz, Davidson, & Maer, 1975; Davidson, 1998; Davidson et al., 1999) although this evidence

78

has been contested (Wager, Phan, Liberzon, & Taylor, 2003). Watson et al. (2014) and Kreifelts et al. (2009) also found that social processing, such as people-selective and voice-selective processing, occur mainly on the right side in the STS and STG. One difference between the current study and these previous studies is that the emotionality of our task came from a top-down judgment, rather than from passive viewing of emotional vs. neutral stimuli. Thus, participants were using inferential judgments to decide on the emotionality of the stimuli, which could preferentially recruit the left side (Gazzaniga, 1995). The left STG has also been implicated in auditory short-term memory (Leff et al., 2009), so this could also have played a role, as the participants may have engaged their short-term memory of the auditory stimuli during processing in order to make a correct judgment. The marginally enhanced activity in the right PCG towards emotion judgments may indicate that areas of the extended MNS stimulate classical MNS activation during emotion perception. Since this cluster was situated in the frontal lobe, it could be indicative of MNS activity involved in intentional attributions (Iacoboni et al., 2005). As one might expect, there was marginally greater activity in the pitch distance condition than the control conditions in this cluster, indicating responsivity to non-emotional intention judgments as well. The right lateralization of this cluster could be driven by the emotion judgment, given that emotional vocalizations have been found to be processed preferentially in the right hemisphere (George et al., 1996; Shirmer & Kotz, 2006). There were no differences between conditions when data were examined in channel space. This is likely because multiple brain sources were found to contribute to activity at the central electrodes, making it difficult to find differences as clearly as in

79

source space. For instance, several occipital sources were found to contribute to activity at the central electrodes in our study, where mu is typically measured. This is problematic, given that the alpha activity from occipital sources could easily mask mu activity originating from motor sources when observed in channel space (McGarry, Russo, Schalles & Pineda, 2012; Moore, et al., 2011). It is important to note that the differences we observed across experimental conditions cannot be attributed to difference in task difficulty. In a pilot study we found that the emotion and pitch distance judgments were equal in difficulty. Although our nonbiological control was not included in the pilot, we assume that difficulty level was comparable to that of the experimental conditions given that participants were asked to make interval size judgments on these stimuli as well. There has been a good deal of debate about the existence and function of the MNS in humans (Dinstein, Thomas, Behrmann, & Heeger, 2008). Some of the debate stems from controversial claims regarding the possible evolutionary role of the MNS in language development and socio-emotional communication (Dinstein et al., 2008), while some is due to the lack of direct evidence for "mirror neurons". The latter stems from complications involved with single-cell recording in humans. Nonetheless, there are multiple lines of evidence to support the existence of some form of action-observation, or "mirror" network in humans (for a review, see Rizzolatti & Craighero, 2004). For parsimony, we choose to refer to that network as the MNS despite the minimal evidence for "mirror neurons" in humans. One controversial claim that has sparked wide interest is the possible role of the MNS in emotional understanding. While there is some empirical evidence to support this

80

role (Carr et al., 2003; Ferri et al., 2006; Grosbras et al., 2006; Oberman et al., 2005), questions remain about the extent to which findings may have been due to differences in stimulus properties. This was one reason for our interest in comparing emotional versus non-emotional judgments of movements in the same set of actions. Our results are consistent with the view that there is an action-observation network and our evidence suggests that it is context-dependent. Specifically, we found that the results were judgment-dependent rather than stimulus-dependent. To elaborate, there are several lines of empirical research evidence pointing to a role of the MNS in the understanding of emotions. It has been shown, for example, that when we see someone smiling, we automatically mimic their smiling action (reliably measured via electromyography; Dimberg, Thunberg, & Elmehed, 2000). Indeed, "smiling" (zygomaticus major) muscles are activated more during perception of positive emotions, whereas "frowning" (corrugator supercilli) muscles are activated more during perception of negative emotions (Dimberg, et al., 2000), and mimicry is greater towards emotional vs. non-emotional movements (Moody & McIntosh, 2011). Facial mimicry has been observed in response to dynamic signals including speech (Hess & Blairy, 2001) and song (Chan, Livingstone & Russo, 2013), and thus does not depend upon prolonged exposure to the apex of an emotional expression. Some evidence suggests that facial mimicry is mediated by the MNS (Carr et al., 2003). Presumably, the MNS relays this information to the limbic system, thus helping us empathize with another person (Carr et al., 2003; LeDoux 1993). Research on individual differences provides more evidence that the MNS is involved in empathy. For instance, it has been found that individuals scoring highly on

81

empathy scales exhibit greater-than-average MNS activation in general during social perception (Gazzola, Aziz-Zadeh & Keysers, 2006; Singer et al., 2004). In addition, individuals with autism, who have difficulty with empathy, have been noted to have lower-than-average MNS activity during perception of other people's movements, but normal activity during execution of their own movements (Williams et al., 2006; Oberman et al., 2005). This finding has led to the provocative hypothesis that problems with empathy in autism may be related in some manner to a dysfunctional MNS (Dapretto et al., 2005; Oberman et al., 2005). In addition to individual differences studies, several studies have specifically examined the role of the MNS in emotion processing (Carr et al., 2003; Wicker et al., 2003; Enticott et al., 2008; Zaki et al., 2009; Pineda & Hecht, 2009; Perry et al., 2010; Aragón, Sharer, Bargh & Pineda, 2013; Grosbras et al., 2006; Ferri et al., 2013). For instance, Carr and colleagues (2003) examined fMRI activation during the observation and imitation of emotional facial expressions. They found activation in equivalent areas for observation and imitation, including MNS regions such as the bilateral inferior frontal cortex, and extended "emotional" mirroring areas such as the right insula, an area known to communicate with the limbic system. Carr et al. (2003) concluded that the MNS is involved in both the observation of and imitation of emotional action. Furthermore, they suggest that when we observe emotional action, we simulate this action neurally. This simulation presumably feeds into the limbic system, allowing us to feel how we would feel if we were making the perceived emotional expression ourselves. Wicker and colleagues (2003) conducted a similar study involving odour perception. They asked participants to smell pleasant or unpleasant odours, or to watch someone else smell a

82

pleasant or unpleasant odour. They observed enhanced BOLD activation in the MNS and the limbic system, indicating again that the MNS is involved in both the experience and observation of emotional states. The role of simulation in the perception of emotional action has been supported in various other fMRI (Zaki et al., 2009; Ferri et al., 2013; Grosbras et al., 2006), MEG (Enticott et al., 2008) and EEG (Pineda & Hecht, 2009; Perry et al., 2010) studies. In an fMRI study, Zaki et al. (2009) found that accuracy on an emotion perception task was correlated with activation in several areas of the MNS as well as other areas involved in mental attribution. Pineda & Hecht (2009) found enhanced EEG responses in the MNS towards static emotionally expressive images (Pineda and Hecht, 2008). Grosbras et al. (2005) found enhanced activation in the IFG, anterior STS, STG, pre-central sulcus (PCS), insula, and amygdala towards human grasping movements that were performed in an angry manner as opposed to a neutral manner, demonstrating that the classical MNS as well as extensions of the MNS are involved to a greater extent during emotional than neutral movements. Ferri et al. (2013) found enhanced MNS activity, specifically in the IFG, PCG, and parietal lobe (PL) towards grasping movements that were accompanied by an emotional facial expression. They concluded that the surrounding emotional context influences the degree of MNS activity during movement perception. There is also some precedent for the notion that the MNS is relevant for the perception of song (Lévêque and Schön, 2013). Our results suggest that in the domain of song, perception can occur through a structural (i.e., interval size) or emotional lens, and when perception occurs through an emotional lens, the left STG of the extended MNS is recruited more heavily. Speech, similarly, has been shown to involve the MNS (Warren

83

et al., 2006), and can also be considered to have a structural (linguistic) and emotional aspect (Warren et al., 2006). While our focus is often on the linguistic qualities of speech, we presumably require more MNS activation in speech when we're interested in its emotionality. A future direction of the current study would be to compare song and speech in terms of their reliance on the MNS during emotional versus structural processing. The present study supports and advances recent findings that the extended MNS plays a special role in the perception of emotional action (Wicker et al., 2003; Enticott et al., 2008; Zaki et al., 2009; Pineda & Hecht, 2009; Singer et al., 2004). Enhanced activation was found in the left STG towards audio-visual perception of human song when participants were asked to make an emotional judgment. The manipulation of task conditions while holding stimuli constant supports the premise that emotion-oriented processing of stimuli contributes to enhanced activity in the extended MNS and that this enhancement does not depend on systematic differences between emotional and neutral stimuli.

84

Chapter IV. Study 3: Bottom-up and top-down processing in the mirror system during perception of seen and heard emotional action

85

Abstract
Background: When we see or hear another person execute an action, we tend to automatically simulate that action. Evidence for this has been found at the neural level (specifically in the parietal and premotor brain regions, referred to collectively as the mirror neuron system (MNS)) and the behavioural level (through an observer's tendency to mimic observed movements). It has been suggested that this simulation process plays a key role in emotional understanding. Recent research consistent with this premise demonstrates that the human MNS is activated to a greater extent during (a) visual observation of emotional vs. non-emotional body actions, and (b) auditory observation of emotional vs. non-emotional vocalizations. Questions remain regarding how the mirror system responds during auditory perception of emotional vs. non-emotional nonvocal actions. In addition, it is not clear whether top-down processes play a role in MNS engagement during perception of human actions. Methodology: The current fMRI study examined emotional facilitation of the MNS in auditory and visual modalities using a 2 (Modality: auditory, visual) x 2 (Stimulus type: emotional, non-emotional) x 2 (Judgment type: emotional, non-emotional) factorial design. Participants observed a series of emotional or non-emotional actions involving the hands, presented via vision or audition. In one condition, they judged whether the action just observed was emotional or neutral in nature. In another condition, they made non-emotion based judgments of whether the action just observed was unimanual or bimanual. Results and Implications: The bottom-up stimulus manipulation yielded greater activation towards emotional vs. neutral stimuli in several areas of the MS, including the inferior frontal gyrus (pars opercularis), towards visual stimuli, and the supramarginal

86

gyrus towards auditory stimuli. In addition, our data suggest that the auditory modality is more responsive to positive emotions, whereas the visual modality is more responsive to negative emotions. Meanwhile, emotional judgments did not enhance MNS activation. Taken together, the data suggest that MNS responsiveness to body actions is less affected by the top-down task of judgment type, while it is reliably affected by bottom-up differences in stimulus emotionality.

87

Introduction
Understanding the motor intentions of another individual is a key factor in understanding another person's emotions. When we experience an emotion, this is often accompanied by movements that naturally express how we feel. Several researchers have hypothesized that when observing another person moving emotionally, the observer creates an internal simulation of the perceived emotional movements in his or her brain (Bastiaansen, Thious, & Keysers, 2009; Carr et al., 2003), as well as a peripheral simulation via his or her body (Chartrand & Bargh, 1999; Niedenthal, 2007). This simulation allows us to project how we feel when we make such movements (i.e., embarrassed, happy, fearful, or surprised), onto the person being observed, and thus results in a form of resonance between observed and experienced emotions. Many consider this internal simulation system to be a key component of empathy (Bastiaansen et al., 2009; Iacoboni & Dapretto, 2006; Oberman & Ramachandran, 2007). The current study examines the role of a candidate neural system for such simulation of others' emotions­ the human mirror neuron system (MNS). Mirror neurons were first discovered in monkeys, using single cell recordings. The early mirror neuron researchers found that neurons in the ventral premotor and inferior parietal cortices fire in a comparable way at the individual cell level during the execution and perception of identical movements (di Pellegrino et al., 1992), as well as different movements that have a matching end goal (Umilta et al., 2001). Since the discovery of mirror neurons in nonhuman primates, non-invasive neuroimaging research with humans has identified a similar system in the human brain, measured at the multiple-neuron level using methods such as fMRI, EEG, and MEG (Buccino et al., 2001; Carr et al., 2003; Iacoboni et al.,

88

2005; for a review, see Rizzolatti & Craighero, 2004). This system has been localised primarily to premotor and parietal cortices in the human brain, though some evidence suggests neural responses to action perception and production extend more broadly to also include posterior portions of the superior and middle temporal gyri (STG & MTG; Grezes & Decety, 2001; Grosbras, Beaton, & Eikoff, 2012; Iacoboni et al., 2001; Molenberghs, Cunnington, & Mattingley, 2012). While its role at the level of individual neurons is yet to be determined (although see Mukamel, Ekstrom, Kaplan, Iacoboni, & Fried, 2010), the MNS is thought to function similarly in humans to its homologue system in monkeys. The MNS has been shown to be involved in the visual perception of emotion expressed via body movements and the auditory perception of emotion expressed by the voice, but it is not yet clear whether this role extends to auditory perception of body movements. In addition, while various researchers have implemented procedures that draw on either bottom-up processing only (ie., passive viewing of stimuli; Enticott, Johnston, Herring, Hoy & Fitzgerald, 2008; Fecteau, Pascual-Leone, & Theoret, 2008) or also recruit top-down methods in order to enhance MNS processing (ie., counting pauses between stimuli presented to enhance attention; Oberman et al., 2005; Oberman, Ramachandran & Pineda, 2008), the role of top-down vs. bottom-up manipulations on activation in the MNS remains unclear. The current study examines MNS engagement during auditory and visual perception of emotional and neutral body movements, as well as the manner in which top-down and bottom-up emotion manipulations may differentially influence this response. Like mirror neurons investigated in the monkey brain (Umilta et al., 2001), the human MNS is active similarly during the perception and execution of identical actions

89

(Buccino et al., 2001; Fadiga, Fogassi, Pavesi & Rizzolatti, 1995; Rizzolatti & Craighero, 2004), as well as different actions with similar intentions (Iacoboni et al., 2005). Researchers have theorized that the MNS may also play a special role in the perception of emotional action, as a key component of empathy (Carr et al., 2003; Gallese, Keysers & Rizzolatti, 2004). This notion has been supported empirically by individual differences studies, which reveal greater MNS engagement during action perception among individuals with higher levels of empathy (Gazzola, Aziz-Zadeh, & Keysers, 2006), and lower engagement in individuals with lower levels of empathy, such as individuals with autism (Oberman et al., 2005; Williams et al., 2006). MNS activation has also been found to vary with how well individual participants perform at judging the emotions of others (Enticott et al., 2008; Zaki, Weber, Bolger & Oschner, 2009). In addition, it has been demonstrated that the MNS may exhibit superior levels of involvement during perception of emotional versus non-emotional actions (Carr et al., 2003; Ferri et al., 2013; Grosbras & Paus, 2006; Wicker et al., 2003). In one such study using fMRI, Grosbras & Paus (2006) examined the role of the MNS in emotion perception by asking participants to view dynamic depictions of angry and neutral hand actions, as well as angry and neutral facial expressions. Actions involved reaching and grasping an object, and performers were instructed to grasp objects in a neutral or angry manner. It was found that the ventral premotor cortex (vPMC) and the supramarginal gyrus (SMG) of the MNS, as well as the insula, which has strong functional ties to the MNS (Carr et al., 2003; Pineda, 2008), were activated to a greater extent when viewing angry grasping gestures as opposed to neutral gestures, as well as when viewing both angry and neutral facial expressions. While the authors did not find

90

significant differences in MNS activation towards emotional compared to neutral facial expressions, marginally significant differences in the vPMC reflected the pattern that emerged when viewing body movements. The authors concluded that the MNS, specifically the vPMC, shows greater engagement during perception of emotional compared to neutral actions. Moreover, the SMG shows a similar pattern, at least in terms of object-directed body movements. In another recent fMRI study, Ferri et al. (2013) demonstrated greater activity in purported mirroring areas including the inferior frontal gyrus (IFG) and precentral gyrus (PCG) during visual depictions of angry versus neutral actions, implicating these areas in the perception of angry actions. In their study, participants watched three categories of stimuli. Action stimuli consisted of a neutral grasping movement. Emotional face stimuli consisted of a happy, neutral or angry facial expression. Emotional action stimuli consisted of a neutral grasping movement accompanied by either a happy, neutral, or angry facial expression. The authors found that neutral grasping movements accompanied by an angry face enhanced activation in the IFG and PCG, suggesting that the presence of an emotional context was associated with greater MNS activation. Thus, available evidence supports the premise that the MNS not only plays an important role in the perception of emotion in others' actions, but also that its response profile is sensitive to the emotional content of an observed action. The MNS has been found to respond to the execution and perception of not only visual but also auditory depictions of action (Gazzola et al., 2006). Gazzola et al. (2006) had participants listen to mouth actions as well as hand actions, wherein the performer operated on an object by manipulating it with the mouth or the hand. Enhanced activation

91

was found in a left temporo-parietal-premotor circuit towards auditory depictions of action, including the left IFG (BA44), BA6, inferior parietal lobule (IPL), and bilateral middle temporal gyri and superior temporal sulci (STS). The left IPL was activated to a greater extent during perception of hand vs. mouth movements presented via audition, and a ventral portion of the left IFG (BA44) responded more to mouth actions. They also found that activation in BA44, BA6, the IPL, and the supplementary motor area (SMA) correlated with participants' scores on the Perspective Taking subset of the Davis Empathy Scale (Gazzola et al., 2006). These results support the existence of an auditory mirror system in humans, and suggest that activation in this system may be related to trait empathy. Warren et al. (2006) further examined the role of the auditory MNS in emotion perception using non-verbal emotional vocalizations. Participants listened to emotional vocalizations including triumph, amusement, fear and disgust, as well as a baseline vocalization created artificially using elements of the authentic vocalizations. They found enhanced activation in the left IFG towards positively-valenced emotions, and enhanced activation in the pre-SMA towards emotional vocalizations that were positive in valence and high in arousal. These results suggest that the MNS codes the emotionality of vocal stimuli presented via audition, at least when the valence is positive. However, it remains unknown how the MNS responds to emotional compared to neutral auditory presentations of non-vocal body movements. A number of potential stimulus confounds exist in previous studies that have looked at MNS engagement during the visual depiction of emotional vs. neutral action that limit the interpretation of results. Grosbras and Paus (2006) used angry vs. neutral

92

grasping actions as their stimuli, which could have differed between conditions on aspects other than emotionality, such as speed or movement contour. In the Ferri et al. (2013) study, neutral grasping actions were paired with emotional facial expressions to create emotion + action stimuli, so it is difficult to determine whether enhanced activity was due to perception of dynamic emotional action itself, or to a combination of MNS engagment during action perception and emotional responses to faces. Because of these limitations, how an observer processes visual non-facial emotional actions should be explored further, as well as how the MNS responds to auditory presentation of non-vocal emotional actions, which is yet to be examined. One way to control for systematic differences between emotional and neutral stimuli is to use a top-down study manipulation. In this type of design, stimuli are kept identical across task conditions. Study 2 of this dissertation found evidence for top-down modulation of the mu rhythm (indicative of MNS activation) towards emotional vs. neutral judgments of song stimuli. A source-localization procedure indicated that topdown modulation of the mu rhythm was significant in the left STG, an area considered to be part of the extended MNS (Pineda, 2008). The use of fMRI in the present study should enable enhanced spatial sensitivity, as EEG source localization methods are generally estimated to localize from 4 mm at best to 20 mm at the most basal areas of the scalp (Acar & Makeig, 2013), while fMRI is capable of much higher levels of resolution, with users generally choosing a resolution of 3 mm for uniform whole-brain analyses (Toga & Mazziota, 2002). This enhanced spatial sensitivity offered by fMRI will help determine whether other areas of the MNS are also selectively recruited during emotional judgments of action stimuli.

93

Castelli, Frith, Happe and Frith (2002) have postulated that top-down governance of MNS activation could arise via connections to the the temporal pole and/or medial prefrontal cortex (mPFC) through the STS. The temporal pole/mPFC could mediate activation in the STS during the perception of social information, leading a person to respond more strongly during situations with social relevance (Castelli et al., 2002; Press, Gillmeister, & Heyes, 2006). The STS could then relay these enhanced signals to the MNS, leading to enhanced activation in the MNS during emotional situations (Rizzolatti, 2005). This could be a neuropsychological route to supporting enhanced simulation of emotional vs. neutral movements, which I suggest could be manipulated in a top-down manner, depending on a person's knowledge of the social significance of a certain context. Some empirical research has already provided evidence that MNS activation towards non-biological movements is sensitive to top-down manipulations, perhaps by employing this neural pathway. Engell, Burke, Fiehler, Bien & Rosler (2008b) compared top-down and bottom-up processing of animacy in non-biological movement in the MNS, to determine whether top down and bottom up processes play different roles in its activation. They showed participants robotic movements that were either smooth or discontinuous (bottom-up stimulus manipulation), which were accompanied by a judgment of stimulus colour, or by judgments of the participant's self-reported ability to simulate the depicted action (top-down task manipulation). They found that both the smoothness of the stimulus (bottom-up manipulation) and simulation judgments (topdown manipulation) resulted in enhanced activation of the MNS when participants observed robotic movements. The authors concluded that both bottom-up and top-down manipulations are sufficient to enhance MNS responses to non-human movement. This

94

suggests that while bottom-up manipulations reliably lead to enhanced activation in the MNS, top-down manipulations can also play an important role in MNS facilitation, at least during observation of non-human as opposed to human action. The authors note that an interesting next step would be to examine whether MNS activation toward human movement can also be modulated by the perceiver's task. In the current study, I examine this question of whether MNS activation when observing human movement is affected by the perceiver's task. I also revisit the question of whether the MNS is involved to a greater extent during emotional vs. neutral perception of human actions, in both auditory and visual modalities, by manipulating emotionality at stimulus and task levels. Stimuli were emotional or neutral in nature, allowing for an assessment of bottom-up processes involved in the perception of action. Perceivers were asked to make judgments about the emotional vs. non-emotional elements of identical action stimuli, allowing for an assessment of top-down processes in the perception of action. MNS activation was measured during auditory or visual depictions of action while the perceiver made judgments. Stimuli included happy, neutral, and angry hand actions. Before the main scanning procedure took place, a localiser procedure was applied during which participants had to execute, listen to, or view different hand actions. This allowed more specific isolation of areas involved in mirroring in this particular sample. The specific questions were as follows. First, how are emotional vs. neutral stimuli processed in the MNS? Bottom-up processing was measured across modalities (emotional > neutral stimuli), as well as within each modality. I predicted that emotional stimuli would lead to greater activation in the MNS than neutral stimuli, across judgment

95

type and modality. Secondly, how are the same stimuli processed in the MNS, when judgment is manipulated (emotional or neutral)? Top-down processing was measured across modalities (emotional > neutral judgments), as well as within each modality (auditory: emotional > neutral judgments; visual: emotional > neutral judgments). I predicted that emotional judgments would lead to greater MNS activation than neutral judgments, across stimulus type and modality. Third, are there differences across modality depending on the valence of an emotion? Bottom-up differences in processing different emotion types were examined in each modality. Previous findings suggested that there may be enhanced activation towards happy emotions in the auditory domain (Warren et al., 2006), and towards angry emotions in the visual domain (Grosbras et al., 2006). Finally, does empathy vary with MNS responsivity to emotions? On the basis of Gazzola et al. (2006), I predicted that scores on the Perspective Taking subscale of the Davis Empathy Scale should correlate with MNS responsivity to emotional action in each perceptual modality.

Methods
Participants
Twenty-two students from the Bangor University undergraduate psychology participant pool were tested. In order to qualify for testing, participants had no neurological or psychiatric impairments, and passed an initial prescreen procedure to ensure they were safe to scan. Two participants were excluded from the final sample due to excessive movement-related artifacts in their fMRI data. For the remaining 20 participants that composed the final sample, the average age was 19.65 years +/-2.62 SD,

96

and the average education was 14.16 years +/- 1.3 SD. Thirteen males and 7 females were included in this final sample.

Stimuli
Localiser Stimuli. For the action localiser, participants were required to perform, watch or listen to two different actions. These actions were ripping a paper towel and opening/closing a wide-mouthed water bottle. For the video stimuli, a male actor was filmed performing each of these actions in a sound attenuated studio using a digital video camera and microphone. Only the actor's torso was seen in the videos. Stimuli were presented as visual-alone as well as audio-alone, and participants were also asked to execute the actions themselves. During localiser scanning, a paper towel roll was situated beneath the participant's left hand, and a water bottle was situated beneath the participant's right hand. Experimental Stimuli. For the main experiment, stimuli were selected to vary in emotionality (happy/angry/neutral) and number of hands involved in creating the action (one/two) and were presented either in the visual domain (video without sound) or the auditory domain (sound without video). Visual presentations depicted only the male actor's hands/body, and not his face. See Table 1 for a layout of stimulus categories, and a list of stimuli included in each category. Exemplars from each category were recorded in a sound-attenuated studio using a digital video camera, and piloted for recognition of emotionality and number of hands on visual analog scales, in both the auditory and the visual modality. The four best exemplars from each emotional category, and the eight best exemplars from the neutral category, were chosen as the final stimuli (allowing for a total of eight neutral and eight emotional stimuli). Final stimuli were selected based on 97

possessing equivalent low-level features such as duration of audio and visual components, presence of objects, and number of hands present in the film. Happy and angry stimuli were also selected based on equivalency in intensity (dB A) and attack (time to peak loudness).
Table 1. Actions used in final stimuli.

Happy Angry Neutral

Unimanual Shaking a baby rattle Squeaking a dog toy Ringing a bell impatiently Slamming a phone down Turning a book page Turning on a hair dryer Clicking a click pen Writing with marker

Bimanual Waving a tambourine Banging a click-clack toy Breaking a pencil Crumpling a sheet of paper Sharpening a pencil Cutting with scizzors Using a stapler Snapping a paper clip

Apparatus
Before imaging, all videos were shown in their full, intact state (audio-visually) to participants via a Mac computer with headphones, in order to familiarize participants with which sounds accompanied which actions. While all stimuli had been piloted unimodally to ensure that emotionality and number of hands were apparent during listening only, we added this familiarization procedure as an extra insurance that participants would be able to recognize heard actions. During fMRI scanning, stimuli were presented using Presentation software from a PC computer. Visual stimuli were presented using a back projection system on an LCD full HD monitor behind the scanner, reflected onto a mirror above the participants' eyes. Auditory stimuli were presented via high quality sound attenuated headphones designed for scanner compatibility. Scanning took place using a 3-T Philips MRI scanner using a SENSE phasedarray head coil, implementing a T2* functional scan. A gradient echo sequence was applied, with 3.75 x 3.75 in-plane resolution for whole-brain analysis. There was a 4-mm 98

slice thickness with a 0.5 mm gap and a flip angle of 90 degrees. The TE was 30 ms and the TR was 2.5 s. The first two volumes were discarded due to initial system warm-up. A T1 structural scan was also conducted for anatomical orientation of data. The T1 scan used a 240 x 240 mm2 matrix with voxel dimensions of 2 x 2 x 2 mm, a TR of 12 ms and TE of 3.5 ms, and a flip angle of 8. Scans were inspected to see whether there were any distortions at the beginning or end of the procedure. Five scans were discarded at the end of the procedure for one participant, and 10 scans were discarded at the end of the procedure for a second participant. All other participants' scans were intact across the procedure.

Design
The experiment followed a 2 (Judgment: emotion, neutral) x 2 (Stimulus: emotion, neutral) x 2 (Modality: audio, visual) repeated measures within-subjects mini-block fMRI design. The dependent variables were BOLD activation in the MNS during video/auditory presentation, as well as accuracy and reaction time for each judgment task. See Figure 1a for a visualization of the design.
Figure 1a. Study Design.

Figure 1a. Judgment emotionality (emotion, neutral) stimulus emotionality (emotion, neutral), and modality (audio-alone, visual-alone) were manipulated.

99

Procedure
Before entering the scanner, participants were familiarized with bimodally presented audio-visual versions of all the stimuli they would experience unimodally in the scanner. In the scanner, the main experimental procedure was preceded by the localiser task, in which participants either viewed videos, listened to sounds, or executed actions themselves, in counterbalanced, blocked order. Actions included in each of these conditions were ripping a paper towel, opening a water bottle, or closing a water bottle. In the listen (auditory only) condition, participants heard each action while a fixation cross appeared on the screen. In the view (visual only) condition, participants viewed a video of each action onscreen, preceded by a fixation cross. In the execute (action only) condition, participants saw a colour cue to show them which action they should complete. A white dot indicated they should pick up the white paper towel roll, and a blue dot indicated they should pick up the blue water bottle. When each dot turned orange, this meant it was time to complete the action. They had 6 seconds to complete the action, during which the orange dot became progressively smaller over 3 size changes. When the dot turned into a fixation cross again, this meant the participant should have completed the action with hands returned to rest position. An experimenter remained in the scanner room during the localiser procedure to assist the participant if the paper towel roll or water bottle became difficult for the participant to reach. Following the localiser task, the main experimental procedures began. A miniblock/mixed design was used for stimulus presentation. Judgment conditions were presented in mini-blocks, alternating emotional judgment and neutral hand judgment conditions for each of auditory and visual stimulus types, counterbalanced across

100

participants. Individual emotional and neutral stimuli were presented in a randomized order within each mini-block. In the emotional judgment mini-block, participants were asked to press one button if the stimulus was emotional, or press a different button if the stimulus was not emotional. In the neutral judgment block, participants were asked to press one button if the depicted action involved one hand, and a different button if the depicted action involved two hands. The four mini-block types (2 (Judgment) x 2 (Modality)) were presented in counterbalanced order, three times each per run, over two runs. Buttons that corresponded to the correct answer were randomized on every trial, requiring the participant to make their judgment first (i.e., emotional or neutral, 1 hand or 2 hands) and then look at which button their answer corresponded to. This was done to decrease motor planning during observation of videos, which could interfere with experimentally-manipulated areas of brain activation. Within each mini-block, trials unfolded as follows: 4-second instruction screen presented only at the beginning of each mini-block, 1 second fixation cross, 2-3 second video/audio clip presentation, 500 ms blank screen, 2-second judgment screen to make button press, 1 second jittered fixation cross, 2-3 second video presentation, and so on, until 8 clips had been seen and judged. The length of videos varied but were equated across stimulus conditions. If a video was shorter than 3 seconds, the remainder of the 3 seconds depicted a blank screen. Eight trials (two stimulus types (emotional, neutral) x two hand types (one, two) x two unique exemplars) were presented in each block, making each mini-block approximately 1 minute in length. Participants were given a 2-minute rest period in between runs, resulting in the entire procedure lasting approximately 40 minutes (in addition to a five-

101

minute structural scan). At the end of the scanning session, participants filled out the Davis Empathy Scale. See Figure 1b for a sample depiction of the procedure.
Figure 1b. Sample Study Sequence.

Localiser: (Execute  Listen)  (Execute  View)
Because I intended to examine activity in MNS areas responsive to auditory perception and execution of action as well as visual perception and execution of action, the localiser was created with a combination of AND and OR conjunction analyses ­ areas included in the localiser were required to be activated during both listening AND execution of action, OR during both viewing AND execution of action ((Execute  Listen)  (Execute  View)). The individual contrasts contributing to the localiser consisted of the listen condition-fixation cross viewing periods in between sounds, the view condition-fixation, and the execute condition-fixation. Each of these contrasts was run at an extent threshold of 5 voxels and a liberal uncorrected p value < 0.01. I chose a low significance threshold to optimize power and include as many mirroring areas from

102

each participant as possible. The localiser conjunction analysis was run with the requirement that individual contrasts entering the localiser for comparison already be significant, following Nichols et al. (2005). Binary figures were created for each of execute, view, and listen conditions, and then combined into two AND masks, one for auditory mirroring and one for visual mirroring. The auditory mirroring AND mask required activation during both execute and listen in a given area in order for that area to be included. The visual mirroring AND mask required activation during both execute and view conditions in a given area in order for that area to be included. Finally, the results of each of these AND masks were added together to create an auditory x execute + visual x execute OR mask. See Figure 2 for a visual depiction of the mask. In addition to the conjunction analysis performed to create the mask used in the present study, an additional conjunction analysis was performed on the localizer data, with each individual participant's localizer data entered into the conjunction separately. It was found that the IFG, IPL, and the PCG were each activated in overlapping voxels during listen, view, and execute conditions in every participant. This analysis, while exploratory, supports the premise that every participant in this sample exhibited some degree of overlapping activity in traditionally defined mirroring areas during the execution and perception of action. See Figure 2c for a visual depiction of this separate conjunction analysis across individual subjects using localizer data.

103

Figure 2a.

Figure 2a. Localiser. Red = execute, Green = Listen, and Blue = View. White = overlap of Listen x Execute  View x Execute. See Venn diagram for colours corresponding to each condition or conjunction. Areas that were conjunctively activated between listen & execute or view & execute included the bilateral IFG, bilateral IPL, especially the SMG, bilateral pre- and post- central gyri, and bilateral inferior and middle TG. Conjunction figure was produced on a single subject's anatomical image using a custom Matlab script. Figure 2b.

Figure 2b. Localiser. Only includes areas meeting the threshold for Listen and Execute or View and Execute (ie., the white, yellow and pink colours from Figure 2a). This conjunction figure was produced using MRIcron software. Figure 2c.

Figure 2c. Conjunction across individual subjects' localizer data. Includes areas of overlap between Listen & Execute or View & Execute for each individual subject, then selects for only those areas that also overlap across all subjects (excluding activity that may have involved mirroring within each individual subject if it did not overlap across all). Classical mirroring areas including voxels of overlap across all participants for View & Execute include the IPL (BA7) & IFG (BA44), with additional activity in the PostCG, STG, thalamus, and caudate. For Listen & Execute, classical mirroring areas such as the IPL (BA7) and IFG (BA44) were activated, as well as the postCG, auditory cortex, putamen, caudate, and thalamus.

104

Data analysis
Behavioural data were examined for accuracy and reaction time. For each of these dependent variables, an ANOVA was conducted with judgment type (emotion, neutral), stimulus type (emotion, neutral) and modality (auditory, visual) as within-subjects factors. A follow-up ANOVA was then conducted, with emotion type (happy, angry, neutral) as the within-subjects factor. Before preprocessing, anatomical imaging data were manually oriented with the Statistical Parametric Mapping (FIL Methods Group, 2013) T1 template, and each participant's functional scan was realigned with SPM's EPI template, in order to ensure proper orientation of scans. Data were then auto-aligned using SPM, unwarped, corrected for slice timing, coregistered and normalized to individual participants' T1-segmented anatomical scans with a resolution of 3 mm, and spatially smoothed at 8 mm using SPM8 software. Imaging data were analyzed using an event-related fMRI design, focusing on data collected during viewing of each video, with separate regressors for the instruction, fixation cross, and response periods. A localiser-based ROI analysis was applied, focusing contrasts specifically on areas involved in auditory and visual mirroring, as identified during the localiser task. Follow-up analyses compared individual emotion types within each modality. Contrasts of interest were calculated within the localiser mask created from the same group of participants. This mask was applied during calculation of results at the fixed effects level. All significant clusters reported in the results below reached a significance level of k = 10 voxels and p < 0.005, uncorrected. Due to a priori

105

hypotheses, areas considered part of the classical MNS are reported regardless of whether they met an additional cluster-level FWE-corrected threshold of p < 0.05. Areas outside of the classical MNS but within our MNS mask that are significant for a specific contrast are reported if they met the corrected cluster-level p-threshold (p < .05, FWE corrected), or if they were the only area activated for that contrast.

Results
Behavioral analyses
Accuracy. Accuracy scores (converted to proportions) are plotted in Figure 3a. Accuracy was analyzed using a repeated-measures ANOVA, with modality (auditory, visual), judgment (emotion, neutral) and stimulus (emotion, neutral) as within-subjects factors. Accuracy was low across conditions, and in some conditions appeared to be at or below chance. Chance is considered here to be less than 50%, as there were 2 answer choices but there was also a possibility of not answering at all as the user had 2 seconds to complete their response. The low accuracy scores were probably a result of the difficulty of the task in the scanner, in addition to the randomization of correct answer buttons. There was a main effect of modality, such that accuracy was greater in the visual domain than in the auditory domain; F (1,19) = 28.76, p < .001, = .602. There was a

main effect of judgment, such that emotional judgments had lower accuracy than neutral judgments overall; F(1,19) = 94.73, p < .001, = .833. There was also a main effect of

stimulus, such that emotional stimuli had greater accuracy than neutral stimuli overall (see Figure 3a); F(1,19) = 26.08, p < .001, = .579. There was also a modality x

judgment interaction, which showed that the auditory modality was driving differences in accuracy between judgment types ­ auditory accuracy was lower for emotional than 106

neutral judgments, whereas visual accuracy did not change for emotional vs. neutral judgments; F(1,19) = 80.46, p < .001, = .809. There was a modality x stimulus = .574, demontrating that in the

interaction as well; F(1,19) = 25.649, p < .001,

auditory modality, accuracy was higher for emotional than neutral stimuli, whereas in the visual modality, accuracy was constant across emotional and neutral stimuli (see Figure 3a). In addition, there was a judgment x stimulus interaction. This showed that accuracy varied differently by emotionality for bottom-up (stimulus) vs. top-down (judgment) contexts ­ accuracy increased with emotionality at the stimulus level, but decreased with emotionality at the judgment level; F(1,19) = 12.72, p = .002, = .401. Finally, there

was also a modality x judgment x stimulus interaction. This interaction suggests that the auditory modality was driving the judgment x stimulus interaction: in the auditory domain, accuracy increased with emotionality at the stimulus level, and decreased with emotionality at the judgment level, but in the visual modality, accuracy was not affected by emotionality for stimuli or judgments; F(1,19) = 18.68, p < .001, = .496.

Next, we broke down the emotional stimuli into happy and angry categories, to examine differences in accuracy between emotion types. We ran an ANOVA with modality (auditory, visual) and emotion (happy, angry, neutral) as 2 within-subjects factors. As with the earlier analysis, a main effect of modality emerged, with overall greater accuracy for visual than auditory stimuli; F(1,19) = 13.612, p = .002, = .417.

There was also a main effect of emotion, with overall accuracy greatest for happy, followed by angry, then lowest for neutral stimuli; F(1,19) = 13.331, p < .001, = .412.

In addition, there was a modality x stimulus interaction, such that for auditory stimuli, accuracy was greatest for happy, followed by angry and then neutral, whereas for visual

107

stimuli, accuracy was greatest for angry, followed by happy and then neutral; F(1,19) = 33.818, p < .001, = .640. Post-hoc, Bonferroni-corrected t-tests (p-threshold of .05/2

= .025) indicated that differences between angry and happy stimuli within each modality were significant; auditory happy vs. angry: t(19) = 5.577, p < .001; visual happy vs. angry: t(19) = -2.448, p = .024.
Figure 3a. Accuracy scores.

1! 0.9! 0.8! 0.7! Proportion(Correct( 0.6! 0.5! 0.4! 0.3! 0.2! 0.1! 0! Emotional! Neutral! Emotional! Neutral! Emotional! Neutral! Bimodal! Auditory! Visual! Judgment! Stimulus!

Figure 3a. Accuracy for emotional vs. neutral judgments and stimuli, across auditory and visual modalities, converted to proportions. Chance performance is estimated to be .33, as there were 3 response possibilities: response 1, response 2, or no response.

Reaction Time. Reaction time (RT) data are plotted in Figure 3b. The data was subjected to a 2 (modality: auditory, visual) x 2 (judgment: emotional, neutral) x 2 (stimulus: emotional, neutral) repeated-measures ANOVA. There was a main effect of

108

modality, such that responses to auditory stimuli were slower than responses to visual stimuli; F(1,19) = 40.47, p < .001, = .681. There was a main effect of judgment,

indicating that responses to all stimuli were slower during emotional judgments than during neutral judgments; F(1,19) = 40.74, p < .005, = .682. There was also a main

effect of stimulus, showing that responses to emotional stimuli were faster than responses to neutral stimuli; F(1,19) = 50.62, p < .001, = .727. There were no interactions. See

Figure 3b for a graphical depiction of RT results. Following this analysis, emotion was broken down into happy, angry and neutral categories, and a repeated-measures ANOVA was applied with modality (auditory, visual) and stimulus (angry, happy, neutral) as the within-subjects factors. Again, there was a main effect of modality; F(1,19) = 9.264, p = .007, = .328. There was also a

main effect of stimulus, such that angry lead to the slowest responses, happy to the next slowest responses, and neutral to the fastest responses; F(2,38) = 9.589, p < .001, = .335. In addition, there was a modality x stimulus interaction; F(2, 38) = 13.934, p < .001, = .423. This interaction demonstrated that while responses to angry and neutral

stimuli remained relatively similar between modalities, there was a difference between modalities in response to happy stimuli, with response time being slower in the auditory domain, and faster in the visual domain. While a post-hoc Bonferroni-corrected t-test indicated that the difference in speed of response towards happy vs. angry stimuli was significant in the auditory modality, t(19) = 3.171, p = .005, speed to happy vs. angry stimuli was not significantly different in the visual domain, t(19) = 1.496, p = .151.

109

Figure 3b. Reaction time.

1000! 900! 800! Reaction(Time(in(ms( 700! 600! 500! 400! 300! 200! 100! 0! Emotional! Neutral! Emotional! Neutral! Emotional! Neutral! Bimodal! Auditory! Visual! Judgment! Stimulus!

Figure 3b. Reaction time for emotional vs. neutral judgments and stimuli, across auditory and visual modalities.

Mask analyses: Auditory OR visual MNS regions
All imaging analyses were conducted within the localizer mask. Significant areas of activation for each contrast are also reported in Table 2. See the Appendix A for supplementary whole brain analyses.

110

Bottom-up analyses. Our first set of analyses focuses on bottom-up differences in emotional vs. neutral action perception within each modality. Auditory: Emotional > Neutral Stimuli. For auditory stimuli, emotional stimuli elicited greater activation than neutral stimuli in the left IFG, specifically in the orbitalis (BA45). In addition, the left SMG of the IPL was more activated towards emotional vs. neutral stimuli (see Figure 4a). One area outside of the MNS, the right cerebellum, met the corrected cluster-level threshold. There were no clusters that were activated to a greater extent for neutral than emotional stimuli.
Figure 4a.

Figure 4a. Auditory: Emotional > Neutral Stimuli. Surface rendered map of activations and accompanying parameter estimates. Significant clusters include the left supramarginal gyrus (SMG) and the left inferior frontal gyrus (IFG) orbitalis. Purple colour represents more brain activation, while blue represents less brain activation. All four stimulus conditions (Auditory emotional, auditory neutral, visual emotional, and visual neutral) are shown in the bar graphs accompanying each figure so the reader can visualize how the conditions included in this pairwise comparison compare to patterns in the other modality. All surface rendered figures were produced using SPM8 software.

111

Visual: Emotional > Neutral Stimuli. An extensive cluster in the right IFG emerged from the visual emotional > visual neutral stimulus contrast, specifically the pars opercularis, showing greater activation during emotional vs. neutral visually-presented stimuli (see Figure 4b). When the reverse contrast was applied, no clusters were significantly more activated towards neutral stimuli than emotional stimuli.
Figure 4b.

Figure 4b. Visual: Emotional > Neutral Stimuli. Surface rendered map of activations and accompanying parameter estimates. The right IFG (opercularis) was significant for this comparison. Purple colour represents more brain activation, while blue represents less brain activation. All four stimulus conditions (Auditory emotional, auditory neutral, visual emotional, and visual neutral) are shown in the bar graphs accompanying each figure so the reader can visualize how the conditions included in this pairwise comparison compare to patterns in the other modality. All surface rendered figures were produced using SPM8 software.

Interaction: (Emotional Auditory > Neutral Auditory) vs. (Emotional Visual > Neutral Visual). In order to examine whether emotional content of stimuli impacted activity more in either individual modality than the other, we also ran an interaction of emotional > neutral stimuli in the auditory vs. visual modalities. The right SMG was more responsive to emotional than neutral stimuli in the auditory modality (see Figure 4c(i)), and the right IFG (pars opercularis) was more responsive to emotional than neutral stimuli in the visual modality (Figure 4c(ii)).

112

Figure 4c(i).

Figure 4c(i). Interaction: (Emotional Auditory > Neutral Auditory) > (Emotional Visual > Neutral Visual). Surface rendered map of activations and accompanying parameter estimates. The right SMG was significant for this comparison. Purple colour represents more brain activation, while blue represents less brain activation. All four stimulus conditions (Auditory emotional, auditory neutral, visual emotional, and visual neutral) are shown in the bar graphs accompanying each figure so the reader can visualize how the conditions included in this pairwise comparison compare to patterns in the other modality. All surface rendered figures were produced using SPM8 software. Figure 4c(ii).

Figure 4c(ii). Interaction: (Emotional Visual > Neutral Visual) > (Emotional Auditory > Neutral Auditory). Surface rendered map of activations and accompanying parameter estimates. The right IFG (opercularis) was significant for this comparison. Purple colour represents more brain activation, while blue represents less brain activation. All surface rendered figures were produced using SPM8 software.

Conjunction: Auditory  Visual, Emotional > Neutral. In order to examine which areas are concurrently active during emotional perception in both the auditory and visual domains, we also performed a conjunction analysis to see what areas are evenly activated for emotional > neutral across auditory and visual modalities. The only areas found to be activated across both modalities for this analysis were the left STG and the left MTG (see Figure 4d).

113

Figure 4d.

Figure 4d. Conjunction: Auditory & Visual, Emotional > Neutral. Conjunction area of auditory + visual emotional > neutral contrasts in yellow, auditory-only in red, visual-only in green. Conjunction figure produced on a single subject's anatomical image using a custom Matlab script.

Interaction: Happy vs. Angry, Auditory vs. Visual. Next, we examined the differences in activation for the two emotional stimulus types included in the study, happy and angry, for auditory and visual modalities. There was an interesting crossover effect during the perception of happy vs. angry emotions across modalities (see Figure 4e). In the auditory modality, happy emotions generated greater activation than angry emotions in several areas, whereas the opposite was the case in the visual modality. This was reflected in a significant interaction of (happy auditory > angry auditory) > (happy visual > angry visual), as depicted in Figure 4e. The inverse interaction, (happy visual > angry visual) > (happy auditory > angry auditory) had no significant clusters. Areas that responded more when observing happy compared to angry actions in the auditory modality include the bilateral IPL and bilateral PCG. Follow-up simple-effects contrasts revealed that the left lingual gyrus responded more when participants observed angry compared to happy emotions in the visual modality. See Table 2 for a complete list of areas that emerged from this interaction and follow-up pairwise comparisons. Interestingly, these findings are reflected in the behavioural data, such that there was greater accuracy for happy auditory stimuli, as well as greater accuracy for angry visual stimuli. 114

Figure 4e.

Figure 4e. Interaction: Happy > Angry, Auditory > Visual. Surface rendered map of activations and accompanying parameter estimates. The PCG and IPL were significant bilaterally for this comparison. Purple colour represents more brain activation, while blue represents less brain activation. All surface rendered figures were produced using SPM8 software.

Top-down analyses. Our next set of analyses focused on examining the effects of top-down emotionality in the MNS, for auditory and visual modalities. Auditory: Emotional > Neutral Judgments. No significant clusters were activated to a greater extent for emotional vs. neutral judgments. When the reverse contrast was applied, it was found that the left IPL was activated more strongly for neutral judgments than emotional judgments. Visual: Emotional > Neutral Judgments. A cluster in the right calcarine gyrus demonstrated significant differences in activation between emotional and neutral judgments (see Figure 4f). Meanwhile, the left pre- and post-central gyri as well as the right lingual gyrus were more activated in the visual domain for neutral judgments than emotional judgments.

115

Figure 4f.

Figure 4f. Visual: Emotional > Neutral Judgments. Surface rendered map of activations and accompanying parameter estimates. The right calcarine gyrus was significant for this comparison. Purple colour represents more brain activation, while blue represents less brain activation. All four stimulus conditions (Auditory emotional, auditory neutral, visual emotional, and visual neutral) are shown in the bar graphs accompanying each figure so the reader can visualize how the conditions included in this pairwise comparison compare to patterns in the other modality. All surface rendered figures were produced using SPM8 software.

Within Stimulus type. Effects of top-down emotional judgments may have been suppressed due to ceiling effects of MNS activation towards all emotional stimuli, regardless of judgment. In order to see whether this was the case, a contrast was run for emotional > neutral judgments within just neutral stimuli and just emotional stimuli. Each of these yielded no significant clusters, suggesting that ceiling effects driven by emotional stimuli were not an issue.

The Davis Empathy Scale
In order to examine whether self-reported empathy scores were correlated with activation in the MNS, following Gazzola et al. (2006), I ran a correlation of Emotional > Neutral stimuli, as well as Emotional > Neutral judgments, with the total score on the Davis Empathy Scale, as well as the Perspective Taking subscale of the Davis Empathy Scale. None of these correlations met significance.

116

Table 2.

MNI Coordinates

x y Stimulus: Emotional ­ Neutral, Auditory modality Right Fusiform Gyrus 37 42 -37 Right MTG 22 45 -40 Right STS 22 48 -46 Left ITG 20/21 -45 -37 Left ITG 20/21 -48 -49 Left supramarginal 40 -51 -43 gyrus Left MTG 39 -39 -49 Left MTG 39 -54 -64 Right thalamus 27 21 -28 Parahippocampal 28 15 -22 gyrus Right lingual 27 12 -34 gyrus Right Cerebellum 37 21 -58 Right Cerebellum 18 15 -76 Left Cerebellum 18 -6 -79 Left Midbrain 70 -3 -16 Right Medial n/a 12 -7 Wall Right Midbrain n/a 6 -13 Left IFG - Orbitalis 44/45 -51 23 Left Temporal 47 -54 17 Pole Left Cerebellum 37 -21 -64 Th - Parietal n/a -18 -25 Medial Temporal 27 -6 -34 Lobe Stimulus: Neutral - Emotional, Auditory Modality No significant clusters Stimulus: Emotional ­ Neutral, Visual modality Right IFG 44 39 11 Opercularis Right IFG 45/44 60 20 Opercularis Right IFG 45 57 26 Triangularis Right Midbrain n/a 6 -19
Region BA

z -17 -5 1 -20 -14 28 7 4 -2 -11 -2 -23 -20 -20 -14 -8 -11 -5 -11 -26 -5 -5

t-value

Cluster Size

pcorr value

4.75 4.72 4.16 4.52 3.25 4.34 4.3 4.24 4.1 3.85 3.47 4 3.7 3.38 3.91 3.45 3.43 3.58 3.45 3.49 3.47 3.13

180

0.076

26 155

0.73 0.108

104

0.228

216

0.047

31

0.682

33 26 45

0.664 0.73 0.558

34 16 -2 -11

5.2 4.22 4.16 4.92

165

0.0 51

59

0.3 83

117

Left Brainstem Left SN/VTA Left MTG Left MTG Left STG Left ITG Right MTG Left MTG Right MTG Middle Occipital Gyrus Right STG

n/a n/a 39 22 22 37 22 39 39 39 42

3 -3 -45 -54 -57 -42 54 -45 51 42 54

-25 -16 -52 -43 -46 -49 -37 -67 -58 -58 -37

-5 -11 13 7 16 -17 -5 10 13 7 19

4.23 3.47 4.66 4.1 3.64 4.39 4.28 3.68 3.54 3.4 3.44

140

0.0 8

13 32 19 30

0.8 67 0.6 42 0.7 97 0.6 65 0.8 21

17

Stimulus: Neutral - Emotional, Auditory Modality No significant clusters Stimulus: Emotional ­ Neutral, Auditory vs. Visual 37 42 Fusiform Gyrus 42 60 Right IPL, SMG Stimulus: Emotional ­ Neutral, Visual > Auditory

-40 -25

-14 19

4.33 3.6

23 32

0.7 64 0.6 74 29 0.7 03 0.3 66

4.28 Right IFG, opercularis 44 39 11 Stimulus Interaction: Happy > Angry, Auditory > Visual Left SPL 7 -21 -67 Left IPL Left IPL Right MFG Right Precentral Gyrus Left Cerebellum Left Lingual Gyrus Right Calcarine Gyrus Right IPL 7 7 6 6 18 18 17 7 -36 -27 39 45 -6 0 15 30 -49 -55 5 2 -70 -67 -73 -46 34 43 43 43 55 43 -5 7 7 37 4.79 4.4 4.32 4.33 3.49 4.31 4.2 3.87 4.09 44 161 72

38

0.6 16 0.0 95

0.5 63

118

Right Angular Gyrus Right IPL Left Midbrain Left ITG Left Precentral Gyrus Left Precentral Gyrus Right MTG Right MTG Right Medial Wall

7 7 n/a 37 6 4/6 22 39 n/a

30 30 -3 -45 -30 -30 63 63 15

-61 -55 -22 -37 -1 -7 -37 -49 -7

46 40 -17 -20 61 55 7 7 -8 1

3.97 3.94 3.83 3.45 3.31 3.26 3.27 3.01 3.17 2.96 16 10 0.8 28 0.8 86 11 11 13 0.8 77 0.8 77 0.8 58

Right thalamus n/a 15 -13 Stimulus Interaction: Angry > Happy, Visual > Audio No significant clusters Stimulus: Auditory, Happy > Angry Left IPL 41 -39 -37 Left supramarginal gyrus Lateral Fissure, left IPL Th - Parietal Left ITG Left Fusiform Gyrus Th - Prefrontal Th - Parietal 40 41 n/a 37 37 n/a n/a -48 -42 24 -42 -42 12 -24 -37 -28 -22 -40 -52 -4 -28

25 25 25 -5 -17 -14 -5 1

3.97 3.5 3.29 3.82 3.74 3.04 3.73 3.38

45

0.6 02

10 17 13 11

0.8 55 0.8 0.8 31 0.8 47

Stimulus: Auditory, Angry > Happy No significant clusters Stimulus: Visual, Happy > Angry No significant clusters Stimulus: Visual, Angry > Happy Left Lingual Gyrus 18 Right Calcarine 17 Gyrus Left Lingual 18 Gyrus

-15 18 -6

-82 -73 -73

-2 4 -5

4.73 4.7 4.15

476

0

119

Right Angular Gyrus

7

30

-61

43

4.53

62

0.3 52 0.7 98 0.7 02 0.5 28

Judgment: Emotional ­ Neutral, Visual modality Right Calcarine Gyrus 18 18 Judgment: Neutral - Emotion, Visual modality Left postcentral gyrus 1 -48 Left precentral gyrus Right lingual gyrus 6 18 -45 9

-79

16

3.63

19

-13 -1 -73

52 49 -11

4.36 3.54 3.79

25

38

Judgment: Emotional - Neutral, Auditory modality No significant clusters Judgment: Neutral - Emotion, Auditory modality Left IPL 2 -60

-34

43

4.47

18

0.8 09

Note. Areas of significant activation for each contrast calculated within the localiser mask. Corrected p values are reported following FWE correction procedures. Bolded regions represent clusters, with subclusters non-bolded. All contrasts were conducted at p < .005, uncorrected, with a 10-voxel extent threshold.

Discussion
In the current study, we examined the role of the MNS in the bottom-up and topdown perception of emotional movement, specifically hand movements, during auditory and visual perception. Several areas of the MNS responded more strongly when observing emotional compared to neutral hand actions, including bilateral IFG, in both the visual and auditory domains, suggesting a stimulus-driven impact of emotion on MNS function. Top-down or task-based judgments did not yield comparable MNS enhanced engagement for emotional compared to neutral conditions. Instead, there was greater activation in the left IPL (auditory) and pre- and post-central gyri (visual) during the neutral than the emotional task conditions. These results suggest that bottom-up emotionality facilitates processing in the MNS, but do not support the same possibility for top-down emotionality. Instead, behavioural data suggests that in the current

120

paradigm, emotional judgments may have inhibited processing efficiency, and/or that our other top-down task, hand judgments, may have facilitated MNS processing.

Behavioral Data
Effects of emotionality on accuracy and reaction time. Behavioral data suggested that participants had low accuracy overall, lower than accuracy levels found during piloting. This is probably because of the distracting nature of being part of an fMRI experiment, the speed with which the task had to be conducted during testing, and the randomization of correct answer buttons. However, the pattern of differences between conditions was still very revealing. Across judgment types, participants had higher accuracy for emotional than neutral stimuli. Emotional judgments were more difficult for participants overall than neutral judgments, as denoted in accuracy and reaction time scores. This may indicate that the emotional judgment condition actually impaired processing as compared to the hand judgment condition. When stimuli were separated by emotion type, it was found that accuracy across judgment types was greatest for positive stimuli, followed by negative stimuli, followed by neutral stimuli. This could be due to a greater level of internal simulation occurring during the presentation of emotional stimuli, regardless of judgment type, leading to a greater proficiency in identifying what type of movement was being made (either in terms of emotionality or number of hands). This theoretical supposition is supported by our brain data, which showed greater simulation for emotional than neutral stimuli in both modalities. Since MNS responsiveness has been shown to vary with accuracy on emotional judgment tasks, it makes sense that in conditions for which there was greater simulation across subjects (ie., the emotional stimulus conditions), there was also greater accuracy.

121

Imaging data
Bottom up emotionality influences activation in the MNS. Several areas of the MNS, including the bilateral IFG (orbitalis during auditory perception and opercularis during visual perception), bilateral PCG, and bilateral SMG were activated to a greater extent for emotional vs. neutral actions, in the auditory and/or visual modalities. Our findings bolster recent reports that the MNS may be involved to a greater extent during perception of emotional compared to neutral body actions. The present study extends prior work that has focused on emotional action perception in the visual domain (Ferri et al., 2013; Grosbras et al., 2006) by demonstrating greater activation during auditory perception of emotional vs. neutral hand actions as well. In an environment with competing sources for interaction, it would be adaptive to simulate those sources that are emotional more strongly than other sources. This is likely to be the case for both auditory and visual depictions of emotional movement, and the present study is the first to provide evidence supporting this notion. How might facilitated MNS activation to emotional stimuli occur? It has been demonstrated that emotional stimuli activate the limbic system, including the amygdala and insula (Adolphs et al., 1994; Anderson, Christoff, Panitz, De Rosa, & Gabrieli, 2003). The ventral anterior insula, dorsal anterior insula, and posterior insula each share functional ties to the MNS, including the IFG, PCG, and SMG (Deen, Pitskel & Pelphrey, 2011). It is presumed that the insula sends a signal to the MNS during emotional perception to enhance MNS activation towards emotional stimuli (Carr et al., 2003). At the same time, the MNS, which undertakes motor simulation of others' actions, may send information about the movement back to the insula through the same pathways, where it

122

is relayed to the appropriate emotional system to process the emotion conveyed in that movement (Carr et al., 2003). Future studies should investigate the ties between the insula and MNS during emotional action perception tasks, in order to examine the specific locations and temporal dynamics involved in this process. De Lange, Spronk, Willems, Toni & Bekkering (2008) have postulated that the mentalizing system operates in parallel with the MNS to understand more complex higher-level motivations behind others' actions. As Castelli et al. (2002) suggest, the temporal pole and/or medial prefrontal cortex (mPFC), parts of the mentalizing system, could drive activation in the STS during the perception of social information, leading to greater responses during social situations (Castelli et al., 2002; Press, Gillmeister, & Heyes, 2006). Similarly, as the STS provides input to inferior parietal and premotor components of the mirror system (Ungerleider and Haxby, 1994), it is plausible that observation of emotional actions first results in greater STS activity, which relays enhanced signals to the MNS, leading to enhanced activation there (Rizzolatti, 2005). Effects of emotionality on bottom-up action perception in the auditory and visual modalities. There were differences in activation of the MNS in the auditory vs. visual modality in the current study. Emotional sounds activated the SMG of the right IPL more strongly than visual depictions of emotion, while the pars opercularis of the right IFG responded more strongly to emotional movements presented in the visual modality compared to the auditory modality. Only the MTG and STG were activated during the perception of both auditory and visual emotional stimuli, suggesting that these areas are sensitive to the emotional content of movement in an amodal manner. Our within-modality findings suggest that the right IPL may be more responsive to auditory

123

depictions of emotion, whereas the right IFG may be more responsive to visual emotion. In the current study, the behavioral data indicate that performing the task in the auditory modality was more difficult than performing the task in the visual modality, as accuracy was greater and RTs faster for visual stimuli in general. Thus, it is possible that the greater SMG activity during auditory presentation of emotional stimuli in part reflects the increased difficulty to task demands in the auditory modality. It is of note that accuracy for emotional stimuli in the auditory modality was robust across judgment conditions and enhanced in comparison to auditory neutral accuracy. The SMG may have thus evoked greater automatic simulation and thus understanding in the perceiver towards emotional stimuli, regardless of judgment type. The parietal lobe is associated with the control and perception of object-oriented movements (Buccino et al., 2001), and contains visual as well as bimodal audio-visual neurons for vision and action (Gallese et al., 2002). Thus, it plausible that the parietal lobe would be engaged in mapping heard actions onto one's internal representation of what that action may be, in order to determine its emotionality or especially to determine the number of hands required to execute the movement. The neutral > emotional judgment contrast for auditory stimuli showed enhanced activation in the IPL, suggesting an enhanced role for this part of the parietal cortex when deciding how many hands manipulated the object in the heard action, regardless of whether the stimulus type was emotional or neutral. The pars opercularis of the IFG is thought to play a similar role to the SMG ­ supporting internal simulation of movements. It has similar functional roles to the precentral gyrus, prompting some to suggest that the two areas are part of one functional

124

gyrus (Campbell, 1905; Rizzolatti & Craighero, 2004). It has also been implicated in processing of movement intention (Iacoboni, 2001). The IFG has multiple functional ties to the insula (Deen et al., 2011), an area responsible for emotion responding. In the current study, the opercularis of the IFG favoured visual over auditory depictions of emotion. The left IFG pars orbitalis (BA45) showed greater activity when listening to emotional stimuli compared to neutral stimuli. The pars orbitalis has been implicated in cognitive appraisal (Wager et al., 2008) and short-term memory (STM; Owen et al., 1996), but there is reason to believe that in the current study, it was activated due to auditory mirroring properties. This echoes findings by Galati et al. (2008) who conducted a study examining the auditory mirror neuron system. They noted that activation in the auditory MNS spanned the left IFG's pars opercularis and pars orbitalis. Also, the pars orbitalis is an area that emerged from the localiser mask in the current study, so it must have been activated during movement execution in our participants. This supports the possibility that the pars orbitalis is directly sensitive to auditory depictions of movement, rather than its engagement being solely due to cognitive appraisal and STM tasks occurring during our experimental conditions (Owen et al., 1996; Wager et al., 2008). The conjunction analysis examining emotional > neutral stimuli across visual and auditory modalities showed that the STG and MTG were responsive to emotional stimuli in both the visual and auditory modalities. The STG has been implicated in social responding and the MTG in emotional memory, with the STS and STG being shown to possess audio-visual neurons, as well as unimodal neurons in each modality (Kohler et al., 2002; Study 1 of the current dissertation), that respond especially to social stimuli

125

(Watson, Latinus, Charest, Crabbe & Belin, 2014). Watson et al. (2014) showed that the STG and MTG contain face- and voice- selective regions, suggesting audio and visual responsivity to social information in each of these areas (Watson et al., 2014). The current stimuli were not facial or vocal, but they did depict body actions of another individual, which can also be interpreted as social. This could explain why there was enhanced activation in these areas towards emotional stimuli in the current study. While I did not find such top-down differences in the current study, in study 2 of the current dissertation I also found enhanced activation in the left STG towards emotional stimuli during an emotional vs. neutral task condition. This supports the role of the STG implicated in our current bottom-up findings, however, in that both support the role of the left STG in some level of emotional processing. Emotion Type x Modality Interaction. There was also an interesting finding concerning the role of the MNS in processing happy vs. angry actions in the auditory vs. visual domains. It was found that the bilateral IPL and bilateral PCG are more responsive towards happy actions in the auditory domain, and that there is less of a distinction in the visual domain. Interestingly, behavioral data reflected these activation differences, with greater accuracy towards happy stimuli in the auditory domain, and greater accuracy towards angry stimuli in the visual domain. The auditory happy > angry distinction occurred specifically in the bilateral IPL and bilateral PCG. These findings are similar to those by Warren et al. (2006), who found greater engagement in the auditory domain towards highly arousing, positively-valenced vocal emotion compared to other emotions and baseline. It is not clear why the auditory modality would favour positive vs. negative depictions of emotion. In the current study, it could be that the positive auditory stimuli

126

were more interesting ­ a tambourine being hit non-rhythmically but excitedly may have been more salient than a bell being rung impatiently, for instance. We did equate stimuli for duration, intensity, attack, arousal, and accuracy before using these stimuli, but perhaps a low-level difference persisted, which could have resulted in the auditory modality favouring positive stimuli. Furthermore, while positive stimuli in the auditory domain resulted in greater neural recruitment, there may have also been visual attributes of the stimuli that lead them to be preferentially processed in the visual domain, at least in the left lingual gyrus. The left lingual gyrus was activated more strongly towards visual depictions of negative emotions, suggesting that this area favours negative emotions in the visual modality. The lingual gyrus is considered to be involved in attributions of intentions (Brunet, Sarfati, Hardy-Bayle, & Decety, 2000), and is also thought to have functional ties to the amygdala (Catani, Jones, Donato, & Ffytche, 2003), showing enhanced responding to emotional vs. neutral stimuli (Kehoe, Toomey, Balsters & Bokde, 2012). The amygdala has been shown to exhibit enhanced activation especially towards negative emotions (Hamann, Ely, Hoffman & Kilts, 2002; Ledoux 2003). Thus, the amygdala could have influenced responsiveness in the lingual gyrus towards negative emotion in the current study. Effects of emotionality on top-down action perception in auditory and visual modalities. While many significant differences in brain activation for emotional vs. neutral stimuli were present, comparisons between tasks (making emotional compared to neutral judgments) did not yield any activation in the MNS. It would be adaptive to be able to enhance or suppress one's own MNS responsiveness depending on the situation, for example, if one determines in a top-down manner that a particular emotional stimulus

127

is not a priority for them. We did not find evidence for this within the regions that formed our MNS localiser. Interestingly, the bilateral IPL responded more strongly for auditory neutral compared to emotional judgments, as did the bilateral pre- and post- central gyri for visual neutral judgments, indicating that our neutral judgment condition actually yielded greater responsivity in certain areas of the MNS than the emotional judgment condition. Regardless, emotional stimuli drove accuracy differences within each judgment condition, as accuracy seemed to be driven more by bottom-up differences between stimuli than by the actual judgment question. These imaging data indicate that our comparison question (hand judgment) actually stimulated certain areas of the MNS more than the emotion judgment question. A future study should consider comparing the experimental condition to a no-question condition, or to a less movement-oriented question, in order to see whether emotional judgments enhance MNS responsivity in this context. It is possible that the emotional judgment condition actually served to impair processing in the MNS, as reflected in the behavioural data, for which accuracy is lower and reaction time slower for emotional judgments than neutral judgments. Accuracy was robust when stimuli were emotional, but decreased during emotional judgments. Perhaps thinking about stimuli in an emotional manner served to distract participants instead of supporting simulation, accuracy and reaction time. Meanwhile, it appears that participants could simulate and use simulation more effectively for hand judgments than for emotional judgments in the current study. These results have methodological implications for the study of the MNS. Our results suggest that emotional responsivity occurs automatically in this system, and thus

128

task instructions seem to have less of an impact on MNS activation. In order to test this further, a future study could provide participants with instructions to simply observe an emotional stimulus, and compare this to conditions in which the participant is instructed to make an emotional judgment, to confirm whether making a judgment at all makes a difference in MNS processing. It is possible that since participants were always making some sort of judgment, this resulted in a heightened level of attention to the stimulus that yielded high activation to emotional movements based on bottom-up responding, even in the neutral judgment condition. One additional possibility is that action simulation processes were already at ceiling during emotional stimulus conditions, and thus not susceptible to influence from top-down instructions. In order to address this question, we compared judgment conditions within the emotional and neutral stimulus categories separately. There was no enhanced activation in any brain region during emotional processing of neutral stimuli, a stimulus type where emotion-based enhancement would presumably not be at ceiling, indicating that our lack of top-down findings were not because of a ceiling effect. It is also possible that since emotional and neutral stimuli were presented as a mixed set within judgment block, rather than separately blocked, bottom-up emotional processing carried over in time across all stimuli, leading to enhanced responding even in neutral conditions that was resistant to any impact of top-down task instruction. However, since the bottom-up data indicate greater activity in the bilateral IFG and SMG when viewing emotional vs. neutral stimuli in the auditory and/or visual domains, the greater responsiveness to emotional stimuli did not bleed over entirely. It is possible, however, that less overtly emotional stimuli may be more susceptible to top-down influence. In

129

study 2 of the current dissertation, it was found that activity within the left STG was enhanced towards emotional vs. neutral judgments of human sung melodic intervals. These stimuli were chosen because they do have emotional meaning but are not as overtly emotional as, for instance, crumpling paper angrily. Thus, their emotional intent may not grab attention as readily as the emotional stimuli in the current study. A future study should try to manipulate top-down judgments of less saliently emotional stimuli in order to see whether this modulates activation in the MNS. The right calcarine gyrus was activated for emotional vs. neutral task conditions during visual presentations of stimuli. Meanwhile, the left lingual gyrus, which had been activated to a greater extent overall towards negatively emotional stimuli, was preferentially activated during the neutral than the emotional judgment conditions. The calcarine gyrus is a component of the early visual system, and there is evidence to suggest that emotional processing can lead to changes in even these early visual perceptual routes, enhancing activation in such areas (Sabatinelli, Lang, Bradley, Costa, & Keil, 2009). The decrease in the left lingual gyrus during emotional judgments is more puzzling; perhaps there was a decrease in overall simulation during emotional than neutral judgments (as indicated by our MNS data), leading to more activation in the lingual gyrus during neutral judgment processing, while its overall stimulus responsivity remained highest during the presentation of emotional versus neutral stimuli (see discussion of bottom-up findings, above).

Conclusions
The current study found enhanced activation in the MNS towards emotional stimuli, presented either in the auditory or visual modality. Auditory depictions of

130

emotion lead to greater activation specifically in the IPL, whereas visual emotion leads to greater activation in the opercularis of the IFG. Top-down judgments of emotionality did not enhance activation in the MNS, suggesting that activation in the classical MNS to emotional action occurs automatically and is relatively unaffected by top-down direction. Future studies should compare top-down instructions to a passive, no-instruction condition to see if this finding persists.

131

Chapter V: General Discussion

133

The present dissertation examined the role of the mirror neuron system (MNS) in the processing of action, as perceived in a bottom-up and top-down manner, through the audio and visual modalities, in response to emotional and neutral movements performed by humans. Together, the three studies presented in this dissertation support the premise that the classical mirror neuron system is primarily automatic, but more research is needed to determine whether top-down manipulations can uniquely engage the MNS. Study 1 showed that the MNS is affected by bottom-up manipulations of modality. Study 2 demonstrated that the left STG, part of the extended MNS, is affected by top-down manipulations of emotionality, but there were no areas in the MNS itself that met our statistical threshold to be affected by top-down forces. In study 3, it was found that the classical MNS was strongly affected by bottom-up differences in emotionality and modality. In addition, the hand judgment task enhanced activation in the IPL during auditory perception of actions, suggesting some malleability of the IPL when it comes to top-down instructions. Thus, study 1 and study 3 suggest that while the MNS is affected by actual differences between stimuli, such as sensory modality or emotionality, study 2 provides support for the involvement of the STG (an area adjascent to the MNS) and study 3 provides support for involvement of the IPL in top-down study manipulations. Study 1 confirmed the propensity of the MNS to be affected by bottom-up differences in stimuli; in this case, the modality in which stimuli were presented. Mu ERD was desynchronized during perception of movement presented through both auditory and visual modalities as compared to a non-biological control condition, as well as to a greater extent when the movement was presented multi-modally. These findings bolster single-cell MNS research in monkeys in the homologue of the vPMC, area F5,

134

which showed individual neurons that respond to audio-only, visual-only, and audiovisual presentations of movement (Keysers et al., 2003). It also replicated an fMRI study by Kaplan & Iacoboni (2007), showing greater responsivity to audio-visual perception of movement in the vPMC in humans, and in the process, supported the use of EEG measurement of the mu rhythm accompanied by source localization as a time-sensitive and spatially-sensitive measure of MNS activity. Study 2 examined the role of the MNS in the top-down perception of movement by manipulating task instruction and keeping stimuli identical across conditions. While differences within mirroring areas (source-localized to the right PCG) failed to reach significance, a cluster in the left STG showed that when melodic stimuli are perceived in an emotional as opposed to a neutral manner, mu ERD is greater in the left STG. In addition, there was a trend towards greater mu ERD during the pitch distance judgment manipulation in the right PCG, suggesting that focusing on any movement-related aspects of a biological stimulus, as opposed to a non-movement-related judgment (e.g., hair style) facilitates activation in the MNS. This was the first study to show enhanced activation in the extended MNS towards top-down manipulations of emotionality during perception of human song. Study 3 compared bottom-up and top-down activation in the MNS by manipulating judgment type (emotional or neutral) as well as stimulus type (emotional or neutral, auditory or visual), using an fMRI paradigm. It was found that bottom-up differences in emotionality drove MNS responsivity in addition to behavioural measures of speed and accuracy, in the auditory as well as visual domains. This was the first study to find enhanced activation in the MNS during auditory perception of human body

135

movements. For the most part, top-down differences in emotionality did not enhance MNS responsivity, and accuracy and speed were lower overall during emotional judgment conditions. Unexpectedly, our neutral hand judgment task elicited greater activation in the IPL than the emotional judgment task. This finding supports the idea that activation in the MNS can be affected by top down forces, and also suggests that emotionality affects the MNS differently in a bottom up vs. top-down manner ­ leading to enhanced activation, accuracy, and speed when emotionality is perceived bottom-up, but a decrease in each of these dependent variables during top-down emotion judgments.

Top-down and bottom-up findings across studies
Below is a review of top-down and bottom-up findings that were prevalent across studies, and their implications for study of the MNS. Upon reflection of the studies conducted in the current dissertation, I would like to emphasize that in the current context, bottom-up refers to stimulus-driven effects on MNS activation. I concede that it is entirely possible that cognitive functions overseeing traditionally-defined top-down processes in the brain such as attention could have been automatically recruited during stimulus-driven processing in the MNS, and could have affected MNS activation in a traditionally-defined "top-down" manner during these manipulations. Thus it is important to clarify that in the current line of research, "top-down" refers to task-driven effects, rather than to recruitment of top-down processes within the brain, which are difficult to experimentally control.

136

Bottom-up findings across studies: Implications and future directions
Across studies 1 and 3, bottom-up differences between stimuli strongly effected responsivity in the MNS. In study 1, multimodal perception of action lead to greater mu ERD in a left central cluster than unimodal perception, replicating prior work in fMRI (Kaplan & Iacoboni, 2007), and perception of visual action also lead to greater levels of activation than auditory action. In study 3, visual perception of emotion was more pronounced in the IFG of the MNS while auditory perception of emotion was still present in the MNS but more pronounced in the IPL. Across both modalities, emotional stimuli were processed more strongly than neutral stimuli in several MNS areas, including the bilateral IFG, IPL, and PCG. One reason that emotional stimuli are thought to grab automatic attention more effectively than neutral stimuli is that they are semantically related ­ each of the emotional stimuli belongs to an "emotional" category, whereas there often is not a similar type of categorization available for neutral stimulus sets (Talmi, Luk, McGarry & Moscovitch, 2007). In study 3, our neutral stimuli were also semantically related ­ hand movements that each involved manipulation of office supplies. This suggests that it was probably not the semantic relatedness of emotional stimuli in study 3 that primarily affected emotional responsiveness. Another property of emotional stimuli that increases responsivity towards those stimuli is distinctiveness, or saliency (Talmi et al., 2007). In the current study, it is possible that the emotional stimuli were more salient than the neutral stimuli. However,

137

there were also differences in responsivity and accuracy in each modality towards each emotion type, and emotion types were equated at the study outset for saliency and emotional arousal. This suggests that it is not simply saliency or arousal that leads to enhanced responding to certain emotional stimuli in the MNS. However, while we tried to equate all low-level features between conditions as much as possible, there are certain low-level features that are necessary for a stimulus to be considered emotional. Since neutral and emotional stimuli were not equated for arousal or for all low-level dimensions, these small differences could have allowed for a special quality that movements took on when they were emotional. The emotionality of these movements likely stimulated the insula, as an emotional relay station (Anderson, Christoff, Panitz, De Rosa, & Gabrieli, 2003), to respond and to send signals to other brain areas including the MNS, leading to enhanced levels of responsivity compared to neutral emotions (Carr et al., 2003). A future study should compare different types of MNS-eliciting stimuli and examine their effects on MNS activation in different conditions. For example, song and speech should be compared to see whether their effects on MNS responding when emotional, or when presented through the auditory or visual modality, are similar. A potential study paradigm could involve presenting sung and spoken sentences, with neutral semantic meaning but an emotional tune in the song condition, and emotional prosody in the speech condition. All stimuli could be presented as audio-alone, visualalone, and audio-visual, in order to investigate the effects of unimodal vs. multimodal processing on MNS responsivity to emotion in song as well as in speech. Participants could be asked to judge the emotionality of the stimuli out of six possible emotions (happy, sad, angry, fearful, calm, neutral), or to identify a semantic category or meaning

138

of the sung or spoken sentence, out of six possible sentences. This would allow for both a bottom-up measurement of emotionality of sung and spoken information on MNS responsivity, as well as a top-down measurement of emotion processing, in this type of stimuli. Sung and spoken stimuli are intransitive actions, and thus would not be expected to stimulate the IPL (Buccino et al., 2001), but I would hypothesize that emotionality of sung and spoken stimuli would stimulate other areas of the MNS such as the right IFG, as well as the STG, replicating findings concerning the right STG in study 2. In addition, speech primarily stimulates activation in Broca's area in the left hemisphere while song stimulates the right hemisphere more strongly (Zattore, Belin & Penhune, 2002). This difference may mean that connections to the MNS differ between song and speech, and may effect MNS activation differently. Since song is known to involve emotional movement and be created on the basis of communicating emotion, it makes sense that there would be a good degree of neural simulation of emotion during song listening (Molnar-Szakacs & Overy, 2006). Many theorists have argued that song and speech are likely to have evolved from similar origins (Patel, 2010; Mithen, Morley, Wray, Tallerman & Gamble, 2006), and both express emotion through melodic/prosodic sound manipulations (Shon, Magne, & Besson, 2004; Ilie & Thompson, 2006). However, it has also been hypothesized that song, which may enhance neural entrainment and synchronization due to an external rhythm and prescribed melodic intonation, may enhance MNS activation the most (eg., Molnar-Szakacs & Overy, 2006; Overy & Molnar-Szakacs, 2009; McGarry & Russo, 2011). Thus, I would predict that song would enhance MNS activation more than speech. I would also hypothesize that when sung and spoken stimuli are perceived multi-modally, this will enhance responsiveness to these

139

stimuli overall (as in study 1 of the current dissertation towards hand actions, and as in the study by Thompson, Russo, & Quinto, 2008 in which emotion was found to be perceived to a greater extent multi-modally than unimodally). The audio-visual presentation of stimuli in study 2 of the current dissertation is likely to have enhanced mu ERD power across conditions, leading to a greater propensity to detect differences between judgment types in the current study, as compared to a potentially weaker signal if stimuli were presented unimodally. The proposed study above would help to confirm whether this is the case. In addition, vocal depictions of auditory emotion should be compared directly with bodily depictions of auditory emotion in a future study to bolster the comparative results currently found between study 3 and the study by Warren et al. (2006), showing enhancement in several areas of the MNS towards positive arousing vocalizations. Stimuli could consist of a performer either making an emotional hand action, or making a vocalization. All actions should be intransitive (not operating on an object) in order to equate object-orientedness across vocal and non-vocal conditions, since vocal stimuli are by nature intransitive. I would predict enhanced activation in the IFG towards auditory depictions of movement presented vocally and non-vocally, implicating this area in the perception of emotional-movement-oriented sound in general.

Top-down findings across studies: Implications and future directions
In study 2, there was a strong enhancement in the left STG during top-down emotional perception of stimuli, as well as a marginal enhancement in the right PCG. Meanwhile, study 3 did not find enhanced activation towards emotion judgments in extended mirroring areas. One potential reason for this could be that the stimuli used in

140

study 3 may have been more overtly emotional and thus engaged emotion more automatically than in study 2, leading to more of a ceiling effect in automatic emotion processes, and thus less of a capacity to affect MNS responding in a top-down manner. In study 2, emotional stimuli consisted of sung melodic intervals produced in isolation and without context. These stimuli were not strongly emotionally arousing, and possibly required some additional cognitive processing to discern their emotionality. Meanwhile, the stimuli used in study 3 were object-oriented, emotional hand grasping movements. Prior research supports the premise that hand grasping stimuli are highly likely to engage the classical MNS (eg., Buccino et al., 2001; Iacoboni et al., 2005; Grosbras et al., 2006). In addition, the presence of objects meant that more activation was likely to be elicited in the IPL, which is involved in processing object-oriented movement (Buccino et al., 2001), meaning we were able to also see whether the IPL is reactive to emotionality in movements. However, in study 3, even when we examined top-down responsiveness to only neutral stimuli, differences between judgment conditions remained similar, suggesting that top-down emotionality was not being affected by judgment type, and that the MNS is primarily governed by automatic processes, at least when it comes to processing emotion. It is important to note that Study 2 did not find significance in its right PCG cluster, so the results of studies 2 and 3 are not incompatible. In studies 2 and 3, there was always a judgment being made, whether an experimental or control judgment. In study 1, no judgment was used, making it a truly bottom-up study, since there were no top-down forces being manipulated. It is possible that the type of judgment made in studies 2 and 3 always affected MNS responsivity in some manner. Thus, a future study should compare passive viewing of stimuli to

141

conditions in which an emotional judgment is made, in order to see whether the MNS is more responsive during any judgment type, perhaps due to a greater amount of attention paid during the judgment condition. Iacoboni et al. (2005) tried this during an experiment in which they were measuring the responsivity of the MNS during judgments of movement intention, and they found that whether or not a judgment was made, the MNS responded equivalently, supporting the possibility that MNS functionality is automatic and relatively unaffected by top-down manipulations. An additional study should be conducted to see whether this is the case when stimuli are emotional. It has been shown that emotion effects processing in other brain areas in a relatively stimulus-driven manner (Ledoux, 1995; Anderson et al., 2003). Based on this finding, as well as my findings in study 3 wherein top-down manipualations did not enhance MNS responsivity, I would hypothesize is that the MNS would still respond equivalently to emotional vs. neutral stimuli, regardless of whether viewing was active or passive. An additional future study should be conducted comparing emotional judgments to neutral judgments in which the focus on body parts and movement are equated. In study 3, there was actually more activation in the IPL, and thus more simulation, in the non-emotion-oriented condition, and perhaps this was provoked by the body-oriented question posed in our control judgment, wherein participants were asked to focus on the number of hands required to manipulate the object. This is evidence of top-down processing brought about by the non-emotional judgment condition, or else of suppression of processing during the emotional judgment condition. A future study should equate the focus on body parts for emotional and non-emotional judgments in order to definitively determine the effects of an emotional judgment on processing in the

142

MNS. Perhaps this could be done by mentioning hand actions in the question in each judgment (i.e., "Was that hand movement emotional?", or "Was that one hand or two hands?"). Attention could be assessed during the judgment conditions using a dual task paradigm in order to see whether more attention is paid during one condition than the other, by measuring reaction time to a concurrent tone-detection task, for example, in order to examine the role of attention in any differences between conditions. Another future study should examine the suppression of responsivity to emotional movement in a top-down manner ­ perhaps this could affect emotional responsivity in the MNS. In order to do this, one could examine how emotion regulation strategies affect simulation of emotional actions. Two common emotion regulation strategies involve either ignoring an emotionally arousing stimulus ("suppress"), or else attending to the emotionally arousing stimulus but engaging in a reappraisal, such that its intensity is decreased to the perceiver ("reappraise"; Gross & John, 2003). It has been found that while suppression actually leads to greater activity in the insula and amygdala, reappraisal results in earlier responsivity in the PFC, leading to less activity in the insula and amygdala (Goldin, McRae, Ramel, & Gross, 2008). It is possible that "suppress" and "reappraise" strategies would have a similar effect on our neural simulation systems as they do on the insula and amygdala, wherein we feel the need to simulate more if we have tried to suppress an emotion instead of attending to it and processing it consciously. Conversely, conscious attempts to "suppress" might inhibit MNS responsiveness, similar to the later pattern of responsivity in the PFC observed under these conditions, contributing to a less-thoroughly processed emotion. It would also be interesting to examine the functional ties between the MNS and areas involved in emotion regulation

143

during this process, to support the idea that there is cross-talk between the MNS and insula/amygdala during emotion regulation. In addition, it is possible that up-regulating emotional responsiveness to a moving emotional stimulus would enhance MNS responsivity as well, by enhancing emotional responsivity overall, and thus enhancing activation in the limbic system which may feed back to affect MNS responding.

The temporal gyrus, movement simulation, and emotion
Across all studies, various areas of the temporal lobes, including the STS, STG, and MTG, played a role in audio and visual perception of action, as well as in emotional perception of action. While the STS and STG are not thought to possess mirror neurons per se, they are involved during both the perception and production of movement. Iacoboni et al. (2001) found that regions of the STS as well as the STG were responsive during the observation and execution of movement, and the STG has also been found to play a role during listening and executing sound (Gazzola et al., 2006). The STG has even been identified in a meta-analysis as one key area of the human brain that exhibits mirroring properties, in terms of exhibiting enhanced activation during both the perception and execution of action (Molenberghs, Cunnington, & Mattingley, 2012). However, there is reason to believe that there are not actual mirror neurons in these areas ­ instead, the temporal gyrus is responsible for creating a reefference copy of movements that are generated by a person, as well as for perceiving movements of another person (Iacoboni et al., 2001). These signals are relayed between the TG and the MNS including the IFG and IPL. The current series of studies sheds further light on the way in which the STG processes action, in each modality and in terms of emotionality.

144

Watson et al. (2014) found that the right STS and bilateral STG also play a special role in social perception. Participants were shown auditory and visual depictions of action that either included faces and voices or did not. It was found that several areas of the right STS and the bilateral STG were more responsive to faces and voices than to other types of movement stimuli. Thus, these areas are not simply involved in encoding action and relaying it onwards to the MNS for further processing, but also process stimuli on their own. The left STG integrates audio and visual information before it is sent onward to the MNS (Study 1), and also responds to socially relevant stimuli to a greater extent than neutral stimuli. This suggests that greater responsivity in the MNS in study 3 could have been partially due to enhanced stimulation from the STG. Similarly, in study 1, the STG probably played a role in combining audio and visual stimuli before sending that information onwards to the MNS. In study 2, the audio-visual information would have been processed in the STG before being sent onwards to influence the marginallyenhanced responding in the right PCG.

Modality and Emotion
In study 3, it was found that the SMG of the IPL is more responsive to emotion depicted in the auditory modality, and that the opercularis of the IFG was more responsive to emotion depicted in the visual modality. A future study should examine this further in order to see whether this finding extends to other stimulus types. Previous studies finding a special role of BA44/the opercularis of the IFG in perception of emotion in movement (Carr et al., 2003; Wicker et al., 2003; Ferri et al., 2014) have all used visual stimuli, so it is possible that this is only the case in the visual domain. The IPL is primarily responsive to object-oriented movements (Buccino et al., 2001), so it would be

145

interesting to compare auditory depictions of movement that are object oriented or are non-object-oriented, to see whether the IPL still responds in a greater manner towards emotional, non-object-oriented, auditory depictions of movement. It is possible that in this case, it would be primarily the orbitalis of the IFG that would respond to auditory emotional movement, as was found in the current study, as well as by Galati et al. (2008).

Conclusions
In sum, the current dissertation suggests that the MNS is primarily governed by bottom-up processes during perception of human movement, as depicted in the auditory, visual, and audio-visual modalities, under emotional and neutral task conditions. Future studies should continue to examine the role of the MNS in processing emotion, and how this role may change across modalities. Future studies should also examine other ways in which the MNS may actually be affected by top-down strategies, such as by comparing judgment conditions to passive viewing of emotional stimuli, and by employing emotion regulation strategies.

146

Appendix A. Whole-brain fMRI analyses
Whole-brain analyses were conducted at an uncorrected p-value of .005 and an extent threshold of 10 (see Table 1). Areas are summarized below if they are considered part of the classical MNS. Stimulus: Emotional vs. Neutral Across modalities, significant areas for the emotion > neutral contrast included the right IFG (triangularis) and left IFG (opercularis, triangularis, and orbitalis), the right PCG, and the left IPL. In the auditory domain, there was greater activation in the left orbitalis of the left IFG. In the visual domain, there was greater activation in the right IFG opercularis and triangularis, the left IFG triangularis, the left and right IPL, and the left PCG.
Figure 1.

Figure 1. Emotional > Neutral Stimuli, across modalities. Areas activated to a greater extent for emotional than neutral stimuli at p < .005 and a k = 10 extent threshold are depicted in purple.

147

Stimulus: Emotion x Modality Similarly to localizer-constrained results, the contrast of (auditory emotional > auditory neutral) > (visual emotional > visual neutral) was significant in the right supramarginal gyrus, while the inverse interaction was significant in the right IFG (opercularis). Stimulus: Emotion-specific findings There was greater activation in the MNS in the auditory domain for happy emotional hand movements. For the contrast of (auditory happy > auditory angry) > (visual happy > visual angry), the left and right SPL (BA7), and the right MFG (BA44) were significant. For the inverse interaction contrast, (visual angry > visual happy) > (auditory angry > auditory happy), no areas within the classical MS were activated. See Table 1 for the complete list of regions activated throughout the brain for these contrasts, as well as follow-up simple-effects analyses. Judgment: Emotional vs. Neutral A random-effects analysis at p < .005 uncorrected revealed that for the emotion > neutral judgments contrast, across modalities, there was greater activation in the left IFG (orbitalis and triangularis). For the opposite contrast (neutral > emotional), there was greater activation in the supramarginal gyrus of the left IPL towards neutral hand judgments. In the auditory domain, for emotional > neutral judgments, there was enhanced activation in the left IFG (orbitalis). For the same contrast in the visual domain, there was enhanced activation in the left IFG (opercularis) and in the left PCG.

148

Figure 2.

Figure 2. Emotional > Neutral Judgments, across modalities. Areas activated to a greater extent for emotional than neutral judgments at p < .005 and a k = 10 extent threshold are depicted in purple. Table 1. Whole Brain Contrasts.

MNI Coordinates

x y Stimulus: Emotional ­ Neutral, Across modalities
Region BA

z

t-value

Cluster Size

pcorr value

Amygdala R SN/VTA Right MTG R Fusiform Gyrus RCcerebellum Cerebellar Vermis L ITG L STG L MTG

39 3 51 42 33 6 -45 -45 -48

-1 -13 -37 -40 -61 -73 -49 -40 -55

-23 -14 -8 -26 -29 -23 -20 16 10

6.83 5.59 5.32 5.2 4.84 3.93 5.06 4.81 4.55

1718

0

270

0.012

997

0

149

R IFG (Triangularis) R IFG (Triangularis) R STG L Cuneus R Precuneus R Precuneus L Superior medial gyrus L SMA L IFG (Triangularis) L IFG (Opercularis) L IFG (Orbitalis) R MFG R MFG R PCG R Superior Medial Gyrus R Calcarine Gyrus L Calcarine Gyrus L MFG L Cerebellum R MTG L MTG L IPL L Angular Gyrus

45 45

57 54 51 -6 3

26 35 -40 -67 -73 -55 32 23 26 14 26 17 2 2 56 -85 -91 23 -58 -58 41 -64 -61

-2 7 22 25 40 28 49 55 1 4 -5 40 37 46 22 10 13 40 -32 7 19 40 49

4.68 2.95 4.55 4.1 3.83 2.99 4.07 3.72 3.93 3.4 3.24 3.77 3.75 3.18 3.53 3.42 3 3.36 3.35 3.32 3.26 3.2 2.94

61

0.665

56 81

0.718 0.469

7

6 -6 -9

48

0.801

45 44 45

-57 -45 -42 42 33

84

0.443

38 24

0.893 0.976

6

39 9

14 22

0.997 0.982

18 18 44

6 -6 -45 -33 51 -48

19 22 12 10 14

0.99 0.982 0.998 0.999 0.997

7

-30 -42

150

R IFG (Triangularis)

45

51

29

25

3.05

11

0.999

Stimulus: Neutral - Emotional, Across modalities No significant clusters Stimulus: Emotional ­ Neutral, Auditory modality Amygdala R fusiform Gyrus R Parahippocampal Gyrus L IFG (Orbitalis) L ITG L ITG R MFG R Caudate R Caudate R Central Sulcus L Insula Lobe L Cerebellum L Cerebellum L Cerebellum
46 41 45 39 42 36 -42 -39 -45 27 27 21 27 -30 -21 -27 -33 -4 -34 -13 26 -19 -37 32 -16 -13 -19 14 -64 -49 -52 -23 -17 -29 -5 -20 -20 16 22 31 34 16 -26 -23 -35 6.54 5.07 5.05 5.32 4.98 4.52 4.03 4.02 3.3 3.12 3.79 3.49 3.2 3 17 56 0.992 0.766 25 44 0.972 0.865 1403 0 1745 0

Stimulus: Emotional ­ Neutral, Visual modality R IFG (Opercularis) R IFG (Triangularis) R MFG R SN/VTA
44 45 45 36 48 45 6 11 29 32 -19 34 28 19 -11 5.31 4.67 4.39 4.92 86 0.442 475 0.001

151

R Brainstem Midbrain L MTG L MTG L STG L ITG L Pallidum L Putamen R MTG R MTG L MFG L MFG L IFG (Triangularis) L MTG L Angular Gyrus L IPL L IPL R Putamen R IPL R MTG R MTG R STG R Thalamus
37 2 45 45

3 -3 -45 -54 -57 -42 -21 -24 54 54 -36 -42 -48 -45 -45 -48 -30 33 54 51 42 54 12

-25 -16 -52 -43 -46 -49 2 14 -37 -19 53 32 44 -67 -61 -52 -64 8 -37 -58 -58 -37 -7

-5 -11 13 7 16 -17 -2 -5 -5 -8 4 34 -2 10 37 49 40 -8 46 13 7 19 -2

4.23 3.47 4.66 4.1 3.64 4.39 4.35 3.06 4.28 3.9 3.7 3.69 3.53 3.68 3.66 2.92 2.92 3.58 3.57 3.54 3.4 3.44 3.39 18 10 0.991 0.999 16 11 41 0.994 0.999 0.872 21 50 0.985 0.789 132 0.182 91 0.403 14 57 0.996 0.719 159 0.107

152

R Hippocampus L PCG R Caudate R Caudate R Caudate
44

21 -42 15 15 9

-10 5 14 14 8

-14 34 7 -2 -2

3.34 3.25 3.18 3.05 2.99

12 13 14

0.998 0.997 0.996

Stimulus: Emotional ­ Neutral, Auditory vs. Visual
R MTG 37 42 60 -40 -25 -14 19 4.33 3.6 23 32 0.764 0.674

R Supramarginal Gyrus

Stimulus: Emotional ­ Neutral, Visual > Auditory R IFG (Opercularis) R PCG R Superior frontal gyrus R SFG R MFG
44 44 36 45 30 24 45 11 8 56 59 38 34 31 13 19 22 4.88 3.65 3.82 3.42 3.03 78 0.59 61 0.733

Stimulus Interaction: Happy > Angry, Auditory > Visual L SPL L SPL L SPL R SPL R SPL R Precuneus L Pallidum L Putamen
7 7 7 7 7 7 -15 -15 -24 18 24 15 -15 -27 -58 -70 -61 -58 -43 -55 -1 -7 49 46 43 46 46 55 -5 -8 6.8 5.37 4.42 5.86 5.3 5.26 4.51 3.46 69 0.633 943 0 812 0

153

L MTG L MTG L MTG L Insula L Caudate L Insula L Insula Lobe L Superior Orbital Gyrus R MFG R SFG R MFG L Cerebellum L Lingual Gyrus R Calcarine Gyrus R SN R SN Cerebellar Vermis Cerebellar Vermis R STG Hippocampus R Parahippocampal Gyrus R MFG R MFG

22

-45 -63

-22 -16 -37 8 2 20 14 17 5 2 14 -70 -67 -73 -16 -19 -43 -52 -22 -31 -25 26 8

-8 -11 -2 22 22 19 -17 -17 55 61 55 -5 7 7 -26 -20 -20 -26 -2 1 -14 34 22

4.51 3.8 3.55 4.42 4.4 3.73 4.36 3.76 4.33 4.3 4 4.31 4.2 3.87 4.31 4.22 4.25 3.28 4.18 4.01 4.01 3.99 3.73

157

0.14

21 13

-42 -27 -21

129

0.23

48

-27 -24 -15 39 30 30

46

0.843

112

0.311

17 17 17

-6 0 15 6 0 6 3 57

161

0.13

61

0.707

31

0.946

183

0.089

20

36 30 51

32 17

0.941 0.992

44

30

154

R MTG R MTG L MFG L MFG L PCG R Superior medial gyrus R ITG R ITG L Thalamus L Auditory Cortex L Insula L ITG L MTG R MTG R MTG R Amygdala R Thalamus R Thalamus L Precuneus L Middle Cingulate Cortex R Temporal Pole R Medial Temporal Pole

39 39 6 6 6 9

39 33 -30 -18 -30 9 45 42 -15

-67 -55 2 -1 -7 29 -1 -4 -28 -31 -31 -1 -52 -37 -49 -7 -22 -13 -40 -43 17 11

13 13 61 46 55 55 -44 -32 4 13 4 -26 4 7 7 -8 1 1 58 52 -26 -26

3.68 3.24 3.53 3.48 3.26 3.45 3.44 3.37 3.41 3.25 3.23 3.37 3.31 3.27 3.01 3.17 3.1 2.96 3.13 3.12 3.1 2.91

30

0.951

64

0.679

13 16

0.997 0.994

42

0.875

42 42 38

-30 -33 -42 -63 63 63 15 12 15

13 11 16

0.997 0.998 0.994

30

0.951

4 5

0 -6 45 51

13

0.997

10

0.999

Stimulus Interaction: Angry > Happy, Visual > Audio

155

R Mid Orbital Gyrus Stimulus: Auditory, Happy > Angry L Insula L Insula R IPL R Precuneus L Insula Lobe R Insula L Fusiform Gyrus L Fusiform Gyrus R IFG R Putamen L Thalamus L Th-Temporal Primary Auditory Cortex
42 22 20 20 44 48 44 7/40 31

15

41

-5

4.29

25

0.972

-27 -21 27 18 -30 33 -36 -39 30 24 -3 -6 -36

11 2 -40 -37 11 -28 -34 -25 8 -13 -16 -25 -37

19 22 37 46 -14 1 -2 -8 22 16 16 16 25

5.34 5.21 4.86 4.47 4.81 4.76 4.7 4.39 4.62 4.33 4.23 4.13 3.97

75

0.094

56

0.173

25 26 36

0.489 0.473 0.338

15 19 13

0.673 0.594 0.715

11

0.757

Stimulus: Auditory, Angry > Happy No significant clusters Stimulus: Visual, Happy > Angry R Anterior Cingulate Cortex Stimulus: Visual, Angry > Happy R IPL L Lingual Gyrus L Lingual Gyrus
18 7 27 -15 -6 -58 -82 -73 43 -2 4 4.78 4.73 4.15 49 75 0.023 0.007 12 41 1 4.23 11 0.775

156

R Calcarine Gyrus R Lingual Gyrus R Lingual Gyrus L SPL L MTG

17

18 15 18

-73 -70 -58 -58 -19

4 -8 -11 55 -14

4.7 4.09 3.76 4.42 3.95

70

0.009

7

-21 -51

23 10

0.103 0.272

Judgment: Emotional ­ Neutral, Across modalities L IFG (Orbitalis) L IFG (Orbitalis) L IFG (Triangularis) L IFG (Triangularis) R STG L Anterior Cingulate Cortex
45 45 45 -48 -39 -54 -57 48 -3 29 38 35 20 -31 50 -8 -11 1 13 -8 13 4.63 3.61 3.3 4.09 3.66 3.28 24 10 12 0.971 1 0.999 92 0.241

Judgment: Neutral - Emotional, Across modalities L IPL L Supramarginal Gyrus L Cingulate Gyrus
24 -60 -51 -6 -34 -34 -13 43 34 28 5.17 4.06 4.5 14 0.664 26 0.331

Judgment: Emotional ­ Neutral, Visual modality L Superior Medial Gyrus L IFG (Opercularis) R Calcarine Gyrus L PCG
44 17 4 -12 -60 18 -24 56 17 -79 -28 13 16 16 55 4.05 3.96 3.63 3.43 21 21 19 14 0.984 0.984 0.99 0.997

Judgment: Emotional - Neutral, Auditory modality L IFG (Orbitalis)
45 -45 38 -8 3.68 31 0.399

157

L IFG (Orbitalis) L IFG (Orbitalis)

45

-45 -36

29 35

-11 -8

3.25 2.9

Note. Corrected p values are reported following FWE correction procedures. Bolded regions represent clusters, with sub-clusters non-bolded. All contrasts were conducted at p < .005, uncorrected, with a 10 voxel extent threshold.

Discussion
Overall, MNS results from the whole brain analysis are similar to those in the localiser-restricted analysis, implicating the bilateral IFG, IPL, and PCG in the bottom-up perception of emotional vs. neutral stimuli (see Study 3 for more details on localiserbased analyses). Interestingly, whole brain analyses revealed several regions considered part of the classical MNS (Rizzolatti & Craighero, 2004) to be activated during top-down emotional vs. neutral judgments, a finding that was not evident when analyses were restricted to the MNS localiser task in study 3. Regions significantly activated for emotional > neutral judgments included the left orbitalis and triangularis of the IFG. In the visual domain, emotional > neutral areas included the left IFG opercularis and the left PCG. In the auditory domain, this included the left IFG orbitalis. Top-down findings did not persist once the localizer mask was applied. This could mean that other areas close to the MNS, without specific mirroring properties that reached group-level significance in our participants, were involved in the top-down perception of action in an emotional vs. neutral manner. The differential findings regarding MNS activation during top-down judgments during whole-brain vs. maskbased analysis could also help explain why top-down judgments have been shown to influence MNS activity in prior studies, where the MNS is defined using an entire brain region (eg., the IFG opercularis) rather than being able to focus on the part of that region

158

that actually possesses mirroring properties in a particular sample. Our localizer mask provides extra assurance that included brain areas are actually involved in mirroring in our specific sample, and thus the whole brain data should be interpreted more cautiously. The left IFG orbitalis that was more responsive to emotional than neutral hand actions in the top-down manipulations, implicating the orbitalis in emotion judgments, rather than in automatic responsiveness to emotionality. The orbitalis has been implicated in emotional appraisal as well as the perception of emotion in the context of language (Wager et al., 2008), so this could be the reason it was activated during judgments of emotion. It has also been implicated in STM, specifically in cases where the participant holds two stimuli in working memory in order to make a decision about such stimuli (Owen et al. 1996). However, since the orbitalis did reach significance as part of the MNS in our localizer task, and has been implicated in prior research as coding auditory depictions of movement (Galati et al., 2008), it is also possible that the orbitalis plays a legitimate role in enhanced neural simulation based on top-down influences in emotionality in the current study, one that was not powerful enough to reach significance for top-down judgments within the localizer-based regions. The left IFG opercularis, part of Broca's area, is involved in language comprehension (Hinke et al., 1993), which could explain its activation in the current study, as making a judgment of emotionality is likely to involve some degree of linguistic processing. In addition, Molnar-Szakacs, Iacoboni, Koski & Mazziotta (2005) have shown that there is a functional segregation within the opercularis, with certain areas responding during observation and execution of action (such as those contained in the study 3 localiser) whereas other areas are only involved in action observation. This means

159

that activation could have occurred in this region for reasons not related to action observation-execution matching, explaining why it was shown to be activated during visual emotion > neutral judgments in the whole brain analysis but was not included in the MNS localizer for judgment contrasts (only for visual emotional > stimulus processing). However, it is probable that making a hand judgment also involves some degree of linguistic processing, and the hand judgment condition yielded a lower degree of processing for this contrast. It could be the case that the hand judgment condition is more easily completed using only simulation mechanisms, since a portion of the IPL was activated to a greater extent for neutral hand judgments in study 3. The left lateralization of findings in whole brain analyses of top-down judgments stands in contrast to the largely bilateral and right-sided locations of significance for bottom-up differences between stimuli. Emotionality is often found to evoke stronger levels of processing on the right side of the brain (Schwartz, Davidson, & Maer, 1975; Davidson, 1998; Davidson et al., 1999). This left lateralization could indicate that while the bottom-up manipulation elicits an emotional reaction more typical of automatic emotion processing, the top-down manipulation elicits brain activation involved in semantic processing, which is predominantly processed in the left side of the brain (Vigneau et al., 2006). A left lateralization for emotion judgments was also found in study 2 of the current dissertation, wherein emotion judgments elicited enhanced activation in the left STG. In conclusion, while the localizer-restricted analyses presented in study 3 provided valuable insights into the role of the MNS in processing emotion, the wholebrain analysis allowed for examination of regions that were not included in the MNS

160

localiser. These regions are less likely to have mirroring properties, as they were not significantly involved in execution + observation matching across our sample. Meanwhile, bottom-up processing involved a series of areas in the MNS that did reach significance within the localizer as well, bolstering the claim that these areas are part of the MNS in our sample.

161

Appendix B. Time-series Discussion: Study 2 data
In Study 2 of the current dissertation, I was interested in comparing mu ERD during different tasks related to the perception of human song. Because human song is a dynamic stimulus that changes across time, it is relevant to consider whether the mu response changes across time in relation to changes in the sung stimuli. Some specific questions are whether the mu response could change immediately, during perception of the sung notes, and whether mu suppression continues in between notes, or whether it is only suppressed during the sound itself. It is also possible that an expectancy effect could occur, in which greater mu ERD could occur in between notes while the participant awaits the second note. In Study 2, event-related spectral perturbations were used to examine differences in mu ERD between conditions. Because of the nature of ERSP analyses, in which differences in mu amplitude are visualized across time and frequency, they allow for a visual depiction of the exact time in which differences between conditions occur. Observations of the exact time in which mu amplitude varied depending on condition are discussed in this section. Time-series data was examined for each comparison made within Study 2 of the current dissertation, in the 8-13 hz range (see Study 2 for a detailed description and analysis of this data). Areas that were significantly different between conditions were examined in terms of time in addition to the presence or absence of significant differences between conditions. For the left STG cluster, the difference between experimental and control conditions was consistent across the entire time series. Thus, there were no differences across time with respect to the effects of the emotion judgment on mu suppression in this

163

cluster. For the right PCG cluster, an area of the brain that encompasses part of the motor cortex, there were no significant differences found between conditions. Thus the time course of significant differences could not be examined. However, the marginal degree of mu suppression that occurred for each condition was explored. The condition that exhibited the most obvious changes across time was the hair judgment condition, for which desynchronization appeared greatest after the first note (at 500-800 ms) and again after the second note (1200-1500 ms). The observation that only the hair condition showed obvious time-related differences suggests that these differences were driven by the stimulus rather than the task (as the task in this condition is not considered to evoke mu suppression). The other two tasks are likely to have increased mu suppression across the entire time window, since the viewer was oriented towards the emotion or the intention in each of these tasks, tasks which are thought to enhance MNS activity. In the non-movement-oriented hair judgment task, any mu suppression can be attributed to the stimulus alone. Thus, the stimulus-related time course is perhaps best examined in this condition. The time at which desynchronization occurred maps onto the approximate time at which the musical notes occurred in the 4 different interval stimuli, with desynchronization beginning immediately after note onset. Thus, in a future study, one might predict that in absence of a task, mu desynchronization occurs immediately after note onset, and lasts the duration of each note. It should be noted that there were 2 actors performing 4 different intervals and these data are collapsed together, therefore the varying timing across the different stimuli is not parsed apart in these data, making it difficult to precisely match time series data to stimuli. However, it appears that it was during each note that mu was desynchronized

164

most. A future study should equate and measure the time series across conditions of musical intervals to see whether the second burst of mu ERD occurs directly before, during, or after onset of the second note, to determine the role of mu ERD in expectancy versus responsivity to sung intervals.

165

Permissions

166

8/7/2014

Rightslink Printable License

SPRINGER  LICENSE TERMS  AND  CONDITIONS
Aug  07,  2014

This is a License Agreement between Lucy M McGarry ("You") and Springer ("Springer") provided by Copyright Clearance Center ("CCC"). The license consists of your order details, the terms and conditions provided by Springer, and the payment terms and conditions.

All  payments  must  be  made  in  full  to  CCC.  For  payment  instructions,  please  see information  listed  at  the  bottom  of  this  form. License  Number License  date Licensed  content  publisher 3443690507482 Aug  07,  2014 Springer

Licensed  content  publication Experimental  Brain  Research Licensed  content  title Licensed  content  author Licensed  content  date Volume  number Issue  number Type  of  Use Portion Number  of  copies Audio-visual  facilitation  of  the  mu  rhythm Lucy  M.  McGarry Jan  1,  2012 218 4 Thesis/Dissertation Full  text 5

Author  of  this  Springer  article Yes  and  you  are  the  sole  author  of  the  new  work Order  reference  number Title  of  your  thesis  / dissertation Expected  completion  date Estimated  size(pages) Total Terms  and  Conditions None The  role  of  the  mirror  neuron  system  in  bottom-up  and  top-down perception Oct  2014 150 0.00  CAD

Introduction The publisher for this copyrighted material is Springer Science + Business Media. By clicking "accept" in connection with completing this licensing transaction, you agree that the following terms and conditions apply to this transaction (along with the Billing and Payment terms and conditions established by Copyright Clearance Center, Inc. ("CCC"), at the time that you opened your Rightslink account and that are available at any time at http://myaccount.copyright.com). Limited License
https://s100.copyright.com/AppDispatchServlet

167

1/4

8/7/2014

Rightslink Printable License

With reference to your request to reprint in your thesis material on which Springer Science and Business Media control the copyright, permission is granted, free of charge, for the use indicated in your enquiry. Licenses are for one-time use only with a maximum distribution equal to the number that you identified in the licensing process. This License includes use in an electronic form, provided its password protected or on the university's intranet or repository, including UMI (according to the definition at the Sherpa website: http://www.sherpa.ac.uk/romeo/). For any other electronic use, please contact Springer at (permissions.dordrecht@springer.com or permissions.heidelberg@springer.com). The material can only be used for the purpose of defending your thesis limited to universityuse only. If the thesis is going to be published, permission needs to be re-obtained (selecting "book/textbook" as the type of use). Although Springer holds copyright to the material and is entitled to negotiate on rights, this license is only valid, subject to a courtesy information to the author (address is given with the article/chapter) and provided it concerns original material which does not carry references to other sources (if material in question appears with credit to another source, authorization from that source is required as well). Permission free of charge on this occasion does not prejudice any rights we might have to charge for reproduction of our copyrighted material in the future. Altering/Modifying Material: Not Permitted You may not alter or modify the material in any manner. Abbreviations, additions, deletions and/or any other alterations shall be made only with prior written authorization of the author(s) and/or Springer Science + Business Media. (Please contact Springer at (permissions.dordrecht@springer.com or permissions.heidelberg@springer.com) Reservation of Rights Springer Science + Business Media reserves all rights not specifically granted in the combination of (i) the license details provided by you and accepted in the course of this licensing transaction, (ii) these terms and conditions and (iii) CCC's Billing and Payment terms and conditions. Copyright Notice:Disclaimer You must include the following copyright and permission notice in connection with any reproduction of the licensed material: "Springer and the original publisher /journal title, volume, year of publication, page, chapter/article title, name(s) of author(s), figure number(s), original copyright notice) is given to the publication in which the material was originally published, by adding; with kind permission from Springer Science and Business Media" Warranties: None Example 1: Springer Science + Business Media makes no representations or warranties with respect to the licensed material. Example 2: Springer Science + Business Media makes no representations or warranties with respect to the licensed material and adopts on its own behalf the limitations and disclaimers established by CCC on its behalf in its Billing and Payment terms and conditions for this
https://s100.copyright.com/AppDispatchServlet

168

2/4

8/7/2014

Rightslink Printable License

licensing transaction. Indemnity You hereby indemnify and agree to hold harmless Springer Science + Business Media and CCC, and their respective officers, directors, employees and agents, from and against any and all claims arising out of your use of the licensed material other than as specifically authorized pursuant to this license. No Transfer of License This license is personal to you and may not be sublicensed, assigned, or transferred by you to any other person without Springer Science + Business Media's written permission. No Amendment Except in Writing This license may not be amended except in a writing signed by both parties (or, in the case of Springer Science + Business Media, by CCC on Springer Science + Business Media's behalf). Objection to Contrary Terms Springer Science + Business Media hereby objects to any terms contained in any purchase order, acknowledgment, check endorsement or other writing prepared by you, which terms are inconsistent with these terms and conditions or CCC's Billing and Payment terms and conditions. These terms and conditions, together with CCC's Billing and Payment terms and conditions (which are incorporated herein), comprise the entire agreement between you and Springer Science + Business Media (and CCC) concerning this licensing transaction. In the event of any conflict between your obligations established by these terms and conditions and those established by CCC's Billing and Payment terms and conditions, these terms and conditions shall control. Jurisdiction All disputes that may arise in connection with this present License, or the breach thereof, shall be settled exclusively by arbitration, to be held in The Netherlands, in accordance with Dutch law, and to be conducted under the Rules of the 'Netherlands Arbitrage Instituut' (Netherlands Institute of Arbitration).OR: All disputes that may arise in connection with this present License, or the breach thereof, shall be settled exclusively by arbitration, to be held in the Federal Republic of Germany, in accordance with German law. Other terms and conditions: v1.3
You  will  be  invoiced  within  48  hours  of  this  transaction  date.  You  may  pay  your  invoice by  credit  card  upon  receipt  of  the  invoice  for  this  transaction.  Please  follow  instructions provided  at  that  time.   To  pay  for  this  transaction  now;  please  remit  a  copy  of  this  document  along  with  your payment.  Payment  should  be  in  the  form  of  a  check  or  money  order  referencing  your account  number  and  this  invoice  number  RLNK501371992. Make  payments  to  "COPYRIGHT  CLEARANCE  CENTER"  and  send  to:   Copyright  Clearance  Center Dept  001 P.O.  Box  843006 Boston,  MA  02284-3006 Please  disregard  electronic  and  mailed  copies  if  you  remit  payment  in  advance.
https://s100.copyright.com/AppDispatchServlet

169

3/4

8/7/2014

Rightslink Printable License

Questions?  customercare@copyright.com  or  +1-855-239-3415  (toll  free  in  the  US)  or +1-978-646-2777. Gratis  licenses  (referencing  $0  in  the  Total  field)  are  free.  Please  retain  this  printable license  for  your  reference.  No  payment  is  required.

https://s100.copyright.com/AppDispatchServlet

170

4/4

8/7/2014

Rightslink Printable License

SPRINGER  LICENSE TERMS  AND  CONDITIONS
Aug  07,  2014

This is a License Agreement between Lucy M McGarry ("You") and Springer ("Springer") provided by Copyright Clearance Center ("CCC"). The license consists of your order details, the terms and conditions provided by Springer, and the payment terms and conditions.

All  payments  must  be  made  in  full  to  CCC.  For  payment  instructions,  please  see information  listed  at  the  bottom  of  this  form. License  Number License  date Licensed  content  publisher 3443690901473 Aug  07,  2014 Springer

Licensed  content  publication Cognitive,  Affective,  &  Behavioral  Neuroscience Licensed  content  title Licensed  content  author Licensed  content  date Type  of  Use Portion Number  of  copies The  role  of  the  extended  MNS  in  emotional  and  nonemotional judgments  of  human  song Lucy  M.  McGarry Jan  1,  2014 Thesis/Dissertation Full  text 5

Author  of  this  Springer  article Yes  and  you  are  the  sole  author  of  the  new  work Order  reference  number Title  of  your  thesis  / dissertation Expected  completion  date Estimated  size(pages) Total Terms  and  Conditions None The  role  of  the  mirror  neuron  system  in  bottom-up  and  top-down perception Oct  2014 150 0.00  USD

Introduction The publisher for this copyrighted material is Springer Science + Business Media. By clicking "accept" in connection with completing this licensing transaction, you agree that the following terms and conditions apply to this transaction (along with the Billing and Payment terms and conditions established by Copyright Clearance Center, Inc. ("CCC"), at the time that you opened your Rightslink account and that are available at any time at http://myaccount.copyright.com). Limited License With reference to your request to reprint in your thesis material on which Springer Science and Business Media control the copyright, permission is granted, free of charge, for the use
https://s100.copyright.com/AppDispatchServlet

171

1/4

8/7/2014

Rightslink Printable License

indicated in your enquiry. Licenses are for one-time use only with a maximum distribution equal to the number that you identified in the licensing process. This License includes use in an electronic form, provided its password protected or on the university's intranet or repository, including UMI (according to the definition at the Sherpa website: http://www.sherpa.ac.uk/romeo/). For any other electronic use, please contact Springer at (permissions.dordrecht@springer.com or permissions.heidelberg@springer.com). The material can only be used for the purpose of defending your thesis limited to universityuse only. If the thesis is going to be published, permission needs to be re-obtained (selecting "book/textbook" as the type of use). Although Springer holds copyright to the material and is entitled to negotiate on rights, this license is only valid, subject to a courtesy information to the author (address is given with the article/chapter) and provided it concerns original material which does not carry references to other sources (if material in question appears with credit to another source, authorization from that source is required as well). Permission free of charge on this occasion does not prejudice any rights we might have to charge for reproduction of our copyrighted material in the future. Altering/Modifying Material: Not Permitted You may not alter or modify the material in any manner. Abbreviations, additions, deletions and/or any other alterations shall be made only with prior written authorization of the author(s) and/or Springer Science + Business Media. (Please contact Springer at (permissions.dordrecht@springer.com or permissions.heidelberg@springer.com) Reservation of Rights Springer Science + Business Media reserves all rights not specifically granted in the combination of (i) the license details provided by you and accepted in the course of this licensing transaction, (ii) these terms and conditions and (iii) CCC's Billing and Payment terms and conditions. Copyright Notice:Disclaimer You must include the following copyright and permission notice in connection with any reproduction of the licensed material: "Springer and the original publisher /journal title, volume, year of publication, page, chapter/article title, name(s) of author(s), figure number(s), original copyright notice) is given to the publication in which the material was originally published, by adding; with kind permission from Springer Science and Business Media" Warranties: None Example 1: Springer Science + Business Media makes no representations or warranties with respect to the licensed material. Example 2: Springer Science + Business Media makes no representations or warranties with respect to the licensed material and adopts on its own behalf the limitations and disclaimers established by CCC on its behalf in its Billing and Payment terms and conditions for this licensing transaction.
https://s100.copyright.com/AppDispatchServlet

172

2/4

8/7/2014

Rightslink Printable License

Indemnity You hereby indemnify and agree to hold harmless Springer Science + Business Media and CCC, and their respective officers, directors, employees and agents, from and against any and all claims arising out of your use of the licensed material other than as specifically authorized pursuant to this license. No Transfer of License This license is personal to you and may not be sublicensed, assigned, or transferred by you to any other person without Springer Science + Business Media's written permission. No Amendment Except in Writing This license may not be amended except in a writing signed by both parties (or, in the case of Springer Science + Business Media, by CCC on Springer Science + Business Media's behalf). Objection to Contrary Terms Springer Science + Business Media hereby objects to any terms contained in any purchase order, acknowledgment, check endorsement or other writing prepared by you, which terms are inconsistent with these terms and conditions or CCC's Billing and Payment terms and conditions. These terms and conditions, together with CCC's Billing and Payment terms and conditions (which are incorporated herein), comprise the entire agreement between you and Springer Science + Business Media (and CCC) concerning this licensing transaction. In the event of any conflict between your obligations established by these terms and conditions and those established by CCC's Billing and Payment terms and conditions, these terms and conditions shall control. Jurisdiction All disputes that may arise in connection with this present License, or the breach thereof, shall be settled exclusively by arbitration, to be held in The Netherlands, in accordance with Dutch law, and to be conducted under the Rules of the 'Netherlands Arbitrage Instituut' (Netherlands Institute of Arbitration).OR: All disputes that may arise in connection with this present License, or the breach thereof, shall be settled exclusively by arbitration, to be held in the Federal Republic of Germany, in accordance with German law. Other terms and conditions: v1.3
You  will  be  invoiced  within  48  hours  of  this  transaction  date.  You  may  pay  your  invoice by  credit  card  upon  receipt  of  the  invoice  for  this  transaction.  Please  follow  instructions provided  at  that  time.   To  pay  for  this  transaction  now;  please  remit  a  copy  of  this  document  along  with  your payment.  Payment  should  be  in  the  form  of  a  check  or  money  order  referencing  your account  number  and  this  invoice  number  RLNK501371999. Make  payments  to  "COPYRIGHT  CLEARANCE  CENTER"  and  send  to:   Copyright  Clearance  Center Dept  001 P.O.  Box  843006 Boston,  MA  02284-3006 Please  disregard  electronic  and  mailed  copies  if  you  remit  payment  in  advance. Questions?  customercare@copyright.com  or  +1-855-239-3415  (toll  free  in  the  US)  or +1-978-646-2777.
https://s100.copyright.com/AppDispatchServlet

173

3/4

8/7/2014

Rightslink Printable License

Gratis  licenses  (referencing  $0  in  the  Total  field)  are  free.  Please  retain  this  printable license  for  your  reference.  No  payment  is  required.

https://s100.copyright.com/AppDispatchServlet

174

4/4

Ryerson(University( Yeates(School(of(Graduate(Studies( 350(Victoria(St( Toronto,(ON( M5B2K3( ( ( July(28th,(2014.( ( ( To(Whom(It(May(Concern:(( ( This(letter(gives(my(permission(for(Lucy(McGarry(to(include(the(following(work(in( her(dissertation,(for(which(I(am(a(coRauthor:( (
McGarry, L. M., Pineda, J. A., & Russo, F. A. (2014). The role of the extended MNS in emotional and nonemotional judgments of human song. Cognitive, Affective, & Behavioral Neuroscience, 1-13.

(

McGarry, L. M., Russo, F. A., Schalles, M. D., & Pineda, J. A. (2012). Audio-visual facilitation of the mu rhythm. Experimental brain research, 218(4), 527-538.

( ( Sincerely,(

( ( Frank(Russo,(Ph.D.( Associate(Professor,(Psychology( Ryerson(University( (

175

Ryerson University Yeates School of Graduate Studies 350 Victoria St Toronto, ON M5B2K3 August 8th, 2014. To Whom It May Concern: This letter gives my permission for Lucy McGarry to include the following work in her dissertation, for which I am a co-author:
McGarry, L. M., Russo, F. A., Schalles, M. D., & Pineda, J. A. (2012). Audio-visual facilitation of the mu rhythm. Experimental brain research, 218(4), 527-538.

Sincerely,

Matt Schalles PhD Candidate, Cognitive Science University of California, San Diego

176

July 28th, 2014

Ryerson University Yeates School of Graduate Studies 350 Victoria St Toronto, ON M5B2K3 To Whom It May Concern: This letter gives my permission for Lucy McGarry to include the following work in her dissertation, for which I am a co-author: McGarry, L. M., Pineda, J. A., & Russo, F. A. (2014). The role of the extended MNS in emotional and nonemotional judgments of human song. Cognitive, Affective, & Behavioral Neuroscience, 1-13. McGarry, L. M., Russo, F. A., Schalles, M. D., & Pineda, J. A. (2012). Audio-visual facilitation of the mu rhythm. Experimental brain research, 218(4), 527-538. Sincerely,

Professor of Cognitive Science University of California, San Diego La Jolla, CA 92093

177

References
Adolphs, R., Tranel, D., Damasio, H., & Damasio, A. (1994). Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala. Nature, 372, 669­672. doi:10.1038/372669a0 Akalin Acar, Z., & Makeig, S. (2013). Effects of forward model errors on EEG source localization. Brain Topography, 26(3), 378­96. doi:10.1007/s10548-012-0274-6 Alais, D., & Burr, D. (2004). Ventriloquist effect results from near-optimal bimodal integration. Current Biology, 14, 257­262. doi:10.1016/S0960-9822(04)00043-0 Allison, T., Puce, A., & McCarthy, G. (2000). Social perception from visual cues: role of the STS region. Trends in Cognitive Sciences, 4(7), 267­278. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/10859571 Anderson, A. K., Christoff, K., Panitz, D., De Rosa, E., & Gabrieli, J. D. E. (2003). Neural correlates of the automatic processing of threat facial signals. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 23(13), 5627­33. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/12843265 Aragón, O. R., Sharer, E. a, Bargh, J. a, & Pineda, J. a. (2013). Modulations of mirroring activity by desire for social connection and relevance of movement. Social Cognitive and Affective Neuroscience. doi:10.1093/scan/nst172 Barnes, G., Hillebrand, A., & Hirata, M. (2009). Magnetoencephalogram. Scholarpedia. Barraclough, N. E., Xiao, D., Baker, C. I., Oram, M. W., & Perrett, D. I. (2005). Integration of visual and auditory information by superior temporal sulcus neurons responsive to the sight of actions. Journal of Cognitive Neuroscience, 17, 377­391. doi:10.1162/0898929053279586 Bastiaansen, J. a C. J., Thioux, M., & Keysers, C. (2009). Evidence for mirror systems in emotions. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, 364(1528), 2391­404. doi:10.1098/rstb.2009.0058

178

Belouchrani, A., Abed-Meraim, K., Cardoso, J. F., & Moulines, E. (1997). A blind source separation technique using second-order statistics. IEEE Transactions on Signal Processing, 45, 434­444. doi:10.1109/78.554307 Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 57, 289 ­ 300. doi:10.2307/2346101 Brunet, E., Sarfati, Y., Hardy-Baylé, M. C., & Decety, J. (2000). A PET investigation of the attribution of intentions with a nonverbal task. NeuroImage, 11(2), 157­66. doi:10.1006/nimg.1999.0525 Buccino, G., Binkofski, F., Fink, G. R., Fadiga, L., Fogassi, L., Gallese, V., ... Freund, H. J. (2001). Action observation activates premotor and parietal areas in a somatotopic manner: An fMRI study. European Journal of Neuroscience, 13, 400­404. doi:10.1046/j.1460-9568.2001.01385.x Buccino, G., Riggio, L., Melli, G., Binkofski, F., Gallese, V., & Rizzolatti, G. (2005). Listening to actionrelated sentences modulates the activity of the motor system: a combined TMS and behavioral study. Brain Research. Cognitive Brain Research, 24(3), 355­63. doi:10.1016/j.cogbrainres.2005.02.020 Calvert, G. A., Campbell, R., & Brammer, M. J. (2000). Evidence from functional magnetic resonance imaging of crossmodal binding in the human heteromodal cortex. Current Biology!: CB, 10, 649­657. doi:S0960-9822(00)00513-3 [pii] Calvo-Merino, B., Glaser, D. E., Grèzes, J., Passingham, R. E., & Haggard, P. (2005). Action observation and acquired motor skills: an FMRI study with expert dancers. Cerebral Cortex (New York, N.Y.!: 1991), 15(8), 1243­9. doi:10.1093/cercor/bhi007 Carr, L., Iacoboni, M., Dubeau, M.-C., Mazziotta, J. C., & Lenzi, G. L. (2003). Neural mechanisms of empathy in humans: a relay from neural systems for imitation to limbic areas. Proceedings of the National Academy of Sciences of the United States of America, 100(9), 5497­502. doi:10.1073/pnas.0935845100 Casasanto, D., & Dijkstra, K. (2010). Motor action and emotional memory. Cognition, 115(1), 179­185. Castelli, F., Frith, C., Happé, F., & Frith, U. (2002). Autism, Asperger syndrome and brain mechanisms for the attribution of mental states to animated shapes. Brain: A Journal of Neurology, 125(Pt 8), 1839­ 49. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/12135974

179

Catani, M., Jones, D. K., Donato, R., & Ffytche, D. H. (2003). Occipito-temporal connections in the human brain. Brain: A Journal of Neurology, 126(Pt 9), 2093­107. doi:10.1093/brain/awg203 Cavanagh, P., Labianca, a T., & Thornton, I. M. (2001). Attention-based visual routines: sprites. Cognition, 80(1-2), 47­60. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/11245839 Chan, L., Livingstone, S., & Russo, F. A. (2013). Automatic facial mimicry of emotion during perception of song. Music Perception, 30, 361­367. Chartrand, T., & Bargh, J. (1999). The chameleon effect: The perception-behavior link and social interaction. Journal of Personality and Social Psychology, 76(6), 893­910. Retrieved from http://psycnet.apa.org/journals/psp/76/6/893/ Cheng, Y., Chou, K.-H., Decety, J., Chen, I.-Y., Hung, D., Tzeng, O. J.-L., & Lin, C.-P. (2009). Sex differences in the neuroanatomy of human mirror-neuron system: a voxel-based morphometric investigation. Neuroscience, 158(2), 713­20. doi:10.1016/j.neuroscience.2008.10.026 Cochin, S. (1999). Observation and execution of movement: Similarities demonstrated by quantified electroencephalography. European Journal of Neuroscience, 11, 1839­1842. doi:10.1046/j.14609568.1999.00598.x Cochin, S., Barthelemy, C., Lejeune, B., Roux, S., & Martineau, J. (1998). Perception of motion and qEEG activity in human adults. Electroencephalography and Clinical Neurophysiology, 107(4), 287­295. Cohen-Seat, G., Gastaut, H., Faure, J., Heuyer, G. (1954). Etudes expérimentales de l'activité nerveuse pendant la projection cinématographique. Rev. Int. Filmologie, 5, 7­64. Cross, E. S., Hamilton, A. F. D. C., & Grafton, S. T. (2006). Building a motor simulation de novo: observation of dance by dancers. NeuroImage, 31(3), 1257­67. doi:10.1016/j.neuroimage.2006.01.033 Cross, E. S., Liepelt, R., Hamilton, A. F. D. C., Parkinson, J., Ramsey, R., Stadler, W., & Prinz, W. (2012). Robotic movement preferentially engages the action observation network. Human Brain Mapping, 33(9), 2238­54. doi:10.1002/hbm.21361 Dapretto, M., Davies, M. S., Pfeifer, J. H., Scott, A. a, Sigman, M., Bookheimer, S. Y., & Iacoboni, M. (2005). Understanding emotions in others: mirror neuron dysfunction in children with autism spectrum disorders. Nature Neuroscience, 9(1), 28­30. doi:10.1038/nn1611

180

Davidson, R. J. (1998). Anterior electrophysiological asymmetries, emotion, and depression: Conceptual and methodological conundrums. Psychophysiology, 35(5), 607­614. Davidson, R. J. (1998). Anterior electrophysiological asymmetries, emotion, and depression: Conceptual and methodological conundrums. Psychophysiology, 35(5), 607­614. Davidson, R. J. (1999). The functional neuroanatomy of emotion and affective style. Trends in Cognitive Sciences, 3(1), 11­21. De Lange, F. P., Spronk, M., Willems, R. M., Toni, I., & Bekkering, H. (2008). Complementary systems for understanding action intentions. Current Biology!: CB, 18(6), 454­7. doi:10.1016/j.cub.2008.02.057 Deen, B., Pitskel, N. B., & Pelphrey, K. a. (2011). Three systems of insular functional connectivity identified with cluster analysis. Cerebral Cortex (New York, N.Y.!: 1991), 21(7), 1498­506. doi:10.1093/cercor/bhq186 Delorme, A., & Makeig, S. (2004). EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. Journal of Neuroscience Methods, 134(1), 9­ 21. Delorme, A., Sejnowski, T., & Makeig, S. (2007). Enhanced detection of artifacts in EEG data using higher-order statistics and independent component analysis. NeuroImage, 34, 1443­1449. doi:10.1016/j.neuroimage.2006.11.004 Devlin, H. (2013). What is functional magnetic resonance imaging (fMRI)? Psych Central. Retrieved from http://psychcentral.com/lib/what-is-functional-magnetic-resonance-imaging-fmri/0001056 Di Pellegrino, G., Fadiga, L., Fogassi, L., Gallese, V., & Rizzolatti, G. (1992). Experimental Brain Research 9. Experimental Brain Research, 91(1), 176­180. Di Pellegrino, G., Fadiga, L., Fogassi, L., Gallese, V., & Rizzolatti, G. (1992). Understanding motor events: a neurophysiological study. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 91, 176­180. doi:10.1007/BF00230027 Dimberg, U., Thunberg, M., & Elmehed, K. (2000). Unconscious facial reactions to emotional facial expressions. Psychological Science, 11(1), 86­89. doi:10.1111/1467-9280.00221

181

Dinstein, I., Thomas, C., Behrmann, M., & Heeger, D. J. (2008). A mirror up to nature. Current Biology, 18(1), R13­R18. Driver, J. (1996). Enhancement of selective listening by illusory mislocation of speech sounds due to lipreading. Nature, 381, 66­68. doi:10.1038/381066a0 Dubose, C. N., Cardello, A. V., & Maller, O. (1980). Effects of colorants and flavorants on identification, perceived flavor intensity, and hedonic quality of fruit-flavored beverages and cake. Journal of Food Science, 45, 1393­1399. doi:10.1111/j.1365-2621.1980.tb06562.x Engel, A., Burke, M., Fiehler, K., Bien, S., & Rosler, F. (2008). How moving objects become animated: the human mirror neuron system assimilates non-biological movement patterns. Social Neuroscience, 3(3-4), 368­87. doi:10.1080/17470910701612793 Enticott, P. G., Johnston, P. J., Herring, S. E., Hoy, K. E., & Fitzgerald, P. B. (2008). Mirror neuron activation is associated with facial emotion processing. Neuropsychologia, 46(11), 2851­4. doi:10.1016/j.neuropsychologia.2008.04.022 Fadiga, L., Craighero, L., Buccino, G., & Rizzolatti, G. (2002). Speech listening specifically modulates the excitability of tongue muscles: A TMS study. European Journal of Neuroscience, 15, 399­402. doi:10.1046/j.0953-816x.2001.01874.x Fadiga, L., Craighero, L., & Olivier, E. (2005). Human motor cortex excitability during the perception of others' action. Curr Opin Neurobiol, 15, 213­218. doi:10.1016/j.conb.2005.03.013 Fadiga, L., Fogassi, L., Pavesi, G., & Rizzolatti, G. (1995). Motor facilitation during action observation: a magnetic stimulation study. Journal of Neurophysiology, 73, 2608­2611. Fecteau, S., Carmant, L., Tremblay, C., Robert, M., Bouthillier, A., & Theoret, H. (2004). A motor resonance mechanism in children? Evidence from subdural electrodes in a 36-month-old child. Neuroreport, 15(17), 2625-2627. Fecteau, S., Pascual-Leone, A., & Théoret, H. (2008). Psychopathy and the mirror neuron system: Preliminary findings from a non-psychiatric sample. Psychiatry Research, 160, 137­144. doi:10.1016/j.psychres.2007.08.022

182

Ferri, F., Ebisch, S. J. H., Costantini, M., Salone, A., Arciero, G., Mazzola, V., ... Gallese, V. (2013). Binding action and emotion in social understanding. PloS One, 8(1), e54091. doi:10.1371/journal.pone.0054091 Frey, S., Campbell, J. S. W., Pike, G. B., & Petrides, M. (2008). Dissociating the human language pathways with high angular resolution diffusion fiber tractography. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 28, 11435­11444. doi:10.1523/JNEUROSCI.238808.2008 Galati, G., Committeri, G., Spitoni, G., Aprile, T., Di Russo, F., Pitzalis, S., & Pizzamiglio, L. (2008). A selective representation of the meaning of actions in the auditory mirror system. NeuroImage, 40(3), 1274­86. doi:10.1016/j.neuroimage.2007.12.044 Gallese, V., Fadiga, L., Fogassi, L., & Rizzolatti, G. (1996). Action recognition in the premotor cortex. Brain, 119 (Pt 2), 593­609. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/8800951 Gallese, V., Fogassi, L., Fadiga, L., & Rizzolatti, G. (2002). Action representation and the inferior parietal parietal lobule. In Attention & Performance XIX. Common Mechanisms in Perception and Action (pp. 247­266). Oxford: Oxford University Press. Gallese, V., Keysers, C., & Rizzolatti, G. (2004). A unifying view of the basis of social cognition. Trends in Cognitive Sciences, 8(9), 396­403. doi:10.1016/j.tics.2004.07.002 Gangitano, M., Mottaghy, F. M., & Pascual-Leone, A. (2001). Phase-specific modulation of cortical motor output during movement observation. Neuroreport, 12, 1489­1492. doi:10.1097/00001756200105250-00038 Gastaut H J, B. J. (1954). EEG changes during cinematographic presentation; moving picture activation of the EEG. Electroencephalography and Clinical Neurophysiology, 6(3), 433. Gazzaniga, M. S. (1995). Principles of human brain organization derived from split-brain studies. Neuron, 14(2), 217­228. Gazzola, V., Aziz-Zadeh, L., & Keysers, C. (2006). Empathy and the somatotopic auditory mirror system in humans. Current Biology, 16(18), 1824­9. doi:10.1016/j.cub.2006.07.072

183

George, M. S., Parekh, P. I., Rosinsky, N., Ketter, T. A., Kimbrell, T. A., Heilman, K. M., & Post, R. M. (1996). Understanding emotional prosody activates right hemisphere regions. Archives of Neurology, 53(7), 665. Giard, M. H., & Peronnet, F. (1999). Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study. Journal of Cognitive Neuroscience, 11, 473­ 490. doi:10.1162/089892999563544 Goldin, P. R., McRae, K., Ramel, W., & Gross, J. J. (2008). The neural bases of emotion regulation: reappraisal and suppression of negative emotion. Biological Psychiatry, 63(6), 577­86. doi:10.1016/j.biopsych.2007.05.031 Grafton, S. T., Arbib, M. A., Fadiga, L., & Rizzolatti, G. (1996). Localization of grasp representations in humans by positron emission tomography. 2. Observation compared with imagination. Experimental Brain Research., 112, 103­111. doi:10.1007/BF00227183 Grèzes, J., Armony, J. L., Rowe, J., & Passingham, R. E. (2003). Activations related to "mirror" and "canonical" neurones in the human brain: An fMRI study. NeuroImage, 18, 928­937. doi:10.1016/S1053-8119(03)00042-9 Grèzes, J., & Decety, J. (2001). Functional anatomy of execution, mental simulation, observation, and verb generation of actions: a meta-analysis. Human Brain Mapping, 12(1), 1­19. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/11198101 Grosbras, M.-H., Beaton, S., & Eickhoff, S. B. (2012). Brain regions involved in human movement perception: a quantitative voxel-based meta-analysis. Human Brain Mapping, 33(2), 431­54. doi:10.1002/hbm.21222 Grosbras, M.-H., & Paus, T. (2006). Brain networks involved in viewing angry hands or faces. Cerebral Cortex (New York, N.Y.!: 1991), 16(8), 1087­96. doi:10.1093/cercor/bhj050 Gross, J. J., & John, O. P. (2003). Individual differences in two emotion regulation processes: implications for affect, relationships, and well-being. Journal of Personality and Social Psychology, 85(2), 348­ 362. Retrieved from http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.85.2.348

184

Hamann, S. B., Ely, T. D., Hoffman, J. M., & Kilts, C. D. (2002). Ecstasy and agony: activation of the human amygdala in positive and negative emotion. Psychological Science, 13(2), 135­41. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/11933997 Hari, R., Forss, N., Avikainen, S., Kirveskari, E., Salenius, S., & Rizzolatti, G. (1998). Activation of human primary motor cortex during action observation: a neuromagnetic study. Proceedings of the National Academy of Sciences of the United States of America, 95(25), 15061­5. Retrieved from http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=24575&tool=pmcentrez&rendertype=abst ract Harris, T., Harris, S., Rubin, J. S., & Howard, D. M. (1998). The Voice Clinic Handbook. London: Whurr Publishers Ltd. Hess, U., & Blairy, S. (2001). Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy. International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology, 40(2), 129­41. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/11165351 Hickok, G. (2009). Eight problems for the mirror neuron theory of action understanding in monkeys and humans. Journal of Cognitive Neuroscience, 21, 1229­1243. doi:10.1162/jocn.2009.21189 Hinke, R. M., Hu, X., Stillman, A. E., Kim, S. G., Merkle, H., Salmi, R., & Ugurbil, K. (1993). Functional magnetic resonance imaging of Broca's area during internal speech. Neuroreport, 4, 675­678. Hirai, M., Hiraki, K. (2005). An event-related potentials study of biological motion perception in human infants. Cogn. Brain Res, 22, 301­304. Iacoboni, M., & Dapretto, M. (2006). The mirror neuron system and the consequences of its dysfunction. Nature Reviews Neuroscience, 7(12), 942­51. doi:10.1038/nrn2024 Iacoboni, M., Koski, L. M., Brass, M., Bekkering, H., Woods, R. P., Dubeau, M., ... Rizzolatti, G. (2001). Reafferent copies of imitated actions in the right superior temporal cortex. Proceedings of the National Academy of Sciences of the United States of America, 98(24), 13995­13999. Iacoboni, M., Molnar-Szakacs, I., Gallese, V., Buccino, G., Mazziotta, J. C., & Rizzolatti, G. (2005a). Grasping the intentions of others with one's own mirror neuron system. PLoS Biology, 3(3), e79. doi:10.1371/journal.pbio.0030079

185

Iacoboni, M., Molnar-Szakacs, I., Gallese, V., Buccino, G., Mazziotta, J. C., & Rizzolatti, G. (2005b). Grasping the intentions of others with one's own mirror neuron system. PLoS Biology, 3(3), e79. doi:10.1371/journal.pbio.0030079 Iacoboni, M., Woods, R. P., Brass, M., Bekkering, H., Mazziotta, J. C., & Rizzolatti, G. (1999). Cortical mechanisms of human imitation. Science (New York, N.Y.), 286, 2526­2528. doi:10.1126/science.286.5449.2526 Ilie, G., & Thompson, W. F. (2006). A Comparison of Acoustic Cues in Music and Speech for Three Dimensions of Affect. Music Perception. doi:10.1525/mp.2006.23.4.319 Jousmäki, V., & Hari, R. (1998). Parchment-skin illusion: sound-biased touch. Current Biology!: CB. doi:10.1016/S0960-9822(98)70120-4 Kaplan, J. T., & Iacoboni, M. (2007). Multimodal action representation in human left ventral premotor cortex. Cognitive Processing, 8(2), 103­13. doi:10.1007/s10339-007-0165-z Kehoe, E. G., Toomey, J. M., Balsters, J. H., & Bokde, A. L. W. (2013). Healthy aging is associated with increased neural processing of positive valence but attenuated processing of emotional arousal: an fMRI study. Neurobiology of Aging, 34(3), 809­21. doi:10.1016/j.neurobiolaging.2012.07.006 Keuken, M. C., Hardie, A., Dorn, B. T., Dev, S., Paulus, M. P., Jonas, K. J., ... Pineda, J. A. (2011a). The role of the left inferior frontal gyrus in social perception: An rTMS study. Brain Research, 1383, 196­205. doi:10.1016/j.brainres.2011.01.073 Keuken, M. C., Hardie, A., Dorn, B. T., Dev, S., Paulus, M. P., Jonas, K. J., ... Pineda, J. A. (2011b). The role of the left inferior frontal gyrus in social perception: An rTMS study. Brain Research, 1383, 196­205. doi:10.1016/j.brainres.2011.01.073 Keysers, C., & Gazzola, V. (2009). Expanding the mirror: vicarious activity for actions, emotions, and sensations. Current Opinion in Neurobiology, 19(6), 666­71. doi:10.1016/j.conb.2009.10.006 Keysers, C., & Gazzola, V. (2010). Social neuroscience: mirror neurons recorded in humans. Current Biology, 20(8), R353­4. doi:10.1016/j.cub.2010.03.013 Keysers, C., Kohler, E., Umiltà, M. a, Nanetti, L., Fogassi, L., & Gallese, V. (2003). Audiovisual mirror neurons and action recognition. Experimental Brain Research, 153(4), 628­36. doi:10.1007/s00221003-1603-5

186

Kilner, J. M., & Lemon, R. N. (2013). What we know currently about mirror neurons. Current Biology, 23. doi:10.1016/j.cub.2013.10.051 Kohler, E., Keysers, C., Umiltà, M. A., Fogassi, L., Gallese, V., & Rizzolatti, G. (2002). Hearing sounds, understanding actions: action representation in mirror neurons. Science (New York, N.Y.), 297(5582), 846­8. doi:10.1126/science.1070311 Lahav, A., Saltzman, E., & Schlaug, G. (2007). Action representation of sound: audiomotor recognition network while listening to newly acquired actions. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 27(2), 308­14. doi:10.1523/JNEUROSCI.4822-06.2007 Laufs, H., Kleinschmidt, A., Beyerle, A., Eger, E., Salek-Haddadi, A., Preibisch, C., & Krakow, K. (2003). EEG-correlated fMRI of human alpha activity. NeuroImage, 19, 1463­1476. doi:10.1016/S10538119(03)00286-6 Laufs, H., Krakow, K., Sterzer, P., Eger, E., Beyerle, A., Salek-Haddadi, A., & Kleinschmidt, A. (2003). Electroencephalographic signatures of attentional and cognitive default modes in spontaneous brain activity fluctuations at rest. Proceedings of the National Academy of Sciences of the United States of America, 100, 11053­11058. doi:10.1073/pnas.1831638100 Le, R. M., Pineda, J. A., & Sharma, A. (2009). Motor ­ auditory ­ visual integration!: The role of the human mirror neuron system in communication and communication disorders. Journal of Communication Disorders, 42, 299­304. doi:10.1016/j.jcomdis.2009.03.011 LeDoux, J. (2003). The emotional brain, fear, and the amygdala. Cellular and Molecular Neurobiology, 23(4-5), 727­38. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/14514027 LeDoux, J. E. (1995). Emotion: clues from the brain. Annual Review of Psychology, 46, 209­35. doi:10.1146/annurev.ps.46.020195.001233 Lepage, J. F., & Théoret, H. (2007). The mirror neuron system: grasping others' actions from birth? Developmental science, 10(5), 513-523. Lévêque, Y., & Schön, D. (2013). Listening to the human voice alters sensorimotor brain rhythms. PloS One, 8(11), e80659. doi:10.1371/journal.pone.0080659 Logothetis, N. K., Pauls, J., Augath, M., Trinath, T., & Oeltermann, A. (2001). Neurophysiological investigation of the basis of the fMRI signal. Nature, 412, 150­157. doi:10.1038/35084005

187

Maeda, F., Kleiner-Fisman, G., & Pascual-Leone, A. (2002). Motor facilitation while observing hand actions: specificity of the effect and role of observer's orientation. Journal of Neurophysiology, 87, 1329­1335. doi:10.1152/jn.00773.2000 Mather, G., Radford, K., & West, S. (1992). Low-level visual processing of biological motion. Proceedings. Biological Sciences / The Royal Society, 249, 149­155. doi:10.1098/rspb.1992.0097 McGarry, L. M., Russo, F. a, Schalles, M. D., & Pineda, J. a. (2012). Audio-visual facilitation of the mu rhythm. Experimental Brain Research. doi:10.1007/s00221-012-3046-3 McGarry, L. M., & Russo, F. a. (2011). Mirroring in Dance/Movement Therapy: Potential mechanisms behind empathy enhancement. The Arts in Psychotherapy, 38(3), 178­184. doi:10.1016/j.aip.2011.04.005 McGurk H, M. J. (1976). Hearing lips and seeing voices. Nature, 264, 746­748. Meltzoff, A. N., & Moore, M. K. (1977). Imitation of facial and manual gestures by human neonates. Science, 198(4312), 75-78. Meredith, M. A., & Stein, B. E. (1986). Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration. Journal of Neurophysiology, 56, 640­662. doi:citeulike-article-id:844215 Mithen, S., Morley, I., Wray, A., Tallerman, M., & Gamble, C. (2006). The singing neanderthals: the origins of music, language, mind and body. Cambridge Archaeological Journal. doi:10.1017/S0959774306000060 Molenberghs, P., Brander, C., Mattingley, J. B., & Cunnington, R. (2010). The role of the superior temporal sulcus and the mirror neuron system in imitation. Human Brain Mapping, 31(9), 1316­26. doi:10.1002/hbm.20938 Molenberghs, P., Cunnington, R., & Mattingley, J. B. (2012). Brain regions with mirror properties: a metaanalysis of 125 human fMRI studies. Neuroscience and Biobehavioral Reviews, 36(1), 341­9. doi:10.1016/j.neubiorev.2011.07.004 Molnar-Szakacs, I., Iacoboni, M., Koski, L., & Mazziotta, J. C. (2005). Functional segregation within pars opercularis of the inferior frontal gyrus: Evidence from fMRI studies of imitation and action observation. Cerebral Cortex, 15, 986­994. doi:10.1093/cercor/bhh199

188

Molnar-Szakacs, I., & Overy, K. (2006). Music and mirror neurons: from motion to "e"motion. Social Cognitive and Affective Neuroscience, 1(3), 235­41. doi:10.1093/scan/nsl029 Moody, E. J., & McIntosh, D. N. (2011). Mimicry of Dynamic Emotional and Motor-Only Stimuli. Social Psychological and Personality Science, 2(6), 679­686. doi:10.1177/1948550611406741 Moore, A., Gorodnitsky, I., & Pineda, J. (2012). EEG mu component responses to viewing emotional faces. Behavioural Brain Research, 226(1), 309­16. doi:10.1016/j.bbr.2011.07.048 Mukamel, R., Ekstrom, A. D., Kaplan, J., Iacoboni, M., & Fried, I. (2010). Single-neuron responses in humans during execution and observation of actions. Current Biology, 20(8), 750­6. doi:10.1016/j.cub.2010.02.045 Muthukumaraswamy, S. D., Johnson, B. W., & McNair, N. A. (2004). Mu rhythm modulation during observation of an object-directed grasp. Cognitive Brain Research, 19, 195­201. doi:10.1016/j.cogbrainres.2003.12.001 Neuper, C., Wortz, M., & Pfurtscheller, G. (2006). ERD/ERS patterns reflecting sensorimotor activation and deactivation. Prog Brain Res, 159, 211­222. Niedenthal, P. M. (2007). Embodying emotion. Science (New York, N.Y.), 316(5827), 1002­5. doi:10.1126/science.1136930 Nishitani, N., Avikainen, S., & Hari, R. (2004). Abnormal imitation-related cortical activation sequences in Asperger's syndrome. Annals of Neurology, 55(4), 558­62. doi:10.1002/ana.20031 Oberman, L. M., Hubbard, E. M., McCleery, J. P., Altschuler, E. L., Ramachandran, V. S., & Pineda, J. A. (2005). EEG evidence for mirror neuron dysfunction in autism spectrum disorders. Cognitive Brain Research, 24, 190­198. doi:10.1016/j.cogbrainres.2005.01.014 Oberman, L. M., & Ramachandran, V. S. (2007). The simulating social mind: the role of the mirror neuron system and simulation in the social and communicative deficits of autism spectrum disorders. Psychological Bulletin, 133(2), 310­27. doi:10.1037/0033-2909.133.2.310 Oberman, L. M., Ramachandran, V. S., & Pineda, J. a. (2008). Modulation of mu suppression in children with autism spectrum disorders in response to familiar or unfamiliar stimuli: the mirror neuron hypothesis. Neuropsychologia, 46(5), 1558­65. doi:10.1016/j.neuropsychologia.2008.01.010

189

Overy, K., & Molnar-Szakacs, I. (2009). Being together in time: Musical experience and the mirror neuron system. Music Perception, 26(5), 489­504. Owen, a M., Evans, a C., & Petrides, M. (1996). Evidence for a two-stage model of spatial working memory processing within the lateral frontal cortex: a positron emission tomography study. Cerebral Cortex, 6, 31­8. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/8670636 Patel, A. D., Hopkins, J. J., & Diego, S. (2010). Music, biological evolution, and the brain. In Emerging Disciplines (pp. 1­37). Perry, A., Troje, N. F., & Bentin, S. (2010). Exploring motor system contributions to the perception of social information: Evidence from EEG activity in the mu/alpha frequency range. Social Neuroscience, 5(3), 272­84. doi:10.1080/17470910903395767 Pfurtscheller, G. (1992). Event-related synchronization (ERS): an electrophysiological correlate of cortical areas at rest. Electroencephalography and Clinical Neurophysiology, 83, 62­69. doi:10.1016/00134694(92)90133-3 Pfurtscheller, G., & Aranibar, A. (1979). Evaluation of event-related desynchronization (ERD) preceding and following voluntary self-paced movement. Electroencephalography and Clinical Neurophysiology, 46, 138­146. doi:10.1016/0013-4694(79)90063-4 Pfurtscheller, G., & Lopes da Silva, F. H. (1999). Event-related EEG/MEG synchronization and desynchronization: basic principles. Clinical Neurophysiology!: Official Journal of the International Federation of Clinical Neurophysiology, 110(11), 1842­57. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/10576479 Pfurtscheller, G., Neuper, C., Andrew, C., & Edlinger, G. (1997). Foot and hand area mu rhythms. In International Journal of Psychophysiology (Vol. 26, pp. 121­135). doi:10.1016/S01678760(97)00760-5 Pfurtscheller, G., Neuper, C., & Krausz, G. (2000). Functional dissociation of lower and upper frequency mu rhythms in relation to voluntary limb movement. Clinical Neurophysiology, 111, 1873­1879. doi:10.1016/S1388-2457(00)00428-4

190

Pfurtscheller, G., Zalaudek, K., & Neuper, C. (1998). Event-related beta synchronization after wrist, finger and thumb movement. Electroencephalography and Clinical Neurophysiology - Electromyography and Motor Control, 109, 154­160. doi:10.1016/S0924-980X(97)00070-2 Pineda, J. a. (2005). The functional significance of mu rhythms: translating "seeing" and "hearing" into "doing". Brain Research Reviews, 50(1), 57­68. doi:10.1016/j.brainresrev.2005.04.005 Pineda, J. a. (2008). Sensorimotor cortex as a critical component of an "extended" mirror neuron system: Does it solve the development, correspondence, and control problems in mirroring? Behavioral and Brain Functions, 4, 47. doi:10.1186/1744-9081-4-47 Pineda, J. A., & Hecht, E. (2009). Mirroring and mu rhythm involvement in social cognition!: Are there dissociable subcomponents of theory of mind!? Biological Psychology, 80(3), 306­314. doi:10.1016/j.biopsycho.2008.11.003 Press, C., Gillmeister, H., & Heyes, C. (2006). Bottom-up, not top-down, modulation of imitation by human and robotic models. The European Journal of Neuroscience, 24(8), 2415­9. doi:10.1111/j.1460-9568.2006.05115.x Redcay, E. (2008). The superior temporal sulcus performs a common function for social and speech perception: implications for the emergence of autism. Neuroscience and Biobehavioral Reviews, 32(1), 123­42. doi:10.1016/j.neubiorev.2007.06.004 Riskind, J. H., & Gotay, C. C. (1982). Physical posture: Could it have regulatory or feedback effects on motivation and emotion? Motivation and Emotion, 6(3), 273­298. doi:10.1007/BF00992249 Rizzolatti, G. (2005). The mirror neuron system and its function in humans. Anatomy and Embryology, 210(5-6), 419­21. doi:10.1007/s00429-005-0039-z Rizzolatti, G., & Arbib, M. A. (1998). Language within our grasp. Trends in Neurosciences. doi:10.1016/S0166-2236(98)01260-0 Rizzolatti, G., & Craighero, L. (2004). The mirror-neuron system. Annual Review of Neuroscience, 27, 169­92. doi:10.1146/annurev.neuro.27.070203.144230 Rizzolatti, G., Fadiga, L., Gallese, V., & Fogassi, L. (1996). Premotor cortex and the recognition of motor actions. Cognitive Brain Research, 3(2), 131­41. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/8713554

191

Rizzolatti, G., Fadiga, L., Matelli, M., Bettinardi, V., Paulesu, E., Perani, D., & Fazio, F. (1996). Localization of grasp representations in humans by PET: 1. Observation versus execution. Experimental Brain Research. Experimentelle Hirnforschung. Experimentation Cerebrale, 111, 246­ 252. doi:10.1007/BF00227301 Rizzolatti, G., Fogassi, L., & Gallese, V. (2001). Neurophysiological mechanisms underlying the understanding and imitation of action. Nature Reviews. Neuroscience, 2, 661­670. doi:10.1038/35090060 Romanski, L. M., Tian, B., Fritz, J., Mishkin, M., Goldman-Rakic, P. S., & Rauschecker, J. P. (1999a). Dual streams of auditory afferents target multiple domains in the primate prefrontal cortex. Nature Neuroscience, 2, 1131­1136. doi:10.1038/16056 Romanski, L. M., Tian, B., Fritz, J., Mishkin, M., Goldman-Rakic, P. S., & Rauschecker, J. P. (1999b). Dual streams of auditory afferents target multiple domains in the primate prefrontal cortex. Nature Neuroscience, 2(12), 1131­6. doi:10.1038/16056 Russo, F. a, & Thompson, W. F. (2005). The subjective size of melodic intervals over a two-octave range. Psychonomic Bulletin & Review, 12(6), 1068­75. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/16615330 Russo, F. a., Sandstrom, G. M., & Maksimowski, M. (2011). Mouth versus eyes: Gaze fixation during perception of sung interval size. Psychomusicology: Music, Mind and Brain, 21(1-2), 98­107. doi:10.1037/h0094007 Sabatinelli, D., Lang, P. J., Bradley, M. M., Costa, V. D., & Keil, A. (2009). The timing of emotional discrimination in human amygdala and ventral visual cortex. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 29(47), 14864­8. doi:10.1523/JNEUROSCI.327809.2009 Sauseng, P., Klimesch, W., Stadler, W., Schabus, M., Doppelmayr, M., Hanslmayr, S., ... Birbaumer, N. (2005). A shift of visual spatial attention is selectively associated with human EEG alpha activity. The European Journal Of Neuroscience (Vol. 22, pp. 2917­2926). doi:10.1111/j.14609568.2005.04482.x

192

Schellenberg, E. G., & Trehub, S. E. (1996). Children's discrimination of melodic intervals. Developmental Psychology, 32(6), 1039­1050. doi:10.1037//0012-1649.32.6.1039 Schirmer, A., & Kotz, S. a. (2006). Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing. Trends in Cognitive Sciences, 10(1), 24­30. doi:10.1016/j.tics.2005.11.009 Schön, D., Magne, C., & Besson, M. (2004). The music of speech: Music training facilitates pitch processing in both music and language. Psychophysiology, 41, 341­349. doi:10.1111/14698986.00172.x Schwartz, G. E., Davidson, R. J., & Maer, F. (1975). Right hemisphere lateralization for emotion in the human brain: interactions with cognition. Science (New York, N.Y.), 190, 286­288. doi:10.1126/science.1179210 Shimada, S., & Hiraki, K. (2006). Infant's brain responses to live and televised action. Neuroimage, 32(2), 930-939. Singer, T., Seymour, B., O'Doherty, J., Kaube, H., Dolan, R. J., & Frith, C. D. (2004). Empathy for pain involves the affective but not sensory components of pain. Science (New York, N.Y.), 303(5661), 1157­62. doi:10.1126/science.1093535 Skipper, J. I., Nusbaum, H. C., & Small, S. L. (2005). Listening to talking faces: Motor cortical activation during speech perception. NeuroImage, 25, 76­89. doi:10.1016/j.neuroimage.2004.11.006 Spunt, R. P., & Lieberman, M. D. (2012). Dissociating modality-specific and supramodal neural systems for action understanding. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 32(10), 3575­83. doi:10.1523/JNEUROSCI.5715-11.2012 Spunt, R. P., Satpute, A. B., & Lieberman, M. D. (2011). Identifying the what, why, and how of an observed action: an fMRI study of mentalizing and mechanizing during action observation. Journal of Cognitive Neuroscience, 23(1), 63­74. doi:10.1162/jocn.2010.21446 Stein, B. E., & Meredith, M. A. (1993). The Merging Senses. Cognitive Neuroscience Series (p. 221). Talmi, D., Luk, B. T. C., McGarry, L. M., & Moscovitch, M. (2007). The contribution of relatedness and distinctiveness to emotionally-enhanced memory. Journal of Memory and Language, 56(4), 555­574. doi:10.1016/j.jml.2007.01.002

193

Thioux, M., Gazzola, V., & Keysers, C. (2008). Action Understanding: How, What, and Why. Current Biology!: CB, 18(10), R431­34. doi:10.1016/j.cub.2008.04.007 Thompson, W. F., Russo, F. a., & Quinto, L. (2008). Audio-visual integration of emotional cues in song. Cognition & Emotion, 22(8), 1457­1470. doi:10.1080/02699930701813974 Thompson, W., Russo, F., & Livingstone, S. (2010). Facial expressions of pitch structure in music performance. Psychonomic Bulletin Review, 17, 317­322. Thornton, I. M., Pinto, J., Shiffrar, M., & Shiffrar, M. (1998). The visual perception of human locomotion. Locomotion Perception, (33). Toga, A. W., & Mazziotta, J. C. (2002). Brain Mapping: The Methods (Vol. 1). London: Academic Press. Turella, L., Pierno, A. C., Tubaldi, F., & Castiello, U. (2009). Mirror neurons in humans: Consisting or confounding evidence? Brain and Language, 108, 10­21. doi:10.1016/j.bandl.2007.11.002 Ulloa, E. R., & Pineda, J. A. (2007). Recognition of point-light biological motion: Mu rhythms and mirror neuron activity. Behavioral Brain Research, 183, 188­194. doi:10.1016/j.bbr.2007.06.007 Umiltà, M. a, Kohler, E., Gallese, V., Fogassi, L., Fadiga, L., Keysers, C., & Rizzolatti, G. (2001). I know what you are doing. a neurophysiological study. Neuron, 31(1), 155­65. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/11498058 Ungerleider, L. G., & Haxby, J. V. (1994). "What" and "where" in the human brain. Current Opinion in Neurobiology, 4, 157­165. Vigario, R., Sarela, J., Jousmaki, V., Hamalainen, M., & Oja, E. (2000). Independent component approach to the analysis of EEG and MEG recordings. IEEE Trans Biomed Eng, 47, 589­593. doi:10.1109/10.841330 Vigneau, M., Beaucousin, V., Hervé, P. Y., Duffau, H., Crivello, F., Houdé, O., ... Tzourio-Mazoyer, N. (2006). Meta-analyzing left hemisphere language areas: Phonology, semantics, and sentence processing. NeuroImage. doi:10.1016/j.neuroimage.2005.11.002 Wager, T. D., Barrett, L. F., Bliss-Moreau, E., Lindquist, K., Duncan, S., Kober, H., ... Mize, J. (2008). The neuroimaging of emotion. In M. Lewis, J. M. Haviland-Jones, & L. F. Barrett (Eds.), Handbook of Emotions, 3rd ed. (Guilford P., pp. 249­271). New York.

194

Walden, B. E., Prosek, R. A., Montgomery, A. A., Scherr, C. K., & Jones, C. J. (1977). Effects of training on the visual recognition of consonants. Journal of Speech and Hearing Research, 20, 130­145. Warren, J. E., Sauter, D. a, Eisner, F., Wiland, J., Dresner, M. A., Wise, R. J. S., ... Scott, S. K. (2006a). Positive emotions preferentially engage an auditory-motor "mirror" system. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 26(50), 13067­75. doi:10.1523/JNEUROSCI.3907-06.2006 Warren, J. E., Sauter, D. a, Eisner, F., Wiland, J., Dresner, M. A., Wise, R. J. S., ... Scott, S. K. (2006b). Positive emotions preferentially engage an auditory-motor "mirror" system. The Journal of Neuroscience: The Official Journal of the Society for Neuroscience, 26(50), 13067­75. doi:10.1523/JNEUROSCI.3907-06.2006 Watson, R., Latinus, M., Charest, I., Crabbe, F., & Belin, P. (2014). People-selectivity, audiovisual integration and heteromodality in the superior temporal sulcus. Cortex, 50, 125­36. doi:10.1016/j.cortex.2013.07.011 Welch, R. B., DuttonHurt, L. D., & Warren, D. H. (1986). Contributions of audition and vision to temporal rate perception. Perception & Psychophysics, 39, 294­300. doi:10.3758/BF03204939 Wicker, B., Keysers, C., Plailly, J., Royet, J. P., Gallese, V., & Rizzolatti, G. (2003). Both of us disgusted in My insula: the common neural basis of seeing and feeling disgust. Neuron, 40(3), 655­64. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/14642287 Williams, J. H. G., Waiter, G. D., Gilchrist, A., Perrett, D. I., Murray, A. D., & Whiten, A. (2006). Neural mechanisms of imitation and "mirror neuron" functioning in autistic spectrum disorder. Neuropsychologia, 44(4), 610­21. doi:10.1016/j.neuropsychologia.2005.06.010 Zaki, J., Weber, J., Bolger, N., & Ochsner, K. (2009). The neural bases of empathic accuracy. Proceedings of the National Academy of Sciences of the United States of America, 106(27), 11382­7. doi:10.1073/pnas.0902666106 Zatorre, R. J., Belin, P., & Penhune, V. B. (2002). Structure and function of auditory cortex: Music and speech. Trends in Cognitive Sciences. doi:10.1016/S1364-6613(00)01816-7

195

