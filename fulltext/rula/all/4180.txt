COMPUTING VALUE-AT-RISK AND EXPECTED SHORTFALL IN OPERATIONAL RISK by Desire Issiaka Bakassa-Traore A thesis presented to Ryerson University in partial fulfilment of the requirements for the Degree of Master of Science in the program of Applied Mathematics

Toronto, Ontario, Canada, 2015 © Desire Issiaka Bakassa-Traore 2015

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS

I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my thesis may be made electronically available to the public.

ii

ABSTRACT Operational Risk has become more popular in the past fifteen years. The Basel committee realized its importance and banks have to allocate more capital charge, yet this is still not enough. With these new rules, banks have put in place new procedures to compute their risk measures and allocate enough capital charge to avoid bankruptcy. The Basel committee under Basel II has proposed different approaches to compute risk measures for Operational Risk, namely the Basic Indicator Approach, the Advanced Measurement Approach and the Standardized Approach. In our research, we will study the case of Loss Distribution Approach, which has been discussed before, and will contribute to the field by using a heavy-tailed distributed severity: g-and-h distributed. Then, we will analyze and test some methods to compute the value-at-risk( VaR) and conditional value-at-risk or expected shortfall (CVaR).

iii

ACKNOWLEDGMENTS

I would to express my gratitude to the chair of the committee, Dr Garnet Ord for taking the some of his precious time to be here for my defense thesis. I would also like to thank the committee members, Dr. Marcos Escobar and Dr. Alexander Alvarez for willing to be here as well. In addition, a thank you to Dr. Sebastian Ferrando, the Graduate Program Director whom interviewed me prior to my admission at Ryerson and played a key role in the fact that I am her today. And finally, a special thank you to a special supervisor, Dr. Pablo Olivares who introduced me to Operational Risk, was a great profesor in Probability Theory and Financial Mathematics, supported me through difficult times, is and will always be an inspirational mentor

iv

DEDICATION To my father, Colonel D´ esir´ e Bakassa-Traor´ e (1951 - 2005)). A bright man who always pushed me to go higher, sometimes the military ways but left this world too soon...

v

TABLE OF CONTENTS

Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ii

Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

Acknowledgments

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iv

Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

v

List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii

List of Figures

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ix

List of Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ix

1

Introduction

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

2

Loss distribution approach in OR and Single cell loss VaR/CVaR 2.1 2.2 2.3 2.4 2.5 2.6 2.7

. . . . . . . . .

5 5 6 8 15 29 38 42

LDA: General concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Loss Model - OR definition and OR matrix . . . . . . . . . . . . . . . . . . . . Univariate mixing g-and-h distribution . . . . . . . . . . . . . . . . . . . . . . . Truncating and mixing internal and external data . . . . . . . . . . . . . . . . Monte Carlo VaR and CVaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Computing VaR/CVaR by recursive convolution . . . . . . . . . . . . . . . . . Computing VaR/CVaR by Fast Fourier Transform . . . . . . . . . . . . . . . . vi

2.8

Computing VaR/CVaR by Panjer recurrence . . . . . . . . . . . . . . . . . . .

48

3

Computing VaR/CVaR in aggregate loss 3.1

. . . . . . . . . . . . . . . . . . . . . . . .

55 55 55 56 57 68

Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 3.1.2 Properties of Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Fundamental Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 3.3

Aggregated losses in two cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . Aggregated losses with mixed truncated g-and-h probability distribution . .

4

Methods for parameter estimation 4.1 4.2

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

70 70 72

Method of Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . .

5

Conclusion

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

75

Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

A ONE CELL CASE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

B TWO CELLS CASE

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

82

Bibliography

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86

vii

LIST OF TABLES

1.1

Capital charge and weight of each Type of Risk . . . . . . . . . . . . . . . . .

2

2.1 2.2 2.3

Statistical Measures of the Losses by Monte Carlo . . . . . . . . . . . . . . . . Dependence of VaR and CVaR with  = 200 on Quantile  . . . . . . . . . . . Dependence of VaR and CVaR with  = 200 on severity parameter h with  = 99.5% (One can observe that h  1 is unrealistic) . . . . . . . . . . . . . .

32 34

34

2.4

Dependence of VaR and CVaR on Quantile mixing internal and external data 37

3.1 3.2

Statistical Measures of the Aggregate Losses in two cells by Monte Carlo . . Dependence of VaR and CVaR with  = [200, 100, 100] on Quantile  . . . .

64 64

viii

LIST OF FIGURES

2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8

Variation of g-and-h density for different values of parameter g . . . . . . . . Variation of g-and-h density for different values of parameter h . . . . . . . . Density function of g-and-h mixing internal and external data . . . . . . . . . Empirical VS Theoretical g-and-h density functions . . . . . . . . . . . . . . . Empirical g-and-h VS normal density functions . . . . . . . . . . . . . . . . . . Quantile Comparison between g-and-h and normal distribution . . . . . . . . Mix internal and external g-and-h VS normal distribution . . . . . . . . . . . VaR comparison between g-and-h and normal distributions mixing internal and external data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

13 14 22 30 31 33 36

37 39 40

2.9

Two fold cumulative convolution function . . . . . . . . . . . . . . . . . . . . .

2.10 Two fold density convolution function . . . . . . . . . . . . . . . . . . . . . . .

3.1 3.2

Dependence of VaR on quantile  . . . . . . . . . . . . . . . . . . . . . . . . . . Dependence of CVaR on quantile  . . . . . . . . . . . . . . . . . . . . . . . . .

66 67

ix

Chapter 1

INTRODUCTION

The twentieth century and the beginning of the twenty-first century have been economically and financially very hard for the world. These centuries have seen The Panic of 1907 with bank failures, the Great Depression of 1929, the Subprime mortgage crisis of 2008. Along with these crisis, the bad internal behaviors of some employees - such as J´ er^ ome Kerviel, a French trader who caused the loss of 50 Billions euros to Soci´ et´ e G´ en´ erale - have caused bankruptcy to their financial institutions. As a result, banks were weakened in their comfort and some regulations were put in place so that they have sufficient capital reserve based on the risk structure. These were sometimes too strict. In the 90's, Operational Risk, which was a type of risk neglected, started to monopolize attention. The Basel Committee on Banking Supervision implemented more detailed regulations in Operational Risk. Banks are allowed to explore advanced models but must submit them to the regulator for approbation before being inserted to their daily tasks. This will mitigate the excessive weight of capital required. (see Enhancing Bank Transparency - Public disclosure and supervisory information that promote safety and soundness in banking systems; Basel Committee on Banking Supervision - September 1998). Nowadays, we notice an increasing interest toward Risk Management in the Financial Industry, especially Operational Risk. We become more aware of its importance even though,

1

Type of Risk Credit Market Operational Total

Capital Charge 6% 0.4% 1.6% 8%

Weight 75% 5% 20% 100%

Table 1.1: Capital charge and weight of each Type of Risk

in most banks, people still give more attention to Market and Credit Risks. Table (1.1) illustrates the capital charge and weight allocated to Operational Risk compared to the other types of risks. We also notice an increase of research in the field. Most of them are theoretical. Only recently financial institutions have incorporated analytical models based on some applied research. This is probably due to the fact that the models are not compliant with real case scenarios, or have not yet been studied deep enough. Either way, financial institutions do not necessarily give access to their full data for academic research, nor the data available is enough. Nonetheless, standard techniques such as Monte Carlo simulation, and Fourier Transform are being used for Operational Risk Management to compute risk measures. In this thesis, we would like to approximate the Value-at-Risk (VaR) and the Expected Shortfall (ES) in Operational Risk. On top of Monte Carlo simulation, we will design a methodology to use other methods, which, we believe, could be less expensive in term of time.

2

For the thesis purpose and given the methods used, we will divide our analysis in three parts : On the first part, we will review some concept, definition and properties which will be useful to determine the VaR and CVaR. Then we will review methods and test them for single Operational Risk cell. On the second part, we will analyze the aggregate loss in two Operational Risk cells i.e two risk categories defined by the type of event and the business line by using other methods to compute the overall loss based on the dependence of the risk categories, introduced by a copula for severities and frequencies. In this analysis, we will contribute to the Operation Risk research field by exploring the g-and-h distribution as an alternative to model severities. This distribution is heavy-tailed which results convenient to analyze extreme losses. On the other hand, it has no explicit density function expression thus we have faced some numerical challenges. Given the severity distribution, we will discuss the computation of VaR and CVaR using Monte Carlo approach and recursive convolution, Fast Fourier Transform and Panjer recurrence for a single cell. For two cells, we will model the dependence among cells via Copula and, again, compute the VaR and CVaR. Another contribution to the field is to explore the methods for parameter estimation. Due to the lack of real data available, we will generate our own. We know that financial institutions do not only use their internal data and only use losses above a certain amount. Therefore, we will discuss the truncation and the mixture of internal and external data. We will also discuss the parameter estimation based on the Methods of Moments and Maximum Likelihood Estimators. The structure of the thesis is as follow: Chapter 2 explains the

3

Loss Distribution Approach by giving a history as well as a definition of Operational Risk to make the concept clear to the reader, the general concept of the VaR and the Expected Shortfall. Also, we will provide the reader with some properties and simulations of univariate mixing g-and-h, Truncation and mixing internal with external data before exploring the computing methods for the VaR and the CVaR for a single cell using Monte Carlo, recursive convolution, Fast Fourier Transform and Panjer recurrence. Chapter 3 focuses on the computation of the same risk measures in aggregate loss by modeling the dependence among cells via copula. We will present the aggregate model, review the copula properties and present the results obtained using Clayton copula. Chapter 4 explores the parameter estimation methods: Method of Moments and MLE. Finally, Chapter 5 concludes our research.

4

Chapter 2

LOSS DISTRIBUTION APPROACH IN OR AND SINGLE CELL LOSS VAR/CVAR

2.1

LDA: General concepts

Basel II is the second Basel Accord. It consists of regulations and laws issued by the Basel Committee on Banking Supervision. It proposes VaR as a risk measure to evaluate the capital charge. Due to the fact that VaR is a quantile, we need a distribution of Operational Losses. This distribution is called Loss Distribution Approach. Under this approach, the bank estimates, for each business line/risk type cell, the probability distributions of the severity (single event impact) and of the one year event frequency using its internal data. With these two distributions, the bank then computes the probability distribution of the aggregate operational loss. The total required capital is the sum of the Value-at-Risk of each business line and event type combination( see Frachot et al.(2001)). LDA is a statistical approach which is very popular in actuarial sciences for computing aggregate loss distributions. Under the Loss Distribution Approach, the bank estimates, for each business line/risk type cell, the probability distribution functions of the single event impact and the event frequency for the next (one) year using its internal data, and computes the probability distribution function of the cumulative operational loss (see Frachot et al. (2001)). The LDA model for the total loss Lt on an interval [0,1], generally t = 1year in a 5

Financial Institution can be formulated as:

J

Lt =

=1

Lt ;

()

where Lt =  Xi
i=1

()

Nt

()

()

(the loss of a single cell );

j = 1, 2, .., J Nt
()

 1 (Frequency of loss events - The number of times a loss occurs in a particular time

frame.) Xi
()

 2 (Severity - The size of the losses)

LDA is very popular because it allows one hold less capital than with standard methods proposed by the Basel Committee. In general terms, LDA consists in estimating the parameters 1 and 2 and compute V aR i.e the Value-at-Risk such that FL (V aR ) = , where FL is the cumulative distribution function of the loss L.

2.2

Loss Model - OR definition and OR matrix

Operational Risk was introduced by the Basel Committee. It is a process including risk assessment, risk decision making and also risk of Loss due to bad internal procedures, it's a risk due to human error. The Basel II Committee defines operational risk as: "The risk of loss resulting from inadequate or failed internal processes, people and systems

6

or from external events (see Basel Committee on Banking Supervision (2006, p. 204))". It includes legal risk but not strategic and reputational risk. Therefore they have in place a list of seven risk categories called event types: (1) Internal Fraud - misappropriation of assets, tax evasion, intentional mismarking of positions, bribery, (2) External Fraud - theft of information, hacking damage, third-party theft and forgery, (3) Employment Practices and Workplace Safety - discrimination, workers compensation, employee health and safety (4) Clients, Products and Business Practice - market manipulation, antitrust, improper trade, product defects, fiduciary breaches, account churning. (5) Damage to Physical Assets - natural disasters, terrorism, vandalism. (6) Business Disruption and Systems Failures - utility disruptions, software failures, hardware failure. (7) Execution, Delivery and Process Management - data entry errors, accounting errors, failed mandatory reporting, negligent loss of client assets.

These seven event types permit to collect data for the eight business lines: Corporate Finance, Trading and Sales, Retail Banking, Commercial Banking, Payment and Settlement, Agency Services, Asset Management and Retail Brokerage. The combination in pairs of the seven event types and eight risk categories ( 56 cells) form the Operational Risk Matrix. 7

2.3

Univariate mixing g-and-h distribution

The g-and-h family of distributions was introduced by Tukey (1977), found application in Operational Risk in Moscadelli (2004) and Dutta and Perry (2007) and has been discussed by Degen et al. (2007) on the subadditivity of VaR for g-and-h random variables and showed that for reasonable g-and-h parameter values, superadditivity when estimating high quantiles. It allows the transformation of standard normal random variables with g and h parameters to generate non-normal distributions (see Kowalchuk and Headrick (2010).). Definition 2.1. A g-and-h distributed random variable X is a transformed normal variable Z according to X =A+B egZ - 1 hZ 2 e 2 = A + Bk (Z ) g (2.1)

with A and B determining the location and scale of the distribution, respectively, whereas g and h > 0 determine skewness and the kurtosis of the distribution. We define the function k (x) as: egx - 1 hx2 e 2 ,x  R g

k (x ) =

(2.2)

Its derivative is: k  (x ) =
hx2 1 ((g + hx)egx - hx)e 2 g

(2.3)

Notice that k (x) is a one-to-one function as its derivative k  is strictly positive for g > 0 or strictly negative for g < 0. Hence, the inverse function exists even though there is no analytic expression for it. Its probability density function (p.d.f.) can be computed by an

8

elementary approach: fX (x) = fZ ( k -1 (x) k -1 ( x )


(2.4)

where fZ is the probability density function (p.d.f.) of a standard normal distribution. Notation: X  g - and - h(A, B, g, h)

Remark 2.2. The parameter g controls the skewness whereas h governs the heavy-tailedness of a distribution. This was shown using MatLab in graphs 2.1 and 2.2 When g = 0. Equation (2.1), interpreted as the limit g 0 is given by:
hZ 2 2

X = A + BZe

(2.5)

and is called the h-distribution. Although, when h = 0, we have: X =A+B egZ - 1 g (2.6)

and is called the g-distribution. This distribution is a scaled log-normal distribution (see Johnson and Kotz (1970)). In the next lemma we summarize the cumulative distribution function (c.d.f.), the probability density function (p.d.f.) and the quantile function of the g-and-h distribution. Lemma 2.3. Let X  g - and - h(A, B, g, h) be a random variable then their c.d.f, p.d.f.

9

and quantile function are given respectively by: FX (y ) = (k -1 ( fX (y ) = y-A )) B
-1 ( y -A ))2 B 2

(2.7) k  ( k -1 ( (2.8) (2.9)

y - A -1 )) B y-A y-A -1 ) = k  QZ ( ) QX (y ) = FX (y ) = k   - 1 ( B B
(k 1  e- B 2

where , fZ and QZ are the c.d.f., p.d.f. and the quantile function of a standard normal distribution. Proof. From equation (3.8), noting that k (.) is a strictly increasing function: y-A y-A =  k -1 B B

F X (y ) = P k (Z ) <

(2.10)

After differentiating with respect to y we have a density given equation (2.8). The quantile function follows computing its inverse.

On the other hand, moments of a g-and-h distribution have been obtained, see example Iglewicz and Martinez (1984). For g > 0 and 0 < h <
1 n

the n-th moment of the g-and-h is

finite and given by (see Dutta and Perry (2007, Appendix D).):  l  l -r ) g ] 2  exp [( 2 (1-lh)  r    g l 1 - lh 

r n r=0 (-1)  

   n  n-l l  A B E (X ) =   l=0  l 
n n

(2.11)

The next result is about the probability of a sum of independent random variables with g-and-h distribution. It will be needed in computing the p.d.f of the sum of severities. 10

Lemma 2.4. Let X1  g - and - h(A, B, g, h) and X2  g - and - h(A, B, g, h) independent random variables, then: i) FX1 +X2 (y ) = FY1 +Y2 ( y - 2A ) B (2.12)

where Y1 , Y2 are two independent random variables Yi  g - and - h(0, 1, g, h), i = 1, 2. ii) 1  2
1 0

FY1 +Y2 (y ) = =

(v (y, z ))e- 2 dz = E ((v (y, Z ))
R

z2

(v (y, z ))dz

(2.13)

where v = v (y, z ) solves: (egv - 1)e
hv 2 2

- g (y - k (z )) = 0

(2.14)

iii) The p.d.f. is given by: 1 2g e- 2 ((1+h)v1 (y,z )+z ) [(g + hv1 (y, z ))egv1 (y,z ) - hv1 (y, z )]-1 dz (2.15)
1 2 2

fY1 +Y2 (y ) =

R

Proof.

FX1 +X2 (y ) = P(X1 + X2 < y ) = P(A + B (k (Z1 )) + A + B (k (Z2 )) < y ) = P (k (Z 1 ) + k (Z 2 ) < = F Y1 + Y2 ( y - 2A ) B 11 y - 2A ) B

By the convolution product formula and equation (2.4): FY1 +Y2 (y ) = =
R

FY1 (y - x)dFY2 (x) =
R R

FY1 (y - x)fY2 (x)dx

(k -1 (y - x))fZ (k -1 (x))(k  (k -1 (x)))-1 dx

Taking the change of variable z = k -1 (x) we have: (k -1 (y - x))fZ (k -1 (x))(k  (k -1 (x)))-1 dx =
R R

(k -1 (y - k (z )))fZ (z )dz

=

1  2

(k -1 (y - k (z )))e- 2 dz
R

z2

Now, for a function u and random variable Y  g - and - h(0, 1, g, h) such that E (u(Y )) < , after the change of variable y = FX (x) we note that:
1 1

Eu(Y ) =
R

u(x)fY (x)dx =

0

u(QY (y ))dy =

0

u(k (QZ (y )))dy

But specializing the previous result for g (x) = FY1 (y - x) FY1 +Y2 (y ) = P (Y1 + Y2 < y ) = E (1[Y1 +Y2 <y] = E (E (1[Y1 +y2 <y] Y2 ))
1

= E (FY1 (y - Y2 )) =
1

0

FY1 (y - k (QZ (y )))dz
1

=
0

(k -1 (y - k (QZ (y )))dz =

(v (y, z ))dz
0

Equation (2.15) is obtained in a similar way. Namely: fY1 +Y2 (y ) = =
R

fY1 (y - x)fY2 (x)dx =
R R

fZ (k -1 (y - x))(k  (k -1 (y - x)))-1 fZ (k -1 (x))(k  (k -1 (x)))-1 dx

fZ (k -1 (y - k (z )))(k  (k -1 (y - k (z ))))-1 fZ (z )dz 1 2g e- 2 ((1+h)v1 (y,z )+z ) [(g + hv1 (y, z ))egv1 (y,z ) - hv1 (y, z )]-1 dz
1 2 2

=

R

12

Remark 2.5. The function k -1 is not explicitly available. Zeros of k need to be computed for different values of the argument. The algorithm is implemented in MatLab. Figure 2.1 shows density functions of a g-and-h probability distribution with parameters A = 0, B = 1, h = 0.25 and g = 1(blue line), g = 2(green line) and g = 3 (red line). As we can observe g is a parameter capturing the asymmetry or skewness of the data.

Figure 2.1: Variation of g-and-h density for different values of parameter g

In figure 2.2 the graph of a g-and-h p.d.f. is shown for different values of the parameter h. The curves provide the shapes for the functions with parameters A = 0, B = 1, g = 2 and 13

h = 0.1(blue line), h = 0.3(green line) and h = 0.3 (red line). It illustrates how the parameter h captures the thickness in the tail of the distribution.

Figure 2.2: Variation of g-and-h density for different values of parameter h

Note that the code to retrieve graphs 2.1 and 2.2 is in Appendix A (densitydependgh.m) Given a random variable L representing the loss in a single cell the value-at-risk at level , denoted by V aR , solves FL (V aR ) = . The conditional value-at-risk or expected shortfall at level , denoted by CV aR() or

14

ES () is defined as 1 1-
1 

ES () = E (L L > V aR ) =

V aR d

(2.16)

Notice that the conditional value-at-risk represents the average loss in a single cell given that the loss surpasses the value-at-risk (see for example Pavel V. Shevchenko, 2011).
k We denote by FX the convolution of function FX with itself k times. Under a model

with Poissonian frequencies and g-and-h severities described by a random variable X , by conditioning on the number of loss events, the c.d.f. of the loss L is given by: e- k k FX k =0 k !
+

FL (y ) =

(2.17)

1 0 = FX , i.e. the dirac delta at zero and the c.d.f. of X respectively. = 0 and FX where FX

Unfortunately this equation does not have an explicit solution. Numerical methods are needed. In forthcoming sections we study the implementation of different approaches.

2.4

Truncating and mixing internal and external data

Due to the characteristic of the data available, modeling the severity by a standard g-and-h distribution has limited application. Two modifications are required. On one hand, losses below certain threshold are not usually reported by banks and another financial institutions, it suggests the introduction of a truncated distribution. On the other hand, in order to increase the amount of data available, banks use a combination of internal and external information, with possible different probability distributions. It leads to the use of mixing distributions to model losses. Here we assume frequencies 15

come from two independent Poisson distributions, in general with different parameters, and severities come from a mixing of independent g-and-h probability distributions with different parameters and truncation levels. We begin by tackling both problems separately, then we consider both, truncation and mixing together. For a random variable X we define its truncated c.d.f. with truncation level T as:

FX,T (y ) = P (X < y X > T ), y  R The respective p.d.f. and quantile functions are denoted by fX,T and QX,T .

(2.18)

Results from the previous section can easily adapted by noticing that a truncated distribution is nothing but a conditional one on the set [X > T ], where T is the truncation level. Proposition 2.6. Let X  g - and - h(A, B, g, h) truncated at T then:  k -1 FX,T (y ) =
y -A B

-  k -1
T -A B

T -A B

1 -  k -1

1[T,+) (y )

(2.19)

-1 QX,T (y ) = FX (y ) = k  QZ ((1 - FY (

y-A y-A ) y + FY ( ))) B B

(2.20)

where FX,T (y ) and QX,T are the c.d.f. and the quantile function of a standard normal distribution respectively, and
(k 1 - fX,T (y ) =  e -A B 2  (1 -  (k - 1 ( T B ))) -1 ( y -A ))2 B 2

( k  (k - 1 (

y - A -1 ))) 1[T,+] (y ) B

(2.21)

is its p.d.f.

16

Proof. FX,T (y ) = P (X < y X > T ) = = P [X < y ]  [X > T ] P (X > T ) FX (y ) - P (X < y, X  T ) FX (y ) - FX (T ) = 1 - FX (T ) 1 - FX (T )

where the last equality holds for y > T and is equal to zero otherwise. Combining equation (2.7) in Lemma 1 with the expression above we have equation (2.19). Differentiating the latter leads to equation (2.21). Finally, we notice that by definition the value x = QX,T (y ) solves FY,T (x) = y . Then, for y > T: y= FX (x) - FX (T ) 1 - F X (T )

or equivalently (1 - FX (T ))y + FX (T ) = FX (x), then:
-1 ((1-FX (T ))y +FX (T )) = QX ((1-FX (T ))y +FX (T )) = k QZ ((1-FX (T ))y +FX (T )) QX,T (y ) = FX

The truncated moments are obtained in the next proposition: Proposition 2.7. Let X  g - and - h(A, B, g, h) and u a real valued function such that E (u(X )) < +. Then: E (u(X ) X > T ) =
1 In particular for h < n :

1  -A B 2 2  (1 -  (k - 1 ( T B )))

+
-A k-1 ( TB )

u(Bk (z ) + A)e

-z 2 2

dz

(2.22)

E (X

n

  l n-l n 1 1  n B A   X > T) = (1 - hl)- 2 Il (T, A, B, g, h) T -A l   2 - 1 g B (1 - (k ( B )) l=0  l  17

(2.23)

where:

Il (T, A, B, g, h) = Jl (T, A, B, g, h) - Kl (T, A, B, g, h)
l

(2.24) (2.25)

Jl (T, A, B, g, h) = Kl (T, A, B, g, h) = Proof.
+ T

(-1)m e 2(1-hl) (-1)m e 2(1-hl) (
m2 1 A-T m - )(1 - hl) 2 ) B 1 - hl

m2

m=0 l m=0

(2.26)

E (u(X ) X > T ) = =

u(x)fX,T (x)dx =

1 1 - FX (T )
+ T

+ T -

u(x)fX (x)dx (k  (k - 1 (

(2.27) x - A -1 ))) (2.28) dx B

1  -A - 1 ))B 2 (1 -  (k ( T B
x- A B

u(x)e

(k-1 ( x-A ))2 B 2

After the changes of variables u =

and z = k -1 (u) we have:
+
T -A B

E (u(X ) X > T ) = =

1  -A (1 -  (k - 1 ( T B )))B 2 2 1  T -A - 1 (1 - (k ( B )))B 2 2

u(Bu + A)e-

k-1 (u)2 2

-1 (k  (k -1 (u))) (2.29) du
z2

+
-A k -1 ( T B )

u(Bk (z ) + A)e- 2 dz

(2.30)

18

We specialize for u(x) = xn . E (X n X > T ) = = 1
+

-A -A ( 1 - ( T B ))B 2 TB 1 -A - 1 ( 1 -  (k ( T B )))B 2

(Bu + A)n fZ (k -1 (u))(k  (k -1 (u)))-1 du
+
-A ) k-1 ( TB

(2.31) (2.32)

(Bk (z ) + A)n fZ (z )dz
+
-A k-1 ( TB )

=

1 -A ( 1 -  (k - 1 ( T B )))B 2

   n  l n-l B A    l=0  l 
n n

k (z )l fZ (z )dz

(2.33)

=

   n  l n-l  B A  -A  ( 1 -  (k - 1 ( T B )))B 2 2 l=0   l  1    n  l n-l -l  B A g  -A  ( 1 -  (k - 1 ( T B ))B 2 2 l=0   l  1
n

+
-A k-1 ( TB )

k (z )l e- 2 dz

z2

(2.34)

+
-A ) k -1 ( T B

=

(egz - 1)l e

hlz 2 2

- 2 e(2.35) dz

z2

(2.36)

19

Now, expanding again:
+
-A k -1 ( T B )

(egz - 1)l e
+

hlz 2 2

e- 2 dz

z2

(2.37)

   l    =   m=0  m 
l l

-A k -1 ( T B )

(-1)l-m emgz e

hlz 2 2

e- 2 dz

z2

(2.38)

   l    (-1)l-m =   m=0  m 
l

+
-A ) k-1 ( TB

e- 2 ((1-hl)z
1

2 -2mgz )

dz

(2.39)

  (mg )2  l  l-m 2(1-hl)   (-1) e =   m=0  m    (mg )2  l    (-1)l-m e 2(1-hl) =   m=0  m 
l

+
-A k -1 ( T B )

e

-1 (1-hl)(z 2 -2 (1mg z +( 1mg )2 ) 2 -hl) -hl

dz

(2.40)

+
-A k -1 ( T B )

e

(1-hl)(z - (1mg )2 -1 2 -hl)

dz

(2.41)

 =

  (mg )2 1  l    (-1)l-m (1 - hl)- 2 e 2(1-hl)  2   m=0  m 
l

k -1

1 T -A mg 2 - (1 - hl)(2.42) B (1 - hl)

where the last equality occurs after the change of variable y = (1 - hl) 2 (z -

1

mg (1-hl) )

Next, consider the case of mixing g-and-h distributions, namely, let Yi  g - and - h(Ai , Bi , gi , hi ), i = 1, 2. be independent random variables and independent also of a random variable U  U (0, 1). A mixing g-and-h random variable defined as: Y = 1[U <p] Y1 + 1[U p] Y2 where p is the (known) proportion of internal data. Denote by k i (x ) = (egi x - 1) hi x2 e 2 , i = 1, 2 gi 20 (2.44) (2.43)

Their c.d.f., p.d.f. and quantile function are respectively:

FY (y ) = P(1[U <p] Y1 + 1[U p] Y2 < y ) = P(1[U <p] Y1 U < p) + P(1[U p] Y2 U  p) = P(Y1 < y )P(U < p) + P(Y2 < y )P(U  p) = pFY1 (y ) + (1 - p)FY2 (y ) By Lemma 2.3, we have:
-1 ( FY (y ) = p(k1

y - A1 -1 y - A2 ( )) + (1 - p)(k2 )) B1 B2
-1 ( y -A1 ))2 B1 2

(2.45)
 -1 k1 ( (k 1

(k1 p fY (y ) = pfY1 (y ) + (1 - p)fY2 (y ) =  e 2B1 -1 y -A2 2

y - A1 -1 )) B1 (2.46) (2.47)

1 - p (k2 ( B2 ))  - 1 y - A2 -1 2 +  e (k2 k2 ( )) B2 2B2 y - A2 y - A1 ) + (1 - p )k 2  Q Z ( ) QY (y ) = pk1  QZ ( B1 B2

In figure 2.3 the graph of the empirical density of a mixture of g-and-h functions with parameters A1 = 0, B1 = 1, g1 = 2, h1 = 2, A2 = 5, B2 = 1, g2 = 4, h2 = 0.2, p = 0.5 is shown. The density is computed using a Gaussian kernel. We observe a bimodal probability density function in agreement with the theory.

21

Figure 2.3: Density function of g-and-h mixing internal and external data

We have the following proposition for the probability distribution of the sum of two mixing g-and-h random variables. Proposition 2.8. Let Y1 and Y2 be two independent mixing g-and-h random variables given

22

by equation (2.43) then:
-z 2 p2 (vm1 (y, z ))e 2 dz 2 R -z 2 p (1 - p )B 2 (um2 (y, z ))e 2 dz 2B1 R -z 2 p (1 - p )B 1 (um1 (y, z ))e 2 dz 2B2 R -z 2 ( 1 - p )2 (vm2 (y, z ))e 2 dz 2 R

FY1 +Y2 (y ) = + + +

(2.48)

where vmi = vmi (y, z ), i = 1, 2 solves: (egi vmi - 1)e
hi vm2 i 2

- gi (ki (z ) +

2Ai - y ) = 0, i = 1, 2 Bi

and umi = umi (y, z ), i = 1, 2 solves: (egi umi - 1)e
hi um2 i 2

- gi (k3-i (z ) +

A1 + A2 - y ) = 0, i = 1, 2 Bi

and vm1 (y, z ) 1 -1 (vm1 (y,z )2 ) -z2 p2  e2 e 2 dz 2 R y 2 p (1 - p )B 2 um1 (y, z ) 1 -1 (um1 (y,z )2 ) -z2  e2 e 2 dz 2B1 y R 2 um2 (y, z ) 1 -1 (um2 (y,z )2 ) -z2 p (1 - p )B 1  e2 e 2 dz 2B2 y R 2 vm2 (y, z ) 1 -1 (vm2 (y,z )2 ) -z2 (1 - p)2  e2 e 2 dz 2 y R 2

fY1 +Y2 (y ) = + + +

(2.49)

(2.50)

23

Proof. FY1 +Y2 (y ) = = FY1 (y - x)fY2 dx
R

(2.51) (2.52)

 -1 y - x - A1 -1 y - x - A2  p k1 ( ) + (1 - p) k2 ( ) B1 B2 R   p  e  2B1
-1 ( x-A1 ) 2 k1 B1 2

 -1 (k1 k1

x - A1 -1 ) B1 x - A2 -1  )  dx B2 
-1 ( x-A1 ))2 B1 2

(2.53)

+

1-p  e 2B2 p2  2B1 p (1 - p )  2B1 p (1 - p )  2B2 (1 - p)2  2B2

-1 ( k2

x-A2 2 ) B2 2

 -1 (k 2 k2

(2.54) x - A1 -1 ) dx (2.55) B1 x - A1 -1 )) dx(2.56) B1

=

R

-1  (k 1 -1  (k 2

(k1 y - x - A1 )e B1

 -1 (k1 k1  -1 k1 ( (k1

+

R

(k1 y - x - A2 )e B2 (k2 - x - A1 ))e B1 (k2 - x - A2 ))e B2

-1 ( x-A1 ))2 B1 2

+

R

-1 y  (k 1 ( -1 y ( (k2

-1 ( x-A2 ))2 B2 2

 -1 (k2 k2 (  -1 k2 ( (k2

x - A2 - 1 )) dx (2.57) B2 x - A2 -1 )) dx (2.58) B2 (2.59)

+

-1 ( x-A2 ))2 B2 2

R

= I1 + I2 + I3 + I4 where Ij corresponds with the j-th term in the previous expression. After we combine the change of variables u = integrals above: I1 = I2 = I3 = I4 =
z2 p2 -1 y - 2A1 (k1 ( - k1 (z )))e- 2 dz 2 R B1 z2 p(1 - p)B2 - 1 y - B 2 k 2 (z ) - A 2 - A 1 (k1 ( ))e- 2 dz 2B1 B2 R z2 p(1 - p)B1 - 1 y - B 1 k 1 (z ) - A 1 - A 2 (k2 ( ))e- 2 dz 2B2 B2 R z2 (1 - p )2 y - 2 A 2 -1  (k 2 ( - k2 (z )))e- 2 dz 2 B2 R

x- A i Bi

-1 (u), i = 1, 2 depending on the and z = ki

(2.60) (2.61) (2.62) (2.63)

therefore, we have (2.48) and by differentiating we get (2.50). 24

Finally we tackle the more general case of truncated mixing g-and-h severities with a similar procedure. Let YTi  g - and - h(Ai , Bi , gi , hi , Ti ), i = 1, 2 be truncated independent g-and-h random variables with respective truncation levels T1 and T2 , and Y = 1[U <p] YT1 + 1[U p] YT2 (2.64)

The probability distribution of the sum of mixing g-and-h is given in the next theorem. Theorem 2.9. Let Y1 and Y2 be independent g-and-h random variables with parameters (Ai , Bi , gi , hi ), i = 1, 2 satisfying equation: Yi = 1[Ui <p] YT1 + 1[Ui p] YT2 , i = 1, 2. (2.65)

where U1 and U2 are independent random variables, and independent of YTi also, uniformly distributed on [0, 1], then: FY1 +Y2 (y ) = E1
-1 ( k1 -1 ( k1 y -T1 -A1 ) B1

T2 -A1 ) B1

((vm1 (y, z ))e- 2 dz

z2

-1 ( - E1 (k1
-1 ( k2 -1 ( k2

T1 - A1  -1 y - T1 - A1 -1 T2 - A1 ( ( ))) 2 ((k1 )) - (k1 )) B1 B1 B1 ((um2 (y, z ))e- 2 dz
z2

+ E2  -

y -T1 -A2 ) B2

T2 -A2 ) B2

-1 2E2 (k1 (
-1 ( k2 -1 ( k2

T1 - A1 -1 y - T1 - A2 -1 T2 - A2 )))((k2 ( )) - (k2 ( ))) B1 B2 B2 ((um1 (y, z ))e- 2 dz
z2

+ E3

y -T1 -A2 ) B2

T2 -A2 ) B2

-1 - E3 (k2 (
-1 ( k2

T2 - A2  -1 y - T1 - A2 -1 T2 - A2 )) 2 ((k2 ( )) - (k2 ( ))) B2 B2 B2 ((vm2 (y, z ))e- 2 dz
z2

+ E4

y -T1 -A2 ) B2

-1 ( T2 -A2 ) k2 B2

-1 - E4 (k2 (

T2 - A2  -1 T2 - A2 -1 y - T1 - A2 )) 2 ((k2 ( )) - (k2 ( ))) (2.66) B2 B2 B2 25

where: p2 -1 ( T1 -A1 )))2 2 (1 - (k1 B1 p(1 - p)B2 T2 -A2 -1 ( T1 -A1 ))) - 1 2B1 (1 - (k2 ( B2 )))(1 - (k1 B1 p (1 - p )B 1 T1 -A1 - 1 -1 ( T -A2 ))) 2B2 (1 - (k1 ( B1 )))(1 - (k2 B2 2 (1 - p ) -1 ( T2 -A2 )))2 2 (1 - (k2 B2

E1 = E2 = E3 = E4 =

(2.67) (2.68) (2.69) (2.70)

Proof. As in proposition 2.6 we can write:
-1 y -A i -1 T -A i (ki ( Bi )) - (ki ( Bi )) -1 ( T -Ai )) 1 -  (k i Bi

FYTi (y ) =

1[Ti ,+] (y )

(2.71)

-1 (y ) = ki  QZ ((1 - FY ( QYTi (y ) = FXT i

y - Ai y - Ai )y + FY ( ))) Bi Bi

(2.72)

where QZ is the quantile function of a standard normal distribution. fYTi (y ) = 1
-1 ( T -Ai )))B (1 - (ki i Bi



2

e

-

-1 ( (ki

y -Ai 2 )) Bi 2

 -1 (ki ( (ki

y - Ai -1 ))) 1[Ti ,+] Bi

(2.73)

Combining proposition 2.49 for Y = 1[U <p] Y T1 + 1[U p] Y T2 it gives: FY (y ) = pFYT1 (y ) + (1 - p)FYT2 (y ) = p( + (1 - p)(
-1 T1 -A1 -1 y -A 1 ( B1 )) ( B1 )) - (k1  (k 1 -1 ( T1 -A1 )) 1 -  (k 1 B1

)1[T1 ,+] (y )

-1 y -A 2 -1 T 2 -A 2 (k2 ( B2 )) - (k2 ( B2 )) -1 ( T2 -A2 )) 1 - (k2 B2

1[T2 ,+] (y ))
 -1 (k 1 (k 1 (

fY (y ) = p

1
-1 ( T1 -A1 )))B (1 - (k1 1 B1

 e 2

-

-1 ( (k 1

y -A1 2 )) B1 2

y - A1 -1 ))) 1[T1 ,+] (y ) B1 y - A2 -1 ))) 1[T2 ,+] (y ) B2 (2.74)

+ (1 - p)

1
-1 ( T2 -A2 )))B (1 - (k2 2 B2

 e 2

-

-1 ( y -A2 ))2 (k 2 B2 2

 -1 (k 2 (k 2 (

26

Then for y > T1 + T2 :

FY1 +Y2 (y ) = =

FY1 (y - x)fY2 (x)dx
R y -T 1 T2 - 1 y - x- A -1 T -A  (k1 ( B1 1 )) - (k1 ( 1B1 1 )) p( ) -1 ( T -A1 ))  1 -  (k 1 B1 -1 T -A 2 - 1 y - x- A 2 ( B2 ))   (k 2 ( B2 )) - (k2 -1 ( T2 -A2 )) 1 - (k2 B2

(2.75) (2.76)

+

(1 - p)(

(2.77)
 -1 (k1 (k1 (


-1 ( y -A1 ))2 B1 2

(k1  1 -  p e -1 ( T -A1 )))B  (1 -  (k 1 1 2 B 1

y - A1 -1 ))) 1[T1 ,+] B1

(2.78)

+

(1 - p)

1
-1 ( T2 -A2 )))B (1 - (k2 2 B2

 e 2

-

-1 ( (k2

y -A2 2 )) B2 2

 -1 (k 2 (k 2 (

 y - A2 -1 ))) 1[T2 ,+] (x ) dx (2.79) B2  (2.80)

= I1 + I2 + I3 + I4
x- A i Bi -1 (u), i = 1, 2 depending on the and z = ki

After we combine the change of variables u =

27

integrals above: I1 = E1 = E1
y -T1 -A1 B1 T2 -A1 B1 -1 ( k1 -1 ( k1

-1 ( (k 1 (

z2 y - 2A1 -1 T1 - A1 - k (z ))) - (k1 ( )))e- 2 dz B1 B1 z2 y - 2A1 - k (z )))e- 2 dz B1 y -T1 -A1 ) B1

(2.81) (2.82) (2.83) (2.84) (2.85)
-1  ( (k 2 (k 2

y -T1 -A1 ) B1

T2 -A1 ) B1

-1 ( (k 1 (

-

-1 T1 E1 (k1 (
-1 ( k1

- A1 ))) B1

-1 ( k1 -1 ( k1

T2 -A1 ) B1

e- 2 dz

z2

= E1

y -T1 -A1 ) B1

-1 ( T2 -A1 ) k1 B1

-1 ( (k 1 (

z2 y - 2A1 - k (z )))e- 2 dz B1

-1 - E1 (k1 (

T1 - A1  -1 y - T1 - A1 -1 T2 - A1 ( ( ))) 2 ((k1 )) - (k1 )) B1 B1 B1
-1 y ( ((k1
(k2 - x - A1 -1 T1 - A1 ( )) - (k1 )))e- B1 B1 -1 ( y -A2 ))2 B2 2

I2 = B1 E2 = E2 = E2  -

y -T1 T2
-1 ( k2

x - A2 -1 ))) (2.86) B2 (2.87) (2.88) (2.89)

y -T1 -A2 ) B2

-1 ( k2

T2 -A2 ) B2 y -T1 -A2 ) B2

-1 ( ( (k 1 -1 ( (k 1 (

z2 y - B2 u - A1 - A2 -1 T1 - A1 ( )) - (k1 )))e- 2 dz B2 B1 z2 y - B2 u - A1 - A2 ))e- 2 dz B2

-1 ( k2

-1 ( T2 -A2 ) k2 B2

-1 ( 2E2 (k1 y -T1 T2
-1 ( k2 -1 ( k2

T1 - A1 -1 y - T1 - A2 -1 T2 - A2 ( ( )))((k2 )) - (k2 ))) B1 B2 B2
(k1 - x - A2 -1 T2 - A2 ( )) - (k2 ))e- B2 B2 -1 ( y -A1 ))2 B1 2

I3 = B2 E3 = E3 = E3

-1 y ( ((k2

-1  ( (k1 (k1

x - A1 -1 ))) (2.90) dx B1 (2.91) (2.92) (2.93) x - A2 -1 ))) (2.94) dx B2 (2.95) (2.96) (2.97)

y -T1 -A2 ) B2

T2 -A2 ) B2 y -T1 -A2 ) B2

-1 ( (k 2 ( -1 ( (k 2 (

z2 y - x - A2 -1 T2 - A2 ( )) - (k2 ))e- 2 dz B2 B2 z2 y - B1 k1 (z ) - A1 - A2 ))e- 2 dz B2

-1 ( k2 -1 ( k2

T2 -A2 ) B2

-1 - E3 (k2 (

T2 - A2  -1 T2 - A2 -1 y - T1 - A2 )) 2 ((k2 )) - (k2 ( ))) ( B2 B2 B2
-1 y ( ((k2
(k2 - x - A2 -1 T2 - A2 ( )) - (k2 ))e- B2 B2 -1 ( y -A2 ))2 B2 2

I4 = B2 E4 = E4 = E4

y -T1 T2
-1 ( k2

 -1 ( (k2 (k2

y -T1 -A2 ) B2

-1 ( k2

T2 -A2 ) B2 y -T1 -A2 ) B2

-1 ( (k 2 ( -1 ( (k 2 (

z2 y - B2 k2 (z ) - A1 - A2 -1 T2 - A2 )) - (k2 ( ))e- 2 dz B2 B2 z2 y - B2 k2 (z ) - A1 - A2 ))e- 2 dz B2

-1 ( k2 -1 ( k2

T2 -A2 ) B2

-1 - E4 (k2 (

T2 - A2  -1 y - T1 - A2 -1 T2 - A2 )) 2 ((k2 ( )) - (k2 ( ))) B2 B2 B2 28

therefore, we have (2.66).

2.5

Monte Carlo VaR and CVaR

We aim to simulate the loss process:
Nt

Lt =

k=0

Yk

(2.98)

where (Nt )t0 is a Poisson process with intensity  > 0 describing the frequency process or number of losses on [0, t] and (Yi )iN is a sequence of independent random variables with common g-and-h probability distribution and parameters A, B, g and h. Both random elements are independent. For notational simplicity we drop the dependence on cell i and fix the time period , typically a year, dropping the time dependency in equation (2.98). Hence we write N and L instead of Nt and Lt . Simulation of severities, i.e. numbers coming from a g-and-h distribution is a matter of evaluating a non-linear function of standard normal random variables. Algorithms to simulate the latter as well as frequencies given by a Poisson distribution, are widely known. By combining both we will be able to simulate losses in a single cell. The algorithm goes in the following lines: Algorithm 2.10. (1) Inputs: A,B,g,h, (parameters), n(number of simulated losses).

(2) Generate n Poisson random variables with parameter  given by a vector N . (3) Generate N × n standard normal random numbers given by matrix Z . (4) Generate N × n g-and-h random numbers given by Y = k (Z ). 29

i (5) Compute the losses Li = N k=0 Yk , i = 1, 2, . . . , n.

In Figure 2.4 both the theoretical (blue line) and empirical (green line) g-and-h density functions are shown. The parameters are A = 0, b = 1, g = 2, h = 0.25. The empirical p.d.f. is obtained by simulating n = 10000 g-and-h numbers according to the previous algorithm with the same parameters via a Gaussian kernel. We observe a reasonable agreement between both.

Figure 2.4: Empirical VS Theoretical g-and-h density functions

Next we show in figure 2.5 the empirical density of the loss distribution (green line) in 30

comparison with a normal density with the same mean and variance (blue line). Parameters for severity are A = 100000, B = 1, g = 2 and h = 0.25. The empirical p.d.f. of the loss random variable, conditionally on having observed at least one loss event and obtained also by a Gaussian kernel, is observed to have thicker tail than the normal distribution. Frequency of losses is assumed to follow a Poisson distribution with an average  = 200 events per year. Notice that actually the loss distribution is not absolutely continuous, it has a positive mass at zero.

Figure 2.5: Empirical g-and-h VS normal density functions

31

 / measure(L)  = 100  = 200  = 300

Mean(L) 10,003,000 19,999,086 30,001,299

STD(L) 999,980 1,415,281 1,732,590

Skewness(L) 0.1021 0.0727 0.0562

Excess of kurtosis(L) 3.0109 3.0036 3.0096

Median(L) 10,000,000 20,000,752 30,001,280

Table 2.1: Statistical Measures of the Losses by Monte Carlo

In table 2.1 we present a summary with relevant statistical information about empirical losses on a single cell of the Operational risk Matrix, obtained by Monte Carlo simulation and assuming a Poisson probability distribution for the frequency with  = 200 events per year and a g-and-h severity distribution with parameters A = 100000, B = 1, g = 2 and h = 0.25. We observe that asymmetry and excess of kurtosis are translated from the severity to the loss probability distribution formation itself. Moreover, a quantile comparison between the empirical loss density obtained from a Gaussian kernel and the normal distribution is given in figure 2.6. Quantile levels range from 95% to 99.5%.

32

Figure 2.6: Quantile Comparison between g-and-h and normal distribution

In table 2.2 results for the Var and CVaR on a single cell are shown, using a model with Poisson frequencies of intensity  = 200 and a range of values for the VaR/CVaR levels. VaR is computed by obtaining the correct percentile from the sequence of generated losses. CVaR is computed according to equation 2.16 In table 2.3 the dependence of VaR and CVaR on the parameters of the severity are shown.

33

measure/ VaR CVaR

95% 22,400,458 22,975,101

97.5 % 22,801,680 23,372,236

99 % 23,400,597 23,852,866

99.5% 23,701,560 24,174,057

Table 2.2: Dependence of VaR and CVaR with  = 200 on Quantile 

measure/h VaR CVar

0.1 23,701,306 24,235,562

0.25 23,701,560 24,174,057

1 26,790,688 1.121×109

2 24,899,532,560 99.624×1015

Table 2.3: Dependence of VaR and CVaR with  = 200 on severity parameter h with  = 99.5% (One can observe that h  1 is unrealistic)

The algorithm to generate losses for a single cell, with severities coming from a mixed truncated g-and-h distribution, and frequencies representing external and internal losses requires minor modifications from Algorithm 2.10. Notice that a Poisson process having intensity , counting two type of events representing internal and external losses, is equivalent to consider two independent Poisson processes with respective intensities 1 = p and 1 = (1 - p), where p is the probability to have an internal event. We can compute separately the losses in cell i due to internal events, denoted by L(i, 1), from the losses due to external events, denoted by L(i, 2). It leads to the following algorithm: Algorithm 2.11. (1) Inputs: A1 , B1 , g1 , h1 , , A2 , B2 , g2 , h2 , n, p.

34

(2) Generate n Poisson random variables with parameter 1 = p and 1 = (1 - p) given by vectors Ni , i = 1, 2. (3) Generate Ni × n standard normal random numbers given by matrices Zi , i = 1, 2. (4) Generate Ni × n g-and-h random numbers given by Yi = ki (Z ), i = 1, 2.
i (5) Compute the losses L(i, j ) = k= 0 Yk , i = 1, 2, j = 1, 2, . . . , n.

N (j )

(j )

(6) Compute the aggregate loss L(1, j ) + L(2, j ). In figure 2.7 it can be observed the empirical density of the loss for a single cell. Parameters used in the simulation are A1 = 10000, B1 = 1, h1 = 0.25, g1 = 2 for the severities of internal data and A2 = 20000, B2 = 1, h2 = 0.3, g2 = 2 for external data, i.e. a distribution with heavier tails and more skewed. The frequency of loss events is a Poisson distribution with an average of  = 100 events per year. The losses are chosen from external and internal data at 50%. It is compared with a normal distribution with the same mean and variance. Heavier tails can be observed.

35

X 10

6

Figure 2.7: Mix internal and external g-and-h VS normal distribution

Table 2.4 shows the results of mixing internal and external data

In figure 2.8 percentiles of the loss probability distributions with g-and-h severities following a mixture of g-and-h distributions and mixed Poissonian frequencies with parameters as above are contrasted with the percentiles of an equivalent normal distribution (green line). Table 2.4, graphs 2.7 and 2.8are obtained using losssincelmix.m

36

measure/ VaR CVaR

95% 1,770,355 1,837,688

97.5 % 1,820,537 1,886,774

99 % 1,880,501 1,944,731

99.5% 1,920,866 1,981,561

Table 2.4: Dependence of VaR and CVaR on Quantile mixing internal and external data

Figure 2.8: VaR comparison between g-and-h and normal distributions mixing internal and external data

37

2.6

Computing VaR/CVaR by recursive convolution

Alternatively to a Monte Carlo approach, the value-at-risk at level  for a single cell can be computed by solving numerically the equation FL (x) = , where FL verifies equation (2.17). The series need to be truncated at some value M . Convolution products can be computed recursively following a similar approach than in Lemma 2.4. More precisely, for the case when severities are given by a random variable with g-and-h probability distribution and k  3 the recursion formula follows as:
k (y ) = FX (k-1) (k-1)

R

FX FX

(y - x)dFY (x) =
R

FX

(y - x)fX (x)dx

=
R

(k-1) (k-1)

(y - x)fZ (k -1 (x))(k  (k -1 (x)))-1 dx (y - k (z ))fZ (z )dz
z2

=
R

FX

=

1  2

F (k-1) (y - k (z ))e- 2 dz
R

(2.99)

First, we compute the convolution product of two independent g-and-h probability distributions, cumulative and density functions, according to formulas (2.12), (2.13) and (2.15). The integrals are approximately calculated by the trapezoid rule. It requires to solve for the inverse of function k at several points determined by the truncation interval over the integral. In figures 2.9 and 2.10 the respective two fold cumulative and density convolutions functions are shown. The parameters of the distribution are A = 0, B = 1, g = 2 and h = 0.25. Interval in the trapezoid rule is [-3, 3] while a number of 100 points were used. 38

Figure 2.9: Two fold cumulative convolution function

39

Figure 2.10: Two fold density convolution function

40

The algorithm to compute the VaR goes in the following lines: Algorithm 2.12. (1) Inputs: parameters:A,B,g,h, truncation levels: my,My,mz,Mz,TL,

points in trapezoid:M. (2) Compute the two-fold c.d.f. convolution F 2 . (3) Compute the k-th fold c.d.f.convolution F k from formula (2.99), k = 3, . . . , T L. (4) Compute FL . (5) Solve FL (x) =  to compute V aR . (6) Compute the conditional VaR, ES =
1 1 1-  V

aR d .

When combining external and internal data with truncated values, the algorithm is similar, but now we use equation (2.74) for the p.d.f. Hence:
k (y ) = FX

p

1
-1 ( T1 -A1 )))B (1 -  (k 1 1 B1



2
-1  ( (k1 (k1

.
R

FX

(k-1)

(y - x )e

-1 ( x-A1 ))2 (k1 B1 - 2

x - A1 -1 ))) 1[T1 ,+] (x)dx B1

+

(1 - p )

1
-1 ( T2 -A2 )))B ( 1 -  (k 2 2 B2



2 x - A2 -1 ))) 1[T2 ,+] (x)dx B2

.
R

FX

(k-1)

(y - x )e

-1 ( x-A2 ))2 (k2 B2 - 2

 -1 (k2 (k2 (

Although we have not implemented numerically, the algorithm seems to require a considerable computational effort.

41

2.7

Computing VaR/CVaR by Fast Fourier Transform

Here, we will study a methodology to compute VaR/CVaR via FFT. Note that the characteristic function of the loss process driven by the model described by equation (2.98), denoted by Lt (u) = E (eiuLt ) is computed as follows: Lt (u) = E EeiuLt Nt
Nt

= E(
k=0

E eiuYk = E (E (eiuYk )Nt

= E (Y (u))Nt ) = E (elog(Y (u))Nt ) = E (ei(-i log(Y (u)))Nt ) = Nt (-i log(Y (u))) = et(Y (u)-1)

where Y (u) is the characteristic function of a random variable Y with probability distribution g - and - h(A, B, g, h). On the other hand the characteristic function of the severity Y following a g-and-h distribution can be computed as: Y (u) = eiuA Y (uB ) = = = eiuA B eiuA B eiuA B eiuBx fY (x)dx
R

eiuBx fZ (k -1 (x))(k  (k -1 (x))-1 dx
R

eiuBk(z ) fZ (z )dz
R

(2.100)

By inversion Fourier formula, for x, y > 0:
T e-ixu - e-iyu 1 lim Lt (u)du 2 T + 0 iu T e-ixu - e-iyu 1 lim et(Y (u)-1) du (2.101) 2 T + 0 iu + + 1 1 e-ixu Lt (u)du = e-ixu et(Y (u)-1) du (2.102) 2 0 2 0

FLt (x) - FLt (y ) = = fLt (x) =

42

where the density is understood conditionally on the loss being greater than zero. Efficient methods to compute the integrals above are based on the Fast Fourier Transform, see for example Bailey and Swarztrauber (1993) for details. Moreover, its efficiency can me improved with the use of a Fractionary Fourier Transform (FRFT). The latter has been applied by Chourdakis (2004) in connection to option prices. The method FRFT offers a great advantage over FFT when the c.d.f. is calculated repetitively over only a small interval, as it is the case when we try to compute the VaR solving the equation FLt (x) = . FRFT provides more flexibility in choosing the grid about which the function is to be evaluated. Our approach is based on a double approximation. First, the characteristic function of the loss has to be computed from the integrals in expression (2.100) then approximated by (2.101). It goes in the following lines: eiuA B iuA e B
0 -M 1 M1

eiuBk(z ) fZ (z )dz 
R M1 -M 1
z

eiuA B

M1 -M1

eiuBk(z ) fZ (z )dz
M1 -M 1

(2.103) (2.104) (2.105) (2.106)

= = =

eiuz eiuB (k(z )- B ) fZ (z )dz =
M1 0

eiuz h1 (z )dz

eiuz h1 (z )dz +

eiuz h1 (z )dz
M1

0

e-iuz h1 (-z )dz +

0

eiuz h1 (z )dz

where h1 (z ) =

z 1 iuB (k(z )- B )+iuA fZ (z ). Be

Now, we build a grid of N points: (zk = k )k=0,1,...,N -1 , and the corresponding grid in the space of the characteristic function Y (u) given by (uj )j =0,1,...,N -1 , uj = - N 2  + j , where  and  are the sizes of the subintervals in the corresponding grids.

43

Next using a trapezoidal rule we approximate both integrals at any point uj as follows:
M1 0

eiuj z h1 (z )dz =  

N 0 N -1 k=0 N -1

eiuj z h1 (z )dz eiuj zk h1 (zk )wk = 
N

N -1 k =0

ei(- 2 +j )k h1 (zk )wk
N

= 
k=0 N -1

ei(- 2 +j )k h1 (zk )wk =  eijk h2 (zk )

N -1 k=0

eijk e-i 2 k h1 (zk )wk
N

= 
k=0

where wk = 2 for k = 1, . . . N - 2 and w0 = wN -1 = 1, h2 (zk ) = h2 (k ) = e-i 2 k h1 (zk )wk
N

On the other hand, the inverse discrete FT of the vector h2 = (hk ) is given by: IDF T (h2 ) = 1 N
N -1 k =0

ei N jk h2 (k )
2 N .

2

Then, by identifying the terms we find the relationship between  = In summary we have the approximations:
M1 0 M1 0

eiuj z h1 (z )dz = N IDF T (h2 ) eiuj z h1 (z )dz = DF T (h3 )

where h3 (k ) = h2 (-zk ) and DF T (h3 ) is the discrete FT of the vector h3 . As explained above discrete fractionary FT(DFRT) allows for a more efficient computation of the characteristic function. It generalizes DFT and it is defined for a vector h and   R as:
N -1

DF RTk (h, ) =

e-i2jk h(k )

j =0

44

and its inverse: IDF RTj (h, ) = The choice of  =
1 N

1 N

N -1 k=0

ei2jk h(j )

leads to the standard FT approach. Actual choice of  will depend on

the interval on which the values of the characteristic function is calculated and the precision required. Moreover, as in Bailey and Swarztrauler (1991,1994) the fractionary FT can be calculated by applying three times a standard FT with 2N points. Indeed: DF RTk (h, ) = e-ik
2

 IDF T (DF T (y )  DF T (z ))

where DFT and IDFT and the discrete direct and inverse FT respectively, and the vectors y and z are defined as: y = ((hj e-ij  )j =0,...N -1 , 0j =0,...N -1 )
2

(2.107) (2.108)

z = ((eij  )j =0,...N -1 , (ei(N -j )  )j =0,...N -1 )
2 2

Here v  u represents the Hadammard or componentwise product. Now, we proceed to compute the c.d.f. of the loss Lt given by equation (2.101). Notice that it is continuous everywhere except at x = 0. Moreover, for large values of the parameter t for the intensity of frequencies, as the ones found in Operational Risk context, the jump size at zero is associate with the probability of zero losses is exp(-t) which becomes a negligible quantity. Therefore, for practical purposes the c.d.f. is considered continuous. For a large value M2 , using again a trapezoidal approximation on grids x = {xk = 2 k, k = 0, . . . , N - 1} and 45

(u) = {uj = 2 j, j = 0, . . . , N - 1}. We denote by  ^Lt (u) the estimated characteristic function using the previous procedure. Then: FLt (xk ) - FLt (xk-1 )  
M2 e-ixk-1 u - e-ixk u 1  ^Lt (u)du 2 0 iu 2 N -1 -ixk uj  e-i(xk-1 -xk )uj - 1 e  ^Lt (uj )wj 2 j =0 iuj

=

2 N -1 -ixk uj  e-i2 uj - 1 e  ^Lt (uj )wj 2 j =0 iuj 2 N -1 -i2 2 kj e-i2 uj - 1 e  ^Lt (uj )wj 2 j =0 iuj 2 N -1 -i2 2 kj e h4 (uj ) 2 j =0 2 N -1 -i22 2 kj1 2 e h4 (uj ) = DF RT (h4 , 1 ) 2 j =0 2 2 -ik2 1 e  IDF T (DF T (y1 )  DF T (z1 )) 2 1 =
2 2 2 
2 1

=

=

= = where h4 (uj ) =
e-i2 uj -1  ^Lt (uj )wj , iuj

and

y1 = ((h4 (j )e-ij z1 = ((eij
2 1

)j =0,...N -1 , 0j =0,...N -1 )
2 1

)j =0,...N -1 , (ei(N -j )

)j =0,...N -1 )

Finally, we recover the c.d.f. from the increments by:
N -1

FLt (xj ) = with FLt (x0 ) = FLt (0) = 0.

k=1

(FLt (xk ) - FLt (xk-1 ))

We invoke the procedure above several times to compute the c.d.f. of losses FLt (xj ) in solving the equation FLt (x) =  by Newton-Rhapson to obtain the V aR and the CV aR . The algorithm can be summarized as follows:

46

Algorithm 2.13.

(1) Initialization

(a) Select FRFT parameters: , 1 , 2 1 and 2 .

(b) Select loss distribution parameters: , A, B, g, h

(2) Compute Y (u) in a grid [0, (N - 1)1 ) to produce values  ^Y (u).
^Y (u)-1) (3) Compute  ^Lt (u) = e(

(4) Compute Fk = F (xk ) - F (xk-1 ) in a grid [0, (N - 1)2 ). (5) Compute F (xk ) = k l=1 Fl . (6) Solve FLt (x) =  by Newton-Rhapson to compute V aR . (7) Compute CV aR = by the trapezoidal rule. Alternatively we compute the characteristic function of Y as above then noting that:
k

1 

1 

V aR d

k

(u) j =1 Yk

=
j =1

Y (u) =

eikuA eikuA k  ( uB )) = X Bk Bk

eiuBk(z ) fZ (z )dz
R

k

and an estimated of the characteristic function can be obtained as:  ^Lt (u) = e-t (1 + (t)k  ^Y ) k ! k =1
N1

47

2.8

Computing VaR/CVaR by Panjer recurrence

Panjer's recursion has been introduced by Harry J. Panjer in 1981. By using this recursion,
t we would like to calculate the aggregate loss distribution C of Lt = N i=0 Yi

assuming a discretized severity F . In this section, we will simply review the method to compute the VaR and CVaR.

DISCRETE VERSION We define the sequences:

fk = P(Yi = kh); k  N0 p n  = P (N = n ) ; n  N 0 gn = P(L = nh); n  N0 ;

(2.109) (2.110) (2.111) (2.112)

where N0 = {0, 1, 2,...} For h > 0, the size discretized interval. Yi for i  1, the sequence of i.i.d. severity random variables. N is a discrete random variable, independent of the sequence and L, the compound loss e.g a Poisson distribution. Theorem 2.14. Let a,b  R, with a,b  0. If for n = 1,2,.... and k = 1,....n a distribution follows the recursion

p n = (a + 48

b )pn-1 n

(2.113)

then gn = 1 1 - af0 k (a + b )gn-k fk n k =1
n

(2.114)

and


g0 = wN (f0 ) =

j =0

j pj f0

(2.115)

where wN is the probability generating function of N.

The proof of theorem 2.14 is based on the book of Rolski(1999) , we need three lemmas. Lemma 2.15. For any j  N0 and n = 1,2,...
n

E Y1
n

i=1

Yi = j =

j n

(2.116)

Proof: We set Ln =  Yi . We have:
i= 1

n

nE[X1 Ln = j ] =

i=1

E[Xi Ln = j ]
n

= E

i= 1

Yi Ln = j

= E Ln Ln = j = j
n Lemma 2.16. For any j, k  N0 ; n = 1,2,... and fk denoting the n-fold convolution of fk .

49

 P Y1 = k  Proof:

 fk fj -k Yi = j = n fj  i= 1
n

(n-1)

(2.117)

P(X1 = k Y1 + Y2 + ... + Yn = j ) = =

P(Y1 = k, Y2 + ... + Yn = j - k ) P(Y1 + Y2 + ... + Yn = j ) P(Y1 = k )P(Y2 + ... + Xn = j - k ) P(Y1 + Y2 + ... + Yn = j ) fk fj -k
(n-1) n fj

=

Lemma 2.17. For any j  N0 ;

j k=0

fk fj -k

(n-1)

n = fj

(2.118)

Proof:
n n

1 =
k=0 j

P(Y1 = k P(Y1 = k

i=1 n i=1

Yi = j ) Yi = j )

=
k=0 j

=

k =0

 fk fj -k
n fj

(n-1)

Proof of the theorem. For n=0

50

n f0 (y ) = P(Y1 + Y2 + ... + Yn = 0)

= P(Y1 = 0, Y2 = 0, ..., Yn = 0)
n

=
i=1

P (Y i = 0 )

= P(Y1 = 0)n
n = f0 (Y )

0 = 0 and and for n  1; f0

g 0 = P (L = 0 )
N

= P(

i=1

Yi = 0)
 n=1 n

= P (N = 0 ) +


P

i= 1

Yi = 0 N = n P(N = n)

= p0 +

n=1

P(Yi = 0)n pn

= wN (f0 )
n Also, for n  1; f0 = 1. We have:

51



gj

=
n=0

n pn fj  n=1 n pn fj

0 = p0 fj + 

= 0+
 n=1

a+ a + bE
j

b n pn-1 fj n Y1 j
n i=1

=
n=1 

Yi = j

n pn-1 fj n n Yi = j pn-1 fj

=
n=1

a+b

=

=

=

=

=

=

i=1  ( n - 1 ) j  k fk fj -k n a+b . pn-1 fj n j f n=1 k=0 j j  k (n-1) n .fk fj -k pn-1 afj +b n=1 k=0 j j j  k (n-1) (n-1) fk fj -k +b a .fk fj -k j n=1 k=0 k=0  j k (n-1) a + b. fk fj -k pn-1 j n=1 k=0 j  k (n-1) a + b. fk fj -k pn-1 j n=1 k=0 j  k (n) a + b. fk fj -k pn j n=0 k=0 j

k .P Y1 = k k=0 j

pn-1

=
k=0

a + b.
j

k fk gj -k j a + b.

= af0 gj + Rearranging, we get:

k=1

k fk gj -k j

j

(gj - af0 gj ) =

a+b
k =1

k 1 fk gj -k  gj = j 1 - af0

k (a + b )gj -k fk j k =1

j

The frequency can only have four discrete distributions: Poisson, binomial, negative 52

binomial and geometric. In our case as we decided, we will use the Poisson distribution which means that a = 0 and b = .We have then

pn = 0 +

  n-1 - n - pn-1 = . e = e n n (n - 1 )! n!

(2.119)

Example 3.6. 1. Suppose N  P oi() and fk = P(Yi = k ) =

1 4

for k = 1, 2, 3, 4 and f0 = 0.

P(L = nh) for n = 0, 1, 2, 3, 4. Assume h = 1. This example is very basic to be able to do the calculation by hand using Panjer's recursion.

- (t-1) f0 = 0; f1 = f2 = f3 = f4 = 1 4 . a = 0; b =  ; p0 = e ; wN (t) = e

P (L = n ) = g n =

 n

n k =1

 k.gn-k fk gives us by the recursion:

g0 = wN (f0 ) = wN (0) = e- g1 = g2 = g3 = g4 = =   1 g0 f1 = g0 1 1 4 2  1  g1 f1 + g0 f2 = g1 + g1 2 2 2 4  2 3  1 2 1 g2 f1 + g1 f2 + g0 f3 = g2 + g1 + g1 3 3 3 3 4 3 4  2 3 4 g3 f1 + g2 f2 + g1 f3 + g0 f4 4 4 4 4  1 2 1 3 1 g3 + g2 + g1 + g1 4 4 4 4 4 4 53

With the results obtained, the probability for the loss distribution can be calculated by:     e -         1 gn =  g0  4       1  n -1   k.gn-k + g1     4 n k=1 for n = 0 for n = 1 for n = 2, 3, 4

The algorithm to compute Var/CVaR by Panjer recursion is the following: Algorithm 2.18. (2) For n = 1, 2, ... (a) Calculate fn . (b) Calculate gn =
1 1-af0 k )gn-k fk .  (a + b n n

(1) Initialization: calculate f0 and g0 and set G0 = g0

k=1

(c) Calculate Gn = Gn-1 + gn . (d) Interrupt the procedure if Gn is larger than the require quantile level , e.g.  = 0.999. Then the estimate of the quantile q is n×h (3) Then do an increment (n+1) and return to step 2

54

Chapter 3

COMPUTING VAR/CVAR IN AGGREGATE LOSS

In Banks and Financial Institutions, Losses occurring in a certain Business line may have effects on the others and lead to losses on the others. Thus, we have a dependence existing between the losses of these business lines. To model this dependence, we are introducing in our thesis the concept of copula. Embrechts et al. (2003) introduced an application of copula to Risk Management.

3.1

Copulas

A d dimensional copula is a d variate probability distribution function on [0, 1] × ... × [0, 1] = [0, 1]d with uniform marginal distributions. A copula C (u) as a function may be defined as follow (see Cherubini(2004)). C (u) = C (u1 , u2 , ...ud )  [0, 1]d  [0, 1], u = (u1 , u2 , ..., ud )

The above definition implies that for any given copula C (u) there exists a random vector U = (U1 , U2 , ...Ud ) with Ui , i = 1, ..., d being a uniform random variable on [0, 1] and C (u1 , ..., ud ) = P (U1  u1 , ..., Ud  ud ), ui  [0, 1], i = 1, ..., d

3.1.1

Properties of Copulas

This section presents some useful properties of copulas. 55

(1) Characterization of Copula: A function C (u1 , ...ud ) defined on [0, 1]d is a copula if and only of the following conditions are satisfied. a. C (u1 , ..., ud ) is increasing in each ui  [0, 1] b. C (1, ..., 1, ui , 1, ..., 1) = ui for all i = 1, ..., d c. For all (a1 , ..., ad ), (b1 , ..., bd )  [0, 1]d with ai  bi , i = 1, ..., d,
2 2


i1 = 1 id =1

(-1)i1 +...+id C (u1i1 , ..., udid )  0

where uj 1 = aj and uj 2 = bj for all j = 1, ..., d (2) Sklar Theorem: Let F be a d dimensional PDF with marginal distributions F1 , ..., Fd . Then, there exists a copula C (u1 , ..., ud )  [0, 1]d  [0, 1] such that for all x1 , x2 , ..., xd  [-, ], F (x1 , x2 , ..., xd ) = C (F1 (x1 ), ..., Fd (xd )) (3) Invariance Property: Let (X1 , X2 , ..., Xd ) be a random vector with continuous marginal distributions and copula C . Let T1 , ...Td be strictly increasing functions. Then

(T1 (X1 ), ..., Td (Xd )) also has the copula C .

3.1.2

Fundamental Copulas

This presents some popular copulas and their properties. (1) The Gaussian Copula: Let   Rd×d be a correlation matrix. The Gaussian copula with parameter matrix  is defined by the following equation. C (u) =  -1 (u1 ), ..., -1 (ud ) 56

where -1 is the inverse cumulative distribution function of a standard normal distribution  is is the joint cumulative distribution function of a multivariate normal distribution with mean vector zero and correlation matrix . (2) The T Student Copula: Let   Rd×d be a correlation matrix. The T student copula with parameter matrix  and  degrees of freedom is defined by the following equation.
-1 -1 C, (u) = T, T (u1 ), ..., T (ud )

where T -1 is the inverse cumulative distribution function of univariate central Student T distribution with  degrees of freedom and T, is is the joint cumulative distribution function of multivariate Student T distribution with mean vector zero and correlation matrix . (3) The independence Copula: The independence copula can be defined as follows.

C (u) = u1 u2 ...ud

3.2

Aggregated losses in two cells

We study now the aggregated losses in two cells of the OR matrix corresponding to the loss event of type j . This loss event affects all business lines to a different extend. For the seek of concreteness we fix two cells (j, k ) and (j, m), where (j, k ) reflects the j-th loss event at the k-th business line, j = 1, 2, . . . , 8, k, m = 1, 2, . . . , 7. Loss events and business lines are explained in chapter 2.

57

The losses in the respective cells are given by:
Nt (j,m) (j,m)

Lt (j, m) = Lt (j, k ) = The aggregated loss in both cells is:

l=1 Nt (j,k) l=1

Xl Xl

(3.1) (3.2)

(j,k)

LA t = Lt (j, m) + Lt (j, k )

(3.3)

The analysis of the total aggregated loss follows in a similar way, of course at much more computational effort. We first consider the case of severities given by no-truncated g-and-h distributions coming from internal data. The case of a combination of internal and external truncated data follows in a similar way. In addition to common loss events, in the seven business lines, it is convenient to take into account loss events that affect exclusively a particular business line. Equations (3.1) and (3.2) are rewritten as:
0 Nt

Lt (j, m) = Lt (j, k ) =

l =1
0 Nt

X0,l X0,l

(j,m)

Nt (j,m)

+
l=1 Nt (j,k)

Xl Xl

(j,m)

(3.4) (3.5)

(j,k)

+
l =1

(j,k)

l =1

where (Nt0 )t0 ,(Nt (j, m))t0 and (Nt (j, k ))t0 are independent Poisson processes with respective intensities 0 , jm and jk . The process (Nt0 )t0 accounts for the number of common loss events while (Nt (j, m))t0 and (Nt (j, k ))t0 account for the number of loss events exclusively affecting cells (j, k ) and 58

(j, m) respectively. Severities coming from exclusive events in two different cells are independent. Also severities coming from a single cell are independent and equally distributed. On the other hand severities coming from the same loss event, denoted by Xl Xl
(j,k) (j,m)

and

have marginals coming from a g-and-h distribution with respective parameter set

k = (Ak , Bk , gk , hk ) and m = (Am , Bm , gm , hm ) and dependence driven by a copula Cj . Next, we follow a Monte Carlo approach to compute the VaR and CVaR of the aggregated loss. The main difficulty is in generating the common dependent severities. to this end we consider a conditional copula simulation approach, consisting in generating pairs of random variables (U1 , U2 ) with dependence driven by the copula Cj , by conditioning on the first component and generating the second one using the inverse transform of the conditional distribution , i.e. U2 = C -1 (V1 U1 ), there U1 and V1 are uniform independent random variables in (0, 1). For copulas within the Archimedian class the inverse of the conditional copula is explicitly available. Once the pair (U1 , U2 ) is generated, successive monotone transforms lead to pair of random variables (Z1 , Z2 ) and (X0,l
(j,m)

, X0,l ) with the same copula dependence than the original

(j,k)

uniform random variables and the desired marginal distribution. We summarize the results in the following proposition: Proposition 3.1. Let the loss processes in cells (j, m) and (j, k ), j = 1, 2, . . . , 8, k, m = 1, 2, . . . , 7 be given by equations (3.4) and (3.5) and aggregated loss process given by equation

59

(3.3). Let the pair (U1 , U2 ) of random variables on [0, 1] with dependence driven by the copula Cj and uniform marginals and define the transforms: X1 = A1 + B1 k1 (Z1 ) = A1 + B1 k1 (-1 (U1 )) X2 = A2 + B2 k2 (Z2 ) = A2 + B2 k2 (-1 (U2 )) where: kl (x) = egl x - 1 hl x2 e 2 , x  R , gl > 0 , l = 1 , 2 gl (3.8) (3.6) (3.7)

Then the pair (X1 , X2 ) has a copula dependence Cj and marginal g-and-h p.d.f. with parameters l = (Al , Bl , gl , hl ), l = 1, 2.. Proof. First, notice that kl  -1 is a strictly increasing function as the composition of two strictly increasing functions. By the inverse transform method Zl = -1 (Ul ) has standard Gaussian p.d.f. On the other hand by construction Xl has a g-and-h distribution with parameter l . Finally, strictly increasing transforms preserve the copula dependence. More precisely: FX1 ,X2 (x1 , x2 ) = P (X1  x1 , X2  x2 ) = P (A1 + B1 k1 (-1 (U1 ))  x1 , A2 + B2 k2 (-1 (U2 ))  (3.9) x2 ) x 1 - A1 -1 x2 - A2 , U 2    k2 ) B1 B2 -1 x 1 - A1 -1 x2 - A2 = FU1 ,U2 (  k1 ,   k2 ) B1 B2 -1 x 1 - A1 -1 x 2 - A2 = C (  k1 ,   k2 ) = C (FX1 (x1 ), FX2 (x2 )) B1 B2
-1 = P (U1    k1

(3.10) (3.11) (3.12)

where the last two equalities follow from the relationship between copulas and joint c.d.f. and the expression for the marginal c.d.f. in equation (2.7). 60

Remark 3.2. Once common severities are generated the procedure to simulate losses and to compute VaR and CVaR follow the lines of algorithm 3.9. Remark 3.3. Pairs of random numbers coming from a random vector (U1 , U2 ) with copula C and uniform marginals are obtained using a conditional generation approach. See for example Embrechts, Lindskog and McNeil (2001). The conditional probability distribution is obtained by differentiating the copula with respect to the corresponding variable. In the case of bivariate copulas for 0 < x, y < 1: P (U2  y U1 = x) = lim P (U2  y x  U1 < x + h)
h0

(3.13) (3.14) (3.15) (3.16) (3.17)

P (U2  y, x  U1 < x + h ) h0 P (x  U1 < x + h) P (U2  y, U1  x + h) - P (U2  y, U1  x) = lim ) h0 P (U1  x + h) - P (U1  x) FU ,U (x + h, y ) - FU1 ,U2 (x, y ) = lim 1 2 h0 FU1 (x + h) - FU1 (x) = lim =
FU1 ,U2 (x,y ) x

fU1

=

C (x, y ) x

The algorithm to compute the aggregated VaR and CVaR can be written as: Algorithm 3.4.
0 0 0 0 1.- Initialization: Set N , number of simulated losses, jm = (A0 jm , Bjm , gjm , hjm ),

0 0 0 0 jk = (A 0 jk , Bjk , gjk , hjk ), parameters of the g-and-h distribution for common sever-

ities coming from an event type j at business lines m and k respectively, jm = (Ajm , Bjm , gjm , hjm ), jk = (Ajk , Bjk , gjk , hjk ), parameters of the g-and-h distribution for idiosyncratic severities coming from an event type j at business lines m and k respectively, 0 , jm , jk parameters of the Poisson processes, , level of the Valueat-Risk, T , simulation time. 61

0 2.- Simulate Poisson processes NT , NT (j, m) and NT (j, k ).

3.- Generate (U1 , U2 ) with copula Cj and uniform marginals, following: 3a) Generate independent uniform numbers U1 and V1 in (0, 1). 3b) Set U2 = (D(1) C )-1 (V1 U1 ) 4.- Compute Z1 = -1 (U1 ) and Z2 = -1 (U2 ).
jm jk 5.- Compute g-and-h severities X0 = kjm (Z1 ) and X0 = kjk (Z2 ).

0 0 6.- Repeat steps 3-5 NT times to obtain two 1 × NT vectors of common severity losses in

cells (j, m) and j, k ). 7.- Generate NT (j, m) severities with a g-and-h p.d.f. having parameter jm . 8.- Generate NT (j, k ) severities with a g-and-h p.d.f. having parameter jk . 9.- Compute the loss Lt (j, m) at cell (j, m) by adding common and idiosyncratic losses in steps 7 and 8. 10.- Compute the loss Lt (j, k ) at cell (j, k ) by adding common and idiosyncratic losses in steps 7 and 9. 11.- Compute the aggregate loss LA t = Lt (j, m) + Lt (j, k ). 12.- Repeat steps 6-11 until complete a vector of 1 × N aggregated losses. 13.- Compute the percentile of vector LA t to obtain V aR .

62

14.- Integrate V aR on the interval [, 1] to obtain CV aR . For implementation we specifically take a Clayton copula given by equation: C (u, v, ) = (u- + v - - 1)-  ,  > 0
1

The Clayton copula has low dependence tail, with coefficient on lower dependence tail L = 2-  . Moreover, it has a simple structure, depending on a single parameter, with an
1

explicit expression for the inverse of its first order partial derivative. Thus: D(1) C (v u) = (1 + u (v - - 1))- Inverting, we have:
1  - +1 - 1) + 1)-  ,  > 0 (D(1) C )-  (v u) = (1 + u (v
 1  +1 

, > 0

Therefore step 3b) in the algorithm is done by setting:
 (V 1 U 2 = (1 + U 1
 - + 1

- 1) + 1)- 

1

which guarantees that the pair (U1 , U2 ) has a copula dependence C .

63

 / measure(L)  = [200 100 100]  =[100 200 100]  = [100 100 200]

Mean(L) 100,841,939 70,389,713 70,698,855

STD(L) 13,671,902 10,334,457 25,627,416

Skewness(L) 31.4262 37.2724 59.0799

Excess of Kurtosis(L) 1,437 1,785 3,775

Median(L) 100,277,096 70,110,517 70,061,178

Table 3.1: Statistical Measures of the Aggregate Losses in two cells by Monte Carlo

measure/ VaR CVaR

95% 110,571,658 121,799,690

97.5 % 113,174,873 130,599,525

99 % 116,654,190 169,114,639

99.5% 120,651,194 259,787,044

Table 3.2: Dependence of VaR and CVaR with  = [200, 100, 100] on Quantile 

In table 3.1 we present a summary with relevant statistical information about empirical losses in two cells of the Operational risk Matrix, obtained by Monte Carlo simulation and assuming a Poisson probability distribution for the frequency In table 3.2 results for the Var and CVaR for two cells are shown, using a model with Poisson frequencies of intensity  = [200, 100, 100] and a range of values for the VaR/CVaR levels. VaR is computed by obtaining the correct percentile from the sequence of generated losses. In figure (3.1), perlossint, perlossint2, perlossint3 and perlossint4 respectively represent the variation of the VaR at quantiles 95%, 97.5 %, 99 % and 99.5%.

64

In figure (3.2), corvar1, corvar2, corvar3 and corvar4 respectively represent the variation of the CVaR at quantile 95%, 97.5 %, 99 % and 99.5% and different types of losses.

Results are obtained using losstwocell.m in Appendix 1.

65

18

x 10

8

16

perlossint1 perlossint2 perlossint3 perlossint4

14

12

10 VaR 8 6 4 2 0 95

95.5

96

96.5

97

97.5 Alpha

98

98.5

99

99.5

100

Figure 3.1: Dependence of VaR on quantile 

66

2.5

x 10

10

2

corvar1 corvar2 corvar3 corvar4

1.5 CVaR 1 0.5 0 95

95.5

96

96.5

97 Alpha

97.5

98

98.5

99

Figure 3.2: Dependence of CVaR on quantile 

67

3.3

Aggregated losses with mixed truncated g-and-h probability distribution

Consider now the case where severities in both cells (j, m) and (j, k ) come from a mixing of truncated internal and external data. Now, severities on each cell can be written as a combination of internal and external g-andh random variables truncated at respective thresholds TI and Te . Internal severities are related by a copula CI , while the dependence of external severities is driven by the copula Ce . Internal are external data are independent among them. The following proposition provides the joint c.d.f. and p.d.f. of severities on both cells, suggesting a method to simulate them. Proposition 3.5. Let U1 and U2 two independent random variables on (0, 1), 0 < p < 1 and
e I + 1[U1 >p] X1 Y1 = 1[U1 <p] X1 e I + 1[U2 >p] X2 Y2 = 1[U2 <p] X2

(3.18) (3.19)

e e I I ) are couples of independent random variables indicating in, X2 ) and (X1 , X2 where (X1 I I ) has a depen, X2 ternal and external severities losses in cells 1 and 2. The pair (X1 e e dence given by the copula CI and the pair (X1 , X2 ) has a dependence given by the cop-

ula Ce . The marginal c.d.f. are truncated g-and-h truncated at levels TlI and Tle , i.e.
I I I e e e e e XlI  g - and - h(AI l , Bl , gl , hl , TI ) and Xl  g - and - h(Al , Bl , gl , hl , Te ).

Then, the joint c.d.f. FY1 ,Y2 is given by:
e (x2 ) + FX e (x1 )F I (x2 )]p(1 - p) FY1 ,Y2 (y1 , y2 ) = CI (FX I (x1 ), FX I (x2 ))p2 + [FX I (x1 )FX1 X 1 1 2 1 2

2 e (x1 ), FX e (x2 ))(1 - p) + Ce (FX1 2

68

Proof. From the Law of total Probability:

FY1 ,Y2 (y1 , y2 ) = P(Y1  y1 , Y2  y2 )
I e I I = P(X1  y1 , X2  y2 U1 < p, U2 < p)p2 + P(X1  y1 , X 2  y2 U1 < p, U2  p)p(1 - p) e I e e + P(X1  y1 , X2  y2 U1  p, U2 < p)p(1 - p) + P (X1  y1 , X 2  y2 U1  p, U2  p)(1 - p)2
e (x2 )]p(1 - p) + [FX e (x1 )F I (x2 )]p(1 - p) = CI (FX I (x1 ), FX I (x2 ))p2 + [FX I (x1 )FX1 X 1 1 2 1 2

2 e (x1 ), FX e (x2 ))(1 - p) + Ce (FX1 2
e (x2 ) + FX e (x1 )F I (x2 )]p(1 - p) = CI (FX I (x1 ), FX I (x2 ))p2 + [FX I (x1 )FX1 X 1 1 2 1 2

2 e (x1 ), FX e (x2 ))(1 - p) + Ce (FX1 2

Therefore equation for the joint c.d.f. immediately follows from the relationship between copulas and joint c.d.f.

69

Chapter 4

METHODS FOR PARAMETER ESTIMATION

In the chapter we review some methods for the estimation of parameters in the model. We first consider data coming from a well delimited mixing of internal and external data. The data set consists in observations of the number of internal and external data over the same period, respectively nI and ne respectively. As in previous chapters we assume both frequencies are independent Poisson distributed with parameters I and e . Severities coming form internal data have a common distribution g - and - h(AI , BI , gI , hI ) with known truncation level TI , while external severities have a common distribution g - and - h(Ae , Be , ge , he ) with unknown truncation level Te . The set of internal and external observed severities are x1 , x2 , . . . xne and y1 , y2 , . . . ynI . The parametric space is:  = { = (I , e , Te , I , e ), I = (AI , BI , gI , hI ), e = (Ae , Be , ge , he )}

4.1

Method of Moments

The method of moments is susceptible of application as expression for the moments of a g-and-h are available for moderated values of the skewness parameter h, see for example Iglewicz and Martinez (1984). In the simplest case of non-truncated severity data, well delimited between external and

70

internal observations the problem requires solving four non-linear equation that equal theoretical truncated moments given by equation (2.23) with the empirical four first moments. Hence, assuming the condition h < E (X X > T ) = E (X 2 X > T ) = E (X 3 X > T ) = + E (X 4 X > T ) = + where:
1

1 4

to guarantee the existence of the first four moments:
nI j =1

1 1 BI I1 (TI , I ) = x ¯ I = AI0 (TI , I ) + 2 (1 - (u(T ,  )) g n BI I I I I

xj
nI j =1

2 BI 1 BI 1 2 A I ( T ,  ) + 2 I ( T ,  ) + I2 (TI , I ) = 0 1 I I I I I 2 2 gI nI BI (1 - (u(TI , I )) gI

x2 j

1 2 BI I1 (TI , I ) A3 I I0 (TI , I ) + 3AI 2 gI BI (1 - (u(TI , I )) 3AI
2 3 BI BI 1 I ( T ,  ) + I3 (TI , I ) = 2 I I 2 3 nI gI gI nI j =1

x3 j

2 1 4 3 BI 2 BI I (T ,  ) A I ( T ,  ) + 4 A I ( T ,  ) + 6 A 0 1 I I I I I I I 2 (1 - (u(T ,  )) 2 2 I I gI BI gI I I

4AI

3 4 BI BI 1 I ( T ,  ) + I4 (TI , I ) = 3 I I 3 4 nI gI gI

nI j =1

x4 j

J0 (TI , I ) = 1; J1 (TI , I ) = 1 - exp 2(1-hI )
2

J2 (TI , I ) = 1 - J1 (TI , I ) + exp 1-2hI
9

J3 (TI , I ) = 1 - J2 (TI , I ) + exp 2(1-3hI )
8

J4 (TI , I ) = 1 - J3 (TI , I ) + exp 1-4hI
1 1 AI - T 1 AI - T ); K1 (TI , I ) = K0 (TI , I ) - exp 2(1-hI ) (( - ))(1 - hI ) 2 ) BI BI 1 - hI 2 1 AI - TI 2 K2 (TI , I ) = K1 (TI , I ) + exp 1-2hI (( - ))(1 - 2hI ) 2 ) BI 1 - 2hI 9 1 AI - TI 3 K3 (TI , I ) = K2 (TI , I ) - exp 2(1-3hI ) (( - ))(1 - 3hI ) 2 ) BI 1 - 3hI 8 1 AI - T 4 K4 (TI , I ) = K3 (TI , I ) + exp 1-4hI (( - ))(1 - 4hI ) 2 ) BI 1 - 4hI

K0 (TI , I ) = (

Il (TI , I )) = Jl (TI , I ) - Kl (TI , I ), l = 1, 2, 3, 4 71

or equivalently the parameters solve the equations: BI 2 I1 (TI , I ) - BI (1 - (u(TI , I ))x ¯I = 0 gI 2 BI BI 1 2 A2 I ( T ,  ) + 2 I2 (TI , I ) - BI (1 - (u(TI , I )) I ( T ,  ) + 1 I I I 0 I I 2 gI nI gI AI0 (TI , I ) +
2 BI I1 (TI , I ) A3 I I0 (TI , I ) + 3AI gI 2 3 BI BI 1 2 + 3AI 2 I2 (TI , I ) + 3 I3 (TI , I ) - BI (1 - (u(TI , I )) nI gI gI 3 A4 I I0 (TI , I ) + 4AI 2 BI BI I1 (TI , I ) + 6A2 I 2 I2 (TI , I ) gI gI nI j =1

(4.1)
nI j =1

x2 (4.2) j =0 (4.3)

nI j =1

x3 j =0

(4.4) (4.5)

+ 4AI

4 3 BI BI 1 2 I ( T ,  ) + I (T ,  ) - B I (1 - (u(TI , I )) 3 I I 3 4 4 I I nI gI gI

x4 j =0

(4.6)

with similar equations for external data.

4.2

Maximum Likelihood Estimation

The likelihood, after neglecting terms not depending on  can be written as:

l(xI , xe , ) = log L(xI , xe , ) = log L(xI , I ) - I + nI log(I ) + log L(xe , e , Te ) - e + ne log(e ) = l(xI , I ) - I + nI log(I ) + l(xe , e , Te ) - e + ne log(e )

As the likelihood splits into four well delimited terms, each one depending on a different part of the parameter vector, they can be treated separately.

72

We first focus on the part corresponding to the internal data. Then from equation:
nI (k -1 ( xm -AI ))2 T - AI BI l(xI , I ) = - log(BI )nI - log(1 - (k ( )))nI - BI 2 m=1 -1 nI

-
m=1

log(k  (k -1 (

x m - AI )))-1 BI (u2 (xm , I ) 2 m=1
nI

= - log(BI )nI - log(1 - (u(TI , I )))nI -
nI

-
m=1

log(k  (u(xm , I )))-1

where u(x, I ) solves: BI k (u( x, I )) + AI - TI = BI Similarly:
ne (k -1 ( xm -Ae ))2 T - Ae Be l(xe , e ) = - log(Be )ne - log(1 - (k ( )))ne - Be 2 m=1 -1 ne

egu(x,I ) - 1 1 hu2 (x,I ) e2 + AI - TI = 0 g

-
m=1

log(k  (k -1 (

x m - Ae -1 ))) Be (u2 (xm , e ) 2 m=1
ne

= - log(Be )ne - log(1 - (u(Te - e )))ne -
ne

-
m=1

log(k  (u(xm , e )))-1

where vk (e ) solves: Be k (v (x, e )) + Ae - Te = Be egv(x,e ) - 1 1 hv2 (x,e ) e2 + Ae - Te = 0 g

The estimations of I and e are computed separately, as they do not depend on the ^I = remainder parameter it easily follows that 
nI PI

^e = and 

ne Pe ,

where PI and Pe are the

length of the periods in which the process is observed. To simplify we take PI = Pe = 1.

73

By differentiating with respect to the parameters we obtain the following set of normal equations, using chain rule and implicit differentiation: l(xI , I ) AI
nI (u2 (xm , I ) fZ (u(TI , I ))  u(TI , I )) k (u(TI , I )) nI - (1 - (uk (I )) AI 2 m=1

= - -

nI m=1

log(k  (u(xm , I )))-1

and similar expressions for other parameters. Here k  (x) = + =
1 1 2 2 1 1 [g (g + hx)egx + hegx - h] e 2 hx + [g (g + hx)egx + hegx - h] e 2 hx g g 1 2 1 ((g + hx)egx - hx)hxe 2 hx g 1 1 hx2 g (g + hx) + h + (g + hx)egx - h2 x2 - h e2 g

^ = argmin l(x, ), i.e. the Maximum likelihood estimators are defined as the vector  value that minimizes the log-likelihood for all values of  in the parametric space. They solve the following system of equations: l(xI , I ) AI l(xI , I ) gI l(xI , I ) =0 BI l(xI , I ) = 0; =0 hI = 0;

74

Chapter 5

CONCLUSION

In our thesis, under the framework of the Loss Distribution Approach, we explored various methods to compute the Value-at-Risk and Expected Shortfall. Due to the heavy-tailedness and the complexity of the g-and-h severity, we have faced some numerical challenges. Nevertheless, we implemented some of them and verified their accuracy in a way that was not done before. After exploring the one cell case, we analyzed the two cells case while considering and modeling the dependency among cells via copula.For the single cell case:

1) We explored and implemented the Monte Carlo algorithm for internal and mixed data, 2) we reviewed the properties of the recursive convolution and applied truncation to the algorithm for mixed data, 3) We explored the Fractionary Fourier Transform algorithm, and finally, 4) We reviewed the theorem and algorithm of Panjer's recurrence. For the two cells case, we used the Monte Carlo Simulation to Compute The VaR and CVaR. We used the Clayton Copula, which has a simple structure with an explicit expression for the inverse of its first order partial derivative, to model the dependence among cells. Previous studies have proven that Monte Carlo is very reliable. Fast Fourier Transform is 75

quick but is dependent on the parameters chosen. A wrong estimation of these parameters could lead to erroneous results. One could compare FFT and Monte Carlo to obtain accurate parameters. We saw that the methods of computing the two risk measures of our analysis, namely VaR and ES/CVaR were actually proven and good methods for stable risk measures. One could think in our case (heavy-tail severity) that CVaR, given the fact that it is higher, is better since the end goal is for the banks to have enough capital reserve to avoid bankruptcy. Also, in the future perhaps, one could find more results on the g-and-h distribution and use the methods introduced in this analysis.

76

Appendices

77

Appendix A

ONE CELL CASE

(1) densitydependgh.m plots the density functions of g-and-h dependeding on parameters g and h

1 2 3 4 5 6 7 8 9 10 11 12 13 14

%r u n n i n g d i f f e r e n t d e n s i t y g - and - h A=0; B=1; gv = [ 1 , 2 , 3 ] ; h=0 . 2 5 ; hv=[ . 2 , . 2 5 , . 5 , . 8 ] ; g =2; f g 1=d e n s i g a n d h (A, B, gv ( 1 ) , h ) ; f g 2=d e n s i g a n d h (A, B, gv ( 2 ) , h ) ; f g 3=d e n s i g a n d h (A, B, gv ( 3 ) , h ) ; f h 1=d e n s i g a n d h (A, B, g , hv ( 1 ) ) ; f h 2=d e n s i g a n d h (A, B, g , hv ( 2 ) ) ; f h 3=d e n s i g a n d h (A, B, g , hv ( 3 ) ) ; x=l i n s p a c e ( - 5 , 5 , 1 0 0 ) ; p l o t ( x , fg1 , x , fg2 , x , fg3 ) ; p l o t ( x , fh1 , x , fh2 , x , f h 3 ) f n=d e n s i g a n d h (A, B, g , h ) ; [ y , f s ] = gandhsim (A, B, g , h , 1 0 0 0 0 0 ) ;

and below is the code gandhsim.m of the function gandhsim

78

1 2 3 4 5 6 7 8

%g e n e r a t e n numbers coming from a g - and - h d i s t r i b u t i o n ( y ) % and compute t h e e m p i r i c a l d e n s i t y by ks f i l t e r ( normal k e r n e l ) %i n p u t s : p a r a m e t e r s : a , b , g , h ; n : n o . o f v a l u e s f u n c t i o n [ y , f ] = gandhsim ( a , b , g , h , n ) rn=randn ( 1 , n ) ; y=a+b  ( ( exp ( g  rn ) - 1 ) .  exp ( h  r n . ^ 2 / 2 ) ) . / g ; xi = linspace ( -5 ,5 ,100) ; f = k s d e n s i t y ( y , x i , ' f u n c t i o n ' , ' pdf ' ) ;

(2) losssincel.m returns the loss, the VaR and the CVaR for a single cell

1 2 3 4

f u n c t i o n [ L , f , o r v a r , e s h ]= l o s s s i n c e l (A, B, g , h , lambda , alpha , n ) %monte c a r l o s i m u a l t i o n l o s s i n a s i n g l e c e l l , %f r e q P o i s s o n , s e v g - and - h , n : n o . o f l o s s e s %t y p i c a l data i n o r : h=0 . 2 5 ; g =2; A=100000; B=1; lambda =100; a l p h a = 95

5 6 7 8 9 10 11 12 13

N=p o i s s r n d ( lambda , 1 , n ) ; L=z e r o s ( 1 , n ) ; f o r k=1:n rn=randn ( 1 ,N( k ) ) ; y=A+B  ( ( exp ( g  rn ) - 1 ) .  exp ( h  r n . ^ 2 / 2 ) ) . / g ; L( k )=sum ( y ) ; end xx = l i n s p a c e ( 0 , max(L) , 3 0 0 ) ; f = k s d e n s i t y (L , xx , ' f u n c t i o n ' , ' pdf ' ) ;

79

14 15 16 17 18 19 20 21 22 23 24 25 26

%f n o r=pdf ( ' Normal ' , xx , mean (L) , s t d (L) ) ; %p l o t ( xx , f , xx , f n o r ) LS=(L - mean (L) ) . / s t d (L) ; h=k s t e s t ( LS ) ; o r v a r=p r c t i l e (L , a l p h a ) ; normvar1=norminv ( 0 . 0 1  alpha , mean (L) , s t d (L) ) ; a l f=l i n s p a c e ( 9 5 , 9 9 . 5 , 1 0 0 ) ; %comparing v a r s with p o i s s o n - gandh and normal d i s t r i b u t i o n pernorm=norminv ( 0 . 0 1  a l f , mean (L) , s t d (L) ) ; p e r l o s s=p r c t i l e (L , a l f ) ; i i n t=l i n s p a c e ( ( a l p h a . / 1 0 0 ) , 1 , 1 0 0 ) ; p e r l o s s i n t=p r c t i l e (L , i i n t  1 0 0 ) ;%VaR on p e r c e n t i l e s d e f i n e d by i i n t e s h=t r a p z ( i i n t , p e r l o s s i n t ) . / ( 1 - a l p h a . / 1 0 0 ) ;%computing e x p e c t e d shortfall

(3) losssincelmix.m returns the loss, the VaR and the CVaR for a single cell mixing internal and external data

1

f u n c t i o n [ L , f , o r v a r , e s h ]= l o s s s i n c e l m i x (A1 , B1 , g1 , h1 , lambda , A2 , B2 , g2 , h2 , p , alpha , n )

2 3 4 5

%monte c a r l o s i m u a l t i o n l o s s i n a s i n g l e c e l l , %f r e q mixed P o i s s o n , s e v two g - and - h , n : n o . o f l o s s e s %p : p r o b a b i l i t y o f s e l e c t i n g t h e f i r s t o f t h e s e v e r i t i e s %t y p i c a l data i n o r : h1=0 . 2 5 ; g1 =2; A1=10000; B1=1; lambda =100; p=0 . 5 ; h2=0 . 3 ; g2 =2; A2=20000; B2=1; lambda =100; p=0 . 5 ; a l p h a=0 . 9 5

80

6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22

L1=l o s s s i n c e l (A1 , B1 , g1 , h1 , p  lambda , alpha , n ) ; L2=l o s s s i n c e l (A2 , B2 , g2 , h2 , ( 1 - p )  lambda , alpha , n ) ; L=L1+L2 ; xx = l i n s p a c e ( 0 , max(L) , 3 0 0 ) ; f = k s d e n s i t y (L , xx , ' f u n c t i o n ' , ' pdf ' ) ; %f n o r=pdf ( ' Normal ' , xx , mean (L) , s t d (L) ) ; %p l o t ( xx , f , xx , f n o r ) %LS=(L - mean (L) ) . / s t d (L) ; %h=k s t e s t ( LS ) ; o r v a r=p r c t i l e (L , a l p h a  1 0 0 ) ; normvar1=norminv ( alpha , mean (L) , s t d (L) ) ; a l f=l i n s p a c e ( 9 5 , 9 9 . 5 , 1 0 0 ) ; %comparing v a r s with p o i s s o n - gandh and normal d i s t r i b u t i o n pernorm=norminv ( 0 . 0 1  a l f , mean (L) , s t d (L) ) ; p e r l o s s=p r c t i l e (L , a l f ) ; i i n t=l i n s p a c e ( alpha , 1 , 1 0 0 ) ; %D e s i r e m u l t i p l i e d a l p h a by 100 p e r l o s s i n t=p r c t i l e (L , i i n t  1 0 0 ) ;%VaR on p e r c e n t i l e s d e f i n e d by i i n t % d e s i r e d i v i d e d i i n t by 100

23

e s h=t r a p z ( i i n t , p e r l o s s i n t ) . / ( 1 - a l p h a ) ;%computing e x p e c t e d s h o r t f a l l

81

Appendix B

TWO CELLS CASE

losstwocell.m computes the aggregate loss of two cell, then the VaR and CVaR

1 f u n c t i o n [ LA, o r v a r , c o r v a r ]= l o s s t w o c e l l ( lambda , AV,BV, gv , hv , t h e t a , alpha , n ) 2 3 %monte c a r l o s i m u a l t i o n a g g r e g a t e l o s s i n two c e l l s , common l o s s e s a r e 4 %g e n e r a t e d from a c l a y t o n c o p u l a 5 %i n p u t s : f r e q P o i s s o n 1 x3 v e c t o r , s e v g - and - h p a r a m e t e r s AV,BV, gV 6 %hv , each a 1 x4 v e c t o r , n : n o . o f s i m u l a t e d l o s s e s 7 % t h e t a : param c a l y t o n c o p u l a 8 %t h e t a : parameter c l a y t o n c o p u l a 9 %t y p i c a l data i n o r : lambda =[200 100 1 0 0 ] ; AV=[200000 200000 100000 1 0 0 0 0 0 ] ; BV=[2 2 1 1 ] ; gv =[ 3 3 2 2 ] ; hv =[0 . 5 0 . 5 0 . 2 5 0 . 2 5 ] ; t h e t a =5; a l p h a=0 . 9 9 ; 10 t h e t a0 j m =[AV( 1 ) , BV( 1 ) , gv ( 1 ) hv ( 1 ) ] ; 11 t h e t a 0 j k =[AV( 2 ) , BV( 2 ) , gv ( 2 ) hv ( 2 ) ] ; 12 t h e t a j m =[AV( 3 ) , BV( 3 ) , gv ( 3 ) hv ( 3 ) ] ; 13 t h e t a j k =[AV( 4 ) , BV( 4 ) , gv ( 4 ) hv ( 4 ) ] ; 14 N0=p o i s s r n d ( lambda ( 1 ) , 1 , n ) ; 15 N1=p o i s s r n d ( lambda ( 2 ) , 1 , n ) ; 16 N2=p o i s s r n d ( lambda ( 3 ) , 1 , n ) ; 17 LAjmp=z e r o s ( 1 , n ) ;

82

18 LAjkp=z e r o s ( 1 , n ) ; 19 LA0jm=z e r o s ( 1 , n ) ; 20 LA0jk=z e r o s ( 1 , n ) ; 21 LAjk=z e r o s ( 1 , n ) ; 22 LAjm=z e r o s ( 1 , n ) ; 23 LA=z e r o s ( 1 , n ) ; 24 f o r k=1:n 25 [ l o s s 0 j m , l o s s 0 j k ]= l o s s s 0 ( theta0jm , t h e t a 0 j k , t h e t a , N0( k ) ) ; 26 l o s s j m=l o s s s g a n d h ( thetajm , N1( k ) ) ; 27 l o s s j k=l o s s s g a n d h ( t h e t a j k , N2( k ) ) ; 28 LAjmp( k )=sum ( l o s s j m ) ; 29 LAjkp ( k )=sum ( l o s s j k ) ; 30 LA0jm ( k )=sum ( l o s s 0 j m ) ; 31 LA0jk ( k )=sum ( l o s s 0 j k ) ; 32 LAjm( k )=LAjmp( k )+LA0jm ( k ) ; 33 LAjk ( k )=LAjkp ( k )+LA0jk ( k ) ; 34 LA( k )=LAjm( k )+LAjk ( k ) ; 35 end 36 xx = l i n s p a c e ( 0 , max(LA) , 3 0 0 ) ; 37 f = k s d e n s i t y (LA, xx , ' f u n c t i o n ' , ' pdf ' ) ; 38 %f n o r=pdf ( ' Normal ' , xx , mean (L) , s t d (L) ) ; 39 %p l o t ( xx , f , xx , f n o r ) 40 LS=(LA- mean (LA) ) . / s t d (LA) ; 41 h=k s t e s t ( LS ) ; 42 o r v a r=p r c t i l e (LA, 1 0 0  a l p h a ) ; 43 normvar1=norminv ( alpha , mean (LA) , s t d (LA) ) ;

83

44 a l f=l i n s p a c e ( 9 5 , 9 9 . 5 , 1 0 0 ) ; 45 %comparing v a r s with p o i s s o n - gandh and normal d i s t r i b u t i o n 46 pernorm=norminv ( 0 . 0 1  a l f , mean (LA) , s t d (LA) ) ; 47 p e r l o s s=p r c t i l e (LA, a l f ) ; 48 i i n t=l i n s p a c e ( alpha , 1 , 1 0 0 ) ; 49 p e r l o s s i n t=p r c t i l e (LA, 1 0 0  i i n t ) ;%VaR on p e r c e n t i l e s d e f i n e d by i i n t 50 c o r v a r=t r a p z ( i i n t , p e r l o s s i n t ) . / ( 1 - a l p h a ) ;%computing e x p e c t e d s h o r t f a l l

The functions loss0 and lossgandh (1) loss0.m

1 f u n c t i o n [ l o s s 0 1 , l o s s 0 2 ]= l o s s s 0 ( t h e t a 0 1 , t h e t a 0 2 , t h e t a , n ) 2 %monte c a r l o s i m u a l t i o n l o s s e s i n two c e l l s , common l o s s e s a r e

3 %g e n e r a t e d from a c l a y t o n c o p u l a 4 %i n p u t s : lambda , f r e q P o i s s o n 5 %t h e t a 0 2 i n both c e l l s 6 % n : no. of simulated l o s s e s 7 % t h e t a : param c a l y t o n c o p u l a 8 %t y p i c a l data i n o r : hv =[0 . 5 AV=[10000 20000]; n = 500; BV=[1 1 ] ; gv =[3 2]; , s e v g - and - h p a r a m e t e r s t h e t a 0 1 and

0 . 2 5 ] ; t h e t a 0 1 =[AV( 1 ) ,BV( 1 ) , gv ( 1 ) , hv ( 1 ) ] ;

9 %t h e t a 0 2 =[AV( 2 ) ,BV( 2 ) , gv ( 2 ) , hv ( 2 ) ] ; t h e t a =10; 10 u=c o p u l a r n d ( ' Clayton ' , t h e t a , n ) ; 11 z=norminv ( u , 0 , 1 ) ; 12 l o s s 0 1 p=t h e t a 0 1 ( 1 )+t h e t a 0 1 ( 2 )  ( ( exp ( t h e t a 0 1 ( 3 )  z ( : , 1 ) ) - 1 ) .  exp ( t h e t a 0 1 ( 4 ) z ( : , 1 ) . ^2/2) ) . / theta01 (3) ;

84

13 l o s s 0 2 p=t h e t a 0 2 ( 1 )+t h e t a 0 2 ( 2 )  ( ( exp ( t h e t a 0 2 ( 3 )  z ( : , 2 ) ) - 1 ) .  exp ( t h e t a 0 2 ( 4 ) z ( : , 2 ) . ^2/2) ) . / theta02 (3) ; 14 l o s s 0 1=l o s s 0 1 p .  ( l o s s 0 1 p > 0) ; 15 l o s s 0 2=l o s s 0 2 p .  ( l o s s 0 2 p > 0) ; 16 %p l o t ( l o s s 0 1 , l o s s 0 2 , ' . ' )

(2) lossgandh.m

1 f u n c t i o n L=l o s s s g a n d h ( thetajm , n ) 2 %monte c a r l o s i m u a l t i o n l o s s i n a s i n g l e c e l l , 3 %o u t p u i t : n s e v e r i t y l o s s e s from a g - and - h d i s t r i b u t i o n with param t he ta j m 4 %i n p u t s : lambda , f r e q P o i s s o n , th et a jm s e v g - and - h parameters , n : n o . o f losses 5 %t y p i c a l data i n o r : h=0 . 2 5 ; g =2; A=100000; B=1; lambda=100 6 %t he t aj m =[A, B, g , h ] ; 7 z=randn ( 1 , n ) ; 8 L=th et a jm ( 1 )+t h et aj m ( 2 )  ( ( exp ( t h et aj m ( 3 )  z ) - 1 ) .  exp ( t he ta j m ( 4 )  z . ^ 2 / 2 ) ) . / t he ta j m ( 3 ) ;

85

REFERENCES

[1] Antoine Frachot, Pierre Georges, and Thierry Roncalli. Loss distribution approach for operational risk. March 2001. Working Paper, Cr´ edit Lyonnais, Groupe de Recherche Op´ erationnelle. [2] Antoine Frachot, Thierry Roncalli, and Eric Salomon. The Correlation Problem in Operational Risk. January 2004. [3] Basel Committee on Banking Supervision, Enhancing Bank Transparency - Public disclosure and supervisory information that promote safety and soundness in banking systems; September 1998. [4] Basel Committee on Banking Supervision. International Convergence of Capital Measurement and Capital Standards: A Revised Framework. Bank for International Settlements, Basel, June 2006. [5] Basel Committee on Banking Supervision. Operational Risk Management. Bank for International Settlements, Basel, September 1998. [6] Basel Committee on Banking Supervision. Operational Risk: Supporting Document to the New Basel Capital Accord. Bank for International Settlements, Basel,January 2001.

86

[7] Bjørn Sundt and William S. Jewell. Further results on recursive evaluation of compound distributions. Astin Bulletin, 12:27-39, 1981. [8] Boris Iglewicz and Jorge Martinez. Some properties of the tukey g and h family of distributions. Communications in Statistics - Theory and Methods, 13(3):353-369, 1984. [9] Bruno Dupire. Monte Carlo Methodologies and Applications for Pricing and Risk Management. Risk Books, London, 1998. [10] Christian P. Robert and George Casella. Monte Carlo Statistical Methods. Springer Texts in Statistics. Springer-Verlag, Secaucus, NJ, USA, 2nd edition, August 2004. [11] Christian P. Robert. Simulation of truncated normal variables. Statistics and Computing, 5(2):121-125, 1995. [12] Daren B. H. Cline. Convolutions of distributions with exponential and subexponential tails. Journal of the Australian Mathematical Society. Series A, 43:347-365, 1987. [13] Dirk P. Kroese, Thomas Taimre, and Zdravko I. Botev. Handbook of Monte Carlo methods. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ, USA, 2011. [14] Dominik Kortschak and Hansj¨ org Albrecher. Asymptotic results for the sum of dependent non-identically distributed random variables. Methodology and Computing in Applied Probability, 11(3):279-306, 2008. [15] Edward Omey and Eric Willekens. Second order behaviour of the tail of a subordinated 87

probability distribution. Stochastic Processes and their Applications, 21 (2):339-353, 1986. [16] Eric Willekens. Asymptotic approximations of compound distributions and some applications. Bulletin de la Soci´ et´ e Math´ ematique de Belgique. S´ e rie B, 41(1): 55-61, 1989. [17] Falko Aue and Michael Kalkbrener. LDA at work: Deutsche banks approach to quantifying operational risk. Journal of Operational Risk, 1(4):49-93, 2006. [18] Gareth W. Peters, Adam M. Johansen, and Arnaud Doucet. Simulation of the annual loss distribution in operational risk via Panjer recursions and volterra integral equations for value-at-risk and expected shortfall estimation. Journal of Operational Risk, 2(3):29-58, 2007. [19] Garold J. Borse. Numerical Methods with MATLAB: A Resource for Scientists and Engineers. International Thomson Publishing, 1st edition, 1996. [20] George S. Fishman. Monte Carlo: Concepts, algorithms, and applications. Springer Series in Operations Research. Springer-Verlag, New York, NY, USA, 1996. [21] Hansjørg Albrecher, Christian Hipp, and Dominik Kortschak. Higher-order expansions for compound distributions and ruin probabilities with subexponential claims. Scandinavian Actuarial Journal, 2010(2):105-135, 2010. [22] Harry H. Panjer. Recursive evaluation of a family of compound distributions. Astin Bulletin, 12:22-26, 1981. 88

[23] Irving W. Burr. Cumulative frequency functions. The Annals of Mathematical Statistics, 13(2):215-232, 1942. [24] Jean-Michel Marin, Kerrie Mengersen, and Christian P. Robert. Bayesian modeling and inference on mixtures of distributions. In Dipak K. Dey and C. R. Rao, editors, Handbook of Statistics, volume 25, chapter 16, pages 459-507. Springer, New York, NY, USA, 2005. [25] Jim Pitman. Probability. Springer texts in statistics. Springer-Verlag, New York, NY, USA, 1993. [26] John P. Robertson. The computation of aggregate loss distributions. Proceedings of the Casualty Actuarial Society, 79(1):57-133, 1992. [27] Kabir Dutta and Jason Perry. A tale of tails: An empirical analysis of loss distribution models for estimating operational risk capital. Working Paper 06-13, Federal Reserve Bank of Boston, April 2007. [28] Klaus B¨ ocker and Claudia Kl¨ uppelberg. Operational VAR: a closed-form approximation. Risk, pages 90-93, December 2005. . [29] Leonid Koralov and Yakov G. Sinai. Theory of Probability and Random Processes. Springer, 2nd edition, September 2007. [30] M. C. Jones. The relationship between moments of truncated and original distributions plus some other simple structural properties of weighted distributions. Metrika, 37(34):233-243, 1990. 89

[31] Matthew Pritsker. Evaluating value at risk methodologies: Accuracy versus computational time. Journal of Financial Services Research, 12(2):201-242, 1997. [32] Matthias Degen, Paul Embrechts, and Dominik D. Lambrigger. The quantitative modeling of operational risk: Between g-and-h and evt. Risk and Insurance, 37(2):1-32, 2007. [33] Merran Evans, Nicholas Hastings, and Brian Peacock. Statistical Distributions. WileyInterscience, June 2000. [34] Nicholas Metropolis and Stan Ulam. The Monte Carlo method. Journal of the American Statistical Association, 44(247):335-341, 1949. [35] Luo, X.L., Shevchenko, P.V. and Donnelly J.B. (2007). Addressing the impact of data truncation and parameter uncertainty on operational risk estimates. The Journal of Operational Risk 2(4), 3-26. [36] Paul Glasserman. Monte Carlo Methods in Financial Engineering, volume 53. Springer, 2004. [37] Pavel V. Shevchenko. Implementing loss distribution approach for operational risk. Applied Stochastic Models in Business and Industry, 26(3):277-307, 2010. [38] Pavel V. Shevchenko. Modelling Operational Risk Using Bayesian Inference, 2011. [39] Philippe Barbe and William P. McCormick. Asymptotic expansions for infinite weighted

90

convolutions of heavy tail distributions and applications. Memoirs of the American Mathematical Society, 2004. [40] Philippe Barbe and William P. McCormick. Asymptotic expansions for infinite weighted convolutions of rapidly varying subexponential distributions. Probability Theory and Related Fields, 141(1-2):155-180, 2008. [41] R. Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions. Journal of Banking & Finance, 26(7):1443-1471, 2002. [42] Rayner, G.D. and MacGillivray, H.L. Numerical maximum likelihood estimation for the g-and-k and generalized g-and-h distributions. Statistics and Computing, 2002, 12,1,57-75. [43] Rhonda K. Kowalchuk and Todd C. Headrick. Simulating multivariate g-and-h distributions. The British journal of mathematical and statistical psychology, 63(1):63-74, 2010. [44] Roger B. Nelsen. An Introduction to Copulas. Springer Series in Statistics. Springer, 2nd edition, January 2006. [45] Rupert Brotherton-Ratcliffe. Monte carlo motoring. In Bruno Dupire, editor, Monte Carlo Methodologies and Applications for Pricing and Risk Management, pages 331334. Risk Books, London, 1998. [46] Søren Asmussen. Ruin probabilities. Advanced series on statistical science and applied probability, 2. World Scientific, Singapore, 2000. 91

[47] Sergey Foss, Dmitry Korshunov, and Stan Zachary. An Introduction to Heavy-Tailed and Subexponential Distributions, volume 38 of Springer Series in Operations Research and Financial Engineering. Springer, 2011. [48] Sergey Foss, Dmitry Korshunov, and Stan Zachary. Convolutions of long-tailed and subexponential distributions. Journal of Applied Probability, 46(3):756-767, 2009. [49] Sheldon M. Ross. Introduction to probability and statistics for engineers and scientists. Academic Press, 4th edition, 2009. [50] Todd C. Headrick, Rhonda K. Kowalchuk, and Yanyan Sheng. Parametric probability densities and distribution functions for tukey g-and-h transformations and their use for fitting data. Applied Mathematical Sciences, 2(9):449-462, 2008. [51] Umberto Cherubini, Elisa Luciano and Walter Vecchiato. Copula methods in finance. [52] Wikipedia - Fractional Fourier transform. [53] William Feller. An introduction to probability theory and its applications. Vol. II. Second edition. John Wiley & Sons Inc., New York, NY, USA, 1971.

92

