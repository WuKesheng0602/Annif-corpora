ORDER SELECTION IN UNSUPERVISED LEARNING AND CLUSTERING FOR ARBITRARY AND NON-ARBITRARY SHAPED DATA

by Mahdi Shahbaba M.Sc. Boras University, Sweden, 2010

A dissertation presented to Ryerson University in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2015 c Mahdi Shahbaba 2015

Author's Declaration

I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my dissertation may be made electronically available to the public.

ii

Abstract

Order Selection in Unsupervised Learning and Clustering for Arbitrary and Non-arbitrary Shaped Data
Mahdi Shahbaba Doctor of Philosophy, Electrical and Computer Engineering Ryerson University, 2015
This thesis focuses on clustering for the purpose of unsupervised learning. One topic of our interest is on estimating the correct number of clusters (CNC). In conventional clustering approaches, such as X-means, G-means, PG-means and Dip-means, estimating the CNC is a preprocessing step prior to finding the centers and clusters. In another word, the first step estimates the CNC and the second step finds the clusters. Each step having different objective function to minimize. Here, we propose minimum averaged central error (MACE)-means clustering and use one objective function to simultaneously estimate the CNC and provide the cluster centers. We have shown superiority of MACEmeans over the conventional methods in term of estimating the CNC with comparable complexity. In addition, on average MACE-means results in better values for adjusted rand index (ARI) and variation of information (VI). Next topic of our interest is order selection step of the conventional methods which is usually a statistical testing method such as Kolmogrov-Smrinov test, Anderson-Darling test, and Hartigan's Dip test. We propose a new statistical test denoted by Sigtest (signature testing). The conventional statistical testing approaches rely on a particular assumption on the probability distribution of each cluster. Sigtest on the other hand can be used with any prior distribution assumption on the clusters. By replacing the statistical testing of the mentioned conventional approaches with Sigtest, we have shown that the clustering methods are improved in terms of having more accurate CNC as well as ARI and VI. Conventional clustering approaches fail in arbitrary shaped clustering. Our last contribution of the thesis is in arbitrary shaped clustering. The proposed method denoted by minimum Pathways in iii

Arbitrary Shaped (minPAS) clustering is proposed based on a unique minimum spanning tree structure of the data. Our simulation results show advantage of minPAS over the state-of-the-art arbitrary shaped clustering methods such as DBSCAN and Affinity Propagation in terms of accuracy, ARI and VI indexes.

iv

Acknowledgments

First and foremost, I would like to express my special gratitude to my supervisor Professor Soosan Beheshti for sharing her knowledge and wisdom with me, encouraging my research, and supporting me during these past four years. I was tremendously fortunate to have her as my supervisor. My sincere gratitude goes to my committee members, Professors Ebrahim Bagheri, Matthew Kyan, Alireza Sadeghian, Amirnaser Yazdani, and Shahryar Rahnamayan for their brilliant comments and suggestions. I also want to thank Professor Jean Mason for her availability and assistance during my defense. I would also like to thank my parents, my wife, my brother, my sister, my brothersin-law, and my sisters-in-law for their continued support and love. This accomplishment would not have been possible without their help. Special thanks to my colleagues and friends in Signal and Information Processing (SIP) lab for their insightful discussions and collaborations. I would like to acknowledge the Natural Sciences and Engineering Research Council for providing funding for this work.

v

To my wife Maryam, for all of her love and support

vi

Table of Contents
1 Introduction 2 Background 2.1 1 9

Hierarchical and Partitional Clustering Methods . . . . . . . . . . . . . . 10 2.1.1 2.1.2 2.1.3 2.1.4 2.1.5 2.1.6 K-means . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . 11 X-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 17 G-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 19 PG-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 20 Dip-means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.2

Statistical Tests in Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.1 2.2.2 2.2.3 Kolmogorov-Smirnov Test . . . . . . . . . . . . . . . . . . . . . . . 22 Anderson Darling Test . . . . . . . . . . . . . . . . . . . . . . . . . 23 Haritagn's Dip test . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

2.3 2.4

Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . . 25 Arbitrary Shaped Clustering Methods . . . . . . . . . . . . . . . . . . . . 30 2.4.1 2.4.2 2.4.3 2.4.4 Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Normalized Cut Clustering . . . . . . . . . . . . . . . . . . . . . . 31 Voting-K-means . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 DBSCAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 vii

2.4.5

Affinity Propagation Clustering . . . . . . . . . . . . . . . . . . . . 34 37

3 MACE-means Clustering 3.1

Our Formulation and Correct Number of Clusters (CNC) Challenges . . . 38 3.1.1 Naive K-means and Calculating c ^m . . . . . . . . . . . . . . . . . . 39

3.2

Minimum Average Central Error (MACE) . . . . . . . . . . . . . . . . . . 40 3.2.1 3.2.2 Average Central Error . . . . . . . . . . . . . . . . . . . . . . . . . 41 MACE-means criterion . . . . . . . . . . . . . . . . . . . . . . . . . 42

3.3

Calculating Minimum Average Central Error (MACE) . . . . . . . . . . . 42 3.3.1 3.3.2
 Estimating 1/ni Am Cxmi 2 2

using the available cluster compactness 44

2 ) using the available cluster compactness 45 Estimating the Variance (w

3.4

Average Central Error Estimate . . . . . . . . . . . . . . . . . . . . . . . . 48 3.4.1 MSDL-means clustering . . . . . . . . . . . . . . . . . . . . . . . . 49 . . . . . . . . . . . 50

3.5 3.6 3.7

Computational Complexity Analysis and Comparison

Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 58

4 Signature Testing (Sigtest) in Clustering 4.1 4.2

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Signature Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.2.1 4.2.2 Formulation of Signature testing (Sigtest) . . . . . . . . . . . . . . 62 Sigtest in Statistical Testing . . . . . . . . . . . . . . . . . . . . . . 65

4.3

Sigtest in Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.3.1 4.3.2 Sigtest in Hierarchical Clustering . . . . . . . . . . . . . . . . . . . 67 Sigtest in Partitional Clustering . . . . . . . . . . . . . . . . . . . 68

4.4 4.5

Optimum vocabulary size in bag of visual words using Sigtest . . . . . . . 72 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5.1 Hierarchical Clustering . . . . . . . . . . . . . . . . . . . . . . . . . 75

viii

4.5.2 4.5.3 4.6

Partitional clustering . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Adaptive vocabulary size in bag of visual words . . . . . . . . . . . 77

Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

5 Minimum Pathways in Arbitrary Shaped Clustering (minPAS clustering) 5.1 80 Data assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 5.1.1 5.1.2 5.1.3 5.2 5.3 5.4 5.5 Data Skeleton Using Minimum Spanning Tree . . . . . . . . . . . . 81 Minimum Pathways in Arbitrary Shaped Data . . . . . . . . . . . 83 Membership Score . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

minimum Pathway in Arbitrary Shaped clustering (minPAS) . . . . . . . 86 Computational Complexity Comparison . . . . . . . . . . . . . . . . . . . 90 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 102 105 108 111 113 116 126

6 Conclusions and Future Works A Average Central Error (ZSm ) B Cluster Compactness YSm C Folded Normal Distribution D Estimation of  and T Bibliography Glossary

ix

List of Tables
3.1 3.2 3.3 3.4 3.5 3.6 Time complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Real and synthesized benchmark data sets from the literature. . . . . . . Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for real data sets (average over 50 runs). . . . . . . . . . . . . . Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for S data sets (average over 50 runs). . . . . . . . . . . . . . . Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for our 2D synthetic data sets (averaged over 50 runs). . . . . . Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for our 3D synthetic data sets (averaged over 50 runs). . . . . . 50 52 53 54 55 57

4.1 4.2 4.3 5.1 5.2 5.3 5.4 5.5 5.6 5.7

Benchmark datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 Synthetic data sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 G-means and G-means-Sigtest . . . . . . . . . . . . . . . . . . . . . . . . . 76 Quality Quality Quality Quality Quality Quality Quality of of of of of of of clustering clustering clustering clustering clustering clustering clustering in ring data set. . . . . spiral and ball data set. in heart data set. . . . in Atom data set. . . . in chain link data set. . in half moon data set. . in real data sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 93 93 97 97 100 100

x

List of Figures
1.1 2.1 2.2 3.1 3.2 3.3 3.4 3.5 3.6 3.7 Clustering with order selection. . . . . . . . . . . . . . . . . . . . . . . . . 2

Visualization of Jensen's inequality. . . . . . . . . . . . . . . . . . . . . . . 14 EM algorithm iterations for estimating maximum likelihood. . . . . . . . 15 Three clusters with 100 samples each (N = 300). The three bold points 2 = 3. . . . . . . . . . . . . . . . are centers and the cluster variation is w  In this example, m = 3, d = 2, and m = 2. The two estimated centers are c ^21 and c ^22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Expected value and standard deviation of ZSm for a range of m (here, 2 = 1, d = 3, N = 500). . . . . . . . . . . . . . . . . . . . . . . . m = 5, w Expected value and standard deviation of YSm for a range of m (here, 2 = 1, d = 3, N = 500). . . . . . . . . . . . . . . . . . . . . . . . m = 5, w 2 = 1 and, n = 100. . . . . . . . . . . . T1 (m) and T2 (m) when m = 5, w i  2 = 2,  2 = 2.09, m Typical behavior of ySm , (m = 5, N = 500, w ^w ^ = 5). . True zSm and its estimate in (3.35), when m ranges between 2 and 15 (m = 5, m ^ = 5, N = 500, d = 3). . . . . . . . . . . . . . . . . . . . . . . . (a) Histogram of 500 randomly generated samples belong to a Gaussian distribution with zero mean and unit variance. (b) The actual data without any manipulation. (c) Sorted absolute values of the samples. . . . . . (a) Histogram of 1000 randomly generated samples drawn from a Gaussian mixture model with mean values equal to zero and 5, and unit variances. (b) The actual data without any manipulation. (c) Sorted absolute values of the samples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (a) sorted absolute version of 100 samples belong to a Gaussian distribution with zero mean and unit variance. (b) the same plot while x axis and y axis are swapped. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (a) Solid blue line is 100 samples of the noisy observed data (SNR = 5). (b) Blue line is 100 samples of the sorted absolute values of the noisy observed data crossing the noise confidence region (Red line) at wn = 1.47. The area between the red lines is the noise confidence region with probability 0.999997. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ten observed samples (red dots) and their corresponding boundaries (blue bars). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi

39 42 43 45 46 48 49

4.1

60

4.2

61

4.3

62

4.4

64 65

4.5

Hierarchical clustering with data splitting criterion. . . . . . . . . . . . . . Sigtest in hierarchical clustering: (a) H0 holds (no split), and (b) H1 holds (split). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.8 General procedure of partitional clustering with order selection. . . . . . . 4.9 Sigtest for model verification in Gaussian mixture models. . . . . . . . . . 4.10 (a) Test image from Caltech101, (b) general example of quantizing features (blue dots) with their nearest centers (red dots) and (c) representing them as a histogram over the visual words. . . . . . . . . . . . . . . . . . . . . . 4.11 Accuracy of SVM classifier for different number of clusters (size of visual vocabulary) for 15 objects category from Caltech101 data set. Black dashed shows the accuracy at the location of K = 500 (fixed size assumption), green dashed shows the chosen value K=593 by G-means-Sigtest, and red dashed line shows the accuracy of G-means for estimated K = 1184. 4.12 Accuracy of SVM classifier for different number of clusters (size of visual vocabulary) for 4 objects category from Caltech101 data set. Black dashed line shows the accuracy at the location K = 500 (fixed size assumption), green dashed line shows the chosen value K= 74 by G-means-Sigtest, and red dashed line shows the accuracy of G-means for estimated K = 218. . . Two centered clusters. . . . . . . . . . . . . . . . . . . . . . . . Minimum spanning tree of 300 samples. . . . . . . . . . . . . . Minimum pathways between two samples from the same and clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Dissimilarity Scores based on assumption of having C1 = x150 . 5.5 Dissimilarity Scores based on assumption of having C1 = x201 . 5.6 Sample Scores for Ci (blue line) and Ci+1 (red line). . . . . . . 5.7 Ring data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.8 Spiral and ball data set. . . . . . . . . . . . . . . . . . . . . . . 5.9 Heart. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.10 Atom data set. . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.11 Chainlink data set. . . . . . . . . . . . . . . . . . . . . . . . . . 5.12 Half moon data set. . . . . . . . . . . . . . . . . . . . . . . . . 5.1 5.2 5.3 . . . . . . . . . . . . different . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.6 4.7

68 69 70 71

73

78

79 81 82 84 88 89 89 92 94 95 96 98 99

D.1 Increasing the distance between clusters and representing the result of Sigtest for different combinations of  and T (warmer points show more reliable combinations). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 D.2 Estimated  and T parameters using Genetic algorithm for different distances between clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

xii

Chapter 1

Introduction
Clustering has wide range of applications in different disciplines of science and engineering such as bioinformatics, genetics, image segmentation [1], voice recognition, document classification and weather classification [2­4]. Even astronomers categorize galaxies and stars using cluster analysis techniques [5]. Clustering is used in monitoring spread of disease and detecting significant spatial disease clusters [6]. In life sciences, clustering is an inevitable step in most of the methods for analyzing protein interactions and grouping gene expressions [7, 8]. Customizing and categorizing Internet search results is another application of clustering on text, image and audio data, which can be used for recommending books, movies and music to users [9]. For example, a user who has searched for a specific book might be also interested in other books written by the same author. Those books are considered as a single cluster based on the selected author. Therefore, any member of the cluster can be suggested to the user. In market research, clustering is mainly used for segmenting or grouping customers and products. The result of clustering also reveals the size and capacity of market, dependencies among different segments, and it gives a better understanding about the acceptance of a product by its users [10­12]. Finding different communities in social networks is another application of clustering in the field of computer science [13].

1

CHAPTER 1. INTRODUCTION Clustering is mainly an unsupervised learning problem, where unlike supervised learning training sets or class labels of data samples are not available. In other words, the only available information is the unlabeled data itself. The goal of a clustering algorithm is to subjectively group observed data samples based on their similarity and dissimilarity [14]. A priori definition of similarity plays an important role in data clustering. Assumed shape or distribution of a cluster such as Gaussian or non-Gaussian [15], the membership definition for samples of clusters, and clustering optimization criteria are fundamental elements for defining similarity in clustering [16], [17]. Challenges in Clustering: In majority of clustering methods a predefined number of clusters is required before clustering process. In real life, however, the correct number of clusters (CNC) is not known a priori. Consequently, one main challenge for these clustering methods is determining the CNC. Another challenge in clustering is having a predefined definition for shape of clusters. Therefore, grouping arbitrary shaped datasets that do not follow any specific known distribution seems to be a difficult task. In the following, a brief history of existing clustering methods is provided to further elaborate on these challenges.

Order Selection

Clusterer

Figure 1.1: Clustering with order selection. Non-arbitrary shaped clustering methods: K-means clustering is one of the earliest partitional clustering methods which needs to have the number of clusters (K ) before clustering [18, 19]. It starts with randomly selecting K cluster centers from the samples and assigns other samples as the cluster members. K-means iteratively updates 2

CHAPTER 1. INTRODUCTION the centers and cluster members until the algorithm converges to a stable solution. Kmeans algorithm is considered as a fast clustering method for partitioning datasets with spherical Gaussian clusters. However, it is very sensitive to the initial choice of the centers and can get trapped in the local optima of its objective function. Model-based methods also have attracted a lot of attention over the non-probabilistic and heuristic clustering techniques [20]. The reason of this superiority is in providing a generative and predictive model, and measuring the uncertainty of the samples assigned to each cluster [21]. Gaussian Mixture Model (GMM), a model based method, is among the most commonly utilized models for clustering. In a GMM framework, each cluster is assumed to have a Gaussian distribution and each sample can have a shared membership with couple of clusters in the mixture model. To estimate the model parameters of a Gaussian mixture, Expectation Maximization (EM) algorithm is considered as an efficient solver which iteratively converges to a solution. EM algorithm is considered as a general form of K-means clustering with this difference that clusters can follow any distribution in Gaussian family. In addition, EM relates samples to their centers using soft assignment, while K-means is limited to hard assignments between centers and their members. Similar to K-means, EM relies on a provided CNC. To estimate the CNC in K-means clustering (order selection in Figure 1.1) a wide range of approaches are suggested in the literature [22]. Pioneer methods of CNC calculation are proposed with K-means clustersr and are validity indexes. According to these approaches, for each a priori assumption for the CNC, an index value is calculated. The method chooses the CNC based on optimizing the index value. Example of these approaches are Xie-Beni index [23], Dunn index [24], Silhouette index [25], DaviesBouldin (DB) index, Calinski-Harabasz (CH) index [26], Krzanowski-Lai index [27], and weighted inter-to intra-cluster ratio (wtertra) [28]. Most recent validity index methods are available in [29], [30]. For example, Figure 1.1 shows two general steps in clustering methods with order selection.

3

CHAPTER 1. INTRODUCTION In general, model based clustering approaches are categorized as hierarchical or partitional clustering processes. Hierarchical Clustering: Another approach for clustering data and estimating the CNC is using hierarchical clustering joined with a proper order selection criterion or statistical test in Figure 1.1. In this scenario, clustering algorithms such as K-means and EM can split the data hierarchically and use an order selection criterion to decide about terminating the splitting procedure. X-means [31], G-means [32] and Dip-means [33] are some of the widely used and state of the art clustering methods based on splitting criteria. Order selection of these methods (in Figure 1.1) can be categorizes as follows: 1) Bayesian Information Criterion (BIC) in X-means; X-means is an extension to K-means which employs BIC to estimate the CNC, but it tends to over-fit the data. 2) AndersonDarling (AD) in G-means; G-means is another wrapper around K-means, which estimates the CNC based on the AD statistical test. It examines the Gaussianity of the estimated clusters and performs better than X-means, but has difficulties for overlapped clusters. 3) Dip test in Dip-means; Dip-means is a newly proposed hierarchical clustering method that extends the choice of cluster distribution from Gaussian to a wider range of unimodal distributions, where Gaussian, log-Normal and student's t-distributions are three examples of this large family. This method employs Hartigan's Dip statistical test to evaluate whether each cluster has a unimodal distribution or not. Partitional Clustering: Similar to hierarchical clustering methods, statistical tests can be used for estimating CNC in partitional clustering approaches. PG-means clustering is an example of a partitional clustering method with order selection. In this method, data and its assumed Gaussian mixture model are projected over the direction of maximum variance of data iteratively. Consequently, the projected versions of the model and data are compared using Kolmogorov-Smirnov (KS) test to decide about the CNC. In term of having Gaussian assumption PG-means is similar to G-means, but it

4

CHAPTER 1. INTRODUCTION has this advantage that can find better models using EM algorithm [34]. In general, clustering methods with statistical tests have a prior assumption on the distribution of the observed data. The observed data provides an empirical distribution function (ecdf) and this ecdf is compared with the desired cdf (Dcdf) in the process of estimating the CNC. In addition to the mentioned hierarchical and partitional clustering approaches, the CNC challenge in clustering is studied from different perspectives. For example, Gap statistic takes the output of any clustering algorithm, and then compares the change within cluster dispersion to that expected under an appropriate reference null distribution [35]. The main disadvantage of this method is its computationally expensive behavior which makes it inefficient for high dimensional data. Another example is System Evolution which clusters data using K-medois [36, 37] and defines a validity index from the viewpoint of a pseudo thermodynamics system [38] for estimating the CNC. This method is efficient for well-separated clusters but has difficulties on overlapped clusters. Arbitrary shaped clustering methods: Unlike the above discussed approaches, a large family of clustering methods don't have any assumption on the shape and distribution of clusters. In these methods the center of cluster is a loose concept which is not defined in the related algorithms. Most of these clustering methods group data samples heuristically and don't rely on statistical tests or order selection approaches for estimating the CNC. Spectral Clustering is one of the well-known clustering methods that can partition arbitrary shaped data [39]. This method is constructed based on the eigenvectors and eigenvalues of similarity matrix of data. The number of clusters is one of the requirements of this algorithm that should be available as a predefined value. Normalized cuts (Ncut) for image segmentation is another clustering approach based on partitioning image graph [1]. This method measures both the total dissimilarity

5

CHAPTER 1. INTRODUCTION among different clusters and the total similarity within clusters. Ncut optimizes its criterion based on a generalized eigenvalue approach to partition the data samples. Similar to spectral clustering, Ncut can work on arbitrary shaped data. The estimation of CNC is not efficiently addressed, and the number of clusters should be provided to the algorithm. Data Spectroscopic (DaSpec) [40] is another extension of spectral clustering methods which is constructed based on the Gaussian kernel matrix of data. This method estimates the number of clusters by identifying the eigenvectors that have no sign changes up to a predefined threshold value. A predefined kernel bandwidth is also another requirement of this algorithm. DaSpec assumes that clusters are well-separated, therefore it cannot recognize clusters with small distances among them. Voting-K-means is an example of clustering methods based on combination of different clustering results [41]. In this method, results of different K-means clusterings for an initial number of clusters leads to a co-association matrix which helps to extract the underlying consistent clusters. Density based clustering approaches are another group of methods that don't have any assumption on the distribution of data. In these methods, high density regions are assumed to be clusters which are separated with low density regions or gaps. Therefore, these type of methods can cluster arbitrary shaped data better than model based algorithms. DBSCAN is an example of state of the art clustering methods for arbitrary shaped data [42]. Affinity Propagation [43] is an example of Graph theoretic clustering approaches which also doesn't have any assumption on the distribution of clusters and can independently estimate the CNC value. This method has a high time complexity and has difficulties in recognizing clusters with complex geometries. Our Objectives: In this thesis, our main focus is on clustering methods that can estimate the CNC and find clusters in arbitrary and non-arbitrary shaped datasets.

6

CHAPTER 1. INTRODUCTION We consider different statistical tests and order selection approaches for estimating the number of clusters and suggest our solutions for estimating CNC and clustering different shapes of data. Thesis Outline: This thesis is organized as follows: In Chapter 2, we provide a brief background on some of the widely used clustering methods, solvers algorithms, dimension reduction and splitting criterion for estimating the number of clusters. In Chapter 3, we estimate the CNC based on a rigorous modeling and analysis of Mean Square Error for different number of centers in clustering. We define the average central error (ACE) that is the difference between the ground truth center of the cluster and our estimation of the center (unavailable error). We present the estimate of this error based on the available compactness error. The proposed MACE-means clustering is constructed based on minimizing the ACE error (Minimum ACE-means). The art of this approach is in probabilistic validation of the unknown ACE by using available cluster compactness. In Chapter 4, we propose a new statistical test denoted by signature test (Sigtest) for estimating the number of clusters. Unlike the existing statistical tests it can be used with any prior assumption on the distribution of the clusters (Dcdf). Since the Dcdf can be replaced with any other desired cdf, using the method with any other prior assumption is analogous to what is presented here. Details of using Sigtest in both hierarchical and partitional clustering is provided. Our simulation results show the superiority of using Sigtest as the statistical test in terms of estimating the number of clusters, adjusted rand index (ARI) and variation of information (VI). In addition, we propose using Sigtest in image classification using bag of visual words (BOVW) [44], [45], [46]. While majority of the BOVW based methods have the assumption of a fixed or predefined visual vocabulary size, we propose using Sigtest for estimating the optimum size of the vocabulary based on Scale Invariant Feature Transform (SIFT) features. We show that adaptive prior estimation of vocabulary size in BOVW has a significant effect on increasing the accuracy

7

CHAPTER 1. INTRODUCTION of image classification along with decreasing the time complexity in some applications. In Chapter 5, we propose minimum pathway arbitrary shaped clustering, denoted by minPAS, as a clustering approach that can work with both arbitrary and non-arbitrary shaped clusters. minPAS can independently estimate the number of clusters with a high level of accuracy compared with the similar methods. The proposed method benefits from the unique tree structure of data and can measures the level of similarity among samples using the minimum pathways. Unlike regular distance measures such as euclidean distance, minimum pathways in minPAS respect the geometry of data and does not impose any assumption on the distribution. Chapter 6 is our conclusion on the proposed methods.

8

Chapter 2

Background
In this Chapter, we briefly discuss some of the widely used clustering methods and their requirements. In Section 2.1, we explain K-means and Mixture of Gaussians as two of the most important bases in hierarchical and partitional clustering methods. We discuss X-means, G-means, PG-means and Dip-means as some of the well known clustering examples based on K-means and Gaussian Mixture models. Kolmogorov-Smirnov, Anderson-Darling and Hartigan's Dip test as statistical tests for splitting criterion in clustering methods are discussed in Section 2.2. In Section 2.3 we explain Principal Component Analysis (PCA) that is one of the basic approaches for data dimension reduction. PCA can be used as a preprocessing step for estimating the number of clusters. Spectral clustering, Normalized Cut, DBSCAN, Voting-K-menas, and Affinity Propagation as examples of clustering methods that can cluster arbitrary shaped data are discussed in Section 2.4.

9

CHAPTER 2. BACKGROUND

2.1
2.1.1

Hierarchical and Partitional Clustering Methods
K-means

K-means is one of the most well known and widely used partitional clustering methods. The algorithm can be implemented easily and has a reasonable speed for converging to the clustering solution. However, it has the following limitations: K-means is very sensitive to the initialization error and can be trapped in local optima of its objective function. K-means also has its best performance for spherical Gaussian clusters and using that for arbitrary shaped data can lead to a poor clustering result. Assume we have a data set y = [x1 , ..., xN ]T which is an N × n matrix consisting of N observations in an n -dimensional space and each xi (i = 1, . . . , N ) represents a data sample in the feature space. The goal is to partition the data samples into K clusters in a way that in each cluster internal distances among members of the cluster are smaller than distances to points outside of the cluster. Consider µ = [µ1 , ..., µK ]T as a K × n matrix where µi (i  {1, . . . , K }) is a row of the matrix and the center of the ith cluster. We assume that the number of clusters or K is given [47].

For the first step of clustering, µ should be initialized with K random centers in the n -dimensional space. Then each data point should be assigned to the nearest cluster Cl :

xi  Cl , if

x i - µ l < x i - µj

(2.1)

f or i = 1, ..., N, j = l, and j, l  {1, ..., K }

(2.2)

where µl and µj are the centers of clusters Cl and Cj , respectively. xi is a member of the cluster Cl and || · || is l2 -norm. In the next step, we should recalculate the center of each cluster:

10

CHAPTER 2. BACKGROUND

µl =

1 Nl

xi ,
xi Cl

(2.3)

where Nl is the total number of the samples assigned to the cluster Cl .

After this step all of the centers will be updated to the new values, then data points will be assigned to the nearest centers and this routine continues until either location of the centers remain unchanged or the algorithm reaches to the predefined maximum number of iterations [48].

2.1.2

Mixture of Gaussians

Suppose we are given a data set y = [x1 , . . . , xN ] where xi s are i.i.d. random vectors of length n generated from m unknown Gaussian distributions. In our assumptions, the latent variable zi = j from a multinomial distribution assigns the ith sample of our data set to the j th Gaussian distribution. We wish to model our data set by joint distribution p(xi , zi ):

p(xi , zi ) = p(xi |zi ) p(zi )

(2.4)

having the latent variable zi , the distribution of the data sample xi can be given as:

(xi |zi = j )  N (µj , j )

(2.5)

where µj and j are the mean and covariance matrices of the j th Gaussian distribution. We define j as the probability of choosing the j th Gaussian. Therefore, µ,  and  can be assumed the model parameters which are required to be estimated.

11

CHAPTER 2. BACKGROUND The log-likelihood of the data can be employed to estimate the parameters of this model:

n

l(, µ, ) =
i=1 n m

log p(xi ; , µ, ) (2.6)

=
i=1

log
zi =1

p(xi |zi ; µ, ) p(zi ; ).

Unfortunately, the maximum likelihood estimates of the parameters does not lead to a closed form solution. Thus, finding the derivatives of l(, µ, ) with respect to the model parameters and setting them to zero cannot tackle this problem. Moreover, zi s are unknown which makes it more difficult to find the maximum likelihood estimation.

The Expectation Maximization (EM) algorithm is an iterative algorithm which has two main steps (E-step and M-step). This algorithm can be applied to our problem to estimate the model parameters. In the E-step, zi s will be estimated:

j wi = p(zi = j |xi ; , µ, )

(2.7)

j where wi is the probability of the ith sample being generated by the j th Gaussian com-

ponent, and follows:

j m j =1 wi

= 1. Then in the M-step, model parameters will be estimated as

12

CHAPTER 2. BACKGROUND

j =

j =

i=1 j n i=1 wi xi µj = j n i=1 wi j n T i=1 wi (xi - µj )(xi - µj ) j n i=1 wi

1 n

n j wi

(2.8) (2.9) (2.10)

In the following section, the EM algorithm is explained in details, but before that Jensen's inequality which is a prerequisite to the EM is explained.

Jensen's inequality: Let f be a convex function of random variable X, then:

E [f (X )]  f (E [X ])

(2.11)

In addition, for a strictly convex f , E [f (X )] = f (E [X ]) holds true if and only if X = E [X ] with probability 1 which means X must be a constant. f is a convex function and X (horizontal axis) is a random variable with a 0.5 chance for choosing a and 0.5 chance of choosing b. If f is strictly concave then -f is strictly convex and direction of Jensen's inequalities will be reversed (E [f (X )]  f (E [X ])).

EM algorithm: In some cases, deriving a closed form solution for maximum likelihood may not be possible. Expectation Maximization (EM) is an iterative algorithm which is promising to find the maximum likelihood parameters [49­51]. Let consider an estimation problem in which x = [x1 , . . . , xN ] is a set of N independent training data (observed data) and z = [z1 , . . . , zN ] is a set of latent random variable (unseen data). We aim to fit the parameters of a model p(x, z ) to the data. In this case

13

CHAPTER 2. BACKGROUND

Figure 2.1: Visualization of Jensen's inequality. likelihood of the data is given by:
n

l() =
i=1 n

log p(xi ; ) p(xi , z ; )
z

=
i=1

log

(2.12)

To maximize l() using EM, we repeatedly construct a lower-bound on l (E-step) and then maximize that lower bound to estimate new optimal model parameters (). Consider the following log-likelihood equation:

log p(xi ; ) =
i=1 i=1

log
zi

p(xi , zi ; )

(2.13)

Let assume Qi (z ) as a probability distribution over z (

z

Qi (z ) = 1, Qi (z )  0). Then,

14

CHAPTER 2. BACKGROUND

Figure 2.2: EM algorithm iterations for estimating maximum likelihood. we can rewrite the previous equation as follows: p(xi , zi ; ) Qi (zi ) p(xi , zi ; ) Qi (zi ) (2.14)

=
i=1

log
zi

Qi (zi ) Qi (zi ) log


i=1 zi

Which the last step is derived based on Jensen's inequality and the term:

Qi (zi )
zi

p(xi , zi ; ) Qi (zi ) (2.15)

The above equation (2.15) is expectation of p(xi , zi ; )/Qi (zi ) with respect to zi drawn from distribution Qi (zi ). Therefore, for any set of Qi a lower bound on l() can be estimated. There are many possible choices for Qi . If we have a current guess for  then we will have the lower bound tight at the value of  which means inequality will change into equality at that . By using Jensen's inequality, the bound will be tight at a particular . It means E [f (X )] = f (E [X ]) holds true if and only if X = E [X ], and

15

CHAPTER 2. BACKGROUND this latter is only true for constant X . Therefore:

p(xi , zi ; ) =c Qi (zi ) where c is a constant and it is independent of zi . Since we know
z

(2.16)

Qi (z ) = 1, then:

Qi (zi ) =

p(xi , zi ; ) z p(xi , zi ;  ) = p(zi |xi ; ) (2.17)

where Qi s are set to be posterior distribution of z s, given xi and setting of the parameters . This was E-step of the algorithm which gives a choice of Qi to find the lower bound on the log-likelihood l() that we want to be maximized. In M-step we try to maximize this lower bound in (2.14) to estimate the next optimum :

 = max
i=1 zi

Qi (zi ) log

p(xi , zi ; ) Qi (zi )

(2.18)

The model parameters (µ, , ) of a Gaussian mixture can be estimated by calculating the derivatives of (2.18) with respect to the model parameters and setting them to zero. We can rewrite this equation as follows:

16

CHAPTER 2. BACKGROUND

m

Qi (zi ) log
i=1 zi m k

p(xi , zi ; , µ, ) Qi (zi )

=
i=1 j =1 m k j wi i=1 j =1

Qi (zi = j ) log
1 (2 )n/2 |j |1/2

p(xi |zi = j ; µ, )p(zi = j ; ) Qi (zi = j ) (2.19)

=

log

T -1 exp(- 1 2 (xi - µj ) j (xi - µj )) j wi

Every E-step will be followed by a M-step and this procedure will be repeated iteratively. Since EM causes log-likelihood to converge monotonically, the stopping point for the algorithm will be reached if the increase in l() between two successive iterations is smaller than some tolerance parameter.

2.1.3

X-means Clustering

One of the very first clustering methods which is able to estimate the number of clusters independently is X-means clustering [31], [52]. This method is a wrapper around Kmeans algorithm that hierarchically splits parent clusters into children clusters until the convergence condition is satisfied. X-means relies on Bayesian Information Criterion (BIC) to check whether a subset of data should be split or we should consider it as a single cluster. Consequently it can estimate the number of clusters from number of times that splitting happens. For example, X-means starts clustering with assumption of K = 1 as the number of clusters and then cluster the same data for K = 2. Consequently, clustering results of these two scenarios will be compared by BIC criterion to decide between K = 1 and K = 2. In X-means clustering, there are following steps for grouping the data samples: 1. Initialize K = 1. 2. Run K-means algorithm on the data. 17

CHAPTER 2. BACKGROUND 3. Calculate the BIC of each cluster with K=1 and K=2 4. If BIC for K=1 is less than BIC for K=2, check the test for other clusters. 5. If BIC for K=2 is less than BIC for K=1, increment K by one, go to the step 2. 6. Stop the algorithm if the convergence condition is satisfied, for example K is not changing. The BIC can be calculated as follows: 1 p log N 2

BIC () = L(D) -

(2.20)

where L(D) is the log-likelihood of the data set D based on model  which suggests a certain number of clusters; p is the number of free parameters in the model and N is the size of the data set. With assumption of having clusters generated from spherical Gaussian distributions, the log-likelihood L(D) of the data set D will be defined as follows: L(D) = log
j i 2

pr(xij )

=
j i

log(

^j xij - C nj 1  exp(- N 2  2^ 2 ^d

))

(2.21)

where xij is the ith member of the j th cluster with pr(xij ) as its probability, nj is the ^j as its estimated center and  size of the j th cluster with C ^ 2 is the estimated variance in the clusters. d is the dimension of the data.

Estimations of Cj and  in (2.21) can be derived as follows: ^j = 1 C nj
nj

xij
i=1

(2.22)

18

CHAPTER 2. BACKGROUND  ^2 = 1 N -K ^j xij - C
j i 2

(2.23)

where K is the number of clusters. Based on this fact that log-likelihood for all the data points is sum of the log-likelihood of every single data point [53], log-likelihood L(Dj ) for data points belong to the j th cluster will be defined as follows: nj log(2 ) 2

L(Dj ) = nj log nj - nj log N -

(2.24)

-

nj - K nj d log  ^2 - 2 2

2.1.4

G-means Clustering

G-means is a hierarchical clustering algorithm which benefits from Anderson-Darling statistic test (AD) for evaluating the Gaussianity of clusters. Similar to X-means clustering, this algorithm is also a wrapper around the K-means and can only perform hard clustering. In contrast to X-means, G-means can deal with any distribution from the Gaussian family [32]. In G-means clustering, there are following steps for grouping the data samples: 1. Initialize K = 1. 2. Run K-means algorithm on the data. 3. Project the cluster members onto the direction of the maximum variance in the cluster, this direction can be found by PCA or similar approaches. 4. Use the Anderson Darling test on the projected data to test its Gaussianity. 5. If the data passed the test, check the test for other clusters. 6. If the data didn't pass the test, increment K by one, go to the step 2.

19

CHAPTER 2. BACKGROUND 7. Stop the algorithm if the convergence condition is satisfied, for example K is not changing.

2.1.5

PG-means Clustering

PG-means clustering can learn the number of clusters in a Gaussian Mixture Model (GMM) scheme using Expectation Maximization algorithm (EM) [34]. This method is a partitional clustering approach which simultaneously projects data and its model onto the several random directions in space. PG-means uses Kolmogrove-Smirnov test (KS) to detect any mismatch between the data and the model. Unlike G-means that works with K-means, using EM algorithm gives a better ability of recognizing the overlapped clusters to PG-means. In PG-means clustering, there are following steps for grouping the data samples: 1. Initialize K = 1. 2. Run EM algorithm on the data. 3. Project all of the data and the assumed model of the data onto several random direction in space. 4. Use the Kolmogrove-Smirnov test (KS) on the projected data and model to find any mismatch between them. 5. If the data and model are matched, stop the algorithm and give the final K. 6. If the data and model are not matched, increment K by one, go to step 2.

2.1.6

Dip-means Clustering

Dip-means clustering is constructed based on the Hartigan's dip test of unimodality [33]. According to this clustering method, each sample is a viewer with different distance values from other samples. Using Dip test, distribution of the distance values should be 20

CHAPTER 2. BACKGROUND examined for unimodality. If all viewers pass the unimodality test then null hypothesis of having a single cluster will be approved. Otherwise, a model with more than one cluster should be considered for the samples. This method is a wrapper around K-means. In Dip-means clustering, there are following steps for grouping the data samples: 1. Initialize K = 1. 2. Run K-means algorithm on the data. 3. Calculate the distance matrix for each cluster. 4. Use the Hartigan's Dip test for the distances between each sample and the other cluster members. 5. If the data passed the test, check the test for other clusters. 6. If the data didn't pass the test, increment K by one, go to step 2. 7. Stop the algorithm if the convergence condition is satisfied, for example K is not changing.

2.2

Statistical Tests in Clustering

In statistical testing for clustering, empirical distribution function of data (ecdf) is compared to a desired cdf (Dcdf). The main goal of following statistical testing is to verify whether ecdf can be considered as a sample of Dcdf or not. In general there are two possible hypothesis: · H0 : The observed data (ecdf) is a sample of the desired model (Dcdf) · H1 : The observed data (ecdf) is not a sample of the desired model (Dcdf) The first step of these methods usually requires transforming the observed data into a 1-dimensional data. This preprocessing step is required by majority of the statistical 21

CHAPTER 2. BACKGROUND tests such as AD, KS and Dip test. For example, one well-known approach is using principal component analysis (PCA) to calculate the direction of maximum variance in data (the main principal component), and projecting the data samples onto that direction. G-means and PG-means clustering are examples of clustering methods based on data projection. Dip-means clustering on the other hand works with another transformation of data by calculating the distances between data samples and a proper reference point. Lets x = [xi , · · · , xN ]T be the observed data and xi  Rd , where d is the dimension of data samples. The transformed version of data is y = [y1 , · · · , yN ]T , where yi  R1 . In the following subsections, we briefly discuss three statistical tests KS, AD and Dip which employ different approaches to measure the similarity between the distribution of transformed data y and the Dcdf.

2.2.1

Kolmogorov-Smirnov Test

This test compares the maximum point-wise distance between the ecdf with the Dcdf of the reference distribution. For example, in the case of PG-means clustering the Dcdf has Gaussian distribution. The distance is defined as [54]:

KSscore = sup |FN (yi ) - F (yi )|
i 1iN

(2.25)

where F (yi ) is the Dcdf and FN (yi ) is the ecdf of observed data: 1 N
N

FN (yi ) =

I (yj , yi )
j =1

(2.26)

where I (yj , yi ) =

   1   0

yj < yi yj  yi

(2.27)

The observed samples are compared with a critical value T , to either be accepted

22

CHAPTER 2. BACKGROUND as a sample of a Gaussian distribution (H0 ) or to be not considered as a sample of a Gaussian distribution (H1 ). · H0 : The observed data (ecdf) is a sample of a Gaussian distribution (Dcdf)  KSscore  T · H1 : The observed data (ecdf) is not a sample of a Gaussian distribution (Dcdf)  KSscore > T The critical value T is chosen adaptively by Lilliefor's test statistic which is the result of Monte Carlo calculations in [55].

2.2.2

Anderson Darling Test

Anderson-Darling test is similar to Kolmogorov-Smirnov test in comparing the ecdf and Dcdf, but it calculates a weighted difference between Fn (y ) and F (y ) over all of the N samples [56], [57]:


ADscore = N
-

A(y )(FN (y ) - F (y ))2 dF (y )

(2.28)

note that compare to KS, AD emphasizes more on the tails of the distribution: 1 F (y )(1 - F (y ))

A( y ) =

(2.29)

The observed samples are compared with a critical value T , to either be accepted as a sample of a Gaussian distribution (H0 ) or it is not considered as a sample of a Gaussian distribution (H1 ). · H0 : The observed data (ecdf) is a sample of a Gaussian distribution  ADscore  T · H1 : The observed data (ecdf) is not a sample of a Gaussian distribution  ADscore > T 23

CHAPTER 2. BACKGROUND The critical value T is suggested to be T = 1.8692 for a confidence level of 0.0001 in [32].

2.2.3

Haritagn's Dip test

A more recently proposed method for the purpose of statistical testing in clustering is Hartigan's Dip test. This method generalizes the Gaussian assumption of the two above methods to a unimodal distribution. Unimodal distribution includes distribution such as Gaussian, Log-Normal, Student's t-distribution. The probability density function (pdf), denoted by f , of unimodal distributions is monotonically non-decreasing in (-, yL ) and monotonically non-increasing in (yU , ), where (yL , yU ) for yL  yU is the mode region of distribution. We let (FN , G) = sup |FN (yi ) - G(yi )|, therefore, Dip value of ecdf FN , denoted by D(FN ) is defined as
i 1iN

[58]: D(FN ) = min (Fn , G)
GU

(2.30)

where G is a member of unimodal family U that represents the closest approximation for FN 1 . It is shown that uniform distribution has the smallest Dip value among all of the unimodal distributions, to decide about the unimodality of FN , its Dip value will be compared with the Dip values of uniform distributions U [0, 1]. [33] suggested that if for 1000 bootstraps of U [0, 1], the probability of having D(Fn ) smaller than Dip values of the uniform distributions (we denote it by Dipscore ) is larger than a critical value T , then data is unimodal (H0 ), otherwise it is multimodal (H1 ). · H0 : The observed data (ecdf) is a sample of a unimodal distribution  Dipscore > T · H1 : The observed data (ecdf) is not a sample of a multimodal distribution 
For some yL  yU , G is the greatest convex minorant (g.c.m.) of Fn over (-, yL ), and for (yU , ) it is the least concave minorant (l.c.m.) of Fn . G has the constant maximum slope in (yL , yU ).
1

24

CHAPTER 2. BACKGROUND Dipscore  T The critical value T is suggested to be zero (T = 0) for a significance level of 0, where in all of the cases Dipscore of a data with unimodal distribution should be larger than any other data with uniform distribution [33].

2.3

Principal Component Analysis (PCA)

Principal Component Analysis (PCA) is an algorithm for transforming data samples from a space with high dimensionality to another space with less dimensionality. PCA is an orthogonal transformation and it can only deal with linear data. In general, we have the following assumptions for PCA: 1. Linearity of data (some extensions of PCA consider non-linear data). 2. Large variances have important structures. 3. The principal components are orthogonal. Let y = [x1 , . . . , xN ] be an n × N matrix where each column xi is a variable regarded as a vector belonging to N observations. Assuming these observations are obtained on a large number of variables (N is a large number), there may exist a redundancy in those variables. In other words, some of the variables are correlated with each other, possibly because they are observing the same source and not different independent constructs. The goal of PCA is to decrease this redundancy and represent the data with m (m < N ) synthetic variables or Principal Components (PCs). These PCs are axes of new space which represent the data with less dimensionality. If we assume each xi is a linear combination of m independent sources S = [s1 , . . . , sm ] where si has the same length as xi , then the observed data y can be given as:

y = SA 25

(2.31)

CHAPTER 2. BACKGROUND where A is an m × N mixing matrix with constant elements. Now the question is, what is the possible transformation on the observed data which derives PCs and leads to the less dimensionality. The transformation should preserve the independent constructs of the data and PCs should be the directions in the data space which have high variances. The PC with the highest variance among other components has the maximum information about the data. Therefore, PCA is more about finding the components with high variances [59]. The covariance matrix of the data is necessary for deriving PCs. If we assume that the covariance matrix yy of the normalized data y is not available but the observations have stationary behavior, the estimation of covariance matrix can be given as follows:

^ yy = E [y T y ]   ^11 · · ·    . .. . =  .  .   ^ n1 · · ·
N

(2.32)   ^1n . . .  ^nn     

 ^jk

1 = N -1

(x(i,j ) - µj )(x(i,k) - µk )
i=1

(2.33)

where  ^jk is located at the j th row and the k th column of the covariance matrix and is a measure of relation between xj and xk which µj and µk are means of the variables. x(i,j ) and x(i,k) are the ith observation of xj and xk variables. To decrease the variable redundancy and find the PCs, the first step is to find a linear function of the variables xi (i = 1, . . . , N ) which gives the maximum variance:

y1 = 11 x1 + 21 x2 + . . . + N 1 xN

(2.34)

where 1 = [11 , . . . , N 1 ]T is a vector of N constant values. We look for another linear 26

CHAPTER 2. BACKGROUND function of the data samples (y2 ) which has the second largest variance and is uncorrelated with y1 . The procedure of finding the linear functions can be followed until having the k th (k  N ) linear function of the variables which has the k th largest variance and at the same time is uncorrelated to previously derived functions. Therefore, it is possible to calculate up to maximum n uncorrelated PCs, but we hope to represent the data with less number of PCs which have most of the variations of the data set.

^ yy Then the k th largest PC is given by zk = yk where k is an eigenvector of 
T  = 1) the corresponding to the k th largest eigenvalue k . For unit length k (k k

var(zk ) = k where var(zk ) is the variance of zk . The first PC (y1 ) is related to 1
T ^ yy 1 . The constraint k = 1 is used in derivation which maximizes var(y1 ) = 1

which simplifies the optimization problem and it means sum of squares of elements of 1 is equal to one.

T ^ yy 1 with respect to k = 1 the Lagrangian equation L(1 ) can To maximize 1

be written as follows:
T^ T L(1 ) = 1 yy 1 - (1 1 - 1)

(2.35)

where  is a Lagrange multiplier. Differentiation subject to 1 gives:

 ^ 1 = max L(1 )
1

(2.36)

^ yy   ^ 1 -  ^1 = 0 which gives: ^ yy - I )^ ( 1 = 0

(2.37)

(2.38)

^ yy and it is corresponding where I is the identity matrix. Thus,  is an eigenvalue of 

27

CHAPTER 2. BACKGROUND to eigenvector  ^ 1 . To decide which of the eigenvectors maximizes the variance of PC, we consider that following term should be maximized:
T^ T  ^1 yy  ^1 =  ^1  ^1 T  ^1 =  ^1

(2.39)

=  ^ yy . Therefore,  corresponds to eigenvector  ^ 1 and it should be the largest eigenvalue of 
T ^ yy 2 subject to being uncorrelated with The second PC (y2 ) also should maximizes 2

previous PC (y  ^ 1 ) which means E [(y  ^ 1 )T (y2 )] = 0.
T^ E [(y  ^ 1 )T (y2 )] =  ^1 yy 2 T^ = 2 yy  ^1 T = 2 1  ^1 T = 1 2  ^1 T = 1  ^1 2

(2.40)

Then any of the following conditions will be useful to define the PCs uncorrelated:
T^  ^1 yy 2 = 0, T^ 2 yy  ^1 = 0

(2.41)

T  ^1 2 = 0,

T 2  ^1 = 0

(2.42)

If we choose the later constrain and having the normalization constrain then the Lagrangian function will be:
T^ T T L(2 ) = 2 yy 2 - (2 2 - 1) -  2  ^1

(2.43)

28

CHAPTER 2. BACKGROUND where  and  are Lagrange multipliers. Maximization with respect to 2 gives:

 ^ 2 = max L(2 )
2

(2.44)

^ yy   ^ 2 -  ^ 2 -  ^1 = 0
T gives: Multiplication by  ^1

(2.45)

T^ T T  ^1 yy  ^ 2 -  ^1  ^2 -  ^1  ^1 = 0

(2.46)

T Since first two terms are zero and  ^1 ^ 1 = 1 then  = 0.

^ yy   ^ 2 -  ^2 = 0

(2.47)

and ^ yy - I )^ ( 2 = 0 (2.48)

^ yy , and  So  is an eigenvalue of  ^ 2 the corresponding eigenvector. Similar to the first
T ^ yy  PC,  =  ^2 ^ 2 should be as large as possible. This procedure can be repeated to find

^ yy and k is the corresponding all of the PCs as the k is the k th largest eigenvalue of  eigenvector: var(yk ) = k (2.49)

We can summarize the calculation of PCs as follows: 1. Normalize data. 2. If covariance matrix of the population is not available calculate the covariance matrix of the samples.

29

CHAPTER 2. BACKGROUND 3. Calculate eigenvectors and eigenvalues. 4. Keep m top eigenvectors which are related to the m largest eigenvalues. 5. Multiply eigenvectors by the data to calculate the PCs. An extension of PCA for calculating principal curves is introduced in [60].

2.4
2.4.1

Arbitrary Shaped Clustering Methods
Spectral Clustering

Spectral clustering can partition arbitrary shaped data if the number of clusters K is available [61, 62]. In general spectral clustering methods, K largest eigenvectors of the Laplacian of the affinity matrix will be used for partitioning data. Following steps show the process of spectral clustering for an observed dataset x = [x1 , x2 , · · · , xN ]T [39]: 1. Calculate the affinity matrix A  RN ×N , where Aij is the distance between samples xi and xj :
- xi -xj 2 2 2

Aij = e where i = j and Aii = 0.

(2.50)

2. Define D as a diagonal matrix where each Dii is a summation of the elements in the ith row. 3. Calculate the Laplacian matrix L as follows: L = D-1/2 AD-1/2

(2.51)

4. Find the K largest eigenvectors of L, denoted by 1 , 2 , · · · , K , and form the

30

CHAPTER 2. BACKGROUND matrix P  RN ×K as follows:

P = [1 , 2 , · · · , K ]

(2.52)

5. Normalize P and denote it by Q as follows: Pij (
2 2 j Pij )
1

Qij =

(2.53)

where each row of Q is a data point in RK . 6. K-means or similar algorithms can cluster rows of Q into K clusters. 7. If the ith row of Q is a member of cluster j , then assign the original point xi to the cluster j .

2.4.2

Normalized Cut Clustering

Normalized Cuts (N-Cut) clustering is proposed for image segmentation in [1]. This method constructs a weighted graph of data, where samples are nodes and weights on edges between nodes reflect a measure of similarity between samples. More specifically, edge weights are inversely proportional to the distances between nodes. N-Cut tries to find two optimal partitions in the data which removing the edges between them will have the minimum cut. In this setting, a cut is the total value of the removed edges and N-cut for two partitions A and B can be given as follows: cut(A, B ) cut(A, B ) + assoc(A, V ) assoc(B, V )

N cut(A, B ) = and

(2.54)

cut(A, B ) =
uA,v B

w(u, v )

(2.55)

31

CHAPTER 2. BACKGROUND where w(u, v ) is the weight between nodes u and v . The assoc(A, V ) = w(u, t)

uA,tV

is the total connections between nodes in A and all nodes in the graph. The same definition is valid for assoc(B, V ). The following shows required steps for partitioning an image data using N-cut: 1. Given an image dataset, set up the weighted graph of the image with nodes and weights for the connected nodes. 2. Solve the following relation for eigenvectors with the smallest eigenvalues:

(D - W )x = Dx

(2.56)

where W is an N × N symmetrical matrix of weights and D is an N × N diagonal matrix with d(i) =
j

w(i, j ) as the ith element on its diagonal. d(i) is the total

connection from node i to all of the nodes. 3. Bipartition the graph using the eigenvector with the second smallest eigenvalue that minimizes the N-cut. 4. Decide if the partitions should be subdivided recursively based on the stability of cut and having N-cut less than a predefined value. The number of segments in the above steps can be controlled by the maximum value of N-cut, but this 2-way cut procedure has drawbacks on treating oscillatory eigenvectors and the approach is computationally wasteful [1]. Therefore, instead of 2-way cut a K-way partitioning using all of the eigenvectors based on a given K is suggested.

2.4.3

Voting-K-means

Voting-K-means algorithm combines clustering results of several K-means clusterings for an initial given number of clusters. The resulted co-association sample matrix which shows overall outcome of clusterings is then used to extract the underlying consistent 32

CHAPTER 2. BACKGROUND clusters [41]. Following steps show the required procedure for partitioning data using Voting-K-means for a given initial number of clusters K [41]: 1. Do R times: · Randomly select K cluster centers among the N data samples. · Organize the N samples in random order, keeping track of the initial data indexes. · Run the K-means algorithm with the reordered data and cluster centers and update the co-association matrix according to the partition thus obtained over the initial data indexes. 2. Detect the consistent clusters through the co-association matrix.

2.4.4

DBSCAN

Density-based spatial clustering of applications with noise (DBSCAN) is a well known and widely used clustering approach for arbitrary shape clusters [42]. In this method, clusters are regions with high density of samples which are separated by lower density areas. This general definition suggests that clusters can follow any non-convex and arbitrary shaped geometry. DBSCAN algorithm requires to have two input parameters available before the task of clustering, -neighborhood, and minimum number of points (minPts). All the members of e-neighborhood of xi are within epsilon distance (distance usually is defined based on the euclidean distance). for any xj , and a member of -neighborhood of xi , we have the distance between xi andxj defined as dxi xj . The -neighborhood is the distance from any xi sample in the data set that any other sample xj within this distance will be considered as a neighborhood sample for the xi . In other words, for any neighborhood sample xj with the the distance dij from xi , the following inequality is true: 33

CHAPTER 2. BACKGROUND

dij  neighborhood

(2.57)

The minPts defines the possible minimum number of samples for the clusters. In this method, samples with at least minPts number of samples in their neighborhood are called core-samples, and samples with less number of neighborhoods are non-core samples. Core samples are mainly located in the denser area of the data set, while noncore samples belong to regions with less density. The samples which are neither core sample nor non-core sample are considered as outliers. Based on the above definition, any xj sample is directly density-reachable from the sample xi , if there is a chain of intermediate samples which are all core samples like xi . In general, DBSCAN starts with an arbitrary sample and check it for being a core sample. If it is a core sample then a cluster will be emerged otherwise it will be assumed as an out-lier sample which has this potential to be assigned to an undiscovered cluster.

2.4.5

Affinity Propagation Clustering

Affinity propagation or clustering by passing messages between data points is introduced in [43], and relies on the similarity matrix of data as the input of algorithm. Here, the number of clusters will be estimated simultaneously and the algorithm doesn't rely on a predefined distribution for clusters. According to this method, an iterative process of sending messages back and forth among the data samples will lead to emergence of clusters and their exemplars or centers. In this context, there are two types of messages to be sent by data samples to each other: responsibility and availability . We consider s(i, k ) as the similarity value between the sample xi and sample xk . When the goal is to minimize the squared error, each similarity is set to a negative squared error. Therefore, we can formulate s(i, k ) as the negative Euclidean distance between the samples: s(i, k ) = -||xi - xk ||2 34 (2.58)

CHAPTER 2. BACKGROUND Accordingly, r(i, k ) is the responsibility message sent from the sample i to the sample k , which shows the significance of k as an exemplar for i considering all other available exemplars for i. a(i, k ) is the availability message sent from the potential exemplar k to the sample i, which shows how appropriate is it for the sample i to choose the sample k as its exemplar knowing the vote of other samples to select the sample k as an exemplar. Consequently, the r(i, k ) and a(i, k ) can be defined as follows:

r(i, k )  s(i, k ) -

max
k s.t. k =k

a(i, k ) + s(i, k )

(2.59)

where s(i, k ) is the similarity between the sample i and all of the available exemplars except the exemplar k . a(i, k ) shows the availability of all of the exemplars excluding the sample k for the sample i.

a(i, k )  min 0, r(k, k ) +

max{0, r(i , k )}

(2.60)

i s.t. i { / i,k}

where r(k, k ) is the self responsibility of the sample k which relies on the s(k, k ). Here, s(k, k ) is the self similarity or preference of the sample k that could be given as a prior knowledge in the beginning of the clustering. Sample k with a large predefined value of s(k, k ) has higher chance to be served as an exemplar, therefore, pref erence values can dictate the final number of clusters. The self availability of k shows the positive votes or responsibilities sent from all of the samples excluding the sample i to choose the sample k as the exemplar.

a(k, k ) 

max{0, r(i , k )}
i s.t. i =k

(2.61)

Having the availability and responsibility values, any sample k that maximizes the a(i, k ) + r(i, k ) is an exemplar for the sample i. In summary, availabilities will be updated based on the responsibilities and respon35

CHAPTER 2. BACKGROUND sibilities will be updated based on the availabilities. Then, exemplars will be suggested based on the combination of them. This procedure will be repeated iteratively until a predefined condition for terminating the algorithm is satisfied. In some cases, a numerical oscillation might occur which need to be avoided by using a damping factor between 0 and 1 in the algorithm. It is suggested that a damping factor equal to 0.5 can avoid most of the oscillations. Another suggested approach is adding a tiny amount of noise to the similarity matrix.

36

Chapter 3

MACE-means Clustering
Majority of the existing clustering methods that can estimate the number of clusters independently need to solve two optimization problems. One optimization problem for estimating the number of clusters and one for clustering data based on the estimated number of clusters. The methods that have optimization stages such as K-means based methods are sensitive to the initial optimization parameter and local optima. These errors, in most of the algorithms will not be detected, which propagates to the results of clustering. One scenario is that number of clusters is chosen correctly, but optimization error causes a difference between the true center and the estimated center. In another scenario, the chosen number of clusters is not correct, and the error of mismatching between samples and true centers is also added to the optimization error. Inspired by [63], [64] and [65], we penalize the errors of clustering and optimization based on a probabilistic approach. This chapter is motivated by searching for a quantitative measure that can evaluate the clustering error [66].

37

CHAPTER 3. MACE-MEANS CLUSTERING

3.1

Our Formulation and Correct Number of Clusters (CNC) Challenges

Observed data of length N, x = [x1 , . . . , xN ]T where xi  R1×d , is available and the data is generated by m cluster model. Centers of these clusters are rows of matrix c m , c m  Rm ×d (with dimension d):
  T c m = [c1 , . . . , cm ] ,

(3.1)

The observed data x is a sample of random variable X with the following statement:
      X1 . . . XN   c x1 . . . c xN   W1 . . . WN    ,  

    =    

    +    

(3.2)

X = c x + W.

(3.3)

  where c x is the associated centers of the data, i.e., each cxi is an element of cm , and W

is the representative of a random variable that demonstrates the variations in the clusters. For example, if the variations are assumed to be from independent and identically
2I distributed (iid) Gaussian distributions, we have WiT  N (0, w d×d ). Figure 3.1 shows

an example of such model in 3 - dimensional space (d = 3) with three centers (m = 3).

CNC Challenges: A clustering method aims for estimating the correct centers c x . In this estimation, finding the CNC (m ) is an important task. Here, we model the problem of CNC calculation and clustering as follows: c ^m = [^ cm1 , . . . , c ^mm ]T . x - x - c

(3.4)

38

CHAPTER 3. MACE-MEANS CLUSTERING

25 20 15 10 Z 5 0 -5 -10 15 10 5 0 -5 Y -10 -5 5 0 X 10 20 15 25

Figure 3.1: Three clusters with 100 samples each (N = 300). The three bold points are 2 = 3. centers and the cluster variation is w where c ^m is the vector of estimated centers in m-clustering. Arrows show that from the correct centers, members of the clusters are generated and from the members of the clusters the estimated centers are calculated. The main challenge is to find an optimum m from a feasible range of values, m  [mmin , mmax ]. In an efficient clustering method m ^ =m , i.e., the CNC is found.

3.1.1

Naive K-means and Calculating c ^m

Naive K-means is a clustering algorithm that provides center estimates for a given number of clusters m. For the available m and randomly initialized c ^0 m (where the superscript zero represents the initial step of the iterative optimization steps), K-means estimates the compactness error ySm , which is the error between the available data and estimated centers. At each iteration step, the following optimization is solved by K-means:

l 2 ySm (^ cl ^l m) = x - c m 2,

+1 l c ^l cl m = arg min ySm (^ m ), c ^l m

(3.5)

max c ^m = c ^l m ,

lmax ySm = ySm .

(3.6)

where lmax is the step when the convergence to a desired compactness error is satisfied.

39

CHAPTER 3. MACE-MEANS CLUSTERING K-means can converge to the solution of clustering with a reasonable speed but it has some shortcomings that need to be considered before any clustering. One of the issues with K-means is that the objective function in (3.5) makes it limited to clusters with spherical Gaussian distributions. Also, K-means is very sensitive to initialization error for selecting centers of clusters and can be trapped in local optima of the objective function.

CNC Challenges with K-means: Naive K-means can be used when CNC is known a priori. However, its wide use is also for cases that CNC is not known. In this scenario, a range of number of clusters is first considered and additional processing compares these number of clusters to come up with an estimate of CNC. For example, in some of these methods, the compactness error (ySm in (3.6)) is used as a part of the criterion. Validity index methods, such as CH, DB, KL, Sil, and wtertra are examples of such methods. Note that the compactness error itself is a decreasing function of m and cannot provide any estimate of CNC by itself.

3.2

Minimum Average Central Error (MACE)

Considering (3.4) it seems desirable to have estimate of the sample of error1 :
 ^m 2 ZSm = Cx -C 2.

(3.7)

This error denoted by Average Central Error (ACE), is the error between the true centers with correct number of centers and the estimated centers with m number of centers. In the following, we provide a unique method of estimating this error and will show how comparison of this error for a range of m is an efficient method for CNC estimation. We will show how the available compactness error can be used in estimating the ACE.
1

^m resulted by the random variable X . Please note that c ^m is a sample of the random variable C

40

CHAPTER 3. MACE-MEANS CLUSTERING

3.2.1

Average Central Error

The ACE, for when the number of clusters is assumed to be m, can be formulated as follows:
m

ZSm =
i=1

ZSmi ,

(3.8)

where ZSmi is the ACE in the ith cluster: 1 ^mi 2 . C - C 2 ni xmi

ZSmi =

(3.9)

1 , · · · , X ni ]. Therefore, we have : Denote members of this cluster with Xmi = [Xmi mi

     

c x1 mi . . . c xni
mi

     

  -    

1 Xmi . . . ni Xmi



 1 . . . 1

   ^ C  mi , 

     -     

(3.10)

 Cx mi

^mi , - Xmi - C = 1 ni
ni j Xmi , j =1

(3.11) (3.12)

^mi C

Figure 3.2 shows an example in which m = 3 and m = 2. In this case K-means provides ^2 = [^ an estimate of centers as C c21 , c ^22 ]T . As the figure shows, the associated cluster
1 with c ^21 has n2 = 9 members that includes one member of c 3 denoted by x21 , and eight 2 9 members of c 1 denoted by [x21 , · · · , x21 ]. For these nine members of x21 and based on

(3.10), we have:
   T c x21 = [c3 , c1 , · · · , c1 ]

- x21 - [1, 1, · · · , 1]T ^21 . 1×10 c

(3.13)

41

CHAPTER 3. MACE-MEANS CLUSTERING

2 1.8 1.6 1.4 1.2 1 Y

c1

 c2
1 x21

0.8 0.6 0.4 0.2 0 -2.5 -2 -1.5

-1

-0.5

0

Figure 3.2: In this example, m = 3, d = 2, and m = 2. The two estimated centers are c ^21 and c ^22 .

3.2.2

MACE-means criterion

Minimizing ACE over a considered range of m results an estimate of CNC. This method is denoted as MACE-means. In the following, we show required steps for deriving MACE and employing that in data clustering.

3.3

Calculating Minimum Average Central Error (MACE)

The average central error (3.9) is (for details see Appendix A): 1  = Ami Cx mi ni 1 + 2 ni
ni

ZSmi

2 2

where
Ami

  1-  . . = .  

1 ni

1 ni

The variance and mean of the ACE are as follows: 1  Ami Cx mi ni 1 2  , ni w

E [ZSmi ] =

42


X 0.5

c ^21

c3
1



c ^22

1

1.5

2

2.5

3

Wj2
j =1

2 + 2 ni

ni

Wj Wk
j =k

(3.14)

··· .. . ···

1 ni

   ,  

. . . 1-
1 ni

2 2

+

(3.15)

CHAPTER 3. MACE-MEANS CLUSTERING 2 4 w . n2 i

var[ZSmi ] =

(3.16)

where the above relations are result of assuming the same variance in clusters. We show that in practice, the proposed clustering method based on this assumption can deal with clusters with different variances to an acceptable level. An example of the behavior of these statistics is shown in Figure 3.3. As these values show, the standard deviation is

10 Expected value and standard deviation of Zsm

3

Std[Zsm] E[Zsm]

10

2

10

1

10

0

10

-1

10

-2

-3 10 0

2

4 6 m (number of clusters)

8

10

Figure 3.3: Expected value and standard deviation of ZSm for a range of m (here, m = 5, 2 = 1, d = 3, N = 500). w much smaller than the expected value itself and can be ignored in comparison. Therefore, in comparing m-clustering, we can focus on estimating and comparing the expected value
 for feasible sets of m clusters. Consequently, we only need to estimate the Ami Cx mi 2 /n i 2

2 in advance. In the following two in (3.15), which requires to have the variance w

subsections, we provide methods for estimating these values by only using the observed data.

43

CHAPTER 3. MACE-MEANS CLUSTERING

3.3.1

 Estimating 1/ni Am Cxmi

2 2

using the available cluster compactness

The available cluster compactness ySm in (3.6) is a sample of random variable YSm . Average cluster compactness error in the ith cluster is: 1 ^mi 2 Xmi - C 2, ni

YSmi =

(3.17)

by simplifying (3.17), we will have (See Appendix B for more details): 1  = Ami Cx mi ni 2 - 2 ni
ni

YSmi

2 2

Wj Wk +
j =k ni

ni - 1 n2 i

Wj2 -
j =1

2 n2 i

ni

ni

Wj
j =1 k=1

c xk +

2 ni

ni

Wj c xj , (3.18)
j =1

consequently, the expected value and variance of cluster compactness are as follows: 1  Ami Cx mi ni 1 ni
ni 2 c xj - i=j

E [YSmi ] = =

2 2

+

ni - 1 2 w , ni

(3.19) (3.20)

i 1 ni - 1 2 2 w , ( c xj ) + 2 ni ni j =1

n

2 4w var[YSmi ] = 2 ni

ni 2 c xj j =1

4 2 i  2 2(ni - 1) 4 - 3 w ( cxj ) + w , ni n2 i j =1

n

(3.21)

An example of expected value and standard deviation of YSm is shown in Figure 3.4. Comparing (3.20) and (3.21), the variance is of order of 1/ni th smaller than the expected value, therefore we can assume that the available ySmi is a good representative of its expected value. Therefore, from (3.19) the following relation will be given: 1  Ami Cx mi ni ni - 1 2 w . ni

2 2

 ySmi -

(3.22)

44

CHAPTER 3. MACE-MEANS CLUSTERING

10 Expected value and standard deviation of Ysm

3

Std[Ysm] E[Ysm]

10

2

10

1

10

0

-1 10 0

2

4 6 m (number of clusters)

8

10

Figure 3.4: Expected value and standard deviation of YSm for a range of m (here, m = 5, 2 = 1, d = 3, N = 500). w

3.3.2

2 ) using the available cluster compactEstimating the Variance (w

ness
In this section, we use the available cluster compactness to find an estimate of variance. Using (3.20) for the mith cluster of m-clustering, we have: 1 ni
ni 2 c xj j =1
i ni - 1 2 1 2 c - 2( w + i (m), xj ) + ni ni j =1

n

ySmi =

(3.23)

where i (m) represents the divergence of ySmi from its mean and therefore, E [ i (m)] = 0. For the overall cluster compactness of m-clustering we have:
m

ySm =
i=1

1 ( ni

ni 2 c xj j =1

i 1 ni - 1 2 2 - 2( c w ) + (m), xj ) + ni ni j =1

n

(3.24) (3.25)

= T1 (m) + T2 (m) + (m),

45

CHAPTER 3. MACE-MEANS CLUSTERING where
m

T1 (m) =
i=1 m

(

1 ni

ni 2 c xj - j =1

i 1 2 ( c xj ) ), n2 i j =1

n

(3.26)

T2 (m) =
i=1 m

(

ni - 1 2 w ), ni
i (m),

(3.27) (3.28)

(m) =
i=1

E [ (m)] = 0,

A typical behavior of terms T1 (m) and T2 (m) is shown in Figure 3.5. T2 (m) is mainly a

20 Calculated Values of T1 and T2

15

T1 T2

10

5

0 1 2 3 4 5 6 7 m (number of clusters) 8 9

2 = 1 and, n = 100. Figure 3.5: T1 (m) and T2 (m) when m = 5, w i

function of variance and the number of elements in each cluster. In general, if ni is large enough such that (ni - 1)/ni  1, we have:
2 T2 (m) = mw .

(3.29)

On the other hand, as m grows to be larger than the true m , for each mith cluster,
   we have c xj  cmi , where cmi is one single true unknown center. Although cmi is not

46

CHAPTER 3. MACE-MEANS CLUSTERING known, this property causes T1 (m) in (3.26) to be negligible for m  m . Consequently, for the available cluster compactness in (3.23) with the range of m-clustering we have:    T1 (m) + T2 (m) + (m)   T2 (m) + (m)

m < m (3.30) m m

ySm =

while m is unknown, the behavior of this cluster compactness is such that it can help us in finding an estimate of variance. An estimate of variance can be calculated as follows:

k = arg min(ySm ),
m 2  ^w =

(3.31)
mmax m=k

1 mmax - k + 1

ySm m ni . i=1 ni -1

(3.32)

where the second equation is the result of using (3.27) and (3.30). Please note that, while it is known that minimizing ysm for a range of m does not provide a consistent estimate of CNC, the above analysis shows that this minimization is beneficial in estimating the variance. Figure 3.6 shows a typical behavior of ySm , T2 , and how this averaging works. In this example, m = 5, and two of the clusters are highly overlapped, which has forced the expected value of ySm to give a minimum at m = 4. As it can be seen, T2 is a function
2 , which for m  4 is the same as y of w Sm . Therefore, minimum of ySm can be used to 2 , but it does not give the correct m directly. In other word, if estimate the variance w

we ignore the effect of l2-norm in calculation of ySm , ySm will be always a monotonically decreasing function of m. Nevertheless, as Figure 3.6 confirms the minimum of E [ZSm ] occurs at CNC =5.

47

CHAPTER 3. MACE-MEANS CLUSTERING

10 Calculated T2 and expected values of Ysm and Zsm

4 T2 E[YSM] E[ZSM]

10

3

10

2

10

1

10

0

10

1

1

2

3

4

5 6 7 m (number of clusters)

8

9

10

2 = 2,  2 = 2.09, m Figure 3.6: Typical behavior of ySm , (m = 5, N = 500, w ^w ^ = 5).

3.4

Average Central Error Estimate

To estimate the ACE based on (3.22) and (3.32), we have: 1  Ami Cx mi ni ni - 1 2  ^w , ni

2 2

= ySmi -

(3.33)

using this result and (3.32) in (3.15) follows as: ni - 2 2  ^w , ni

z ^Smi = ySm -

(3.34)

which can be used to provide the following estimate for ACE:
m

z ^Sm =
i=1

z ^Smi ,

(3.35)

consequently, the estimated CNC by MACE is as follows:

m ^ = arg

m[mmin ,mmax ]

min

z ^Sm .

(3.36)

48

CHAPTER 3. MACE-MEANS CLUSTERING An example of true and estimate of zSm is shown in Figure 3.7.

350 300 250 200 150 100 50 0

Estimated Zsm True Zsm

1

2

3

4

5 6 7 m (number of clusters)

8

9

10

Figure 3.7: True zSm and its estimate in (3.35), when m ranges between 2 and 15 (m = 5, m ^ = 5, N = 500, d = 3).

3.4.1

MSDL-means clustering

MACE-means clustering can also be denoted as Minimum Structure Description Length (MSDL-means clustering). According to (3.3), the density function of the observed data based on the true clusters is:
 f ( X ; Cx )=

1 (
2 )N 2w

exp-

 X -Cx

2 /2 2 w 2

,

(3.37)

Therefore, the description length of the observed samples can be modeled as follows [63]:
 DL(X ; Cx )=-

1  log2 (f (X ; Cx )), N

(3.38)

49

CHAPTER 3. MACE-MEANS CLUSTERING Consequently, the description length of the associated cluster centers for m-clustering is: ^m ; C  ) = - DL(C x 1 ^m ; C  )) = log2 log2 (f (C x N log2 e ZSm . 2 2w

2 + 2w

(3.39)

where the last equation is the result of (3.8). This shows how minimizing the ACE is equivalent to minimizing the description length based on m-clustering that we denote by m-clustering structure description length.

3.5

Computational Complexity Analysis and Comparison

Computational complexity of K-means is O(mN dl), where m is the number of clusters, N is the length of the data, d is the dimension of the data and l is the fixed number of iterations in the optimization stage. Computational complexity of MACE-means is analogous to G-means, which is O(m) × O(mN dl). This is obtained based on m + 1 required runs of K-means for estimating the minimum ZSm at m. Here, ZSm is calculated based on YSm which is given by K-means. Therefore ZSm calculation doesn't impose a significant computational complexity on the method. Number of iterations for K-means algorithm in G-means is l = 100, and in KL, CH, DB, wtertra, Sil and MACE-means is l = 35, while this value for Expectation-Maximization (EM) optimization algorithm in PG-means is l = 10. Table 3.1 gives a comparison of computational complexity for different methods 2 . Table 3.1: Time complexity
Method MACE-means KL and CH DB and wtertra Sil G-means PG-means Time complexity O(m) × O(mN dl) O(N d) × O(mN dl) O(d(m2 + N )) × O(mN dl) O(dN 2 + N m) × O(mN dl) O(m) × O(mN dl) O(m2 N d2 l + mN log(N ))

[67] [67] [67] [32] [34]

2 Time complexity of DaSpec was not available, but it was comparable to indexed-based clustering methods. In our simulation experiments, the complexity of X-means will be discussed in Section 3.6.

50

CHAPTER 3. MACE-MEANS CLUSTERING

3.6

Experimental Results

In this section we compare the performance of MACE-means clustering with other known approaches. The compared methods are stand alone approaches, such as PG-means [34], G-means [32], X-means [31], and Data Spectroscopic clustering (DaSpec) or they are validity indexes used with K-means, such as Silhouette (Sil) [25], Davies-Bouldin index (DB), Calinski-Harabasz index (CH) [26], Krzanowski-Lai index (KL) [27] and, weighted inter-to intra-cluster ratio (wtertra) [28]. Our results are shown for the following three sets of data: 1. Six available data sets from UCI Machine Learning repository [68], that satisfy clustering problem statement in Section 3.1: Breast, Vertebral [69], Seeds, Wave Forms, Multiple Features (dutch handwritten) and Water Treatment Plant. Table 3.2 shows characteristics of these data sets. 2. Four synthetic data sets (S1 to S4) introduced in [70] are selected because of their Gaussian nature which are suitable for MACE-means clustering. These four data sets are generated based on different levels of overlap between Gaussian clusters. Table 3.2 shows characteristics of these data sets. 3. A large set of synthetic data in 2D and 3D feature spaces with the main focus on estimating the number of clusters in low dimension. The sets are generated with random centers and various levels of overlapping. The experimental result for the first data set is shown in Table 3.3. It includes the mean and STD of the estimated number of clusters for 50 runs. The ARI and VI values are also averaged over 50 runs. The CNC is presented by m . Note that for some of the data sets, X-means did not converge to a solution, these data sets are marked by (N/A).3
In calculating the computational complexity, X-means seems to be comparable with MACE-means and G-means. However, even though X-means is based on Kd-tree [31], that is supposed to speed up the method, the algorithm is very slow and in occasions does not converge.
3

51

CHAPTER 3. MACE-MEANS CLUSTERING Table 3.2: Real and synthesized benchmark data sets from the literature. Data set Breast Vertebral Seeds Wave Form Multiple Features Water Treatment Plant S1 - S4 Number of data vectors 699 310 210 5000 2000 527 5000 Dimension of data 9 6 7 21 649 38 2 Number of clusters 2 3 3 3 10 13 15

ARI and VI indexes are only calculated for data sets that have the true class labels and clustering methods that provide the estimated labels in addition to the estimated number of clusters. As the table shows, most of the methods give a robust estimation, i.e., their STD values are negligible. However, on average, MACE-means suggests a closer estimation to the accurate number of clusters and better values for ARI and VI.

The experimental results for 50 runs on the second data set in 2D space is presented in Table 3.4, where the number of clusters is known and the true labels are not available. As it is shown in the table, MACE-means, Sil, DB and CH are giving the most accurate estimations of the number of clusters. But among the mentioned methods, only MACEmeans estimates the m robustly with zero error. The remaining of the methods such as X-means, G-means and PG-means tend to overestimate the number of cluster. Our synthetic 2D data sets are generated by random selection of centers in a square of 20 × 20 for a range of true number of clusters. The variance in the clusters is one or two, and each data set is generated with 100 samples per cluster. Table 3.5 shows the results of different clustering methods for 50 runs over the explained data sets. For example, the first value in the second column (3±0) is the average of 50 simulations. Where for all of the simulations, the cluster variance is one and in each run the algorithm selects three random centers with uniform distribution in the square of 20 × 20. Note that with this set up we are covering a large set of possibilities as in each run there is a uniform chance 52

CHAPTER 3. MACE-MEANS CLUSTERING Table 3.3: Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for real data sets (average over 50 runs).
E [m ] ± ST D[m ] Method Vertebral m = 3 MACE-means ARI VI X-means G-means ARI VI PG-means ARI VI DaSpec ARI VI Sil+K-means DB+K-means KL+K-means wtertra+K-means CH+K-means 3±0 N/A N/A N/A 5±0 N/A N/A 1±0 N/A N/A 2±0 N/A N/A 2±0 2±0 2±0 3±0 2±0 Breast m = 2 2±0 0.211±0.423 0.081±0.162 N/A 92±0 0.011±0.023 0.849±1.698 10±0 0.041±0.082 0.441±0.883 1±0 0±0 0.161±0.323 2±0 2±0 2±0 4±0 2±0 Seeds m = 3 3 ±0 0.119±0.292 0.111±0.273 8±0 2±0 0.078±0.191 0.136±0.335 1 ±0 0±0 0.183±0.448 2±0 0.076±0.187 0.135±0.332 2±0 2±0 3±0 3±0 3±0 Multiple Features m = 10 11±0 0.019±0.077 0.149±0.599 16±0 33±0 0.010±0.040 0.209±0.838 1±0 0±0 0.143±0.575 1 ±0 0±0 0.143±0.575 2±0 7 ±0 18±0 4±0 18±0 Wave Forms m = 3 3±0 0.035±0.063 0.087±0.349 N/A 13±0 0.012±0.048 0.133±0.533 6±0 0.018±0.074 0.087±0.349 1±0 0±0 0.068±0.2746 2±0 2±0 3±0 4±0 2±0 Water Treatment Plant m = 13 10.1±0.31 N/A N/A N/A 6±0 N/A N/A 1±0 N/A N/A 7±0 N/A N/A 2 ±0 5±0 2±0 4±0 5±0

for different degrees of cluster overlapping. For each method, mean and standard deviation of the estimated number of clusters (m ^ ) along with the success rate in predicting the CNC is provided in the Table 3.5. The success rate is the percentage of the times that the true number of clusters is estimated correctly. As the table shows, by increasing the number of generated clusters as well as increasing the variance, recognizing the overlapped clusters will be a challenging task. All the comparing methods seem to tolerate and distinguish overlapping clusters with
2 , while MACE-means performs well a minimum distance between centers equal to 3w 2 . This performance was evaluated by 90 percent even for center distances as small as 2w

success rate in estimating the CNC. Table 3.5 also confirms that MACE-means provides a reliable success rate as the mean is closer to CNC and robustness of MACE-means

53

CHAPTER 3. MACE-MEANS CLUSTERING Table 3.4: Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for S data sets (average over 50 runs).

Method MACE-means X-means G-means PG-means DaSpec Sil+K-means DB+K-means KL+K-means wtertra+K-means CH+K-means

E [m ^ ] ± ST D[m ^] S1 S2   m = 15 m = 15 15±0 15±0 20±0 19±0 95±0 77±0 19±0 30±0 5±0 1±0 14.03±0.18 14.03±0.18 14±0 14.03±0.18 15.96±0.18 4±0 7.26±1.46 14.03±0.18 15.96±0.18 15.96±0.18

S3 = 15 15±0 16±0 87±0 18±0 1±0 15±0 15.96±0.18 2±0 15.86±0.73 15±0 m

S4 = 15 15±0 N/A 63±0 24±0 1±0 15±0 13.96±0.18 5±0 17.03±0.18 15±0 m

relative to other methods is proved by the small standard deviation values. The average of ARI and VI values over 50 simulations also confirms the accuracy of MACE-means compared with other methods, where larger ARI and smaller VI values show better clustering results. It seems that one of the main factors for larger standard deviation in other methods is due to level of sensitivity to the initial optimization parameters and being trapped in local minima. Table 3.6 shows similar results when the data dimension is increased to 3. In this case, for each of the 50 runs the centers are chosen with uniform distribution in a cube of 20×20×20. This increase in dimension of the data makes it easier to distinguish the clusters. Therefore, the methods were able to give a better clustering result for a larger number of clusters and variance values compared to Table 3.5. As the table shows, MACE-means is the most accurate and robust method in estimating the m for all of the data sets. The averaged ARI and VI values also confirm the superiority of MACE-means over the other methods.

54

CHAPTER 3. MACE-MEANS CLUSTERING Table 3.5: Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for our 2D synthetic data sets (averaged over 50 runs).
m = 3 d=2 MACE-means Success (%) ARI VI X-means Success (%) G-means Success (%) ARI VI PG-means Success (%) ARI VI DaSpec Success (%) ARI VI Sil+K-means Success (%) DB+K-means Success (%) KL+K-means Success (%) wtertra+K-means Success (%) CH+K-means Success (%)
2 =1 w 2 =2 w 2 =1 w

m = 4
2 =2 w 2 =1 w

m = 5
2 =2 w 2 =1 w

m = 6
2 =2 w

3±0 100 0.166±0.408 0±0 3±0 100 3±0 100 0.166±0.408 0±0 3±0 100 0.166±0.408 0±0 3±0 100 0.166±0.408 0±0 2.5±0.512 50 3±0 100 3±0 100 3±0 100 3±0 100

3±0 100 0.133±0.327 0.082±0.202 2.75±0.444 75 2.5±0.51299 50 0.089±0.218 0.103±0.254 3.25±1.118 50 0.071±0.175 0.155±0.381 2.2±0.410 20 0.088±0.217 0.093±0.228 2.2±0.410 20 2.2±0.410 20 3.65±1.308 30 3±0 100 2.75±0.444 75

3.9±0.307 90 0.067±0.191 0.123±0.348 3.8±0.410 80 3.6±0.680 70 0.038±0.109 0.132±0.374 3.45±0.686 55 0.040±0.113 0.118±0.334 2.85±1.04 35 0±0 0.173±0.490 2.95±0.887 35 3.35±0.670 45 4.5±1.539 50 2.9±0.640 15 3.7±0.470 70

3.95±0.223 95 0.072±0.203 0.132±0.374 3.1±0.967 50 3.5±1.357 20 0.058±0.164 0.104±0.295 3.15±0.988 55 0.058±0.164 0.105±0.297 2.25±1.118 10 0±0 0.173±0 3.15±0.988 55 3.15±0.988 55 3.15±0.988 55 3.4±0.598 45 3.15±0.988 55

5.3±0.470 70 0.096±0.305 0.012±0.038 4.4±0.502 40 4.7±0.470 70 0.096±0.305 0.012±0.038 4.1±0.852 40 0.096±0.303 0.0133±0.042 2.95±1.276 5 0.078±0.247 0.027±0.087 3.1±0.552 5 3.7±0.571 5 3.4±1.095 10 4.25±3.35±0.670 60 4.7±0.470 70

4.75±0.910 45 0.074±0.235 0.066±0.209 3.75±0.966 30 4.05±0.944 30 0.074±0.235 0.066±0.209 3.3±0.571 5 0.060±0.191 0.059±0.187 2.15±0.875 0 0.060±0.191 0.059±0.187 3.05±0.686 5 2.95±0.510 0 4.6±2.303 5 3.5±0.760 10 3.7±0.923 25

5.95±0.223 95 0.078±0.271 0.017±0.061 5.4±1.095 65 5.7±0.656 50 0.078±0.271 0.017±0.061 5.45±0.510 45 0.078±0.271 0.017±0.060 3.55±0.604 0 0.047±0.164 0.045±0.158 3.95±1.145 10 4.3±1.080 10 5.35±2.539 15 3.8±0.767 0 5.85±0.366 85

5.65±0.587 55 0.081±0.282 0.006±0.021 5.4±1.500 35 5.25±0.850 35 0.079±0.275 0.014±0.051 4.95±0.686 5 0.081±0.281 0.007±0.027 2.25±0.716 0 0.036±0.126 0.076±0.266 3.45±1.145 5 4±1.297 15 7.15±2.719 5

0 5.3±0.923 45

As the Table 3.5 and Table 3.6 show, MACE-means gives the best performance over the 2D and 3D Gaussian data sets. X-means is the second best method in terms of estimating the number of clusters. G-means and PG-means are the next successful methods but have less accuracy compared with MACE-means in terms of ARI, VI and estimated number of clusters. The provided results are highly affected by the number of iterations (see l in Section 3.5) in each run of the K-means or EM algorithm. In other words, both 55

CHAPTER 3. MACE-MEANS CLUSTERING of the mentioned algorithms are sensitive to the initialization error and can easily get trapped in local minima of their objective functions. To solve this issue, it is suggested to choose a large enough value for l and then choose the best solution among all of the iterations (the most optimized objective function). The choice of l should be a trade off between accuracy and computational complexity. In all of the simulations, we limited the l parameter of our method to its minimum value which is used by other methods.

3.7

Conclusions

In this Chapter, MACE-mean clustering was proposed as a wrapper around K-means for simultaneous clustering and estimating the correct number of clusters. We defined the Average Central Error (ACE) and proposed a method for calculation of CNC based on minimizing this error. One of the main contributions of this work was to provide probabilistic bounds for the unavailable ACE using the available cluster compactness. In clustering approaches, the initialization error propagates to the clustering process and affects estimation of CNC. Robustness of MACE-means is due to the choice of a single objective function that is used in both clustering and order selection. Comparison between MACE-means and widely used clustering methods demonstrated the robustness and accuracy of the proposed method in estimating the CNC, even for highly overlapped clusters. Time complexity of clustering methods are compared and it was shown that MACE-means has one of the lowest time complexities among them.

56

CHAPTER 3. MACE-MEANS CLUSTERING

Table 3.6: Mean and standard deviation of estimated number of clusters (E [m ^] ± ST D[m ^ ]) for our 3D synthetic data sets (averaged over 50 runs).
m = 5 d=3 MACE-means Success (%) ARI VI X-means Success (%) G-means Success (%) ARI VI PG-means Success (%) ARI VI DaSpec Success (%) ARI VI Sil+K-means Success (%) DB+K-means Success (%) KL+K-means Success (%) wtertra+K-means Success (%) CH+K-means Success (%)
2 w

m = 6
2 w

m = 7
2 w

m = 8
2 w

=2

=3

2 w

=2

=3

2 w

=2

=3

2 w

=2

2 =3 w

5±0.324 75 0.098±0.309 0±0 4.95±0.510 75 5±0.648 60 0.090±0.287 0.026±0.082 4.7±0.656 65 0.097±0.306 0.012±0.038 3.2±0.951 10 0.056±0.179 0.075±0.237 3.9±0.967 30 3.95±0.887 30 5.45±1.959 55 3.75±0.786 20 4.6±0.753 70

4.8±0.615 76 0.066±0.211 0.007±0.024 4.65±0.587 70 4.6±0.820 60 0.066±0.211 0.075±0.239 4.5±1.051 40 0.046±0.146 0.079±0.252 2.65±0.875 0 0.020±0.064 0.117±0.371 3.6±1.231 30 3.85±1.136 35 4.5±1.192 50 3.9±0.911 20 4.5±0.827 65

5.95±0.394 85 0.079±0.274 0.014±0.051 5.8±0.410 80 5.8±0.695 65 0.079±0.274 0.014±0.051 5.95±0.944 60 0.078±0.272 0.015±0.055 3.5±1.051 0 0.027±0.095 0.077±0.266 4.3±1.260 15 4.35±0.988 10 5.95±1.848 45 3.9±1.29 25 5.65±0.745 75

5.75±0.444 75 0.057±0.200 0.073±0.255 5.5±0.888 55 5.7±0.978 40 0.056±0.194 0.063±0.220 5.4±0.994 30 0.053±0.183 0.057±0.197 3±0.794 0 0.011±0.040 0.111±0.387 4.25±1.251 20 4.2±1.239 25 6.35±2.73 30 4.45±0.887 10 5.45±0.759 60

6.85±0.489 75 0.064±0.240 0.024±0.091 6.9±1.209 35 6.8±0.695 65 0.057±0.215 0.028±0.106 6.85±0.988 65 0.056±0.211 0.030±0.115 3.8±0.894 0 0.007±0.027 0.109±0.410 5.5±1.277 20 5.3±0.978 10 7.75±3.109 45 4.35±1.424 10 6.4±1.046 65

6.7±0.571 75 0.061±0.228 0.032±0.121 6.2±0.951 40 6.2±0.615 30 0.058±0.217 0.028±0.105 5.9±0.788 20 0.057±0.214 0.027±0.101 2.8±1.056 0 0.033±0.127 0.061±0.231 4.25±1.208 0 4.45±0.944 0 6.8±3.205 0 4.3±1.454 10 5.75±1.482 35

7.9±0.447 80 0.057±0.228 0.021±0.084 7.5±1.147 35 7.5±1.051 30 0.050±0.202 0.030±0.121 7.1±1.071 15 0.049±0.198 0.028±0.113 3.5±1.192 0 0±0 0.129±0.519 5.8±1.641 15 5.9±1.619 15 11.2±3.847 5 4.45±1.234 0 7.3±0.923 55

7.6±0.598 65 0.043±0.173 0.039±0.156 7.15±0.812 40 7±1.123 25 0.043±0.173 0.039±0.156 6.65±0.988 25 0.042±0.171 0.039±0.159 2.5±0.760 0 0.009±0.038 0.096±0.386 5.1±1.651 5 5.55±1.316 10 9.75±3.850 5 4.5±1.317 0 6.95±0.998 35

57

Chapter 4

Signature Testing (Sigtest) in Clustering
We propose a new statistical test denoted by signature testing (Sigtest) with the application in clustering and image classification. Sigtest relies on probabilistic validation of empirical distribution function of data. We implement Sigtest to estimate the number of clusters in hierarchical and partitional clustering. In addition we propose a new adaptive estimation of the vocabulary size in image classification. Simulation results on both real and synthetic data confirm superiority of Sigtest over existing statistical tests in both hierarchical and partitional clustering as it estimates the number of clusters more accurately. Sigtest also shows advantages in terms of adjusted rand index (ARI) and variation of information (VI). In addition, using Sigtest for adaptive choice of vocabulary size in bag of visual words improves the efficiency of the Support Vector Machines (SVM) classifier as well as reducing the time complexity of the overall algorithm.

58

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

4.1

Introduction

Sigtest can be employed for estimating the number of clusters. Unlike existing statistical tests it can be used with any prior assumption on the distribution of the clusters (Dcdf). In this Chapter, we focus on the application of Sigtest when prior assumption on distribution of clusters is Gaussian. Using the method with any other prior assumption is analogous to what is presented here. The chapter is organized as follows: Section 4.2 gives details of deriving Sigtest. Section 4.3 shows applications of Sigtest in hierarchical and partitional clustering along with BOVW image classification. Section 4.5 shows simulation results of using Sigtest on real and synthetic data sets, and Section 4.6 presents the conclusion.

4.2

Signature Testing

As discussed in Section 2.2, the goal of all statistical testing methods is to propose an efficient distance measure between the Dcdf and ecdf. In the following, we elaborate on an idea that our desired cdf can be represented by its signature through transformation of data which can benefit drastically in defining a distance measure. A signature of a cdf is a new cdf that is derived from the original cdf, which has much smaller variations and represents a set of dense data samples. The following example illustrates the notion of signatures. Consider a data that has been generated independently and identically distributed (iid) from a Gaussian distribution N (0,  2 ) with zero mean and variance  2 . Figure 4.1 (a) shows the histogram of 500 samples of a Gaussian random variable with zero mean and  = 1, N (0, 1). Figure 4.1 (b) shows 100 of such samples (length of the data is 500) that are plotted simultaneously over each other. As this figure shows, the original samples vary between ±3.5 ( = 1) which theoretically happens with the probability of 0.9995 for a Gaussian

59

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.1: (a) Histogram of 500 randomly generated samples belong to a Gaussian distribution with zero mean and unit variance. (b) The actual data without any manipulation. (c) Sorted absolute values of the samples. distribution. However, Figure 4.1 (c) shows the same 100 samples when the absolute value of those Gaussian samples are sorted. Based on this observation, it seems that the cdf of the sorted absolute version of the Gaussian distribution has much smaller variance compared to the cdf of data itself. For example, while the ranges in Figure 4.1 (b) and (c) are identical (0 < xlabel < 500, and 0 < ylabel < 4), the area shown in Figure 4.1 (c) compare to the area shown in Figure 4.1 (b) for exactly the same data is much smaller and denser. Figure 4.2 shows the same transformation on a mixture of two Gaussians that leads to a denser area (Figure 4.2 (c)) which can be used for signature testing. We denote such dense transformations of the original cdfs as the signature of those cdfs. In the following, we describe how those signatures can be used as statistical tests to compare ecdfs with the desired cdf. 60

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.2: (a) Histogram of 1000 randomly generated samples drawn from a Gaussian mixture model with mean values equal to zero and 5, and unit variances. (b) The actual data without any manipulation. (c) Sorted absolute values of the samples.

61

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.3: (a) sorted absolute version of 100 samples belong to a Gaussian distribution with zero mean and unit variance. (b) the same plot while x axis and y axis are swapped.

4.2.1

Formulation of Signature testing (Sigtest)

Let V = [V1 , V2 , · · · , VN ]T be a vector of iid random variables of length N , where v = [v1 , v2 , · · · , vN ]T is a sample of that random variable. For any given z the following random variable: g (V, z ) = 1 N
N

I (vi , z )
i=1

(4.1)

depicts the averaged number of vi s less than z, where I (vi , z ) was defined in (2.26) . If we let w = [w1 , w2 , · · · , wN ]T to represent the vector of sorted absolute values of v , then: n N

g (v, wn ) =

(4.2)

that means the g (v, wn ) is the normalized index of the absolute sorted version of v . Figure 4.3 (a) and (b) show this relationship by swapping the xlabel and ylabel.

62

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING It can be shown that [64]:

E [g (V, z )] = Fa (z ) var[g (V, z )] = 1 Fa (z )(1 - Fa (z )) N

(4.3) (4.4)

where Fa is the cdf of absolute value of vi s. This confirms that the variance of the index of the sorted version is of order of 1/N th of the original random variable (Fa and F have variances of the same order). Consequently sorted version of the original random variable is a good signature candidate of the original random variable. This signature has been used in denoising approaches to validate which small values of an observed samples are members of the noise distribution by comparing the sorted version of the absolute value of the observed data with the following boundaries:

S (z, ) = E [g (V, z )] +  S (z, ) = E [g (V, z )] - 

var[g (V, z )] var[g (V, z )]

(4.5)

where SK (z, ) and SK (z, ) are the probabilistic upper and lower bounds and the  parameter is related to probabilistic validation to satisfy the desired confidence probability pc for estimating the bounds of the index1 . An example of this signature testing for denoising is shown in Figure 4.4, where (a) is noisy signal with SN R = 5. Figure 4.4 (b) shows sorted version of the noisy data (blue line) and confidence region of the noisy data (red line). As the figure shows, at wn = 1.47 data is crossing the lower boundary which suggests values between zero and
1

Based on the Chebychev inequality, for the confidence probability pc : P r{|g (V, z ) - E [g (V, z )]|  J } = pc (4.6)

we will have J = 

var[g (V, z )], and it leads to the  

1 1-pc

which gives the upper limit of .

63

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.4: (a) Solid blue line is 100 samples of the noisy observed data (SNR = 5). (b) Blue line is 100 samples of the sorted absolute values of the noisy observed data crossing the noise confidence region (Red line) at wn = 1.47. The area between the red lines is the noise confidence region with probability 0.999997. wn are related to noise. While this idea can be used for any assumption on the desired distribution, in the following we provide the details for when the desired distribution is Gaussian. Calculation of Fa : Calculating a closed form for cdf of the absolute value of the random variable can be cumbersome. Note that as the desired cdf is known, in practical applications, we can find a good estimate of Fa by using sampling approaches such as Importance Sampling, Inverse Transform Sampling and Markov Chain Monte Carlo (MCMC). For a Gaussian distribution F , however, the closed form is (details are in Appendix C): 1 z-µ z+µ Fa (z ) = [erf (  ) + erf (  )] 2  2  2 (4.7)

where µ and  are mean and standard deviation of V respectively. The above cdf is identical to (4.3) and can be used in (4.5) to define the signature of Dcdf. In the following section, we show that this result can be easily extended for Gaussian mixture 64

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.5: Ten observed samples (red dots) and their corresponding boundaries (blue bars). models.

4.2.2

Sigtest in Statistical Testing

In Sigtest statistical testing, we would like to verify whether the observed data y = [y1 , · · · , yN ]T (defined in Section 2.2 ) belongs to a desired cdf (Dcdf). To use the sorting signature for such verification, we first sort the absolute value of the observed y and denote it by t = [t1 , · · · , tN ]T . We then compare i, which is the index of ti for any z = ti , with S (ti , ) and S (ti , ) that are the upper and lower bounds from the Dcdf in (4.5) for that observed ti . Consequently, each observed data will be tested against the bounds as follows: ci () =    0   1

S (ti , ) < otherwise

i N

< S (ti , )

(4.8)

this value is a flag to check whether the ith sample is inside the provided boundaries by Dcdf.

65

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING The overall Sigtestscore for y is suggested to be the percentage of consistency of the observed data with the Dcdf: 1 N
N

Sigtestscore () =

ci ()
i=1

(4.9)

Sigtestscore should be less than critical value T to accept the null hypothesis that ecdf is a sample of the Dcdf (H0 ), otherwise it is not a sample of the Dcdf (H1 ): · H0 : The observed sample (ecdf) is a sample of the Dcdf  Sigtestscore () < T · H1 : The observed sample (ecdf) is not a sample of the Dcdf  Sigtestscore ()  T For example, Figure 4.5 shows the results for () = 4.5 when 10 observed data samples are sorted. S (ti , ) and S (ti , ) for each sample are calculated, where blue bars represent the validated boundaries of the samples. As the figure shows, samples 2, 7 and 8 are outside of the boundaries and the rest are within the boundaries, where Sigtestscore (4.5) = 0.7. Similar to other statistical testing, The parameters  and T need to be chosen through some statistical analysis. In Appendix D, we provide detailed steps in choosing  and T for the case of Gaussian distribution using genetic algorithm. The optimal values with this approach are =0.53, T = 1.72 respectively. Note that for any other desired cdf same approach can be used for calculation of this parameters. Algorithm 1 shows Sigtest in statistical testing.

4.3

Sigtest in Clustering

In the following, we demonstrate how Sigtest can be used as a statistic test for estimating the number of clusters in hierarchical and partitional clustering. We also illustrate advantages of using Sigtest in image retrieval based on BOVW in order to estimate the size of visual vocabulary and improving the accuracy of classification. 66

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING Algorithm 1 Sigtest in Statistical Testing
Input: Input sample y of length N , model assumption (desired cdf), critical values T and . Output: result of the test for the model. 1: Sigtestscore ()  0 2: t  sort(abs(y )) 3: compute S and S f rom (4.5) 4: for i = 1 to N do i i 5: if N > S or N < S then 1 6: Sigtestscore ()  Sigtestscore () + N 7: end if 8: end for 9: if Sigtestscore () < T then 10: H0 : y  Dcdf 11: else 12: H1 : y  / Dcdf 13: end if

4.3.1

Sigtest in Hierarchical Clustering

In divisive or top down hierarchical clustering methods, we start at the top with all of the samples in one cluster. If the number of clusters is not known, samples will be split recursively until every cluster has only one sample. Adding a splitting criterion at each splitting stage can also estimate the number of clusters in hierarchical clustering. As a result, the process of cluster splitting will be stopped at the estimated number of clusters. In general, the splitting criterion is a statistic test. It compares the ecdf of the data and the desired cdf (Dcdf) as a splitting criterion: · H0 : The cluster data (ecdf) is a sample of the Dcdf (Split: No). · H1 : The cluster data (ecdf) is not a sample of the Dcdf (Split: Yes). Figure 4.6 shows an example of such hierarchical clustering with order selection in form of splitting criterion. Methods such as G-means and Dip-means clustering are examples of such clustering. The shaded block is the splitting step, i.e., the statistical test. The splitting test for G-means is AD test (Subsection 2.2.2 ) and for Dip-means is Dip test (Subsection 2.2.3). We propose to use Sigtest as the splitting criterion of the hierarchical methods. This 67

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Start

Data,   1

      ( ,  )   1

No

 = 

No

  

Yes

Split 
No

Yes

Yes

End

   + 1

   + 1

Figure 4.6: Hierarchical clustering with data splitting criterion. test can be used for both G-means and Dip-means clusterings and replace AD and Dip test. Figure 4.7 shows how this splitting criterion works. The solid lines are precalculated boundaries of sorted elements ti (S (ti , ) and S (ti , ) in (4.5)). The dashed line shows the sorted version of the absolute value of the data for two clusters. As it can be shown in Figure 4.7 (b), Sigtest suggests to split data with an ecdf which is not a sample of Dcdf. While in Figure 4.7 (a), it suggest that the ecdf is a sample of Dcdf and the related cluster will not be split. Detailed examples are provided in the simulations section.

4.3.2

Sigtest in Partitional Clustering

If the number of clusters is known to be K , partitional clustering method minimizes a given clustering criterion by iteratively relocating data points between K clusters until

68

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

(a)

(b)

Figure 4.7: Sigtest in hierarchical clustering: (a) H0 holds (no split), and (b) H1 holds (split). a (locally) optimal partition is attained [71]. If the number of clusters is not known, it needs to be estimated using a proper statistical test. For a considered range of K  [Kmin , Kmax ], ecdf of the data and the Dcdf of a model will be compared for: · H0 : The observed data (ecdf) is a sample of the model with Dcdf of K clusters. · H1 : The observed data (ecdf) is not a sample of the model with Dcdf K clusters. Starting from Kmin , the statistical test increases the value of K until H0 is satisfied. Figure 4.8 shows a partitional clustering method based on Gaussian mixture models (GMM), where Expectation Maximization (EM) is employed to estimate the parameters of the mixture model (center of clusters µ, covariance matrices  and components mixing factors  ). The shaded block shows the statistical testing step. This test in PG-means clustering is the KS test (briefly described in Subsection 2.2.1). We propose to replace KS with Sigtest and show the advantageous in the simulation section. In the case of Gaussian

69

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Start

Data, K=1

EM (Data, K)

K=K+1
No

GMM = K(µ,,)

Statistical Test
Yes

No

EM convergence

Yes

End

Figure 4.8: General procedure of partitional clustering with order selection. mixture models, the desired Fa (z ) used in (4.7) is in the form of [72] 2 :
K

Fa (z ) =
j =1

j Faj (z )

(4.10)

where Faj (z ) is the Gaussian cdf of the j th component, where provided in (4.7) and j is the mixing factor of that component in the mixture. Figure 4.9 shows an example of using Sigtest for such verification. In this example, the true number of clusters is 4 and Figure 4.9a shows the histogram of the data (y ).
2

Details for calculating Faj (z ) is provided in C.

70

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

(a) Histogram of projected data belong to (b) Ecdf of data (dashed line) is not a sama mixture of four Gaussians. ple of Dcdf (solid line) for K = 3.

(c) Ecdf of data (dashed line) is a sample of Dcdf (solid line) for K = 4.

Figure 4.9: Sigtest for model verification in Gaussian mixture models. If K is considered to be 3, the upper bound and lower bounds of Sigtest are solid lines in Figure 4.9b. The ecdf in this case, however, is the dashed line and as the figure shows it falls out of the boundaries. The method therefore increases the value of K to 4. Figure 4.9c shows the boundaries for K = 4. As the figure shows, in this case the ecdf completely fits within the boundaries. Sigtest stops and the estimated number of clusters is 4.
3

3

The optimum choice of alpha and T has improved the result of Sigtest compare to [73] and [74].

71

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

4.4

Optimum vocabulary size in bag of visual words using Sigtest

Image classification using bag of visual words is constructed based on transforming a 2D image and representing that into a 1D histograms. This approach has the following main steps: 1) feature extraction (methods such as scale-invariant feature transform (SIFT) [75], Dense SIFT [76] and Histograms of Oriented Gradients (HOG) [77]) 2) feature quantization to build a visual vocabulary of size K, 3) training and classification using Support Vector Machines (SVM) or similar classifiers. SIFT is an image descriptor for image matching and image recognition. It is computed from image intensities around the key point locations in image. SIFT is invariant to scaling transformations, rotations and translations in image domain and it is robust to moderate changes in illumination. Dense SIFT is a similar approach where SIFT descriptor is computed over dense grids in the image domain. HOG descriptor splits image into overlapped cells and computes histograms of gradients for each cell. Unlike SIFT, this method is not rotation invariant, however it is normalized with respect to image contrast. The concentration of our work is on the second step using SIFT feature of image. Conventionally for this step a fixed size for visual vocabulary is considered. For example, for most cases, they start with K = 500 words. This number is then given to k-means for calculation of those 500 visual words (centers). The SIFT of each image is then compared to these visual words and the histogram of the membership of these SIFTs to the centers is then provided. Figure 4.10 shows an example of a test image from Caltech101 data set (Figure 4.10 (a)) along with its histogram (Figure 4.10 (c)). Figure 4.10 (b) is a general example to show the middle step for feature quantization. Note that prefixing the vocabulary size, K , is providing a suboptimal solution for the problem of bag of visual words. Motivated by this fact, we propose to find an

72

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

(a)

(b)

(c) Figure 4.10: (a) Test image from Caltech101, (b) general example of quantizing features (blue dots) with their nearest centers (red dots) and (c) representing them as a histogram over the visual words. adaptive number of vocabulary size K by using the hierarchical G-means-Sigtest. This preprocessing algorithm in step 2 can benefit the next step and the overall answer of the classification. In the following next section, benefits of such preprocessing is elaborated for a set of image data sets (Subsection 4.5.3).

4.5

Experimental Results

We use real data sets with the following characteristics :

73

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING Table 4.1: Benchmark datasets. Data set Iris Pendigits 15 objects category 4 objects category Breast cancer Leukemia Optical digits Seed Wave Form Human activity MNIST COIL20 Number of samples 150 3498 45312 10008 699 70 1797 210 5000 2947 4000 1440 Dimension of data 4 16 128 128 9 40 64 7 21 561 784 1024 Number of clusters 3 10 15 4 2 3 10 3 3 6 10 20

Where 15 objects and 4 objects categories are from test cases in Caltech101 data sets [78]; MNIST and COIL20, from [79] and [80]; Rest of data sets are obtained from UCI repository [81]. In addition to the benchmark data sets, the synthetic data is a set of Gaussian clusters with  = 1 and 100 samples in each cluster (Table 4.2). The centers of clusters are chosen randomly inside a hypercube with each side of 20 . Table 4.2: Synthetic data sets. Data set S1 S2 S3 S4 Number of samples 1000 2000 3000 4000 Dimension of data 4 10 16 32 Number of clusters 10 20 30 40

Adjusted Rand Index (ARI) and Variation of Information (VI) are used for comparison and to measure the quality of clustering. A more efficient clustering has a smaller value of VI and larger value of ARI [82], [83]. In the following table, N/A shows that clustering method was unable to converge to a solution.

74

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

4.5.1

Hierarchical Clustering

The comparison results between G-means and G-means-Sigtest is shown in Table 4.3. G-means-Sigtest is the G-means where the AD splitting criterion is replaced by Sigtest. Each number in the table is in the form of E [ · ] ± std[ · ] which shows the mean and standard deviation of the estimated values based on the averaged results over 20 simulation. As the table shows, G-means-Sigtest compared with the G-means has a better estimation of the number of clusters. For Gmeans-Sigtest also we have smaller VI and larger ARI indexes (better performance). We denote Dip-means-Sigtest in which Dip statistical test is replaced by Sigtest. Table 4.3 shows the result for Dip-means and Dip-means-Sigtest for both synthetic and real data sets. As the table shows, replacing Dip test with Sigtest has significantly improved the result of clustering in terms of estimated number of clusters, ARI and VI indexes.

4.5.2

Partitional clustering

The comparison result between PG-means and PG-means-Sigtest where the statistical test KS is replaced by Sigtest is shown in Table 4.3. Each number in the table is in the form of E [ · ] ± std[ · ] which shows the mean and standard deviation values based on the averaged results over 20 simulation. As the table shows, PG-means-Sigtest compared with the PG-means has a better estimation of the number of clusters. For PGmeans-Sigtest also we have smaller VI and larger ARI indexes. Table 4.3 also shows a comparison between MACE-means clustering and the above mentioned methods. As the table shows, MACE-means has its best performance on Gaussian data sets and has difficulties in clustering real and non-Gaussian data sets.

75

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING Table 4.3: G-means and G-means-Sigtest
Data set Iris VI ARI Optical digits VI ARI Leukemia VI ARI Seed VI ARI Pendigits VI ARI COIL20 VI ARI wave form VI ARI Human Activity VI ARI Breast cancer VI ARI MNIST VI ARI S1 VI ARI S2 VI ARI S3 VI ARI S4 VI ARI G-means 3±0 0.52±0 0.56±0 18.50±1.32 1.76±0.07 0.27±0.03 4±0 0.53±0 0.75±0 2±0 0.81±0 0.47±0 27.96±2.92 1.63±0.06 0.46±0.02 41.40±2.95 1.56 ±0.06 0.43±0.02 9±0 1.76 ±0 0.23±0 29.20±3.27 2.13 ±0.08 0.32±0.02 52±8.14 2.38 ±0.20 0.20±0.02 23.80±0 2.63 ±0.03 0.28±0 10.5±0.60 0.09±0.07 0.95±0.02 20.1±0.44 0.003±0 0.99±0.0 31.35±1.49 0.02±0.02 0.98±0.01 44.4±1.75 0.07±0.02 0.97±0.01 G-means-Sigtest 3±0 0.52±0 0.56±0 6.36±0.77 1.29±0.05 0.63±0.02 3±0 0.36±0 0.84±0 2±0 0.81±0 0.47±0 17.28±0.75 1.55±0.02 0.50±0 18.90±2.23 0.73 ± 0.07 0.64 ±0.02 5±0 1.49 ± 0 0.25±0 4±0 1.02 ± 0 0.53±0 2±0 0.32 ± 0 0.84±0 10.20±1.09 1.69 ± 0.02 0.45±0 10.15±0.48 0.07±0.08 0.97±0.05 20.05±0.22 0.001±0 0.99±0 30.9±0.91 0.01±0.01 0.99±0 43.85±1.81 0.06±0.03 0.97±0.01 1±0 2.299±0 0±0 8.65±2.32 0.08±0.09 0.81±0.31 17.85±3.9 0.27±0.52 0.82±0.32 18±8.20 1.047±0.74 0.42±0.34 13.1±4.48 2.07±0.32 0.096±0.02 1±0 2.299±0 0±0 9.55±1.09 0.05±0.06 0.92±0.15 20.45±0.51 0.01±0.01 0.99±0 29.65±1.81 0.034±0.14 0.97±0.12 39.95±1.05 0.01±0.04 0.98±0.04 10±1.16 0.08±0.09 0.954±0.05 19.95±0.94 0.08±0.04 0.94±0.02 27.7±1.08 0.138±0.04 0.89±0.03 35.45±1.35 0.24±0.09 0.75±0.09 9.95±0.60 0.05±0.06 0.972±0.04 20.25±0.71 0.02±0.02 0.98±0.01 29.15±1.34 0.134±0.04 0.92±0.02 36.9±1.51 0.15±0.08 0.84±0.10 Dip-means 2±0 0.60±0 0.53±0 1±0 2.302±0 0±0 2±0 0.76±0 0.52±0 1±0 1.0986±0 0±0 7±0 1.586±0 0.34±0 3±0 2.73±0 0.07±0 2±0 1.106±0 0.371±0 3±0 0.770±0 0.49±0 N/A Dip-means-Sigtest 3±0 0.67±0.13 0.56±0.11 16±0 1.22±0 0.62±0 3±0 0.30±0 0.88±0 3±0 0.66±0 0.71±0 10.2±0.44 1.401±0.00 0.57±0.00 44.8±0.83 1.40±0 0.54±0 5±0 1.427±0 0.291± 21±1.4 1.59±0.01 0.37±0 N/A 6±0 0.973±0 0.500±0 N/A 6±0 0.928±0 0.524±0 N/A 6±0 1.39±0 0.24±50 N/A 4±0 1.38±0 0.29±0 N/A 11±0 1.45±0 0.47±50 N/A 10±0 1.38±0 0.48±0 N/A 4±0 1.21±0 0.003±0 N/A 6±0 1.30±0 0.001±0 N/A PG-means 2±0 0.46±0 0.56±0 N/A PG-means-Sigtest 4±0 0.413±0 0.841±0 N/A MACE-means 5±0 0.133±0.326 0.101±0.248 1±0 0.115±0.514 0±0 2±0 0.095±0.261 0.065±0.179 3±0 0.111± 0.273 0.1194± 0.292 1±0 0.115±0.514 0±0 1±0 2.99±0 0±0 2±0 0.184±0.451 0.061±0.151 1±0 1.78±0 0±0 2±0 0.081±0.162 0.211±0.423 1±0 0.115±0 0±0 9.95±0.223 0.003±0.015 0.049±0.219 20±0 0±0 1±0 30.50 ± 0.527 0.003±0.003 0.016 ± 0.128 41±1.25 0.09±0.01 0.88±0.11

76

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

4.5.3

Adaptive vocabulary size in bag of visual words

As discussed in Subsection 4.4, the size of vocabulary is a pre-assumed fixed number in the range of K = [500, 1000] in most cases. Here, we suggest using G-means and Gmeans-Sigtest to adaptively estimate the size of the vocabulary for 15 objects category and 4 objects category data sets, instead of the traditional fixed value of K = 500. This two methods are chosen due to their high accuracy and fast convergence in large data sets. Figure 4.11 illustrates the accuracy of classification for 15 class of objects based on different number of clusters or vocabulary sizes (blue line). In this figure, green and red dashed lines show the estimated number of clusters using G-means-Sigtest and original G-means respectively. As figure shows, G-means-Sigtest chooses 593 for the size of vocabulary that results in the highest accuracy of classification. Fixed value of K=500 has less accuracy of classification and G-means chooses 1184 as the vocabulary size with much less accuracy in classification. Figure 4.12 shows similar results for BOVW experiment on 4 class of objects. As the figure shows, in this case both Gmeans-Sigtest and Gmeans choose less number of vocabulary size than 500. While Gmeans chooses 218, Gmeans-Sigstest chooses much smaller value of 74 with the highest accuracy in classification for different vocabulary sizes. Sigtest results in the smallest possible number of clusters along with the highest classification rate with less time complexity compared to when K is pre set to 500.

4.6

Conclusions

In this Chapter, we proposed Signature Testing (Sigtest) as a new statistical testing method for estimating the number of clusters in both hierarchical and partitioning clustering methods. In addition we propose using Sigtest in image classification using bag of visual words for adaptive choice of the size of visual vocabulary. Simulation results

77

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.11: Accuracy of SVM classifier for different number of clusters (size of visual vocabulary) for 15 objects category from Caltech101 data set. Black dashed shows the accuracy at the location of K = 500 (fixed size assumption), green dashed shows the chosen value K=593 by G-means-Sigtest, and red dashed line shows the accuracy of G-means for estimated K = 1184. confirm advantageous of using Sigtest as the statistical test in clustering in terms of more accurate choice of number of clusters, and better values for ARI and VI. The results also show that Sigtest improves the accuracy of image classification as well as reducing the time complexity in bag of visual words.

78

CHAPTER 4. SIGNATURE TESTING (SIGTEST) IN CLUSTERING

Figure 4.12: Accuracy of SVM classifier for different number of clusters (size of visual vocabulary) for 4 objects category from Caltech101 data set. Black dashed line shows the accuracy at the location K = 500 (fixed size assumption), green dashed line shows the chosen value K= 74 by G-means-Sigtest, and red dashed line shows the accuracy of G-means for estimated K = 218.

79

Chapter 5

Minimum Pathways in Arbitrary Shaped Clustering (minPAS clustering)
In this chapter we consider data clustering for arbitrary shaped clusters. Briefly, this class of clusters do not follow a simple and regular known distributions such as Gaussian or Log-normal. In general, shape of an arbitrary cluster cannot be easily modeled by a single mathematically available distributions. Therefore, majority of the model based clustering approaches are unable to cluster arbitrary shaped clusters with a reasonable level of error.

5.1

Data assumptions

The main assumptions in arbitrary shaped clustering are as follows: data samples represent clusters with arbitrary shapes, arbitrary densities and arbitrary sizes. Figure 5.1 is an example of arbitrary shaped clusters which shows a ring cluster with another cluster inside it. In this Figure, the center cluster could be approximated with a Gaussian

80

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) distribution while the ring cluster cannot be modeled with simple distributions.

1.5

1

0.5

0

-0.5

-1

-1.5 -1.5

-1

-0.5

0

0.5

1

1.5

Figure 5.1: Two centered clusters. consequently, clustering methods which are limited to a specific class of distributions will not be able to recover these clusters properly.

5.1.1

Data Skeleton Using Minimum Spanning Tree

Let X = [x1 , x2 , · · · , xN ]T be a vector of N samples, where xi  RD , and D shows the data dimensionality. sN ×N is a symmetric dissimilarity matrix for the samples, where s(i, j ) = dxi xj (dxi xj  R) is a weight to measure the distance between xi and xj . We define G(X, E ) as an undirected graph, where E = {eij : e(xi , xj ), (i, j )  [1, · · · , N ]} is a vector of undirected edges between the samples in X . The weight of edge eij is denoted by dxi xj and can be calculated as follows:
2 2

dxi xj = xi - xj

(5.1)

We let E be an acyclic subset of E (E  E ) that connects all of the samples, and its size is |E | = N - 1. Therefore, the overall weight of edges, WE , can be given as follows:

81

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

WE =
E

dxi xj

(5.2)

Then, minimum spanning tree (MST) T (X, E  ) is also an acyclic subgraph of G that passes through all of the samples in X and has the following set of edges [84], [85]:

E  = arg min
E E E

WE

(5.3)

Therefore, among all of the possible trees in X , MST has the minimum overall weight Wmin . The MST of samples in Figure 5.1 is shown in Figure 5.2. As the figure shows, any two samples xi and xj have exactly one edge eij between them. The MST of data can be constructed by any of proposed algorithms in [86] or [87].

Figure 5.2: Minimum spanning tree of 300 samples. In the following section, we use MST of data as a robust and unique structure to define the minimum pathways between samples in the tree.

82

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

5.1.2

Minimum Pathways in Arbitrary Shaped Data

In this section, we employ the minimum spanning tree of the data samples for defining a unique pathway between members of arbitrary shaped clusters. These pathways will be used as relational measures to evaluate the dependency of each sample to an exemplar candidate of each cluster. The notion of MST requires that any pair of xi and xj in T (X, E  ) have a unique
 shortest path between them. For an arbitrary shaped data set X , we denote Tp (Xxi xj , Ex ) i xj

as the minimum pathway between xi and xj , which is a subtree of T (Tp  T ). Xxi xj = [xi , xi+1 , · · · , xj ] (Xxi xj  X ) is a sequence of samples that construct the minimum path
 between xi and xj . Ex = {ekk+1 : e(xk , xk+1 ), i  k < j } represents the set of edges i xj

for each consecutive samples in X .

 We let dp xi xj be the distance weights of edges in E between xi and xj :

dp xi xj = [dxi xj (1), · · · , dxi xj (k )]

(5.4)

In other words, dp xixj includes all of the step sizes that are required to traverse from xi to xj and vice versa. Figure 5.3a shows an example of a minimum pathway in arbitrary shaped data (minPAS). In this figure, minPAS (the blue subtree) has connected two samples xi (red dot) and xj (green dot) from the same cluster. Connected samples are members of Xxi xj and the step sizes are members of dp xixj . Figure 5.3b shows another minPAS for the same data with two samples from different clusters. As these figures show, the step sizes in each pathway can be used for learning the level of similarity between samples in arbitrary shaped clusters.

83

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

1.5

1.5

1

1

0.5

0.5

0

0

-0.5

-0.5

-1

-1

-1.5 -1.5

-1

-0.5

0

0.5

1

1.5

-1.5 -1.5

-1

-0.5

0

0.5

1

1.5

(a) minPAS between xi and xj when samples belong to the same clusters.

(b) minPAS between xi and xj when samples belong to different clusters.

(c) Step sizes in minPAS between xi and xj . Samples belong to the same cluster.

(d) Step sizes in minPAS between xi and xj . Samples belong to different clusters.

Figure 5.3: Minimum pathways between two samples from the same and different clusters.

84

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

5.1.3

Membership Score

Clustering methods with assumption of having a specific distribution for clusters cannot be applied directly on arbitrary shaped data. This is mainly a result of having distance based similarity measures in those clustering methods. As a result, simple similarity measures like euclidean distance cannot work well on clusters with complex geometries. For example, euclidean distances between the samples xi and xj s in Figure 5.3a and Figure 5.3b suggest more similarity between the samples in Figure 5.3b compared to Figure 5.3a. Therefore, this can lead to a wrong sample membership in clusters. For example, clustering methods like K-means with assumption of spherical clusters are not able to cluster these samples correctly. Our motivation in this Subsection is defining an efficient membership score which can assign arbitrary shaped samples to their related exemplars without imposing any predefined geometry on the data (for example, Gaussian assumption imposes spherical clusters). Using (5.4), we let Score(xi , xj ) be the dissimilarity score between the samples xi and xj :

Score(xi , xj ) = max dp xi xj (l)
1lk

(5.5)

Figure 5.3c shows required step sizes for traversing between xi and xj in Figure 5.3a, where dissimilarity Score(xi , xj ) = 0.11. In a similar example, Figure 5.3d shows required step sizes for traversing between xi and xj in Figure 5.3b, where dissimilarity Score(xi , xj ) = 0.33 As the figures show, this simple dissimilarity score can be employed as a membership score to assign samples to their related exemplars. For example, two samples with larger jumps in the pathway between them are less likely to be from the same cluster compared with two other samples that have smaller jumps in their pathway. However, this is only one of the possible membership scores based on the minPAS between two samples and

85

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) it can be extended for designing similar scores.

5.2

minimum Pathway in Arbitrary Shaped clustering (minPAS)

In the previous section, we explained details of extracting a unique tree structure for arbitrary shaped data. We showed that the minimum pathways between samples can be used as a dissimilarity measure between samples. In the following, we show steps of our proposed clustering method, denoted by minimum Pathways in Arbitrary Shaped clustering (minPAS clustering). minPAS clustering groups samples in one cluster and then iteratively increases the number of clusters until a stopping criterion is satisfied. Lets Ci be the first cluster exemplar chosen from the data samples with the following conditions: 1) Ci is not a leaf in the MST, 2) Ci has the minimum averaged distance with its nearest neighbors in the MST. It follows that for any Ci , the dissimilarity Score(Ci , X ) between samples and the exemplar can be categorized into two main regions with the maximum and minimum dissimilarities. The region of minimum dismin , includes all of the members of exemplar C . The region similarity, denoted by Ri i

with maximum dissimilarity are samples that cannot be assigned to the exemplar. At each iteration of the algorithm, Rmax includes a subset of the region with maximum dissimilarity. We iteratively subtract Rmax s from the range of available samples and
min . assign the final remaining samples to Ri

We let Z be an intermediate set to track the available samples. The initial set of values in Z is X : Zi,0 = {x1 , x2 , · · · , xN } I) Excluding non-members of Ci : 86 (5.6)

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)
max at the K th step is defined as follows: For a chosen Ci , the Ri,k +1

max Ri,k +1 = {x|Score(Ci , x) = max Score(Ci , y )} y Zi,k

(5.7)

max is greater than  , where  is the minimum possible number of members if the size of Ri,k +1

in a cluster ( is a predefined parameter in majority of arbitrary shaped clustering methods):
max |Ri,k +1 | > 

(5.8)

max can be one or more potential cluster(s) that we are going to discover in the then Ri,k +1

next steps. Therefore, we exclude it from Zi,k to reach to the final members of Ci+1 :
max Zi ,k+1 = {x|x  Zi,k and x  Ri,k +1 }

(5.9)

max |   . Z  will be members of the The above routine (I) will be repeated until |Ri,k i +1

cluster with exemplar Ci :
min Ri = Zi

(5.10)

II) Stopping Criterion at Each Ci :
s be the union set of all members of the previous exemplars: We let Ri i s Ri = j =1 s is smaller than N (total number of samples): if the size of Ri min Ri

(5.11)

s |Ri |<N

(5.12)

This indicates that not all of the data set is labeled. Therefore, we continue searching

87

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

Figure 5.4: Dissimilarity Scores based on assumption of having C1 = x150 . for new clusters Ci s and redefine the Z as follows:
s Zi+1,0 = {x|x  X and x  Ri }

(5.13)

Consequently, we choose a new exemplar Ci+1  Z (Ci+1 should not be a leaf in the MST and it should have the minimum averaged distance with its neighbors in the MST) and calculate Score(Ci+1 , X ). Next, for the updated Z and Ci+1 , we start over from the procedure (I) (5.7) to the end except (5.12) is not true. The dissimilarity scores of the exemplars can be used to find the label of each sample xi : label(i) = arg min(Score(Cj , xi ))
j

(5.14)

For example, Figure 5.4 shows the dissimilarity score of each sample related to Ci =
max and excluding that x150 as the exemplar (Score(Ci , X )). Here, there is only one Ri, 0 min . Then, the new exemplar C from the samples leads to the region of Ri i+1 = x201 max , which leads to Score(C min  Rmin is chosen from Ri, i+1 , X ) in Figure 5.5. Since Ri 0 i+1

covers all of the N samples, the algorithm stops. Figure 5.6 shows the final scores for two possible exemplars. Consequently, using (5.14) the labels of clusters will be provided.

88

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

Figure 5.5: Dissimilarity Scores based on assumption of having C1 = x201 .

Figure 5.6: Sample Scores for Ci (blue line) and Ci+1 (red line).

89

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) Algorithm 2 shows steps of the explained procedure in minPAS clustering. Algorithm 2 minPAS clustering
Input: data X ,  . Output: Cluster labels, number of clusters K . 1: T (X, E  )  M ST (X ) 2: i  0 3: Z  X 4: Rs   5: while |Rs | < N do 6: ii+1 7: Z  X - Rs 8: choose Ci  Z 9: calculate Score(Ci , X ) using (5.5) 10: calculate Rmax using (5.7) 11: while |Rmax | >  do 12: Z  Z - Rmax 13: calculate Rmax using (5.7) 14: end while min 15: Ri Z i s min 16: R  j =1 Rj 17: end while 18: K  i 19: for i = 1  N do 20: label(i) = arg minj (Score(Cj , xi )) 21: end for

5.3

Computational Complexity Comparison

Computational complexity of minPAS clustering is O(N 2 )+O(KE log(N )), where O(N 2 ) is related to the calculation of similarity matrix for N samples. O(KE log(N )) is the required computational complexity in Dijkstra's algorithm for calculating the shortest pathways between samples, where E is the number of edges in the minimum spanning tree of data and K is the number of clusters. DBSCAN has a computational complexity of O(N 2 ). Affinity Propagation has a computational complexity of O(N 2 l), where l is the number of iterations in the algorithm.

90

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

5.4

Experimental Results

In this section we compare minPAS clustering with Data Spectroscopic clustering (DaSpec), Affinity Propagation (AP), DBSCAN and Agglomerative clustering using ward's method. In our experiments, the number of clusters is provided to ward's method clustering, while the rest of methods can estimate it independently. The methods are compared based on arbitrary shaped datasets such as Atom and Chain link in [88], and our synthetic datasets as well as real data sets such as Iris, Seeds and Wine. The adjusted random index (ARI) and variation of information (VI) are two quality measures that we have employed in our analysis [83] [82]. Figure 5.7 shows the comparison between methods on a ring cluster with a Gaussian cluster in the center. We have repeated the experiment for different clustering parameters (minPts in DBSCAN and  in minPAS clustering) and only provided the distinct results. As the figure shows, AP and ward's method clustering cannot distinguish the clusters correctly. minPAS, DaSpec and DBSCAN are the only clustering methods that for a specific range of parameters can cluster the samples correctly. DBSCAN can only recognize the clusters for a small range of 10  minP ts  20, while the parameter of minPAS clustering is less sensitive and provides the correct results for a wider range of 5    198. Table 5.1 shows the values of ARI and VI, where smaller VI and larger ARI show a better clustering result. As the table shows, minPAS clustering has the best ARI and VI values among the methods. Table 5.1: Quality of clustering in ring data set. Method minPAS Affinity Propagation Ward's method DBSCAN DBSCAN DBSCAN DaSpec Parameter [5,198] ARI 1 -0.002 -0.021 0.986 1 0 1 VI 0 2.152 0.956 0.044 0 0.636 0

5 [10,20] 30

91

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) Affinity Propagation.

(b) Ward's method.

(c) minPAS for 5    198.

(d) DBSCAN for minPts = 5.

(e) DBSCAN for 10  minP ts  20.

(f) DBSCAN for minPts = 30.

(g) DaSpec.

Figure 5.7: Ring data set.

92

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) Figure 5.8 shows the result of clustering for a 3-dimensional spiral cluster with a Gaussian ball on top of it. As the figure shows, only minPAS was able to recognize the clusters with a significantly better VI and ARI values (Table 5.2), where changing the parameters didn't make any improvement in the results. Table 5.2: Quality of clustering spiral and ball data set. Method minPAS Affinity Propagation Ward's method DBSCAN DaSpec Parameter ARI 0.977 -0.063 -0.078 0 0 VI 0.041 1.179 0.896 0.376 0.376

Figure 5.9 shows the comparison between methods on heart data set. minPAS provides an accurate clustering result for a wide range of parameter 5    48. The second successful method is DBSCAN which in its best case is limited to minPts = 5 and it was not able to cluster all of the samples correctly. Table 5.3 shows better ARI and VI values for minPAS clustering on heart data set. Table 5.3: Quality of clustering in heart data set. Method minPAS Affinity Propagation Ward's method DBSCAN DBSCAN DaSpec Parameter [5,48] ARI 1 0.120 0.189 0.997 0.994 0 VI 0 1.523 1.294 0.050 0.018 1.053

5 10

Figure 5.10 shows the simulation results on Atom data set consists of two clusters, a small ball in the center of a spherical cluster. The best result of clustering belong to minPAS, where DaSpec, DBSCAN, AP and ward's method had the worst clustering results (Table 5.4). Figure 5.11 shows the simulation results of Chain link data set. Chain link consists of two linked rings in 3-dimensional space. As the figure shows, minPAS clustering 93

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) minPAS.

(b) Ward's method .

(c) DBSCAN.

(d) Affinity Propagation.

94

(e) DaSpec.

Figure 5.8: Spiral and ball data set.

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) DaSpec.

(b) Affinity Propagation.

(c) Ward's method .

(d) minPAS for 5    48.

(e) DBSCAN for minPts = 5.

(f) DBSCAN for minPts = 10.

Figure 5.9: Heart.

95

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) DBSCAN for 4  minP ts  8.

(b) DBSCAN for 9  minP ts  184.

(c) DBSCAN for minP ts  185.

(d) minPAS for 4    140.

(e) minPAS for 141    283.

(f) minPAS for 284    398.

(g) minPAS for   399.

(h) Ward's method .

96
(i) Affinity Propagation. (j) DaSpec.

Figure 5.10: Atom data set.

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) Table 5.4: Quality of clustering in Atom data set. Method minPAS minPAS minPAS DBSCAN DBSCAN Affinity Propagation Ward's method DaSpec Parameter [4,140] [141,283] [284,398] [4,8] [9,184] ARI 0.985 0.995 1 0.8554 1 0.260 0.2384 0 VI 0.038 0.0157 0 0.353 0 1.118 1.065 0.832

can recognize two clusters for the wide range of 5    498, while in DBSCAN the same result is given for a much smaller range of parameter minPts, 5  minP ts  153. This fact shows that minPAS clustering is less sensitive to the choice of its parameter compared to DBSCAN. The ARI and VI values are shown in Table 5.5. Table 5.5: Quality of clustering in chain link data set. Method minPAS DBSCAN Affinity Propagation Ward's method DaSpec Parameter [5,498] [5,153] ARI 1 1 0.213 0.280 0 VI 0 0 1.504 0.806 0.693

Figure 5.12 shows the simulation results of half moon data sets. Among the methods, minPAS has the most accurate result of clustering for a wide range of  . DBSCAN is the second accurate method with more sensibility to the parameter minPts. Table 5.6 shows minPAS clustering has the best ARI and VI values among other methods. Table 5.7 shows the comparison between clustering methods on three real data sets Iris, Seeds and Wine, where the number of clusters in each of them is three. As the table shows, Ward's method has the best result in terms of ARI and VI. The second best method is DaSpec. minPAS and AP provide a better estimation of the number of clusters but have difficulties in partitioning the clusters.

97

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) DBSCAN for minP ts  154.

(b) DBSCAN for 5  minP ts  153.

(c) minPAS for 5    498.

(d) minPAS for   499.

(e) DaSpec.

(f) Affinity Propagation.

(g) Ward's method.

Figure 5.11: Chainlink data set.

98

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING)

(a) minPAS for 5    98.

(b) DBSCAN for minP ts = 5.

(c) DBSCAN for minP ts = 7. (d) DBSCAN for minP ts = 10.

(e) Affinity Propagation.

(f) Ward's method

(g) DaSpec.

Figure 5.12: Half moon data set.

99

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) Table 5.6: Quality of clustering in half moon data set. Method minPAS DBSCAN DBSCAN DBSCAN Affinity Propagation Ward's method DaSpec Parameter [5,98] 5 7 10 ARI 1 0.599 0.458 0.031 0.146 0.754 0.157 VI 0 0.783 0.814 1.380 1.204 0.418 0.988

Table 5.7: Quality of clustering in real data sets. Method minPAS Estimated K ARI VI Affinity Propagation Estimated K ARI VI Ward's method Estimated K ARI VI DBSCAN Estimated K ARI VI DaSpec Estimated K ARI VI Iris 3 0.563 0.508 2 0.449 0.845 N/A 0.731 0.499 2 0.568 0.462 2 0.568 0.462 Seeds 3 0.002 1.196 3 0.001 1.253 N/A 0.713 0.587 3 0.001 1.253 2 0.459 0.815 Wine 1 0 1.086 3 0.296 1.391 N/A 0.931 0.198 1 0 1.086 3 0.371 1.017

5.5

Conclusion

In this Chapter, we proposed minimum pathways in arbitrary shaped clustering (minPAS clustering) for clustering arbitrary shaped data. minPAS can estimate the number of clusters independently and measure the similarity of samples based on minimum pathways between them. The proposed method does not impose any assumption on the 100

CHAPTER 5. MINIMUM PATHWAYS IN ARBITRARY SHAPED CLUSTERING (MINPAS CLUSTERING) distribution of data and can recognize arbitrary shaped clusters accurately. The simulation results on a wide range of arbitrary shaped clusters shows the superiority of minPAS clustering over similar methods in terms of accuracy and insensitivity to the algorithm parameter.

101

Chapter 6

Conclusions and Future Works
In this thesis, we studied three problems of data clustering in terms of estimating the number of clusters, the role of statistical tests as splitting criteria in partitional and hierarchical clustering and challenges in clustering and estimating the number of clusters in arbitrary shaped data. We proposed minimum averaged central error (MACE)-means clustering as a new clustering method in Chapter 3. MACE-means clusters data and estimate the correct number of clusters (CNC) by minimizing the data reconstruction error. We derived the probabilistic bounds on unobservable data reconstruction error by using observable data error, and showed that minimizing the upper bound of this error leads to estimation of the CNC. Unlike majority of the clustering methods which have different objective functions for estimating the CNC and clustering data, MACE-means is constructed based on a unique objective function for both of them. The experimental results showed superiority of MACE-means in estimating the number of clusters over similar approaches as well as ARI and VI values. Note that MACE-means dependency on the assumption of having the same variance in clusters is a disadvantage of the method which should be addressed in the future work. MACE-means was proposed based on K-means due to its low computational complexity and simplicity. Nevertheless, MACE has the potential to

102

CHAPTER 6. CONCLUSIONS AND FUTURE WORKS be used with other clustering methods. Another potential future work will be extending the MACE fundamentals to use with clustering methods with wider range of assumptions beyond the spherical Gaussian. As MACE-means has better performance on low dimensional data, working on scalability of the algorithm will be another direction for improving MACE-means. We proposed Signature testing (Sigtest) as a statistical test in Chapter 4. Sigtest is motivated by this fact that sorted absolute value of observed data has much smaller variation compared to the original data and the resulted data will be represented in a much denser space. This dense region of the transformed data is used for designing a signature for any desired cumulative distribution function (cdf). We showed analytical steps for deriving upper bound and lower bound of the designed signature and used it for comparison with the empirical cumulative distribution function (ecdf) of the test data. We showed applications of Sigtest in hierarchical and partitional clustering algorithms where statistical tests in G-means, PG-means and Dip-means clustering algorithms were replaced by Sigtest. The simulation results showed that resulted clustering algorithms, denoted by G-means-Sigtest, PG-means-Sigtest and Dip-means-Sigtest have significantly improved the accuracy of clustering compared to the original methods. Another proposed application of Sigtest was adaptively estimating the size of vocabulary in bag of visual words (BOVW) problem. While majority of the BOVW methods use a prefixed vocabulary size, we show that using Sigtest can improve the accuracy of image classification and also decrease the time complexity of the algorithm. As fundamentals of Sigtest has been proposed for any considered distribution, applying Sigtest on non-Gaussian distributions is a possible interesting future work to extend the applications of Sigtest. In Chapter 5 we proposed minimum Pathways in Arbitrary Shaped (minPAS) clustering for data sets with arbitrary distributions. MinPAS is constructed based on minimum spanning tree structure of samples. We showed that having the tree structure of data, each sample can be related to the exemplar of the cluster using a minimum pathway. As

103

CHAPTER 6. CONCLUSIONS AND FUTURE WORKS a result, the similarity measure between samples will be highly affected by geometry of the data samples without relying on distribution assumptions. The experimental results showed that minPAS is more efficient than state of the art methods such as DBSCAN, DaSpec, and Affinity Propagation in terms of accuracy in clustering and having less sensitivity to the choice of minimum size of a cluster. Currently, we select exemplars from the samples which are not leaves in the tree structure and have the minimum averaged distance with their neighbors. While the experimental results show this choice of exemplars is promising, one future work for minPAS clustering could be a robust approach for selecting the best exemplars. Another possible future work can be detecting the outliers and excluding them from minPAS clustering. Data visualization could be another useful future work for deciding about the choice of clustering algorithm. Visualizing high dimensional data can give a better understanding about the distribution of clusters, and consequently makes it easier to select an appropriate algorithm for arbitrary or non-arbitrary shaped data. Data stream clustering using the proposed clustering methods could be another direction for future work. While widely used stream clustering methods such as BIRCH [89] and C2ICM [90] need to have the number of clusters as a predefined value, the proposed methods could be employed for stream clustering and estimating the number of clusters simultaneously.

104

Appendix A

Average Central Error (ZSm)
From equation (3.9), we have
 ZSmi =
1 ni

  I  

c x1 mi . . . c n x i
mi





     - Bmi     

1 Xm i . . . ni Xm i

     
2, 2

(A.1)

Where I is an identity matrix and Bmi has the following format:
 Bmi
1 ni

  . . =  . 

··· .. . ···

1 ni



1 ni

1 ni

  . . .  , 

(A.2)

Therefore: ZSmi =
 where Xmi = Cx + Wmi gives: mi

1  ICx - Bmi Xmi mi ni

2 2,

(A.3)

ZSmi =

1  (I - Bmi )Cx - Bmi Wmi 2 2, mi ni

(A.4)

105

APPENDIX A. AVERAGE CENTRAL ERROR (ZSM ) where I - Bmi = Ami and then ZSmi will be given as follows: 1  Ami Cx - Bmi Wmi 2 2, mi ni

ZSmi = since AT mi Bmi = 0, therefore:

(A.5)

ZSmi =

1  Ami Cx mi ni 1 = 2 ni
ni

2 2

+

1 Bmi Wmi 2 2, ni
ni

(A.6)

Bmi Wmi 2 2

Wi2
i=1

2 + 2 ni

Wj Wk ,
j =k

(A.7)

2 , the E [Z assuming that all of the clusters have the same w Smi ] will be:

E [ZSmi ] =

1  Ami Cx mi ni

2 2

+

1 2  . ni w

(A.8)

To find the variance of ZSmi , we need to derive the variance and covariance terms in equation (A.6). The first term is a constant with zero variance. The remaining terms are related to (A.7) and have covariance and variances as follows: 1 var[ 2 ni
ni

Wi2 ] =
i=1

1 4 (2ni w ), 4 ni

(A.9)

where the above equation is derived based on the definition of a zero mean (E [wi ] = 0) chi-squared random variable. 2 n2 i
ni

var[

Wj Wk ] =
j =k

4 ni (ni - 1) 2 2 wj wk , 2 n4 i

(A.10)

2 for all of the clusters, it can be simplified as follows: by assuming the same w ni

var[

2 n2 i

Wj Wk ] =
j =k

4 ni (ni - 1) 4 w , 2 n4 i

(A.11)

106

APPENDIX A. AVERAGE CENTRAL ERROR (ZSM ) 4 1 cov ( 2 4 ni ni
ni ni

Wi2 ,
i=1 j =k

Wj Wk ) = 0,

(A.12)

where (A.12) is always equal to zero, and that is because of having i.i.d. data samples. In other words, both E [Wi2 Wj Wk ] and E [Wi3 Wj ] are zero. Finally, there will be the following statement for var[ZSmi ]: 2(ni - 1) 4 2 4 2 4 w + w = 2 w . 3 3 ni ni ni

var[ZSmi ] =

(A.13)

107

Appendix B

Cluster Compactness YSm
From equation (3.17), we have
 YSmi =
1 ni

 X1 . . . Xni

 X1 . . . Xni

     
2 2

  I  

     - Bmi     

,

(B.1)

Where Bmi is defined in A. We set Ami = I - Bmi :
 YSmi =
1 ni

    

1- . . .

1 ni

··· .. . ···

1 -n i . . .

      X1 . . . Xni

     
2 2

=

1 Ami Xmi 2 2, ni

(B.2)

1 -n i

1-

1 ni

 + W , therefore: For each sample we have Xi = Cx i i

YSmi = =

1  Ami (Cx + Wmi ) 2 2 mi ni 1 1  2 Ami Cx + Ami Wmi mi 2 ni ni

(B.3)
2 2

+

1  T (W T Ami Cx + Cx Ami Wmi ), mi mi ni mi

knowing that AT mi Ami = Ami it follows: Ami Wmi 2 2 ni - 1 = ni
ni

Wi2
i=1

2 - ni

ni

Wj Wk ,
j =k

(B.4)

108

APPENDIX B. CLUSTER COMPACTNESS YSM and
ni T Ami Wmi Cx mi

=
i=1

Wi c xi

1 - ni

ni

ni

Wi
i=1 j =1

c xj ,

(B.5)

therefore, YSmi will be given as follows: 1  Ami Cx mi ni 2 n2 i
ni

YSmi =

2 2

-

Wj Wk +
j =k

ni - 1 n2 i

ni

Wi2
i=1

2 - 2 ni

ni

ni

Wi
i=1 j =1

c xj

2 + ni

ni

Wi c xi , (B.6)
i=1

2 , the E [Y Assuming that all of the clusters have the same w Smi ] will be given as follows:

E [YSmi ] =

1  Ami Cx mi ni

2 2

+

ni - 1 2 w , ni

(B.7)

where 1  Ami Cx mi ni therefore 1 E [YSmi ] = ni
ni 2 c xi i=1
i ni - 1 2 1 2 c w . - 2( xi ) + ni ni i=1

2 2

=

1 ni

ni 2 c xi - i=1

i 1 2 ( c xi ) , n2 i i=1

n

(B.8)

n

(B.9)

var[YSmi ] will be given by calculating the variance of each term in equation(B.6): 2 n2 i
ni

var[-

Wj Wk ] =
j =k

4 n4 i

ni 2 2 wj wk , j =k

(B.10)

var[

ni - 1 n2 i var[

ni

Wi2 ] =
i=1 ni

(ni - 1)2 4 4 (2n1 w + · · · + 2 ni  w ), ni 1 n4 i c xj ] =
i 4 2 ( c xj ) n2 i j =1

(B.11)

-2 n2 i

ni

n

ni 2 w , i i=1

Wi
i=1 j =1 ni

(B.12)

2 var[ ni

c xi Wi ]
i=1

4 = 2 ni

ni 2 2 w c , i xi i=1

(B.13)

109

APPENDIX B. CLUSTER COMPACTNESS YSM -2 n2 i
ni ni

cov (

Wi
i=1 j =1

c xj ,

2 ni

ni

c xi W i ) =
i=1

-8 n3 i

ni

ni

c xi
i=1 j =1

2 c xj wj ,

(B.14)

The rest of possible covariance terms will be zero as samples are i.i.d.: 4 2 i  2 4 2 2(ni - 1) 4  - w ( cxi ) + 2 w var[YSmi ] = w 2 3 ni ni ni i=1
n ni 2 c xi . i=1

(B.15)

110

Appendix C

Folded Normal Distribution
Let  be a sample of standard Gaussian distribution,   N (0, 1), with density function () as follows: 1 2 () =  e- /2 , 2 R (C.1)

therefore, the distribution function of  can be given as follows:
 

() =
-

(v )dv =
-

1 2  e-v /2 dv, 2

R

(C.2)

We let v be a sample of Gaussian distribution with mean µ and standard deviation  , v  N (µ,  ). Therefore, random variable V can be written as V = µ +  . It follows that W = |V | = |µ +  | is a random variable with a folded normal distribution, where all of the negative values of V are folded to the positive region of the distribution.

111

APPENDIX C. FOLDED NORMAL DISTRIBUTION Consequently, cdf of the sorted sample w, where w  [0, ), will be as follows:

Fa (w) = P (W  w ) = P (|V |  w ) = P (|µ +  |  w ) = P (-w  µ +    w ) -w - µ w-µ w )   w-µ -w - µ = ( ) - ( )   = P( (C.3)

Since (-) = 1 - (), we will have: -w - µ w-µ ) - ( )   w-µ w+µ = ( ) + ( )-1   w 1 1 v+µ 2 = {exp [- ( ) ] 2  0  (2 ) 1 v-µ 2 ) ]}dv + exp [- ( 2  (C.4)

Fa (w ) = (

In case of Gaussian mixture models, the above cdf will be calculated as follows [72]:
K

Fa (z ) =
j =1

j Faj (z )

(C.5)

where Faj (z ) is the Gaussian cdf of the j th component, and j is the mixing factor of that component in the mixture.

112

Appendix D

Estimation of  and T
Here, we propose an approach to determine the proper values of  and T with assumption of Gaussian Dcdf. Lets consider a set of possible combinations of  and T values. For each combination, the similarity score A(, T ) can be calculate using (4.9):    1   0

A(, T ) =

Sigtestscore () < T Sigtestscore ()  T

(H0 ) (H1 )

(D.1)

We consider A(, T ) for two different scenarios: 1) A0 (, T ), where ecdf of the data is related to a single cluster and Dcdf is a single Gaussian as well, 2) A1 (, T ), where, ecdf of the data is relate to two overlapped clusters and Dcdf is a single Gaussian. A proper combination of  and T should suggest to split the two clusters, and at the same time should not split the single cluster. In Figure D.1, the synergistic combinations are shown by warmer colors for different distances between two overlapped clusters. Consequently, the following minimization problem can be used to estimate the proper  and T : ^] = min[A1 (, T ) - A0 (, T )] [^ , T
,T

(D.2)

where 0 < T  1 and 0 <  

1 1-pc

(i.e. pc = 0.99 in (4.6)) are possible constraints 113

APPENDIX D. ESTIMATION OF  AND T

Figure D.1: Increasing the distance between clusters and representing the result of Sigtest for different combinations of  and T (warmer points show more reliable combinations). for the minimization problem. To solve the (D.2), we have employed genetic algorithm (GA) with a population size of 100. Figure D.2 shows the result of GA simulations for estimating  and T for different distances between the center of clusters. This result is consistent with the synergistic regions in the Figure D.1 for the optimum  and T . Consequently, to be in a safe range for the optimum behavior of Sigtest, we set the averaged values of 1.72 and 0.53 for  and T respectively. In the case of Gaussian mixture models, the T value will be adaptively calculated based on the assumed mixing factor j in (C.5). As a result, the T value in a mixture model will be give as follows:

114

APPENDIX D. ESTIMATION OF  AND T

Figure D.2: Estimated  and T parameters using Genetic algorithm for different distances between clusters.

T = 0.53 max j
j

(D.3)

115

Bibliography
[1] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888­905, 2000. [2] F. U. Siddiqui and N. A. M. Isa. Enhanced moving k-means (emkm) algorithm for image segmentation. Consumer Electronics, IEEE Transactions on, 57(2):833­841, 2011. [3] M. Hua, M. K. Lau, J. Pei, and K. Wu. Continuous k-means monitoring with low reporting cost in sensor networks. Knowledge and Data Engineering, IEEE Transactions on, 21(12):1679­1691, 2009. [4] C. C. Hung, , and L. Wan. Hybridization of particle swarm optimization with the k-means algorithm for image classification. In In Computational Intelligence for Image Processing CIIP'09. IEEE Symposium on, pages 60­64, 2009. [5] Woncheol Jang and Martin Hendry. Cluster analysis of massive datasets in astronomy. Statistics and Computing, 17(3):253­262, 2007. [6] Daniel B Neill and Andrew W Moore. A fast multi-resolution method for detection of significant spatial disease clusters. In Advances in Neural Information Processing Systems, page None, 2003. [7] Marcilio CP De Souto, Daniel SA De Araujo, Ivan G Costa, Rodrigo GF Soares, Teresa B Ludermir, and Alexander Schliep. Comparative study on normalization 116

BIBLIOGRAPHY procedures for cluster analysis of gene expression datasets. In Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, pages 2792­2798. IEEE, 2008. [8] Noam Kaplan, Moriah Friedlich, Menachem Fromer, and Michal Linial. A functional hierarchical organization of the protein sequence space. BMC bioinformatics, 5(1):196, 2004. [9] Deng Cai, Xiaofei He, Zhiwei Li, Wei-Ying Ma, and Ji-Rong Wen. Hierarchical clustering of www image search results using visual, textual and link information. In Proceedings of the 12th annual ACM international conference on Multimedia, pages 952­959. ACM, 2004. [10] Sara Dolni car and Friedrich Leisch. Behavioral market segmentation of binary guest survey data with bagged clustering. Springer, 2001. [11] Michel Wedel and Wagner A Kamakura. Market segmentation: Conceptual and methodological foundations, volume 8. Springer Science & Business Media, 2012. [12] Michael J Shaw, Chandrasekar Subramaniam, Gek Woo Tan, and Michael E Welge. Knowledge management and data mining for marketing. Decision support systems, 31(1):127­137, 2001. [13] Stijn Van Dongen. Graph clustering via a discrete uncoupling process. SIAM Journal on Matrix Analysis and Applications, 30(1):121­141, 2008. [14] R. Xu and D. Wunsch. Survey of clustering algorithms. Neural Networks, IEEE Transactions on, 16(3):645­678, 2005. [15] P. D. McNicholas and S. Subedi. Clustering gene expression time course data using mixtures of multivariate t-distributions. Journal of Statistical Planning and Inference, 142(5):1114­1127, May. 117

BIBLIOGRAPHY [16] A. K. Jain. Data clustering: 50 years beyond k-means,. Pattern Recognition Letters, 31(8):651­666, 2010. [17] C. C. Aggarwal and C. K. Reddy. volume 31. CRC Press, 2013. [18] James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281­297. Oakland, CA, USA., 1967. [19] John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Applied statistics, pages 100­108, 1979. [20] L. Baibing. A new approach to cluster analysis: the clustering function based method. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):457­476, 2006. [21] R. De la Cruz-Mesa, F. A. Quintana, and G. Marshall. Model-based clustering for longitudinal data. Computational Statistics and Data Analysis, 52(3):1441­1457, 2008. [22] M. Chiang and B. Mirkin. Intelligent choice of the number of clusters in kmeans clustering: An experimental study with diferent cluster spreads. Journal of Classifcation, 27(1):3­40, 2010. [23] X. Xie and G. Beni. A validity measure for fuzzy clustering. Pattern Analysis and Machine Intelligence, IEEE Trans, 8:841­847, 1991. [24] J. C. Dunn. Well-separated clusters and optimal fuzzy partitions. Journal of Cybernetics, 4(1):95­104, 1974. [25] L. Kaufman and P. J. Rousseeuw. Finding groups in data: an introduction to cluster analysis. John Wiley and Sons, 344, 2008.

118

BIBLIOGRAPHY [26] T. Caliski and J. Harabasz. A dendrite method for cluster analysis. Communications in Statistics, 3(1):1­27, 1974. [27] W. J. Krzanowski and Y. T. Lai. A criterion for determining the number of groups in a data set using sum-of-squares clustering,. Biometrics, 44(1):23­34, 1988. [28] A. Strehl. Relationship-based clustering and cluster ensembles for high dimensional data mining. PhD thesis, The University of Texas at Austin, May 2002. [29] N. Yousri, M. Kamel, and M. Ismail. A novel validity measure for clusters of arbitrary shapes and densities. pages 1­4. Pattern Recognition 2008. ICPR 2008. 19th International Conference on, 2008. [30] Gayar J. Kittler K. Kryszczuk, P. Hurley and F. Roli (Eds.). Estimation of the number of clusters using multiple clustering validity indices, in: N. in Multiple Classifier Systems, pages 114­123, 2010. [31] D. Pelleg and A. W. Moore. X-means: Extending k-means with efficient estimation of the number of clusters. Proceedings of the Seventeenth International Conference on Machine Learning, ICML '00, Morgan Kaufmann Publishers Inc., pages 727­ 734, 2000. [32] G. Hamerly and C. Elkan. Learning the k in kmeans. in Neural Information Processing Systems, 17(281), 2004. [33] A. Kalogeratos and A. Likas. Dip-means: an incremental clustering method for estimating the number of clusters. In Advances in neural information processing systems, pages 2393­2401, 2012. [34] G. Hamerly and Y. Feng. Pg-means: learning the number of clusters in data. In Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, p. 393. MIT Press, 2007. vol. 19. 119

BIBLIOGRAPHY [35] R. Tibshirani, G. Walther, and T. Hastie. Estimating the number of clusters in a dataset via the gap statistic. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):411­423, 2000. [36] Shalini S Singh and NC Chauhan. K-means v/s k-medoids: A comparative study. In National Conference on Recent Trends in Engineering & Technology, volume 13, 2011. [37] Alan P Reynolds, Graeme Richards, and Vic J Rayward-Smith. The application of k-medoids and pam to the clustering of rules. In Intelligent Data Engineering and Automated Learning­IDEAL 2004, pages 173­178. Springer, 2004. [38] J. Zheng K. Wang, J. Zhang, and J. Dong. Estimating the number of clusters via system evolution for cluster analysis of gene expression data. Information Technology in Biomedicine, IEEE Transactions on, 13(5):848­853, 2009. [39] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849­856, 2002. [40] Tao Shi, Mikhail Belkin, and Bin Yu. Data spectroscopy: Eigenspaces of convolution operators and clustering. The Annals of Statistics, pages 3960­3984, 2009. [41] Ana Fred. Finding consistent clusters in data partitions. In Multiple classifier systems, pages 309­318. Springer, 2001. [42] M. Ester, H. P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. Kdd, 96(34):226­231, August 1996. [43] B. J. Frey and D. Dueck. Clustering by passing messages between data points. science, 315(5814):972­976, 2007. 120

BIBLIOGRAPHY [44] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray. Visual categorization with bags of keypoints. In ECCV SLVC Workshop, 2004. [45] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In ICCV, 2003. [46] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. [47] L. I. Kuncheva and D. P. Vetrov. Evaluation of stability of k-means cluster ensembles with respect to random initialization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(11):1798­1808, 2006. [48] B. S. Everitt, S. Landau, M. Leese, and D. Stahl. Hierarchical clustering. Cluster Analysis, 71-110, 5th edition, 2011. [49] T. K. Moon. The expectation-maximization algorithm. Signal processing magazine, IEEE, 13(6):47­60, 1996. [50] B. Flach and P. Hlavac. Expectation maximization algorithm. Computer Vision: A Reference Guide, pages 265­268, 2014. [51] S. Borman. The expectation maximization algorithm-a short tutorial. Submitted for publication, pages 1­9, 2004. [52] T. W. Chen, C. H. Sun, H. H. Su, S. Y. Chien, D. Deguchi, I. Ide, and H. Murase. Power-efficient hardware architecture of k-means clustering with

bayesian-information-criterion processor for multimedia processing applications. Emerging and Selected Topics in Circuits and Systems, IEEE Journal on, 1(3):357­ 368, 2011.

121

BIBLIOGRAPHY [53] Q. Zhao, M. Xu, and P. Franti. Knee point detection on bayesian information criterion. pages 431­438, Vol. 2, 2008. In Tools with Artificial Intelligence ICTAI'08, 20th IEEE International Conference on. [54] F. J. Jr Massey. The kolmogorov-smirnov test for goodness of fit. Journal of the American statistical Association, 46(253):68­78, 1951. [55] H. W. Lilliefors. On the kolmogorov-smirnov test of normality with mean and variance unknown. Journal of the American Statistical Association, 62(318):399­ 402, 1967. [56] T. W. Anderson and A. Darling. A test of goodness of fit. Journal of the American Statistical Association, 49(268):765­769, 1954. [57] C. D. Sinclair, B. D. Spurr, and M. I. Ahmad. Modified anderson darling test. Communications in Statistics-Theory and Methods, 19(10):3677­3686, 1990. [58] J. A. Hartigan and P. M. Hartigan. The dip test of unimodality. The Annals of Statistics, 13(1):70­84, 1985. [59] I. Jolliffe. Principal component analysis. John Wiley and Sons, Ltd, 2002. [60] N. Vlassis J. J. Verbeek and B. Krse. A k-segments algorithm for finding principal curves. Pattern Recognition Letters, 23(8):1009­1017, June 2002. [61] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395­416, 2007. [62] FRBMI Jordan and F Bach. Learning spectral clustering. Adv. Neural Inf. Process. Syst, 16:305­312, 2004. [63] S. Beheshti and M. A. Dahleh. Noisy data and impulse response estimation. IEEE Transactions on Signal Processing, 58(2):510­521, 2010.

122

BIBLIOGRAPHY [64] S. Beheshti, M. Hashemi, X. P. Zhang, and N Nikvand. Noise invalidation denoising. Signal Processing, IEEE Transactions on, 58(12):6007­6016, 2010. [65] S. Beheshti and M. Dahleh. A new information-theoretic approach to signal denoising and best basis selection. Signal Processing, IEEE Transactions on, 53(10):3613­ 3624, 2005. [66] M. Shahbaba and S. Beheshti. Mace-means clustering. Signal Processing, Elsevie, 105:216­225, 2014. [67] L. Vendramin, R. J. G. B. Campello, and E. R. Hruschka. Relative clustering validity criteria: A comparative overview. Statistical Analysis and Data Mining, 3(4):209­235, 2010. [68] K. Bache and M. Lichman. UCI machine learning repository, 2013. [69] A. R. da Rocha Neto, R. Sousa, G. de A. Barreto, and J. S. Cardoso. Diagnostic of pathology on the vertebral column with embedded reject option. In Proceedings of the 5th Iberian conference on Pattern recognition and image analysis, IbPRIA'11, Springer-Verlag, BerlinHeidelberg, pages 588­595, 2011. [70] P. Fr¨ anti and O. Virmajoki. Iterative shrinking method for clustering problems. Pattern Recognition, 39(5):761­775, 2006. [71] S. Ayramo and T. Karkkainen. Introduction to partitioning-based clustering methods with a robust example, reports of the dept. of math. Technical report, Inf. Tech. (Series C. Software and Computational Engineering), 1/2006, University of Jyvskyl, 2006. [72] C. R. Shalizi. Advanced Data Analysis from an Elementary Point of View. ch.19, sec. 4, pp. 378- 384. [Online]. Available, 2012.

123

BIBLIOGRAPHY [73] M. Shahbaba and S. Beheshti. Efficient unimodality test in clustering by signature testing. Acoustics, Speech and Signal Processing (ICASSP), IEEE International Conference on, 2014. [74] M. Shahbaba and S. Beheshti. Model verification of gmm clustering based on signature testing. Electrical and Computer Engineering (CCECE), 2014 IEEE 27th Canadian Conference on, 2014. [75] David G Lowe. Object recognition from local scale-invariant features. In Computer vision, 1999. The proceedings of the seventh IEEE international conference on, volume 2, pages 1150­1157. Ieee, 1999. [76] Anna Bosch, Andrew Zisserman, and Xavier Munoz. Image classification using random forests and ferns. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1­8. IEEE, 2007. [77] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886­893. IEEE, 2005. [78] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. In on Generative-Model Based Vision, Workshop, 2004. [79] D. Cai, X. He, J. Han, and T. Huang. Graph regularized non-negative matrix factorization for data representation. In PAMI, 2011. [80] D. Cai, X. He, and J. Han. Speed up kernel discriminant analysis. The VLDB Journal, 20(1):21­33, 2011. [81] M. Lichman. UCI Machine Learning Repository. University of California, School of Information and Computer Science, CA, 2013. 124

BIBLIOGRAPHY [82] M. Meil. Comparing clusterings-an information based distance. Journal of Multivariate Analysis, 98(5):873­895, 2007. [83] L. Hubert and P. Arabie. 2(1):193218, 1985. [84] S. Pemmaraju and S. Skiena. Minimum spanning trees. Computational Discrete Mathematics: Combinatorics and Graph Theory in Mathematica. Cambridge, England: Cambridge University Press, 8(2):335­336, 2003. [85] L. Galluccio, O. Michel, P. Comon, M. Kliger, and A. O. Hero. Clustering with a new distance measure based on a dual-rooted tree. Information Sciences, 251:96­113, 2013. [86] R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical Journal, 36(6):1389,1401, November 1957. [87] J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proc. Amer. Math. Soc., 7:48­50, 1956. [88] A. Ultsch. Clustering with som: U*c. In Proc. Workshop on Self-Organizing Maps, Paris France, pages 75­82, 2005. [89] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. Birch: an efficient data clustering method for very large databases. In ACM SIGMOD Record, volume 25, pages 103­114. ACM, 1996. [90] Fazli Can. Incremental clustering for dynamic information processing. ACM Transactions on Information Systems (TOIS), 11(2):143­164, 1993. Comparing partitions. Journal of classification,

125

Glossary
ACE Averaged Central Error. AD Anderson-Darling Statistical Test of Gaussianity. ARI Adjusted Rand Index. AP Affinity Propagation. BIC Bayesian Information Criterion. BOVW Bag of Visual Words. cdf cumulative distribution function. CH index Calinski-Harabasz index. CNC Correct Number of Clusters. DB index Davies-Bouldin index. DBSCAN Density-based spatial clustering of applications with noise. Dcdf Desired cumulative distribution function. Dip Dip statistic's Hartigan Test of Unimodality. ecdf empirical cumulative distribution function. EM Expectation Maximization. 126

BIBLIOGRAPHY GA Genetic Algorithm. GMM Gaussian Mixture Model HOG Histograms of Oriented Gradients. KL index Krzanowski-Lai index. KS Kolmogrov-Smirnov Statistical Test of Gaussianity. MACE Minimum Averaged Central Error. MCMC Markov Chain Monte Carlo. minPAS Minimum Pathways in Arbitrary Shaped Clustering. minPts minimum number of points for each cluster in DBSCAN. MSDL Minimum Structure Description Length. MST Minimum Spanning Tree. N-cut Normalized cut. PC Principal Component. PCA Principal Component analysis. SIFT Scale Invariant Feature Transformation. Sigtest Signature Testing. Sil index Silhouette index. SNR Signal To Noise Ratio. STD Standard Deviation. SVM Support Vector Machines. 127

BIBLIOGRAPHY VI Variation of Information. wtertra weighted inter-to intra-cluster ratio.

128

