TX
mr

*2

oo c*

SEQUENTIAL MONTE CARLO METHODS FOR MULTI-SENSOR TRACKING WITH APPLICATIONS TO RADAR SYSTEMS
by

Alon Shalev Housfater B.Sc., University of Maryland at College Park, 2004

A thesis presented to Ryerson University in partial fulfillment of the requirement for the degree of Master of Applied Science in the Program of Electrical and Computer Engineering.

Toronto, Ontario, Canada, 2006

(c)

Alon Shalev Housfater, 2006
PROPERTY OF RYERSON UNIVERSITY LIBRARV

UMI Number: EC53498

INFORMATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

UMI

<§)

UMI Microform EC53498 Copyright2009 by ProQuest LLC All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code.

ProQuest LLC 789 East Eisenhower Parkway P.O. Box 1346 Ann Arbor, Ml 48106-1346

Author's Declaration
I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend tliis thesis to other institutions or individuals for the purpose of scholarly research. Signature

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. Signature

ii

Instructions on Borrowers
Ryerson University requires the signatures of all persons using or photocopying this the sis. Please sign below, and give address and date.

iii

Abstract
Sequential Monte Carlo Methods for Multi-Sensor Tracking with Applications To Radar Systems Alon Shalev Housfater M.A.Sc., Electrical Engineering, Ryerson University, 2006 The aim of this thesis is to explore specific sequential Monte Carlo (SMC) methods arid their application to the unique demands of radar and bearing only tracking systems. Asynchronous radar networks are of special interest and a novel algorithm, the multi ple imputation particle filter (MIPF), is formulated to perform data fusion and estimation using asynchronous observations. Convergence analysis is carried out to show that the algorithm will converge to the optimal filter. Simulations are performed to demonstrate the effectiveness of this filter. Next, the problem of multi-sensor bearing only tracking is tackled. A particle based tracking algorithm is derived and a new filter initialization scheme is introduced for the specific task of multi-sensor bearing only tracking. Simulated data is used to study the efficiency and performance of the initialization scheme.

iv

Acknowledgments
I wish to express my gratitude to several people who were involved in this thesis. I would like to thank my thesis advisor, Dr. Xiao-Ping Zhang, for his comments and insights. His guidance and critical advice were invaluable to me. I would like to acknowledge and thank Dr. Yifeng Zhou of Defence R&D Canada who introduced me to the topic of radar tracking and provided valuable guidance and advice. My many friends at CASPAL and elsewhere were a source of support and comfort throughout my studies. I will always think of them fondly. I cannot end without thanking my family, this thesis would not have been possible without their support, love and encouragement. They were always there for me and it is to them I dedicate this work. Toronto ·, August 2006 Alon Shalev

Contents
1 Introduction 1.1 Motivation and Contributions 1.2 Organization 2 Target Tracking - Theoretical Aspects 2.1 Mathematical Preliminaries 2.1.1 Notation 2.1.2 Logic and Set Theory 2.1.3 Weak Convergence on Metric Spaces 2.1.4 Probability Theory 2.2 The Nonlinear Dynamic Estimation Problem 2.3 Linear Gaussian State Space Models: Kalman Filter 2.3.1 Extended Kalman Filter 2.4 Sequential Monte Carlo Methods 2.5 Convergence of Sequential Monte Carlo Methods 2.5.1 Nonlinear Estimation Formulation in Probability Space 2.5.2 Particle Filter Formulation in Probability Space 2.5.3 Almost Sure Convergence 3 Target Tracking for Radar Systems 3.1 Problem Formulation 3.2 Classical Algorithms 3.2.1 Extended Kalman Filter 3.2.2 Range Parameterized Extended Kalman Filter 4 Multiple Sensor Tracking with Non-Response 4.1 Problem Formulation 4.2 Multiple Imputations 4.3 Multiple Imputation Particle Filter 4.3.1 Approximation of the Imputing Function 4.3.2 Multiple Imputations Particle Filter Algorithm 4.4 Iterative Multiple Imputation Particle Filter 1 3 4 5 5 5 6 6 7
8

10
11 12 13 14 15 16 21 21 23 23 25

27 27 28 30 30 31 34

vi

4.5

4.6

Convergence Analysis 4.5.1 Filter For: ^ulation in Probability Space 4.5.2 Almost Sure Convergence Performance Analysis 4.6.1 Radar Network Data Fusion . 4.6.2 Simulation Results

34 34 37 40 40 41 45 45 46 48 49 50 50 54 54 55 56

5

Sequential Monte Carlo Methods for Multi-Site Bearing Only Tracking 5.1 Problem Formulation 5.2 Least Squares Initialization 5.3 Bearing only Target tracking with Monte Carlo Algorithms 5.3.1 Particle Filter for Bearing Only Tracking 5.4 Performance Analysis 5.4.1 Simulation Results Concluding Remarks 6.1 Conclusions 6.2 Future Research

6

Bibliography

vii

List of Figures
4.1 4.2 4.3 4.4 5.1 5.2 Multiple Imputation - Imputation Diagram Multiple Imputation Particle Filter - Diagram Performance of MTPF and EM for Two Radars - Missing Data Ratio of %50 . Performance of MIPF and EM for Three Radars - Missing Data Ratio of %25 30 33 43 44

Performance of the EKF and Particle filter in Bearing Only Tracking 52 Average performance of the EKF and Particle filter in Bearing Only Tracking. 53

I

viii

Chapter 1 Introduction
Radar systems are complex entities which are composed of several different subsystems such as target identification, track registration [1] and others [2]. One key subsystem is the tracking subsystem, this subsystem utilizes a tracking or filtering algorithm to estimate the position of the target from noisy radar measurements. The radar traditionally operates in a polar coordinate system due to the mechanics of the radar sensor [2] while the target motion is best modeled in cartesian coordinates. This fact makes the radar tracking prob lem a nonlinear one due to the coordinate system transition between measurements and state. Nonlinear filtering problems are notoriously difficult to solve due the analytically intractable mathematics. Several approximation schemes have been proposed to resolve this problem such as the extended Kalman filter (EKF) [3], Gaussian sums and numerical integration methods [4]. These approximation methods are often unreliable, prone to fil ter divergence and their rate of convergence decreases as the dimension of the problem increases. A relatively new approach to nonlinear estimation is the Sequential Monte Carlo (SMC) method. SMC methods, also known as particle filters (PF), are a set of powerful stochastic algorithms able to compute the optimal filter for a wide range of nonlinear sys tems. First suggested in [5] and [6], the particle filter uses a randomized or Monte Carlo sampling to approximately obtain the optimal state estimator. There exists significant lit erature on particle filters since they were first suggested, their performance was further enhanced by the development of the auxiliary particle filter [7] and Rao-Blackwellised

1

Particle Filter, refer to [8], [9], [10] for a detailed discussion of these topics. Another problem of fundamental importance is that of missing data. Handling miss ing data has always been a part of statistical analysis, which frequently occurs in various surveys and experiments [11], [12], [13]. Significantly, missing data behavior also ap pears in the context of data fusion as asynchronous sensors. This can occur in a radar network where the radars might be many miles apart, thus making it difficult to syn chronize their observations [1]. There are several standard approaches in dealing with missing data; one is to employ linear prediction techniques to align the data to the in stance of missing data, others include formulating a Kalman filter with a time-varying transition matrix that accounts for the missing observations [1]. Also, one can apply the Expectation-Maximization (EM) algorithm to resolve the missing data [12], These algo rithms either assume a linear system or ignore the often nonlinear state dynamics. Thus, available methods are either not applicable to nonlinear systems or fail to incorporate the knowledge of the state dynamics into the algorithm. Conversely, the particle filter does not account for the missing data behavior when performing data fusion and estimation. Therefore, one needs to confront the problem of performing fusion on a nonlinear system in the presence of missing data. Many radar systems can also operate in a passive mode, while in this mode the radar system measures only the bearing of the target. This type of tracking system is also known as an Electronic Warfare Device. Such systems are useful since while standard radar track ing can be detected by sensing the electromagnetic radiation emitted by the radar; the passive radar uses reflections of ambient radio signals to observe the target, thus the tar get is unaware that it is being tracked. This type of tracking environment is known as bearing only target tracking or target motion analysis (IMA). Similar to radar networks, bearing only tracking can be performed by multiple observation stations to increase the reliability and performance of the estimation. As in standard radar tracking, the bear ing only tracking is formulated as a nonlinear state space estimation problem. However, unlike radar tracking, bearing only estimation introduces additional problems such as

2

observability [14], initialization and tracker instability [15]. The problems of initializa tion and tracker stability are intertwined; sufficiently incorrect filter initialization may cause tracker divergence after a few time iterations. The standard solution to the bear ing only initialization problem is to apply the range parameterized extended Kalman filter (KPEKF) p6], [17], The RPEKF consists of a bank of EKFs; thus, there is always a risk of tracker divergence since the EKF is a linearized sub-optimal filter. Therefore, it is of great interest to formulate an initialization procedure that is equivalent to the RPEKF, but applied to the particle filter which, has no divergence problems due. to linearization.

1.1 Motivation and Contributions
Joint fusion and estimation for nonlinear multi-sensor networks are common tasks in many applications. These tasks can be performed by applying SMC methods, which are a set of powerful methods for nonlinear estimation. However, in many multi-sensor net works, sensors are not synchronized. Asynchronous sensors cause some observations to be missing at any given time instance. SMC methods do not directly account for such sys tem behavior; thus, it is of interest to resolve this dichotomy so one can apply SMC meth ods in the setting of asynchronous multi-sensor estimation. With these considerations in mind, the multiple imputation particle filter (MIPF) is introduced. The MTPF algorithm allows efficient multi-sensor tracking with incomplete observations. The algorithm oper ates as a bank of particle filters where the multiple imputations (MI) method is used to generate multiple complete observation, each complete observation set is fed to a particle filter. The output of the particle filters are then linearly combined. The problem domain of asynchronous radar networks is explored as an application for the MIPF. Another commonly encountered estimation problem is bearing only tracking. Many applications use a network of bearing only sensors for tracking uses. However, this prob lem cannot be solved using a straight forward application of SMC methods due to the inherent instability of the bearing only measurement model. This instability manifests itself as tracker divergence when the filter is initialized with values that are too "differ 3

ent" from the true initial state. Thus, a new initialization scheme for the particle filter is suggested, this scheme is applicable for a tracker operating in a multi-sensor bearing only environment. It utilizes several bearing measurements to obtain multiple position estimates using a least squares approach. These estimates are in turn used to construct an initial state distribution which can be used to initialize the particle filter.

1.2

Organization

The organization of this thesis is as follows: Chapter 2 presents a general mathematical c jlopment of relevant nonlinear estimation theory. In chapter 3, the discussion focuses

on the application domain of radar and bearing only tracking from a nonlinear system perspective. Chapter 4 introduces a new filtering algorithm, the MIPF. Chapter 5 presents a new initialization scheme for multi-sensor bearing only particle filter. The thesis con cludes with final remarks and future directions of research.

4

Chapter 2 Target Tracking - Theoretical Aspects
Understanding sequential Monte Carlo methods require an array of mathematical tools; this section reviews the basic concepts involved in formulating and solving nonlinear es timation problems. First, a brief review of necessary mathematical theory and notation is presented. Next, these mathematical tools are used to formulate the general nonlinear estimation problem and its special case of Gaussian linear state space model. Solutions to the estimation problems such as the particle filter, Kalman filter and EKF are discussed. Finally, the particle filter is analyzed in an abstract setting in order to explore its conver gence. The material in this chapter was extracted from various mathematics and signal processing sources [8], [18], [19], [9], [20], [21],

2.1 Mathematical Preliminaries
2.1.1 Notation

Let R denote the extended real number system, the symbol R+ denotes the set of all real positive numbers; 1R+ -- {r G R: r > 0}. Similarly, N = {..., --2, --1,0,1,2,...} denotes the set of integers while N+ denotes the set of non-negative integers N+ = {0,1,2,...}. Bold faced letters such as X denote vector-valued variables. A variable's dimension is specified in the discussion. A sequence of values vi,v2,... ,v^ is written in shorthand as where i indexes the sequence.

5

2.1.2

Logic and Set Theory

Let X be a set, then x G X indicates that X contains the element x. The symbol V denotes 'for all', Vx E X means for all members of set X. Let Y be another set such that every member of Y is also in X, then write Y C. X, i.e. Y is a subset of X. Consider the two sets X and Y, their union X U Y is the set such that any member x E X and y E Y is contained inJuY. The intersection of the two sets X and Y is defined as the set of members of X and Y where each member is both in X and Y.

2.1.3

Weak Convergence on Metric Spaces

Topological Spaces A topological space is defined as the set X with a system of subsets r that contain the null set, the set X itself, the union of every one if its subsystems and the intersection of every one of its finite subsystems. The sets in. T are called the open sets of the topological space (X, r). A neighborhood of the point x  X is the set containing an open set which contains x. The member x E X is a limit point of a subset M C X if every neighborhood of x contains at least one point m E M different from x. Concluding this brief discussion of topological spaces, one notes that the critical notion of a topological space allows us to discuss the concept of limit and thus of convergence [20]. Metric Spaces Consider a set X where for any pair p,q E X, there is a function the following conditions hold, 1. d (jp, q) > 0 and d(p, q) -- Qiiip = q.
2. d(p,'q) = d(q,p).

such that

3. d{p, q) < d(jp, I) + d(q, I) where I £ X. One can think of d as a distance measure between a pair of points p , q E X . Call the set (E,d) a metric space [20], where d is the metric function and E is the ambient space. 6

Note that a metric space is also a topological space since a metric function d to induces a topology in the following way: Let x0 £ X,r £ R+ and-associate with those two members a set iS^rco", T) = -[x E X: d{x, x0) < r}, this set is the open sphere with center XQ and radius r. It can be shown [20] that the collection of sets S = system of open sets and thus (_£?, S) is a topological space. Continuous Functions The symbol / : X --·» Y denotes a function whose domain is the set X and range is the set Y, for every x functions, f : X
G

r) : x0 £ X,r £ R+} forms a

X, assign a corresponding element fix)

G

Y. Consider two

Y and g : Y --> Z, then the composite mapping g o f is defined

as (g o f)(x) = g(f{x)). Let (X, dx), (Y, dy) be two metric spaces and define the function f : X e
G

Y. The function / is continuous at point c 6 X if for every positive real number there exists a positive real number 5 £ R+ such that for all x
< G

X satisfying

dx(x, c)

5 will also satisfy dy(f(x), /(c))

<

e. The function / is said to be continuous if it

continuous at every point in its domain. Weak Convergence A specialized kind of convergence known as weak convergence [21] is commonly defined on metric spaces. Consider a metric space (E, d) and a sequence of points in this space {en
G

then the sequence en weakly converges to the point e, limn^00 en
G

= e if

and

only if lim^oo d(e n , e) = die , s) for all e

E.

2.1.4

Probability Theory

A probability space [19] is written as the set triplet (Q, T, P) where Q is the sample space, T is a cr-algebra [21] containing all possible events and P is the probability measure, P : T --> R . The cr-algebra T is typically taken to be the Borel cr-algebra, i.e. the a-algebra generated by the open sets, of the sample space VL when possible and is written B(Q). The

7

density function of X, when it exists, is defined as a function satisfying the relation P( B ) -- I f x ( x ) d x for any B e F , Jb where B is an arbitrary Borel set in T. A random variable is a function X with the domain Q and whose range is contained in R, then write X : Q --> M. The expectation of the random variable X is defined as E(X) = [ X ( w ) F ( d w ) . Jo. X is P-integrable, i.e. with respect to the probability measure P, if and only if the integral above exists and is finite. Consider the collection of random variables defined

on the probability space (fi, JF, P), they are said to be independent if and only if for any collection of Borel sets {Bj e

P(
\j=1

eB

s)

e Bj).
j=1

J

A probability measure of special interest in this work is the Gaussian distribution. Let the event space be the n-dimensional Euclidean space, f2 = Mn with the cr-algebra, T = then the Gaussian distribution is defined F(X e B ) = ^27r^/2|Sji/2
exP

~

m)T

} dx for 311y Borel set B

e

^

where S is a symmetric, positive definite covariance matrix and m is the mean vector. It is standard shorthand notation to write this Gaussian distribution as J\f(m, X)).

2.2

The Nonlinear Dynamic Estimation Problem

The first component of any estimation' problem is the state equation, this equation relates the system's current state to its past state using a stochastic difference equation. Consider an n^-dimensional discrete-time dynamic system whose time evolution can be described by a vector equation of the form

X n = / n (X n _i,V n ),
8

(2.1)

where Xn is the nx-dimensional state vector, / is the nonlinear system transition function and Vn is the state noise term. Note that by only considering the past state Xn_i, Equa tion 2.1 makes the implicit assumption that the state sequence {Xn; n E N+} is a Markov process. Also, let the process's initial state be described by an initial probability distrib ution fi (Xn). Assume there exists a state transition probability density, p (Xn|Xn_i), which is the conditional density of the probability measure that describes how likely is for the current state value to be Xn given the past state is Xn_i. A sensor takes noisy observa tions of the dynamic system (2.1) described above, these measurements are related to the dynamic system's state by the measurement equation of the form, Yn -- g n (Xn, Wn)

(2.2)

where Yn is the n^-dimensional observation vector, g n is the nonlinear measurement func tion and Wn is the observation noise component. Note that in Equation 2.2 the observa tion sequence {Yn; n E N+} is conditionally independent given Xn. Assume there exists a probability density p (Yn|Xn), which is the density of the probability measure that de scribes how likely is the measurement Yn given that the current state is Xn. Our aim is to estimate recursively in time the filtering distribution p (Xn| Yn) so that one may take various expectations of the form (2.3) for any p (Xn|Yn)-integrable h : Rn= --> R. Common choices for the function h are the min imum mean squared error (MMSE) estimator /z(Xn) = Xn and the maximum a posteriori (MAP) estimator ^ f ( 1 / p (Xn|Yn)) S (Xn) where p(Xn|Yn) is maximized \ 0 for all otherXn

where 5 is the dirac delta function. The general solution for this estimation problem can be written as the following recursive formula, p(X,,lY,,)=p(X,,_1|Y,,_1) 9 p(Y,,|X,,)p(Xn|Xn_1) P (YJYo.^0

(2.4)

However, the denominator of equation 2.4 above cannot usually be evaluated analytically. As a result, one must resort to approximation and numerical integration methods as will be discussed later on in this chapter.

2.3

Linear Gaussian State Space Models: Kalman Filter

Assume that the state transition function f and measurement function g, defined in sec tion 2.2, are linear functions while the noise terms in the state space model are additive Gaussian random processes. Thus, one can rewrite equations 2.1-2.2 as follows Xn -- FnXn-.! + Vn

Yn = GnXn + Wn, where F n is a n x x n. x matrix, G n is an n y x nx matrix and Vn ~ jV(0, Rn), Wn ~ -A/"(0, Q n ) are the independent identically distributed (i.i.d.) Gaussian state noise and observation noise respectively. Given the assumptions of the linear Gaussian state space model, it can be shown [22] that the filtering probability density function p (Xn|Y0:n) is also Gaussian. Since the filtering density is Gaussian, it is characterized by its first and second moments [19], which can be written

where Xn|n is the estimate of the state at time n and ~P n \ n is its associated covariance matrix. In order to obtain these estimates one can apply the well known Kalman filter [3] in two stages; first predict the state using information from time instance n -- 1,

10

then update these prediction estimates with the current measurement at time n as follows = Pnln-lG^GnPnln-iG^ + Rn

Xn,n = Xnln_! + Kn (yu -- GnXn|n_i Pn|n = (I -- KnGn) Pn|n_!.
Note that under the linear Gaussian assumptions above, the Kalman filter is the optimal MMSE state estimator [22].

2.3.1

Extended Kalman Filter

While the Kalman filter is optimal for linear target dynamics, it is not applicable for non linear system dynamics. One approach to filter nonlinear systems is to linearize the state space model and then apply the standard Kalman filter on the linearized model. This approach is known as the extended Kalman filter (EKF) [2]. Consider a state space model of the following form,

x n = / n (X n _ x )+V n Y n - ^(X n )+W n ,

(2.5) (2.6)

where all symbols are as defined in previous sections. Define the jacobians of the state and observation transition functions,

Jf
Jt

A
A

cbc

dg n dx. x--X-n\n--l then by taking a vector Taylor series expansion of model 2.5-2.6 one obtains the linearized state space model X,, = /,, (x^-i) + (x>_, - X^,^) + V,, (2.7) (2.8)

Y,, = g,, (x,,,,,_a) + J» (X,, - X^-i) .

Clearly, equations 2.7 - 2.8 are a linear Gaussian state space model and therefore the Kalman filter can be applied. 11

2.4

Sequential Monte Carlo Methods

Due to the problems associated with the EKF, an alternative approach was suggested in the form of the particle filter. A particle filter adopts a Monte Carlo sampling strategy in order to approximate the filtering distribution p (X0;n.|Yn). It is approximated by a set of N particles, or possible values of the state, {a4}il=i with associated weighting coeffi cients where i indexes the particles at time n. The filtering distribution's particle

approximation can be written as the sum,
N

P (X0:,,|Y0;,,) ~ J2
i=l

(Xo,,, - 4J ·

(2-9)

Thus one can take expectations as in Equation 2.3,
N

E (Jl (X

0:n

|Y0;n)) =

h (X 0:n )p (X0:n|Y0:,,) dX 0:n ~
i=l

( X 0:n) "

The particles are sampled from an Importance Function which is denoted ir (Xo ;n |Yo:n)This Monte Carlo technique of Importance Sampling works by sampling N states from the importance function TT (X0;t1|Yo;n.), 4 .-n~ ^ ( X o :n|Y0:n), and evaluate the i-th particle's weighting coefficient using the formula
Wn

(2-10)

p(Y0:n|X0:n)p(X0:n) f-xT\ " ^ (,X.0:n| l0:nj

(2.11)

Note that the weighting coefficient wln is un-normalized. It is normalized using the for mula w\ = w\/ ^ order to make the approximate density a probability mea

sure. The procedure is discussed so far generates an approximation of the distribution p (X0;n|Yo;n), however this is not a recursive procedure. In other words, one must sample the entire-state trajectory XQ.n to obtain the approximation given in equation 2.9. In order to turn this procedure into a sequential Monte Carlo filter, assume the importance function has the following decomposition
72

7T (X0:,,|Y0:n) = TT (X0|Y0) H TV (XfclXo*-!, Yo*) . k=1 12

(2.12)

Using this assumption, one can modify the Monte Carlo approximation 2.10-2.11 into a recursive procedure where at time n it operates as follows,
x\ ~ 7T
=

Y0;n)

~i

p ("y" n i 4 ) p ( 4 i 4 -i) 7r«|4:n-nYo:n)
to

and then normalize the weights wln = wln/ JZjLi

obtain the particle set {a,'o;n,

The importance function itself may be any function that obeys the condition 2.12, how ever there are several standard choices. One is the -prior importance function where one uses the state transition density as the importance function, n (Xo:n[Y0;n) = p The

optimal importance function is the function which minimizes the variance of the importance weights, it can be shown [8] that this function is p (Xn|a:^_1, Yn). The particle filter can be written in algorithm notation as follows. The Particle Filter Algorithm · Initialize the particles X · For n = 1, 2,... · For i = 1,. . . , N Sample the importance function x \ ~ 7r (Xn|Y0:ri) · Evaluate the importance weights: wln = /" ,7 " and normalize w\ = wln/ Xw=i
Q

/x(X0) and set W -- 1 / N for i = 1 , . . . , N .
Q

· Sample the index d ( i ) distributed according to discrete distribution such that P(d(£) = I) = w l n for I = 1,..., N. · Set x\ = and wln = 1/iV for i -- 1,. . . , N .

2.5

Convergence of Sequential Monte Carlo Methods

In the previous section, a particle filtering scheme was discussed, this method uses a large number of particles to approximate the filtering distribution. However, the issue 13

of convergence remains; more explicitly, let N be the number of particles, then one is in terested in exploring whether the distribution approximation converges as N increases; these questions of convergence are discussed in this section. The particle filter is refor mulated to make it more amendable to convergence analysis. Next, its almost sure weak convergence is analyzed and proved.

2.5.1

Nonlinear Estimation Formulation in Probability Space

Following the development in [18] let (Q, T, P) be a probability space where T = £>(Mnx) is the Borel set of Rnx. On this probability triple define a vector-valued stochastic process X = {Xn,n 6 N+} where nx is the dimension of the state space of X. The process X is Markov with initial distribution /J, and probability transition kernel K(x n |a; n -i) P(Xn <E A\X n - 1 = a:n_i) = f K{x n \x n - 1 )dx , A <= JA The process X can be viewed as hidden state process to be estimated. Next, define a stochastic process Y -- { Y n ,n £ N + } where n y is the dimension of the state space of Y. The process Y is conditionally independent of X F(Y n
G

B\X n = x n ) - [ g{y n \x n )dw n , B JB

G

The stochastJc process Y can be considered a noisy observation of the hidden Markov process X. Define the family of probability distributions, P {x k d \yi : m ) = P (X k 6 dx k , Gdxi 1^1,..., Y m ) .

Bayes' theorem lets use write the joint distribution of the state at time n,
n

P (xou) = AT (ICQ) J J K (rcfclrcfc--I) g (yk\x k ),
k=l

and the recursive equations (2.13) ) 14 g(y. i \x n )p(x 0 : n \yi : n -i). (2.14)

As mentioned in section 2.2, one is typically interested in the marginal distributionp (x n \y n ), using the equations 2.13-2.14 one can write, p(x n \y n -i) =
P(xn\yn)

/ p (x n -i\y n ^) K (x^x^) dx n ^ J R": R"x i -1 9 (yrx\Xn) V (^vlz/n-l) · 9 (VNLXN) P (X \YN-L) N .J Rn*

(2.15) (2.16)

It will prove useful to rewrite the equations above in a different way, let ^bea function defined as ip : Rnx --> M and v a measure, then using the standard notation,

0,<p) = J <pv,
one can rewrite 2.15-2.16, (P On|z/n-i) ,<p) = (P (x n \y n ), Kip) (p (x n \y n ) ,tp) = (p (xn|y n _ x ) , g)~ 1 (p (x n \y n -y) , tpg) .

2.5.2

Particle Filter Formulation in Probability Space

As discussed in section 2.4, the particle filtering method recursively approximates the fil tering distribution p (xn\yn) at time n. The approximation is done by generating a "cloud" of N particles that generate an empirical measure pN (xn\yn) as follows P N (XNLVN) = 1
N

5 {X N - < ) , i=1

where 5 denotes the dirac delta function. The algorithm is recursive in the sense that the particles at time n -- 1, are used to generate the particles at time n,

and thus obtain the empirical measure p N {x n \y n ). The basic algorithm proceeds as fol lows; assume that at time n there are N particles distributed approximately according to p (xn-i\yn-i). Then one samples xln ~ K These new particles are distributed

approximately according to p (x n \y n _ x) [3]. The empirical distribution can be written,
N
N (Xn\Vn-l) = Tf X P ) <*(*n -

N i=l 15

,

which is an approximation oip (x n \y n -j). Substituting the empirical measure p N (a;n|t/n_i) to Equation 2.16, one obtains the Monte Carlo approximation,
N

PN

(a?n|2/n) = 2J3 W^n)
t=l

9 (VNL^N)

5 {XN

~ ^N) >

the empirical distributionp N (x n \y n ) approximates the desired filtering distributionp (x n \y n ),
N N

PN

On Iy n ) =
t=l

wx

n5

{x n -

xl

n)

, ^ v? n = 1,
i=l

where vf TL oc g (?yn|^n) are the importance weights. The particle filter performs a resampling procedure to obtain an un-weighted empirical distribution, p N (x n \y n ) = 1
N 6

(Xn

_

>

t=i by removing particles with low weights and propagating particles with high weight. The resampling is typically done by sampling the weighted empirical distribution, x z n ~ pN (xn\yn). It turns out that the resampling stage is a critical algorithmic step that stabi lizes the algorithm so that increasing number of particles are not necessary as time pro gresses [8].

2.5.3

Almost Sure Convergence

Preliminaries Consider a metric space (E, d) and let {a n }^ = 1 , be two sequences of continuous

functions indexed by n G N+, write these sequence of functions a n ,b n : E --> E. Moreover, let there be two other sequences of functions kn/ki:n as follows kn = ano bn k\-n -- k n O k n -1 o . . . ki, the functions k n and /c]:n are perturbed by the function c N : E --> E, k£ = c N o a t
N hl:n
--

o cN o

bt
K

--

N kn

o

K n-1 °

kN

N o "·· k 1·

16

Assume that as N increases, the perturbation becomes smaller, i.e. lim;v_oo c N = I where I is the identity function on E. One wishes to know whether given that cN will converge in a predefined manner, will k^_t converge to ki-t. It turns out that in order for k^t to converge, the perturbation function cN must converge in a uniform manner [18], [20]. Thus, cN needs to satisfy the following condition lim eyy = e JV-voo lim c N (ejv) V J = e, JV-»oo (2.17)

for all sequences e^r, e 6 E. Assuming condition 2.17, one can prove [18] the following lemma Lemma 1 Let a n ,b n / k n ,ki : n and c N be as defined above, then if c N satisfies condition 2.17, the following is true,
N --too

lim k^f -- k n and lim k?.,, -- kVn. 1 n
N-+oo -

Convergence of the Particle Filter We discussed some convergence properties of functions on abstract spaces. We now relate these abstract concepts with the particle filter in order to establish its convergence. Let E = V (MTM*) be the space of probability measures over the nx-dimensional Euclidean space RTM*. We endow the space with the topology of weak convergence. In this topology, the convergence on the space is defined as follows, consider a sequence of probability measures {vN e V (K"^) : N -- 1,..., oo}, then we say the sequence vN <E V (Mnx) weakly converges to v (lini/v-,00 uN = v) if and only if, for any continuous bounded function tp £

N-- »oo

lim (is N , ip) = O, <p),

where

is the space of all continuous bounded functions on I"1. In other words, lim v N = v weakly lim (u N , <p) = (v, cp) V <p 

N--*oo

N --KX>

For the sake of completeness, note that it is a well known result that one can choose a countable set A = so that it completely determines convergence. Thus, 17

one can write
N-- too

lim vx = v weakly

N--> oo

lim (vn, (f>i) -- {y, <p) V </?£ £ A.

Using the countable set -A, one can define the metric d associated with this space and topology, l(a> ^i) -- (&> <P)| d(a'6) = S"~~2f|M ' 7/ where || [| is defined as the supremum norm of ipi on C b (KTM*).

Now that the convergence space is established, let us proceed by identifying the ab stract function sequences an,bn with particle filtering operations. Let the sequence of func tions bn : V (Mnx) --> V (Mna:) be the mappings bn(y) = /
,/RTII

K{x n \x n ^ 1 )vdx n - l l

(2.18)

for arbitrary v e *P (RTM*). Thus, the following formula is true P (®n|2/n-l) = K (V («n-l|2/n-l)) , since p (x n _i\y n _i) e V (Rnj:). Recall from section 2.5.3 the sequence of functions, b n , is defined as a sequence of continuous functions, thus to ensure that the mapping (2.18) is continuous one assumes that the transition kernel of the signal, K(x n \x n -i), is Feller [19]. The property of a kernel being Feller is defined as that for any continuous bounded function <p, Kip is also continuous, i.e. cp e C6(Rna0 =* K<p e 0,(Knx)Given the assumption of Feller property and the definition 2.18, one can show that the sequence of functions bn have the following convergence property, lim {b n (v N ), ip) = (&n(f), <p), N--·oo G C&(Rn*).

Define the sequence of functions a n be the mappings a n : V (Mnx) --· V (Rnx) such that K M , <p) = 0, g)~ x {v, <pg) for any ip e C^R"*). 18 (2.19)

This definition implies p{x n \y n ) = ClnCpOCnlZ/n-l)) = a n O b n (p{x n - X \y n -l) ) . In order to ensure that the mapping a n is continuous, assume that the function g{y n \') is a continuous bounded strictly positive function 9(yn\') e C6(Rn»), g(y n \x n ) > 0 V x n e Rn«. (2.20)

The positivity assumption is necessary so that the term (V, g) in Definition 2.19 is never zero. If the function g(yn|-) satisfies condition 2.20 and liniAr_»oo vN = u, it implies lim (a n (v N ),(p) = (on(v),(p). N-* oo In the context of particle filtering, c N is a stochastic perturbation that occurs due to the sampling of the importance function as discussed in section 2.5.2. Define the random perturbation function c N as follows
*"("> =
i=l

where {VjYj=\ is a collection of random variables with common distribution u and N > 0 is an integer. The following lemma can be shown to be true: Lemma 2 If c N is defined as above, then it satisfies condition 2.17 almost surely. Given the definitions for a n / b n and c N , one can see that the following formula are true PN
On|

y n ) = C N oa n oc N ob n (p (^n-i|yn-i)) = kn (p On-i|y

n-i))

P N O'n |y n ) = k£ n O C N {jl) = k£ n ( (fl N ) , where \i N = c N {ji). Knowing this result and using lemmas 2 and 1, the following theorem can be proven Theorem 1 Assuming the transition kernel K is Feller and that the likelihood function g is bounded, continuous and strictly positive, then lim Ar _ >00 p^ (x n \y n ) = p (rc n |t/ n ) almost surely. 19

In conclusion, a result is obtained that indicates the particle filter will convergence with the true optimal filtering distribution with probability 1 as the number of particles in creases toward infinity. However, one must note that this is a weak result since there is no guarantee that a. finite number of particles will convergence or nor does it guarantee that the particle filter will generate an approximation with bounded error. Thus, this con vergence results suggests the particle filter is well behaved but more work is necessary to understand the behavior of the true particle filter. It turns out stronger assumptions are needed in order to guarantee the strong convergence of the particle filter (in the sense of bounded error for finite number of particles). Refer to the work at [23] for a complete and in-depth discussion of convergence results for particle filters.

20

Chapter 3 Target Tracking for Radar Systems
In this chapter, the radar and bearing only tracking are discussed from the nonlinear system perspective. Due to the significant nonlinearities of the radar tracking and bear ing only problems, classical approximation techniques must pay special consideration to issues such as observability and tracker initialization. First, the problems of radar and bearing only tracking are formulated. Next, well known strategies such as the EICF and RPEKF are discussed. The material in this chapter was taken chiefly from refer ences [16], [24], [8], [1], [25],

3.1 Problem Formulation
Assume the target motion is well modeled by linear state dynamics. These dynamics are obtained by considering the target's state in cartesian coordinates. A common target motion model is the near constant velocity model [1]. For this model, the state transition equation can be written as the matrix equation,

Xn = $nXn_x + TnVn,
·where /I $n At n 0 0 \

0
0 \0

1
0 0

0

0
'

1 At n 0 1

21

and
r

/Ai£/2 0 \ _ Atn 0 0 Atl/2 " V 0 AtJ

The state vector Xn is defined as Xn = [x n , x' n , y n , y' n ] T where x n , y n are the cartesian co ordinates of the target's position and x'n, y'n are associated velocity components. The twodimensional system noise Vn ~ J\f(0, S^) is assumed to be Gaussian i.i.d. process. The symbol Atn denotes the time difference between two measurements at time n. Note the distinction between the integer n £ N+ which indexes the state sequence and the "ana log" time difference Atn e R+. The timing difference between successive measurements, Atn is assumed to be a time varying model parameter. It is clear that the state transition matrix 3?n uses Newton's laws of motions in a straightforward manner to transition from the past state to the current one. More interesting, note that the matrix Tn turns the state process into a non-homogenous Markov process by making the noise variance time vary ing. The noise transition matrix models the phenomena that as more time passes between successive measurements, the less deterministic and more random the state trajectory be comes. Assume the radar is located at (x s > n , y ,n) where x Sin ,y SjTl are the cartesian coordinates
s

of the sensor location at time n. The sensors observations Yn are traditionally taken in polar coordinates [25], these sensor coordinates cause the state model to become nonlinear due to this coordinate change. The observation model is written,
Yn.(

V

M = ( n J \ (.O^n

Z-s,n) ~t~ (y-n

~%d ) +W,,, Vsj-a) ) J

(3.1)

where 9 n ,r n e R are the bearing and range measurements at time n, respectively. The system noise Wn ~ J\f(0, is a Gaussian i.i.d. process.

In bearing only tracking, the bearing 0 is available to perform tracking with. Therefore, one rewrites the observation model 3.1 as follows, Y n = 6 n = arctan {y n - y S)Tl ) / (x n - x Si7l ) + W n . 22 (3.2)

Where the noise W n is a Gaussian random variable and Y n denotes the noisy bearing observation. Note that the bearing only problem is far more difficult than the standard radar tracking problem. Indeed, if the sensor is stationary, (i.e. y SiTl = y s ,m, x s,n
=
x s,m

for all n, m e N+), the state Xn is not observable [26]. It can be shown that problem is observable if and only if the sensor's motion has at least one more non-zero derivative compared to the target's motion; in the case of near constant velocity, this implies that the sensor must maneuver in an accelerating manner. This problem of observability for a single sensor disappears if multiple sensors observe the target, i.e., the state space system is always observable assuming multiple bearing readings [26]. The model above 3.2 can be easily extended to K multiple sensors by stacking the multiple observations arctan arctan
Y« J \

where

denotes observation made by sensor i at time n and

^s,n) denotes the z-th

sensor's cartesian coordinate at time n.

3.2

Classical Algorithms

3.2.1 Extended Kalman Filter
Since the tracking problems discussed above are nonlinear, the standard Kalman filter cannot be used directly. Instead, the Kalman filter ( [24], [27]) operates on a linearized version of the observation model 3.1, this filtering strategy is known as the EKF ( [24],

23

[27]). Using notation introduced in section 2.3.1, calculate the Jacobians J' =

(
J'n

(j/n ys

V

~Vn --ys,rx ^ { p ^ n 2Zs,n) 2 0 (yn. y.3,rt)2 *Es,n ~f~ In *Es, n 0

,n) ""t"~ (Z/n ' ' 2-S,N) "I" (J/N

Z/s,n) )
0

^ ^ (Z/N

*£s,rc 2/S,'

-v
-1

US^TL) ) 0

/

and obtain the linearized observation model, xn = ^nxn_i + rn vn arctan (JJTL\TI--1 2/s,n) / {^n\ri-- 1 2<s,n) Y, 2 . \2\ 1/2 I + Jn ( Xn -- Xn|n_i ) , ·En\n-- 1 -Es,ri) (z/n|n--1 Us,n) where Xn|n_i = [a;n|n_i, rc^n-u 2/^|n-i]T is
state

prediction at time n as discussed

in section 2.3.1. Applying the standard Kalman filter, the prediction stage is, -Xnin_x
Pn|,,-1 =

j£X n-^-n--l|n--1
J£Pn-l|,,-l ( J i f + rjs,,r,,,

paired with the estimation update stage, K,, = P,,|,,_! (Jjf J»P,,|,,_! (J»f + R* Xnin = Xnin_i + Kn {~Y n -- J9nxnln^
Pn|n = (I -- KnJ^)Pn|n_l-

Similarly, one can derive the EKF for bearing only tracking 3.2, JI = --yn ys,n ((yn--!/s,n)2 + (xn-Is,n)2 with a linearized state space model xn - $nxn_! + rnvr ~^~n arctail (jJn\n--\ Us,n) / 24 1 ^s,n) ""f" J72 f-^-n X!nln--1 0 ( Xn (l/n ys,n)^ -1 0 , ZEs.n ~)~~ S' n Xn--X S ,T.

The EKF for bearing only tracking is similar to the one discussed above where one sub stitutes the radar problem's measurement Jacobian, Jft, with the Jacobian associated with the bearing only measurement model.

3.2.2

Range Parameterized Extended Kalman Filter

A significant issue with the EKF is the problem of divergence due to incorrect initializa tion. The EKF must be initialized with an initial range estimate, however, this estimate can be highly inaccurate since only the bearing information is available. As a consequence, the EKF can diverge after a few time iterations regardless of the accuracy of the bearing observations [15]. Researchers attempted to resolve this range initialization problem by introducing the RPEKF [16], [17]. This filter is constructed as a filter bank of N indepen dent EKFs, where each EKF is initialized with a different range estimate. After a few time iterations one can apply convergence diagnostics to determine which EKF is nondivergent and eliminate the diverging filters. This is done by first picking a coefficient of variation CR for each EKF which governs the stability of the associated filter. Usually the initial range is known to be between two limits, 0 < rmin < rmaa;. One can compute a sequence of ranges that progresses from rrnin to rmax and assign a sequence mem

ber to each EKF. The standard way of obtaining this range sequence [16] is to use the geometric progression
m m min

where

is the range used to initialize the i-th EKF. Also, define a coefficient of variation

CR for each EKF to obtain the associated standard deviation o~i -- r\/CR. Given an initial bearing measurement 0Qr initialize the EKF bank as follows (Ti sin OQ \ TICOS OQ

the initial covariance matrix

P i% 0

is also initialized using

60

and r£.At time n, the results

of each EKF are combined by keeping track of a weighting coefficient jiiTl that is updated 25

recursively
Tz,n
=

Ti.n--lP(2/n|fc)j

initially, it is assumed that all EKFs are equally valid and set all weighting coefficients 7i)0 = 1/N. The EKF likelihood density function p(yn\i) is evaluated by assuming a Gaussian distribution
p

{yn\i) Si, n

11 . n =Y eXP 77 (jjri yj det(6i)nJ -- Hi t nPi > n ln-iIi^n + Rn,
/

/n(^i,n|n--l))

cr*
^itn (jJn

fn (%i,n\n-- 1))

once the weighting coefficients calculated
|n
Pn\n
= =

are obtained, the estimation and covariance matrix are

/
5Z/£=1 Ti,n

1 ^Yi,n-Ei,n\n
^i,n|n) (-^n|n ^ -

"1" ('^n\n

Simulations in [16] show that the RPEKF outperforms the EKF under most situations, es pecially during initialization and low observability conditions such as very high or near zero bearing rate. This enhanced performance comes with an increase in computational complexity since the RPEKF is a collection of EKFs with the ad ditional overhead of com puting the filter weighting coefficients.

26

Chapter 4 Multiple Sensor Tracking with Non-Response
In this section we present a new method of fusing multiple observations in a nonlinear system while accounting for missing data. This is done by combining particle filtering with the multiple imputations (MI) technique. The MI method replaces the missing data with imputations, i.e. randomly drawn values, to form multiple complete data sets. Each data set is then particle filtered, the results of those multiple particle filtering operations are then combined as a weighted sum. The problem domain of data fusion in an asyn chronous radar network is used to test the performance of this algorithm.

4.1 Problem Formulation
Consider a time-varying stochastic system with Xn denoting the state at time instance n, It is assumed that Xn behaves according to a non-homogenous Markov chain with transition probabilities as described by the recurrence equation Xn = ^n(Xn_1,Wn), (4.1)

where W n is random evolution noise, assumed to be an i.i.d. stochastic process and ip n is the non-homogenous evolution transformation. Also, let the system be observed by K

27

sensors .where the measurement is modeled un= :

V^(x Kz,/c)y
n;

let Un denotes the noisy observation of the state Xn such that V Tl j- is an i.i.d noise process and tpk is the measurement transformation for sensor k, respectively. Let C/ n ,/= denote the kentry of the K-dimensional vector Un. At each time instance n,some sensor observations may not be available or missing. In order to handle this missing data,, consider the random indicator variable Rn,fc which corresponds to observation Unjc/ this variable indicates if observation U* is available or not
Rn,fc
:

1 observation is available from sensor k at time n 0 observation is missing from sensor k at time n at time

Next, define the missing information set Zn as the collection of observations instance n for all observers k -- 1,..., K such that information set Yn is the collection of for all k =

= 0. Similarly, the available such that = 1. It is

assumed that the missing data mechanism is independent of the missing observations given the available observations, this can be written as P(Rn,fc|Zn, Yn) = PCR^IY,,) for all k, n. (4.2)

This standard statistical assumption is known as Missing at Random (MAR) [11]. Our objective is to obtain the posteriori probability density function of the state given all past and present observations, written as p(Xri|Y0;71) where Y0;Ti. denotes all observations from the initial time instance to time instance n.

4.2

Multiple Imputations

The technique of MI was developed by Rubin [11] to deal with missing data in surveys. Missing data can introduce bias into the statistical estimation process, the existence and severity of the missing data bias depends on the missing data mechanism. Unfortunately, 28

the structure of the missing data mechanism is almost never available for analysis. How ever, under the condition of MAR (4.2), it can be shown [11] that one does not need to know the structure of the missing data mechanism. Consider K sensors all observing the same hidden state X, let Y denote the set of all available observations from the sensors and Z denote the set of all missing observations. Moreover, let R -- [Rj,..., R/<~]T be the J<"-dimensional indicator vector for the response of the sensors. Note that this problem is not time dependent and there is no state space model (4.1). The probability density p(X|Y, R) can be written by the integral equation,
p(X|Y, R) = Jp(X|Y, Z, R) p(Z|Y, R)dZ, (4.3)

using the condition of MAR, it can be shown [11] that equation 4.3 reduces to p(X|Y, R) = y*p(X|Y, Z)p(Z\Y)dZ. This simplication implies that one does not need to know the statistical structure of the missing information mechanism. One can approximately compute the density p(X|Y, R) by the Monte Carlo approximation
M

p(X|Y) = Jim - £>(X|Y, ZJ ), i=i

(4.4)

where Zj ~ p(Z|Y) are the multiple imputations indexed by j = 1,..., M. See Figure 4.1 for a diagram of these imputations, it is easy to see that given a single partial measure ment, the ME algorithm creates N likely full measurements. Notice that MI does not use the past observations and the state transition equation in estimating the density p(X|Y). This is significant since many real world problems are well modeled by a Markov structure, which does use past values to determine the present ones. Thus, in such application, it is expected that the performance of the MI be non-optimal.

29

Original Data

Imputation 1

Imputation 2

Imputation N

Multiple Imputations

Available Data

Available Data

Available Data

Figure 4.1: Multiple Imputation - Imputation Diagram.

4.3

Multiple Imputation Particle Filter

A new algorithm to resolves the mentioned deficiencies in the particle filtering and mul tiple imputations algorithms is presented. This algorithm performs the fusion using both the state and observation dynamics while accounting for the missing data.

4.3.1

Approximation of the Imputing Function

The MI method draws imputations from the missing data probability density p(Zn|Y0:n) as shown by Equation 4.4. However, in the context of a state space model with missing information, this density is unknown. In similar applications with unknown missing data probability density, a common solution is to draw the imputations using Markov Chain Monte Carlo (MCMC) methods [12]. These MCMC methods are iterative in nature and may not be applicable in some problem domains such as real time systems. In this section

30

a new approach is presented to performing the imputation process by utilizing particle approximation techniques. The imputing probability density p(Zn|Y0:ri) can be written p(Zn|Y0:n) =

J p(Z |X )p(X |Y
n n n

0:n)dXn.

(4.5)

Note that the filtering density p(Xn|Y0:n) appears inside the integral, since one does not know this density p(Zn|Y0:n) cannot be sampled from directly. However, equation 4.5 suggests the following approximation. First, find a discrete density p(Xn|Y0;n) that ap proximate the true filtering density well. Then using relationship 4.5, one can obtain the discrete density p^Z^IYo^), which will approximate the desired density p(Zn|Y0:n). Then one can write the approximate filtering density p(Xn|Y0;n) using a particle approximation N
p(X,,!Y0;,,) =
i=1 S ' J ( X n-

x,,,i),

(4.6)

where the particle set {w^, X^}^ is obtained by performing the particle filtering with no regard for missing data. Substituting this approximation into equation 4.5 one obtains the approximate proposal function 0(Zn|Yo:n) N
f>(Zn|Y0:n) « ^(Z,,|Y0:n) = ^^>(ZnlX",0i=l

Assume p(Zn|Xnji) = p(Yn[Xnii) then this mixture of densities is known and can be sam pled from in a straightforward procedure.

4.3.2

Multiple Imputations Particle Filter Algorithm

First, the filter draws random observations or imputations, from a proposal function <fi
zjn

~ 0(Zn|YO:n) for

j =1 , . . . ,M ,

where each imputation

zjn

has an associated weight wjn as in section 4.3.1. Note that the

filtering probability density p(Xn|Y0:n) can be written [13]
p(Xn|Y0;n) =

J p(X |U
n

0:ra_i, Yn)p(Zn|Y0:rl)<2Zn.

(4.7)

31

The Multiple Imputations Particle Filter Algorithm · Initialize particle sets x j'1 ~ · For times n = 1, 2,... · Impute additional measurements 4 ~ 0(Zn|Yn) and setZV£ = {4,Yn} · For
j = 1 , . . . ,M fi(X. o)

arid set W Q 1 = 1 / N for

i --

1,. . . , N , j = 1,. . . , M .

· Sample the importance function x^ 1 ~ TC (Xn|iU£) For i = 1 , . . . , N · Evaluate the importance weights: For i = 1 , . . . , iV · For i -- 1,..., iV Sample the index dj(i ) distributed according to discrete distribution such that ¥(d(i) = I) -- w^ 1 for I -- 1,..., N . . · Set xif -- and
=

-- P1 ) and normalize w%n = wlTJ

^4

1/iV for i = 1 , . . . , N .

By forming the imputed data sets tion, rewrite equation 4.7

= {Z^ Yn} and taking a Monte Carlo approxima ;_ ·
M

·' . (4.8)

p(X,,|Y0;,,) «

wip(X,,|U0;,,_i, Ui).

3=1 Next, the algorithm performs particle filtering on each data set U7{ to obtain the approxi mation
N

- <·'), i=1

(4.9)

where x^1 is the i particle for the j imputation at time instance

n and w% % is its

associated

weight. Finally, the algorithm combines the multiple particle filtering results by substi tuting equation 4.9 into equation 4.8 to obtain an approximation of the desired density
M N

p(X,,|Y0;,,) » £ £ «'i5(X,, - a#). 3=1 i=l

32

See Figure 4.2 for a diagram of the full MIPF algorithm. It is clear from the diagram that the MIPF is eqx^valent to a bank of particle filters where each particle filter processes a single imputation. The results of each particle filter are then combined as a weighted sum to obtain a single approximation of the filtering probability density.

Observer 1

Observer 2

Observer K

Observation 1

Observation 2

Observation K

Approximate Imputation Density

Draw Imputations

Imputation 1

Imputation 2

Imputation N

Particle Filter 1

Particle Filter 2

Particle Filter N

Weighted Sum

Estimator

Figure 4.2: Multiple Imputation Particle Filter - Diagram.

33

4.4

Iterative Multiple Imputation Particle Filter

One can increase the approximation accuracy of the MTPF algorithm by introducing an iterative version of the algorithm; this modification allows us to refine the approximation of the imputation proposal function. Let m denote the iteration number of the algorithm, one can rewrite equation 4.5, (4.10) where pm(Zn|Y0;n) denotes the missing data density obtained atiteration m andpm_1(XnjY0;n) the a posteriori density function obtained at iteration m -- 1. For the initial iteration, set p1(Xn|Y0;n) to be the approximation obtained by following the algorithm described in sections 4.3.1,4.3.2. Thus, the iterative algorithm is a feedback particle filter where the feedback data at iteration m is the estimated density pm(Xn|Y0:n).

4.5
4.5.1

Convergence Analysis
Filter Formulation in Probability Space

Let (5T2, JF, P) be a probability space where T -- B(Rna:) is the Borel set of Mnx, the Borel set is the standard set of ail possible probability events on Mn:c. On this probability triple define a vector-valued stochastic process X = {Xn,n e N+} where nx is the dimension of the state space of X. The process X is Markov with initial distribution XQ ~ probability transition kernel K(x^xn^) P(X n e A\X n -i -- x n -{) -- [ Kixnlx^dXsAeBiW 1 *) JA The process X can be viewed as the hidden state process to be estimated, for example in a radar tracking problem domain, X would represent be the true object position. Next, define a stochastic process W -- {Wn, n e N+} where Wn = ..., W^} and is an
/J,

and

n^-dimensional vector for 1 < i < k. The process W is conditionally independent of X P(W n e B\X n = x n )

I

g(w n \x n )dw n , B e B(M. nwXk ). 34

(4.11)

The process W can be regarded as k noisy observations of the hidden markov process X, in our application these observations would represent all observations from multiple radars. Let the density of W conditional on X have the following factorization k g(w n \x n ) = ]~fpi(iu^|a;n). i=i (4.12)

This factorization can be regarded as the requirement that given an array of k sensors, each sensor's observation is independent of all others. Thus, combining the statements 4.12 and 4.11, one arrives at P(WieC\Xn= Xn)= [ s(<|a;n)d<,Ce£(R^).
Jc

Consider the non-response vector-valued stochastic process R -- {Rn, n £

where R is

a n^-dimensional vector. Let {rzn G (0, 1)}TM^ be indicator variables. Define the following sets, 2:n = {w l n \r z n = 0 for 0 < i < y n = {w x n \r l n = 1 for 0 < i < n w } , and let the probability density h(z n \ro : n , yo :n ) be h(^z n \vQ :n} yo-.Ti) = P(^Z n £ dz n \R^ : m -- TQ :rn ^YQ :m yo-.m)·

Clearly, the probability density h represents our knowledge of the non-response mecha nism. Consider the following probability densities of interest P (%i\yk:j) = P{Xi G dXi\Y k = y k ,...,Yj = yj) P (p^m |2/0:rri! ^*0:m) The probability density p R(,X m (E
2/0:mj RQ-.TTI ?~0:rn) ·

(4.13)
(4-14)

r 0:7n ) can be thought of as the posterior probability den

sity which combines the data from observations and non-response while the family of probability densities p (Xi\yk:j) describe the probability distribution of the state given only 35 PROPERTY OF RYERSON UNIVERSITY LIBRARY

a set of observations. Traditionally, one is interested in obtaining the probability density P (xn\y n , r n) to compute estimates such as MAP and MMSE. The distributions p (x n \y n , r n ) and p (xn\yn) are related by the following expression P {x n \y n , r n ) =

J p (x \y ) h(z \y
n n n

0 : n )dz n .

(4.15)

Standard Bayesian filtering theory gives us the equation P Sp^nYUn) substituting this expression into 4.15, t \ ^ P\X n \y n ,r n )
f u r
= /

g(w n \x n )p On\y n -i) , i \ r i i J g{w n \x n )p (ic n |y n _i)
r

1

\f
tU0:n-l,2/n)

giwnMpixnly^) \
-7--7 j 7 j T

J

\J g{w n \x n )p {Xr^yn^) J

dz n .

(4.16)

This equation cannot be generally solved except for very specific models such as lin ear Gaussian. Thus, one must resort to approximation strategies. Consider a set of val ues or particles distributed approximately according to p (x7l-i\yn-i), then sample xln ~ K{xn\xln_1) using the standard bootstrap procedure. The particles are distributed approx imately according to p (xn\yn-i) [18], they construct the empirical distributions, 1 p N (x n \y n ,,i) = --
N
N

i=1
PN {xn

\y n ) = X>n<^>
i=l

where i
n

=

g(^nK)

The additional knowledge of the non-response can be incorporated into the estimation by substituting the empirical distribution pN (xn\yn) into Eq. 4.15 in place of the true distribution p (xn\yn). Having done so, one arrives at the Monte Carlo approximation P N (^n|yn, r n ) = / h(z n \w Q:n -x, y n )p N {Xn\y n ) dz n .

36

The integral above cannot be evaluated explicitly so a naive Monte Carlo procedure is adopted to approximate the integral. Let the imputation set {z° }^0 be sampled from the nonresponse density, z3n ~ h(zn\w0:n_iy yn), then one arrives at the relation 1 M p N > M (x n \y n ,r n ) = p N (x n \y n ,r n ) -- ^ ^ 4 i=1

Therefore, the empirical density p N > M (x n \y n , r n ) approximates the desired probability den sity p (x n \y n , r n ) in terms of two sets {to*, and {z j n }jL ± .

4.5.2

Almost Sure Convergence

The integral expression 4.16 can be thought of as a sequence of three transformations whose overall result is taking a probability density pN>M (a:n_i [yn_i, rn_1) to the next one in time p (x n \y n , r n ). One can write this sequence of maps as p (£ n |y n _i) --> p (x n \y n ) --> P (xn\yn, rn) where p (xn\yn-i) and p (xn\yn) are intermediate distributions due to the par ticle filtering. In order to prove the convergence of this sequence of mappings, let us begin with an abstract argument and later on show how it is related to the algorithm at hand. Consider a metric space (E, d) and let {an}^!=1, and be sequences

of continuous functions a n ,b n ,d n : E --*· E indexed by n 6 N+. Also, let In ~ d n O CL n O b n . Define two, not necessarily continuous, perturbation functions f M , c N : E --> E in the following way. Let us assume that as N and M increase, cN and fM will converge to the identity function. Now, perturb kn using these two functions
ln'M
=

fM

0d

no

cN o ano CN O bn.

Let e M and e N be a sequence of elements in the metric space E indexed by M and N respectively and let e E E denote a single element of E. It is assumed that fM, cN satisfy the following conditions for all such sequences eM,eN/
N-*00 M-->00

lim e/vr = e =4> lim c N (e^) -- e
N--^oo

(4.17) (4.18)

lim CM

= e =4> lim /M(e^) = e,
M --»oo

37

then the following lemma can be shown true [18] Lemma 3 Let a n ,b n , k n and c N be as defined above. Then if c/v satisfies condition 4.17, lim k£ N-> oo n Moreover, satisfies
N--TOO

= kn.

lim e^r = e

N-->OO

lim

(ew) = k n (e).

Then one can prove the following lemma Lemma 4 Let d n , an, 6n and / M ,
&e as defined above, then if conditions 4.17-4.18 are satisfied,

lim e N = e =4> lim /M( lim (d n o k,,)(e N )) = Z,,(e). jV--too M--too TV--too Proof Let l^' M -- f M ° d n o Lemma 3, lim e^r -- e =£- lim k n (ejy) = k n {e), N--tco TV--too since d n is continuous lim TV--too (e N ) = k n (e) => lim (d n o k^)(e N ) = (d,, o fcn)(e). iV--too where k% -- c N o a n o c N o b n and k n = a n o b n . Then by-

Now set e -- (d n o k n ){e) and &M = ]imjV_+00(dn ° k n)(&N) for all M then clearly, e, &M G E and limM-too Using condition 4.18, the argument follows

lim e M = e =4> lim f M (fiM) = e M--too M--too => lim f M ( lim (d n o k^)(e N )) = (d n o k n )(e) = l n {e). M--too N--too

Specializing this abstract discussion to the domain of MIPF, let E -- VTM* be the space of probability measures endowed with the topology of weak convergence as described in

38

section 2.1.3. Let v E V n x be an arbitrary probability measure and ip is any continuous bounded function, define the mappings (a n (i/),^) = (b n (v),ip) = I / g{y n \x r i )v{dx n ) ) \jR n x J / / ip(x n )g(y n \x n )v{dx n ) J]R"x

/ ip(x n )K(dx r i \x n - l )u(dx n _ 1 ). JJRNX

Assuming K is Feller (i.e. and the function g is continuous bounded strictly positive then an, bn can be shown to be continuous. Also, let the sequence of functions dn be mappings dn : Vn* --> "P71* such that
dniy)

= / v(dx n \z n ,y n ) H (dz n ), Jw-

(4.19)

which implies (dn( v),tp) = = JR"I / · <p{zn)v(dx n \z n ,y n )H(dz n )

It is convenient for the mappings d n to be a continuous operator, this is quite reasonable since one can interpret that requirement as the fact that adding or removing observers will influence the observations in a continuous manner (where the continuity is in the function space sense). Moreover, define the perturbation functions c N and f M

c

" ^Pv'
=

=

lit***

j=l

where Vi,Wj are i.i.d. random variables with common distributions v c , vj respectively. One arrives at the following lemma, Lemma 5 IfcF and f M are as defined above, then they satisfy conditions 4.17-4.18 almost surely. Consider the empirical measurepN'M {xn\yn, rn), it is easy to see that P N , M (z n |y n , r n ) = f M °d n oc N oa n oc N ob n (p N ' M (rc n _i|y n -i, ^n-i)) = k^ M {jp N M ( a; n _i|y n _i, r n _! Thus, one obtains the following theorem: 39 '

Theorem 2 Assuming the transition kernel K is Feller and g is bounded, continuous and strictly positive, then almost surely lim lim p N M {x n \y n i r n ) = p (:x n \y n , rn), N-->oo M -->co where the convergence is in the zveak sense Proof Using the definitions above, P N (x n \y n ) = (c N O a n O C N
o

b n )(p N (rEn-llS/n-l)) = KN (P N (®n-l|2/n-l))}

and if lim^ooP^ (a?n_i|yn_i) = p (:cn_i|yn_i) then lim^^p^ (xn[?/n) = p(x n \y n ). Also, note that p N * M (x n \y n , r n ) = ( f M o d n o k^)(p N then by lemmas 4 and 5, the argument follows lim ( f
M

M-->oo

N -->-oo

lim (d n o k^)(p N (a;,,_i|2/n-i))) = d n o a n o b n = p (xn|y n , r n ) ,

which implies
N--*-oo M-->oo

lim lim p N ' M (x n \y n , r n ) = p (x n \y n , r n ) . ·

The convergence theorem 2 above indicates that the MIPF will convergence in the weak sense with probability 1 (almost surely) as the number of particles approaches infinity. However, this result does not guarantee that this convergence will happen with a finite number of particles nor that the squared error will be bounded.

4.6
4.6.1

Performance Analysis
Radar Network Data Fusion

Many radar systems are implemented using a radar network in order to cover a large area and increase reliability, these radars transmit observations asynchronously due to the 40

large distances between the radars. The radar network's state space model is formulated and then simulated in order to examine the performance of the MIPF as compared against the EM algorithm. Multiple Radars Model Consider K radars measuring a common target. The target motion is described by a simple near constant velocity state model [1] -^n+1 -- the matrices + rn+1V^,

and Tn+i have been defined in section 3.1. The state vector X n is defined

as Xn = [x(n)> vx(n), y(n), vy(n)]T, and the system noise Vn ~ A/^O, E,,) is assumed to be a Gaussian i.i.d. process. Let ip denote the cartesian to polar transformation

Also let the radar observation model be given by Yl = <p(X,,) + Wi, where W n ~ vV"(0, T, J W ) denotes the system noise, which is assumed to be a Gaussian i.i.d. process. It is also assumed that the system and observation noises are mutually independent.

4.6.2 Simulation Results
Two radar network configurations are simulated: a network of two asynchronous radars and a network of three asynchronous radars. The performance of the MIPF is compared against the standard EM fusion algorithm [12].The two radar network is simulated for twenty time units where at each time instance, the observations are repeated thirty times to obtain performance results. Similarly, the three radar network is simulated for twelve time units where at each time instance, the observations are repeated ten times. Both 41

simulation configurations use a particle filter with N = 250 and M = 40. The perfor mance results for the two radar network are shown in Figure 4.3, the error metric is mean squared error. It is clear that the MIPF offers significant performance improvement over the EM algorithm. Figure 4.4 shows some performance gains by the MIPF for the three radar configuration. However, the performance gain is not as significant as for the two radar network. This discrepancy can be explained as follows, one expects that the more dominant the missing data behavior of the system, the more performance gains we will realize from the multiple imputation particle filter, which is designed to deal with miss ing data, as compared to the EM algorithm. Due to the nature of the simulation, the ratio of missing to available information for the simulated two radar network is 50% while the missing information ratio for the simulated three radar network is almost always 25%. Then, as expected, the multiple imputation particle filter shows significant performance gain for the more aggressive missing data pattern of the two radar network.

42

EM Algorithm ·Multiple Imputation Particle Filter

»v Ii
*iji

0

10 Time Units

15

20

Figure 4.3: Performance of MDPF and EM for Two Radars - Missing Data Ratio of %50

43

10

---EM Algorithm --Multiple Imputation Particle Filter

CE

10
Time Units
i

12

Figure 4.4: Performance of MEPF and EM for Three Radars - Missing Data Ratio of %25

44

Chapter 5 Sequential Monte Carlo Methods for Multi-Site Bearing Only Tracking
Bearing-only tracking is a highly nonlinear problem that can be effectively tackled using SMC methods presented in Chapters 2 and 3. However, a direct application of the particle filter to the bearing only problem is problematic since incorrect filter initialization can cause divergent behavior. A new method for particle filter initialization is presented in this chapter, this approach is applicable for the situation of multiple bearing only sensors. It operates by resolving the initial state using a least squares approach.

5.1

Problem Formulation

Consider a target moving in two dimensional space, for simplicity we'll assume that the target does not maneuver and follows a near constant velocity model [1]. Thus, one has state dynamics equation

xn = $nxn_1 + rnvn)

(5.1)

the matrices 3?,, and Tn have been defined in section 3.1, the state vector is written Xn == [x(n),y(n),v x (n), v y {n)] T and V n ~ A/"(0, S,,) is the state transition noise. Assume that I< sensors are taking noisy bearing measurements of the target. These sensors are modeled

45

using the standard bearing only measurement model

/
Yn =

Yi \
y2

/ arctan ( Vn y\'n ^ \x n -x^ n j
arctan (
x s,ri

) J

+ w,,.

(5.2)

RF J V Let the symbols yxs n, x1

f yn-y*n ^ arctan \x n -xj< n J

J

denote the cartesian position of the z-th sensor at time n. The

observation noise is a K-dimensional Gaussian noise distribution, Wn ~ ^(M^, S,u).

5.2

Least Squares Initialization

As discussed in the previous section, tracker initialization is a major consideration in any bearing only tracking systems. Therefore, a new initialization technique is proposed; it uses previous work done by Don Koks [28] and combines it with particle filtering tech niques. Consider the probability distribution /_/,(Xo) which describes the statistical behav ior of the target's initial state and assume that its initial velocity components are known. Therefore, one needs to generate the particles' position entries using the initial bearing measurements Y0. The idea is to apply a least squares procedure to obtain multiple esti mates of the target position. Next, assume the initial distribution is Gaussian and estimate its statistics using the multiple estimates. Finally, one can draw the particles from this ini tial distribution. Following [28], write the initial range and bearing of the target as r{ and 0{ respectively, where the index i denotes that these range and bearing are taken by the ?>th sensor. The following trigonometric relations are true, Ti COS
=

Xq --

rt- sin#! = yo-y l s ,o, for all sensors 1 < i < K. By writing these relations for all sensors, one can eliminate the unknown initial coordinates [x0, y0] and rewrite the trigonometric equations in matrix 46

form,
A[r!r 2 · · - r K ] T = B,

(5.3)
0 0 0 0

where

A

( COS #1 -- COS 0 2 sin 61 -- sin 62 0 COS 62 0 sin 02 0 0 0

0 0 -- cos 03 -- sin 03

0 0 0 0 cos 9 K-1 sin 0K-I

\
(5.4)

V
and

0

cos 6K · sin 0 K j

-xs,0 ^ ^5,0 \ yl,o + -Vs,0 "®s,0 + -X,3 s,0
B
-Vs,0 + - V s ,0

(5.5)

_Jf-l _1_ xs,0 ^s,0 1 , _..K Vs, 0 + Vs,0 / This is an over-determined matrix equation since A is not a square matrix, a single solu

V

tion does not exist so one may opt for the standard least squares solution which minimizes the squared error. Let the symbol fi denote the range estimate at the coordinate system of the i-th sensor, then write the least squares solution as,

[rx, r 2 · · - r K ] T = (A t A)

1 A T B.

(5.6)

Transfering these estimates to the target's coordinate system,

Xx X2

= (ri cos 0x + X Q s , r 1 sin 0 1 + y^ a ) T = (r 2 cos 0 2 + r 2 sin 0 2 + yj s )T

(5.7)

X x = (r K COS 0 K + xg s , RK sin 0 K + 7Jo, s ) T
Therefore, there are K estimates of the initial position of the initial target. One can com-

47

pute the sample mean and variance of these estimates,

M,, =
SM =

1

K

--

<5'8)
(5.9)

j=l

i=i

Let us make the assumption that the initial state distribution //(X0) is Gaussian, since the initial distribution is Gaussian, one can use the sample mean (Eq. 5.8) and variance (Eq. 5.9) to estimate the distribution's parameters and let /x(X0) ~ Xm). Finally, we've

obtained an approximate form of the initial position's probability density function.

5.3

Bearing only Target tracking with Monte Carlo Algo rithms

As discussed in section 2.4, one is interested in approximating the probability density distribution p(Xn|Yn), this is done by using a set of particles weights where
N

and their associated

is the number of particles and

j

indexes the particle set. The

particles are initialized by sampling from the initial state distribution /i,(X0),

<

=

1/N,

where the initial distribution £i(X0) is obtained by the least squares procedure. At every time iteration, the particles are sampled from an importance function,
Xn

~

7r(X |x^_ , Y ), n 1 n

(5.10)

where 7r(Xn|x^_1, Yn) denotes the importance function that is yet to be specified. A parti cle's weight is evaluated up to a normalizing constant by the following recursive formula

,.J =,J KY»k-.)p(4k-i) " "-1 ^(X^xi^.Yn)

,5 n>

The coefficients are then normalized by dividing the given coefficient by the sum of all weighting coefficients.

48

5.3.1

Particle Filter for Bearing Only Tracking

A straight forward application of the particle filter algorithm to the bearing only track ing application uses the prior importance function (See section 2.4 for more information) and computes the weight using the likelihood function. The prior importance function

p(Xn|Xn_i) can be written P(x,,l4-i)= 4^2|rrsj,,|V2exp
and weighting formula W°n = ^-xXYnlx^), where the likelihood function p(Yn|x^_1) is POfnWn-i) ~^/'(axctan(xi_1), !],,,). (5.14) (5.13)

(x"-

(r^sj,)"1(x,, -

j,
(5.12)

As noted in chapter 2, the prior importance function does not use any measurement in formation in drawing the particle and thus may require more particles as compared to the optimal importance function. In order to incorporate the measurement into the im portance function, let us first linearize the state space model and then derive its optimal importance function. Such an approach to importance functions is known as the linearized

optimal importance function (See [8] and section 2.4), note that this idea is conceptually sim
ilar to the EKF. Following the derivation in 2.4, let the function g : R4 --» R^ denote the observation transformation (Eq. 5.2) and / : R4 --> R4 denote the state transition equation (Eq. 5.1)

Xn

= /(Xn_1)+Vn

(5.15) (5.16)

Yn = g(Xn) 4-Wn, then the linearized state space model's observation equation is written

Y,, ~ 9(/(X,,_i)) +

d9 <x") g

(x» " /(X,,-i)) .

(5.17)

49

Calculating the measurement equation's Jacobian,

(
dg (Xn) 5Xn
A=

(--y+yl n ) (x-;cin)2+(y-virl)2 (~y+y (a:-a:i1n)2+(y-y2,n)2
2

/·_ __1 \ i

yi,n)2

0 o\
0 0
(5.18)

n)

1 2 1 1 ty~ys,".)2 X fx--rr ^ '.n)+

V

(-y+yjfn) (^-^T*)2+(y-y

£n)2

Cx (

J**" 'I I (*-=£n) ^

0 0

J

Then the optimal importance function of the linearized state space model consisting of equations 5.17 and 5.1 can be written as

7r(Xn|<_1;Yn) ~.A/-(Mn>Sn),
where

(5.19)

. S"1 = (rjs,,r,,) i+Ar2^1A M,,
= £,, ((rJSvr,,)"1/(X^1)+ArS^1 x

(5.20) (Yn - 9(/(X,,_!)) + A/(X,,_0))(5.21)

Using this importance function, the full algorithm can be formed by combining the parti cle filter derived above with the least squares initialization scheme.

5.4
5.4.1

Performance Analysis
Simulation Results

The simulation scenario consists of three bearing only observation stations, each one ob serving the same target. The target dynamics obey the state space model (5.1) described in the beginning of this chapter. Two tracking algorithms are compared, the EKF and the particle filter. The EKF's initial state is initialized with the ground truth position and its initial covariance matrix is the zero matrix. Thus the EKF will have no divergent behav ior associated with an incorrect initialization. In contrast, the particle filter is initialized using the least squares initialization technique described in section 5.2. Figure 5.1 shows

50

Bearing Only Particle Filter Algorithm with Least Squares Initialization · Compute M m, and initialize the particle set x~ J\T{S m), let wl0 = 1/N for z = 1,..., iV.

· For times n = 1,2,... · Sample the importance function xln ~ 7V(Mn, Sn) For i = 1,..., iV · Evaluate the importance weights: wln = r
O n
1 ?-, wl and normalize wi, = wl/T) 71 n/

7r(i|l|iJ:n_1,Yo:nj

For i = 1,. . . , N · For £ = 1,..., N Sample the index dii) distributed according to discrete distribution such that P(d(i) = /) = ivln for I = 1,..., N. · Set xln = Xn^ and wln = 1/N for i = 1,..., TV.

the mean square error performance associated with the EKF and particle filter. From Fig ure 5.1, one notes that the particle filter's initial estimation performance is worse than the EKF since the EKF uses the ground truth for initialization while the particle filter uses the least squares approach. However, the crucial observation is that the particle filter's per formance improves after a few time iterations and does not diverge. Indeed, the particle filter's performance exceeds that of the EKF in spite of the EKF's initialization advantage. Additional simulations were performed using the same sensor configuration. These sim ulations were performed by generating 50 Monte Carlo tracks with identical initial state and model parameters. Each track was then recursively filtered using both an EKF and a particle filter. The root mean square error (RMSE) between each track's ground truth and estimated position is computed. The errors of all tracks at a given time are averaged to produce an overall initial performance graph which is shown in Figure 5.2. The first tracker is a particle filter initialized using the new scheme developed above. This algo rithm is compared against an extended Kalman filter which is initialized with the true initial position of the target. In other words, the EKF's initialization is optimal while the particle filter uses the non-perfect initialization. It is clear from Figure 5.2 that on average,

51

the initialization of the particle filter produces the same initial performance as that of the EKF initialized with the true state. These simulation results allow one to conclude that the initialization technique is successful since it enables the particle filter to rapidly converge to its optimal estimation without any loss of initial performance.

0.45

0.4

0.35

0.3

LU

73 0.25

CT CO

0.2

0.15

0.1

0.05

Particle filter extended Kalman filter

0

10

20

30

40

50

60

70

80

90

100

Time Sequence

Figure 5.1: Performance of the EKF and Particle filter in Bearing Only Tracking.

52

0.2

0.18

*-- Particle Rlter - initialized with least squares method -- EKF - initialized with ground truth 0.16

0.14

0.12

LLL

0.08

0.06

0.04

0.02

2

4

6 Time

8

10

12

Figure 5.2: Average performance of the EKF and Particle filter in Bearing Only Tracking

53

Chapter 6 Concluding Remarks
6.1 Conclusions
The work presented largely deals with the application of Monte Carlo methods to the problem of multi-site radar and bearing only target tracking. While radar and bearing only tracking are well studied problems, the particle filtering framework introduces new flexibilities and issues that need to be resolved. First, the problem of target tracking in asynchronous sensor networks is considered. This problem is identified as the problem of fusing multiple sensor measurements in the presence of missing data. The MIPF is introduced to systematically deal with such problems of estimation with missing data. Hie MIPF algorithm draws imputations using a new particle based method, it then com bines the multiple imputations with a particle filtering technique. Some convergence properties are derived for the MIPF, these theoretical results suggest that the algorithm converges in a desirable manner. Simulated data is used to demonstrate the effectiveness and performance of the proposed algorithm. Next, a new multi-site bearing only initial ization scheme for a particle filter-based tracking algorithm is proposed. The initializa tion schemes utilizes a least squares approach to resolve the initial state of the target. The particle filter is tested with the initialization scheme using simulated data and compared against a standard EKF. The initialization scheme is shown to perform well and allows the Monte Carlo algorithm to rapidly converge to the optimal filter.

54

6.2

Future Research

The weak convergence of the Multiple Imputations Particle filter has been studied in Chapter 4. Additional research is required to explore whatever the algorithm's error is bounded and in what sense. Also, the utilization of asynchronous measurements in en hancing the performance of model learning and system identification deserves additional study; especially in the context of particle based system identification methods. Indeed, model learning schemes are a major consideration for all mode based tracking algorithms such as the particle filter, ha much of the tracking literature, tracker initialization is rel egated to the realm of the practitioner, however the research in this thesis demonstrates that filter initialization is a significant problem that bears some relation to model selection and learning. Therefore, initialization schemes and their theoretical effect on the stability of sequential Monte Carlo algorithms require additional research, especially in the context of filters that use a slightly inaccurate model in their derivation.

55

Bibliography
[1] Y. Zhou, A Kalman Filter based Registration Approach for Multiple Asynchronous Sensors, Defense R&D Canada - Ottawa, 2003. [2] P. L. Bogler, Radar Principles with Applications to Tracking Systems, Wiley, New York, 1990. [3] B. Anderson and J. Moore. Optimal Filtering. Prentice-Hall, Englewood Cliffs, NJ, · 1979.

[4] G. Kitagawa, "Non-Gaussian state-space modeling of nonstationary time series," J. Amer. Statist. Assoc., vol. 82, pp. 1032--1041, Dec. 1987. [5] N. Gordon, D. Salmond, and A. Smith, "Novel approach to nonlinear/non-gaussian Bayesian state estimation," IEE Proceedings-F, vol. 140, pp. 107--113, April 1993. [6] G. Kitagawa, "Monte Carlo filter and smoother for non-gaussian nonlinear state space models", J. Comput. Graph. Statist., 1996. [7] M. Pitt and N. Shephard, "Filtering via simulation: auxiliary particle filters," J. Amer. Statist. Assoc., vol. 94, no. 446, pp. 590--599,1999. [8] A. Doucet, S.J. Godsill, and C. Andrieu, "On sequential Monte Carlo sampling meth ods for Bayesian filtering". Stat. & Comp., vol. 10, no. 3, pp. 197-208, 2000. [9] P.M. Djuric, J.H. Kotecha, J. Zhang, Y. Huang, T. Ghirmai, M.F. Bugallo, and J. Miguez, "Particle filtering," IEEE Signal Processing Magazine, vol. 5, pp. 19-38, Sep. 2003.

56

[10] F. Gustafsson, F. Gunnarsson, N. Bergman, U. Forssell, J. Jansson, R. Karlsson, and P-J Nordlund, "Particle Filters for positioning, navigation and tracking/' IEEE Trans actions on Signal Processing, Feb 2002. [11] D. B. Rubin, Multiple Imputation for Nonresponse in Surveys, Wiley Series in Prob ability and Mathematical Statistics, 1987. [12] J. Schafer, Analysis of Incomplete Multivariate Data, Chapman & Hall, 1997. [13] A. Kong, J.S. Liu, and W.H. Wong, "Sequential imputation and Bayesian missing data problems". J. Americian Statistical Association, pp. 278-288,1994. [14] M. B. Ghalia and A. T. Alouani, "Observability requirements for passive target track ing", Proceedings SSST '93., pp. 253-257,1993. [15] V.J. Aidala and S.E. Hammel, "Utilization of modified polar coordinates for bearingonly tracking", IEEE Trans. Automatic Control, vol. 28, no. 3, pp. 283-294, March 1983. [16] N. Peach, "Bearings-only tracking using a set of range parameterised extended KaLman filters," Proc. Inst. Elect. Eng. Contr. Theory Appl., vol. 142, no. 1, Jan. 1995. [17] R. Karlsson and F. Gustafsson, "Recursive Bayesian estimation: bearing-only appli cations", IEE Proc. Radar Sonar Navig., vol. 152, No. 5, October 2005. [18] D. Crisan and A. Doucet, "A survey of convergence results on particle filtering for practitioners," IEEE Trans. Signal Processing, vol. 50, no. 3, pp. 736-746,2002. [19] K. L. Chung, A Course in Probability Theory: Second Edition, Academic Press, (1974). [20] W. Rudin, Principles of Mathematical Analysis, 3rd Edition, McGraw-Hill, 1976 [21] K. Yosida. Functional analysis. Springer-Verlag, Berlin, 6th edition, 1980.

57

[22] T. B. Schon, Estimation of Nonlinear Dynamic Systems Theory and Applications. PhD thesis. Linkopings University, 2006. [23] P. Del Moral, Feynman-Kac Formulae: Genealogical and Interacting Particle Systems with Applications, Springer, 2004. [24] S.-h. Cui and C.-q. Zhu, "Application of Kalman Filter to bearing-only target tracking system", 3rd International Conference on Signal Processing (ICSP'06), vol. 2, pp. 1679-1682,1996. [25] X.R. Li and V.P. Jilkov, " A survey of maneuvering target tracking-part HI: measure ment models", hi proceedings of SPIE Conference on Signal and Data Processing of Small Targets, pp. 423-446, Sari Diego, USA, 2001. [26] J.P. Le Cadre, "Properties of estimability criteria for target motion analysis", IEE Pro ceedings - Radar, Sonar and Navigation, no. 2, vol. 145, pp. 92 - 99,1998. [27] L. G. Weiss, S.C.A Thomopoulos /'Target detection and localization from bearingonly and bearing/range measurements", Proc. Aerospace Control Systems, May 2527,1993, pp. 549-553 [28] D. Koks, "Passive geolocation for multiple receivers with no initial state esti mate", Electronic Warfare Division: Electronics and Surveillance Research Labora tory, DSTO.

58


