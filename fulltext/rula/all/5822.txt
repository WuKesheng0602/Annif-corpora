IC TESTING USING THERMAL IMAGE BASED ON INTELLIGENT CLASSIFICATION METHODS

by

Furat Al-Obaidy MASc., University of Technology, Baghdad, Iraq, November 1999 BASc., University of Technology, Baghdad, Iraq, July 1996

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Applied Science in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2016 Â©Furat Al-Obaidy, 2016

AUTHOR'S DECLARATION

I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

Abstract IC Testing Using Thermal Image Based on Intelligent Classification Methods
Furat Al-Obaidy Master of Applied Science, Electrical and Computer Engineering Ryerson University, Toronto, 2016

The goal of this thesis is to propose an algorithm which would can locate the defect IC on the PCB during their manufacturing phase based on a thermal image. A 3-dimensional PCB finite-element model is developed to estimate the temperature profile of stacked ICs. Image processing by noise removing and region of interest segmentation are applied. Two sets of feature extraction are presented; first-order histogram features and Gray Level Co-occurrence Matrix (GLCM) features. The Principle Component Analysis (PCA) method is applied to decrease the feature's extractions into smallest uncorrelated input. Three main intelligent techniques; Multilayer Perceptron (MLP), Support Vector Machine (SVM), and Adaptive NeuroFuzzy Inference System (ANFIS) are used to classify the thermal conditions of ICs into normal and faulty status. On validation, the proposed approach applies to do thermal testing on Arduino UNO. The experimental evaluation is performed to detect the fault condition on the real time operating PCB.

iii

ACKNOWLEDGEMENTS
First, I would like to gratefully and sincerely thank my advisor, Dr. F. Mohammadi, for her kindly instructions and help in this research. This work could not have been finished without her guidance, support, and time. Her belief in me throughout the difficult times during my research and my graduate studies is what keeps me motivated to this day. I would also like to thank my thesis committee for generously offering their time, guidance and good will throughout the review of this document. I would like to thank my friends, and all the members of the Ryerson Microsystems research group for their constructive guidance and suggestions as well as their consistent encouragement. Last but not the least, I thank my wife and my family for everything I have today. They made me who I am today and sacrificed many of their own dreams so that my dreams could become reality. They provided every tool that I needed to succeed, and their constant encouragement made the completion of this degree possible.

iv

DEDICATION This thesis is dedicated to the memory of my father, mother, wife, and family, For their endless love, support, and encouragement.

v

Table of Contents
iii Abstract iv Acknowledgements v Dedication viii List of Tables ix List of Figures 1 Chapter 1: Introduction 1.1. Introduction 1 1.2. Literature Review 2 1.3. Thesis Objectives 4 1.4. Thesis Layout 5 7 Chapter 2: Review of the Fault and Testing Techniques of Integrated Circuits 2.1. Introduction 7 2.2. Fault, Defect, and Failure Relationship 7 2.3. VLSI Defects Types 7 2.4. Fault Detection and Simulation 10 2.5. Digital PCB Testing Methods 10 2.5.1. Destructive Testing Methods 10 2.5.2. Non-destructive Testing Methods 11 2.6. Summary 13 15 Chapter 3: Fundamentals of Thermal Testing 3.1. Introduction 15 3.2. Infrared Thermography Principle 15 3.3. Principles of the Infrared Temperature Measurement 17 3.4. Classification of Thermography Techniques 19 3.4.1. Passive Thermography 19 3.4.2. Active Thermography 19 3.5. Thermography advantages and Limitations 19 3.6. Summary 20 21 Chapter 4: Introduction to Intelligent Classification Models 4.1. Introduction 21 4.2. Artificial Neural Networks 21 4.2.1. The architecture of the MLP 23 4.2.2. The Training of the MLP 24 4.3. Support Vector Machine 25 4.4. Adaptive Neuron-Fuzzy Inference System 26 4.5. Summary 29 Chapter 5: Proposed Method for Fault Detection and Classification of the 31 PCBs 5.1. Introduction 31 5.2. Proposal PCB Modeling 33 5.2.1. Geometry Model Generation 34 5.2.2. Physical Materials and Thermal Parameters 35
vi

5.2.3. Mesh Generation 37 5.2.4. Simulation Results and Verification 38 5.3. Image Enhancement 42 5.3.1. Image Denosing 42 5.3.2. Image Segmentation for ROI 45 5.4. Feature Extraction 48 5.4.1. Statistical Features Based First Order Histogram Statistical 49 Analysis 5.4.2. Statistical features based on Gray Level Co-occurrence Matrix 51 5.5. Principle Analysis Components 55 5.6. Classification Modules 57 5.7. Results and Discussion 58 5.7.1. Classification Results based on MLP 59 5.7.2. Classification Results based on ANFIS 64 5.8. Summary 69 71 Chapter 6: Experimental Evaluation 6.1. Introduction 71 6.2. Platform Setup 71 6.2.1. Real- PCB Under Test 71 6.2.2. FLIR Infrared Camera Unit 74 6.2.3. Data Acquisition System and Auxiliary Unit 74 6.3. Experimental Setup 75 6.4. Image capture and processing 77 6.5. Classification Results 84 6.6. Summary 88 89 Chapter 7: Conclusions and Future Work 7.1. Conclusions 89 7.2. Summary of Contributions of this thesis 91 7.3. Future Work 92 93 References 100 Glossary

vii

List of Tables
Table 4.1: Arrangement of ANNs based on their functional characteristics Table 5.1: Geometry PCB Dimension Table 5.2: Thermal properties for the PCB Table 5.3: Mesh limits of detailed model Table 5.4: Comparison of SNR and PSNR values of different existing filters Table 5.5: Distribution of conditions for entire samples Table 5.6: Samples for the comparison results of MLP Training Functions Table 5.7: Samples for testing and output results of the Model Table 5.8: Summarized setting parameters for the MLP network Table 5.9: ANFIS parameters setting Table 5.10: Samples for testing and output results of the ANFIS Model. Table 5.11: SVM parameters setting Table 5.12. a: Comparative training results Table 5.12.b: Comparative testing results Table 6.1: IR camera parameters set up Table 6.2. a: Comparison of training results Table 6.2. b: Comparison of testing results 22 34 36 38 45 59 61 63 63 64 66 68 68 69 76 87 88

viii

List of Figures
Figure 2.1: a) Resistive Bridge Defect, b) Interconnect Defect Figure 3.1: Infrared Thermography equipment Figure 3.2: Radiation received by the infrared camera Figure 4.1: Architecture of a Multi-Layer Feed-Forward Network Figure 4.2: Schematic diagram of an SVM classification Figure 4.3: A basic ANFIS model with two input data and two member functions Figure 5.1: The main progress steps of the proposed algorithm Figure 5.2: 3D layout of the PCB Figure 5.3: Top and side views of the PCB Figure 5.4: Three dimensional views of the meshed PCB geometry Figure 5.5: The simulation results for temperature distribution on the proposal PCB's surface Figure 5.6: Temperature variations vs Power dissipation Figure 5.7: Comparative results of image denoising methods Figure 5.8: Comparison of segmentation results for different methods Figure 5.9: Image cropping approach Figure 5.10: Image histogram representation for both color and gray thermal image Figure 5.11: Features changing plots versus temperature Figure 5.12: Flow diagram of PCA algorithm Figure 5.13: Block diagram of the General Intelligent Classification steps Figure 5.14: The proposed neural network architecture layout Figure 5.15: The MLP network performance Figure 5.16: ANFIS model structure Figure 5.17: The Performance Evaluation of ANFIS during training phase Figure 5.18: The ANFIS Fuzzy rules of the classify network Figure 5.19: The performance graph of the ANFIS Fault type classification model Figure 6.1: Arduino UNO board Figure 6.2: ATmega328P IC configuration Figure 6.3: ATmega8U2 IC configuration
ix

9 16 17 23 26 27 32 35 35 38 40

41 44 47 48 49 54 56 57 60 62 64 66 67 67 72 73 73

Figure 6.4: FLIR SC4000 camera Figure 6.5: Real PCB with IR capturing system Figure 6.6: Thermal profile for UUT Figure 6.7: Samples of captured IR image for real PCB Figure 6.8: Filtering image results Figure 6.9: Image segmentation (i.e. Otsu's method) and cropping results Figure 6.10.a: Features changing plots versus temperature for IC1 Figure 6.10.b: Features changing plots versus temperature for IC2 Figure 6.11: Proposed neural network architecture layout Figure 6.12: Proposed neural network architecture layout Figure 6.13: ANFIS structure Figure 6.14: RMSE response of ANFIS Figure 6.15: Graphical classification output based on ANFIS

74 76 78 78 79 79 81 83 84 84 85 85 86

x

Chapter 1: Introduction
1.1. Introduction Technology scaling has enabled greater integration because of reduced transistor dimensions. The significant increase in complexity of Integrated Circuits (ICs), as the core of modern digital circuits, has substantially improved the semiconductor integration level production by generation. With technology scaling, the very large scale integrated (VLSI) circuits density has grown exponentially. This leads to increase the manufacturing defects in circuits. This rapid technology scaling has resulted several reliability problems such as instance manufacturing defects, power dissipation limits and parametric variations. These threats can effect correct program execution which is the most significant aspect of any computer system. Therefore, testing of a digital circuit becomes imperative, particularly, in VLSI technologies [1]. In this context, testing techniques for digital circuits have to be improved and augmented to achieve quality levels as circuit density and area continue to grow. Traditional testing methods of digital circuits includes electronic test, visual inspection, signal analyzer and burn-in scheme. These methods are used for diagnosis and eliminate the defects and speed up weak component failures. However, problems with a short circuit, a faulty transistor, or a circuit that creates hotspot will stay invisible [2]. Therefore, the thermal testing methods are useful in the process of designing and testing of complex VLSI circuits. The thermal testing method is one of the Non-Destructive Testing (NDT) techniques for

1

testing and showing hot spots on operating ICs, depending on the failure conditions such as shorts, over stressed or faulty components [2]. The primary benefits of NDT are to detect the flaws and defects of materials without changing or destroying their functionality and guarantee the safe operation of the system components. Infrared thermography method is a common thermal testing technique in NDT in which an external stimulus is used to heat the Unit Under Test (UUT) while an Infrared (IR) camera records the temperature. The shifting of the normal thermal dissipation pattern is a sign of fault occurrence. Infrared thermography testing technique is a consecutive approach involving thermal image enhancement, detection, main feature extraction, classification, decision making and determining whether the IC has a fault or not [2, 3]. 1.2. Literature Review A brief review about the Infrared thermography testing methods for PCB and electronic circuits is presented in the following sections: H. Moldovan, M. Marcu, and M. Vladutiu [2], this work presents a testing method using infrared thermal signatures to detect and locate a fault in PCB. The proposed method is used to classify the ICs into three main categories (e.g. functional fault free, non-functional, faulty and less reliable). In this method, a stimuli vector applies to the PCB controllable inputs, and then a certain thermal response is observed for each IC on the PCB. The classification system uses an artificial neural network to classify the samples extracted from the IR image. The main drawback of this approach is the time and cost of solution achievement to get the thermal signature for every test vector applied to the primary inputs of the PCB.

2

J. Varghese, T. Singh, and S. Mohan [4], this work presents a method for thermal analysis of the PCB using MATLAB. Depending on loading condition of the PCB, two parameters are used for analysis the thermal image, the highest temperature and area of the highest temperature. Image segmentation algorithms are used based on short changes in the intensity. The histogram threshold approach is used for this study. The program compares each histogram and finds out the image with a maximum number of pixels of white or the worst-case image. But, this study is not recommended for the complex PCBs because the choice of selecting a region of interest should be improved by having been clustering based on segmentation methods. C. Wagh, and V. Baru [5], presents detection of the faulty region on PCB by thermal image processing. In this research, the samples for thermal images of fault and fault free PCBs are experimentally obtained. The statistical analysis tool such as a Principal Component Thermography (PCT) technique is used to process the IR image sequences and increase the contrast of the processed data. Here, to reduce the amount of computations, the SVD technique based on the PCT is used. The euclidian distance is used for fault diagnosis and identification by comparing the features. The proposed method is conditional due to the excessive work of associate calculations. J. Jianliang, and Y. Jiang [6], presents a fault diagnosis for component's location on the PCB. The authors worked on a template matching method to locate one specific circuit component. The main process includes thermal source identification, feature extraction, and thermal pattern recognition. Standard thermal images for PCBs are captured, and databases of the coordinate and characteristic values of the circuit components are built. Then a thermal image of the circuit under the test is matched with the feature template's images of every
3

component pre-stored in the databases. The results show that thermal image drift can be reduced, but with an increase of accuracy, the time takes a higher computational complexity and a longer time. S. Huang, C., and K. Cheng [7], in this study, present the analysis of PCBs for fault diagnosis by vector quantization. The feeding data to Hopfield neural network is used based on the code word. The code word is generated from the IR image. However, the main drawback of this study is that the mean values of the code words are nonlinearly distributed through the all codebooks, especially for small mean values. Some other approaches have done on the temperature analysis of PCBs with different techniques of thermal imaging. Thermal analysis of PCBs has been done in [8]; in this paper, Finite -Element Model (FEM) via Galerkin approach applies to analyze the temperature behavior of PCBs for different width of copper and different amount of current. In addition, [9], describes another approach for analysis of thermal reliability of components on the PCBs, presented by ANSYS software to improve the reliability of the system. In [10], a qualitative based measurement of thermal anomalies has been done by detecting the feature point and region of interest with using the stable region algorithm and matching with euclidian distance. 1.3. Thesis Objectives An intelligent testing technique should be able to detect ICs fault, extract features from the IR images, estimate the defective conditions, and do correct classification of the faults. For this purpose, the prime aim of this thesis is to detect the defective ICs on the PCBs by thermal image processing. To achieve this goal, the rigid testing method at each processing step is required to improve the efficiency of fault detection during the life time of an IC.
4

1.4. Thesis Layout Chapter 2 In Chapter 2, we describe a theoretical background of fault diagnosis, explain defects in digital circuits and describe the fault detection and simulation. Furthermore, the testing technique is reviewed in this chapter. Destructive and non-destructive testing methods will be explained, thoroughly, in this chapter. Chapter 3 This chapter will cover the principle of infrared thermography technology and the fundamental of the infrared temperature measurements. Moreover, the classification of thermography technique, its advantages and disadvantages will be presented in this chapter. Chapter 4 Chapters 4 will introduce the intelligent classification methods. A detailed review of the different classification techniques applied for detection and classification of occurred faults is explained in this chapter. Also, we demonstrate the related proposed architectures and the corresponding learning algorithms in this chapter. Chapter 5 This chapter will cover the main contributions of this work. The proposed PCBs modeling steps will be explained in this chapter. This chapter also demonstrate our approach for

5

IR image processing and classification methods. The simulation results of our work are discussed in this context. Chapter 6 This chapter will focus on the applied and implemented of the proposed method in the earlier chapters. The procedure of performing testing method, experimentally, will discuss in this chapter. Image capturing, processing, and feature extraction from the UUT of Arduino are illustrated. Experimental results are presented at the end. Chapter 7 This chapter will present the conclusion, summary of the contributions of the thesis, and future works. Finally, the reference section will cover all the sources used in this endeavor.

6

Chapter 2: Review of the Fault and Testing Techniques for Integrated Circuits
2.1. Introduction This chapter introduces the preliminaries helpful for the rest of this thesis. The fault, defect, and failure relationship are discussed in section 2.2. Defect types in VLSI circuits are described in section 2.3. The necessaries of fault detection and simulation process are briefly introduced in section 2.4. The PCB testing methods, including destructive and non-destructive test methods and their types, are discussed in section 2.5. 2.2. Fault, Defect, and Failure Relationship The defect is defined as the difference between the aimed design and the implemented hardware. If any component fails to do its duties, its output will be drastically changed. In this case, a component is said to have a fault. Fault is a representation of a physical defect reflecting a physical condition that causes a circuit to fail to do in a required way. A failure is defined as the inability of the component to perform its design role. A failure in a system is due to the incorrect design and specifications, defects during the manufacturing process, or due to the aging and environmental causes [11]. 2.3. VLSI Defect Types In the past, the main sources of defects in VLSI circuits were opened, shorts and bridges circuits. This is expected to stay in complex circuits where the increasing number of PCB layers leads to a risk of open vias and metal bridges. Nowadays, in silicon technologies, shorts and
7

open due to the particles and spots, opens in vias and interconnection lines, silicidation problems, etc. are commonly seen. In addition, in this context, new technologies introduce new types of defects which we explain about them. These types of faults occurring in VLSI circuits can be arranged as follow: 1. Resistive Bridge Defects: these defects happen when two or more distinct nodes of the circuit get connected due to a fault. Resistive bridge defects are one of the typical commonly of manufacturing defects. Figure 2.1.a, shows a bridge defect between two parallel interconnection lines [12]. 2. Resistive opens defects: defined as an imperfect resistor connected between two circuit nodes. A stuck-open defect is a special case of a resistive open in which the resistance value is large. These include open contacts, metallization open and open in diffusion [13, 14]. 3. Interconnect malfunctions: a full open in an interconnection happens when the conductive material is fully broken. The main causes for the interconnecting open defects during the manufacturing process can be due to the several factors, such as chemical polishing process, metal filling, spots during the lithography, and lens imperfections. This defect is an interconnected line, shown in figure 2.1.b [12]. 4. Delay defect: these faults cause the combinational delay of a circuit to exceed the clock period. Delay fault includes gate-delay fault, line-delay fault, and path-delay fault. 5. Memory defects: faults happened in memory sections are normally pattern sensitive, cell coupling, and single stuck-at faults in the address decoder logics.

8

Track 1 Extra material creating a bridge(short) Track 2 (a)

Full Open
Track Track defect (Full open line)

Full Open

(b)

Figure 2.1: a. Resistive Bridge FullDefect, b. Interconnect Defect.
Open Full Open

6. Parametric defects: They refer to the difference between the actual and expected power or performance criteria for the component. For example, IC may function at a certain voltage, but not over the designed range. 7. Temperature-dependent defects: They occur when a connection between two circuit nodes has a conductance, high or low enough to be connected at the normal temperatures. This defect is environmentally sensitive, and appears only under certain environmental conditions. However, at high or low temperatures, the conductance decreases so much that the connection is disconnected [15]. 8. Early-life Failures: Some individual ICs will fail early. These failures can be seen as manufacturing faults that are not manifesting as a defect just after the manufacturing. Defects and mistakes always cause early-life failures: material defects, design mistakes, errors in assembly, etc. This type cannot be detected by the manufacturing test that is performed at once after the fabrication process [15].
9

2.4. Fault Detection and Simulation

Detection and locating of faults in a digital circuit or PCB, is known as the fault diagnosis. With VLSI technology scaling, fault diagnoses techniques are important to replace or discard the faulty component in the board. In addition, fault simulation is the process of simulating a circuit with a set of test patterns and a set of faults. The response of the circuit compares with the fault-free circuit. If the response does not match, the set of test patterns detects the fault [16].

2.5. Digital PCB Testing Methods

The aim of testing methods is the effective screening of manufactured circuits to detect the faults. Because of this work deals with thermography-related VLSI test methods, we focus on manufacturing test of PCBs that are produced by VLSI techniques [15]. Based on the variety of models on different levels of abstraction, several destructive and non-destructive test methods have been developed to perform the efficient and highly reliable testing procedures that detect the internal features or surface of the PCB components [17, 18].

2.5.1. Destructive Testing Methods There are many conventional destructive testing techniques proposed so far that can check the VLSI circuits with some techniques such as logic, functional, delay and/or current testing such as Level Sensitive Scan Design (LSSD), scan path design, scan logic, random access scan, Built Â­In Â­Self Test (BIST) technique, the ad-hoc technique, partitioning technique, adding extra pin's techniques, and signature analysis technique. However, with developing the PCB
10

production technologies; these destructive testing methods are restricted obviously. At present, increasingly PCBs have a greater density and more electrical nodes. This PCBs need to avoid arranging many test points which make the performance testing method just test part of electrical nodes. The electrical performance testing method needs contact PCBs and probably damaged PCBs. This method needs a fixture for each PCBs, therefore the cost is high. It will be necessary to introduce new and efficient testing techniques to beat these difficulties [17, 19].

2.5.2. Non-destructive Testing Methods

With Non-Destructive Testing (NDT) methods, we can detect, locate, measure, and test surface or internal layers of the components causing no damage or change the integrity of its properties. Thus, non-destructive testing can be done efficiently for the manufactured items.

Today, modern nondestructive tests are used in production fields to ensure product functionality and reliability. Several types of the NDT procedures are being used in the industry such; Acoustic Emission Testing (AET), Thermal or Infrared (IR) Testing, Ultrasonic Testing (UT), and Radiographic Testing (RT). Each of these techniques has its own advantages and disadvantages that mainly relate to the testing system cost, speed, accuracy, and safety. Following sections present the most often used NDT methods such as UT, RT, AE, and IR [20].

1. Ultrasonic Testing method: Ultra-high frequency wave is sent into the component to detect an internal structure of an element under the test. The main advantage of the ultrasonic testing technique is that it can seep into objects of high thicknesses and, therefore, the flaws can be distinguishable. The main disadvantage of it is that it
11

requires the materials which should be homogenous with uniform surface roughness. It requires a medium to transmit the waves from the transducer into the bulk, additionally, and ultrasound is a point inspection technique that can be timeconsuming for large areas.

2. Radiography Testing: In radiography, the sample shadow is generated using penetrating radiation rays, such as gamma or x-ray. Images are recorded using x-rays on the film called as radiograms. The recorded radiograms have different contrast levels depending on the flaws, thickness, densities, and the nature of its chemical composition. Radiograms require access from both sides of the sample being tested because it operates based on a transmission mode.

3. Acoustic Emission Testing; Acoustic emission is the phenomenon of sound generation in materials when they are under stress. Most materials designed to withstand high-stress levels emit acoustic energy when stressed. Acoustic emission is used to non-intrusively monitor structural integrity and characterize the behavior of materials when they undergo deformation, fracture, or both. Unlike ultrasonic or radiography techniques, acoustic emission does not require external energy for inspection. Acoustic emission techniques have been used to monitor components and systems during processing, detecting and locating leaks, and testing pressurized vessels. One of the main problems with acoustic emission is that it produces large amounts of data that needs to be stored and retrieved for analysis.

12

4. Infrared Thermography Testing; Thermal testing, or infrared thermography testing, is used to measure the surface temperatures based on the infrared radiation which is emitted from the surface of an object as heat. The infrared radiation can be detected using thermal devices such as the infrared camera. The thermal image captured can be analyzed to retrieve information about the component subsurface thermal body, which is then used to understand its internal configuration. Such information helps in determining the presence or nature of defects in the body. In the recent years with the advancements in the infrared cameras and data processing codes, thermal imaging is a standalone technique for testing of several applications [20].

In the applications of VLSI testing techniques, the infrared thermography testing method has been occupied an important position in the predictive and preventive PCB failure. It has advantages of non-contact, less susceptible to electromagnetic interference, safety, and reliability. This technology can be diagnostic target instant visualization and verify thermal profile. In addition, the infrared thermography can test PCBs through its real running [21, 22].

2.6. Summary

The fundamental of faults, defects, and failure relationship was reviewed in this chapter. The main kinds of faults occurring in VLSI chips were described. The benefit of working with the simulation process for fault detection in the VLSI circuits was defined. The difference between destructive and non-destructive testing methods for PCB was presented. Importance of using infrared thermography for VLSI circuit testing was illustrated. In brief, infrared thermography has various advantages like wide temperature range, usability with smaller areas,
13

easy for understanding over other contactless methods. With progress in algorithms, infrared thermography technique can be used for a wide variety of application right from defect detection and analyzing of multi-layered PCBs to circuit design optimization.

14

Chapter 3: Fundamentals of Thermal Testing
3.1. Introduction Temperature is a physical parameter that has been used as a parametric test observable for ICs in different scenarios, while infrared thermography is a process of temperature measurement that detects the invisible infrared radiation and converts the energy from visible light into an electrical signal. The thermal imaging technique is a fully non-contact technique. Since images of components are difficult to physically access, thermal technique can scan them. Thermal testing methods can be defined as temperature measuring for the detection of structural defects in an IC [17]. This chapter will introduce how an infrared image can be generated and how it should be interpreted. The knowledge of how to interpret an infrared image will be important for the understanding of the rest of this thesis. Various fundamentals to clear the thermography idea will be discussed in this chapter. 3.2. Infrared Thermography Principle Infrared thermography is defined as an equipment which detects infrared energy emitted from an object, converts it to temperature, and displays the image of the temperature distribution. This equipment comprises the IR camera and the thermography processing unit. The camera includes the IR optics, IR sensor, unit for conversion of electrical into video signals, display, and memory card. Thermography processing unit is processor unit or a Personal Computer (PC) using special software, and they process data from the camera memory card. A main block diagram of measurement by infrared thermography equipment is shown in figure 3.1 [23]. For
15

discussion about the main characteristics of infrared thermography equipment, we can say: it can be captured as a temperature distribution on a surface, and it can be measured from a distance without contacting an object. Besides that, a temperature can be measured in a real time.

Infrared Thermography

Camera detects infrared emitted from the object.

It then transforms the amount of infrared energy into the temperature.

Displaying as infrared image.

(a)

Infrared Thermography

Atmosphere

Scanning

Condensing

Detection

Amplification

Display

Synchronizing
Object

(b)

Figure 3.1: Infrared Thermography equipment.

16

3.3. Principles of the Infrared Temperature Measurement Infrared radiation is a part of the electro-magnetic spectrum, and it is radiated from the surface of the target object. The total power of infrared radiation (Itot) consists of three main parts: the emission from the main object (Iobj), the emission from the surroundings (Erefl) and the emission from the atmosphere (Iatm). It can be expressed as the following equation. In addition, the process is shown in Figure 3.2.  =  +  +  (3.1)

Object
    ( )4 (1 -  ).   ( )4

Atmosphere
   .   ( )4 (1 -  )   .   ( )4 (1 -  )    ( )4

IR Camera

Trefl

Figure 3.2: Radiation received by the infrared camera [25].

The equation (3.1) can be rewritten at terms of three collected radiation power terms depending on the StefanÂ­Boltzmann law for a gray-body radiator as follows [24,25]:  The emission of the target object (Iobj): It can be expressed as an equation (3.2). Where (obj) is the emittance of the object, (atm) is the transmittance of the atmosphere, () is the Boltzmann constant, and (Tobj) is the temperature of a gray body.  =    .   ( )4
17

(3.2)



The reflection emission from surrounding (Irefl): It can be written by equation (3.3). Where the term (1-  ) is the reflection of the object and (Trefl) is the ambient temperature.  = (1 -  )   .   ( )4 (3.3)



The emission of the atmosphere (Iatm): This term can be calculated using equation (3.4). Where (1 - atm) is the emittance of the atmosphere, and (Tatm) is the temperature of the atmosphere.  = (1 -  )    ( )4 (3.4)



Substituting Equations (3.2)Â­ (3.4) in the main equation (3.1) to get the equation (3.5). The temperature variations of the target can be evaluated from the equation (3.6) [24, 25].
 =     ( )4 + (1 -  )    ( )4 + (1 -  ) 

(3.5)

 = 

4

 -(1- ) ( )4 +(1- )( )4   .

(3.6)

However, to calculate the correct temperature of the observed target from the IR radiation received by the camera, the properties of the target surface, the temperature of the surrounding objects, camera to the object distance, temperature and the humidity of air must be known. All these limits must be set as input data to the camera software [25].

18

3.4. Classification of Thermography Techniques According to the measurement methods and data processing, thermal imaging can be classified into two main categories depending on the sample excitation: 3.4.1. Passive Thermography: the un-excited infrared radiation (natural emission) that is used to test the sample and detect any defect in its structure. In passive thermography, the temperature difference between a defect and its surroundings is used to distinguish it. 3.4.2. Active Thermography: in active thermography an external source is used to excite the test sample thus creating temperature variation between the defective and non-defective areas within the test sample. This is accomplished in various manners as current flow and potential differences in the suspect board, then powered up and an infrared video system is used to generate an image representing the evolution of infrared radiation from the energized components on the board [25]. Active thermography is mostly used in NDT applications, where an exterior, test signal applies to the sample under test to detect any thermal contrast between areas under interest [24]. 3.5. Thermography advantages and Limitations Every experimental method has the advantages and limitations. For thermography, we could say that the pros are [18]:  Contactless technique: no physical contact, no interaction with a sample under test.  Can inspect large parts in one test.  Ability to test different materials and composites.  Ability to diagnosis components of less accessibility.
19

 Can be used in real time applications such as during production cycles.  Ease of numerical thermal modeling and easy to set up, use and keep on a production line. The limitations of thermography are arranged:  Depending on the heating source, duration of heating and location of samples.  The capture duration must be studied depending on the material being tested.  The performance of the infrared camera used has a major impact on the capture quality.  Surroundings of the test sample can effect on the image capture.  Need for a sample to heat uniformly. 3.6. Summary In this chapter, definition and principles of the infrared temperature measurement have been explained in details. Main steps of the temperature measuring by the IR camera with their mathematical relations have been provided, and then the classification of thermography techniques has been discussed. Differences between passive thermography and active thermography have been introduced. Finally, thermography advantages and limitations have been introduced.

20

Chapter 4: Introduction to Intelligent Classification Models
4.1. Introduction Classification process is used to train a classifier from a set of labeled data called training data set and to classify test set into one of the classes [26]. In this chapter, we review three main models based on intelligent classification techniques; MLP, SVM, and ANFIS. We describe their structures as well as the corresponding learning algorithms. This chapter will provide the readers with essential background information on intelligent classification techniques. We will apply the above three models in the classifying and detection the faults for the ICs, which will be discussed in the next chapters. 4.2. Artificial Neural Networks We consider a fault classification process using Artificial Neural Network (ANN). In this case, ANN has a remarkable ability to learn and get useful results from complicated and confused data that can be used to extract features. This characteristic of ANN gives an advantage over the conventional classification methods [27]. Neural networks have been used in connection with many different applications. In classification, typically a network will be asked to classify an input pattern as belonging to one of several different possible classes. There are many different types of ANNs, each with its own advantages and limitations depending on the application. Selection and implementation of the network topology should be more desirable with selecting the smallest number of neuron nodes with an appropriate learning
21

algorithm. The purpose of learning algorithm is to minimize NNs structure by clipping unnecessary neurons. The main advantages of that can be reduced the cost of runtime and physical implementation. Table 4.1 briefs the main ANN structures used for typical applications such as classification purposes, image processing, pattern recognition, associative memory, optimization, function approximation, modeling, and control tasks [28].

Table 4.1: Arrangement of ANNs based on their applications. Application Classification Pattern Recognition Associative Memory Optimization Function Approximation Modeling and Control Image Processing ANN Structure MLP, Kohonen, RBF, ART, PNN MLP, Hopfield, Kohonen, PNN Hopfield, recurrent MLP, Kohonen Hopfield, ART MLFFN, CMAC, RBF MLP, recurrent MLP, CMAC, FLN, FPN CNN, Hopfield

In this work, the network of choice for classification is a MLP with a back propagation learning algorithm for supervised classification (which includes a set of inputs and correct outputs are used to train the NN). A MLP is a feedforward artificial neural network model are characterized by the fact that the information starting from the input is only allowed to pass forward in the network to the output through a specified hidden layer, no feedback is allowed. The MLPs are popular due to their computational simplicity, flexibility, finite parameterization, stability, and smaller size for a problem as compared to other architectures [29].
22

4.2.1. The Architecture of the MLP The architecture of this class of network, besides having the input and the output layers, also has one or more intermediary layer or hidden layer [30]. The typical construction of MLP illustrated in figure 4.1. The general design considerations for a neural network includes determining the number of input and output nodes to be used, the number of hidden layers in the network and the number of hidden nodes used in each hidden layer. Typically, the number of state variables represents the number of input neurons, while the general classes of the state of the system decide the number of output neurons. To determine the number of hidden layer nodes, varying numbers of hidden layer are applied for obtaining satisfied results, initially; beginning with a small number of hidden nodes and gradually increasing this number with learning complexity occurs [28].

X1

Y1

X2

Y2

Xn Input Layer Hidden layer Output Layer

Ym

Figure 4.1: Architecture of a Multi-Layer Feed-Forward Network.

23

4.2.2. The Training of the MLP The objective of training an MLP is to produce the desired output (i.e. the target output) when a set of input is applied to the MLP. The proceeding of a neural network starts from the system activated by the input layer where the input data are weighted, and then neurons in the hidden layer perform a user chosen computation method and continue to activate all neurons to the end of this layer. Finally, the output layer determines which characteristics should be read [31, 32]. The MLPs are learned using various algorithms like Back Propagation (BP). Because of the BP network algorithm is simple, a small amount of calculation and parallel advantages. It's currently one of the most used and most mature neural network training algorithms. According to statistics, results, between 80%-90%, researchers are using the BP neural network model [22, 32]. But the limitations of this algorithm are that the traditional BP neural network model in the low learning rate, generalization ability is weak, easy to fall into local minima and the algorithm does not converge. The improved algorithms of traditional back propagation neural network conclude gradient descent Bp algorithms, Quasi-Newton Bp algorithms, and conjugate gradient Bp algorithms [32, 33]. After supervised learning of neurons is over, the trained networks are stored to be used in the algorithm. Whenever an image is taken as input to the algorithm, then simulated by the trained network and from the results; a percentage can be given to which diagnosis should be taken from the data set [22, 31].

24

4.3. Support Vector Machine SVM is a group of learning algorithms primarily used for classification tasks on complicated data such as image classifications. In SVM is a single neural network neuron but without cost function or kernel function. This method has a few advantages such as its precision, the possibility of easy implementation and the speed in the training phase and the classification process. That is the reason for its wide use and one of the popular classification algorithms [34]. Generally, an SVM finds the best separating (maximal margin) hyper-plane between the multiple classes of training samples in the feature space, which leads to maximal generalization. It uses statistical learning theory to search for the regularized hypothesis that fits the available data well without over-fitting [34, 35]. SVM uses a supervised learning approach, which means it learns to classify untrained data based on a set of labeled training data. The initial set of training data is typically identified by domain experts and is used to build a model that can be applied to any other data outside the training set [35]. Internally, SVM manipulates data set to represent them as points in a highdimensional space and then finds a hyper-plane (also called "the model") that optimally separates the two categories or more. As the example in figure 4.2.a, the data set is represented as points in three-dimensional space and the SVM algorithm finds the linear separator that divides the plot into two parts corresponding to two different classes. On the other hand, it is said that one separator is better than another if it generalizes better, i.e. shows better performance on dataset outside of the training set. It turns out that the generalization quality of the plane is related to the distance between the plane and the data points that lay on the boundary of the two data classes. These data points are called "support vectors",
25

and the SVM algorithm determines the plane that is as far from all support vectors as possible. In other words, SVM finds the separator with a maximum margin and is often called a "maximum margin classifier" as shown in figure 4.2.b [35].

Feature map

Separating hyper plane Input space Feature space

a. Principle of SVM in three- dimensions.

Support Vectors Sperating hyper plane Support Vectors

Margin

b. Both hyper planes separate correctly. The optimal separating hyper plane on the left- hand side has a larger margin.

Figure 4.2: Schematic diagram of a SVM classification. 4.4. Adaptive Neuron-Fuzzy Inference System Adaptive Neuron-Fuzzy Inference System (ANFIS) is based on fuzzy logic modeling and uses the artificial neural network as the learning algorithm. Therefore, ANFIS is combined neural networks and fuzzy logic into a hybrid system, so that both can overcome their individual drawbacks as well as a benefit from each other's advantages [37]. The ANFIS has been proven
26

as a classifier with high classification accuracy and fast turning speed for data clustering. ANFIS uses a hybrid algorithm that consists of a combination between BP and least-square estimation techniques. The techniques are implemented in an ANN as a learning algorithm that gives very fast convergence and more accurate in ANFIS target [36]. From the topology point of view as shown in figure 4.3, and the basic flow diagram of computations in ANFIS is illustrated in [38]. The ANFIS consists of five layers. The role of each layer is briefly presented as follows:

x

X1

e1

N1

G1

O
X2 e2 N2 G2

y

Y1

e3

N3

G3

Y2

e4

N4

G4

Fuzzification

"IF-Then"

Normalization

Defuzzification

Figure 4.3: A basic ANFIS model with two inputs data and two member functions.



In layer 1 (Fuzzification stage): the fuzzification algorithm is used to calculate the magnitude of the membership degree using the member function curve. The curve is performed by using the following equations. xi (x) =
1 1+(
x-ci 2bi ) ai

(4.1)

yi (y) =

1
y-ci 2bi ) 1+( ai

(4.2)

27

Where, xi (x) and yi(y) are fuzzied values for each input data (x,y), where as ai, bi and ci are member function parameters.  In layer 2 ("IF-THEN" stage): this stage receives the labeled data from the fuzzification stage and then evaluate the rules based on equations (4.3) to (4.6). 1 = 1 () Ã 1 () 2 = 1 () Ã 2 () 3 = 2 () Ã 1 () 4 = 2 () Ã 2 () Where: e1, e2, e3 and e4 are real values for each `IF-Then' rule.  In layer 3 (Normalization stage): Next, the output signal from the stage of `IFTHEN' rule will be an input signal to the normalization stage. In this stage, every signal is divided by the total of gaining signal as the following equation.  =   Where: eT = e1+e2+e3+e4.  In layer 4 (Defuzzification stage); in this stage, the set data receive from pervious stage is defuzzified to give a crisp number. The membership functions of the output signal as following equation.  =  (  +   +  ) i = 1,2,3,4 (4.8)


(4.3) (4.4) (4.5) (4.6)



 = 1,2,3,4

(4.7)

Where pi, qi, and ri: The Member function parameters.
28



In layer 5 (Neuron addition stage); The last process in the ANFIS operation is called a neuron addition in which all defuzzification signals, Gi are added together as shown below:

 =  

 = 1,2,3,4

(4.9)

Finally, the dataset is classified as training data and testing data in ANFIS's learning process. The total error can be reduced by adjusting the variable membership function and epoch parameters. By selecting, an optimal epoch and several member functions, the performance will be improved accordingly [36]. 4.5. Summary In this chapter, we reviewed three types of the classifier models for detection and the faulty element based on the IR image, which are; MLP, SVM, and ANFIS. These models are used to reduce the amount of training material needed to train a method by selecting the most informative features. In the first section: we presented the concept of MLP model architecture and explained how to the improved the BP algorithms and to determine the optimal training algorithm and the number of hidden layers for MLP. In the second section: the main concepts of SVM model have been briefly introduced. We explained the aim of using the SVM supervised method, by transforming the training data to multiple classes such that the new data set became more separable that of the original training data.

29

In Final section: we considered hybrid intelligent classification systems as an ANFIS. We reviewed an ANFIS model. This model is represented by a neural network with five layers: fuzzification, fuzzy rule, normalization, defuzzification, and neuron stage. In the ANFIS model, the combination between the least-squares estimator with the gradient descent method is used as a hybrid learning method.

30

Chapter 5: Proposed Method for Fault Detection and Classification of the PCBs
5.1. Introduction In this chapter, we propose a method for fault detection and classification of the PCBs. Main steps of the proposed algorithm in this chapter includes: 1. PCB's finite-element model development: A 3-dimensional PCB finite-element model to estimate the temperature profile of the stacked ICs is developed in this chapter. Procedures like entire thermal simulation, including, geometry generation, applied boundary conditions, solving governing equations, meshing, and post-processing are performed using finite-element method. Then the thermal profile of the developed model is verified with a thermal profile of Atmel standard packages. 2. Collecting image samples. 3. Image Enhancement: Proposing an image processing technique for the captured samples includes denoising, segmentation ROI and using the histogram algorithm to extract the image features is used in this chapter. 4. ROI feature extraction: this chapter presents two sets of feature extraction from ROI; first order histogram features and GLCM features.

31

5. Minimize number of features: PCA is applied in our work to minimize the feature's extractions into a minimum uncorrelated variation. 6. Fault classification and detection: To train the software for classification; MLP, SVM, and ANFIS techniques are used to classify the thermal image of ICs into two classes; normal condition and fault condition. 7. We use the capabilities of MATLAB to achieve the above steps. Figure 5.1 shows an overview of the method undertaken in this work.
PCB's finite element model development

Collecting image samples

Image enhancement: 1- Image Denoising 2- Image segmentation

ROI Features Extraction

Minimize number of features by PCA approach

Fault classification and detection

Results display: Normal /Detective ICs

Figure 5.1: Main steps of the proposed algorithm.

32

5.2. Proposed PCB Modeling For the optimization of the infrared (IR) thermography, performance results are necessary to describe relevant thermo physical processes running on the object under study. In simple cases, it is possible to find an analytical solution of the problem, but in the real 3D complex objects, it is necessary to use the numerical solution of the Fourier's differential equation. One of the suitable methods is based on finite element models, FEMs. FEM is a numerical computation technique commonly used in the engineering design process [39]. The basic concept of this method is to divide the area of solution into smaller parts called finite-elements, connected at nodal points (mesh generation) and solution processes. The solution processes are known boundary conditions along with characteristic differential equations which depend on the physics of each layer [39]. Building a detailed finite-element model of the board-level package is difficult due to the complicated structure involved, such as the layer structure, through-holes, and signal wires. Obtaining the optimal size of the finite element model is necessary to build an equivalent PCB model with equivalent material properties [40]. Because of our work relates to simulate the temperature distribution of the PCB and study how the layouts of chip packages affect the temperature environment, the details of copper wire and other electronic circuits of the system are ignored. Therefore, the simplified model can reduce the time computing as well. systematic, the simulation process is paved with the following essential sections.

33

5.2.1. Geometry Model Generation The geometric model for the whole PCB with three main ICs is created. A PCB size is 70Ã55 mm2 with 4 mm thickness. The geometry model and parameter sizes are shown in table 5.1 and figure 5.2[41]. Table 5.1: Geometry PCB Dimension. . Dimensions (xÃyÃz) mm3 IC1 Layout 1 Layout 2 Layout 3 Layout 4 Layout 5
*

Component

IC2 2Ã2Ã0.3 6Ã6Ã1 3Ã3Ã0.2

IC3 1Ã1Ã0.15 2.4Ã2.4Ã1 1.5Ã1.5Ã0.1

PCB Air Die Mold package Bond wires and lead frame

70Ã55Ã4 70Ã55Ã1.7 -

2Ã2Ã0.1 10Ã3.427Ã1 (2.358+1.949Ã16) *

: (Bond wire +lead frame Ã No. of pins).

The first IC, IC1, is based on the simplified Plastic Dual Inline Package (PDIP) model. This is a rectangular shape package with two parallel rows of electrical connecting pins coming out of the two sides of the package. The basic geometric shape is imported from COMSOL library [42, 43]. Other ICs packages; IC2 and IC3 simulate a simplified geometry for a Quad Flat No lead (QFN). These systems have the same layout but different dimensions. This package is a surface mounted device and attached directly on the surface of the PCB [41]. In addition, it is assumed that there are no tiny differences in the heat conduction along the leads and pins of the ICs with the main body of the package. The 3D PCB Geometry and layout configuration are shown in figures 5.2 and 5.3.

34

IC 3 IC1 IC 2

Figure 5.2: 3D layout for the PCB.

Figure 5.3: Top and side views of the PCB. 5.2.2. Physical Materials and Thermal Parameters The next analysis process of FEM is defining layer types and material properties in the PCB modeling, depending on the Atmel corporation data sheet [39,44,45]. Basically, the 3D PCB model comprises five layers:  First Physical Layer: Air convection layer applies to the top surface of the PCB.  Second physical layer: The die element acts the silicon IC layer.

35

 Third physical layer: Bond wires and lead frame layer, with copper material layer.  Fourth physical layer: Mold covers contact's material, bounding wires and dies element.  Fifth physical layer: FR4 material (i.e. fiberglass in an epoxy resin) applies to make the PCB board.

Table 5.2: Thermal properties for the PCB. Layer number 1 2 3 4 5 Definition Air PCB (FR4) Die (Silicon) Bond wires and lead frame Mold Thermal Conductivity (W/m.K) 0.03 0.3 163 400 0.163 Thermal Capacity (kJ/kgÃK) 1 1369 703 385 900

The basic method to move heat away from the source can be explained using the following heat transfer equation:  = (-) =  Ã ( -  ) where Q: heat source (W/m2). Ta and Tb: temperatures of PCB surface and the ambient temperature (). k: thermal conductivity (W/m. K). h: heat transfer coefficient (W/m2.K), h = (R and A; is surface area of the package (m2).
1
th ÃA

(5.1)

), where Rth; is thermal resistance(/),

36

The main properties of the layer materials listed in table 5.2, while the boundary conditions are: 1. The energy equilibrium through the upper side of the body is described by the equation (5.1). 2. Heat energy is exchanged between PCB and surroundings only through the upper side of the PCB. 3. The external air temperature is equal to Tb. 4. Assume air is covered the bottom space of IC1 between mold and PCB surface. 5. For the layers of ICs, heat flux is according to equation (5.1). The power dissipation of ICs groups is considered as a heat source with ambient temperature. 5.2.3. Mesh Generation Once the geometry is generated, it should be meshed into finite elements. To get a solution, FEM first discretizes the body being analyzed in a process meshing. This refers to dividing the known domain into many finite elements with points, or nodes, at each of its vertices. In each node, the physical equilibrium equation is defined and on the object's boundary, the boundary conditions are defined. In this way, the physical problem described by the partial differential equation is broken down into a linear system of equations [39]. The complex geometries always need more elements to represent than simple ones, and so, take a longer time to solve. It means the minimum number of elements is desirable by which an acceptable accuracy is achievable [39,41,44]. The final 3D revision of the meshed geometry of the proposed PCB can be seen in figure 5.4. This model has approximately 129599 tetrahedral and 25177 triangular elements. In addition, the meshing limits used in the PCB are listed in table 5.3.

37

a. Meshed PCB geometry with air layer.

b. Meshed PCB geometry without air layer.

Figure 5.4: Three dimension views of the meshed PCB geometry. Table 5.3: Mesh limits of detailed model. Number of elements Number of boundary elements Number of edge elements Number of Tetrahedral elements Number of triangular elements Meshing volume 129599 25177 1877 129599 25177 21940.0 mm3

5.2.4. Simulation Results and Verification The performance of the simulation model is evaluated and verified by comparing its thermal performance to thermal profile of Atmel standard packages.

38

Thermal performance of a package is measured by the ability to dissipate the power required by the ICs into its surroundings. The electric power drawn by the PCB generates heat on the surface of the IC. This heat is conducted through the die to the surface and transferred to the surrounding air by convection. Each heat transfer step has a corresponding thermal resistance coefficient to the heat flow. Commonly used coefficients are Rth(jA) (junction to ambient air). The thermal performance can be characterized by equation (5.1). Based on Atmel data sheet [46], the average thermal resistance for IC1 (PDIP), and IC2 (QFN) model are 55 and 23 /W, respectively. With the same information for physical properties such as thermal conductivities and thermal capacities of the different layers of ICs as given in tables 5.1 and 5.2, the heat transfer coefficient of IC1 and IC2 can be got. Then, the power dissipation Pdissp curve versus temperature changes (Ta-Tb) provides thermal behavior for the developed model. Figure 5.5.a, shows the simulation results for power dissipation changed between 0.6W Â­ 1.14W to get temperature range 34.055 Â­ 43.264, respectively, for IC1 model (PDIP). Figure 5.5.b, shows power dissipation varied between 1W Â­ 2.337W to get temperature range 29.734 Â­ 43.325 for IC2 (QFN) model. For IC3 (QFN) model, assuming the power dissipations swept between 0.6W Â­ 1.5W to obtain the temperatures range between 30.331 Â­ 43.699 as illustrates in figure 5.5.c. Results for power dissipation versus temperature changes agreement with the thermal profile of Atmel packages that will present in experimental results. Consequently, simulation results from the thermal model provided good samples on the total temperature distribution that will use well in the classification approach.

39

44 42 40

Temperature (C)

38 36 34 32 30

a. Temperature variations vs. power dissipation for IC1 (PDIP) model.

44 42 40 38 36 34 32 30 28 26 24 22 20

Temperature (C)

b. Temperature variations vs. power dissipation for IC2 (QFN) model.

50 48 46 44 42 40 38 36 34 32 30 28 26 24 22 20

Te mperature (C)

0.6 0.680.760.840.92 1 1.081.161.241.32 1.4 1.48

c. Temperature variations vs. power dissipation for IC2 (QFN) model. Figure 5.5: Thermal profile for the developed model.

1.00 1.08 1.15 1.23 1.31 1.39 1.46 1.54 1.62 1.69 1.77 1.85 1.92 2.00 2.08 2.16 2.23

0.600 0.632 0.664 0.696 0.728 0.760 0.792 0.824 0.856 0.888 0.920 0.952 0.984 1.016 1.048 1.080 1.112

Power Dissipation (Watt)

Power Dissipation (Watt)

Power Dissipation (Watt)

40

We have captured a series of about 656 thermal images, with a 260Ã420 image size under (.png) format, from the surface of the PCB. These samples at various thermal loading conditions depend on the thermal characteristics of ICs, as shown in figure 5.6.

a. IC1 with Fault State.

b. IC1 with Normal State.

c. IC2 with Fault State.

d. IC2 with Normal State.

e. IC3 with Fault State.

f. IC3 with Normal State.

Figure 5.6: The simulation results for temperature distribution on the PCB's surface.

41

5.3. IR Image Enhancement The basic goal of image enhancement is to process the image so we can view and assess the visual information with greater clarity. Image enhancement, therefore, is rather subjective because it depends on the information the user is hoping to extract from the image. The removal of noise and the segmentation of images are main enhancement techniques [47]. 5.3.1. Image Denoising Processing the acquisition noise should be included to improve the visualization of defects. Therefore, noise filtering would be an important pre-processing technique to improve the contrast and achieve better estimation of the size of the defects [48]. Thermal image has the characteristics of high noise and low contrast. Noise sources include infrared camera system, electronic sensor, turbulence in the environments, etc. Main types of noise include Gaussian noise, sources shot noise, salt and pepper noise, and thermal noise. These noises result in image degradation which directly affect the accuracy of image segmentation and feature extraction. Therefore, denoising process is aimed to find a nondegraded image from the degraded or noisy image [49,50]. Two main types of noise, applied in this work to create different viewing of unclearly for the IR images, are:  Gaussian noise: This noise appears in IR images from imaging equipment such as sensors, electronic circuit's noise, or amplifiers noise because of poor lighting and temperature rising.  Salt and pepper noise: This noise called spike or impulse noise, because of this noise, dark and bright points or dots appear in the image. The heated faulty elements or dust pieces in the image equipment can cause this noise.
42

Test images are artificially corrupted by Gaussian noise plus salt and pepper noise with noise power density are 1%, and 0.1%, respectively, as described in [50]. To drop the confused noise, many algorithms have suggested based on linear and nonlinear filters and each algorithm has its advantages and limitations. We investigate on using seven common types such; the median filter, Wiener filter, Gaussian filter, the wavelet threshold method, simple digital filter and, block matching three-dimensional filter (BM3D) [51,52]. Simulation results show that the method based on above filters could work with noise and could preserve the details of the images as shown in Figure 5.7. Comparative performance of using the different filters is measured by two quantitative factors: signal-to-noise ratio (SNR) as in Equation (5.3) [53], and peak signal-to-noise ratio (PSNR) as Equation (5.2) [54]. These two factors use to decide which filter is suitable for using with the IR image as in table 5.4. In this comparison, the median filter is very effective for salt and pepper noise removing. Notably, experimental results presented, insists us to conclude that BM3D with median filters performed well results (as in figure 5.7.h). BM3D is the best choice of removing the Gaussian noise [52]. The Gaussian noise has been largely removed, but at the expense of a slight degradation in image quality.  = 10 10 (  ) where: MSE: is the mean square error between the original image and a distorted image.  = 10 10 (
  2552

(5.2)

)

(5.3)

where: Âµ: The mean image pixel intensity values and, : The standard deviation of the image pixel values.

43

a. The noisy Image (Gaussian + salt and pepper).

b. Using Median filter.

c. Using Gaussian filter.

d. Using Simple Digital filter.

e. Using Wavelet threshold filter (Soft type).

f. Using Wavelet threshold filter (Hard type).

g. Using Wiener filter.

h. Using BM3D with Median filter.

Figure 5.7: Comparison results for the image filtering methods.
44

Table 5.4: Comparison of SNR and PSNR values of different existing filters. Filter Type Simple Digital filter Gaussian filter Wiener filter Wavelet threshold filter (hard type) Wavelet threshold filter (Soft type) Median filter BM3D filter BM3D+Median filter Parameter SNR( dB) PSNR(dB) 4.628 20.590 -0.392 15.569 4.628 20.590 3.630 19.592 4.477 20.439 9.370 25.332 5.868 21.830 12.982 28.944

5.3.2. Image Segmentation for ROI The aim of image segmentation is separating a digital image into independent partitions or regions that are more significant to recognize the objects in an image. Segmentation occupies a very important role in image processing because so often the vital first step which must be successfully taken before next tasks such as feature extraction, and classification [55,56]. Through this stage, a thermal image of PCB is converted into a grayscale image, which carries the intensity information of the image. In the grayscale image, the faulty region shows more brightness than the normally repeated region in the equipment. The segmentation methods can be divided into threshold segmentation methods, region segmentation methods, edge detection methods and clustering segmentation methods. The result of image segmentation method depends on many factors such as intensity, region continuity, computation complexity, noise resistance, and accuracy [56]. One of the most widely used and convenient method is threshold segmentation technology [4]. This technology is based on a threshold value to turn a grayscale image into a

45

binary image. The key of this technology is to select the threshold value. The reason for using this technology on ICs testing always has temperatures higher than the environment. By using a threshold method, the image of the main object can be separated from its background. However, threshold segmentation technology is useful and a common method for detecting defects in the thermal images [58]. In the threshold process; the heating abnormal in the PCB element's image can be detected by setting a certain threshold filtering [4], if the original image is (, ), the threshold image (, ) is defined as follows equation. 1, (, ) >  (, ) = { 0, (, ) <  where:  is the threshold temperature. Various algorithms are widely interested in the threshold thermal image; our work considers four main thresholding methods; iterative thresholding method, Fuzzy-C mean method, Sauvola method, and Otsu thresholding method. To evaluate the practical performance of the proposed algorithm, the analysis of above segmentation methods and then compare the results of each algorithm is done. The result of the simulation showed that it greatly enhanced. Otsu thresholding works well with acceptable results for automated thresholding of the gray image, as shown in figure 5.8. In addition, Otsu thresholding method is used for the noisy image. The principle of Otsu method requires using the function gray-thresh which chooses the optimal threshold to minimize the interclass variance of the black and white pixels as explain in [57,59,60]. The cropping approach is used at this stage to find the ROI. This process leads to less complexity in the image processing and time because the selected region will minimize. The IC
46

(5.4)

under test region is cropped by using the enclosing rectangle shape to enclose the segmented pattern in a compact window, and convert the selected area into another image. The width of ROI is adjusted accordingly such that all the thermal effect on the chip is preserved. Thus making the segmentation process easier, classification accuracy is higher and production cost, in long run, is reduced [61]. Figure 5.9 illustrates an example, with the using a cropping approach in the original image and the result of the cropped region of interest.
noisy IC

IC

a. Noisy Image for IC1 b. Denosing Image for IC1. (Salt and pepper + Gaussian) noise. Global Thresholding - Otsu's Method Global Thresholding - Iterative Method

sauvola Image c. Iterative method.

d. Otsu's method. Fuzzy-C mean method

e. Sauvola method.

f. Fuzzy-C means method.

Figure 5.8: Comparison of segmentation results for different methods.

47

b. Cropped image (size=101*101).

a. Original image (size=260Ã420).

Figure 5.9: Image cropping approach.

5.4. Feature Extraction Feature extraction is done after the image enhancement stage. The main goal of feature extraction is to get the most relevant information from the original data (i.e. image) and represent that information in a lower dimensional space. For large size of data set, the data represent into a minimum form set of features or feature set. This process is called feature extraction. A good feature set has discriminating information, which can distinguish one object from other objects. It must be as robust as possible to avoid generating different feature codes for the objects in the same class [62,63]. Many feature extraction methods for image processing have been developed. Some of the most used statistical properties of the gray level of the pixels comprise an image. Normally, in images, there is a periodic occurrence of certain gray levels. The spatial distribution of gray levels is calculated. Some of the statistical representation is the histogram statistical analyses
48

[62]. In our approach, image histogram features are extracted by converting an image to gray color space and applying the histogram statistical analysis method. 5.4.1. Statistical Features Based First Order Histogram Statistical Analysis Histograms are widely used in the first step of the thermal image process. The histograms can be useful in gathering information about an image and in determining which pixel values are important in an image. Formally, the histogram is the grayscale or a color histogram of an image is the frequency of existence of pixels of the same intensity in the ROI. It can be visualized as if each pixel is placed in a bin corresponding to the color intensity of that pixel as in figure 5.10 then all the pixels in each bin are added up and displayed [63,64].

15

x 10

4

10

5

0

0

50

100

150

200

250

300

18000 16000 14000 12000 10000 8000 6000 4000 2000 0 0 50 100 150 200 250

a. Original Image

b. Histogram Image

Figure 5.10: Image histogram representation for both color and gray thermal image.

49

Mathematically, the histogram is the distribution of the probability function () of the intensity levels (i) in the histogram bins, and defines using the following equation. () =
No.of pixels with gray level (i) Total number of pixels in ROI

(5.5)

The histogram is used for defining first order statistical features of the image, such as mean value, Standard deviation, skewness, and kurtosis. These features build a feature vector to retrieve similar images from the database. The definitions of these parameters are given below [64,65]. 1. Mean value: It measures the average value of the intensity values. If mean value is high, then it means that the image is bright and if it is low, then the image is dark. The mean of the probability distribution can be calculated by the equation (5.6). Where (L) represents the total number of bins in histogram figure.
 =  =1 . ()

(5.6)

2. Standard deviation (std): the standard deviation is the second order moment, and it shows the contrast of gray level intensities. The low value of the standard deviation shows a low contrast and the high value shows the high contrast of the image. This can be computed from its mean value as in equation (5.7).

2  =  =1( - ) . ()

(5.7)

3. Skewness: measure the inequality of the intensity level distribution about the mean. The value will be positive or negative of the skewness. Negative value shows that the large number of intensity values is on the right side of the mean. Positive value shows
50

that many intensity values are on the left side of the mean. Zero value indicates that distribute the intensity values is relatively equal on both sides of the mean. Skewness can be defined in (5.8).
1  =1(  3

 =

- )3 . ()

(5.8)

4. Kurtosis: the fourth order moment is kurtosis. It is used to measure the flatness of the thermal distribution for each region. Lower kurtosis shows that the temperature distribution within the region is homogenous. The higher value of the kurtosis shows that the peak of the distribution is sharp, and the tail is longer and fatter. Kurtosis can be defined as an equation (5.9).
1

4  = 4  =1( - ) . ()

(5.9)

5.4.2. Statistical features based on Grey Level Co-occurrence Matrix A second-order histogram is an array that is formed based on the probabilities that pairs of pixels separated by a certain distance and a direction, which have co-occurring gray levels. This array, or second-order histogram, is also known as the gray level co-occurrence matrix (GLCM) [65,66]. The basic difference between above approach and GLCM is that first order statistics estimate only the properties of individual pixel values while second order statistics estimates spatial relationships between pixel gray levels of the image happening positions relative to each other. Some of the features that can be extracted from the GLCM are energy, image contrast, homogeneity, correlation, and entropy:

51

1. Energy: The energy feature measures the uniformity of the intensity level distribution. If a value is high, then the distribution is little intensity level. Energy returns the sum squared of the GLCM function as in Equation (5.10).
2  =  , (, )

(5.10)

where: x: is the gray level of the reference pixel. y: is the gray level of neighboring pixel. P(x,y): is the normalized GLCM.

2. Contrast: Contrast feature measures the spatial frequency of an image. It is the difference between the highest and the lowest values of a contiguous set of pixels as in Equation (5.11).
2 2  =  ,| - | . (, )

(5.11)

3. Homogeneity: The closeness of gray levels in the spatial distribution over the image. The homogeneity calculated through the following equation.
 =  , 1+|-|
(,)2

(5.12)

4. Correlation: Correlation that brings out how correlated a reference pixel to its neighbor over an image. The correlation representation for row and column in the GLCM matrix is illustrated in Equation (5.13).
C =  ,
(,).(,)-(  )  

(5.13)

where: Âµ is the mean value of p and,  : Standard deviations of p.
52

5. Entropy: it measures the disorder or complexity of an image. If the value of entropy is high, then the distribution has more intensity levels in the image. A simple image has low entropy while a complex image has high entropy. It can be defined following equation.
Entrpy = -  , (, ) Ã ((, )

(5.14)

Figure 5.11 shows feature changing plots versus temperature increase of ICs under test. The analysis show that energy; homogeneity, skew, and Kurtosis decrease with increasing ROI temperature, since contrast, entropy, mean, and standard deviation show a consistent increasing with increasing ROI temperature.
30 IC2 25 20

80

IC3
IC1

70
60 50 40 30 20 10 0
IC2 IC3 IC1

Mean

10 5 0

STD
29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

15

Temperature (C)

a. Mean Feature vs. Temperature.
18 16 14 IC2 IC3

b. Standard Deviation Feature vs. Temperature.
300
IC2 IC3 IC1

250
200

Skewness

12 10 8 6 4 2 0

IC1

Kurtosis

150 100 50 0
29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

Temperature (C)

c. Skewness Feature vs. Temperature.
53

d. Kurtosis Feature vs. Temperature.

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

Temperature (C)

Temperature (C)

1.2 1

1.4
IC2

1.2 1

IC1 IC3

Contrast
IC2 IC3 IC1

0.8

Energy

0.8 0.6

0.6 0.4 0.2 0

0.4
0.2 0

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

Temperature (C)

e. Energy Feature vs. Temperature.

1.000 0.995

Homogeneity

0.985 0.980 0.975 0.970 IC2 IC3 IC1

Correlation

0.990

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Temperature (C)

Temperature (C)

g. Homogeneity Feature vs. Temperature.

h. Correlation Feature vs. Temperature.

0.40 0.35 0.30 0.25

IC2 IC3 IC1

Entropy

0.20 0.15 0.10

0.05
0.00

f. Entropy Feature vs. Temperature. Figure 5.11: Features variations plots versus the temperature.

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

Temperature (C)

54

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6

29.7 30.4 31.1 31.8 32.5 33.2 33.8 34.5 35.3 35.8 36.5 37.2 37.8 38.5 39.2 39.9 40.6 41.3 41.9 42.6 43.3 43.9 44.6
Temperature (C)

f. Contrast Feature vs. Temperature.

IC2 IC3 IC1

5.5. Principle Analysis Components Principle Component Analysis (PCA) is a statistical analysis tool used for multidimensional data set and represent them in a way to highlight the similarities and differences in the original data set. The pattern matching of the data becomes difficult when the data dimension is high. In such situations, PCA comes in handy for analyzing and graphically representing of such data. Once the patterns in the data are found, the data is compressed by reducing the data dimensions with little loss of information. This dimension reduction can be achieved by transforming the original data set into a series of uncorrelated principal components. Principle algorithm for PCA is shown in figure 5.12. In this flowchart, the first step is to collect the data set to be analyzed. Mean value of the data set is calculated. The calculated mean value is subtracted from the data set to normalize the data. Covariance matrix is calculated for the normalized data matrix. From the covariance matrix, we calculate the eigen-values and the corresponding eigenvectors. The eigen-vectors are arranged in the descending order of size in the eigenvector matrix. Then the eigen-vector with the highest eigen-value is considered as the principal component of the data set. More than 95% of the variance is contained in the first three to five components. Main contributions of PCA algorithm are [18, 67]: 1. Extract the most important information from the data table. 2. Compress the size of the data set by keeping only this important information. 3. Simplify the description of the data set. In this work, to ease the processing of the IR image sequences and to increase the consistency of our results, the added action conducted using the PCA approach due to the

55

number of features for each IR image sample is nine features. So that, by applied PCA analysis, minimizing these features into just three uncorrelated variables is achieved.

Calculate the mean of data and normalize them

Calculate covariance matrix

Extract the Eigen-values and corresponding eigenvectors

Find the highest Eigen-values and eigenvectors

Find Principal Components

Rebuild data

Figure 5.12: Diagram flow of the PCA algorithm.

56

5.6. Classification Modules In this section, the final step of the process, shown in figure 5.1, is explained. After gathering the main features of the IR images, the next key step is fault classification and detection. We can feed these features into a classifier to discriminate the thermal images into different categories. The whole process of the intelligent classification can learn the relationship between a data set of training database and a dataset of the testing database. It trains and creates a fitting network of data set and gives the predicted output, as illustrated in block diagram of figure 5.13 [28]. In this diagram, the intelligent classification technique is used to learn a model called a classifier. The data set is divided into two groups; training set and testing set. In the training phase, the training set is fed to the classifier for labeled data set into one of the classes depending on the target output. In the testing phase, the test samples are verified depending on the target output [68]. Once essential features have been identified, the classification of a fault condition is straightforward performed. The three main intelligent classifier models include MLP, SVM and the fuzzy set theory methods are used in this work for fault classification.

Data base (Training images)

Target Output

Input Samples

Classifier

Output results

Data base (Testing images)

Figure 5.13: Block diagram for the General Intelligent Classification steps. (testing images)
57

5.7. Results and Discussion For simulation analysis, the input data (i.e. the extracted features) are got and arranged in Microsoft offices such as excel, and then entered to the intelligent classification program. The classifier models are trained for functional fault classification. Once trained, the network performance validates using testing data different from the training data. Depending on behaving the real PCB in the experimental part, and characteristics of ICs at [41], the classifier model decides a separate decision for each IC into two main conditions:  Fault-free condition: happens when the maximum temperature of IC's surface is lower than the threshold value (i.e.  < 40).  Fault condition: happens at the maximum temperature of IC's surface equal or higher than the threshold value (i.e.   40).

Furthermore, the whole data set is divided randomly into two different sets: training data set and testing data set. Among 656 samples, 459 samples (i.e. 70% of data set) are selected as a training data set and the remaining 197 samples (i.e. 30% of data set) are selected as a testing data set. The conditions of the PCB for the normal state and the defective state of the entire data set can be illustrated as in table 5.5.

58

Table 5.5: Distribution of conditions for entire samples. UUT IC1 IC2 IC3 Total The normal condition 119 141 152 412 The defect condition 82 226 77 244

The accuracy percentage of classification is defined as the percentage of total samples classified correctly to the total number of samples in the dataset as in Equation (5.15).
Accuracy of classification =
Total no.of samples classified correct Total no. of samples in the data set

 100%

(5.15)

5.7.1. Classification Results based on MLP To decide that the chosen MPL network layout is good enough, several layouts are tested with the number of hidden neurons ranging from 4 to 40 in steps of 4. The configuration that produces the minimum performance factors comprises one hidden layer with 12 neurons. The input layer has 4 neurons, which takes the value of the input data while the output layer has 4 neurons corresponding to the UUT condition. The output layer transfer function is selected to be a Soft max transfer function suitable for the binary output as shown in figure 5.14.

59

Figure 5.14: The proposed neural network architecture layout. As mentioned in the chapter 4, there are several training algorithms, which can apply to training MLP. Here, the performance comparison between three main methods of training algorithms is done. These methods with eight training functions have been implemented by MATLAB program: 1. Gradient Descent algorithms: they have three functions:  Gradient descent backpropagation (i.e. Traingd).  Gradient descent with momentum backpropagation (i.e. Traingdm).  Resilient backpropagation (i.e. Trainrp). 2. Conjugate Gradient algorithms: they have three functions:  Scaled conjugate gradient backpropagation (i.e. Trainscg)  Fletcher-Powell conjugate gradient backpropagation (i.e. Traincgf)  Polak-Ribiere conjugates gradient backpropagation (i.e. Traincgp). 3. Quasi-Newton algorithms: they have two functions:  BFGS quasi-Newton backpropagation (i.e. Trainbfg).  Levenberg-Marquardt backpropagation (i.e. Trainlm).

60

Table 5.6: Samples for the comparison results of MLP Training Functions.
Training Function Hidden Epoch layer 4 Traingd 8 12 Gradient Descent 4 Traingdm 8 12 Trainrp 4 8 12 4 8 12 4 Conjugate Gradient Traincgp 8 12 4 Traincgf 8 12 4 Trainbfg Quasi Newton Trainlm 8 12 4 8 12 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 2 775 440 1000 1000 521 2 624 1000 403 246 Best validation MSE 0.53059 0.54126 0.52677 0.53277 0.54636 0.52745 0.33175 0.33201 0.331 0.33043 0.32915 0.32986 0.40595 0.64485 0.40234 0.40217 0.40088 0.40487 0.40606 0.64868 0.40814 0.0014375 0.0006971 1.0296e-08 Average Training Classification 47.93 % 62.52 % 49.01 % 49.23 % 61.65 % 49.01 % 98.69 % 98.25 % 98.69 % 99.34 % 99.34 % 98.91 % 86.71 % 99.34 % 84.26 % 88.01 % 88.23 % 87.14 % 87.36 % 63.18 % 86.71 % 99.34 % 99.78 % 100 % Average Testing Classification 50.76 % 58.37 % 48.73 % 50.25 % 60.40 % 48.73 % 97.46 % 97.96 % 97.46 % 97.46 % 97.46 % 96.44 % 83.75 % 60.91 % 86.92 % 83.24 % 82.74 % 84.26 % 85.27% 61.92 % 85.27 % 97.46 % 97.96 % 96.44 %

Algorithm

Trainscg

From the results in table 5.6, the convergence speed of Trainlm is higher than other training functions. While the average of the classification percentage between Trainrp, Trainscg and Trainlm functions are near. On the other hands, comparing on epochs and MSE parameter,
61

Trainlm outperformed other training function. The best training algorithm is found to be the Trainlm which converges to the minimum performance factor in the minimum possible time. The training, performance curve, figure 5.15, shows the fast convergence of the selected training algorithm where the performance response reduced to 1.0296e-8 after 246 epochs on average.

Figure 5.15: The MLP network performance response.

To confirm the developed neural network, a set of input data not used within the training stage, is given to the neural network. Neural network output is then compared with the desired output to confirm the neural network. Data set has IR image data covers the full operating range for both healthy and faulty operation modes. Output from the neural network is the ICs condition, according to the table 5.7. If all the UUTs are working with healthy mode the binary outputs would give a value of (0100)b. Under any fault operation, the corresponding output will classify the fault into three main classes: (1000)b= IC1 is a fault, (0010)b=IC2 is a fault, and (0001)b =IC3 is a fault. The comparison between the neural network outputs and the

62

desired target outputs shows the correct prediction of the ICs condition. The summarized proposal MLP network setting parameters in this work can be shown in table 5.8.

Table 5.7 Samples for testing and output results of the MLP Model.
Table 5.7: Samples for testing and output results of the MLP Model .
Input Features extracted (Before minimization by PCA) PCA Correlated fatures
Sample number

(to MLP) IC index

Output Fault status (from MLP)

Homogeneity

correlation

Feature 2

Feature 3

Feature 4

Feature1

Contrast

Kurtosis

Entropy

Energy

Mean

Skew

Std

1 ... 189 ...

0.6598
... 0.8732 ... 0.5674

0.9882 ... 0.9844 ... 0.9898

0.9572 ... 0.8005 ... 0.8709

0.5428 ... 0.9020 ... 0.9015

3.7598 ... 25.4473 ... 15.7644

30.4455 ... 72.3419 ... 59.3511

8.2192 ... 3.1593 ... 4.0140

67.5562 ... 9.9814 ... 16.1122

0.0899 ... 0.3325 ... 0.2488 Â· Â· 0.1407 Â· Â· 0.2061

3.509996 ... -3.71374 ... -1.1600

2.617815 ... 0.248826 ... -1.0041

-0.63745 ... 0.861656 ... 0.1644

1 ... 1 ... 2 ... 3 ... 3

(0100) b Pass PCB (0100) Pass PCB b ... ...

(1000)bb Fault IC Fault IC1 1

...

...

388
... 558 ... 656

(0010) Fault IC2 bbFault IC 2

Â· 0.5668

Â· 0.9898

Â· 0.9354

Â· 0.7821

Â· 6.8617

Â· 40.6199

Â· 6.0841

Â· 37.01695

Â· 1.5408

Â· 0.1611

Â· -0.6911

...

...

Â· 0.7028

Â· 0.9874

Â· 0.8961

Â· 0.8401

Â· 11.8335

Â· 52.2702

... Â·
4.6329

(0100) b Pass PCB (0100) Pass PCB b ... ...

Â· 21.4643

Â· 0.894117

Â· -0.13368

Â· -0.62562

(0001)bb Fault IC Fault IC3 3

Table 5.8: Summarized settings parameters for the MLP network.
70

Parameters Input layer size Hidden layer size Output layer size Training data Testing data Performance criteria Training algorithm Hidden layer activation function

Description 4 12 4 70% 30% Mean squared error (MSE) LevenbergÂ­Marquardt Sigmoid

63

5.7.2. Classification Results based on ANFIS The model of the ANFIS classification adopted by scaled conjugate gradient method scheme is shown in figure 5.16 with parameters setting in table 5.9. To assess the efficiency of our diagnostic technique, we do a series of attempts and compare their accuracy for a different structure of ANFIS.

Figure 5.16: ANFIS model structure.

Table 5.9: ANFIS parameters setting. Parameters Number of Inputs Number of Outputs Number of input membership functions Number of Output membership functions Number of Rules Fuzzy model type Description 4 1 64 16 16 sugeno

64

Table 5.10 shows the training target for fault classification. The training performance is illustrated in figure 5.17. This figure shows making less error for final epochs and reaching the minimum error target, also, there is no further reduction of error possible. It shows that ANFIS scheme with 16 membership functions for each input has better performance. In this structure, it uses 16 fuzzy rules and then have 1280 linear parameters and 48 nonlinear parameters needed to be adjusted. Figure 5.18 illustrates that the predicted output values are found in the ruler viewer of ANFIS, i.e. For the testing feature values (-0.34632, -0.07653, -0.45628, 3), the predicted output is shown as 4 (i.e. IC3 is faulty). This is the complete training and testing procedure of ANFIS for fault classification of the PCBs. In figure 5.19(a, b), there are two different points, circle points (in blue color) which represent the target result and points (in red color) which represent the classified outputs for given training and testing phases. As shown in these figures, the training output results are like the training target values. This shows the ANFIS can classify the faults correctly.

65

Table 5.10 Samples for testing and output results of the ANFIS Model. Table 5.10: Samples for testing and output results of the ANFIS. Model
Input Features extracted
Sample number

(to ANFIS) IC index Output (to ANFIS) Fault status

(Before minimization by PCA) PCA Correlated fatures

Homogeneity

correlation

Feature 2

Feature 3

Feature 4

Feature1

Contrast

Kurtosis

Entropy

Energy

Mean

Skew

Std

1 ... 189 ... 388 ... 558 ... 656

0.6598 ... 0.8732 ... 0.5674 ... 0.5668 ... 0.7028

0.9882 0.9572 0.5428 ... ... ...

3.7598
...

30.4455 ...

8.2192 ... 3.1593 ... 4.0140 ... 6.0841 ... 4.6329

67.5562 ... 9.9814 ... 16.1122 ... 37.01695 ... 21.4643

0.0899 ... 0.3325 ... 0.2488 ... 0.1407 ... 0.2061

3.509996 2.617815 -0.63745 ... ... ...

1 ... 1 ... 2 ... 3 ... 3

1 Pass PCB 1Pass PCB ... ... 2 Fault IC11 2 Fault IC ... ... 3 Fault IC22 3 Fault IC ... ... 1 PCBPCB 1 Pass Pass ... ...
Fault IC33 4 4 Fault IC

0.9844 0.8005 0.9020 25.4473 72.3419
... ... ... ... ... 0.9898 0.8709 0.9015 15.7644 59.3511

-3.71374 0.248826 0.861656 ... -1.1600 ... 1.5408 ... ... -1.0041 ... 0.1611 ... ... 0.1644 ... -0.6911 ... -0.62562

...

...

...

...
6.8617
...

... 40.6199 ...

0.9898 0.9354 0.7821 ... ... ...

0.9874 0.8961 0.8401 11.8335 52.2702

0.894117 -0.13368

.

75

Figure 5.17: Performance Evaluation of ANFIS during training phase. .

66

Figure 5.18: ANFIS Fuzzy rules for the classify network.

4.5 4 3.5 3

Training Phase
4.5 4 3.5 3

Testing Phase

Output

2.5 2 1.5 1

Output
Target Output
0 100 200

2.5 2 1.5 1

0.5
0 300 400

0.5
0 500 0 50 100 150 200

Target Output
250

Data set Index

Data Set index

a. Training result. .

b. Testing result. Figure 5.19: Performance graph for the ANFIS .Fault type classification model. .

67

Consequently, the SVM classifier model is implemented according to the parameters set in the table 5.11. In this classifier, a kernel function, called quadratic function, is used to the hidden layers for the same effect. This technique improves the average accuracy about 97.16% and 94.87% in training and testing, respectively, after the epoch equal to 10000. Therefore, this method can classify correctly 446 samples out of 459, and 185 samples out of 197 in training and testing phases, respectively. Table 5.12 (a, b), summarizes the comparative results of the various models used for the detection of the defect in the ICs.

Table 5.11: SVM parameters setting. Parameters Number of Inputs Number of Outputs Kernel function Method for separating The tolerance value No. of classes Description 4 1 Quadratic Sequential Minimal Optimization 1e-3 4

Table 5.12.a: Comparative training results. Parameters No. of all samples No. of training samples Network structure Epochs No. of Unclassified samples Average Accuracy of classification MLP 656 459 4-12-4 246 4 99.12% Training process ANFIS SVM 656 656 459 459 4-16-1 4-0-1 75 10000 2 13 99.96% 97.16%

68

Table 5.12.b: Comparative testing results. Parameters No. of all samples No. of training samples Network structure Epochs No. of Unclassified samples Average Accuracy of classification Testing process MLP ANFIS SVM 656 656 656 197 197 197 4-12-4 4-16-1 4-0-1 246 75 10000 7 5 12 96.16% 97.46 % 94.87%

5.8. Summary In this chapter, The finite-element method was successfully used for thermal propagation in 3D-PCBs with different sizes and physical layouts of ICs. For the proposed model, the main equivalent material properties of the PCBs and ICs are designed and simulated to estimate the temperature profile of stacked ICs. This developed model was shown to simulate the thermal profile of PCBs with a high accuracy for the experimental results. Then, the sample database for non-destructive testing IR image is obtained from the finite-element model. To improve the accuracy of thermal fault detection of ICs, the algorithms of denoising, segmentation, and significant feature extraction are presented in details;  First stage: To drop the confused noise, a comparative study of different filtering method's performance for IR images is measured to decide a suitable filter.  Second stage: To improve the quality of thermal image segmentation, the advanced threshold method, the Otsu thresholding algorithm, produces good results compared with the other different techniques.

69



Third stage: to extract the significant image features, the histogram is used for calculating the first and second order statistical features of the image.

The supervised classification techniques such as MLP, SVM, and ANFIS are applied. The fault detection, classification and simulation results of various classifier models are pointed out. The performance of each classifier for classification of fault is given in table 5.12.

70

Chapter 6: Experimental Evaluation
6.1. Introduction The kernel of this thesis is to develop a tool that will help to manufacturers of PCBs in testing their production lines with an acceptable rate of fault detection and minimize test time. This chapter summarizes that how we can apply the proposed approach on a real PCB. Also, we evaluate of our proposed approach in the earlier sections of this chapter. The PCB unit, namely Arduino UNO, is used to provide us the thermal profile for the UUT. 6.2. Platform Setup The main imaging equipment of the experimental setup includes a project hardware board, FLIR infrared camera system, digital thermometer and data acquisition unit is explained in this section; 6.2.1. Real PCB under Test Arduino board is one of the famous electronic devices in the world due to being open source, flexible software, and hardware. This board is a micro-controller board, which is a small circuit that comprises many electronic components. There are many types of Arduino hardware available on the field. In this work, the Arduino UNO based on the ATmega328 is selected as shown in figure 6.1. The main properties of this board are as follows:     Micro-controller ATmega328P chip. ATmega8U2 chip. Operating voltage 5V. Input voltage (limits) 6-20V
71

    

Digital I/O pins 14. Maximum DC current per I/O pin 20 mA. SRAM 2 KB and EEPROM 1 KB. Clock speed 16 MHz. Dimension board: width of 70 mm Ã high 54 mm.

IC2: ATmega8U2

IC1: ATmega328P

Figure 6.1: Arduino UNO board.

According to the thermal profile of Arduino UNO board, the hot spot is expected to appear on the two main ICs on the board which is; ATmega328P and ATmega8U2. These ICs are considered as the UUTs.  ATmega328P IC: PDIP with two parallel rows of electrical connecting pins comes out from the two sides of the package. The typical dimension is 34.798Ã7.493Ã4.5724 mm3. A typical ATmega328P IC and PDIP structure are illustrated in figure 6.2. More specifications of the IC are prepared in [41].

72

Figure 6.2: ATmega328P IC configuration.  ATmega8U2 IC: Quad Flat No lead (QFN), with size 7Ã7Ã1.2 mm3 and lead pitch 0.8 mm. This package is a surface-mounted unit and should be stacked directly on the surface of the PCB. The leads and connections are made on the four sides of the package that is mounted. A typical ATmega8U2 IC is illustrated in figure 6.3. The technical description of the ATmega8U2 IC can be reviewed in [41].

Figure 6.3: ATmega8U2 IC configuration.

73

6.2.2. FLIR Infrared Camera Unit The infrared camera used in the evaluation setup is a FLIR SC4000 camera shown in figure 6.4. This is a high-speed, high-resolution, high sensitivity, and science-grade infrared camera with Gigabit Ethernet. Using the PC-Link (Gigabit Ethernet), the camera system captures and transfers 125fps with 320Ã256-pixel imagery. In this work, Gigabit Ethernet connect the camera due to its faster speed and allows a higher frame rate. thus, the IR camera system can measure the temperature "through" the unit being tested. More specifications of the FLIR SC4000 camera system can be found in [69, 70].

Figure 6.4: FLIR SC4000 camera. 6.2.3. Data Acquisition System and Auxiliary Unit IR images are captured and analyzed using an integrated data acquisition unit of the optical system, supported by PC-based FLIR Researcher IR max software (i.e. Version 4.20.2.74), for data acquisition, analysis, and image capturing. The obtained input data are recorded in real time for later analysis using this software package [70]. Other auxiliary unit is a digital thermometer which is used with thermocouple probe to measure the ambient temperature of the surrounding area. The digital thermometer used in the present work is HH-23A from Omega Industries [71].
74

6.3. Experimental Setup In this setup, the IR camera should be able to generate right thermal image and should be able to increase the accuracy of the image. Object parameters such as emissivity, the atmospheric temperature and distance of measurement are to be set before the image capturing. The emissivity of the surface under test should be uniform; therefore, there are various methods for determining the emissivity of an object. One of the methods to find the emissivity of objects is to use materials in an emissivity table [25]. The atmospheric temperature is measured by a thermocouple and a digital thermometer. There is a possibility that there might be changes in atmospheric temperature. Therefore, an updated temperature must be fed into the object parameter, setting of the image to keep the track of the atmospheric temperature. To excite the ICs, the circuit board is programmed to exceed total micro-controller current for a period by running at least 10 I/O pins (e.g. lighting 10 LED's) on high mode and draw 20mA from each one. According to Arduino UNO specifications, the total current sourced from all I/O pins must not exceed 200mA, Finally, a steady-state image of real time operating UUT board is taken to get the temperatures on the PCB surface. All the setting parameters for the operation of IR camera system should be fixed during the experiment. Table 6.1 shows the main setting parameter values for IR cameras are used. The experimental IR capturing system is set as shown in figure 6.5.

75

Figure 6.5: Real PCB with IR capturing system.

Table 6.1: IR camera parameters set up. Parameters Image format Distance Emissivity Lens Focal Length Resolution Atmospheric temperature Value . png 0.65 m 0.94 25 mm 320 (H) x 256 (V) 23.0 

76

6.4. Image Capture and Processing The IR camera is used to obtain the temperature changes verse power dissipation of ATmega328P and ATmega8U2 ICs. Power dissipation of the ICs is calculated according to equation (5.2) by applying thermal resistance values for Atmel standard packages. Figure 6.6 illustrates the power dissipation versus temperature changes for two hot units. In the first unit; ATmega328P, the power dissipation is changed from 0.632W to 0.751W to get the temperature range 34.612Â­42.322, as shown in figure 6.6.a. In the second unit; ATmega8U2, the power dissipation is varied from 1.086W to 1.296W to get the temperature range 34.761Â­41.913, as shown in figure 6.6.b. From results above, the IR thermography shows a near result for the temperature profile got from finite element simulation and experimental results. After ICs are excited; the thermal image sequence of the PCB is captured using the IR camera. Figure 6.7 illustrates the samples of captured images for temperature distribution on the PCB's surface with the geometry. The captured thermal images are filtered and segmented to extract ROI according to the proposed approach as shown in figures 6.8 and 6.9.
45.00

Te mperature (C)

40.00 35.00 30.00 25.00 20.00

Figure 6.6.a: Temperature variations vs. power dissipation for ATmega328P chip.

0.63 0.64 0.64 0.65 0.66 0.66 0.67 0.69 0.69 0.70 0.71 0.71 0.72 0.74 0.74 0.76

Power Dissipation (Watt)

77

42 40

Temperature (C)

38 36 34 32

30
1.09 1.11 1.12 1.14 1.15 1.15 1.18 1.19 1.20 1.20 1.23 1.23 1.23 1.24 1.26 1.26 1.26 1.29
Power Dissipation (Watt)

Figure 6.6.b: Temperature variations vs. power dissipation for ATmega8U2 chip. Figure 6.6: Thermal profile for UUT.

a. IC1 (ATmega328P) at 41.81.3 .

b. IC1 (ATmega328P) at 36.3 .

c. IC2 (ATmega8U2) at 39.48 .

d. IC2 (ATmega8U2) at 36.04 .

Figure 6.7: Samples of captured IR image for real PCB.

78

without noise image

Final Filter Image

a. Original Image.

b. Using BM3D + Median filter.

Figure 6.8: Filtering image results.

Figure 6.9: Image segmentation (i.e. Otsu's method) and cropping results.

79

In the next stage, the extracted features are calculated and stored in a database as feature vectors. The features include: mean, standard deviation, skewness, kurtosis, energy, entropy, correlation, contrast, and homogeneity are got by using the probability distribution of the histogram levels. Figures 6.10 (a, b) shows feature variation plotted versus the temperature increase of IC1 and IC2.

0.9 0.8 0.7

160 140 120 100

Contrast

0.6

Mean

0.5 0.4 0.3 0.2 0.1 0 34.1 35.5 36.8 40.7 Temperature (C)
IC1

80 60 40 20 0 34.1 35.5 36.8 40.7 Temperature (C)
IC1

42.3

42.3

a. Contrast Feature vs. Temperature.

b. Mean Feature vs. Temperature.

1.005 1.000 0.995
IC1

1.2 IC1 1 0.8

Homogeneity

0.990 0.985 0.980 0.975 0.970 0.965 0.960 34.1 35.5 36.8 40.7 42.3

Energy

0.6 0.4 0.2 0 34.1 35.5 36.8 40.7 Temperature (C) 42.3

Temperature (C)

c. Homogeneity Feature vs. Temperature.

d. Energy Feature vs. Temperature.

80

90

70 60 50 40
IC1

80
70 60

Skeww
IC1

STD

50 40 30 20 10

30 20 10 0

0

34.1

35.5

36.8

40.7

42.3

34.1

35.5

36.8 Temperature (C)

40.7

42.3

Temperature (C)

e. Standard Deviation Feature vs. Temperature.
1 0.9 0.8

f. Skewness Feature vs. Temperature.

4500 4000 3500 3000
IC1

Correlation

Kurtosis
IC1

0.7 0.6 0.5 0.4 0.3 34.1 35.5 36.8 40.7 42.3

2500 2000 1500 1000 500

0

34.1

35.5

Temperature (C)

36.8 40.7 Temperature (C)

42.3

g. Correlation Feature vs. Temperature.
3.5 3

h. kurtosis Feature vs. Temperature.

2.5
Entropy
2 1.5 1 0.5
IC1

0

34.1

35.5

36.8
Temperature (C)

40.7

42.3

i. Entropy Feature vs. Temperature. Figure 6.10.a: Features changing plots versus temperature for IC1.

81

0.16

8 7 6 5 4 3 2
IC2

0.14
0.12

Contrast

0.08 0.06 0.04 0.02 0

Mean

0.1

1 0

IC2

34.8 35.6 36.7 37.9 38.3 39.3 39.5 40.3 40.4 Temperature (C)

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4 Temperature (C)

a. Contrast Feature vs. Temperature.
40 35 30

b. Mean Feature vs. Temperature.
0.999 0.998 0.997

Homogeneity
IC2

25
STD
20 15 10 5 0

0.996 0.995 0.994 0.993 0.992 0.991
IC2

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4 Temperature (C)

Temperature (C)

d. Standard Deviation Feature vs. Temperature.
20 18 16 14 12 10 8 6 4 2 0
IC2

c. Homogeneity Feature vs. Temperature.
1 IC2

0.98
0.96

Energy

Skeww

0.94
0.92 0.9 0.88

34.8 35.9 36.8 38.1 39.0 39.5 40.3 40.4 Temperature (C)

34.8

35.9

36.8

38.1

39.0

39.5

40.3

40.4

Temperature (C)

e. Skewness Feature vs. Temperature.

d. Energy Feature vs. Temperature.

82

1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

350
IC2

300 250

Correlation

Kurtosis
IC2

200 150 100 50 0

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4 Temperature (C)

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4 Temperature (C)

g. Correlation Feature vs. Temperature.
0.4
0.35 0.3 0.25

h. kurtosis Feature vs. Temperature.

Entropy

0.2 0.15 0.1 0.05 0
IC2

34.8 35.6 36.6 37.8 38.2 39.3 39.5 40.2 40.4 Temperature (C)

i. Entropy Feature vs. Temperature. Figure 6.10.b: Features changing plots versus temperature for IC2.

83

6.5. Classification Results After capturing and processing the image, the final step is the intelligent fault diagnosis and decisions. From the experimental classification, results can be concluded to the optimal MLP architecture as shown in figure 6.11. In the figure 6.12, the convergence of the LevenbergMarquardt BP training algorithm is illustrated where the performance factor reduced to 1.187e-7 after 1634 epochs.

. Figure 6.11: Proposed neural network architecture layout.

10

0

Best Training Performance is 1.187e-07 at epoch 1634
Train Best

Mean Squared Error (mse)

10

-2

10

-4

10

-6

0

200

400

600

800

1000

1200

1400

1600

1634 Epochs

Figure 6.12: Proposed neural network architecture layout.

84

The model of the ANFIS classification topology is shown in figure 6.13. According to the feature reduction results, three superior features have been selected by the PCA and then are fed into the ANFIS classifier. The ANFIS system has six rules, 4 inputs, and one output. After taking 30 epochs, the RMSE is 0.0300841. The decrement in the error while training the ANFIS is shown in the figure 6.14.

Figure 6.13: ANFIS structure.

Performance evaluation 0.3

0.25

RMSE value

0.2

0.15

0.1

0.05

0

10

20

30

40

50 Epochs

60

70

80

90

100

Figure 6.14: RMSE response of ANFIS.
85

.

A graphical view of ANFIS output is shown in figure 5.15 (a,b), yellow squares represent the target of ANFIS while the red circles represent the output values. Where the squares and circles overlap with each other, it means that the ANFIS target matches the output value. The circle sign represents the difference in ANFIS output and the actual value for both phases; training and testing.

Training Phase
3.5 3 2.5

Output

2

1.5

1
0.5 0

Target Output
0 50 100 150 200 250

Data set Index

a. Training phase.
Testing Phase

3.5 3 2.5

Output

2 1.5 1 0.5 0
0 10 20 30 40 50 60 70 Output Target 80 90

Data Set index

b. Testing phase. Figure 6.15: Graphical classification output based on ANFIS.
86

Finally, table 6.2 shows that the ANN with MLP network present training and testing accuracy about 98.43% and 94.186%, respectively. Therefore, it detects 189 samples out of 192, and 77 samples out of 82 samples after 1643 epoch in the training and testing process, respectively. The SVM method achieves an average accuracy of 88.54% and 88.90% in training and testing, respectively. Therefore, this method can classify correctly 170 samples out of 192 and 73 samples out of 82 in training and testing phases, respectively, after 10000 epochs. The ANFIS produces the training and testing process accuracy of 97.44% and 97.43%, respectively, with 30 epoch only. For 82 testing samples, only two unclassified samples got with ANFIS while were 5, and unclassified samples with ANN and SVM respectively.

Table 6.2.a: Comparison of training results. Parameters No. of all samples No. of training samples Network structure Epochs No. of Unclassified samples Average Accuracy of classification MLP 274 192 4-12-4 1643 3
98.43%

Training Process ANFIS SVM 274 274 192 192 4-16-1 4-0-1 30 10000 4 22 97.44% 88.54%

87

Table 6.2.b: Comparison of testing results. Testing Process Parameters MLP ANFIS SVM No. of all samples 274 274 274 No. of training samples 82 82 82 Network structure 4-12-4 4-16-1 4-0-1 Epochs 1643 30 10000 No. of Unclassified samples 5 2 9 94.18% 97.43% 88.90% Average Accuracy of classification

6.6. Summary Experimental evaluation has been performed to confirm the proposed approach in this chapter. The results of FEM simulations were experimentally verified in real PCB experiments. In the first part, the major equipment in the experimental setup was discussed in this chapter. In the second part, Arduino UNO board was presented to verify the testing results. Two ICs which are; ATmega328P and ATmega8U2 chips were considered as the UUT in this work. Then the infrared FLIR SC4000 system was used to capture and transfer 320Ã256 pixels imagery. In next part, filtering and segmentation process were implemented on the thermal image to extract main histogram features. The feature's pre-processing is implemented to generate new parameters by PCA method. It allows getting data, which less correlated and lower order. The final part was classifed fault condition for the PCB based on MLP, SVM and ANFIS classifier. The performance of each classifier for detection and classification of fault was given in table 6.2. From the experimental results, the efficiency of the proposed approach has been shown.

88

Chapter 7: Conclusion and Future Work
7.1. Conclusions Although several techniques in the literature were proposed to study fault detections and diagnosis, still challenges related to the performing reliable testing systems for PCB manufacturing has been remained because of increasing complexity of PCBs. The goal of this thesis is to develop an intelligent testing approach to tackle these related challenges. The aim of our work is to develop a more robust method at each processing stage to improve the condition for fault detection in the UUTs. Main contributions of our work are as follows:  Infrared thermography as a non-destructive fault detection testing of PCB is presented. The fundamental and essential theoretical background in this field have been reviewed in this thesis. This background information is provided to help and assist the researchers of these technologies in a better understanding of the subject.  In this thesis, a three-dimensional PCB model comprising five physical layers has been designed and simulated by the FE analysis software. The IR images for the PCB model are captured and successfully presented.  Through a preprocessing step, to remove the thermal noise (i.e. Salt & pepper + Gaussian noises) from IR images, combination of Median and BM3D filters is applied. Performance of this advanced filter is verified by using the performance metrics such as SNR and PSNR factors.  The enhanced IR images are segmented and cropped to identify ROI object. Various algorithms have compared the IR image to a threshold. From the results, it can be

89

inferred that the Otsu's algorithm works better for IR images as compared with the other thresholding algorithms. The Otsu's algorithm works on finding the best optimum threshold value and minimizing the variance among the different image pixels.  After that, the PCA is performed on the first and second order of histogram features to extract the major information, which reduces the number of features significantly. These major features are then fed into the intelligent classifier models for training and then testing.  In the classification work, the results conducted in this part can be summarized as follows: 1. The three classifier models, MLP, SVM, and ANFIS, are used to classify the fault condition of ICs corresponding to different classes. The effectiveness of models tested by comparing the performances and accuracy of classification. 2. MLP technique used to develop the classified model and the data required for training the model used for the simulation the PCB model. Various training algorithms trained with the different number of hidden neurons to arrive at the optimal model. The tool disadvantages are that they train slowly and training network process has taken a large time. The developed MLP classifies the fault condition for ICs with average percentage accuracy 96.16% in testing phase after 246 epochs. 3. ANFIS software has used to develop the classified model. Various membership functions trained to arrive at the suitable model. The

90

ANFIS model provided fast train and high accuracy. Furthermore, the network trained for 75 epochs and the average percentage accuracy about 97.46 % of test data. 4. In the multi-classes SVM classifier model, the algorithm achieved the average accuracy of 94.87% in testing phases.  Experiments based on Arduino UNO board have performed to show the efficiency and applicability of the developed method. The experimental results of classifier models illustrate in Table 6.2. The efficiency of the proposed method has been shown in Table 6.2. 7.2. Summary of Contributions of this thesis The major contribution of this thesis is that it proposes a set of test techniques to decrease test application time with different considerations, includes testing efficiency and defect diagnosis. The following parts summarizes the main contributions of this work:  Designing a 3D-PCB complex geometry model based on FE software of COMSOL. In this model, three types of ICs are implemented. The dimensions, physical properties such as thermal conductivities, thermal capacities, and densities of different layers of PCB is selected to generate the thermal profile for a numerical population of data sets.  This project has classified the defects of PCBs into IC level groups. This method increases the efficiency of the testing system in locating the defect IC on the PCB. Since a PCB pattern is produced in different processes, classification of defects can help in determining the sources which create errors and reduce production cost in the long run.

91



To increase the quality of IR image, we arrange median with BM3D filters in series. This advanced filter is useful for eliminating different thermal noises.

7. 3. Future work The future work should be recommended in the following directions:  Extending the intelligent classification algorithm in multi-layer PCBs; classification techniques can be applied in multi-layer to locate the exact position of the defected component.  Automatic extraction of optimal features; as we mentioned in the earlier chapters, successfully applying of the intelligent classification algorithms depends on choosing the proper IR image features. Automatic extraction of the optimal features of the IR image seems an interesting research area for future researchers.

92

REFERENCES
[1] P. Philemon Daniel, "Software-Based Self-Test Techniques for Online Test and Diagnosis of Embedded Controllers and Memories", Doctor of Philosophy Thesis, National Institute of Technology, INDIA, 2014. [2] H. Moldovan, M. Marcu, and M. Vladutiu, "PCB Testing Using Infrared Thermal Signatures", IEEE Instrumentation and Measurement Technology Conference Proceedings (IMTC 2005), Ottawa, Canada, pp. 1970Â­1974, 2005. [3] P. Hedayati Vahid, S. Hesabi, D. Lauredndeau, and X. Maldaque, "A defect detection approach in thermal images", Department of Electrical and Computer Engineering, Laval University, Quebec, Canada, May 2015. [4] J. Varghese k, T. Singh, and S. Mohan, "PCB Thermal Image Analysis using MATLAB", International Journal of Recent Advances in Engineering & Technology (IJRAET), vol. 2, no. 3, pp. 46Â­52, 2014. [5] C. R. Wagh and V. B. Baru, "Detection of Faulty Region on Printed Circuit Board with IR Thermography", International Journal of Scientific & Engineering Research, vol. 4, no. 11, pp. 1Â­4, 2013. [6] J. Xu and J. Li, "Components Locating in PCB Fault Diagnosis Based on Infrared Thermal Imaging", IEEE Second International Conference on Information and Computing Science (ICIC '09), Washington, DC, USA, pp. 7Â­9,2009. [7] S. Huang, C. Mao, and K. Cheng, "A VQ-based approach to thermal image analysis for printed circuit boards diagnosis", IEEE Transactions On Instrumentation and Measurement, vol. 54, no. 6, pp. 2381Â­2388, Dec. 2006. [8] M. Norhisham, I. Bugis, I. Wani Jamaludin, and R. Ranom, "Thermal Analysis On PCB Using Galerkin Approach", IEEE 4th International Conference on Modeling, Simulation, and Applied Optimization (ICMSAO), Kuala Lumpur, pp.1Â­6, 2011. [9] S. Xu and X. Li, "Analysis on thermal reliability of key electronic components on PCB board", IEEE Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering (ICQR2MSE), 2011 International, Xi'an, pp. 52Â­54, 2011.

93

[10] M. Shawal and S. Kabir, "Thermal Imaging for Qualitative-based Measurements of Thermal Anomalies in Electrical Components", 2011 Saudi International Electronics, Communications, and Photonics Conference (SIECPC), Riyadh, pp. 1Â­6, IEEE, 2011. [11] F. A. Rassul, M. A. Abdulsada, and F. R. Abusief, "Functional Test Generation for Guided Control System", International Journal of Computer and Electrical Engineering, vol. 2, no. 5, pp. 943Â­947, Oct. 2010. [12] J. Garc la Gervacio, "An Aware Methodology to Evaluate Circuit Testability for Small Delay Defects", Doctor of Philosophy Thesis, National Institute of Astrophysics, Optics, and Electronics, Mexico, 2009. [13] M. A. Nummer, "A DFT Technique for Testing High-Speed Circuits with Arbitrarily Slow Testers", Master's Thesis, University of Waterloo, Canada, 2001. [14] J. Li, C. W. Tseng, and E. J. McCluskey", Testing for Resistive Opens and Stuck Opens", IEEE International Test Conference, Baltimore, MD, pp. 1049Â­1058, 2001. [15] N. Aghaee, "Thermal Issues in Testing of Advanced Systems on Chip", Master's Thesis, LinkÃ¶ping University, Sweden, 2015. [16] S. Raj, "A Faster Approach towards Fault Detection and Diagnosis in ASICs and FPGAs", Master's Thesis, International Institute of Information Technology, India, 2010. [17] J. Altet and A. Rubio, "Thermal Testing of Integrated Circuits", first edition Ed., SpringerScience & Business Media, Catalunya, 2002. [18] R. Par Vataneni, "Principal Component Thermography for Steady Thermal Perturbation Scenarios", Master's Thesis, Clemson University, 2009. [19] J. Bedsole, R. Raina, and M. S. Abadir, "Very Low Cost Testers: Opportunities and Challenges", IEEE Design & Test of Computers, vol. 18, no. 5, pp. 60Â­69, Oct. 2001. [20] "Introduction to Nondestructive Testing", American Society for Nondestructive Testing. [Online].Available:https://www.asnt.org/MinorSiteSections/AboutASNT/Intro-to-NDT, Accessed:June.18, 2016. [21] S. Akole and V. B. Kulkarni, "Thermal Analysis in Electronics With Thermography: A Review", International Journal of Advance Foundation And Research In Science & Engineering (IJAFRSE), vol. 1, no. Special, pp. 1Â­7, Mar. 2015.

94

[22] H. Cui, Y. Xu, J. Zeng, and Z. Tang, "The Methods in Infrared Thermal Imaging Diagnosis Technology of Power Equipment", IEEE Electronics, Information, and Emergency Communication Conference (ICEIEC), pp. 246Â­251, Beijing, 2013. [23] "What is Infrared Thermography?", Challenge for the future. [Online].

Available:http://www.infrared.avio.co.jp/en/products/ir-thermo/what-hermo.html, Nippon Avionics Co. Ltd, Ed. Accessed: June. 18, 2016. [24] R. Usamentiaga, P. Venegas, J. Guerediaga, and L. Vega, "Infrared Thermography for Temperature Measurement and Non-Destructive Testing", Sensors, vol. 14, no. 7, pp. 12305Â­12348, Jul. 2014. [25] "Thermo Vision SDK", in User's Manual (no. T559014), Program version 2.6 SP2 ed., USA, FLIR Systems, Inc., 2013. [26] S. Ganapathy, K. Kulothungan, S. Muthurajkumar, and M. Vijayalakshmi, "Intelligent feature selection and classification techniques for intrusion detection in networks: a survey", EURASIP Journal on Wireless Communications and Networking, no. 1, pp. 1Â­16, 2013. [27] S. Pappala, "Device specific key generation technique for anti-counterfeiting methods using FPGA based physically un-clonable functions and artificial intelligence", Master's Thesis, The University of Toledo, 2012. [28] O. Esmail Mahmoud, "An Intelligent Engine Condition Monitoring System", Doctor of Philosophy Thesis, Newcastle University, UK, 2009. [29] C. kun Ãzkan and F. Sunar Erbek, "The Comparison of Activation Functions for Multispectral Landsat TM Image Classification", Photogrammetric Engineering & Remote Sensing, vol. 69, no. 11, pp. 1225Â­1234, Nov. 2003. [30] J. Subash Chandra Bose, K. R. Shankar Kumar, and M. Karnan, "Detection of Microcalcification in Mammograms using Soft Computing Techniques", European Journal of Scientific Research, vol. 36, no. 1, pp. 103Â­122, Sep. 2012. [31] W. Chen, "A Generalized Logic-Based Approach for Intelligent Fault Detection and Recovery in Power Electronic Systems", Master's Thesis, University of Connecticut, 2015. [32] S. Mohd Arif Wani, "Comparative Study of Back Propagation Learning Algorithms for Neural Networks", International Journal of Advanced Research in Computer Science and Software Engineering, vol. 3, no. 12, pp. 1151Â­1156, Dec. 2013.
95

[33] V. Vacic, "Computer Science Department at the University of California", Computer Science Department at the University of California, 2005. [34] I. Kitanovski, B. Jankulovski, I. Dimitrovski, and S. Loskovska, "Comparison of Feature Extraction Algorithms for Mammography Images", IEEE 4th International Congress on Image and Signal Processing (CISP), Shanghai, pp. 888Â­892, 2011. [35] K. Mertsalov and M. McCreary, "Document Classification with Support Vector Machines, Jan. 2009. [36] S. Hassan, "Fault detection, classification and location in Underground Cables", Master's Thesis, Fayoum University, Egypt, 2014. [37] X. Zhi Gao, "Soft Computing Methods for Control and Instrumentation", Doctor of Philosophy Thesis, Institute of Intelligent Power Electronics Publications, Helsinki, 1999. [38] I. Furkan, "Intelligent Question Classification for E-Learning Environments by Data Mining Techniques", Master's Thesis, Bahcesehir University, Istanbul, 2008. [39] A. Pascal Lambert, "Thermal-Mechanical Analysis of System- Level Electronic Packages for Space Applications", Master's Thesis, Montana State University, Montana, 2012. [40] B. Zhang, P. kuan Liu, H. Ding, and W. Cao, "Modeling of board-level package by Finite Element Analysis and laser interferometer measurements", Microelectronics Reliability, vol. 50, no. 7, pp. 1021Â­1027, 2010. [41] "ATmega48A/PA/88A/PA/168A/PA/328/P Complete", [Online]. Accessed: Available: June. 18,

http://www.atmel.com/devices/atmega328p.aspx?tab=documents, 2016.

[42] M. Marami, "Generation and evaluation of dynamic compact thermal model of electronic packages", Master's Thesis, Ryerson University, Toronto, 2007. [43] "COMSOL Multiphysics user's guide, Â© 1998Â­2012 COMSOL by COMSOL AB.," [Online]. Available: https://www.comsol.com/comsol-multiphysics, Accessed: June. 18, 2016. [44] F. Farrokhi, F.A. Mohammadi "Temperature and Power Measurement of Modern Dual Core Processor by Infrared Thermography", Proceeding of the IEEE International Symposium on Circuits and Systems (ISCAS2010), pp. 1603 - 1606, Paris, France, May 30June 02, 2010

96

[45] W.B. Dupey, "Atmel Corporation, package material declaration", USA: Atmel Corporation. [Online]. Available: http://www.atmel.com/about/quality/package.aspx.

Accessed: Jun. 18, 2016. [46] "Thermal characteristics of Atmel packages," in Data sheet (Thermal Specifications), USA: Atmel Corporation, 2004. [Online]. Available:

http://electronix.ru/forum/index.php?act=Attach&type=post&id=7114.Accessed: Jun. 18, 2016. [47] C. Solomon and T. Breckon, "Fundamentals of Digital Image Processing", First edition Ed. UK: John Wiley & Sons, Ltd, 2011. [48] M. Awais Farooque and J. S.Rohankar, "Survey On Various Noises And Techniques For Denoising The Color Image", International Journal of Application or Innovation in Engineering & Management (IJAIEM), vol. 2, no. 11, pp. 217Â­221, Nov. 2013. [49] "Noise in the photographic image in "Digital Image quality testing", Imatest LLC, USA, Documentation. [Online]. Available: http://www.imatest.com/docs/noise/, Accessed: Jun. 18, 2016. [50] V. A, T. C, and L. B. H, "Image Denoising for different noise models by various filters: A Brief Survey", International Journal of Emerging Trends & Technology in Computer Science (IJETTCS), vol. 3, no. 6, pp. 42Â­45, Dec. 2014. [51] M. Lebrun, "An Analysis and Implementation of the BM3D Image Denoising Method," Image Processing On Line, vol. 2, pp. 175Â­213, 2012. [52] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, "Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering", IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080Â­2095, Aug. 2007. [53] P. Banerjee, "Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation", International Journal of Computer Trends and Technology (IJCTT), vol. 17, no. 3, pp. 121Â­124, Nov. 2014. [54] J. Varghese, M. Ghouse, S. Subash, M. Siddappa, M. Samiulla Khan, and O. Bin Hussain, "Efficient adaptive fuzzy-based switching weighted average filter for the restoration of impulse corrupted digital images", Image Processing, IET, vol. 8, no. 4, pp. 199Â­206, 2014.

97

[55] J. S. Jakati and S. S Matad, "PCB defect detection based on pattern matching and segmentation algorithm", International Journal of Advanced Research in Computer and Communication Engineering, vol. 3, no. 9, pp. 8007Â­8011, Sep. 2014. [56] P. Agrawal, S. K. Shriwastava, and S. S.Limaye, "MATLAB implementation of image segmentation algorithms", 3rd IEEE International Conference on Computer Science and Information Technology (ICCSIT), 2010, Chengdu, 2007, pp. 427Â­431. [57] Y. Chieh Chou and L. Yao, "Automatic Diagnosis System of Electrical Equipment using Infrared Thermography", IEEE International Conference on Soft Computing and Pattern Recognition (SOCPAR), Malacca, 2009, pp. 155Â­160. [58] S. Bansal and R. Maini, "A Comparative Analysis of Iterative and Otsu's Thresholding Techniques", International Journal of Computer Applications, vol. 66, no. 12, pp. 45Â­47, Mar. 2013. [59] M. Hetal and A. Baxi, "A Review on Otsu Image Segmentation Algorithm", International Journal of Advanced Research in Computer Engineering & Technology (IJARCET), vol. 2, no. 2, pp. 387Â­389, Feb. 2013. [60] B. Al-Mahadeen, and I. H. Al Tarawneh, "Signature Region of Interest using Auto cropping", IJCSI International Journal of Computer Science, vol. 7, no. 2, pp. 1Â­5, Mar. 2010. [61] S. Ahmed Medjahed, "A Comparative Study of Feature Extraction Methods in Images Classification", Image, Graphics and Signal Processing, vol. 7, no. 3, pp. 16Â­23, Mar. 2015. [62] G. Kumar, "A Detailed Review of Feature Extraction in Image Processing Systems," Fourth IEEE International Conference on Advanced Computing & Communication Technologies, Rohtak, 2014, p. 5Â­Â­12. [63] B. Wiecek, "Review on thermal image processing for passive and active thermography," 27th IEEE Annual Conference Engineering in Medicine and Biology, Shanghai, 2006, pp. 686Â­689. [64] F. Malik and B. Baharudin, "The Statistical Quantized Histogram Texture Features Analysis for Image Retrieval Based on Median and Laplacian Filters in the DCT Domain", The International Arab Journal of Information Technology, vol. 10, no. 6, pp. 1Â­9, Nov. 2013.
98

[65] G. A B, M. C. Chandrashekhar, and M. Z. Kurian, "Texture Feature Extraction of Video Frames Using GLCM", International Journal of Engineering Trends and Technology (IJETT), vol. 4, no. 6, pp. 2718Â­2721, Jun. 2013. [66] X. He, "An Outlier Detection Approach for PCB Testing Based on Principal Component Analysis," Master's Thesis, Colorado State University, Colorado, 2011. [67] S. Ganapathy, K. Kulothungan, S. Muthurajkumar, and M. Vijayalakshmi, "Intelligent feature selection and classification techniques for intrusion detection in networks: a survey", EURASIP Journal on Wireless Communications and Networking, vol. 2013, no. 1, pp. 1Â­16, 2013. [68] "ThermoVision SC4000 Technical Specifications", Â© Copyright 2006, FLIR system, Inc.[Online].Available:http://www.flir.com/assets/f606a0fb4c7d41869053907a8900a6ef.pd f, Accessed: June.18, 2016. [69] T. Fakhry, "Optimization of a compact thermal model for a Ball Grid Array (BGA) package using experimental data", Master's Thesis, Ryerson University, Toronto, 2011. [70] S. Ailani, "Development of Infrared Thermal Mapping Technique for Electronic Devices", Master's Thesis, Ryerson University, Toronto, 2008.

99

Glossary
AET ANN ANFIS BIST BP BM3D ET FEM GLCM GWT IC IR IRT LM LSSD MLP NTD MSE PC PCA PCB PCT PDIP PSNR PQFP RT SNR SVD SVM UT UUT VLSI Acoustic Emission Testing Artificial Neural Network Adaptive Neuron-Fuzzy Inference System Built Â­In Â­Self Test Back Propagation Block Matching Three Dimensional Electromagnetic Testing Finite Element Model Gray Level Co-occurrence Matrix Guided Wave Testing Integrated Circuit Infrared Infrared Testing Levenberg-Marquardt Level Sensitive Scan Design Multi-Layer Perceptron Non Destructive Testing Mean Square Error Personal Computer Principle Component Analysis Printed Circuit Board Principal Component Thermography Plastic Dual Inline Package Peak Signal-to-Noise Ratio Plastic Quad Flat Package Radio graphic Testing Signal-to-Noise Ratio Singular Value Decomposition Support Vector Machine Ultrasonic Testing (UT) Unit Under Test Very Large Scale Integrated

100


