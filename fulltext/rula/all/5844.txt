Predicting the time-to-deliver of software changes
by Sokratis Tsakiltsidis Bachelor of Science in Information Technology, Alexander Technological Institute of Thessaloniki, 2012 A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science Toronto, Ontario, Canada, 2016 c Sokratis Tsakiltsidis 2016

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

iii

Predicting the time-to-deliver of software changes Master of Science 2016 Sokratis Tsakiltsidis Computer Science Ryerson University

Abstract
In this thesis we examine the application of survival analysis on time-to-deliver data. Successful prediction of the time necessary to deliver a new feature or fix a reported defect can assist in various phases and aspects of software development. We identify and try to overcome limitations when dealing with time-to-event data. Our proposed methodological framework includes use of survival analysis, utilization of incomplete information that might be available as censored data, and incorporation of random-eects through mixed-eects models for identification of hierarchical/clustered data within our dataset. We explore and experiment with a dataset from a large scale commercial software over a twelve year period of time. We show that we can successfully implement survival analysis, and that incorporation of random-eects provides a considerable advantage, however, incorporation of censored information is not proven to be advantageous in this case.

v

Acknowledgements
This Master dissertation is submitted to fulfill the requirements of the MSc of Computer Science at Ryerson University in Toronto, Canada. The work carried out in this dissertation has been has been supervised by Dr. Andriy Miranskyy. Firstly, I would like to express my sincere gratitude to my supervisor for his continuous support, understanding, and valuable advice. His guidance and mentorship helped me in all the time of research and writing of this thesis. Besides my supervisor, I would like to thank Dr. Petros Pechlivanoglou for his insightful guidance and amazing support during my period of study. Additionally, I would like to thank my thesis examination committee: Dr. Chen Ding and Dr. Vojislav Mi siÂ´ c, for agreeing to read and provide their feedback on this thesis. Last but not least, I would like to thank my family, my friends, and of course Lila for their support, but most importantly for their patience while fulfilling the requirements of this degree.

vii

Dedication
To my father.

ix

Contents
Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Figures 1 Introduction 1.1 1.2 1.3 1.4 1.5 1.6 Motivation and problem statement . . . . . . . . . . . . . . . . . . . . . Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proposed Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Novelty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii v vii ix xv

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii 1 1 2 2 4 4 5 7 7 9 10 15 15 17 18 19 19

2 Literature Review 2.1 2.2 2.3 Software quality assurance . . . . . . . . . . . . . . . . . . . . . . . . . . Number of bugs remaining . . . . . . . . . . . . . . . . . . . . . . . . . . Time-to-deliver estimation . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Methodology and Implementation 3.1 3.2 3.3 Theoretical framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . Regression analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 3.3.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . Ordinary Least Squares . . . . . . . . . . . . . . . . . . . . . . . xi Regression estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.2 3.4 3.4.1 3.4.2 3.5 3.5.1 3.5.2 3.5.3 3.5.4 3.5.5 3.6 3.6.1 3.6.2 3.7 3.7.1 3.7.2 3.7.3 3.7.4 3.7.5 3.8

Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . Censoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Skewness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Survival function . . . . . . . . . . . . . . . . . . . . . . . . . . . Hazard function . . . . . . . . . . . . . . . . . . . . . . . . . . . . Non-parametric estimators . . . . . . . . . . . . . . . . . . . . . . Semi-parametric estimation . . . . . . . . . . . . . . . . . . . . . Parametric estimation . . . . . . . . . . . . . . . . . . . . . . . . Fixed-eect models . . . . . . . . . . . . . . . . . . . . . . . . . . Random-eect models . . . . . . . . . . . . . . . . . . . . . . . . Akaike Information Criterion . . . . . . . . . . . . . . . . . . . .

20 21 21 23 25 26 27 27 28 29 32 32 33 34 35 36 36 37 37 38 41 41 45 46 47 48 48 51 54 57 59 61

Understanding the limitations . . . . . . . . . . . . . . . . . . . . . . . .

Survival Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Mixed-eects models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Diagnostics and validation . . . . . . . . . . . . . . . . . . . . . . . . . . R-squared . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Residual standard deviation . . . . . . . . . . . . . . . . . . . . . Kendall rank correlation coe cient . . . . . . . . . . . . . . . . . Accuracy of slow/fast classification . . . . . . . . . . . . . . . . .

Censoring scenario analysis . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Evaluation 4.1 4.2 4.3 Data description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Non-parametric analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . Semi-parametric analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 4.3.2 4.4 4.4.1 4.4.2 4.5 4.6 4.7 Fixed-eect models . . . . . . . . . . . . . . . . . . . . . . . . . . Mixed-eects/frailty models . . . . . . . . . . . . . . . . . . . . . Fixed-eect models . . . . . . . . . . . . . . . . . . . . . . . . . . Mixed-eects/frailty models . . . . . . . . . . . . . . . . . . . . .

Parametric analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Considering censoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii

4.8

4.7.1 Validation of Hypothesis . . . . . . . . . . . . . . . . . . . . . . . 4.7.2 Stakeholder feedback . . . . . . . . . . . . . . . . . . . . . . . . . Threats to validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63 63 65

5 Conclusions, Summary and Future Work 5.1 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix A Results Tables Appendix B Analysis scripts B.1 Main analysis script . . . . . . B.2 Data preprocess . . . . . . . . B.3 Survival analysis helper script B.4 Survival analysis helper script B.5 External validation . . . . . . B.6 Linear model metrics . . . . . References Index

67 68 71 87 87 93 95 106 111 119 12m 133

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

. . . . . .

xiii

List of Tables
4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 Number of observations per release. . . . . . . . . . . . . . . . . . . . . . Descriptive analysis table. . . . . . . . . . . . . . . . . . . . . . . . . . . Regression estimates of Cox proportional hazard models. . . . . . . . . . Parametric regression estimates of fixed-eect models. . . . . . . . . . . . Performance metrics for fixed-eect models. . . . . . . . . . . . . . . . . Parametric regression estimates of mixed-eect models. . . . . . . . . . . Performance metrics for mixed-eects models. . . . . . . . . . . . . . . . Performance metrics for fixed-eect models, including developer. . . . . . Performance metrics for fixed-eects models for external validation. . . . 42 44 49 52 54 56 57 57 58 58 73

4.10 Performance metrics for mixed-eects models for external validation. . . A.1 Akaike information criterion. Fixed-eect models. . . . . . . . . . . . . .

A.2 Standard deviation of the residuals for internal validation predictions. Fixedeect models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 A.3 Kendall rank correlation coe cient for internal validation predictions. Fixedeect models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 A.4 Standard deviation of the residuals for external validation predictions. Fixedeect models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 A.5 Kendall rank correlation coe cient for external validation predictions. Fixedeect models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 A.6 Akaike information criterion. Mixed-eects / frailty models. . . . . . . . 78 A.7 Standard deviation of the residuals for internal validation predictions. Mixedeects / frailty models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 A.8 Kendall rank correlation coe cient for internal validation predictions. Mixedeects / frailty models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 xv

A.9 Standard deviation of the residuals for external validation predictions. Mixedeects / frailty models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.10 Kendall rank correlation coe cient for external validation predictions. Fixedeect models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.11 Standard deviation of the residuals for internal validation predictions on filtered dataset. Fixed-eect models. . . . . . . . . . . . . . . . . . . . . . A.12 Kendall rank correlation coe cient for internal validation predictions on filtered dataset. Fixed-eect models. . . . . . . . . . . . . . . . . . . . . . A.13 Accuracy of fast/slow classification of the models. . . . . . . . . . . . . .

81 82 83 84 85

xvi

List of Figures
3.1 3.2 3.3 3.4 3.5 3.6 4.1 4.2 4.3 4.4 4.5 4.6 Dierent types of censored observations. . . . . . . . . . Right censored data. Representation of the dataset under Representation of skewed distributions. . . . . . . . . . . True eect of fixed variable on the decision (from [6]). . . True eect of random variable on the decision (from [7]). Representation of the truncation process. . . . . . . . . . . . . . study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 24 25 33 34 39

Kaplan-Meier and cumulative hazard curves on the empirical data of the first three releases. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Log transformation on the x-axis of Figure 4.1 for clarity. . . . . . . . . . Survival curves of each distribution compared to the empirical data. . . . Cumulative hazard curves of each distribution compared to the empirical data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Artificial truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Accuracy fluctuation over dierent fast/slow thresholds. . . . . . . . . . .

45 46 50 51 59 64

xvii

Chapter 1 Introduction
1.1 Motivation and problem statement

Computer software development consists of multiple stages, from requirements engineering, to implementation, initial release, and up until the end of further software support [36]. A significant amount of resources during software development is spent on bug fixing and feature implementation. The expected time necessary to fix a single bug or implement a specific feature -- commonly referred as time-to-fix or time-to-deliver, respectively -- is short. However, the large volume of bugs and features involved in software development create logistical problems to stakeholders involved in software development. Misestimating the amount of time needed for resolving reported issues might result in either delay of software launch and/or the launch of a software with degraded quality. Accurate estimation of time-to-deliver can result in e cient resource allocation, as problems expected to be lengthier to solve will have more resources allocated to them. Such improvements can result in significant cost savings, through reduced resource utilization, earlier launching, improved product quality and customer satisfaction. A number of attempts to estimate the expected resolution time can be found in the computer science literature [4, 39, 30, 20, 54, 51, 3, 43, 1, 19]. However, the predicting accuracy of these studies is not adequate. The methodological rigor of these methods varies. Researchers studied the expected time-to-fix using methods as simple as descriptive analysis [33, 53], to sophisticated prediction models that incorporate regression modeling, neural networks, machine learning and survival analysis [39, 20, 54, 3, 43, 19]. 1

Chapter 1. Introduction

1.2. Objective

An extensive review of the literature on the topic can be found in Chapter 2.

1.2

Objective

The primary objective of this study is to generate models that can predict with improved accuracy the development time necessary for a newly reported bug or feature to be resolved. Secondary objectives are (i) understanding which attributes aect development time the most, so that the processes can be altered (to reduce time-to-deliver) and (ii) predicting which reported bug or feature will be resolved below a time threshold (fast) and which above the threshold (slow). In order to reach these objectives, we answer the following research questions: RQ1: How incorporation of random-eects and/or censored data influence performance of a model that predicts time-to-deliver? RQ2: How geographical, churn, and complexity factors aect the duration of the time-to-deliver? Time-to-deliver is the primary outcome of this study and is defined as the dierence between the time-stamp that an issue is reported as delivered and the time-stamp that the report was submitted and assigned to a developer. From here on, we use the term time-to-deliver for both bug fixes and feature implementations.

1.3

Proposed Solution

Time-to-deliver data belong to the family of time-to-event data. Time-to-event or survival data, as they are also known [22, 35], can be found when the outcome of interest is the time elapsed from a starting point until an event of interest takes place. Consequently, the analysis and interpretation of this type of data is referred to as time-to-event or survival analysis. This type of analysis is mainly used in biostatistics and epidemiology and relates to the time between disease onset and death (or another specific event). Such analysis is also used in engineering, where it is referred to as reliability analysis, 2

Chapter 1. Introduction

1.3. Proposed Solution

and studies the time until failure Â­ mainly for mission critical equipment and material [17]. One of the distinctive characteristics of survival data is that observations of the outcome of interest (duration outcome) may be incomplete. In the case of the time-to-deliver variable, it is a reasonable assumption that prediction models can be applied at a certain time point where some of the issue reports are still open Â­ i.e., ongoing. The development process might have started for them, however they have not been resolved yet. Such observations are termed as censored and essentially indicate incompleteness of the data. We are going to incorporate this incomplete information into our models, in order to enhance our sample size and contribute in the process of predictive modeling. More details regarding censoring and its formation is provided in Chapter 3. Skewness of the variable of interest is also highly prevalent in time-to-event data. A symmetric (and, preferably, bell-shaped) distribution is what ideally a researcher would expect to see from the data under study. However, extremely large time periods, in combination with the fact that time-to-event are bound at zero, will likely result to an asymmetry. This is defined as the skewness of the distribution and may vary according to its direction (left/right skewness) and its degree (e.g., moderate, extreme). We will deal with it by using and experimenting with dierent parametric distribution models. More examples and discussion of how to address such issues is given in Chapter 3. Finally, often in a sample, multiple time-to-event observations might belong to the same subject/individual. This results in some correlation between the observations. In the time-to-deliver literature, the use of explanatory variables have been tried to be accommodated to resolve this correlation, such as the experience or reputation of the developer [21, 4]. In addition to the fact that there is a dierence of characteristics that describe every developer, we can try to estimate a model where we acknowledge that there are remaining dierences between subjects (e.g., developers) which are unobservable. In practice, this is accomplished by incorporating random-eects parameters in survival models, which after they are combined with fixed-eect parameters, result in a mixedeects models (or frailty models in the survival context). We are dealing with this issue by introducing the developer as the random-eects term into our models. Mixed-eects and frailty models are discussed in Chapter 3. 3

Chapter 1. Introduction

1.4. Novelty

1.4

Novelty

To the best of our knowledge, this is the first study utilizing parametric survival analysis and mixed-eects modelling on time-to-deliver data in software engineering. Although other researchers have relied on survival analysis tools in the past, we are presenting a comprehensive and replicable methodological approach. We try to cover most scenarios of data structures and limitations that can be faced with this type of data. Namely (i) censoring of the time-to-deliver variable, (ii) skewness of the distribution of time-to-deliver and, (iii) hierarchical structure of the time-to-deliver variable. i Censoring of the response variable is a factor that can frequently exist in timeto-event data, especially when dealing with real-world data. The amount of information that these incomplete observations include can potentially improve the predicting power of the model and should not be disregarded. ii The skewness of the distribution of the response variable is also a considerable factor that might aect the final outcome. Transformations are a potential solution to the skewness problem which we will explore along with alternative options, such as assuming dierent distributional shapes for our data. iii Mixed-eects models have not been extensively used in computer science. We are investigating in this study whether adoption of such methods significantly improves the models' predictive accuracy More details and definitions of the terms that are discussed in the last two Sections are provided in Chapter 3.

1.5

Contribution

The contributions of this work can be summarized as: Â· A way of successfully identifying and understanding the relationship between the dierent explanatory variables and the response variable, in the domain of defect prediction and quality assurance. 4

Chapter 1. Introduction

1.6. Outline

Â· A novel approach, of building a predicting regression model for time-to-deliver data while leveraging incomplete information and random-eects. Â· A prototype tool implementing this novel approach, listed in Appendix B, that can be used by practitioners and academics.

1.6

Outline

In Chapter 2 similar work and other relevant studies are discussed. Chapter 3 introduces the concepts of the statistical methods used, describes the data and provides the methodological approach followed in the analysis. Chapter 4 presents the results of the analysis. Finally, in Chapter 5, a summary of this study is provided, along with a conclusion and a scope for intended future work.

5

Chapter 2 Literature Review
This chapter reviews relevant published studies, where researchers were tackling the same problem: predicting the time necessary to deliver an issue report. We reviewed studies that focused on the general aspect of quality assurance and the number of bugs remaining in a system, as well as studies with an objective to predict the time-to-deliver of a particular issue report. We focused our discussion on the studies' findings as well as the methodological approaches followed. A number of dierent methodologies have been explored, and in many cases were compared to each other in the past. From machine learning algorithms, including NaÂ¨ ive Bayes, Bayesian Networks, and Decision Trees to neural networks and support vector machines, as well as more traditional methodologies and statistical tools for data analysis.

2.1

Software quality assurance

Traditionally, in software analysis and software testing, researchers focused on the number of remaining defects before launch [15, 37, 27]. Such prediction has been shown to improve quality in a variety of aspects, but at the same time it has also been identified as a fundamental challenge in software engineering; better development management, cost e ciency, improved resource allocation, better development coordination and, in general, final software quality improvement, are only some of the acknowledged advantages. Two of the most cited studies in this area come from Fenton and Neil [15], and Lessmann et al. [37]. Both studies are comparing dierent defect prediction models, 7

Chapter 2. Literature Review

2.1. Software quality assurance

illustrating the importance of such an in advance knowledge. Fenton and Neil [15] compared and criticized existing studies and the models amongst them. They pinpoint that novel and methodologically concrete approaches are essential for defect prediction, while empirical studies are of lesser importance in defect prediction domain. However, based on the mistakes that have been made in this domain, they try to lead future researchers into successfully deciding the appropriate model specifications and the data to work with, in the inevitably di cult field of defect prediction. Lessmann et al. [37] tried to benchmark dierent classification models and then propose the best ones suitable for defect prediction. They compared a total of 22 classifiers, based primarily on the area under the receiver operating characteristics curve (AUC) and utilized static code metrics over 10 dierent open sourced datasets. Since their experimental results did not yield significant dierences among the top performing models, additional characteristics, such as computational e ciency, ease of use, and comprehensibility were also included in model selection. However, the information generated just by predicting the number of expected defects is not su cient since there is always a need for further improvements in the software development domain. That is the reason why, more recently, researchers have extended their studies in predicting the time necessary for a defect to get fixed as well as the time needed for new functionality to be developed and incorporated. Machine learning, more sophisticated artificial intelligence approaches, and statistical analysis techniques have been extensively used and compared for time-to-deliver prediction [4, 9, 20, 54]. The rationale behind any method used, is to be able to transfer previous knowledge coming from the already observed data into future similar occurrences. The rationale is also based on the assumption that the past collected data, from previous development cycles or even from dierent software systems, can assist in the prediction process, no matter the approach. Consequently, we rely on the assumption that the main contributing factors and conditions are reasonably homogeneous and robust. Although the approaches might dier, the attributes utilized to achieve this goal across studies are, to a great extend, similar. The information provider in the majority of previous studies is the bug tracking system [47], also commonly referred as issue tracking system, which is an essential component in a well organized software development infrastructure. The records of a tracking system are organized in a database and contain information, such as, the type of the issue, title, description, the time reported, the user 8

Chapter 2. Literature Review

2.2. Number of bugs remaining

submitting the report, the developer that it was assigned to, various metrics related to the changes that took place, the priority and severity of the issue. This type of software is usually integrated with other project management tools, being able to provide an even richer set of attributes. In a recent study, Canforna et al. [9] used a survival analysis technique known as the Cox proportional hazard model (see Chapter 3). However, instead of estimating resolution times, they tried to predict the hazard of the survival of a bug, from injection until resolution. The time interval that they studied, includes the time when the bug is reported, but does not necessarily consider it as the starting point of measure. This work has been a valuable step in modeling expected resolution-time within a survival analysis framework. However, the study failed to account for a number of methodological challenges in the presence of properties in the data structure. Examples of such violations include the presence of censoring, the extreme skewness of the duration data, the limitations of proportional hazard models in generating time predictions, and the presence of within group dependence (e.g. multiple bugs fixed by the same developer). Although the proportional hazard models do account for the limitations mentioned above, they were not incorporated or mentioned in this study. These violations can have important consequences on the estimation of expected resolution-time and, therefore, their impact needs to be considered. The impact of design and code reviews on software was studied by Kemerer et al. [27]. Although the scope of their work is dierent, they utilized regression models and introduced mixed models having the developer as the random-eects attribute; acknowledging the superiority and the advantages of such use. Finally, Schalken et al. [45] also proved the superiority of the mixed-eects models, while they investigated the success of a software process improvement program, in a large financial institution. Using hierarchical linear models they noticed a great enhancement in the sensitivity of analysis of the empirical data they utilized.

2.2

Number of bugs remaining

The majority of studies in this area focused on the number of bugs remaining or are yet to be discovered, within a future time span [15, 37]. These studies have extensively 9

Chapter 2. Literature Review

2.3. Time-to-deliver estimation

utilized tree-based classification methods, Bayesian belief networks, neural networks, analogy-based approaches, and statistical procedures. Sometimes though, researchers mention the importance of being able to predict the time-to-deliver as supplementary to the number of bugs that are expected in the future [43]. We will be focusing on the estimation of the former. Furthermore in our study, in addition to the bug reports, we consider the functionality reported issues as well. In 2010, D'Ambros et al. [14] tried to provide a benchmarking pattern for comparison of defect prediction approaches in terms of accuracy, complexity, and the type of data these require to make predictions. The need for an established comparison and benchmarking methodology across dierent methods, illustrates the plethora of studies in the field of defect prediction and the necessity of prediction reliability.

2.3

Time-to-deliver estimation

In an attempt to provide more detailed predictions around bug resolution, researchers focused on the prediction of time-to-deliver rather than predicting the number of remaining bugs. Dierent approaches, others dealing with the time-to-deliver itself, or in an attempt to simplify the problem just classify a new report as a slow or quick fix. Nevertheless, all of them dealing with the duration of the resolution of a single reported issue [1, 4, 39, 30, 20, 54, 51, 3, 43, 19]. In particular, Bhattacharya and Neamtiu [4], based on prior studies and the methodologies that were followed, tried to illustrate that the models constructed in these studies cannot be easily generalized and adapted into other (external) systems. They used multivariate and univariate regression testing, to assess the predicting power of previously built models, while predicting defects on external datasets. The datasets that were used belong to open source projects that are commonly used for defect prediction. The results of this study show that the predictive power of the models (that they assessed) ranges from 30% to 49% and identified the necessity of finding more appropriate explanatory variables, other than bug severity, bug dependencies, number of developers involved and patches applied for a fix. The significance of Bhattacharya's and Neamtiu's [4] findings, that dierent software have significant dierences in terms of resolution times, can be observed in two other 10

Chapter 2. Literature Review

2.3. Time-to-deliver estimation

studies; the work of Anbalagan with Vouk [3] and Panjer [43]. The best explanatory variables, in terms of predicting the time-to-deliver, diered in each study; Anbalagan and Vouk identified the number of involved developers as the most relevant one, while for Panjer, a combination of the commenting activity in the bug report along with severity, component, and version1 are the most important ones. According to these findings, we take under consideration the necessity of more relevant and correlated to the time-todeliver explanatory variables and study their correlation, as well as their performance on the models built. Additionally, we consider within group dependence on some of the explanatory variables and decide to act accordingly. Kim and Whitehead [30] use the time needed to fix a bug as a criterion for the software's quality. They identified and related the quality of the component that a fix was necessary, the file that the component belongs to, up to the quality of the software in general. They used this information solely to measure the quality of the software components, and did not try to estimate the bug-fix time. The more time is needed for a bug to get resolved, the worst the quality of the component. They concluded that the time-to-deliver is an important measure for quality assurance, after analyzing bug fix statistics for two projects. In a similar study, Koru et al. [33] consider the characteristics of the components and the relationship that they might have with bug existence. They found a strong correlation between component size and bug proneness. Interestingly, they focused on the size of the classes themselves instead of the size of the file, which is similar to the classification of components and functions that we have in our dataset. They utilized a Cox proportional hazard model to estimate the eect of the size of a component on defect proneness. Although we follow the same methodological approach, by utilizing Cox proportional hazard models, we focus on the time-to-deliver instead. Marks et al. [39] identified that the most correlated attributes with the time needed to fix a bug, are dierent between two separate open source software (Eclipse and Mozilla). Additionally, even within the same software, time progression can change the most correlated attributes. The most relative ones in this study were identified by performing sensitivity analysis on the attributes of each software. Components related to the bug, developer, and reporter, as well as the description of the bug are the most important "di1

Variables are listed in the order of their significance.

11

Chapter 2. Literature Review

2.3. Time-to-deliver estimation

mensions" aecting resolution time. They also split the fix time into classes, representing fast, medium, and slow fixes. The random tree classifier that they used could correctly classify 65% of the bugs. We are also evaluating the eectiveness of each attribute and consider the dierences that inevitably exist, not only between dierent software, but at the same time within the same software. Although in this study we will not be evaluating the eectiveness of a model in an external dataset, we acknowledge this limitation and plan to tackle this in the future. The importance of time progression is also discussed in the study of Habayeb [19]. The author studied the eect of temporal characteristics by building a Hidden Markov model that, compared to previous approaches, performs better in predicting the timeto-fix on a Firefox dataset. We take Habayeb's study as one of the most relevant in terms of estimating the time-to-deliver and try to show some considerable dierences in the time-to-event family of the data that this and other studies have been dealing with. However, we are using dierent dataset, attributes, and tooling. In an empirical study, Hewett and Kijsanayothin [20] used various machine learning and computational intelligence techniques in order to achieve optimal results in predicting time-to-fix. In the same way as Marks et al. [39], they tried to understand the reason and the source of the reported issue and categorize them as low, medium, or high duration. For attribute selection and evaluation, they used a wrapper method based technique, while we will be statistically analyzing the correlation of each attribute to the time-tofix. Interestingly, they discuss the importance of the pre- or post-release information, that we are also considering in our hypotheses, as discussed in Section 3.1. Another empirical study, on commercial software this time, was conducted by Zhang et al. [54]. Their goal was to classify a newly reported bug in two classes: slow or quick, based on a preset threshold. They rely on a Markov model-based method to predict the number of bugs that will be fixed in the future, and they also introduce a time prediction approach. Afterwards, they propose a method based on k-Nearest Neighbour and compare its e ciency to other commonly used machine learning techniques (Bayesian Networks, NaÂ¨ ive Bayes, Radial Basis Function Network, and Decision trees). Using three dierent attribute evaluation techniques for categorical data (Chi-square, Gain ratio, and Information gain) they conclude that bug submitter/originator and developer have the most predicting relevance. An other interesting approach, but not so related in terms of methodology, was fol12

Chapter 2. Literature Review

2.3. Time-to-deliver estimation

lowed by WeiÃ et al. [51] They were able to achieve good prediction estimates of fixing eort by utilizing text similarity techniques; fixing eort is defined as the actual personhours that will be needed to deliver the fix. They used Lucene, an Apache text similarity measuring engine, to identify similar past issue reports and be able to categorize a new one. In addition to the generalization problem that was discussed above, researchers have also identified factors impacting bug fixing time that are hard to determine or extract from the issue tracking systems. One of the most common issues in this category is the time that intervenes between the report of the issue and the actual time that a developer starts working on it [53]. Also, duplicate reports is another considerable cost ine ciency [24]. Finally, in the Software Engineering domain, regression analysis has been used to estimate and show dependability of various attributes with the quality of software [29], bug prediction [14, 5], defect density [41], as well as time-to-fix duration [4, 9, 20, 30].

13

Chapter 3 Methodology and Implementation
Based on the literature discussed in the previous chapter, we provide the methodology followed to achieve our goal. We first state the hypotheses in Section 3.1. Subsequently, we discuss the general principles underlying regression modelling in Section 3.2. Next we continue with regression estimation approaches and the limitations associated with using regression analysis to test our hypotheses in Sections 3.3 and 3.4 respectively. Finally, we introduce the concept of survival analysis as the methodological framework we followed to address the limitations of regression analysis when using time-to-event data, in Sections 3.5, 3.6 and 3.7.

3.1

Theoretical framework

In order to answer the main research question of this thesis (the prediction of time-todeliver) we first need to identify the parameters that will constitute a bug report or a feature request as more likely to be resolved. Can the management take corrective actions towards a faster, more e cient, and more qualitative resolution? For that purpose we formed hypotheses on the relation of a number of explanatory variables with the time-to-deliver a bug or feature, based on the literature and input from the provider of the data. On that basis we hypothesized the following: The binary attribute Pre/Post release shows if the issue report was completed before or after the current release that our dataset comes from. If this information is available 15

Chapter 3. Methodology and Implementation

3.1. Theoretical framework

from the time that the bug is reported, we would expect that bugs reported as post release would need more time to be delivered, mainly because there are no time constraints. Respectively, pre release indicated reports, have a specific due date that need to be resolved. Because of this time constraint, we would expect them to get resolved faster. H1 Issue reports that are delivered as pre-release, are expected to get resolved faster than post-release. As shown in [4, 20] the size and the number of components involved in a report can aect the time-to-deliver. Therefore, we would expect bug reports that involve more complex or larger components to require more time to be resolved. Consequently, a bigger number of components involved in a single issue report is also more likely to require longer time to resolve. H2 The number of components involved in the development if an issue report, is expected to be positively correlated to the time-to-deliver. The more the components Â­ the longer the time necessary. Severity and priority of the issue report is a required field in most of the bug tracking systems. This information may also connect to the first element (H1 ) of this list. If the manager/developer know that this task has low severity/priority and therefore can wait until the next release, might lead to a longer duration of resolution. H3 Higher severity and/or priority of the issue report is expected to result in a faster resolution. As it was described before, we are investigating all the possible reported issues from the tracking system. Dierences might occur and be expected for dierent types of reports. Features in general are not always highly ranked in the priority list, in contrast to bugs, that need to get resolved most of the times before the next release. H4 We would generally expect bugs to be resolved faster than features. A small predefined tag/description is given to each issue report. This can be considered as a subcategory of the bug/feature category. 16

Chapter 3. Methodology and Implementation

3.2. Regression analysis

H5 We would expect some issue reports, given their symptom tag, to be more complicated Â­ hence take more time to get resolved. Experience, workload, development type, are only some of the characteristics that can dierentiate between developers. As in previous studies [4, 18], we expect that developers are also a factor aecting the time-to-deliver. We would also expect that within group dependence of the developers, is also a significant factor on the duration of the time-todeliver, therefore we will be incorporating this assumption into our models. H6 The developer assigned to an issue report is expected to be a significant factor affecting the time-to-deliver. Since the development team of the product we are studying is distributed around the world, the country of origin might also aect the time-to-deliver. Dierences in the culture, characteristics Â­ as defined in H6 , as well as dierences in the size of the team in each country, can be contributing in the resolution time variations. H7 Issue reports developed in dierent countries are expected to have variations in the time-to-deliver. As most of the previous studies did, we will use multivariate models towards our goal. However, univariate models will also be built initially, in order to form a baseline model estimation.

3.2

Regression analysis

Regression analysis is a statistical process for estimating the relationship between two (or more) variables [32, 40]. Regression analysis allows us to understand the eect of the explanatory variable(s) on the response variable. Depending on the functional relationship assumed between the response and the explanatory variables, models can be described as linear, generalized linear and nonlinear. In regression analysis, variables are categorized into two sets; the response variable and the explanatory variable(s). Response or dependent variable is the variable of interest that is being measured in the experiment. Explanatory or independent are those variables 17

Chapter 3. Methodology and Implementation

3.2. Regression analysis

that are assumed to be associated with, or can be used to inform predictions of, the response variable. Regression modelling estimates the level of association between the response and the explanatory variable(s). This relationship can be used for predictions. However, as many researchers showed in the past [3, 43, 39, 20], being able to draw inference and understand a relationship that describes a situation might be worth more than using this relationship for prediction purposes.

3.2.1

Linear Regression

Linear regression is the simplest form of regression analysis, where the response and explanatory variables are assumed to be linearly associated. In linear regression, the objective is to find the (straight) line that has the smallest distance from every data point of the response variable [40]. Linear regression is also defined as the study of the linear and additive relationships between variables. Assuming n is the number of observations in a sample, y is a vector of size n capturing the response variable and x is a vector of size n capturing the explanatory variable. Subsequently the linear regression model can be written as: y = 0 + 1 x + , (3.1) where, 0 is the intercept and captures the value of y when x is equal to zero, 1 is the slope and shows the change on y for a unit change on x, and  is the residual or error term which captures the distance of the observed from the estimated value of y . 0 and 1 are also referred as the regression coe cients of the linear regression model. In case of p explanatory variables, the simple linear regression can be extended to a multivariable regression model: y =
0

+

1 x1

+

2 x2

+ ... +

p xp

+ (3.2)

= x + ,

where x is the explanatory variable matrix of size n  (p + 1) and is a vector of regression coe cients of size p + 1. Given estimates of , denoted ^, one can generate predictions of the response variable 18

Chapter 3. Methodology and Implementation

3.3. Regression estimation

y ^, given x, using the equation: E (y | ^, x) = y ^ = x ^. Using 3.2 and 3.3, the error term  can be defined as  = y y ^. (3.3)

3.3

Regression estimation

There are several estimation approaches for regression models. Although the majority of regression approaches are solved using likelihood - based methods, simple problems like the linear regression can be solved with Ordinary Least Square (OLS) regression methods.

3.3.1

Ordinary Least Squares

There are multiple ways of obtaining the best linear unbiased estimates (BLUE) of . The simplest estimation method is the Ordinary Least Squares method. This method estimates the regression coe cient that minimizes the sum of the squared distances between the observed response variable y and the predicted variable y ^. In other words, the OLS method minimizes the sum of the squared errors (SSE): ^ = arg min "
n X i=1

(y

x )2

#

With OLS method, we are able to get an estimate of the regression parameter . As an estimate, it is accompanied with statistical properties. One of these properties is the variance, which can tell us how precise is the estimate. The variance of the errors assists in the estimation of this variance. The coe cients can be estimated using the function: ^ = ( xT x) 1 ( xT y ) , 19 (3.4)

Chapter 3. Methodology and Implementation

3.3. Regression estimation

where

T

denotes the transpose of a vector/matrix; the variance is approximated by: ^2 = T  (y = n p x ^)T (y n p x ^) , (3.5)

where ^ 2 is the variance of the sample and p is the number of parameters being estimated for the model. Estimation of the linear regression parameters with OLS requires a number of assumptions hold: (i) The residuals are independent, there is no statistical correlation between them. (ii) The residuals follow a normal distribution. (iii) The relationship between the response and the explanatory variables is linear. For every explanatory variable there is a function that explains the response variable as a straight line. The slope of each of these lines is not related to the others. The eect of each straight line function to the response variable is additive to each other. This is also referred as additivity. (iv) Homoscedasticity of the residuals. Also known as homogeneity of variance of the errors. Some of these assumptions can be relaxed (e.g normality) with some loss of inference (e.g., no p-values).

3.3.2

Maximum Likelihood

If we assume that the residuals are normally distributed, we could try to estimate the parameters that explain best the distribution of the residuals. The probability that a residual term comes from a normal distribution with mean 0 (since the residuals are centred around zero) and variance 2 is: f ( | , ) = 1 p e
(y x )2 / 2
2



2

.

(3.6)

20

Chapter 3. Methodology and Implementation

3.4. Understanding the limitations

The probability that all residual terms come from the same distribution is: M LE = f (1 | 1 , ) Â· f (2 | 2 , ) Â· Â· Â· f (n |
n,

)=

n Y i=1

f ( i | i , ) ,

(3.7)

which is referred to as the likelihood function. The parameters and that maximize 3.7 are called the maximum likelihood estimates. The maximum likelihood estimator dierentiates and sets the function equal to zero, in order to find the maximum value. It can be shown [46] that the maximum likelihood estimates are:
M LE

= ( xT x) 1 xT y

(3.8)

and
2 M LE

=

(y

x ^)T (y n

x ^)

.

(3.9)

3.4

Understanding the limitations

Time-to-event variables have often characteristics that are inherently not in-line with the assumptions of the linear regression model. Examples of such characteristics are: Â· The presence of censored, incomplete, or missing data; Â· Clustering of observations in hierarchical way (i.e violating the assumption of independence of observations); Â· Skewed, zero-constrained distribution. We will be referring to each item of the list above later in this chapter and provide some insights and explanations on the approaches that we followed in order to overcome them.

3.4.1

Censoring

Censoring exists when some of the observations of the response variable are incomplete due to some cause. Due to this incompleteness, the time-to-event is not accurately known. Although these observations are not "complete", they still include some useful 21

Chapter 3. Methodology and Implementation

3.4. Understanding the limitations

information. There are statistical models that can incorporate this information. Although exclusion of the censored observations enhances simplicity, it might lead to [38]: Â· biased results, Â· loss of e ciency (smaller sample size), Â· increased variance of the estimated values. Types of censored data Below, we use as an example a software to illustrate the dierent types of censoring. We acknowledge two major time points during the release's development: TA as the start time of our observation period and TB as the end of the observation period. Furthermore, ta is the date of reporting and tb is the date of delivery of the issue report. Â· In a case that an issue is reported between these two time points TA  ta  TB and is also resolved within them TA  tb  TB , the observation is complete (i.e., not censored). Â· In a similar case, an issue that is reported between these two time points TA  ta  TB , but, due to time constraints or low priority/severity of the issue, the report could not be resolved before TB . In this case we have a right censored observation in our response variable. Â· Respectively, a left censored observation exists in the case where the time of resolution is TA  tb  TB , but we do not know the submission time of the report. Â· Finally a combination of right and left censoring, results to an interval censored observation, where it is known that the report and/or resolution of the issue happened within an interval time period, however the actual time point is not exactly known. Figure 3.1 outlines the dierent types of censoring, described above. The most common censoring type is right censoring. Especially in the type of problem that we will be dealing with, it is unlikely to have an issue report that the submit date is unknown. On the other hand, it is very likely to have a number of reports still undergoing 22

Chapter 3. Methodology and Implementation

3.4. Understanding the limitations

6 5

left censored right censored

observations

4 3 2 1

interval censored

time

Figure 3.1: Dierent types of censored observations. development and therefore having their resolution time as unknown. Figure 3.2 represents the nature of the data that we will be studying. For this study, we created an artificial sample based on the original dataset, where a certain percentage of the data was assumed to be censored. We tested methods that account for censoring to understand the discrepancy between the "True" findings of the model when censoring is not present vs. the "real-world" scenario where a proportion of the data is censored.

3.4.2

Skewness

As we have seen above, one of the assumptions of the regression model is the normality of the residuals. Normally distributed variables are accompanied with the following properties: Â· The normal curve (bell) is symmetrical around its mean Âµ, Â· The mean divides the area into two equal parts, 23

Chapter 3. Methodology and Implementation

3.4. Understanding the limitations

6 5

observations

4 3 2 1

time

Figure 3.2: Right censored data. Representation of the dataset under study. Â· The total area under the curve (integral) is equal to 1, Â· It is completely defined by the mean and standard deviation . In time-to-event data, this assumption is often violated: Despite that most events occur in short durations, there are often a few events occurring in disproportionately long durations. In addition, as time cannot be negative, time-to-event data are bound to zero. Distributions that do not follow the symmetry of a normal distribution are referred to as skewed or asymmetric distributions. A distribution can be described as: Â· Left or Negative skewed - because its tail extends to the left or to the negative values of the x-axis; Â· Right or Positive skewed - because its tail extends to the right or to the positive values of the x-axis; Â· Extreme tail (positive or negative) - as by its name, this is an extreme condition of the previous categories, in a case where the tail stretches to the left or right of the horizontal axis; 24

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

An example of a left and right skewed distribution is given in Figure 3.3 compared to the normal distribution (or bell curve).

normal left skewed right skewed

Figure 3.3: Representation of skewed distributions.

Transformations As skewness results in violation of the assumptions of regression models, methods have been proposed that rely on transformations of the response variable in order to resemble more closely a normal distribution. These transformations can help with visually inspecting the data as well as applying to the (transformed) data more standard regression approaches. Despite their usefulness, transformations have some caveats too. Transformations imply that the studied relationships now, is that between the explanatory variables and the transformed response variable. In addition, transformations are not always easily invertible. Being able to revert back to the measuring scale that you started with is a very useful feature, most importantly because it gives the capability of easy comparison of the results.

3.5

Survival Analysis

Survival analysis (also known as duration, failure or reliability analysis in engineering) focuses on analyzing time-to-event data. Questions like: "How much time until an event 25

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

(e.g., death) will occur?", "How much time will it take for a new bug to be reported?" or more appropriately for our case: "How much time will it take to resolve a reported issue?", as well as: "What is the proportion of bug reports that will be closed after a certain time threshold?", fall under this category. Survival time or lifetime, is defined as the duration from a specific time point at which we start the observation for an event to occur until the time the event of interest occurs. In our case, the former is the time point that an issue is being reported, which can either be a bug or a functionality request, and the latter is the time point that this report is submitted as resolved. Below we introduce concepts that are central to survival analysis; the survival function, Kaplan-Meier estimator of the survival function, the hazard function, the Cox proportional hazards models, and the accelerated failure time models.

3.5.1

Survival function

The survival function is a monotonic, non increasing function and is defined as the probability that a specific subject will not experience the event of interest until a specific time t. The survival function, for a given time t is defined as: S (t) = P (T > t) = 1 F ( t) , (3.10)

where T is the time when the event occurs, and F (t) is the cumulative distribution function (c.d.f.), i.e., F (t) = P (T  t). The survival function is the complementary of the c.d.f. F (t). The properties of the survival function are as follows:

S (t) 2 [0, 1], S (0) = 1,
t!1

lim S (t) = 0, S ( t2 ) , t1  t2 . 26

S (t1 )

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

3.5.2

Hazard function

The instantaneous risk of an event at time t is called hazard. The hazard function or hazard rate is defined as h(t), which is a conditional probability; i.e., it is conditional for the event to survive until time t. The formal definition is: h(t) = lim P r(t  T < t + dt|T dt!0 dt t) . (3.11)

In other words, we can denote the hazard function as the probability of an event to happen within a fraction of time Â­ a small time interval [t, t + dt). Like the hazard function, the cumulative hazard function H (t) is also not a probability. It represents the accumulation of hazard over time and is given by: H ( t) = Z
t

h(t)dt.
0

(3.12)

The relation between the cumulative hazard function 3.12 and the survival function 3.10 is: H (t) = ln S (t). (3.13)

3.5.3

Non-parametric estimators

Non-parametric estimation is a statistical approach on fitting the empirical data without any theoretical constraints or assumptions. The Kaplan-Meier survival and the NelsonAalen cumulative hazard are both dierent techniques to graphically visualize the distribution of time-to-event data. Since the cumulative hazard and the survival functions are related, based on equation 3.13, the Kaplan-Meier and the Nelson-Aalen estimators can be used interchangeably. It has also been proven that they are asymptotically equivalent [16]. However, there are some dierences and advantages that depend on the sample size that is being studied and other factors [11]. We will not be giving any further details regarding their dierences, because we are only using these estimators for a graphical representation of the empirical data. 27

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

Kaplan-Meier estimator of the survival function The most common way of estimating the survival function non-parametrically is the Kaplan-Meier survival estimator (product-limit estimator) [26]. It is a non-parametric or empirical method of estimating S (t) for right-censored data (or non censored data). A Kaplan-Meier estimator plot is a strictly non-increasing step curve, that can incorporate right censored observations. This plot is built by sorting all the records by their duration, from shortest to longest. Then, cumulatively sum the events and subtract them from the total number of subjects at risk of experiencing the event. Although the event information for censored observations is not available, they contribute to the at-risk population until they are censored. Graphically, they are illustrated as cross-points. As time progresses, the number of observations that remain survived keeps decreasing.

Nelson-Aalen cumulative hazard estimator A non-parametric estimator of the cumulative hazard rate, which can also incorporate the presence of censored data, is the Nelson-Aalen cumulative hazard estimator. In contrast with the Kaplan-Meier survival estimate, the Nelson-Aalen estimate is a strictly non-decreasing, step curve. The curve starts from zero, since the hazard at time zero is equal to zero and accumulates to infinity as time progresses. This plot is essentially accumulating the hazard at every given time.

3.5.4

Semi-parametric estimation

Cox proportional hazard models Cox proportional hazard models [13] is another family of (semi parametric) models for time-to-event data. More specifically, Cox proportional hazard models facilitate identifying a relationship between the hazard rate of an event and one or more explanatory variables. Cox models rely on the assumption that each explanatory variable x has a proportional eect on some baseline hazard h0 . The model is referred to as a semiparametric one since h0 is estimated non-parametrically while the 's are estimated under the assumption that they follow some distribution (i.e., parametrically). Mathematically, 28

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

the model can be expressed as: h(t) = h(0) Â· e
1 x1

Â·e

2 x2

Â·Â·Â·e

n xn

.

(3.14)

This relation is usually converted in a natural logarithm, to get advantage of its properties and transform the multiplicative equation 3.14 to an additive one: ln(h(t)) = ln(h(0)) +
1 x1

+

2 x2

+ Â·Â·Â· +

n xn .

(3.15)

The coe cient in the Cox model concept is interpreted as the eect in the hazard rate, which comes in contrast with the concept of the coe cient in a survival model. Therefore a positive implies higher risk, i.e., shorter survival time, while a negative implies lower risk, i.e., longer survival. The Cox model is fitted using a partial likelihood. Partial likelihoods are useful for the estimation of semi-parametric models. This likelihood function is maximized using the Newton-Raphson method [25].

3.5.5

Parametric estimation

Accelerated failure time models Until now, non-parametric and semi-parametric estimating methods for time-to-event data have been discussed. As already mentioned above, the Kaplan-Meier estimate of the survival function and the Nelson-Aalen cumulative hazard is the best way of graphically representing empirical data (non-parametrically) with incorporation of censored observations. For the estimation of associations between explanatory variables and censored time-to-event observations we introduced the Cox model; a semi-parametric estimator that relies on the hazard function. As we discussed above, the limitation of the Cox models is that inference is drawn on the hazard rather than the time-to-event level. An alternative method is the use of parametric survival models. As by their name, parametric models have all parameters of the models specified to be following a parametric distribution. This is however considered as one of their main disadvantages, the fact that a distribution has to be assumed for the values of the explanatory variables and consequently follow all of the distribution's properties. On the other hand, the main 29

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

advantage is that the estimated value is no longer a hazard, but the time-to-deliver that we mainly want to estimate. An additional advantage is the ability to extrapolate in the presence of censored observations. In parametric survival models, the residuals are assumed to be following a distribution that is more appropriate to the distribution of the data. While numerous distributions have been proposed for the use in time-to-event analysis [31], in this study we will be focusing on four of the most commonly used distributions: the exponential, the Weibull, the lognormal, and the loglogistic. The regression models for a matrix of explanatory variables x under each of the distribution assumptions can be generally specified as:

log(y ) =

AF T x

+ W

(3.16)

where AF T is the vector of coe cients and is a scale parameter whose interpretation is dependent on the distribution assumed. Finally W is the vector of residuals following a distribution that is dependent on the assumed distribution of y . Below we provide the specific model assumptions and parameter interpretation for each distribution assumed. In all cases y is conditional on the explanatory variables x. Â· Exponential Let the time-to-deliver variable y follow an exponential distribution with a probability density function (p.d.f) that is equal to: p( y ; ) = e
y

,

where represents the rate parameter. Under that assumption, it follows that the residuals W , in 3.16, follow a one parameter extreme value distribution. As the scale parameter is fixed and constant over time at the value of 1 ( = 1), AF T is the only vector of parameters to be estimated. AF T x captures the rate parameter, for the appropriate values of x.

30

Chapter 3. Methodology and Implementation

3.5. Survival Analysis

Â· Weibull If y follows a Weibull distribution: p( y ; , k ) = k  y k
1

e

(y/ )k

,

with as the scale and k as the shape parameter, then the residuals W , in 3.16, follow a two parameter extreme value distribution. In this case, the parameter is the scale parameter ( = ) and the shape parameter is equal to AF T x ( AF T x = k ). Â· Lognormal When we assume a lognormal distribution for the time-to-deliver y , the p.d.f. is given by: (ln y Âµ)2 1 p(y ; Âµ, ) = p e 22 , y 2 where Âµ represents the location and the scale parameters of the distribution. The regression model 3.16 has representing the scale parameter ( = ), AF T x the location parameter Âµ ( AF T x = Âµ), and W is assumed to follow a standard normal distribution. Â· Loglogistic Finally, if we assume that y follows a loglogistic distribution, the p.d.f. is: p( y ;  , ) = ( /)(y/) 1 , (1 + (y/) )2

where  > 0 and > 0 are the scale and shape parameters, respectively. In this case, in 3.16, is the inverse of the scale parameter (i.e., =  1 ) and the intercept captures the product of the scale and the shape parameters along with the coe cients of the model (i.e., AF T x =  ). W is assumed to be following a logistic distribution. All parametric models are estimated with the appropriate likelihood function. 31

Chapter 3. Methodology and Implementation

3.6. Mixed-eects models

3.6

Mixed-eects models

One of the assumptions of the regression models we have seen so far is that the observations in the study sample are independent of each other. For example, in our sample it is assumed that all issue reports are independent. However, reports that are dealt by the same developer are expected to be correlated as developer skills can vary. Hence the assumption of independence made in Chapter 3.3 is likely to be violated. A conventional solution would be to introduce dummy covariates to capture the eect of each developer on the time-to-deliver. However, this would imply the estimation of a large number of additional covariates which would complicate and potentially bias our regression estimates. In addition, it is often the case that we are not interested in the eect of the developers on the time-to-deliver, but only want to adjust or quantify the variation across developers. Finally, if the model is to be used as a prediction tool, introducing covariates for the observed developers would make the model unsuitable for predicting the time-to-deliver of a newly hired developer (i.e., out of sample prediction). A potential solution to the problem of independence assumption is the use of mixedeects models [7]. Mixed-eects models utilize both parameters that their true eect, which is the final eect to the model, is assumed to be fixed across levels (the fixedeect) as well as parameters that the eect is assumed to vary across levels with a given distributional pattern (the random-eects). The random-eects parameters can capture potential unobserved variation within levels (e.g., unobserved variation across developers) attributed to nesting or hierarchical data structures. Before moving forward and trying to explain the usefulness and the idea behind the mixed-eects models we further elaborate on the distinction between fixed-eect and random-eects models.

3.6.1

Fixed-eect models

In a fixed-eect model, it is implicitly assumed that the true time-to-deliver for each developer only diers due to the variation on characteristics of the issue report (e.g., pre/post release date). Any excess observed variation across developers is attributed only to the sample variation. In other words, every developer, no matter the dierences that inevitably exist between them, will aect the duration estimation the same. 32

Chapter 3. Methodology and Implementation

3.6. Mixed-eects models

In Figure 3.4 we can observe the eect of three dierent developers. The triangle on the x-axis represents the true average time-to-deliver in the model shown as Âµ and equal to Âµ = x. The true eect for each developer is the circle on the x-axes and it is common across developers. As described before, the eect of every developer on the time-to-deliver is the same on the model. However, the observed values for each developer are dierent Â­ shown by the squares. The assumption that a fixed-eect model makes is that, despite the true eect is the same, there might be variation across developers that is only attributed to the sample size. If we had enough (infinite) information, the observed eect (squares) would perfectly match their true eect (circles).

developer 1

developer 2

developer 3


Figure 3.4: True eect of fixed variable on the decision (from [6]).

3.6.2

Random-eect models

The fixed-eect model, as discussed above, assumes that all developers have the same eect on the duration of the time-to-deliver that we are studying. However, dierences in characteristics (such as experience, maturity, development skills, work load) might cause dierences in the time that every individual needs to deliver a resolved reported issue (dierent eect). In such cases, we decide to use this information as random-eects. The assumption that we make in this case is that the final eect of the developers is a normal distribution, shown at the bottom of Figure 3.5. In comparison with the fixed-eect approach, although our sample is still limited, we expect the mean of the existing sample to match the mean of the case of an infinite sample size. However, what we observe 33

Chapter 3. Methodology and Implementation

3.7. Diagnostics and validation

on Figure 3.5 is the final normal distribution of the final eect of the developers to the time-to-deliver, as well as the observed values (squares) for every developer.

developer 1 developer 2 developer 3


Figure 3.5: True eect of random variable on the decision (from [7]). Frailty models The way we are introducing the random-eects of a variable on our case, is through the concept of frailty [23]. Frailty models are an implementation of random-eects in survival analysis, for introduction of association and unobserved heterogeneity within the variable that is applied to. As described, we will be assuming a Gaussian distribution of the variable used in the frailty models.

3.7

Diagnostics and validation

After a model, or a series of models, have been fitted, it is essential to be able to assess them in various manners. Initially we want to make sure that the assumptions that were defined before fitting a model have not been violated. For example, in the simple case of a linear regression, the residuals should follow a normal distribution. This is easy to evaluate just by plotting the residuals and optically assessing the plot and its 34

Chapter 3. Methodology and Implementation

3.7. Diagnostics and validation

normality. However, for more complex cases, there are some additional tools that can help us compare dierent models. Diagnostics and validation practices presented in this section are only applied to the parametric models. The main reason of this decision is also the major advantage of the parametric models; the fact that the estimates are on the response variable and not hazards.

3.7.1

Akaike Information Criterion

The Akaike Information Criterion (AIC) [2] is a measure used to test the goodness-of-fit across models that are applied on the same outcome of interest and on the same data sample. It is defined as: AIC = 2k 2 ln(L), (3.17) where k is the number of explanatory variables used in the model incremented by 1 (i.e., number of variables + the intercept), and L is the maximum value of the likelihood function. When comparing two models, the one with the smaller AIC value is the one that fits better. As a basic notion while fitting a model, we might say that adding more explanatory variables will always make a model fit better, but will be trading against overfitting and overparameterizing our model. AIC calculates a trade-o between the number of parameters used and the incremental amount of variation explained by adding more parameters. The AIC of a model on its own does not provide a qualitative metric of quality of fit. AIC will only provide comparative information for a collection of models around the same variable of interest. For example, one can use AIC to select the best among parametric models, or among semi-parametric models. However, since the outcome is dierent for these two types of models (time-to-deliver for the fully-parametric and hazard for the semi-parametric), an AIC-based comparison between them is not valid. AIC is also used to assess the predicting contribution of each explanatory variable through the stepwise algorithm. A stepwise algorithm, is an automated process of variable selection, based on the goodness-of-fit (AIC). The algorithm identifies and returns the variables that are contributing su ciently to the improvement of the goodness-of-fit. 35

Chapter 3. Methodology and Implementation

3.7. Diagnostics and validation

3.7.2

R-squared

Another statistic that measures the goodness-of-fit of a model is the R-squared (R2 or r2 ) [42]. R-squared measures the closeness of the data to the fitted regression line and is also known as the coe cient of determination. It is defined as the fraction of the response variable variation that can be explained by the fitted linear model and is given by:
n X i=1

( yi ( yi

y ^i )2 , y Â¯)2 (3.18)

R2 = 1

n X i=1

where y Â¯ indicates the mean of the n real values yi , and y ^i are the predicted n values. R-squared values range between 0 and 1. The higher the value of R2 , the more variability is explained, leading to better fit of the model to the data (R2 = 1 suggests perfect fit). However, R2 is not a perfect measure of goodness-of-fit: e.g., it cannot detect overfitting of the model, indicate whether the explanatory variables of a model have an eect on the response variable, or assess statistical significance of explanatory variables. It can also be misleading in case of non-linear models [34, 50]. Therefore, we resort to other goodness-of-fit measures described below.

3.7.3

Residual standard deviation

After fitting the data with regression analysis, a way to quantify the goodness-of-fit, is by calculating the standard deviation of the residuals [12]. The definition of the residuals is given by equation 3.1; in essence, it represents the vertical distance of the actual value from the fitted curve. The measurement of the standard deviation is used to evaluate the variation, or dispersion, of a sample. Low standard deviation indicates that the predicted values are closer to the mean (which is zero for the residuals) of the sample. On the other hand, high standard deviation indicate a scattered spread of the predicted values. The 36

Chapter 3. Methodology and Implementation

3.7. Diagnostics and validation

standard deviation of the residuals of a fitted model is defined as: v v u n u n uX uX 2 2 u u ( y y ^ ) ( ) i i u u t i=1 t i=1 = .  = n 1 n 1

(3.19)

The standard deviation of the residuals is also referred as the standard error of estimate.

3.7.4

Kendall rank correlation coe cient

Another statistical method that was utilized for measuring the performance of the models, is the Kendall rank correlation coe cient (RCC) [28], also referred as Kendall's  (tau) coe cient. This statistical method is used to measure the ordinal association between two measured sets. As per its name, the statistical method is a measure of rank correlation, which is defined as the similarity on the orderings of the predicted against the true sets of data. The Kendall correlation is high when the two sets have a similar rank Â­ equal to 1 for identical ranking, and low when the rank is no similar Â­ equal to 1 for an inverse ranking. Finally, zero valued correlation coe cient denotes a random chance ranking. In this study, we utilized Kendall rank correlation coe cient by assessing the ranking of the model's predicted values, against the real observations. The correlation test function takes as input these two numerical vectors and yields a numerical output [ 1, 1].

3.7.5

Accuracy of slow/fast classification

A common practice when dealing with time predictions and more specifically with timeto-deliver, is simplification of the outcome. As it has been described in Chapter 2, previous studies classified newly reported issue reports as fast or slow based on a predefined time threshold. Although we did not build classification models, but focused on predicting times, we assessed their predicting eectiveness by setting a threshold and marking the predicted times as fast or slow. In cases of balanced data, accuracy (ACC) is a reliable metric for evaluation of the performance of the model [49]. Accuracy provides a measurement that describes the closeness to the true value and is often referred as trueness. It is defined as the proportion of the true results, among the total number of 37

Chapter 3. Methodology and Implementation

3.8. Censoring scenario analysis

observations under study, and is given by [49]: ACC = TP + TN , P +N (3.20)

where T P is the number of the true positive occurrences Â­ correctly classified as fast, T N is the number of true negative cases Â­ correctly classified as slow, and P + N represents the total number of samples referred to as positive and negative.

3.8

Censoring scenario analysis

One of our objectives was to examine appropriate methodology in the presence of censoring. To investigate the properties of survival analysis models when the data are censored, and to illustrate the methods that can be used in such circumstances, we designed scenario analyses where dierent proportions of our dataset were considered as censored. We utilized truncation Â­ a common approach to artificially generate censored observations. With truncation, we specify preset censoring cut-o points, in order to illustrate conditions of censored information. We defined time points within our dataset that have dierent given proportions of censored information; from 0 to 20% [0, 0.2] with a step of 0.01. For each censored dataset we fitted parametric models with all distributions assumed above and with explanatory covariates and applied diagnostic and validation procedures descibed in Section 3.7 (residual standard deviation, Kendall rank, and classification accuracy). A drawback of the truncation methodology is that the available information is significantly reduced, in order to achieve the desired ratio of censored against non-censored records. This contradicts with the theoretical advantages that consideration of censored information comes with Â­ enhanced sample size. However, we are just examining the influence, by reproducing the presence of censoring in our data. In Figure 3.6 we visually represent the truncation process, which is based on Figure 3.2 presented in Section 3.4.1. In this figure, we included the observations that are being removed due to the cut-o point. As we will be noticing in the next chapter, nonnormality of the data is the main reason of this excessive loss of information as we are truncating.

38

7 6 5

observations

4 3 2 1

time

Figure 3.6: Representation of the truncation process. In this case, in order to artificially reproduce 20% of censoring in our data, we have to remove two observations that their submit time is greater than the selected cut-o time point.

Chapter 4 Evaluation
In this chapter, the methodology provided above is applied on a set of real world, time-todeliver issue report data. We will be presenting results from a generalizable approach that can be applied in similar cases. The structure of this chapter is as follows: in Section 4.1 we will be giving a description of the dataset that we leveraged for this study. Nonparametric analysis on the empirical data is presented in Section 4.2, semi-parametric in Section 4.3, and parametric in Section 4.4. In both Sections 4.3 and 4.4 we explore the influence of the random-eects term after we first evaluate the fixed-eect models. External validation of the models is presented in Section 4.5. Finally, in Section 4.6 influence of censoring is being discussed.

4.1

Data description

The dataset used for this study comes from a commercial software that is still actively being developed. Due to confidentiality issues and in order to preserve anonymity we are not allowed to reveal the name of the company or the software. This is also the reason why many values of the dataset were anonymized or presented in a scale dierent than this of the real values. A total of 60,293 issue reports, delivered in a period of approximately 12 years, with all their attributes, are being analyzed. These observations were recorded through an issue tracking system. Issue or bug tracking systems [47], have been the main source of data for similar studies [4, 18]. The amount of information available comes from the 41

Chapter 4. Evaluation

4.1. Data description

successful development of four sequential releases of the same software, starting from 2003 until early 2016. As previously mentioned in Chapter 1, we consider as an "event" both bug reports and feature requests (collectively referred to as issues ). We chose to split our dataset into two parts, in order to achieve a test/train split that could represent real-world behaviour. Therefore, the first three releases were used as the train set, while the last release played the role of the test set. Table 4.1 shows the number of reported issues per release; in total we analyzed 46,296 observations for training and 13,997 observations for testing purposes. Table 4.1: Number of observations per release. Release ID r r+1 r+2 r+3 No. of observations 13,681 16,956 15,659 13,997

`r ' represents the first release that we have available, `r + 1 ' the one that followed, etc. `r ', `r + 1 ', and `r + 2 ' are used as the train set, while `r + 3 ' is used as the test set. The attributes that were used for model creation are described below. Â· Time-to-deliver : This is our response variable. Although it is not directly provided from the issue tracking system, it is simple to calculate, by subtracting the issue submit time-stamp from the resolution/deliver time-stamp. We converted these dates to UNIX times for easier data manipulation. Due to data disclosure restrictions, only relative time-units are provided. Â· Defect : This binary variable specifies if a given issue report is related to a defect or not. Essentially, a non-defect report infers a feature implementation. Â· Pre/Post release : Submission, development, and completion of an issue report within the same release time window results in pre release. Inability for the issue report to be completed within this time-window gets the issue report to be marked as post release. We can argue that some issue reports, due to their priority/severity can be categorized as mandatory for completion as pre release in advance. Therefore we assume that this flag is a proxy for the priority/severity of the report. 42

Chapter 4. Evaluation

4.1. Data description

Â· Components involved : This numerical attribute specifies the number of components that had to be modified for a given report to get resolved. Although one would argue that this is also information not available a priori, the originator of the report has to specify the main component against which the issue is reported. Based on that information and from previous knowledge, we can get a rough estimate of the components that will be involved. Â· Functions involved : Similar to the components, another numeric attribute that is being used in this study, is the number of functions that were involved to the resolution of a single report. Again, although the number of the functions modified during development of the issue is not known in advance, we can say that this number is correlated with the main function that the issue report is connected. Â· Developer country : The software that we are studying was developed by developers around the world. Since we are dealing with a commercial software that belongs to a well established company, development departments are spread all around the world. Although we notice a considerable amount of work contributed from a single country for all of the releases under study (>50%), we are studying the influence that country dierences might have on the time-to-deliver. This is also one of the anonymized fields of the dataset. For the four releases we have information for, 14 unique countries are involved in the development process. Â· Symptom tag : The person reporting an issue on the tracking system has to select a single symptom tag that briefly describes the issue. The selection has to be made among pre-defined unique tags, that briefly describe the problem or the feature that the assigned developer will be dealing with. A total of 27 tags were used for the total of the issue reports that we have available. Â· Developer : This variable captures the developer who was assigned and resolved the reported issue. The software has 1,236 unique developers involved at the time that we are studying. A descriptive analysis of the attributes in the dataset is presented in Table 4.2. Train set is represented by the first three releases and the test set by the last release.

43

Chapter 4. Evaluation

4.1. Data description

Table 4.2: Descriptive analysis table.
Attributes time-to-deliver1 mean range standard deviation defect, n(%) yes no pre/post release, n(%) pre post components involved mean range standard deviation functions involved mean range standard deviation developer country2 , n(%) country 1 country 2 country 3 country 4 country 5 symptom tag2 , n(%) program defect test failed function needed incorrect i/o core dump
1

Train set 865 0.01 - 46798 2126 44,100 (95.3) 2,196 (3.7) 29,610 (64.0) 16,686 (36.00) 2.45 0 - 136 5.90 29.47 1 - 21,740 291.55 30,965 (66.9) 7,596 (16.4) 3,732 (8.1) 2,272 (4.9) 866 (1.9) 9,558 (20.6) 9,486 (20.5) 8,783 (19.0) 4,079 (8.8) 2,312 (5.0)

Test set 551 0.03 - 50000 1417 12,863 (91.9) 1,134 (8.1) 8,528 (60.9) 5,469 (39.1) 2.49 0 - 228 6.38 32.40 1 - 31,630 403.26 7,966 (56.9) 2,429 (17.4) 1,044 (7.5) 1,032 (7.4) 735 (5.3) 2,830 (20.2) 3,811 (27.2) 2,775 (19.8) 482 (3.4) 378 (2.8)

Time-to-deliver is presented in time units due to confidentiality. 2 Only the five most common values presented in the table.

44

Chapter 4. Evaluation

4.2. Non-parametric analysis

4.2

Non-parametric analysis

After the descriptive analysis of the data, we generated Kaplan-Meier and Nelson-Aalen survival and cumulative hazard estimators, respectively. The steep drop of the KaplanMeier curve presented in Figure 4.1 indicates that a big proportion of the issue reports do not survive for a long time. In Figure 4.2 we applied a logarithmic transformation on our data (which is a standard visualization enhancement technique that preserves the order of the observations while making outliers less extreme). Figure 4.1 and 4.2 suggest that almost 60% of the issue reports suggest that get resolved relatively quickly. However, the flattening of the Kaplan-Meier curve implies that those that survive beyond 150 time units are expected to take a significantly longer time to be resolved. A small proportion of the observations stretches the survival curve to the right side of the x-axis; for the first three releases (which is our train set), 1% of the issue reports were resolved in more than 174 time units. Accordingly, in the cumulative hazard graph on the right of Figure 4.1, there is also a

Kaplan-Meier survival graph
1.0 10 0 0 10000 30000 2 4 6 8

Cumulative hazard graph

0.0

Cumulative hazard rate

Survival

0.2

0.4

0.6

0.8

0

10000

30000

time-to-deliver

time-to-deliver

Figure 4.1: Kaplan-Meier and cumulative hazard curves on the empirical data of the first three releases. Dotted lines represent confidence intervals. 45

Chapter 4. Evaluation

4.3. Semi-parametric analysis

steep slope at the beginning of the curve, proving that there is a time dependency on the hazard of issue reports being resolved. The earlier the distance from the date of an issue being reported, the more likely is for the issue to be solved. However, the slope/gradient of the curve is decreasing over time, hence the hazard as the time progresses lowers. Similar to the Kaplan-Meier curve, reported issues that survive after a certain time are more likely to remain unresolved for a long time.

4.3

Semi-parametric analysis

As discussed earlier, an alternative approach to modeling the eect of explanatory variables on a time-to-event variable is by measuring their eects on the hazard of the event. In this section we will investigate the application of the proportional hazards model on the time-to-deliver variable. We will dierentiate between two models: one that ignores the variation between developers (the fixed-eect model) and one that assumes that there is variation across the developers due to their unobserved characteristics (the random-

Kaplan-Meier survival graph
1.0 10 0 0.1 10.0 1000.0 2 4 6 8

Cumulative hazard graph

0.0

Cumulative hazard rate

Survival

0.2

0.4

0.6

0.8

0.01

1.00

100.00

time-to-deliver (log)

time-to-deliver (log)

Figure 4.2: Log transformation on the x-axis of Figure 4.1 for clarity. Dotted lines represent confidence intervals. 46

Chapter 4. Evaluation

4.3. Semi-parametric analysis

eects model). In both cases the fixed-eect attributes that we used are the following: Â· defect or feature, Â· pre or post release, Â· components involved, Â· functions involved, Â· developer country, Â· symptom tag. Detailed description of each attribute has been given in Section 4.1.

4.3.1

Fixed-eect models

The eect of each explanatory variable on the (log) hazard is given in Table 4.3. In general, the higher the hazard ratio, presented as log(HR) in the table, the greater the likelihood of the issue to be resolved (and in consequence the time-to-deliver will be smaller). A negative log(HR) implies a negative eect of the variable on the likelihood of resolution. The coe cients in a proportional hazards model can also be interpreted as the logarithmic ratio of the hazard, given the characteristics of x, over the baseline hazard. = log(HR) = log((h|x = 1)/(h|x = 0)) for x being a binary variable; when x = 0 the hazard rate is equal to the baseline hazard h(0). Issue reports that correspond to defects have lower hazard against the ones that correspond to feature implementation, hence defects will need more time to get resolved. Reports that have been submitted before the date that the next release is launched have a higher hazard to be delivered, which comes in line with our hypothesis H1 in Chapter 3.1. The larger the number of components involved in the resolution of an issue report Â­ the larger the hazard. Contrarily, although with a smaller eect, the number of functions involved reduce the hazard of the issue report resolution. The model estimated considerable dierences between the hazard rates across the dierent countries the development took place (with county #1 being the reference one). For all categorical variables -- developer country and symptom tag -- an interpretation approach would suggest sorting of the dierent category levels based on their relative eect on the hazard 47

Chapter 4. Evaluation

4.4. Parametric analysis

of delivery compared to the baseline developer country. We observe a negative hazard rate for all symptom tags (Table 4.3). This implies that issues with a symptom tag of build failed, against which all the other tags are compared, has a very high hazard rate of being resolved. The symptom with the lowest log hazard ratio compared to the build failed tag, was the docs incorrect symptom tag. Similar, the rest of the symptom tags can be interpreted.

4.3.2

Mixed-eects/frailty models

The dierences between the fixed-eect and the mixed-eects models with respect to the estimated hazard ratios is reflected in Table 4.3, where both fixed-eect and randomeects models are presented. By observing the hazard ratio estimates of the two models, we infer that adding the random-eect component has limited eect on the fixed-eect estimates. It should be noted however that the interpretation of the fixed-eect terms are now conditional on the random-eect value. Although there are some dierences in their values, the degree and statistical significance of the change is minor. Therefore, inclusion of the random-eect variable did not result in serious implication on the rest of the coe cients. Although the inference from the hazard ratios has not changed, the magnitude of the variance of the random-eects term ( 2 ), shown in Table 4.3, indicates that there exist considerable heterogeneity across the individual developers. Finally, the reduction on the AIC value provides a better fit for the random-eects model against the fixed-eect model.

4.4

Parametric analysis

As discussed in Section 3.5.5, transformations on the response variable might be able to assist in dealing with our extremely skewed distribution that we identified in Figure 4.1. Additionally, a significant advantage of the fully parametric models, that we are utilizing, is that the model is fitted directly on the duration variable rather than the hazard. Especially in the case of time-to-deliver estimation, predicting time, instead of hazard rate, is an important advantage of these models. 48

Chapter 4. Evaluation

4.4. Parametric analysis

Table 4.3: Regression estimates of Cox proportional hazard models.
fixed-eect Variable name log(HR) (SE) defect -0.058 (0.022) pre release 0.401 (0.010) component count 0.018 (0.001) function count -0.0001 (-0.00003) developer country (baseline: 1) 10 -0.543 (0.236) 11 0.460 (0.057) 12 0.200 (0.069) 14 -0.961 (0.700) 19 -0.899 (0.707) 2 0.223 (0.012) 3 -0.208 (0.020) 4 -0.152 (0.094) 5 -0.201 (0.017) 6 0.178 (0.069) 8 0.551 (0.310) 9 0.041 (0.034) symptom tag (baseline: build failed) core dump -1.400 (0.034) corrupt dbase -1.470 (0.061) docs incorrect -1.801 (0.12) function needed -1.230 (0.029) incorrect i/o -1.511 (0.031) incorrect xlat -1.610 (0.201) install add remove files -1.516 (0.142) install configuration -1.682 (0.101) install failed -1.460 (0.063) intgr problem -1.128 (0.085) lost data -1.714 (0.091) mixed code releases -0.812 (0.164) non standard -1.678 (0.062) not to spec -1.484 (0.037) obsolete code -1.685 (0.060) performance -1.725 (0.030) planned xlat -0.753 (0.500) plans incorrect -1.450 (0.091) program defect -1.398 (0.029) program loop -1.545 (0.068) prog suspended -1.453 (0.041) reliability -1.574 (0.043) test failed -1.367 (0.028) usability -1.575 (0.034) AIC 894785 2 2

mixed-eects log(HR) (SE) -0.057 (0.024) 0.283 (0.013) 0.005 (0.001) -0.00005 (0.00002) -0.840 0.300 0.430 -1.27 -1.249 0.112 -0.349 -0.243 -0.172 -0.180 0.484 0.149 (0.464) (0.609) (0.437) (0.841) (0.932) (0.057) (0.084) (0.237) (0.069) (0.205) (0.685) (0.115)

-1.369 (0.036) -1.385 (0.06) -1.760 (0.125) -1.177 (0.031) -1.439 (0.033) -1.537 (0.204) -1.237 (0.151) -1.443 (0.106) -1.270 (0.067) -1.129 (0.091) -1.671 (0.093) -0.651 (0.175) -1.711 (0.064) -1.519 (0.040) -1.635 (0.067) -1.700 (0.037) -0.708 (0.503) -1.434 (0.096) -1.390 (0.031) -1.466 (0.069) -1.449 (0.043) -1.534 (0.045) -1.354 (0.031) -1.516 (0.037) 887928 0.368

SE denotes standard error. Â­ variance of the random-eects term.

49

Chapter 4. Evaluation

4.4. Parametric analysis

We applied four dierent distributions on the empirical time-to-deliver data (exponential, Weibull, lognormal, and loglogistic). Figure 4.3 illustrates the fit for the four distributions against the Kaplan-Meier curve. Graphically, we notice that the Weibull curve is almost identical to the non-parametric Kaplan-Meier curve. Formal comparison between the models using the AIC indicates that the best fitting model with no explanatory variables is the model assuming a Weibull distribution, with an AIC of 644,482. At the same time, the worst AIC value for the estimated models, is the exponential, with a value of 718,783. The lognormal and loglogisitc distribution had an AIC of 649,455 and 650,031 respectively.

1.0

Survival

0.0

0.2

0.4

0.6

0.8

Kaplan-Meier Exponential Weibull Lognormal Loglogistic

0

10000

20000

30000

40000

time-to-deliver

Figure 4.3: Survival curves of each distribution compared to the empirical data.

Parametric regression Subsequently, we follow a parametric regression approach to understand the impact of dierent explanatory variables on time-to-deliver as well as to generate time-to-deliver predictions. The variables we consider as explanatory are the same to those in the semiparametric Section 4.3. Additionally,we built a basic linear model to investigate the extent of deviation from the parametric models from a simple regression solution. 50

Chapter 4. Evaluation

4.4. Parametric analysis

Cumulative Hazard

5

10

15

20

Cumulative hazard Exponential Weibull Lognormal Loglogistic 0 10000 20000 30000 40000

0

time-to-deliver

Figure 4.4: Cumulative hazard curves of each distribution compared to the empirical data. The contribution of each of these variables is considered significant in the predictive e ciency of the model. This was derived by the p-values (< 0.01 in all cases) that the models yield upon creation. At the same time application of a stepwise elimination algorithm (based on the AIC values of the models) on the full model did not eliminate any of the explanatory variables. For accuracy metric, due to the restriction of a balanced dataset, we set the median of the time-to-deliver as our time threshold. This resulted in an even categorization of our data between the two classes Â­ fast below the threshold and slow above.

4.4.1

Fixed-eect models

Table 4.4 presents the results of the best fitting models for each of the distribution assumptions along with their corresponding parameters. We can observe that even after explanatory variable inclusion, the Weibull is the best fitting model based on the AIC. The reason AIC was preferred as an assessment of goodness-of-fit against R2 is because we cannot use R2 values as a measure for parametric models (as discussed in 51

Chapter 4. Evaluation

4.4. Parametric analysis

Table 4.4: Parametric regression estimates of fixed-eect models.
Exponential Variable name (SE) (Intercept) 4.920 (0.036) defect 0.018 (0.023) pre release -0.463 (0.010) component count -0.030 (0.001) function count 0.0001 (0.00002) developer country (baseline: 1) 10 0.526 (0.236) 11 -0.868 (0.058) 12 -0.362 (0.070) 14 1.407 (0.707) 19 1.574 (0.707) 2 -0.468 (0.013) 3 0.248 (0.022) 4 -0.211 (0.095) 5 0.295 (0.018) 6 -0.488 (0.07) 8 -2.184 (0.317) 9 -0.325 (0.035) symptom tag (baseline: build failed) core dump 2.131 (0.034) corrupt dbase 2.214 (0.062) docs incorrect 2.984 (0.123) function needed 2.019 (0.029) incorrect i/o 2.407 (0.031) incorrect xlat 2.333 (0.202) install add remove files 2.413 (0.143) install configuration 2.449 (0.102) install failed 2.261 (0.063) intgr problem 1.755 (0.085) lost data 2.732 (0.091) mixed code releases 1.025 (0.165) non standard 2.878 (0.062) not to spec 2.406 (0.038) obsolete code 3.095 (0.065) performance 2.677 (0.035) planned xlat 0.560 (0.501) plans incorrect 2.280 (0.092) program defect 2.165 (0.029) program loop 2.304 (0.068) prog suspended 2.134 (0.042) reliability 2.426 (0.043) test failed 2.018 (0.029) usability 2.525 (0.034) parameter1 1 AIC 702514 Weibull (SE) 3.449 (0.076) 0.128 (0.048) -0.841 (0.021) -0.040 (0.002) 0.0002 (0.00005) 1.159 -0.986 -0.419 2.015 1.882 -0.472 0.434 0.312 0.421 -0.378 -1.177 -0.09 2.978 3.127 3.815 2.615 3.204 3.410 3.215 3.562 3.099 2.405 3.623 1.725 3.544 3.148 3.572 3.651 1.619 3.081 2.965 3.275 3.081 3.334 2.902 3.337 (0.492) (0.120) (0.145) (1.474) (1.474) (0.027) (0.046) (0.198) (0.037) (0.146) (0.662) (0.072) (0.071) (0.129) (0.257) (0.060) (0.065) (0.421) (0.298) (0.212) (0.131) (0.178) (0.190) (0.343) (0.129) (0.078) (0.135) (0.073) (1.044) (0.192) (0.060) (0.142) (0.087) (0.090) (0.060) (0.072) 2.0849 636975 Lognormal (SE) 2.354 (0.089) 0.180 (0.056) -1.395 (0.025) -0.045 (0.002) 0.0003 (0.00004) 1.537 -1.279 -0.704 2.894 2.368 -0.458 0.814 1.162 0.561 -0.270 -0.823 0.155 (0.575) (0.140) (0.170) (1.724) (1.724) (0.032) (0.053) (0.231) (0.043) (0.171) (0.773) (0.084) Loglogisitc (SE) 2.12 (0.089) 0.245 (0.059) -1.315 (0.024) -0.051 (0.003) 0.0003 (0.00004) 1.587 -1.352 -0.628 2.745 2.218 -0.45 0.708 1.088 0.499 -0.272 -0.612 0.200 3.695 3.813 4.187 2.737 3.736 4.319 3.797 4.538 3.773 2.946 4.239 1.755 3.676 3.594 3.437 4.384 2.340 3.657 3.567 4.074 3.959 4.049 3.605 3.870 (0.569) (0.146) (0.179) (1.455) (1.487) (0.031) (0.049) (0.209) (0.041) (0.161) (0.760) (0.082) (0.079) (0.141) (0.301) (0.070) (0.074) (0.450) (0.320) (0.221) (0.143) (0.202) (0.209) (0.436) (0.156) (0.091) (0.169) (0.082) (1.236) (0.222) (0.068) (0.153) (0.095) (0.100) (0.068) (0.082) 1.3632 638716

3.454 (0.083) 3.551 (0.150) 3.788 (0.301) 2.428 (0.071) 3.426 (0.076) 3.866 (0.492) 3.734 (0.348) 4.291 (0.248) 3.565 (0.153) 2.671 (0.208) 4.029 (0.223) 1.557 (0.401) 3.286 (0.150) 3.254 (0.092) 3.13 (0.158) 4.061 (0.085) 1.822 (1.220) 3.254 (0.224) 3.225 (0.07) 3.822 (0.165) 3.795 (0.101) 3.727 (0.105) 3.320 (0.070) 3.521 (0.084) 2.4373 638941

SE denotes standard error.

52

Chapter 4. Evaluation

4.4. Parametric analysis

Section 3.7.2). Nevertheless, the R2 of the linear model that we built (where response variable was log(y )) with the same explanatory variables, was 0.204. This implies that the model explains  20% of the variability. The number is low, but is not uncommon in modelling complex systems (e.g., in economics, medicine, and psychology [34, 8]). Nonetheless, the statistical significance of the coe cients allows us to draw important conclusions on the eect of each explanatory variable. We can, therefore examine the eect of each explanatory variable on the (log) timeto-deliver as well as the dierences of the coe cients among the four dierent parametric models in Table 4.4. A first general observation is that, for the majority of the explanatory variables, the direction of the eect is the same among all the dierent parametric models. The explanatory variable that seems to have the greater eect on the time-to-deliver is the symptom tag that is associated with the issue report. The non standard tag is the one that aects the time-to-deliver the most in a negative way; always compared with the baseline tag which is the build failed. For both categorical variables (developer country and symptom tag), the results that were observed in the proportional hazards models concur in most cases. An interesting comparison can be made among the components and the functions that need to be adjusted for a report to get resolved. For the former, the greater the number of the components Â­ the shorter the time until resolution. Contrary, more functions involved in the resolution of a report, more time necessary Â­ although the significance of the functions variable is much less significant. In addition, when a report is flagged as needed to be resolved before the next release is launched, the time-to-deliver of this report is shorter. Although the dierences among the four dierent distributions are not identical, their eect is in all cases in a similar direction. Table 4.5 illustrates the performance metrics that we utilized for model comparison.The results presented in this section reflect performance metrics while evaluating the models internally, on the train set. Considering the standard deviation of the residuals (  ) and the Kendall rank correlation coe cient (RCC) the simple linear model yields competitive results. While assessing the residual standard deviation, the exponential model is the best performer among the parametric models. Regarding the Kendall rank correlation coe cient, the Weibull model, which also has the best AIC value, provides the better result. However, the superiority of the simple linear model is still obvious. Finally, when using the models as fast/slow classifiers of the time-to-deliver, the lognor53

Chapter 4. Evaluation

4.4. Parametric analysis

mal model is the one that has the best performance based on the accuracy metric. In this case, the lognormal model significantly outperforms the simple linear model as well, being able to correctly classify approximately 65% of the observations. Table 4.5: Performance metrics for fixed-eect models. Linear model Exponential Weibull Lognormal Loglogistic 2084 2087 2095 2106 2112 0.197 0.157 0.161 0.126 0.128 0.562 0.553 0.586 0.646 0.618 702514 636975 638941 638716



RCC ACC AIC

Based on these findings, an interesting insight is the inconsistency of the best candidate model. This is based on the criteria we measure performance. Naively, we can say that these metrics "challenge" the models in a dierent manner - from the more di cult challenge of the residual standard deviation, to the less di cult of rank correlation coe cient, with accuracy being the less challenging one. The residual standard deviation assesses the predicting accuracy of each observation, while Kendall rank correlation "forgives" non-detailed prediction, as long the ranking is the same. Finally, fast/slow classification of the issue reports, as captured by accuracy, ignores both prediction precision close to the real values and ranking, and focuses on the time threshold that is set.

4.4.2

Mixed-eects/frailty models

In a similar approach as in the proportional hazards models, we introduce the developer as the random-eects attribute in our models. The results of these mixed-eects models are presented in Table 4.6. Along with the coe cients of the fixed-eect terms and their standard errors, the variance of the randomeects variables is also presented. Based on the values of the coe cients there are no significant changes compared to what we have seen in the previous results discussed. The eect of the defect/feature dierentiation, as well as the number of functions involved are the least significant variables. The impact however is considerable, in the direction of making the report to get resolved faster, for both pre release reports and proportional 54

Chapter 4. Evaluation

4.4. Parametric analysis

to the number of components involved. Additionally, we observe that the introduction of the random-eects term resulted in a notable and some times in a directional change on some of the developer's countries. For example, by noticing the values of country 11 in Table 4.4 gives us the impression that issue reports developed in this country tend to get resolved faster than the baseline country (#1), as well as compared with the majority of the rest of the countries. However, in the mixed-eects model, this is no longer true; the standard error around the coe cient is high in all cases, therefore we are driven to the result that the variability within this country is high. The explanation could be based on the definition of mixed-eects models given on Section 3.6 and their dierence with the fixed-eect models. The AIC values propose a dierent distribution as the best candidate model, always compared to the fixed-eect models. The loglogistic distribution has the best AIC value, followed by the lognormal. The Weibull model, which had the best AIC values so far, comes third while assessing the goodness-of-fit based on AIC value. The variance of the random-eects term for the loglogistic distribution is also considerably higher than the one of the Weibull model. This implies that in the loglogistic model the dispersion of the developers is assumed to be higher. Accordingly, Table 4.7 represents performance metrics of the mixed-eects models for internal validation (train set). In this case, inclusion of the random-eects term results in superiority of the parametric models, against the simple linear model we fitted for comparison purposes. In this case, the exponential model is the one performing best, in terms of residual standard deviation, as well as Kendall rank correlation coe cient. However, the loglogistic model is the one that can classify more accurately as fast or slow fix. By comparing the performance metrics of the fixed-eect with the mixed-eects models, Tables 4.5 and 4.7 respectively, we can observe an improvement in the latter. Although the values of the residual standard deviation are not significantly improved, both Kendall rank correlation coe cient and classification accuracy are substantial. However, it is also worth mentioning that, better fit of the models is expected anyway since the number of explanatory variables is increasing (addition of the developer). This is also the reason of the results that are presented in Table 4.8. Although we have not discussed inclusion of the developer as a fixed-eect variable, we conducted this additional experiment and presenting the results in this section only, to show how information about 55

Chapter 4. Evaluation

4.4. Parametric analysis

Table 4.6: Parametric regression estimates of mixed-eect models.
Exponential Variable name (SE) (Intercept) 4.220 (0.070) defect -0.018 (0.025) pre release -0.284 (0.014) component count -0.012 (0.001) function count 0.00003 (0.00005) developer country (baseline: 1) 10 1.488 (0.867) 11 -0.345 (1.324) 12 -0.266 (0.938) 14 1.909 (1.182) 19 2.222 (1.500) 2 -0.258 (0.112) 3 0.462 (0.164) 4 0.064 (0.461) 5 0.121 (0.126) 6 0.345 (0.407) 8 -1.563 (1.360) 9 -0.451 (0.220) symptom tag (baseline: build failed) core dump 2.166 (0.038) corrupt dbase 2.194 (0.067) docs incorrect 3.054 (0.127) function needed 2.036 (0.034) incorrect i/o 2.352 (0.035) incorrect xlat 2.512 (0.206) install add remove files 1.932 (0.156) install configuration 2.192 (0.108) install failed 2.023 (0.071) intgr problem 1.803 (0.092) lost data 2.733 (0.096) mixed code releases 0.962 (0.183) non standard 3.101 (0.067) not to spec 2.574 (0.042) obsolete code 3.010 (0.070) performance 2.748 (0.039) planned xlat 0.569 (0.504) plans incorrect 2.396 (0.099) program defect 2.248 (0.033) program loop 2.267 (0.071) prog suspended 2.286 (0.046) reliability 2.479 (0.048) test failed 2.160 (0.033) usability 2.508 (0.039) scale 1.000 2 1.745 AIC 682761
2

Weibull (SE) 3.371 (0.088) 0.121 (0.048) -0.561 (0.026) -0.012 (0.002) 0.0001 (0.00004) 1.421 -0.851 -0.713 2.251 2.168 -0.291 0.506 0.271 0.318 0.163 -1.235 -0.283 (0.827) (0.950) (0.682) (1.545) (1.672) (0.092) (0.134) (0.387) (0.107) (0.347) (1.129) (0.179)

Lognormal (SE) 2.293 (0.103) 0.272 (0.055) -0.972 (0.030) 0.001 (0.003) 0.0001 (0.00004) 1.825 -1.423 -0.836 3.019 2.557 -0.199 0.796 0.953 0.265 0.456 -1.084 -0.164 3.117 3.062 3.240 2.160 3.030 3.361 2.943 3.345 2.744 2.439 3.665 1.003 3.041 2.998 2.877 3.684 1.883 3.003 3.088 3.387 3.378 3.273 3.017 3.158 (1.066) (1.217) (0.871) (1.780) (1.975) (0.115) (0.167) (0.480) (0.130) (0.438) (1.398) (0.221) (0.078) (0.140) (0.275) (0.068) (0.072) (0.450) (0.344) (0.232) (0.147) (0.207) (0.205) (0.385) (0.139) (0.088) (0.149) (0.081) (1.109) (0.212) (0.067) (0.152) (0.095) (0.098) (0.067) (0.080) 2.206 1.462 631266

Loglogistic (SE) 2.087 (0.104) 0.330 (0.057) -0.936 (0.029) 0.000 (0.003) 0.0001 (0.00004) 1.997 (1.048) -1.411 (1.271) -1.011 (0.907) 2.916 (1.560) 2.451 (1.845) -0.190 (0.118) 0.781 (0.171) 0.965 (0.484) 0.232 (0.135) 0.393 (0.44) -0.926 (1.445) -0.114 (0.229) 3.332 3.312 3.467 2.419 3.294 3.689 3.115 3.566 2.985 2.658 3.845 1.098 3.401 3.268 3.194 3.943 2.278 3.305 3.336 3.591 3.499 3.548 3.231 3.436 (0.076) (0.132) (0.282) (0.068) (0.072) (0.420) (0.316) (0.209) (0.140) (0.201) (0.194) (0.424) (0.146) (0.088) (0.157) (0.079) (1.099) (0.209) (0.067) (0.142) (0.091) (0.096) (0.066) (0.079) 1.230 1.593 630908

2.755 (0.070) 2.783 (0.125) 3.522 (0.245) 2.374 (0.061) 2.888 (0.065) 3.083 (0.398) 2.506 (0.293) 2.918 (0.206) 2.566 (0.132) 2.274 (0.177) 3.330 (0.182) 1.314 (0.338) 3.401 (0.125) 3.038 (0.078) 3.272 (0.132) 3.414 (0.072) 1.453 (0.982) 2.879 (0.188) 2.790 (0.06) 2.946 (0.135) 2.908 (0.085) 3.073 (0.088) 2.722 (0.060) 3.038 (0.072) 1.951 0.889 631740

SE denotes standard error. Â­ variance of the random-eects term

56

Chapter 4. Evaluation

4.5. Validation

Table 4.7: Performance metrics for mixed-eects models. Linear model Exponential Weibull Lognormal Loglogistic 2026 2025 2048 2077 2070 0.305 0.314 0.289 0.255 0.260 0.569 0.605 0.671 0.693 0.703 682761 631740 631266 630908



RCC ACC AIC

Table 4.8: Performance metrics for fixed-eect models, including developer. Linear model Exponential Weibull Lognormal Loglogistic 2084 2041 2036 2068 2064 0.197 0.297 0.289 0.241 0.247 0.562 0.606 0.675 0.695 0.703 683334 632099 631702 631276



RCC ACC AIC

developer can be leveraged by a linear model. However, the main disadvantages of the linear fixed-eect model including developer, is that (i) it cannot make predictions for new developers and (ii) the model will return null values as predictor estimates when the records of a single developer are not su cient in number. Therefore, by comparing Tables 4.7 and 4.8, we can see that the goodness-of-fit of the mixed-eects models is better based on the AIC values. The residual standard deviation and the accuracy metrics are very close in most cases, however, for rank correlation we observe a slight superiority for the mixed-eects models.

4.5

Validation

A common way of validating the fit of the model is by evaluating its predicting performance on a dierent dataset. Although there are dierent ways of conducting this validation, we used data splitting, based on the chronological order of the releases that we have available; we can refer to this split as quasi-chronological or pseudo-temporal split. As discussed before, we used the first three releases as the train set and the fourth as the validation/test set. Although there are di culties and restrictions while validating the results, data partitioning is a common way of assessing the predictive performance. 57

Chapter 4. Evaluation

4.5. Validation

Especially in this case and due to the nature of the data, there might still be overlapping information among the dierent releases. For example, since the development process is continuous, there are issue reports that were submitted during the first three releases but closed sometime within the time-window of the fourth release. Using the same parametric models that were built on the train set, we evaluated their predictive performance by applying them on the test set and then comparing the predicted values with the real ones. The results for both fixed-eect and mixed-eects models are presented in Tables 4.9 and 4.10 respectively. Table 4.9: Performance metrics for fixed-eects models for external validation. Linear model Exponential Weibull Lognormal Loglogistic 1409 1414 1490 1540 1693 0.192 0.191 0.098 0.048 0.035 0.530 0.544 0.575 0.663 0.663



RCC ACC

Table 4.10: Performance metrics for mixed-eects models for external validation. Linear model Exponential Weibull Lognormal Loglogistic 1418 1447 1406 1406 1406 0.197 0.183 0.197 0.180 0.183 0.569 0.551 0.611 0.677 0.675



RCC ACC

Although the fit looks better when comparing the standard deviation values with these from the internal validation, we have to be careful with data interpretation. As already mentioned above, the nature of the data and the selection of the train/test set could be the reason of the residual standard deviation decrease. Overlap of the data and dierences in the descriptive analysis of the response variable that was identified in the descriptive analysis Table 4.2, is primarily the reasons of the dierences that we observe (compare standard deviation of time-to-deliver for train vs test sets in Table 4.2). The fit of the simple linear model still looks competitive in most cases, except from the accuracy. Although the results of the fixed-eect models seem inconsistent, in the mixed-eect models the e ciency is improved, or at least consistent, between the dierent paramteric models. As a final observation we can conclude to the following: loglogistic 58

Chapter 4. Evaluation

4.6. Considering censoring

models yields the best accuracy for both training and validation,; mixed-eects are performing better in all cases and finally, accuracy in validation is slightly worse than that for training Â­ which was expected based on the similarities of the datasets.

4.6

Considering censoring

In the presence of censoring we re-applied all of the validation processes that were described until now. As mentioned in Section 3.8, artificial truncation resulted in a significantly reduced dataset. Figure 4.5, represents this impact on the sample size, for each censoring proportion (achieved by truncation); from the full dataset -- 46,296 records -- with no censoring, to a reduced one -- 2,257 records -- in order to achieve 20% of censoring. The long tailed distribution of the response variable, in combination with the majority of issue reports being resolved very quickly (compared with the mean value of the response variable) results in this sharp reduction. In order to be able to achieve the maximum censoring proportion (20%) we had to go back nine years Â­ in the twelve year time span we are studying.
Truncation results

sample size

10000

20000

30000

40000

0

5

10 % censoring

15

20

Figure 4.5: Artificial truncation Impact of the censoring simulation on the sample size. 59

Chapter 4. Evaluation

4.6. Considering censoring

The results of these experiments are presented in the Appendix A and Tables A.1 to A.10. The Tables can be read as follows: Â· First column represents the censoring proportion that we artificially achieved by truncation of the data. Â· Second column indicates the sample size for each censoring proportion. The reason of the sample degradation as the censoring increases is given above. Â· In the remaining ten columns (eight columns for Tables A.1 and A.6 since we did not calculate AIC for basic linear models because we cannot use them for model comparison, as discussed in Section 3.7.1) the values of each metric are presented Â­ residual standard deviation, Kendall rank correlation coe cient, and AIC. Â­ For standard deviation and Kendall rank tables, the third column provides results of a simple linear model that was built and presented as a baseline criterion. However, and since a linear model cannot incorporate censored information, the deliver time-stamp of the censored observations was set as the censoring cut-o point. For the AIC tables, this column is missing since AIC comparison between parametric and linear models is not valid (as discussed in Section 3.7.1). Â­ Similarly, for standard deviation and Kendall rank tables only (AIC tables are missing this column as well), the fourth column presents results of a simple linear model with the censored observations filtered out. The number of censored observations can be calculated by multiplying the censoring proportion with the sample size. Â­ Columns five, seven, nine, and eleven (three, five, seven, and nine for AIC) represent results of the fully parametric models: exponential, Weibull, lognormal, and loglogistic respectively. Â­ Consequently, the even columns from six to twelve (four to ten for AIC) represent the fully parametric models that completely drop/filter out the censored observations. For the AIC values, note that we can only compare them on each row independently and at the same time among the same type of columns (cens or drop columns). The 60

Chapter 4. Evaluation

4.7. Discussion

definition of the AIC does not allow it to be a criterion when the sample is dierent Â­ which is true for censored against dropped columns in all tables. Some of the results values in the tables are missing, the reason of this inconsistency is the decreased sample size, which leads to reduced information, hence the inability to generate predicted values. Despite this numerical instability of the models, we are presenting the results regardless, with the goal to get as much information as possible. The exponential distribution models are performing slightly better than the rest, which contradicts with the results that were observed on the empirical data (Weibull looked to fit better). However, and as we already identified above, the dierences are minor between the majority of the models. Models that drop the censored observations are reducing their standard deviation as the censoring proportion increases Â­ sample size decreases significantly. At the same time, the models that incorporate large number of censored observations are unstable - the residual standard deviation is increasing extremely in these cases. This is an additional evidence of the right skewed distribution of our data and echoes the fact of the long but, at the same time, thin tail. The results are similar for the Kendall rank correlation coe cient. While trying to compare dierences between fixed and mixed-eects models, in respect to the eect of censoring, we can argue the following. Inconsistencies seem to align, after the censoring proportion is greater than 12% for internal validation. However, these inconsistencies show up earlier (i.e., censoring proportion greater than 5%) when it comes to external validation, due to the smaller sample. In both cases though, internal and external validation, the mixed-eects models seem to be more e cient based on all metrics. Although the dierences are minor in respect of residual standard deviation, rank correlation coe cient is significantly improved in all mixed-eect models. Comparison of AIC values between Tables A.1 and A.6 is valid, since the response variable is not altered and the explanatory variables of the fixed-eect models is a subset of the mixed-eects ones. Therefore, all mixed-eects models have a better goodness-of-fit than the fixed-eect.

4.7

Discussion

Regarding RQ1 "How incorporation of random-eects models and/or censored data influence performance of a model that predicts time-to-deliver?". We extensively analyzed 61

Chapter 4. Evaluation

4.7. Discussion

and compared fixed against mixed-eects models, as well as dierences in the proportion of censored information. For the former, we can confidently say that inclusion of the random-eect term is having a positive impact, enhancing the predicting power and the goodness-of-fit of the models. We compared the performance of the models in various aspects, and in all cases the superiority of the mixed-eects models was proven. Additionally, mixed-eect models will be superior against fixed-eect ones, in cases where prediction is necessary on new data, that potentially new developers have been introduced. In such cases, fixed-eect models will not be able to use historical information about developers and make predictions at all. However, the decision is not only based on AIC values and goodness-of-fit metrics but also on expert's opinion. If the long tails make no sense then even if the AIC is correct the model makes little sense in practice. For incorporation of censored data, the results are unclear. As extensively analyzed and illustrated, non-normality of our data is the main reason of the inconsistent, and in some cases, incomplete results. We argue that the dataset under study is not ideal for artificial truncation. The long tail dominates in all cases, especially in those of increased censoring proportions, which resulted in the extreme values that show up in Appendix A result tables. The phrase "the ends justify the mean" might be appropriate as an explanation to these results Â­ meaning that the long tail is having a great impact on the dataset. For RQ2 "How geographical, churn, and complexity factors aect the duration of the time-to-deliver?", we thoroughly explored the eect of each attribute on the response variable. From the eect of the hazard ratio to the extend of the time-to-deliver on the accelerated failure time models. Our results were consistent and in most cases concur with the theoretical hypothesis that we formed before proceeding with the practical implementation, as discussed in Section 4.7.1 below. As for a final selection of the best model, we showed the advantages of parametric analysis. The ability to overcome all the violations and obstacles that were extensively analyzed is definitely an asset. However, based on the measured performance, the results were not considerably better than in the simpler case of the linear models. Lastly, selection of the most appropriate model might also depend on the desired outcome. Inconsistencies between the dierent parametric distribution models have also been analyzed. 62

Chapter 4. Evaluation

4.7. Discussion

4.7.1

Validation of Hypothesis

Based on the hypotheses that were preliminarily conducted and presented in Section 3.1, we re-iterate and compare with the findings after the theoretical methodology has been applied to the data. For H1 we showed that our hypothesis comes in line with the results for all models that were fitted. Issue reports marked as pre release are most likely to be resolved faster than post ones. At the same time, we utilized the information that we get from this variable as a proxy for severity/priority of the issue report which was hypothesized in H3 . Based on the coe cients of the components involved in an issue report, we observe a negative correlation Â­ the more the components involved, the less the time necessary for delivery. This finding disproves our original hypothesis in H2 . However, for the functions involved, our findings converge towards this direction Â­ although the significance is reduced in the case of the functions. Dierentiation of defect or feature did not make a significant dierence based on our findings, although we hypothesized that defects tend to get resolved faster than feature implementations in H4 . In our first categorical variable, we expected dierences among dierent symptom tags H5 . We were able to identify some dierences among them, as well as to provide an order, from faster to slower, in terms of the duration eect on the time-to-deliver. Based on their brief description and on expert input, we could have also hypothesized the expectation of their di culty. For the random-eect term in our models, we anticipated a within group dependence and variation on the eect among dierent individuals H6 . We discussed the final eect of the variable itself, as well as the eect on the rest of the explanatory variables. Additional insights, such as the variance among the developers, are also a considerable advantage that we considered when using this variable as a random-eects.

4.7.2

Stakeholder feedback

As part of this study, we not only evaluated the results ourselves, but also provided frequent feedback to the team responsible for the current development of the software under study. The prosperous communication and suggestions on further steps and im63

Chapter 4. Evaluation

4.7. Discussion

provements, resulted in some additions and re-iterations on the results presented, as well as in our future work suggestions. Additionally, feedback and information on details that are not readily available from the issue tracking system and the raw data, provided some adjustments and calibrations on the existing models. For example, shifting the slow/fast threshold to the current standards and the development process being followed, resulted in a significant improvement in classification accuracy. In Figure 4.6 the fluctuation of the accuracy performance is shown; shifting the threshold to the right of the median value, results in better accuracy. As illustrated in the same graph, the "current slow/fast threshold" represents the feedback we received from the development team and how they currently classify resolutions as fast or slow. However, additional measures and metrics are necessary, especially when the ratio between the two classes becomes imbalanced.

Accuracy of fast/slow classification on different thresholds
1.00 ACC 0.70 0.75 0.80 0.85 0.90 0.95

median current slow/fast threshold

0

2000

4000 threshold

6000

8000

Figure 4.6: Accuracy fluctuation over dierent fast/slow thresholds. Thresholds above 8,500 get closer to 1 monotonically.

64

Chapter 4. Evaluation

4.8. Threats to validity

4.8

Threats to validity

The threats to validity for this study, as well as the ways to overcome them, are categorized and presented in this section. Construct validity We construct our dataset based on the data collected from the issue reporting system. The system captures a variety of events and activities happening in the organization. However, only a subset of this information was used to built our predictive models. To overcome this threat, we utilized a large subset of the entire software dataset Â­ 4 releases developed over approximately 12 years. Statistical validity We utilized the R software environment for statistical computing and graphics [44]. In order to validate our results, we utilized dierent libraries wherever possible. To prevent biased results, we utilized quasi-chronological or pseudo-temporal (as defined in Section 4.5) as a validation technique. Internal validity In order to avoid researcher bias, we derived and followed strict automated processes for data extraction and processing. One of the most critical internal parts of our study is the artificial truncation process. Although the results are not consistent and optimal in the way we would expect incorporation of censored information to assist in the predicting performance, the truncation process replicates real conditions in the best way possible. External validity Generalizing our findings from a single software to dierent situations is one of our main future interests towards extending this work, although this might not be possible under dierent circumstances. However, the design of this study is based on the rationale of the critical case [52]. The methodological approach is easily reproducible for other software and at the same time readily available to be applied on software with similar attributes recorded by the issue tracking system. 65

Chapter 5 Conclusions, Summary and Future Work
In this study, we utilized survival analysis for estimation of the time-to-deliver, within a large scale commercial software. We successfully leveraged three techniques, that although are well known and commonly used in other academic disciplines (i.e., engineering, financial, and health economic studies), have not been widely utilized in the Software Engineering field of study. Namely, we introduced use of survival analysis for time-to-deliver data, incorporation of randomeects models, and consideration of censored observations. We identified that these techniques are appropriate for our case and also encourage researchers to utilize them in similar cases, by providing the theoretical background and the limitations that these techniques help to overcome. Moreover, we provide the prototype tool implementing this approach. We thoroughly described the advantages for the proposed methodologies: survival models to overcome linear regression limitations, incorporation of censored observations to enhance the sample size, and identification of within group dependence for the randomeects attribute. However, limitations still exist in the use of these techniques. In the case of survival analysis (since the most common way of dealing with survival data is the Cox proportional hazards models), interpretation of the hazard rate might be di cult when dealing with time-to-deliver data. For random-eects models, either these are incorporated in proportional hazards models or accelerated failure time models, the 67

Chapter 5. Conclusions, Summary and Future Work

5.1. Future work

computational complexity increases dramatically Â­ especially when compared to simple linear models. Finally, in censored data consideration, their inclusion in the predictive models can also be complex. Missing values and attributes of the censored observations that are not available when in the state of "censored" might change the model fitting "strategy". If these limitations can be avoided though, integration of these practices can significantly improve the robustness and prediction power of the models. Additionally, they are an e cient way of avoiding the limitations that simple linear models will face in equivalent situations. We believe that our approach can help practitioners to improve prediction of the timeto-deliver, simplifying resource planning. Our results are also of interest to theoreticians, showing applicability of survival analysis, incorporation of censored information, and introduction of a random-eects term, in a new domain.

5.1

Future work

We consider this work as a first step of application of survival analysis in the timeto-deliver estimation. Our plans for future work include: (i) application of the same methodology to other similar datasets, in order to be able to validate our methodological approach. As proposed in other studies, ability to replicate findings (i.e. successfully apply the same methodology or prediction models to other datasets), is one of the most challenging endeavours of predictive modeling [48, 10]. Additionally, (ii) further study of the eect of censored information, especially in comparison with dropping out the censored observations is also of our interest Â­ we set as a constraint though, that the dataset should be censored itself and not apply artificial truncation, as we did in this study. (iii) Finally, regarding random-eects models, more in depth analysis of the final eect of the frailty object in the predictive e ciency as well as introduction of multi-level frailty objects is also a consideration. More complicated and hierarchical structures of group dependence (multi-level models, individual growth models or hierarchical linear models) are a methodological improvement of the single level frailty object. An example on the case we studied would be the incorporation of the hierarchical connection among the developer and the manager of the developer. 68

Chapter 5. Conclusions, Summary and Future Work

5.1. Future work

We look forward to contributing our work on these challenging problems relevant to quality assurance.

69

Appendix A Results Tables
Interpretation of the results is discussed in depth in Chapter 4. Constructional explanation of the tables has already been provided in Chapter 4, however, for easier reference it is repeated here as well: Â· First column represents the censoring proportion that we artificially achieved by truncation of the data. Â· Second column indicates the sample size for each censoring proportion. The reason of the sample degradation as the censoring increases is given in Section 4.6. Â· In the remaining ten columns (eight columns for Tables A.1 and A.6 since we did not calculate AIC for basic linear models because we cannot use them for model comparison, as discussed in Section 3.7.1) the values of each metric are presented Â­ residual standard deviation, Kendall rank correlation coe cient, and AIC. Â­ For standard deviation and Kendall rank tables, the third column provides results of a simple linear model that was built and presented as a baseline criterion. However, and since a linear model cannot incorporate censored information, the deliver time-stamp of the censored observations was set as the censoring cut-o point. For the AIC tables, this column is missing since AIC comparison between parametric and linear models is not valid (as discussed in Section 3.7.1). Â­ Similarly, for standard deviation and Kendall rank tables only (AIC tables are missing this column as well), the fourth column presents results of a simple 71

Appendix A. Results Tables

linear model with the censored observations filtered out. The number of censored observations can be calculated by multiplying the censoring proportion with the sample size. Â­ Columns five, seven, nine, and eleven (three, five, seven, and nine for AIC) represent results of the fully parametric models: exponential, Weibull, lognormal, and loglogistic respectively. Â­ Consequently, the even columns from six to twelve (four to ten for AIC) represent the fully parametric models that completely drop/filter out the censored observations.

72

Table A.1: Akaike information criterion. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 cens exp 702514 607726 555729 497109 241505 232484 96536 91031 84497 79487 61350 60061 53030 47366 44191 41161 39481 33694 31916 30290 27740 drop exp 702514 606377 552426 489710 236796 227122 93425 87577 81211 76197 58171 56876 49480 43835 40758 37845 36147 30325 28673 27103 24634 cens weib 636975 544782 495042 440197 209207 200646 82233 77280 71771 67603 51644 50429 44044 39058 36254 33562 32127 26933 25408 23983 21712 drop weib 636975 543729 492663 435846 206534 197538 80620 75470 69931 65814 49933 48666 42202 37247 34433 31816 30386 25208 23714 22314 20077 cens ln 638941 546001 495820 440389 209234 200571 82411 77395 71869 67707 51635 50414 43957 38943 36136 33441 32009 26783 25259 23833 21557 drop ln 638941 545106 493727 436682 206966 197928 81032 75847 70267 66158 50145 48860 42346 37361 34521 31883 30458 25238 23728 22316 20064 cens lglg 638716 546053 495984 440501 209458 200798 82476 77463 71928 67759 51703 50484 44024 39010 36203 33506 32070 26846 25322 23894 21615 drop lglg 638716 545166 493943 436933 207270 198237 81161 75977 70387 66271 50269 48988 42474 37482 34641 32002 30574 25351 23838 22424 20165

Table shows AIC values of the fixed-eect models including all covariates.

Table A.2: Standard deviation of the residuals for internal validation predictions. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 2084 2105 2140 2168 1932 1935 1826 1838 1861 1878 1957 1958 1984 2006 2002 2004 2011 2048 2052 2064 2074 drop linear model 2084 2076 2093 2029 1710 1706 1439 1422 1454 1480 1492 1496 1457 1387 1352 1333 1304 1292 1313 1294 1303 cens exp 2087 2134 2199 2284 2420 2439 2722 2708 2785 2863 3321 3388 3571 4234 4358 4203 4249 6392 6737 8552 9199 drop exp 2087 2079 2096 2032 1717 1713 1441 1425 1456 1484 1493 1496 1460 1389 1354 1335 1306 1294 1316 1297 1302 cens weib 2095 2147 2212 2295 2431 2446 2742 2752 2839 2972 4706 5807 7137 1883213 993379 77371 64210 692005 745833 16064820 96112146 drop weib 2095 2087 2104 2040 1725 1722 1451 1435 1467 1493 1506 1511 1473 1402 1367 1348 1319 1308 1329 1312 1321 cens ln 2106 2160 2226 2310 2445 2461 2766 2748 2820 2880 3519 3636 3706 255371 160786 40442 33066 426261 461323 1911965 6673515 drop ln 2106 2097 2115 2051 1733 1730 1467 1449 1481 1509 1523 1527 1488 1413 1379 1359 1330 1319 1341 1324 1334 cens lglg 2112 2158 2224 2308 2444 2460 2767 2751 2822 2884 3448 3564 3674 471061 279907 43035 35100 482294 530464 4275393 5714514 drop lglg 2112 2101 2118 2055 1736 1730 1464 1447 1479 1507 1521 1525 1487 1412 1378 1358 1330 1318 1340 1324 1333

Table shows the standard deviation of the residuals, while predicting values on the train set.

Table A.3: Kendall rank correlation coe cient for internal validation predictions. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 0.197 0.190 0.188 0.191 0.223 0.223 0.274 0.277 0.278 0.280 0.304 0.305 0.315 0.322 0.317 0.321 0.322 0.337 0.347 0.344 0.362 drop linear model 0.197 0.188 0.186 0.193 0.211 0.205 0.227 0.216 0.218 0.220 0.249 0.229 0.212 0.212 0.219 0.217 0.221 0.224 0.228 0.235 0.235 cens exp 0.157 0.189 0.190 0.185 0.199 0.201 0.228 0.233 0.241 0.243 0.247 0.246 0.261 0.218 0.232 0.276 0.276 0.197 0.219 0.176 0.201 drop exp 0.157 0.183 0.182 0.187 0.197 0.192 0.220 0.209 0.211 0.212 0.247 0.248 0.223 0.206 0.213 0.211 0.214 0.219 0.220 0.228 0.237 cens weib 0.161 0.159 0.160 0.162 0.162 0.166 0.191 0.176 0.910 0.194 0.184 0.184 0.200 0.002 0.004 0.061 0.071 0.022 0.029 0.011 -0.006 drop weib 0.161 0.161 0.158 0.165 0.165 0.158 0.206 0.196 0.197 0.201 0.224 0.224 0.198 0.170 0.180 0.174 0.179 0.181 0.184 0.197 0.200 cens ln 0.126 0.127 0.130 0.131 0.137 0.141 0.168 0.163 0.174 0.183 0.136 0.142 0.179 0.003 0.005 0.026 0.031 0.015 0.017 0.013 -0.003 drop ln 0.126 0.131 0.127 0.136 0.134 0.124 0.158 0.135 0.133 0.138 0.171 0.171 0.122 0.095 0.099 0.097 0.102 0.106 0.110 0.108 0.111 cens lglg 0.128 0.132 0.134 0.134 0.140 0.144 0.155 0.147 0.166 0.175 0.142 0.148 0.183 0.002 0.003 0.025 0.030 0.015 0.016 0.012 0.007 drop lglg 0.128 0.135 0.131 0.139 0.137 0.128 0.175 0.153 0.151 0.157 0.181 0.179 0.129 0.101 0.105 0.101 0.107 0.111 0.115 0.112 0.116

Table shows estimation of the Kendall rank correlation coe cient. The test compares the ranking of the predicted values compared to the real values (on the train set).

Table A.4: Standard deviation of the residuals for external validation predictions. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 1409 1410 1412 1414 1464 1468 1927 2064 2022 2026 2100 2508 2291 2158 2046 1978 1946 1890 2146 2092 2127 drop linear model 1409 1410 1412 1413 1456 1459 1492 1472 1475 1479 1481 1481 1474 1462 1463 1463 1462 1460 1463 1464 1470 cens exp 1414 1421 1425 1429 1667 1715 3152 3686 3316 4869 7043 7744 8658 19146 27332 9891 9833 39090 45125 57291 65873 drop exp 1414 1411 1412 1456 1452 1530 19092 6919 4966 25441 1816 1696 9579 15795 3683 1473 1480 1507 1534 1519 1480 cens weib 1490 1437 1440 1445 1802 1916 153194 223888 144665 290582 204870 245761 354085 698547 1052286 763103 690576 86005838 166498057 1016885684 1746469745 drop weib 1490 1421 1426 1472 1473 1543 167523 55530 37078 40227 3187 2706 4864 4505 1803 1496 1495 1511 1532 1486 1658 cens ln 1540 1442 1442 1455 1657 1719 44715 51092 62394 95227 94517 107483 162046 185526 172214 325189 283588 41044174 65338997 101860794 177497788 drop ln 1540 1430 1432 1462 1464 1491 14445 8175 8607 7330 2657 2427 2721 1963 1580 1645 1630 1539 1544 1514 1708 cens lglg 1693 1488 1487 1531 1685 1740 115265 112509 130970 190777 145339 169778 176903 312457 296561 777420 666758 43744867 77943281 251401933 424541895 drop lglg 1693 1459 1471 1483 1490 1530 54939 23867 24110 18252 4230 3671 4615 2786 1803 2095 2058 1669 1639 1617 2309

Table shows the standard deviation of the residuals, while predicting values on the test set.

Table A.5: Kendall rank correlation coe cient for external validation predictions. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 0.192 0.191 0.186 0.185 0.154 0.150 0.149 0.152 0.151 0.152 0.146 0.151 0.147 0.142 0.142 0.135 0.135 0.117 0.135 0.135 0.116 drop linear model 0.192 0.189 0.185 0.183 0.139 0.129 0.073 0.080 0.078 0.078 0.085 0.081 0.091 0.087 0.084 0.083 0.087 0.093 0.090 0.085 0.074 cens exp 0.191 0.191 0.187 0.186 0.142 0.143 0.101 0.096 0.093 0.088 0.069 0.067 0.059 0.046 0.044 0.045 0.048 0.043 0.043 0.034 0.036 drop exp 0.191 0.184 0.180 0.161 0.160 0.129 -0.001 0.003 0.005 -0.001 0.037 0.043 0.043 0.043 0.043 0.087 0.005 0.076 0.072 0.073 0.065 cens weib 0.098 0.144 0.142 0.140 0.112 0.117 0.077 0.061 0.059 0.054 0.027 0.025 0.011 0.001 0.000 0.001 0.002 -0.002 -0.001 0.022 0.021 drop weib 0.098 0.150 0.140 0.129 0.127 0.102 -0.003 -0.003 -0.003 -0.003 0.005 0.006 0.006 0.006 0.006 0.034 0.086 0.037 0.035 0.037 0.010 cens ln 0.048 0.098 0.101 0.090 0.092 0.095 0.074 0.068 0.065 0.060 0.015 0.014 0.008 0.003 0.002 0.003 0.004 -0.003 -0.003 0.022 0.022 drop ln 0.048 0.105 0.100 0.107 0.106 0.093 -0.002 -0.002 -0.002 -0.002 0.001 0.002 0.002 0.002 0.002 0.005 0.036 0.011 0.012 0.006 0.001 cens lglg 0.035 0.074 0.075 0.062 0.086 0.089 0.070 0.061 0.060 0.057 0.020 0.018 0.011 0.004 0.003 0.003 0.005 -0.003 -0.003 0.022 0.022 drop lglg 0.035 0.080 0.071 0.098 0.094 0.082 -0.003 -0.003 -0.003 -0.002 0.000 0.000 0.000 0.000 0.000 0.001 0.002 0.007 0.009 0.003 -0.001

Table shows estimation of the Kendall rank correlation coe cient. The test compares the ranking of the predicted values compared to the real values (on the test set).

Table A.6: Akaike information criterion. Mixed-eects / frailty models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 cens exp 682761 589133 537238 478699 229239 220259 88969 83716 77737 72993 59073 57032 49107 42649 39662 36691 35112 29499 30171 28734 25959 drop exp 682761 587786 533953 471847 224928 215358 86911 81422 75566 70955 57820 55126 46937 40191 37217 34425 32849 29045 27536 26070 23290 cens weib 631740 540083 490409 435629 206277 197702 80524 75630 70268 66177 50435 49218 42914 38030 35268 32589 31165 26019 24547 23143 20911 drop weib 631740 539017 488136 431539 203774 194836 79157 74107 68740 64708 49095 47838 41447 36530 33741 31158 29743 24558 23093 21724 19509 cens ln 631266 539230 489366 434227 205420 196792 80157 75250 69915 65888 50151 48926 42572 37705 34957 32298 30875 25729 24255 22858 20627 drop ln 631266 538298 487356 430699 203266 194313 78904 73855 68503 64511 48877 47613 41207 36320 33533 30957 29549 24378 22900 21535 19339 cens lglg 630908 538103 487255 430630 203245 194309 80030 75132 69799 65771 50074 48852 42508 37656 34909 32248 30822 25686 24215 22821 20593 drop lglg 630908 538103 487255 430630 203245 194309 78822 73775 68428 64436 48820 47557 41168 36291 33499 30930 29524 24381 22917 21535 19352

Table shows AIC values of the mixed-eects models including all covariates.

Table A.7: Standard deviation of the residuals for internal validation predictions. Mixed-eects / frailty models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 2026 2045 2075 2093 1802 1804 1611 1626 1651 1672 1708 1709 1746 1785 1789 1774 1770 1803 1801 1811 1799 drop linear model 2026 2016 2027 1956 1610 1608 1364 1345 1380 1406 1421 1424 1377 1293 1249 1230 1202 1189 1207 1189 1184 cens exp 2025 2062 2122 2195 2263 2291 2601 2709 2712 2873 8292 8828 11195 drop exp 2025 2016 2028 1958 1639 1635 1407 1390 1429 1461 1426 1450 1423 1372 1335 1348 1319 cens weib 2048 2089 2149 2222 2330 2353 2563 2709 2762 2970 13681 15946 22575 drop weib 2048 2041 2056 1990 1672 1671 1394 1379 1412 1438 1444 1448 1409 1337 1300 1288 1258 1237 1256 1237 1247 cens ln 2077 2123 2185 2263 2384 2398 2658 2639 2698 2787 6492 7346 9608 drop ln 2077 2071 2089 2023 1706 1705 1437 1424 1457 1484 1491 1495 1458 1387 1350 1329 1300 1288 1308 1291 1301 cens lglg 2070 2118 2180 2257 2374 2389 2634 2622 2673 2761 6904 7656 9460 drop lglg 2070 2081 2016 1699 1698 1428 1416 1448 1475 1484 1488 1450 1341

Table shows the standard deviation of the residuals, while predicting values on the train set.

Table A.8: Kendall rank correlation coe cient for internal validation predictions. Mixed-eects / frailty models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 0.305 0.303 0.308 0.323 0.423 0.424 0.536 0.534 0.532 0.528 0.564 0.565 0.560 0.549 0.542 0.556 0.565 0.572 0.579 0.579 0.600 drop linear model 0.305 0.305 0.311 0.328 0.397 0.393 0.393 0.394 0.387 0.387 0.394 0.396 0.403 0.427 0.448 0.449 0.453 0.459 0.463 0.467 0.488 cens exp 0.314 0.307 0.316 0.325 0.379 0.370 0.457 0.451 0.469 0.457 0.273 0.280 0.285 drop exp 0.314 0.304 0.309 0.325 0.361 0.358 0.350 0.346 0.339 0.336 0.378 0.379 0.363 0.351 0.368 0.350 0.353 cens weib 0.289 0.268 0.278 0.284 0.304 0.281 0.398 0.351 0.368 0.351 0.202 0.209 0.217 drop weib 0.289 0.267 0.270 0.280 0.299 0.293 0.367 0.364 0.356 0.362 0.388 0.388 0.383 0.380 0.378 0.348 0.358 0.398 0.403 0.417 0.417 cens ln 0.255 0.228 0.234 0.237 0.261 0.237 0.321 0.315 0.335 0.310 0.208 0.218 0.226 drop ln 0.255 0.221 0.219 0.228 0.236 0.224 0.304 0.285 0.282 0.286 0.299 0.302 0.289 0.259 0.275 0.282 0.288 0.299 0.308 0.334 0.340 cens lglg 0.260 0.234 0.241 0.244 0.273 0.248 0.335 0.333 0.355 0.339 0.210 0.221 0.230 drop lglg 0.260 0.228 0.226 0.236 0.248 0.236 0.323 0.307 0.304 0.310 0.317 0.320 0.305

Table shows estimation of the Kendall rank correlation coe cient. The test compares the ranking of the predicted values compared to the real values (on the train set).

Table A.9: Standard deviation of the residuals for external validation predictions. Mixed-eects / frailty models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 1418 1422 1433 1459 1589 1588 2146 2195 2101 2080 2463 2892 2539 2278 2152 2136 2123 2021 2248 2191 2266 drop linear model 1418 1422 1438 1446 1488 1490 1499 1485 1481 1486 1489 1491 1489 1480 1483 1476 1472 1471 1473 1475 1482 cens exp 1447 1536 1554 1792 2254 2293 32724 19468 7678 10468 9649 9092 14362 drop exp 1447 1469 1514 1561 1660 1664 20361 10665 3280 10084 1505 1578 1578 1557 1629 1670 cens weib 1406 1417 1443 1487 1727 1820 3407 3624 3608 4576 15074 18774 39338 drop weib 1406 1414 1422 1428 1481 1476 1753 1587 1479 1491 1466 1465 1470 1468 1458 1496 cens ln 1406 1416 1426 1441 1545 1567 1873 1987 1996 2174 7904 9755 21802 drop ln 1406 1412 1415 1420 1449 1444 1457 1450 1440 1439 1471 1468 1462 1458 1465 1470 cens lglg 1406 1416 1428 1448 1606 1644 2018 2174 2182 2299 6869 8284 14181 drop lglg 1406 1412 1417 1423 1460 1454 1455 1448 1442 1441 1476 1474 1467 1468

Table shows the standard deviation of the residuals, while predicting values on the test set.

Table A.10: Kendall rank correlation coe cient for external validation predictions. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 full linear model 0.197 0.191 0.175 0.159 0.113 0.110 0.119 0.130 0.130 0.134 0.138 0.146 0.143 0.133 0.132 0.124 0.123 0.118 0.130 0.129 0.108 drop linear model 0.197 0.186 0.166 0.165 0.122 0.117 0.077 0.080 0.081 0.079 0.084 0.079 0.082 0.077 0.073 0.079 0.084 0.088 0.089 0.084 0.075 cens exp 0.022 0.016 -0.015 0.004 0.009 0.011 0.001 0.001 -0.002 0.000 0.003 0.005 0.006 -0.006 drop exp 0.022 -0.001 0.003 -0.001 -0.006 -0.004 -0.004 -0.005 -0.006 -0.005 -0.011 -0.015 -0.017 -0.006 -0.009 -0.008 cens weib 0.042 0.037 0.002 0.018 0.018 0.018 0.004 0.007 0.005 0.007 0.014 0.015 0.009 -0.004 drop weib 0.042 0.016 0.021 0.014 0.013 0.010 -0.003 -0.002 0.002 -0.010 -0.011 0.006 0.000 0.005 -0.001 -0.001 cens ln 0.048 0.040 0.011 0.020 0.025 0.023 0.005 0.007 0.002 0.005 0.011 0.012 0.004 -0.003 drop ln 0.048 0.017 0.021 0.025 0.021 0.015 0.005 0.001 0.011 -0.002 -0.005 0.010 0.003 0.003 0.002 -0.005 cens lglg 0.048 0.040 0.010 0.019 0.023 0.018 -0.003 0.003 -0.001 0.001 0.012 0.012 0.007 -0.002 drop lglg 0.048 0.016 0.020 0.023 0.017 0.013 0.004 -0.002 0.008 -0.008 -0.006 0.004 -0.001 -0.003 -0.002 -0.010

Table shows estimation of the Kendall rank correlation coe cient. The test compares the ranking of the predicted values compared to the real values (on the test set).

Table A.11: Standard deviation of the residuals for internal validation predictions on filtered dataset. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 45682 39997 36429 17350 16466 6848 6461 6019 5647 4394 3874 3462 3237 2992 2650 2425 2198 1783 1688 1352 1186 linear model 1239 1198 1201 1103 1108 1043 1051 1062 1075 1104 1108 1103 1071 1066 1061 1054 1049 1031 1027 1044 1059 linear model 1239 1194 1190 1075 1053 1003 995 1003 1012 997 932 884 858 839 818 797 793 725 738 782 768 cens exp 1241 1214 1238 1151 1173 8821 3666 3107 1661 1308 1358 2187 1572 1488 1473 4757 7057 10925 9531 36453 29022 drop exp 1241 1196 1193 1086 1063 1006 998 1006 1015 1002 939 893 865 841 820 800 794 726 740 783 768 cens weib 1246 1218 1243 1159 1187 52678843 6332778 2481013 2231651 1991 1520 957527 219761 29110 26482 5413296 9479944 15873387 16989704 157688009 386756744 drop weib 1246 1200 1197 1088 1065 1013 1001 1009 1018 1004 941 891 866 846 826 803 799 731 745 789 775 cens ln 1260 1230 1256 1164 1187 477104 448751 296487 63882 1538 1428 167223 64331 21298 2019 280261 1132558 1088972 1077234 7999907 11172981 drop ln 1260 1212 1209 1095 1072 1018 1008 1017 1026 1014 949 898 875 855 837 809 805 738 753 798 782 cens lglg 1228 1254 1162 1183 1002824 1067571 516372 74993 1414 1395 305640 92739 22452 2069 318945 2415363 2244147 2155302 34900064 7615209 drop lglg 1210 1207 1094 1071 1016 1007 1016 1025 1012 948 898 874 854 836 808 804 737 752 796 781

Table shows the standard deviation of the residuals, while predicting values on the train set. The dataset has been filtered by removing extreme values of the response variable Â­ approximately 1.1% of the initial dataset.

Table A.12: Kendall rank correlation coe cient for internal validation predictions on filtered dataset. Fixed-eect models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 45682 39997 36429 17350 16466 6848 6461 6019 5647 4394 3874 3462 3237 2992 2650 2425 2198 1783 1688 1352 1186 full linear model 0.284 0.272 0.265 0.255 0.259 0.212 0.214 0.212 0.211 0.224 0.227 0.235 0.232 0.241 0.260 0.263 0.265 0.266 0.268 0.265 0.278 drop linear model 0.284 0.267 0.263 0.256 0.250 0.203 0.193 0.193 0.199 0.209 0.220 0.219 0.234 0.241 0.260 0.230 0.217 0.217 0.224 0.223 0.206 cens exp 0.281 0.275 0.270 0.255 0.249 0.127 0.131 0.126 0.149 0.226 0.231 0.078 0.156 0.219 0.286 0.073 0.079 0.079 0.083 0.102 0.172 drop exp 0.281 0.262 0.259 0.233 0.229 0.192 0.179 0.182 0.185 0.191 0.198 0.188 0.210 0.232 0.249 0.218 0.210 0.211 0.217 0.220 0.209 cens weib 0.267 0.261 0.254 0.227 0.218 0.120 0.110 0.095 0.081 0.120 0.171 0.010 0.011 0.017 0.018 0.039 0.056 0.064 0.063 0.083 0.163 drop weib 0.267 0.248 0.243 0.212 0.207 0.149 0.160 0.169 0.177 0.183 0.187 0.182 0.201 0.211 0.225 0.199 0.189 0.195 0.201 0.199 0.184 cens ln 0.238 0.231 0.222 0.189 0.181 0.119 0.109 0.097 0.078 0.102 0.117 0.011 0.011 0.013 0.091 0.039 0.049 0.060 0.061 0.061 0.173 drop ln 0.238 0.216 0.210 0.174 0.169 0.132 0.119 0.122 0.125 0.107 0.123 0.129 0.134 0.142 0.150 0.153 0.135 0.145 0.156 0.153 0.124 cens lglg 0. 0.238 0.230 0.198 0.190 0.121 0.114 0.103 0.084 0.119 0.137 0.010 0.011 0.012 0.080 0.039 0.045 0.053 0.055 0.072 0.177 drop lglg 0. 0.223 0.217 0.182 0.176 0.144 0.131 0.134 0.138 0.126 0.129 0.135 0.140 0.151 0.158 0.162 0.145 0.156 0.166 0.167 0.129

Table shows estimation of the Kendall rank correlation coe cient. The test compares the ranking of the predicted values compared to the real values (on the train set). The dataset has been filtered by removing extreme values of the response variable Â­ approximately 1.1% of the initial dataset.

Table A.13: Accuracy of fast/slow classification of the models.
Cens % 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Sample Size 46296 40680 37524 33847 16986 16513 6994 6646 6207 5891 4545 4494 3995 3590 3391 3198 3102 2662 2547 2445 2257 exp 0.553 0.532 0.526 0.522 0.487 0.488 0.485 0.486 0.493 0.499 0.499 0.498 0.495 0.492 0.495 0.492 0.495 0.489 0.490 0.490 0.490 ACC weib ln 0.586 0.646 0.564 0.600 0.561 0.601 0.559 0.604 0.548 0.620 0.549 0.621 0.551 0.615 0.551 0.616 0.556 0.612 0.558 0.609 0.552 0.612 0.549 0.610 0.552 0.616 0.552 0.620 0.554 0.620 0.550 0.622 0.551 0.624 0.552 0.633 0.551 0.631 0.542 0.631 0.523 0.63 lglg 0.618 0.603 0.604 0.607 0.623 0.623 0.616 0.616 0.611 0.609 0.613 0.613 0.616 0.620 0.620 0.617 0.619 0.632 0.623 0.629 0.623

Accuracy of the models while classifying the duration as fast or slow. Time threshold is set as the median value of the duration of the entire dataset. The models used to calculate these values are corresponding to the results presented in Table A.2.

Appendix B Analysis scripts
B.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

Main analysis script

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # S u r v i v a l a n a l y s i s on time to d e l i v e r ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # c l e a r cache rm( l i s t = l s ( ) ) # # l i b r a r i e s necessary library ( s u r v i v a l ) library ( xtable ) library ( f l e x s u r v ) library ( l a t t i c e ) library ( actuar ) l i b r a r y ( rms ) library ( plyr ) l i b r a r y ( lme4 ) setwd ( " / U s e r s / s o k r a t i s / Documents / xx" ) # # u p d a t e d d a t a s e t c o n t a i n i n g r e c o r d s from 4 r e l e a s e s # # a g g r e a g a t e d a t t h e commit l e v e l data < read . csv ( " . / data / f o u r r e l e a s e s a g g r e g a t e d . anonymized . c s v " ,

87

Appendix B. Analysis scripts

B.1. Main analysis script

24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62

na . s t r i n g s = "" ) source ( " . / s c r i p t s / data p r e p r o c e s s .R" ) source ( " . / s c r i p t s / s u r v r e g c u r v e s h e l p e r .R" ) source ( " . / s c r i p t s / s u r v i v a l a n a l y s i s h e l p e r .R" ) source ( " . / s c r i p t s / v a l i d a t i o n h e l p e r .R" ) data < data p r e p r o ( data ) # data < d a t a [ d a t a $ y < 8760 , ] # # C r eate t h e d a t a s e t 123 f o r t h e 3 f i r s t r e l e a s e s train set data123 < data [ data$ major r e l e a s e number %i n% c ( "v . x" , "v . x+1" , "v . x+2" ) , ] data123 $ major r e l e a s e number < d r o p l e v e l s ( data123 $ major r e l e a s e number ) data123 $ d e v e l o p e r c o u n t r y < d r o p l e v e l s ( data123 $ d e v e l o p e r c o u n t r y ) data123 $ symptom < d r o p l e v e l s ( data123 $ symptom ) # data123$ d e v e l o p e r < d r o p l e v e l s ( data123$ d e v e l o p e r ) # # C r eate t h e d a t a s e t 4 f o r t h e l a s t r e l e a s e as t h e t e s t s e t data4 < data [ data$ major r e l e a s e number %i n% "v . x+3" , ] data4 $ major r e l e a s e number < d r o p l e v e l s ( data4 $ major r e l e a s e number ) data4 $ d e v e l o p e r c o u n t r y < d r o p l e v e l s ( data4 $ d e v e l o p e r c o u n t r y ) data4 $ symptom < d r o p l e v e l s ( data4 $ symptom ) # a l l v a r i a b l e s f o r f i x e d e f f e c t models covs a l l < c( " is defect " , " p r e p o s t ga " , " d i s t i n c t component count " , " d i s t i n c t f u n c t i o n count " , " developer country " , "symptom" )

88

Appendix B. Analysis scripts

B.1. Main analysis script

63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101

data4 f u l l < data4 data4 f r < data4 [ , c ( c o v s a l l , " d e v e l o p e r " ) ] data4 < data4 [ , c o v s a l l ] # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # f u l l a n a l y s i s f u n c t i o n c a l l ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # f u l l a n a l y s i s < s u r v an ( data s u r v = # # # # # # # # # # # # # # # # # CENSORING ## # # # # # # # # # # # # # # # # censoring points in a sequence c e n s seq < seq ( 0 . 0 1 , 0 . 2 , by = 0 . 0 1 ) cens a n a l y s i s < drop a n a l y s i s < list () list ()

data123 )

j < r a t i o cens < s i z e < 0 f o r ( i i n unique ( data123 $ d e l i v e r end date ) ) { j < j + 1 v a l i d < sum ( data123 $ submit date < i ) cens < sum ( data123 $ d e l i v e r end date > i & data123 $ submit date < i ) r a t i o cens [ j ] < cens / valid size [ j ] < valid } # # Loop f o r c a l l i n g t h e s u r v an f u n c t i o n f o r m u l t i p l e # # c e n s o r e d d a t a s e t s b a s e d on c e n s s e q z < 1 f o r ( i c e n s i n c e n s seq ) { tmp data < data123 # g e t t h e p o s i t i o n t h a t has t h e a r a t i o o f c e n s o r i n g e q u a l t o # what we a r e l o o k i n g f o r . However , i f t h e r e i s a p o s i t i o n # t h a t t h e c e n s o r i n g i s g r e a t e r and a t t h e same time

89

Appendix B. Analysis scripts

B.1. Main analysis script

102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140

# t h e o b s e r v a t i o n s a r e more take t h i s position instead . xx < max( which ( abs ( r a t i o c e n s i c e n s ) == min ( abs ( r a t i o cens i cens )))) pos < xx f o r ( k i n xx : j ) { i f ( s i z e [ k ] > s i z e [ xx ] & i c e n s < r a t i o c e n s [ k ] ) { pos < k } } data c e n s < c ( r a t i o c e n s [ pos ] , s i z e [ pos ] , unique ( tmp data$ d e l i v e r end date ) [ pos ] ) tmp data$ c e n s < 0 tmp data$ c e n s [ tmp data$ d e l i v e r end date > data c e n s [ 3 ] ] < # # Keep o n l y t h e o b s e r v a t i o n s t h a t have # # a s u b m i t d a t e e a r l i e r than t h e c e n s o r p o i n t sub data < subset ( tmp data , tmp data$ submit date < data c e n s [ 3 ] ) # drop symtpom l e v e l s sub data$ symptom < d r o p l e v e l s ( sub data$ symptom ) # f o r d e v e l o p e r c o u n t r y v a l u e s t h a t have l e s s than 2 # drop t h e i r r e c o r d s , b e c a u s e t h e y can ' t p r e d i c t a f t e r w a r d s help var < table ( sub data$ d e v e l o p e r c o u n t r y ) sub data < sub data [ sub data$ d e v e l o p e r c o u n t r y %i n% names ( help var ) [ help var > 2 ] , ] # drop d e v e l o p e r c o u n t r y l e v e l s sub data$ d e v e l o p e r c o u n t r y < d r o p l e v e l s ( sub data$ d e v e l o p e r c o u n t r y ) # # Save a copy o f t h e d a t a b e f o r e c h a n g i n g t h e y v a r

1

90

Appendix B. Analysis scripts

B.1. Main analysis script

141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179

# # t o c a l c u l a t e r e s i d u a l s . The f u n c t i o n " r e s i d u a l s " # # s h o u l d not be used when c e n s o r e d i n f o r m a t i o n e x i s t s backup data < sub data # # s e t the y of censored records to : # # " cens point submit date " sub data$ y [ sub data$ c e n s == 1 ] < ( data c e n s [ 3 ] sub data$ submit date [ sub data$ c e n s == 1 ] ) / 3600 # # For drop a n a l y s i s . # # # # # # # # # # # # # # # # # # # # # Remove t h e c e s n o r e d e n t r i e s k e e p t h e not c e n s o r e d sub data 2 < sub data [ sub data$ c e n s == 0 , ] sub data 2 $ symptom < d r o p l e v e l s ( sub data 2 $ symptom )

# drop v a l u e s t h a t have l e s s than 2 b e c a u s e t h e y can ' t p r e d i c t help var < table ( sub data 2 $ d e v e l o p e r c o u n t r y ) sub data 2 < sub data 2 [ sub data 2 $ d e v e l o p e r c o u n t r y %i n% names ( help var ) [ help var > 2 ] , ] sub data 2 $ d e v e l o p e r c o u n t r y < d r o p l e v e l s ( sub data 2 $ d e v e l o p e r c o u n t r y ) # C a l l t h e s u r v an f u n c t i o n f o r t h e c e n s o r e d d a t a cens a n a l y s i s [ [ z ] ] < s u r v an ( data s u r v = sub data , name data = paste ( " c e n s " , i cens , s e p = " " ) ) c e n s a n a l y s i s [ [ z ] ] $ rows < nrow ( sub data ) c e n s a n a l y s i s [ [ z ] ] $prop < data c e n s [ 1 ] c e n s a n a l y s i s [ [ z ] ] $ backup data < backup data print ( paste ( " done c e n s a n a l y s i s o f " , i cens , " with " , c e n s a n a l y s i s [ [ z ] ] $ rows , " records . " ))

91

Appendix B. Analysis scripts

B.1. Main analysis script

180 181 # c a l l t h e s u r v an f u n c t i o n f o r t h e f i l t e r e d / dropped d a t a 182 drop a n a l y s i s [ [ z ] ] < 183 s u r v an ( data s u r v = sub data 2 , 184 name data = paste ( " drop " , i cens , s e p = " " ) ) 185 drop a n a l y s i s [ [ z ] ] $ rows < nrow ( sub data 2 ) 186 drop a n a l y s i s [ [ z ] ] $prop < data c e n s [ 1 ] 187 drop a n a l y s i s [ [ z ] ] $ backup data < sub data 2 188 print ( paste ( 189 " done drop a n a l y s i s o f " , 190 i cens , 191 " with " , 192 drop a n a l y s i s [ [ z ] ] $ rows , 193 " records . " 194 )) 195 196 z < z + 1 197 } 198 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

92

Appendix B. Analysis scripts

B.2. Data preprocess

B.2

Data preprocess

The content of ./scripts/data preprocess.R which is called in the main script, is given below.
1 # # # # # # # # # # # # # # # # # # # # # # # # 2 # # d a t a p r e p r o c e s s i n g ## 3 # # # # # # # # # # # # # # # # # # # # # # # # 4 5 data p r e p r o < function ( data ) { 6 # # Convert d a t e s t o Unix t i m e s 7 data$ submit date < 8 as . numeric ( as . POSIXct ( data$ submit date , 9 format = "%Y % m %d % H.%M.%S" ) ) 10 data$ d e l i v e r end date < 11 as . numeric ( as . POSIXct ( 12 data$ d e l i v e r end date , 13 format = "%Y % m %d % H.%M.%S" , 14 na . rm = T 15 )) 16 17 # # Remove some e n t r i e s t h a t due t o t i m e z o n e 18 # # e r r o r s end up h a v i n g ( s u b m i t d a t e > d e l i v e r end d a t e ) 19 data < data [ data$ d e l i v e r end date > data$ submit date , ] 20 21 # # Remove 3 e n t r i e s t h a t do not have a symptom t a g 22 data < data [ ( i s . na ( data$ symptom ) != 1 ) , ] 23 24 # # A g g r e g a t e 2 symptom t a g s t h a t o v e r l a p 25 data$ symptom [ data$ symptom == ' Test f a i l e d ' ] < 26 " test failed " 27 data$ symptom [ data$ symptom == ' B u i l d f a i l e d ' ] < 28 " build f a i l e d " 29 data$ symptom < d r o p l e v e l s ( data$ symptom ) 30 31 # # C r eate t h e i s d e f e c t v a r i a b l e t h a t we had i n t h e p r e v i o u s d a t a s e t 32 data$ i s d e f e c t < 0 33 # t a b l e ( data$ c l a s s i f i c a t i o n ) 34 data$ i s d e f e c t [ data$ c l a s s i f i c a t i o n %i n% c ( 35 "Code d e f e c t " ,

93

Appendix B. Analysis scripts

B.2. Data preprocess

36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61

"Code D e f e c t " , "Code D e f e c t " , " Test Case D e f e c t " , " Documentation D e f e c t " )] < 1 data < data [ order ( data$ d e l i v e r end date ) , ]

# # Remove f i r s t c h a r a c t e r from a l l # # d e v e l o p e r names ( t o have numeric v a l u e s o n l y ) data$ d e v e l o p e r < substring ( data$ d e v e l o p e r , 2 ) data$ d e v e l o p e r < as . numeric ( data$ d e v e l o p e r ) # # C r eate our " y " v a r i a b l e data$ y < ( data$ d e l i v e r end date

data$ submit date ) / 3600

# # C r eate a b i n a r y v a r i a b l e r e p r e s e n t i n g f a s t / s l o w f i x data$ f s < ' f ' data$ f s [ data$ y > median ( data$ y ) ] < ' s ' # d a t a < na . omit ( d a t a ) data < data [ ( i s . na ( data$ y ) != 1 ) , ] data$ c e n s < 0 data }

94

Appendix B. Analysis scripts

B.3. Survival analysis helper script

B.3

Survival analysis helper script

The content of ./scripts/survival analysis helper.R which is called in the main script, is given below.
1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 2 # # Complete S u r v i v a l A n a l y s i s f u n c t i o n ## 3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 4 5 s u r v an < 6 function ( data surv , 7 covs = covs all , 8 name data = " f u l l " ) { 9 c o v s < paste ( covs , c o l l a p s e = " + " ) 10 c o v s < paste ( " Surv ( y , c e n s == 0 ) ~ " , c o v s ) 11 c o v s f r a i l t y coxph < 12 paste ( c ( covs , " f r a i l t y ( d e v e l o p e r , s p a r s e = F) " ) , 13 collapse = " + ") 14 covs f r a i l t y survreg < 15 paste ( c ( covs , " f r a i l t y . g a u s s i a n ( d e v e l o p e r , s p a r s e = F) " ) , 16 collapse = " + ") 17 18 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 19 # # Kaplan Meier & Cumulative h a z a r d ## 20 # # with confidence i n t e r v a l ## 21 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 22 23 surv c i < 24 s u r v f i t ( Surv ( y , c e n s == 0 ) ~ 1 , c o n f . i n t = TRUE, 25 data = data s u r v ) 26 # # second s u r v f i t where we k e e p o n l y t h e non c e n s o r e d d a t a 27 surv c i 2 < 28 s u r v f i t ( Surv ( y , c e n s == 0 ) ~ 1 , c o n f . i n t = TRUE, 29 data = data s u r v [ data s u r v $ c e n s == 0 , ] ) 30 31 # # P l o t a Kaplan Meier graph and a c u m u l a t i v e h a z a r d graph . 32 # # Noticeable i s the quick f i x of a big proportion of the 33 # # d a t a b u t some o b s e r v a t i o n s a r e d r a g g i n g t h e graph 34 # # to the r i g h t . 35 pdf ( paste ( paste ( " gr ap h s / " , name data ) ,

95

Appendix B. Analysis scripts

B.3. Survival analysis helper script

36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74

"km ch . pdf " , s e p = " " ) ) par ( mfrow = c ( 1 , 2 ) , pty = ' s ' ) plot ( s u r v c i , x l a b = " time to d e l i v e r " , ylab = " Survival " , main = " Kaplan Meier s u r v i v a l graph " ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) plot ( surv ci , fun = "cumhaz" , x l a b = " time to d e l i v e r " , y l a b = " Cumulative hazard r a t e " , main = " Cumulative hazard graph " ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) dev . o f f ( ) pdf ( paste ( paste ( " gr ap h s / " , name data ) , "km ch l o g . pdf " , s e p = " " ) ) par ( mfrow = c ( 1 , 2 ) , pty = ' s ' ) plot ( surv ci , x l a b = " time to d e l i v e r ( l o g ) " , ylab = " Survival " , log = ' x ' , main = " Kaplan Meier s u r v i v a l graph " ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) plot ( surv ci , fun = "cumhaz" , x l a b = " time to d e l i v e r ( l o g ) " , log = ' x ' , y l a b = " Cumulative hazard r a t e " , main = " Cumulative hazard graph " ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) dev . o f f ( ) # # As i t i s shown from t h e g r a p h s 1 and 2 which

96

Appendix B. Analysis scripts

B.3. Survival analysis helper script

75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

# # # # # # # #

r e p r e s e n t a Kaplan Meier c u r v e and a Cumulative h a z a r d graph r e s p e c t i v e l y , most o f t h e r e p o r t s a r e f i x e d v e r y f a s t , w h i l e some o f them t a k e l o n g e r time t o g e t r e s o l v e d .

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # P l o t a b a s i c model f o r two c l a s s e s o f # # a single attribute ( is defect ). pdf ( paste ( name data , "km ch cov . pdf " , s e p = " " ) ) par ( mfrow = c ( 2 , 2 ) ) surv 1 1 < s u r v f i t ( Surv ( y , c e n s == 0 ) ~ i s d e f e c t , c o n f . i n t = FALSE, data = data s u r v ) plot ( surv 1 1 , x l a b = " time to d e l i v e r " , ylab = " Survival " , main = " Kaplan Meier s u r v i v a l graph " , col = c ( 1 , 2 ) , xlim = c ( 0 , 2 0 0 0 ) ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) legend ( " t o p r i g h t " , c ( " d e f e c t " , "non d e f e c t " ) , col = c ( 2 , 1 ) , l t y = 1) plot ( surv 1 1 , fun = "cumhaz" , x l a b = " time to d e l i v e r " , y l a b = " Cumulative hazard r a t e " , main = " Cumulative hazard graph " , col = c ( 1 , 2 ) , xlim = c ( 0 , 2 0 0 0 ) , ylim = c ( 0 , 3 ) ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) legend ( " b o t t o m r i g h t " ,

97

Appendix B. Analysis scripts

B.3. Survival analysis helper script

114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152

c ( " d e f e c t " , "non d e f e c t " ) , col = c ( 2 , 1 ) , l t y = 1) # P l o t a b a s i c model f o r two c l a s s e s o f # # a s i n g l e a t t r i b u t e ( p r e p o s t ga ) . surv 1 2 < s u r v f i t ( Surv ( y , c e n s == 0 ) ~ p r e p o s t ga , c o n f . i n t = FALSE, data = data s u r v ) plot ( surv 1 2 , x l a b = " time to d e l i v e r " , ylab = " Survival " , main = " Kaplan Meier s u r v i v a l graph " , col = c ( 1 , 2 ) , xlim = c ( 0 , 2 0 0 0 ) ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) legend ( " t o p r i g h t " , c ( " p r e ga " , " p o s t ga " ) , col = c ( 2 , 1 ) , l t y = 1) plot ( surv 1 2 , fun = "cumhaz" , x l a b = " time to d e l i v e r " , y l a b = " Cumulative hazard r a t e " , main = " Cumulative hazard graph " , col = c ( 1 , 2 ) ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) legend ( " b o t t o m r i g h t " , c ( " p r e ga " , " p o s t ga " ) , col = c ( 2 , 1 ) , l t y = 1) dev . o f f ( ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

98

Appendix B. Analysis scripts

B.3. Survival analysis helper script

153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191

# # Cox p r o p o r t i o n a l h a z a r d models ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # coxph f u l l < coxph ( as . formula ( c o v s ) , data = data s u r v ) coxph f r f u l l < coxph ( as . formula ( c o v s f r a i l t y coxph ) , data = data s u r v ) coxph step < step ( coxph f u l l , trace = 0 ) # cox zph t e s t < cox . zph ( coxph f u l l ) $ t a b l e [ , 3 ] # t e s t o f p r o p o r t i o n a l i t y ass umpti on print ( "Cox PH done . . . " ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # F u l l y p a r a m e t r i c models ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # We u t i l i z e f u l l y p a r a m e t r i c models , b e c a u s e # # t h e y model t h e time to d e l i v e r , which i s # more a p p r o p r i a t e than t h e h a z a r d e s t i m a t e d # # by t h e Cox models above . However , t h e i r # # d i s a d v a n t a g e i s t h a t we have t o assume # # a d i s t r i b u t i o n f o r our e m p i r i c a l d a t a # # ( or t h e i r r e s i d u a l s ) f o r e f f i c i e n t e s t i m a t i o n . # # The g o o d n e s s o f f i t can be c a l c u l a t e d by t h e # # AIC and o p t i c a l l y from S u r v i v a l g r a p h s . dists < c ( " E x p o n e n t i a l " , " Weibull " , " Lognormal " , " L o g l o g i s t i c " )

# exponential # # # # # # # # # # # # # exp1 < try ( s u r v r e g ( Surv ( y , c e n s == 0 ) ~ 1 , data = data surv , d i s t = " e x p o n e n t i a l " ) ) exp f u l l < s u r v r e g ( as . formula ( c o v s ) , data = data surv , d i s t = " e x p o n e n t i a l " ) exp f r f u l l < survreg (

99

Appendix B. Analysis scripts

B.3. Survival analysis helper script

192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230

as . formula ( c o v s f r a i l t y s u r v r e g ) , data = data surv , dist = " exponential " , x = T ) exp step < step ( exp f u l l , trace = 0 )

S exp < function ( x ) { 1 pexp ( x , 1 / exp ( exp1 $ c o e f f i c i e n t s [ 1 ] ) ) } print ( " E x p o n e n t i a l done . . . " ) # weibull # ######## weib1 < f l e x s u r v r e g ( Surv ( y , c e n s == 0 ) ~ 1 , data = data surv , d i s t = " w e i b u l l " ) weib f u l l < s u r v r e g ( as . formula ( c o v s ) , data = data surv , d i s t = " w e i b u l l " ) weib f r f u l l < survreg ( as . formula ( c o v s f r a i l t y s u r v r e g ) , data = data surv , dist = " weibull " , x = T ) weib step < step ( weib f u l l , trace = 0 ) S weib < function ( x ) { 1 pweibull ( x , exp ( weib1 $ c o e f f i c i e n t s [ 1 ] ) , exp ( weib1 $ c o e f f i c i e n t s [ 2 ] ) ) } print ( " Weibull done . . . " ) # lognormal # ########## ln1 < f l e x s u r v r e g ( Surv ( y , c e n s == 0 ) ~ 1 ,

100

Appendix B. Analysis scripts

B.3. Survival analysis helper script

231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269

data = data surv , d i s t = " l o g n o r m a l " ) ln f u l l < s u r v r e g ( as . formula ( c o v s ) , data = data surv , d i s t = " l o g n o r m a l " ) ln f r f u l l < survreg ( as . formula ( c o v s f r a i l t y s u r v r e g ) , data = data surv , d i s t = " lognormal " , x = T ) l n step < step ( l n f u l l , trace = 0 ) S l n < function ( x ) { 1 plnorm ( x , l n 1 $ c o e f f i c i e n t s [ 1 ] , exp ( l n 1 $ c o e f f i c i e n t s [ 2 ] ) ) } print ( " Lognormal done . . . " ) # loglogistic # # # # # # # # # # # # # lglg1 < f l e x s u r v r e g ( Surv ( y , c e n s == 0 ) ~ 1 , data = data surv , d i s t = " l l o g i s " ) lglg full < s u r v r e g ( as . formula ( c o v s ) , data = data surv , d i s t = " l o g l o g i s t i c " ) lglg fr full < survreg ( as . formula ( c o v s f r a i l t y s u r v r e g ) , data = data surv , dist = " loglogistic " , x = T ) l g l g step < step ( l g l g f u l l , trace = 0 ) S l l < function ( x ) { 1 p l l o g i s ( x , exp ( l g l g 1 $ c o e f f i c i e n t s [ 1 ] ) , 1 / exp ( l g l g 1 $ c o e f f i c i e n t s [ 2 ] ) )

101

Appendix B. Analysis scripts

B.3. Survival analysis helper script

270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308

} print ( " L o g l o g i s t i c done . . . " ) # # # # # # # # # # # # # # # # # # # # # # # # # # # then p l o t a l l t o g e t h e r pdf ( paste ( paste ( " gr ap h s / " , name data ) , " s u r v a l l . pdf " , s e p = " " ) ) plot ( surv ci , c o n f = " none " , x l a b = " time to d e l i v e r " , ylab = " Survival " , lty = 1 ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) l i n e s ( 0 : max( data s u r v $ y ) , S exp ( 0 : max( data s u r v $ y ) ) , col = 2 , l t y = 6) l i n e s ( 0 : max( data s u r v $ y ) , S weib ( 0 : max( data s u r v $ y ) ) , col = 3 , l t y = 2) l i n e s ( 0 : max( data s u r v $ y ) , S l n ( 0 : max( data s u r v $ y ) ) , col = 4 , l t y = 4) l i n e s ( 0 : max( data s u r v $ y ) , S l l ( 0 : max( data s u r v $ y ) ) , col = 5 , l t y = 5) legend ( x = " topright " , legend = c ( " Kaplan Meier " , d i s t s ) , lwd = 2 , bty = "n" , col = c ( " b l a c k " , 2 , 3 , 4 , 5 ) , l t y = c (1 , 6 , 2 , 4 , 5) )

102

Appendix B. Analysis scripts

B.3. Survival analysis helper script

309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347

dev . o f f ( ) pdf ( paste ( paste ( " gr ap h s / " , name data ) , " s u r v a l l ch . pdf " , s e p = " " ) ) plot ( surv ci , fun = "cumhaz" , c o n f = " none " , x l a b = " time to d e l i v e r " , y l a b = " Cumulative Hazard " , ylim = c ( 0 , 2 0 ) , lty = 1 ) grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) l i n e s ( 0 : max( data s u r v $ y ) , log ( S exp ( 0 : max( data s u r v $ y ) ) ) , col = 2 , l t y = 6) l i n e s ( 0 : max( data s u r v $ y ) , log ( S weib ( 0 : max( data s u r v $ y ) ) ) , col = 3 , l t y = 2) l i n e s ( 0 : max( data s u r v $ y ) , log ( S l n ( 0 : max( data s u r v $ y ) ) ) , col = 4 , l t y = 4) l i n e s ( 0 : max( data s u r v $ y ) , log ( S l l ( 0 : max( data s u r v $ y ) ) ) , col = 5 , l t y = 5) legend ( x = " bottomright " , legend = c ( " Cumulative hazard " , d i s t s ) , lwd = 2 , bty = "n" , col = c ( " b l a c k " , 2 , 3 , 4 , 5 ) , l t y = c (1 , 6 , 2 , 4 , 5) ) dev . o f f ( )

103

Appendix B. Analysis scripts

B.3. Survival analysis helper script

348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386

c ( try (AIC( exp1 ) ) , AIC( weib1 ) , AIC( l n 1 ) , AIC( l g l g 1 ) ) aic f u l l < c ( try (AIC( exp f u l l ) ) , AIC( weib f u l l ) , AIC( l n f u l l ) , AIC( l g l g f u l l ) ) aic fr f u l l < c (AIC( exp f r f u l l ) , AIC( weib f r f u l l ) , AIC( l n f r f u l l ) , AIC( l g l g f r f u l l ) ) names ( a i c 1 ) < d i s t s names ( a i c f u l l ) < d i s t s names ( a i c step ) < d i s t s # # Return a l i s t w i t h a l l t h e r e s u l t s . list ( surv c i = surv ci , surv c i 2 = surv c i 2 , coxph f u l l = coxph f u l l , coxph step = coxph step , coxph f r f u l l = coxph f r f u l l , exp1 = exp1 , exp f u l l = exp f u l l , exp f r f u l l = exp f r f u l l , weib1 = weib1 , weib f u l l = weib f u l l , weib f r f u l l = weib f r f u l l , l n 1 = ln1 , ln f u l l = ln f u l l , ln f r f u l l = ln f r f u l l , lglg1 = lglg1 , lglg full = lglg full , lglg fr full = lglg fr full , aic 1 = aic 1 , aic f u l l = aic f u l l ) }

aic 1 <

104

Appendix B. Analysis scripts

B.3. Survival analysis helper script

387 # # end o f s u r v an f u n c t i o n ## 388 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

105

Appendix B. Analysis scripts

B.4. Survival analysis helper script

B.4

Survival analysis helper script

The content of ./scripts/validation helper.R which is called in the main script, is given below.
1 # # # # # # # # # # # # # # # # # # # # # # # # # 2 # # V a l i d a t i o n f u n c t i o n ## 3 # # # # # # # # # # # # # # # # # # # # # # # # # 4 5 validate survival < 6 function ( v a l f i t , 7 f i t data , 8 v a l data , 9 v a l data f r , 10 f u l l data = data ) { 11 12 data4 f u l l < 13 f u l l data [ f u l l data$ major r e l e a s e number == "v . x+3" & 14 f u l l data$ d e v e l o p e r c o u n t r y != " y13 " , ] 15 16 # # # # # # # # # # # # # # 17 # # V i s u a l 1 ## 18 # # # # # # # # # # # # # # 19 20 f i t v a l 4 < s u r v f i t ( Surv ( y , c e n s == 0 ) ~ 1 , data = data4 f u l l ) 21 f i t v a l 123 < s u r v f i t ( Surv ( y , c e n s == 0 ) ~ 1 , data = f i t data ) 22 23 # P l o t a Kaplan Meier f o r t h e s e c o n d a r y d a t a s e t 24 # and a Kaplan Meier f o r t h e primary d a t a 25 pdf ( " v i s u a l s u r v c u r v e s . pdf " ) 26 plot ( 27 f i t val 4 , 28 x l a b = " time to d e l i v e r " , 29 ylab = " Survival " , 30 col = 6 , 31 main = " Kaplan Meier g r a p h s " 32 ) 33 l i n e s ( f i t v a l 1 2 3 , col = 5 ) 34 legend ( 35 x = " topright " ,

106

Appendix B. Analysis scripts

B.4. Survival analysis helper script

36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74

legend = c ( " Primary data " , " Secondary data " ) , lwd = 2 , bty = "n" , col = c ( 5 , 6 ) ) dev . o f f ( ) # # # # # # # # We s e e some d i f f e r e n c e s on t h e e m p i r i c a l d a t a . In t h e new ( v a l i d a t i o n / t e s t ) d a t a t h e s u r v i v a l c u r v e i s more s t e e p i n t h e b e g i n n i n g and extends further to the r i g h t .

# # # # # # # # # # # # # # # # V i s u a l 2 ## # # # # # # # # # # # # # # weib1 < f l e x s u r v r e g ( Surv ( y , c e n s == 0 ) ~ 1 , data = f i t data , d i s t = " w e i b u l l " ) # # P l o t a Kaplan Meier f o r t h e s e c o n d a r y d a t a (4 t h r e l e a s e ) # # and compare w i t h t h e b e s t f i t t e d model from t h e # # t r a i n data ( f i r s t 3 r e l e a s e s ) pdf ( " v i s u a l f i t . pdf " ) plot ( weib1 , x l a b = " time to d e l i v e r " , ylab = " Survival " , main = " Kaplan Meier vs F i t t e d model " , col = 5 ) l i n e s ( f i t v a l 4 , col = 6 ) legend ( x = " topright " , legend = c ( " Weibull model ( primary data ) " , " Kaplan Meier ( s e c o n d a r y data ) " ), lwd = 2 , bty = "n" ,

107

Appendix B. Analysis scripts

B.4. Survival analysis helper script

75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

col = c ( 5 , 6 ) ) dev . o f f ( ) # # # # # # # # # # # # # # } # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Save c o e f 1 and c o e f 2 o f t h e empty models t o s e e how t h e v a l u e s # # change w i t h p r o p o r t i o n o f c e n s o r i n g # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # censored cens coef1 < cens coef2 < We s e e t h a t t h e f i t i s not as good as e x p e c t e d which i s a l s o c o n c e i v e d by v i s u a l l y i n s p e c t i n g t h e p r e v i o u s p l o t (2 x Kaplan Meier c u r v e s ) . However , t h i s i s j u s t t h e e m p i r i c a l data , t h a t a r e e x p e c t e d t o have some c h a n g e s o v e r time . We w i l l be a s s e s s i n g t h e b e s t f i t t e d model from t h e t r a i n s e t , on t h e new t e s t s e t l a t e r on .

f u l l a n a l y s i s $ weib1 $ c o e f f i c i e n t s [ 1 ] f u l l a n a l y s i s $ weib1 $ c o e f f i c i e n t s [ 2 ]

f o r ( i i n 1 : length ( c e n s a n a l y s i s ) ) { c e n s c o e f 1 < c ( c e n s c o e f 1 , c e n s a n a l y s i s [ [ i ] ] $ exp1 $ c o e f f i c i e n t s [ 1 ] ) cens coef2 < c ( c e n s c o e f 2 , c e n s a n a l y s i s [ [ i ] ] $ exp1 $ c o e f f i c i e n t s [ 2 ] ) } x < 0:20 par ( mar = c ( 5 , 4 , 4 , 5 ) + 0 . 1 ) plot ( x, cens coef1 , type = " l " , col = " r e d " , x l a b = " C e n s o r i n g P r o p o r t i o n (%)" , ylab = " c o e f f i c i e n t 1" , main = " e x p o n e n t i a l " , sub = " c e n s a n a l y s i s "

108

Appendix B. Analysis scripts

B.4. Survival analysis helper script

114 ) 115 par ( new = TRUE) 116 plot ( 117 x, 118 cens coef2 , 119 type = " l " , 120 col = " b l u e " , 121 xaxt = "n" , 122 yaxt = "n" , 123 xlab = "" , 124 ylab = "" 125 ) 126 axis ( 4 ) 127 mtext ( " c o e f f i c i e n t 2 " , s i d e = 4 , l i n e = 3 ) 128 legend ( 129 " top " , 130 col = c ( " r e d " , " b l u e " ) , 131 lty = 1 , 132 legend = c ( " c o e f 1 " , " c o e f 2 " ) 133 ) 134 legend ( " top " , 135 col = " r e d " , 136 lty = 1 , 137 legend = " c o e f 1 " ) 138 139 #dropped 140 drop c o e f 1 < f u l l a n a l y s i s $ exp1 $ c o e f f i c i e n t s [ 1 ] 141 drop c o e f 2 < f u l l a n a l y s i s $ exp1 $ c o e f f i c i e n t s [ 2 ] 142 143 f o r ( i i n 1 : length ( drop a n a l y s i s ) ) { 144 drop c o e f 1 < 145 c ( drop c o e f 1 , drop a n a l y s i s [ [ i ] ] $ weib1 $ c o e f f i c i e n t s [ [ 1 ] ] ) 146 drop c o e f 2 < 147 c ( drop c o e f 2 , drop a n a l y s i s [ [ i ] ] $ weib1 $ c o e f f i c i e n t s [ [ 2 ] ] ) 148 } 149 150 x < 0 : 2 0 151 par ( mar = c ( 5 , 4 , 4 , 5 ) + 0 . 1 ) 152 plot (

109

Appendix B. Analysis scripts

B.4. Survival analysis helper script

153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184

x, drop c o e f 1 , type = " l " , col = " r e d " , x l a b = " C e n s o r i n g P r o p o r t i o n (%)" , ylab = " c o e f f i c i e n t 1" , main = " e x p o n e n t i a l " , sub = " drop a n a l y s i s " ) par ( new = TRUE) plot ( x, drop c o e f 2 , type = " l " , col = " b l u e " , xaxt = "n" , yaxt = "n" , xlab = "" , ylab = "" ) axis ( 4 ) mtext ( " c o e f f i c i e n t 2 " , s i d e = 4 , l i n e = 3 ) legend ( " top " , col = c ( " r e d " , " b l u e " ) , lty = 1 , legend = c ( " c o e f 1 " , " c o e f 2 " ) ) legend ( " top " , col = " r e d " , lty = 1 , legend = " c o e f 1 " )

110

Appendix B. Analysis scripts

B.5. External validation

B.5

External validation

The following script is called independently and provides metrics for external validation.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # # # # # # # # # # # # # # # # # # # # # # # # # # # E x t e r n a l v a l i d a t i o n ## # # # # # # # # # # # # # # # # # # # # # # # # # # ## f o r f i x e d e f f e c t # ## f u l l a n a l y s i s # # # # # # # # # # Calculate the residuals for the f r a i l t y models t h a t drop some l e v e l s t h e m s e l v e s . They p r o b a b l y drop t h e l e v e l s t h a t don ' t have enough v a l u e s . f u l l analysis

pred exp <

predict ( f u l l a n a l y s i s $exp f u l l , newdata = data4 ) pred weib < predict ( f u l l a n a l y s i s $ weib f u l l , newdata = data4 ) pred l n < predict ( f u l l a n a l y s i s $ l n f u l l , newdata = data4 ) pred l g l g < predict ( f u l l a n a l y s i s $ l g l g f u l l , newdata = data4 ) res res res res exp < pred exp data4 f u l l $ y weib < pred weib data4 f u l l $ y l n < pred l n data4 f u l l $ y l g l g < pred l g l g data4 f u l l $ y

r c c exp < cor . t e s t ( pred type r c c weib < cor . t e s t ( pred type rcc ln < cor . t e s t ( pred type

exp , data4 f u l l $ y , = ' kendal ' )$ estimate weib , data4 f u l l $ y , = ' kendal ' )$ estimate ln , data4 f u l l $ y , = ' kendal ' )$ estimate

111

Appendix B. Analysis scripts

B.5. External validation

37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75

rcc l gl g < cor . t e s t ( pred l g l g , data4 f u l l $ y , type = ' k e n d a l ' ) $ e s t i m a t e a c c exp < ( sum ( pred exp < 140 & data4 f u l l $ y < 1 4 0 ) + sum ( pred data4 f u l l $ y >= 1 4 0 ) ) / length ( data4 f u l l $ y ) a c c weib < ( sum ( pred weib < 140 & data4 f u l l $ y < 1 4 0 ) + sum ( pred data4 f u l l $ y >= 1 4 0 ) ) / length ( data4 f u l l $ y ) a c c l n < ( sum ( pred l n < 140 & data4 f u l l $ y < 1 4 0 ) + sum ( pred data4 f u l l $ y >= 1 4 0 ) ) / length ( data4 f u l l $ y ) a c c l g l g < ( sum ( pred l g l g < 140 & data4 f u l l $ y < 1 4 0 ) + sum ( pred data4 f u l l $ y >= 1 4 0 ) ) / length ( data4 f u l l $ y )

exp >= 140 &

weib >= 140 &

l n >= 140 &

l g l g >= 140 &

cat ( paste ( " f u l l analysis / fixed / external " , " \ n" , "\ t " , f u l l a n a l y s i s $exp f u l l $ d i s t , " \ tSD : " , round ( sd ( r e s exp ) ) , "RCC: " , round ( r c c exp , 3 ) , "ACC: " , round ( a c c exp , 3 ) , " \ n" , "\ t " , f u l l a n a l y s i s $ weib f u l l $ d i s t , " \ tSD : " , round ( sd ( r e s weib ) ) , "RCC: " ,

112

Appendix B. Analysis scripts

B.5. External validation

76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114

round ( r c c weib , 3 ) , "ACC: " , round ( a c c weib , 3 ) , " \ n" , "\ t " , f u l l analysis $ln f u l l $dist , " \ tSD : " , round ( sd ( r e s l n ) ) , "RCC: " , round ( r c c ln , 3 ) , "ACC: " , round ( a c c ln , 3 ) , " \ n" , "\ t " , f u l l analysis$ l gl g f u l l $dist , " \ tSD : " , round ( sd ( r e s l g l g ) ) , "RCC: " , round ( r c c l g l g , 3 ) , "ACC: " , round ( a c c l g l g , 3 ) ) ) # # # # # # # # # # # # # # # # # # # # # # ## f o r mixed e f f e c t s # ## f u l l a n a l y s i s # # # # # # # # # # S p l i t the data in 2 parts 1 excludes the developers that the sparse matrix i s ignoring 1 w i t h r e s t o f them ( f o r t h i s one we w i l l ignore the developers c o e f f i c i e n t ) c ( dev1 , dev4 , dev6 , . . . )

exclude <

data4 f r p a r t 1 < data4 f r [ ! ( data4 f r $ d e v e l o p e r %i n% e x c l u d e ) , ] data4 f u l l p a r t 1 <

113

Appendix B. Analysis scripts

B.5. External validation

115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153

data4 f u l l [ ! ( data4 f u l l $ d e v e l o p e r %i n% e x c l u d e ) , ] data4 f r p a r t 2 < data4 f r [ data4 f r $ d e v e l o p e r %i n% e x c l u d e , ] data4 f u l l p a r t 2 < data4 f u l l [ data4 f u l l $ d e v e l o p e r %i n% e x c l u d e , ] # For p a r t 1 , p r e d i c t w i l l work f i n e . # t e s t : pred p a r t 1 exp < # p r e d i c t ( f u l l a n a l y s i s $ exp f r f u l l , newdata = d a t a 4 f r ) pred p a r t 1 exp < predict ( f u l l a n a l y s i s $exp f r f u l l , newdata = data4 f r p a r t 1 ) pred p a r t 1 weib < predict ( f u l l a n a l y s i s $ weib f r f u l l , newdata = data4 f r p a r t 1 ) pred p a r t 1 l n < predict ( f u l l a n a l y s i s $ l n f r f u l l , newdata = data4 f r p a r t 1 ) pred p a r t 1 l g l g < predict ( f u l l a n a l y s i s $ l g l g f r f u l l , newdata = data4 f r p a r t 1 ) # # x x For p a r t 2 we have t o i g n o r e t h e c o e f f i c i e n t s for the developers colnames < colnames ( f u l l a n a l y s i s $exp f r f u l l $ x ) colnames 2 < x colnames [ substring ( x colnames , 1 , 5 ) != " f r a i l " ]

tmp s u r v r e g < survreg ( Surv ( y , rep ( 1 , nrow ( data4 f u l l p a r t 2 ) ) ) ~ i s d e f e c t + p r e p o s t ga + d i s t i n c t component count + d i s t i n c t function count + d e v e l o p e r c o u n t r y + symptom , data = data4 f u l l part2 , x = T ) # t h e d a t a s e t i s s m a l l e r now , # so j u s t k e e p t h e columns n e c c e s s a r y x colnames 2 < colnames ( tmp s u r v r e g $ x )

114

Appendix B. Analysis scripts

B.5. External validation

154 x colnames 2 < 155 names ( f u l l a n a l y s i s $exp f r f u l l $ c o e f f i c i e n t s ) %i n% 156 x colnames 2 157 158 pred p a r t 2 exp < 159 tmp s u r v r e g $ x %% 160 f u l l a n a l y s i s $exp f r f u l l $ c o e f f i c i e n t s [ x colnames 2 ] 161 pred p a r t 2 weib < 162 tmp s u r v r e g $ x %% 163 f u l l a n a l y s i s $ weib f r f u l l $ c o e f f i c i e n t s [ x colnames 2 ] 164 pred p a r t 2 l n < 165 tmp s u r v r e g $ x %% 166 f u l l a n a l y s i s $ l n f r f u l l $ c o e f f i c i e n t s [ x colnames 2 ] 167 pred p a r t 2 l g l g < 168 tmp s u r v r e g $ x %% 169 f u l l a n a l y s i s $ l g l g f r f u l l $ c o e f f i c i e n t s [ x colnames 2 ] 170 171 # k e e p t h e p r e d i c t e d v a l u e s as w e l l as t h e r e a l v a l u e s 172 # and t h e n g e t t h e r e s i d u a l s 173 pred y exp < 174 cbind ( c ( pred p a r t 1 exp , exp ( pred p a r t 2 exp ) ) , 175 c ( data4 f u l l p a r t 1 $ y , data4 f u l l p a r t 2 $ y ) ) 176 pred y weib < 177 cbind ( c ( pred p a r t 1 weib , exp ( pred p a r t 2 weib ) ) , 178 c ( data4 f u l l p a r t 1 $ y , data4 f u l l p a r t 2 $ y ) ) 179 pred y l n < 180 cbind ( c ( pred p a r t 1 ln , exp ( pred p a r t 2 l n ) ) , 181 c ( data4 f u l l p a r t 1 $ y , data4 f u l l p a r t 2 $ y ) ) 182 pred y l g l g < 183 cbind ( c ( pred p a r t 1 l g l g , exp ( pred p a r t 2 l g l g ) ) , 184 c ( data4 f u l l p a r t 1 $ y , data4 f u l l p a r t 2 $ y ) ) 185 186 r e s exp < pred y exp [ , 1 ] pred y exp [ , 2 ] 187 r e s weib < pred y weib [ , 1 ] pred y weib [ , 2 ] 188 r e s l n < pred y l n [ , 1 ] pred y l n [ , 2 ] 189 r e s l g l g < pred y l g l g [ , 1 ] pred y l g l g [ , 2 ] 190 191 a c c exp < 192 ( sum ( pred y exp [ , 1 ] < 140 &

115

Appendix B. Analysis scripts

B.5. External validation

193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231

pred y exp [ , 2 ] < 1 4 0 ) + sum ( pred y exp [ , 1 ] >= 140 & pred y exp [ , 2 ] >= 1 4 0 ) ) / length ( pred y exp [ , 1 ] ) a c c weib < ( sum ( pred y weib [ , 1 ] < 140 & pred y weib [ , 2 ] < 1 4 0 ) + sum ( pred y weib [ , 1 ] >= 140 & pred y weib [ , 2 ] >= 1 4 0 ) ) / length ( pred y weib [ , 1 ] ) acc ln < ( sum ( pred y l n [ , 1 ] < 140 & pred y l n [ , 2 ] < 1 4 0 ) + sum ( pred y l n [ , 1 ] >= 140 & pred y l n [ , 2 ] >= 1 4 0 ) ) / length ( pred y l n [ , 1 ] ) acc l g l g < ( sum ( pred y l g l g [ , 1 ] < 140 & pred y l g l g [ , 2 ] < 1 4 0 ) + sum ( pred y l g l g [ , 1 ] >= 140 & pred y l g l g [ , 2 ] >= 1 4 0 ) ) / length ( pred y l g l g [ , 1 ] ) r c c exp < round ( cor . t e s t ( pred y exp [ , 1 ] , pred y exp [ , 2 ] , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) r c c weib < round ( cor . t e s t ( pred y weib [ , 1 ] , pred y weib [ , 2 ] , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) rcc ln < round ( cor . t e s t ( pred y l n [ , 1 ] , pred y l n [ , 2 ] , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) rcc l gl g < round ( cor . t e s t ( pred y l g l g [ , 1 ] , pred y l g l g [ , 2 ] , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) cat ( paste ( " f u l l a n a l y s i s / mixed / e x t e r n a l " , " \ n" , "\ t " , f u l l a n a l y s i s $exp f r f u l l $ d i s t , " \ tSD : " , round ( sd ( r e s exp ) ) , "RCC: " , r c c exp ,

116

Appendix B. Analysis scripts

B.5. External validation

232 "ACC: " , 233 round ( a c c exp , 3 ) , 234 " \ n" , 235 "\ t " , 236 f u l l a n a l y s i s $ weib f r f u l l $ d i s t , 237 " \ tSD : " , 238 round ( sd ( r e s weib ) ) , 239 "RCC: " , 240 r c c weib , 241 "ACC: " , 242 round ( a c c weib , 3 ) , 243 " \ n" , 244 "\ t " , 245 f u l l analysis $ln f r f u l l $dist , 246 " \ tSD : " , 247 round ( sd ( r e s l n ) ) , 248 "RCC: " , 249 r c c ln , 250 "ACC: " , 251 round ( a c c ln , 3 ) , 252 " \ n" , 253 "\ t " , 254 f u l l analysis$ l gl g fr f u l l $dist , 255 " \ tSD : " , 256 round ( sd ( r e s l g l g ) ) , 257 "RCC: " , 258 rcc lglg , 259 "ACC: " , 260 round ( a c c l g l g , 3 ) 261 ) 262 ) 263 264 # # P l o t Accuracy f l u c t u a t i o n s b a s e d on t h r e s h o l d s h o f t i n g 265 266 t h r e s h o l d = 1 : 8 5 0 0 267 a c c = rep ( 0 , 8 5 0 0 ) 268 269 f o r ( i i n t h r e s h o l d ) { 270 acc [ i ] <

117

Appendix B. Analysis scripts

B.5. External validation

271 ( sum ( pred y l n [ , 1 ] < i & 272 pred y l n [ , 2 ] < i ) + sum ( pred y l n [ , 1 ] >= i & 273 pred y l n [ , 2 ] >= i ) ) / length ( pred y l n [ , 1 ] ) 274 } 275 276 plot ( 277 threshold , 278 acc , 279 type = ' l ' , 280 xlim = c ( 2 4 , 8 7 5 0 ) , 281 y l a b = "ACC" , 282 xlab = " threshold " , 283 main = " Accuracy o f f a s t / s l o w 284 c l a s s i f i c a t i o n on d i f f e r e n t t h r e s h o l d s " 285 ) 286 #a x i s ( 1 , a t = s e q ( 0 , 8500 , by = 1 4 0 0 ) ) 287 grid (NULL, NULL, l t y = " d o t t e d " , col = " l i g h t g r a y " ) 288 abline ( h = 0 , v = 1 4 0 , col = " b l u e " ) 289 abline ( h = 0 , v = 1 4 6 0 , col = " g r e e n " ) 290 legend ( 291 " bottomright " , 292 c ( " median " , " c u r r e n t s l o w / f a s t t h r e s h o l d " ) , 293 col = c ( " b l u e " , " g r e e n " ) , 294 lty = 1 295 )

118

Appendix B. Analysis scripts

B.6. Linear model metrics

B.6

Linear model metrics

The following script is called independently and provides metrics for the linear models, that we used as a baseline criterion measure.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # L i n e a r models t e s t s c r i p t ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # c e n s seq < cens drop < < seq ( 0 , 0 . 2 , by = 0 . 0 1 )

list () list ()

j < r a t i o cens < s i z e < 0 f o r ( i i n unique ( data123 $ d e l i v e r end date ) ) { j < j + 1 v a l i d < sum ( data123 $ submit date < i ) cens < sum ( data123 $ d e l i v e r end date > i & data123 $ submit date < i ) r a t i o cens [ j ] < cens / valid size [ j ] < valid } # # Loop f o r c a l l i n g t h e s u r v an f u n c t i o n f o r m u l t i p l e c e n s o r e d d a t a s e t s # # b a s e d on c e n s s e q z < 1 covs here < "y ~ i s d e f e c t + p r e p o s t ga + d i s t i n c t component count + d i s t i n c t f u n c t i o n count + d e v e l o p e r c o u n t r y + symptom" f o r ( i c e n s i n c e n s seq ) { i f ( i c e n s == 0 ) { cat ( c o v s here , ' \ n ' ) } tmp data < data123 xx < max( which ( abs ( r a t i o c e n s r a t i o cens i cens

i c e n s ) == min ( abs (

119

Appendix B. Analysis scripts

B.6. Linear model metrics

36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74

)))) pos < xx f o r ( k i n xx : j ) { i f ( s i z e [ k ] > s i z e [ xx ] & i c e n s < r a t i o c e n s [ k ] ) { pos < k } } data c e n s < c ( r a t i o c e n s [ pos ] , s i z e [ pos ] , unique ( tmp data$ d e l i v e r end date ) [ pos ] ) tmp data$ c e n s < 0 tmp data$ c e n s [ tmp data$ d e l i v e r end date > data c e n s [ 3 ] ] <

1

# # s e t "y" of censored records to : " cens point submit date " tmp data$ y [ tmp data$ c e n s == 1 ] < ( data c e n s [ 3 ] tmp data$ submit date [ tmp data$ c e n s == 1 ] ) / 3600 # #k e e p o n l y t h e o b s e r v a t i o n s t h a t have s u b m i t d a t e < c e n s o r p o i n t sub data < subset ( tmp data , tmp data$ submit date < data c e n s [ 3 ] ) # drop l e v e l s # not s u r e i f n e c e s s a r y now sub data$ symptom < d r o p l e v e l s ( sub data$ symptom ) # drop v a l u e s t h a t have l e s s than 2 b e c a u s e t h e y can ' t p r e d i c t a f t e r w a r d s help var < table ( sub data$ d e v e l o p e r c o u n t r y ) sub data < sub data [ sub data$ d e v e l o p e r c o u n t r y %i n% names ( help var ) [ help var > 2 ] , ] sub data$ d e v e l o p e r c o u n t r y < d r o p l e v e l s ( sub data$ d e v e l o p e r c o u n t r y ) sub data 2 < sub data [ sub data$ c e n s == 0 , ] sub data 2 $ symptom < d r o p l e v e l s ( sub data 2 $ symptom ) # drop v a l u e s t h a t have l e s s than 2 b e c a u s e t h e y can ' t p r e d i c t a f t e r w a r d s help var < table ( sub data 2 $ d e v e l o p e r c o u n t r y ) sub data 2 < sub data 2 [ sub data 2 $ d e v e l o p e r c o u n t r y %i n% names ( help var ) [ help var > 2 ] , ] sub data 2 $ d e v e l o p e r c o u n t r y <

120

Appendix B. Analysis scripts

B.6. Linear model metrics

75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113

d r o p l e v e l s ( sub data 2 $ d e v e l o p e r c o u n t r y ) # # " f u l l l i n e a r model " / c e n s / f i x e d e f f e c t / i n t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m f i t < lm ( as . formula ( c o v s h e r e ) , data = sub data ) l l m y hat < predict ( l l m f i t ) r e s l l m f i t < l l m y hat sub data$ y r e s 1 < sd ( r e s l l m f i t ) k e n d a l cor 1 < round ( cor . t e s t ( l l m y hat , sub data$ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) print ( paste ( " c e n s f i x e d lm" , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) , round ( r e s 1 ) , k e n d a l cor 1 )) # ## " f u l l l i n e a r model " / c e n s / f i x e d e f f e c t / e x t e r n a l ## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m y hat e x t < predict ( l l m f i t , newdata = data4 ) r e s l l m f i t e x t < l l m y hat e x t data4 f u l l $ y r e s 1 e x t < sd ( r e s l l m f i t e x t ) k e n d a l cor 1 e x t < round ( cor . t e s t ( l l m y hat ext , data4 f u l l $ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) print ( paste ( " c e n s f i x e d lm e x t e r n a l " , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) ,

121

Appendix B. Analysis scripts

B.6. Linear model metrics

114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152

" ", round ( r e s 1 e x t ) , k e n d a l cor 1 e x t ) ) # # " drop l i n e a r model " / drop / f i x e d e f f e c t / i n t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m f i t < lm ( as . formula ( c o v s h e r e ) , data = sub data 2 ) l l m y hat < predict ( l l m f i t ) r e s l l m f i t < l l m y hat sub data 2 $ y r e s 2 < sd ( r e s l l m f i t ) k e n d a l cor 2 < round ( cor . t e s t ( l l m y hat , print ( paste ( " drop f i x e d lm" , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) , round ( r e s 2 ) , k e n d a l cor 2 )) # # " drop l i n e a r model " / drop / f i x e d e f f e c t / e x t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m y hat e x t < predict ( l l m f i t , newdata = data4 ) r e s l l m f i t e x t < l l m y hat e x t data4 f u l l $ y r e s 1 e x t < sd ( r e s l l m f i t e x t ) k e n d a l cor 1 e x t < round ( cor . t e s t ( l l m y hat ext , data4 f u l l $ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) print ( paste (

sub data 2 $ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 )

122

Appendix B. Analysis scripts

B.6. Linear model metrics

153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191

" drop f i x e d lm e x t e r n a l " , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) , " ", round ( r e s 1 e x t ) , k e n d a l cor 1 e x t ) ) # # " f u l l l i n e a r model " / c e n s / mixed e f f e c t ( 1 | d e v e l o p e r ) / i n t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # llmer f i t < lmer ( as . formula ( paste ( c o v s here , " + ( 1 | d e v e l o p e r ) " ) ) , data = sub data ) l l m e r y hat < predict ( l l m e r f i t ) r e s l l m e r f i t < l l m e r y hat sub data$ y r e s 3 < sd ( r e s l l m e r f i t ) k e n d a l cor 3 < round ( cor . t e s t ( l l m e r y hat , print ( paste ( " c e n s mixed lmer " , i cens , "(" , data c e n s [ 1 ] , ")" , nrow ( sub data ) , round ( r e s 3 ) , k e n d a l cor 3 )) # # " f u l l l i n e a r model " / c e n s / mixed e f f e c t ( 1 | d e v e l o p e r ) / e x t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m e r y hat e x t < predict ( l l m e r f i t ,

sub data$ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 )

123

Appendix B. Analysis scripts

B.6. Linear model metrics

192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230

newdata = data4 f r , a l l o w . new . l e v e l s = TRUE) r e s l l m e r f i t e x t < l l m e r y hat e x t r e s 1 e x t < sd ( r e s l l m e r f i t e x t )

data4 f u l l $ y

k e n d a l cor 1 e x t < round ( cor . t e s t ( l l m e r y hat ext , data4 f u l l $ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) print ( paste ( " c e n s mixed lmer e x t e r n a l " , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) , " ", round ( r e s 1 e x t ) , k e n d a l cor 1 e x t ) ) # # " drop l i n e a r model " / drop / mixed e f f e c t ( 1 | d e v e l o p e r ) / i n t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # llmer f i t < lmer ( as . formula ( paste ( c o v s here , " + ( 1 | d e v e l o p e r ) " ) ) , data = sub data 2 ) l l m e r y hat < predict ( l l m e r f i t ) r e s l l m e r f i t < l l m e r y hat sub data 2 $ y r e s 4 < sd ( r e s l l m e r f i t ) k e n d a l cor 4 < round ( cor . t e s t ( l l m e r y hat , 3) print ( paste ( " drop mixed lmer " , i cens ,

sub data 2 $ y , type = ' k e n d a l ' ) $ e s t i m a t e ,

124

Appendix B. Analysis scripts

B.6. Linear model metrics

231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267

"(" , data c e n s [ 1 ] , ")" , nrow ( sub data 2 ) , round ( r e s 4 ) , k e n d a l cor 4 )) # # " drop l i n e a r model " / drop / mixed e f f e c t ( 1 | d e v e l o p e r ) / e x t e r n a l # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # l l m e r y hat e x t < predict ( l l m e r f i t , newdata = data4 f r , a l l o w . new . l e v e l s = TRUE) r e s l l m e r f i t e x t < l l m e r y hat e x t data4 f u l l $ y r e s 1 e x t < sd ( r e s l l m e r f i t e x t ) k e n d a l cor 1 e x t < round ( cor . t e s t ( l l m e r y hat ext , data4 f u l l $ y , type = ' k e n d a l ' ) $ e s t i m a t e , 3 ) print ( paste ( " drop mixed lmer e x t e r n a l " , i cens , "(" , round ( data c e n s [ 1 ] , 3 ) , ")" , nrow ( sub data ) , " ", round ( r e s 1 e x t ) , k e n d a l cor 1 e x t ) ) z < } z + 1

125

References
[1] Walid AbdelMoez, Mohamed Kholief, and Fayrouz M. Elsalmy. Improving bug fixtime prediction model by filtering out outliers. In 2013 The International Conference on Technological Advances in Electrical, Electronics and Computer Engineering (TAEECE), pages 359Â­364. IEEE, May 2013. [2] Hirotugu Akaike. A new look at the statistical model identification. Automatic Control, IEEE Transactions on, 19(6):716Â­723, 1974. [3] Prasanth Anbalagan and Mladen Vouk. On predicting the time taken to correct bug reports in open source projects. In 2009 IEEE International Conference on Software Maintenance, pages 523Â­526. IEEE, Sep 2009. [4] Pamela Bhattacharya and Iulian Neamtiu. Bug-fix time prediction models. In Proceeding of the 8th working conference on Mining software repositories - MSR '11, page 207, New York, New York, USA, May 2011. ACM Press. [5] Stamatia Bibi, Grigorios Tsoumakas, Ioannis Stamelos, and Ioannis P Vlahavas. Software defect prediction using regression via classification. In AICCSA, pages 330Â­336, 2006. [6] Michael Borenstein, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. Fixed-Eect Model, pages 63Â­67. John Wiley & Sons, Ltd, 2009. [7] Michael Borenstein, Larry V. Hedges, Julian P. T. Higgins, and Hannah R. Rothstein. Random-Eects Model, pages 69Â­75. John Wiley & Sons, Ltd, 2009. 127

References

References

[8] A. Colin Cameron and Frank A.G. Windmeijer. R-squared measures for count data regression models with applications to health-care utilization. Journal of Business & Economic Statistics, 14(2):209Â­220, 1996. [9] Gerardo Canfora, Michele Ceccarelli, Luigi Cerulo, and Massimiliano Di Penta. How Long Does a Bug Survive? An Empirical Study. In 2011 18th Working Conference on Reverse Engineering, pages 191Â­200. IEEE, Oct 2011. [10] Jerey C. Carver. Towards reporting guidelines for experimental replications: A proposal. In 1st International Workshop on Replication in Empirical Software Engineering, pages 1Â­4, 2010. [11] Enrico Colosimo, FlÂ´ avio Ferreira, Maristela Oliveira, and Cleide Sousa. Empirical comparisons between kaplan-meier and nelson-aalen survival function estimators. Journal of Statistical Computation and Simulation, 72(4):299Â­308, 2002. [12] R. Dennis Cook and Sanford Weisberg. Residuals and Influence in Regression. Monographs on statistics and applied probability. New York: Chapman and Hall, 1982. [13] D. R. Cox. Regression models and life-tables. Journal of the Royal Statistical Society. Series B (Methodological), 34(2):187Â­220, 1972. [14] Marco D'Ambros, Michele Lanza, and Romain Robbes. An extensive comparison of bug prediction approaches. In 2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010), pages 31Â­41. IEEE, May 2010. [15] Norman E. Fenton and Martin Neil. A critique of software defect prediction models. Software Engineering, IEEE Transactions on, 25(5):675Â­689, 1999. [16] Thomas R. Fleming and David P. Harrington. Counting processes and survival analysis, volume 169. John Wiley & Sons, 2011. [17] Ernst G. Frankel. Systems reliability and risk analysis, volume 1. Springer Science & Business Media, 2013. [18] Emanuel Giger, Martin Pinzger, and Harald Gall. Predicting the fix time of bugs. In Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering, pages 52Â­56. ACM, 2010. 128

References

References

[19] Mayy Habayeb. On the use of hidden markov model to predict the time to fix bugs. Master's thesis, Ryerson University, 2015. [20] Rattikorn Hewett and Phongphun Kijsanayothin. On modeling software defect repair time. Empirical Software Engineering, 14(2):165Â­186, May 2008. [21] Pieter Hooimeijer and Westley Weimer. Modeling bug report quality. In Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering - ASE '07, page 34, New York, New York, USA, Nov 2007. ACM Press. [22] David W. Hosmer Jr and Stanley Lemeshow. Applied Survival Analysis: Time-toEvent, volume 317. Wiley-Interscience, 1999. [23] Philip Hougaard. Frailty models for survival data. Lifetime data analysis, 1(3):255Â­ 273, 1995. [24] Nicholas Jalbert and Westley Weimer. Automated duplicate detection for bug tracking systems. In Dependable Systems and Networks With FTCS and DCC, 2008. DSN 2008. IEEE International Conference on, pages 52Â­61. IEEE, 2008. [25] Robert I. Jennrich and Stephen M. Robinson. A newton-raphson algorithm for maximum likelihood factor analysis. Psychometrika, 34(1):111Â­123, 1969. [26] Edward L. Kaplan and Paul Meier. Nonparametric estimation from incomplete observations. Journal of the American statistical association, 53(282):457Â­481, 1958. [27] Chris F. Kemerer and Mark C. Paulk. The impact of design and code reviews on software quality: An empirical study based on psp data. IEEE Transactions on Software Engineering, 35(4):534Â­550, 2009. [28] Maurice G. Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81Â­93, 1938. [29] Taghi M. Khoshgoftaar and Edward B. Allen. Logistic regression modeling of software quality. International Journal of Reliability, Quality and Safety Engineering, 6(04):303Â­317, 1999. 129

References

References

[30] Sunghun Kim and E. James Whitehead. How long did it take to fix bugs? In Proceedings of the 2006 international workshop on Mining software repositories MSR '06, page 173, New York, New York, USA, May 2006. ACM Press. [31] John P. Klein and Melvin L. Moeschberger. Survival analysis: techniques for censored and truncated data. Springer Science & Business Media, 2005. [32] David Kleinbaum, Lawrence Kupper, Azhar Nizam, and Eli Rosenberg. Applied regression analysis and other multivariable methods. Nelson Education, 2013. [33] A. Gunes Koru, Dongsong Zhang, and Hongfang Liu. Modeling the Eect of Size on Defect Proneness for Open-Source Software. In 29th International Conference on Software Engineering (ICSE'07 Companion), pages 115Â­124. IEEE, May 2007. [34] Tarald O. KvÂ° alseth. 39(4):279Â­285, 1985. Cautionary note about r 2. The American Statistician,

[35] Elisa T. Lee and John Wang. Statistical methods for survival data analysis, volume 476. John Wiley & Sons, 2003. [36] M.M. Lehman. Programs, life cycles, and laws of software evolution. Proceedings of the IEEE, 68(9):1060Â­1076, 1980. [37] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. Benchmarking classification models for software defect prediction: A proposed framework and novel findings. Software Engineering, IEEE Transactions on, 34(4):485Â­496, 2008. [38] Kwan-Moon Leung, Robert M. Elasho, and Abdelmonem A. Afifi. Censoring issues in survival analysis. Annual review of public health, 18(1):83Â­104, 1997. [39] Lionel Marks, Ying Zou, and Ahmed E. Hassan. Studying the fix-time for bugs in large open source projects. In Proceedings of the 7th International Conference on Predictive Models in Software Engineering - Promise '11, pages 1Â­8, New York, New York, USA, Sep 2011. ACM Press. [40] Douglas C. Montgomery, Elizabeth A. Peck, and G. Georey Vining. Introduction to linear regression analysis. John Wiley & Sons, 2015. 130

References

References

[41] Nachiappan Nagappan and Thomas Ball. Use of relative code churn measures to predict system defect density. In Software Engineering, 2005. ICSE 2005. Proceedings. 27th International Conference on, pages 284Â­292. IEEE, 2005. [42] Daniel J. Ozer. Correlation and the coe cient of determination. Psychological Bulletin, 97(2):307, 1985. [43] Lucas D. Panjer. Predicting Eclipse Bug Lifetimes. In Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007), pages 29Â­ 29. IEEE, May 2007. [44] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2013. [45] Joost Schalken, Sjaak Brinkkemper, and Hans Van Vliet. Using linear regression models to analyse the eect of software process improvement. In International Conference on Product Focused Software Process Improvement, pages 234Â­248. Springer, 2006. [46] F. W. Scholz. Maximum likelihood estimation. Encyclopedia of Statistical Sciences, 1985. [47] Nicolas Serrano and Ismael Ciordia. Bugzilla, itracker, and other bug trackers. Software, IEEE, 22(2):11Â­13, 2005. [48] Forrest Shull. Research 2.0? IEEE Software, 29(6):4Â­8, 2012. [49] Marina Sokolova and Guy Lapalme. A systematic analysis of performance measures for classification tasks. Information Processing & Management, 45(4):427Â­437, 2009. [50] Andrej-Nikolai Spiess and Natalie Neumeyer. An evaluation of r2 as an inadequate measure for nonlinear models in pharmacological and biochemical research: a monte carlo approach. BMC pharmacology, 10(1):6, 2010. [51] Cathrin WeiÃ, Rahul Premraj, Thomas Zimmermann, and Andreas Zeller. How Long Will It Take to Fix This Bug? In Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007), pages 1Â­1. IEEE, May 2007. 131

References

References

[52] Robert K. Yin. Case study research: Design and methods. Sage publications, 5th edition, 2013. [53] Feng Zhang, Foutse Khomh, Ying Zou, and Ahmed E. Hassan. An Empirical Study on Factors Impacting Bug Fixing Time. In 2012 19th Working Conference on Reverse Engineering, pages 225Â­234. IEEE, Oct 2012. [54] Hongyu Zhang, Liang Gong, and Steve Versteeg. Predicting bug-fixing time: An empirical study of commercial software projects. In Proceedings of the 2013 International Conference on Software Engineering, ICSE '13, pages 1042Â­1051, 2013.

132

Index
Accuracy (ACC), 37 Akaike Information Criterion (AIC), 35 Censoring, 21 Cox proportional hazard models, 28 Diagnostics, 35 Exponential distribution, 30 Fixed-eect models, 32 Frailty models, 34 Hazard function, 27 Kaplan-Meier estimator, 28 Kendall rank correlation coe cient (RCC), 37 Loglogistic distribution, 31 Lognormal distribution, 31 Maximum likelihood estimation (MLE), 21 R-squared, 36 Random-eects models, 33 Research questions, 2 Residual standard deviation, 36 Skewness, 24 Survival function, 26 Time-to-deliver, 2 Time-to-fix, 1 Transformations, 25 Truncation, 38 Weibull distribution, 31 Nelson-Aalen estimator, 28 Ordinary least squares (OLS), 19 Parametric survival models, 29

133


