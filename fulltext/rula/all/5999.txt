Running head: IT'S ALL ABOUT YOU i" "

IT'S ALL ABOUT YOU: PERSONALIZED FACEBOOK NEWS FEEDS' IMPACT ON USERS' EXPOSURE TO IDEOLOGICALLY VARIED CONTENT by Violet MacLeod BJH, University of King's College, 2013

A MRP presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Arts In the Program of Master of Professional Communication

Toronto, Ontario, Canada, 2017 ©Violet MacLeod, 2017

IT'S ALL ABOUT YOU

"

ii"

Author's Declaration for Electronic Submission of a MRP

I hereby declare that I am the sole author of this MRP. This is a true copy of the MRP, including any required final revisions. I authorize Ryerson University to lend this MRP to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this MRP by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my MRP may be made electronically available to the public.

IT'S ALL ABOUT YOU

"

iii"

IT'S ALL ABOUT YOU: PERSONALIZED FACEBOOK NEWS FEEDS' IMPACT ON USERS' EXPOSURE TO IDEOLOGICALLY VARIED CONTENT

Violet MacLeod Master of Professional Communication Ryerson University, 2017 ABSTRACT This critical literature analysis is a comprehensive collection and review of the literature concerning the use of recommender systems to curate social media content, specifically Facebook News Feeds. This Major Research Paper (MRP) critically evaluates the existing research to consolidate the literature on insular online spaces, identify ways in which public opinion could be affected by insular content, and find strengths, weaknesses, and gaps in the literature. After completing an extensive literature review and analysis, it was determined that researchers are polarized by the topic, while journalists (whose articles comprise the considered supplementary literature) are united in their reporting of Facebook as being a filter bubble. While additional empirical research is necessary for a firm conclusion to be drawn about insular online existences, preliminary results indicate that Facebook News Feeds' ability to curate personalized content may be manipulating and limiting users' exposure to ideologically varied media.

IT'S ALL ABOUT YOU

"

iv"

Acknowledgements

I would like to acknowledge and thank those who helped with the completion of this Major Research Paper. First, I would like to thank my supervisor, Dr. Robert Clapperton, for his comments, encouragement, and for challenging me to think critically about insular online environments. I would also like to thank my second reader, Dr. Frauke Zeller, for sharing her time and guidance, especially during the early days of this paper, when I had more chutzpah and abstract ideas than actual direction or knowledge. To my husband, Jacob, thank you for sharing the late nights and long weekends spent researching, formatting and editing. Thank you for helping me to find perspective, happiness and balance throughout this process. I would also like to thank my sister, Sharlane, for the hours she spent listening to me workout a theory or try to wrap my head another author's work.

IT'S ALL ABOUT YOU Table of Contents A. Front Matter

"

v"

Title Page........................................................................................................ Author's Declaration for Electronic Submission of a MRP ................... Abstract ....................................................................................................... Acknowledgement ................................................................... Table of Contents ..................................................................... B. Main Body 01 Introduction................................................................................................ 1.1!Objectives 1.2!Definitions of Key Terms

page i page ii page iii page iv page v page 01

02 Literature Review....................................................................................... page 05 2.1 Content Curation 2.2 Facebook News Feeds and Recommender Algorithms 03 Theoretical Development ......................................................................... 3.1 Effects of Personalization on Perception of Public Opinion 3.2 Echo Chambers and Filter Bubbles 04 Detailed Method of Literature Collection and Analysis ........................... 4.1 Methods 4.2 Analysis 05 Data Analysis ............................................................................................ 5.1 Appraising the Literature 5.2 Synthesis and Critique of the Literature i) Critical Analysis of the Academic Literature ii) Critical Analysis of the Supplementary Literature 06 Discussion of Results................................................................................. 6.1 Limitations 6.2 Considerations for future research 07 Conclusion .......................................................................... page 51 page 43 page 27 page 25 page 14

IT'S ALL ABOUT YOU

"

vi" page 53 page 63 page 72

08 Reference List ............................................................................................ C. Back matter Appendix A ........................................................................ Appendix B ........................................................................

IT'S ALL ABOUT YOU 1. Introduction

"

1"

In 2014, I attended a City Hall-style open house in Calgary, Alberta. At the event, local citizens were arguing about the upcoming construction of a citywide bikeway network. The heated conversations often drifted into discussions of partisan issues or bigger grievances and, as the event progressed, it became obvious that the majority of the participants were either uninformed or misinformed about the issues being discussed. Despite the local news coverage and pages of information available on the city's website, few people voiced balanced or factual viewpoints. Searching for a reason as to why people appeared to be insulated in their opinions, a connection was made between individuals' exposure to news media and a study that showed many American citizens source their information from social media networks, specifically their Facebook News Feeds (Michelle, Gottfried, Barthel & Shearer, 2016). The content available to users on News Feeds is aggregated using automated recommender systems to personalize and filter content in an effort to best suit each user's interests. When Facebook News Feeds are used as a primary news source, one could assume that users are exclusively exposed to content that hosts uniform ideologies creating insular media environments. This concept is what initially generated interest and inspired the research for this Major Research Paper (MRP), which is a critical literature analysis designed to review the existing literature concerning the use of recommender systems to curate social media content, specifically Facebook News Feeds. By identifying strengths, weaknesses, and gaps in the literature about insular online spaces, this MRP aims to identify ways in which public opinion

IT'S ALL ABOUT YOU

"

2"

could be affected by insular content, as well as identify areas for future valuable topical research. The use of personalized curation is also a current topic of interest for Communications Technology and Information Studies Researchers. Academics are exploring how automated recommender systems, used to curate social media content, may be validating users' pre-existing beliefs through exposure to content that is reflective of their online activity (Pariser, 2012). Given this information, it would seem self-evident that the use of these recommender systems to curate Facebook News Feeds is creating insular online spaces resulting in echo chambers or filter bubbles where users are only exposed to like-minded news and media (Pariser, 2012). Through research or experience, one would expect to uncover that individual users' Facebook News Feeds lack variety and diversity, rather they become spaces where users' social identities are used to predict their ideologies and based on their interests and opinions digitally reverberated and corroborate their beliefs by displaying like-minded content on their News Feeds. This reverberation of opinions creates environments where, "individuals are exposed only to information from likeminded individuals," (Jamieson & Cappella, 2007). Given the popularity of social networking systems and their function as mediums for sourcing news online, major news and satirical media outlets such as New York Times (Hess, 2017), The Guardian (Wong, Levin & Solon, 2015), New Scientist (Adee, 2016), Fast Company (Lumb, 2015), Fortune (Ingram, 2015), The Onion ("Horrible Facebook algorithm accident," 2017), and Wired Magazine (Pariser, 2015) have been reporting on the sentiment that Facebook News Feeds

IT'S ALL ABOUT YOU

"

3"

are insular and writing about the bubbled state of users' online existences. The journalism articles present anecdotal and circumstantial evidence that supports the idea that insular existences are being created online through the use of recommender algorithms. After reading them, one could assume that the programing of recommendation algorithms to filter content and, "connect people with information that they will most likely want to consume," (Rader & Gray, 2015) has inadvertently created a space where there is a lack of balanced information. Despite journalists' support, the existing academic research shows varied findings both in support, opposed and impartial to the existence of insular environments created by recommender systems. This MRP uses the aforementioned journalism articles as supplementary literature to determine how the public perceives and understands Facebook News Feeds' ability to create insular online environments. Since journalistic articles are understood to mirror societies' perceptions and beliefs this MRP will further use the articles to compare whether academic research and public sentiment share similar or contrasting viewpoints. The below-listed objectives are used to guide this literature analysis as well as provide a framework when selecting and critiquing the articles.

1.1!Objectives A.! Provide a review of relevant research to determine what is already known pertaining to social media networks, specifically Facebook News Feeds' ability to create insular environments;

IT'S ALL ABOUT YOU

"

4"

B.! Identify and discuss ways that insular environments could impact an individual's perception of public opinion; C.! Critically analyze the academic studies selected during a targeted search pertaining to social media networks, specifically Facebook News Feeds' ability to create insular environments; D.! Make comparisons and connections regarding the academic literature's findings and further connections to the supplementary literature; E.! Suggest recommendations for future research. Reaching these objectives serves to enhance awareness of the relevant issues pertaining to insular online environments created by recommender systems on Facebook News Feeds as well as generate recommendations for areas of future topical research that could potentially lead to a better understanding of the impact online media curation systems have on opinion dynamics, specifically users' perception of public opinion.

1.2!Definitions of key terms Definitions are provided to inform the reader of how the key terms were operationalized within this MRP. Recommender System: A web application that predicts user responses to options. These web applications are used to personalize online content in a variety of ways, including determining what information appears in search results, News Feeds, and advertisement spaces.

IT'S ALL ABOUT YOU

"

5"

Filter Bubble: A personalized online information ecosystem created by algorithms. As a result, users become insulated from different viewpoints. Echo Chamber: A closed media space that has the potential to magnify messages delivered within it and insulate them from rebuttal. As a result, individuals are only exposed to information from likeminded individuals. Public Opinion: The collective opinion of the majority. The term is often used in matters related to government and politics as it is typically thought that the opinions of the majority should be weighed more heavily than opinions of the minority.

2. Literature Review The following section satisfies Objective A by discussing and providing a review of the relevant research pertaining to social media networks, specifically Facebook News Feeds', ability to create insular environments. Within this section an overview of content curation will be provided, followed by an explanation of Facebook's use of algorithms and recommender systems.

2.1 Content Curation Early communication technology research on the Internet predicted it would become a budding public sphere where the flow of ideas and information, as well as civic engagement and debate, would flourish (Corrado & Firestone, 1996). Even before the `invention of the Internet,' opinion theorists argued that if people were exposed to a variety of diverse opinions then there would be more

IT'S ALL ABOUT YOU

"

6"

social agreement and less polarized communities of thought (Degroot, 1974). However, the multiplicity of information online has led to an influx of online content that is unmanageable for the human mind to sort and process; this has led to the use of automated filters to act as informational gatekeepers. Previous to the use of algorithmic recommender systems, news media was disseminated through individuals who were responsible for the processing of information, these individuals are known as gatekeepers (Manning White, 1950; Lewin, 1943). Within mass media communication, each gatekeeper is responsible for an information "gate". The gatekeeper interprets the information available and chooses what is worthy of their audience's attention, allowing the most relevant information through their gate and filtering what they understand to be less relevant (Manning White, 1950). Typically, the public would develop their opinions based on direct media contact as well as interactions with intermediaries (Bo-Anderson & Melen, 1959), but this may be changing with the introduction of Facebook News Feeds, which serve as a new intermediary informational gate. Instead of considering the public's informational needs, Facebook News Feeds calculate the individual's entertainment and informational needs, potentially narrowing the media that users are exposed to. Instead of users being exposed directly to news media sources and then seeking opinions from intermediaries and opinion leaders (Bo-Anderson & Melen, 1959; Lazarsfeld, 1948), the Facebook News Feed acts as an intermediary platform that contains both conditions (it shows a curated news media as well as network connections' reactions). This algorithmic curation has led to a disruption of early opinion dynamics and

IT'S ALL ABOUT YOU

"

7"

communication technology theories. Previous to recommender systems, opinions were primarily influenced by traditional news sources that were written and published for a variety of audiences made up of a variety of people. For example, a newspaper contains a mix of articles for a broad audience leading to a variety of content which enhances discussions between equally informed readers, whereas new technology allows for highly personalized media exposure resulting in a phenomenon where each audience is an audience of one. For example, imagine that every individual who bought a newspaper was being given a personalized copy with articles chosen specifically to suit their beliefs and interests, this scenario leads the to the question: does personalized news have the ability to impact public discourse if individuals are exposed to content with less difference in opinion. The increasing amount of information available online has led to researchers studying the changing consumption of news media. In 2016, the Pew Research Center found that 38% of Americans got their news from an online source whereas a mere 20% sourced their information from print newspapers (Michelle, Gottfried, Barthel & Shearer, 2016). In 2017, a study conducted by the Reuters Institute for the Study of Journalism Research suggests that 51% of people who have access to the Internet are sourcing their news from social media and that the most popular social media application for news is Facebook (Newman, 2017). The study also showed that Latin Americas source more news from social media or chat applications than any other population in the world (Newman, 2017). Despite an increase in global online and social media news

IT'S ALL ABOUT YOU

"

8"

consumption, not all populations online experiences are equal. For example, American's access to online content is understood to be open, in the sense that it is not heavily censored. American Internet consumption is commonly known to be free of government filtration. This is unlike some societies where news media and Internet use is monitored and manipulated by government actors. The lack of censorship is valued by North American culture, however, as a result of an online information overload, societies that value freedom of information may be giving corporations the power of media manipulation. For instance, those who source their news media content from Facebook are allowing their informational needs to be determined by the Facebook corporation. This problem is amplified by the need for intermediaries that provide content filtration and social media's established role as a content aggregator. In 2009, the amount of information online was predicted to double every 72 hours (Bhargava, 2009) and in 2014, IBM estimated that 2.5 quintillion bytes of data were created everyday (Dale, 2014). Research by Cisco suggests that by 2021, it will take one user more than five million years to watch the amount of video content that will cross global I.P. networks monthly (The Zettabyte Era: Trends and Analysis, 2017). As the amount of online content grows, a human being's ability to absorb data remains static as biological limits induce the experience of bounded rationality where individuals are overwhelmed by choice (Simon, 1955). The informational noise created by the amount of online information distorts the human mind's ability to make un-biased and clear-headed decisions (Hilbert, 2012). The mass of information, however, is not the problem,

IT'S ALL ABOUT YOU

"

9"

rather the lack of a perfect filter is (Shirky, 2009). In response to the necessity of an information filter, content curation has become a mainstream practice online, with computer algorithms interpreting and curating the most relevant information for online users to help manage the deluge. Simply put, content curation, "locate(s), organize(s), and distribute(s) links to relevant, high quality content online, voluntary assuming a quality filtering role that traditional publishers once had," (Lowry, 2010). Although this definition is one of many possible interpretations, it is relevant to this MRP as it points to content curation's ability to assume the filtering role that news media and publishers are traditionally known for, furthermore, the qualities outlined match those provided by Facebook News Feeds. Content curation has become central to most social media platforms, specifically to their news feed components. To be highly personalized, social media platforms use algorithms to determine the most pertinent content for each user based on their past activity online. Consequently, this may be creating a narrow and limited news feed that rejects information and media that is ideologically varied from the individual user. Much of content curation is automated and algorithmically derived, thus the information is only as good as the algorithm (Dale, 2014, p. 200). Numerous researchers have found that algorithms and computer systems can be biased (Bozdag, 2013). This bias can be seen in a variety of ways, including societal bias that affects the system design and technical bias, which occurs when the technology being used is limited or suffers from programmer error (Bozdag, 2013). For example, algorithms will reflect the

IT'S ALL ABOUT YOU

"

10"

biases of their programmer(s) (Rainie & Anderson, 2017). As explained by New York Times best-selling author, Eli Pariser, all algorithms host a point of view and are theories of how the world should work interpreted through math and expressed in code (Pariser, 2015). This process allows for errors of bias, interpretation and coding. The biases are powerful and may unintentionally impact what information users are exposed to. For example, Google's algorithmic online advertising system more often shows advertisements for high-income jobs to men than it shows to women (Datta, Tschantz, & Datta, 2015). Even if the intent is not to discriminate, when social preferences are rationally reproduced, so are the existing social biases. Some researchers offer techniques to sabotage the personalization system to disrupt the biases by selecting ideologically challenging content and uninstalling cookies to make it more difficult for the personalization engines to profile users' activities (Pariser, 2011). However, these tactics are tedious, and may not be effective depending how the recommender system is programmed (Bozdag, 2013).

2.2 Facebook News Feeds and Recommender Algorithms In May 2016, researchers found that 62% of American adults sourced their news from social media with 44% getting their news directly form Facebook (Gottfried, 2016). American adults who source their news from either Facebook of Twitter saw an increase of 53% from nine percent during 2012 (The state of the news media ... report on American journalism, 2012). The growth is expected to continue, with the total global Internet traffic predicted to reach 3.3 zettabytes by

IT'S ALL ABOUT YOU

"

11"

2021, this translates to Internet users increasing from 3.3 to 4.6 billion (Cisco Visual Networking Index Predicts Global Annual IP Traffic to Exceed Three Zettabytes by 2021, 2017). With millions of people gaining access to the Internet, it is expected that more people will source their news online. Considering the enhanced reliance on information intermediaries, such as Facebook, to act as information gatekeepers, traditional news media channels are gradually being replaced (Bozdag, 2013, p. 209). Social media platforms, such as Facebook, use algorithms to curate their News Feeds. The now algorithmically filtered news is becoming influential, "determining the content and vocabulary of the public conversation" (Devito, 2016, p. 4). The use of recommender systems to curate content seems to point to a filtering of dissenting ideas in favour of media that closely align with the users' wants. In August 2014, roughly 41,000 posts were published on Facebook every second (Dale, 2014, p. 200). The social media platform's per-day content far exceeded (and continues to exceed) an amount of information manageable by the human mind (Simon, 1955; Hilbert, 2012), leading to the necessity of online information aggregation systems. Two years after Facebook was founded, in 2006 (Luckerson, 2015), the website launched its News Feed, which is a highlypersonalized page where users are exposed to an aggregation of content based on their networks' activity and their perceived personal interests. The information displayed on the News Feed is determined by the likes, links, applications, and activities of the people who make up each users' Facebook network (How News Feed Works, 2017). These interactions with other users (likes, clicks, and

IT'S ALL ABOUT YOU

"

12"

comments) are known as `social gestures' (Bozdag, 2013, p. 211). A news feed determined by social gestures leads to a users' feed being only as varied as the people they follow. Therefore, it would seem that by choosing to friend and follow people of similar beliefs a user could insulate themselves by curating their network to reflect their own ideologies. For example, an educated middle class Republican who chooses to only follow other educated middle class Republican will be most often exposed to media being curated and circulated by and for the opinions held by this group. Facebook News Feeds employ a specific type of algorithm used to perform content curation. The proprietary recommender algorithm known as EdgeRank is often understood to be the algorithm used by Facebook to rank information based on users' most recent engagement (Birkbak & Carlsen, 2016). However, the term EdgeRank has not been used in house by Facebook since 2010, when Facebook released a public version of EdgeRank and stressed that their internal version is constantly being optimized through experimentation (Birkbak & Carlsen, 2016; McGree, 2013). During an interview with Backstrom, Engineer Manager for News Feed Ranking at Facebook, he said that there are as many as 100,000 elements that produce News Feeds, which is far more advanced than the original three EdgeRank elements (affinity, weight and time decay) (McGree, 2013). The more recent versions of the system use machine learning to recommend News Feed content, these algorithms are more sophisticated allowing for more personalized and insular News Feeds. Machine learning and recommender algorithms are sociotechnical systems that recognize the interaction

IT'S ALL ABOUT YOU

"

13"

between technology and people (Rader & Gray, 2015) and, in the case of Facebook, predict users' information needs based on various programmed elements. These predictions based on personal traits may result in a lack of challenging opinions and information, which in turn may result in a pseudoenvironment (Lippmann, 1965) that could extend to users' offline existence and affect their understanding of world events and perception of public opinion. This extension of online opinions to the offline world may be more salient if the user relies on their Facebook News Feed as their primary source for exposure to news media. Therefore, it can be assumed that as the number of users who gather their news via their Facebook News Feed increases so will the number of people who view the world through a narrowed informational lens. The narrowing of the information lens has been explained as positive, as it synchronizes communities and enables people of like mind to have richer conversations by being exposed to similar or the same information (Shirky, 2011). However, researchers at King's College in London found weaknesses in this reasoning. During their study of Pinterest and Last.fm playlists, they found that users who engaged in social gestures on these platforms viewed curation as personal rather than social, thus communities are not necessarily formed during network content curation (Zhong, Shah, Sundaravadivelan, & Sastry, 2013). Furthermore, it can be argued that communities exposed to narrow content can become too like-minded as they are rarely exposed to challenging opinions, rather, the information they consume continually reaffirms their existing beliefs.

IT'S ALL ABOUT YOU 3. Theoretical Development

"

14"

The following section discusses the core theories integral to this paper, these being public opinion, filter bubbles and echo chambers. Further to developing knowledge of the core theories, this section satisfies Objective B by identifying and discussing ways that insular environments could impact people's perception of public opinion.

3.1 Effects of Personalization on Perception of Public Opinion Mass media is widely understood to accurately convey public opinion by ethically and fairly dictating individuals' exposure to information. Media plays a major role in the development of public opinion by ensuring effective contestation and non-domination (Pettit, 1999) Communication theorists have maintained exposure to media as the typical source where individuals begin to form their perception of public opinion and that media plays a major role in forming public opinion. The news media people consume has the ability to manipulate their understanding of public opinion because what is trusted as an `authentic messenger' may not be providing the best aggregation and version of events (Lippmann, 1965). Supporting this idea is persuasive press interference theory that suggests that people infer public opinion from their reading of press coverage (Gunther, 1988). The attitude portrayed by the media is understood to represent what the general public thinks are the issues being reported (Neubaum & Kramer, 2016). Furthermore, people's personal opinions and actions were found to be impacted by their perception of public opinion (Tsfati, Stroud & Chotiner, 2014).

IT'S ALL ABOUT YOU

"

15"

As Facebook News Feeds become a more commonplace source for media, users will need to actively sort and differentiation between content as News Feeds contain a mixture of world news, local news, network news, entertainment sources and non-vetted content (which could include satirical content or factually inaccurate content).Therefore, the substitution of ethical editors for recommender algorithms requires the user to further vet the content they are exposed to and supports the idea that personalized exposure to news media has the ability to impact peoples' perceptions of public opinion. How changing news media consumption impacts users' perception of public opinion seems particularly relevant when one considers Facebook News Feeds' role as a media provider, specifically as a news media source. The importance of varied content that represents competing or dissimilar opinions is integral to rational arguments and the progression of society (Habermas, 1984; Mill, 1985). As proposed by Habermas (1984), the theory of communicative action posits that communication is an act of cooperation where people engage with one another under the caveat of mutual deliberation and argumentation, and it is the foundation for rational arguments. Critics of Habermas's communicative action believe that it is an idealist view (Habermas, 1987) as communicative rationality lacks an agreement between what would be ideal and what is the reality. However, it is an ideal theory for considering the effect of insular environments that lack ideologically varied content. For Habermas, argumentation is made possible by an individual's ability to rationalize and share that rationalization with others (Habermas, 1984), thus, communicative

IT'S ALL ABOUT YOU

"

16"

action is the process of deliberation. Habermas determines that argumentative speech must include a shared search for the better argument through understanding and an absence of coercive force. Based on Habermas's theory, a lack of argumentation in personalized news feeds could jeopardize users' ability to engage in valuable communication that includes argumentation between wellinformed participants. Further considering a curator's ability to restrict information, and more specifically, create an insular environment with a lack of mixed information, one can notice similarities to Lippmann's theory of the pseudo-environment (1965). Lippmann's theory imagines that individuals relate to reality through a curated pseudo-environment. The pseudo-environment consists of curated news coverage that gives the individual an unrealistic representation of public opinion and global occurrences, thus distorting how they act and interact with their environment (Lippmann, 1965). For Lippmann, the public is acted upon by mass media's choice of news coverage and dissemination, which is now being gradually replaced by social media because of its easily accessible aggregation of information, including news. Accordingly, although content curation is necessary, personalized curation based on a user's network and their social gestures is flawed as it may create incomplete (at best) and completely inaccurate (at worst) pseudo-environments. Existing in an incomplete environment may cause an individual to miss key information or public sentiment about polarizing issues. Further to this, someone who is exposed to non-vetted and inaccurate media, known as fake news, could believe wildly inaccurate stories, which are designed with the intent to further polarize or attack a group. As

IT'S ALL ABOUT YOU

"

17"

earlier noted, media consumption has a major impact on how individuals understand and relate to their offline environments, therefore the manipulation of content and curation of online media environments could disrupt and influence users' perception of public opinion. Considering mass medias' effect on perception of public opinion during the 1970s German political scientist, Noelle-Neumann, theorized that the term public opinion refers to, "the interaction between the inclinations, abilities, and convictions of the individual and the agreement of the many, to which the individual has to subordinate himself if he does not want to place himself in isolation outside society," (Noelle-Neumann, 1979, p.151). She writes that it is through news media that individuals come to understand public opinion. She coined the term "Spiral of Silence", which is an analogy used to visually describe the theory. The theory posits that the end of the spiral refers to individuals who are not publicly expressing their opinions because they are fearful of isolation. She suggests that an individual is more likely to go down the spiral if their opinion does not match or align with the perceived majority opinion, (NoelleNeumann, 1979). The spiral of silence has been connected to the issue of accuracy and pluralistic ignorance, where individuals consider the majority view as a minority view. "The mass media, given their ability to portray trends and shifts in the climate of opinion, can bring its compelling force, its threat of isolation, to bear on their audiences,"(Price & Allen, 1990). Audiences are put under the social pressure of public opinion by the media, which is understood to represent public

IT'S ALL ABOUT YOU

"

18"

sentiment (Price & Allen, 1990). If the media is being principally delivered to people online through their personalized and curated Facebook News Feeds, then the media being curated may be interpreted as public opinion. In the past, the steward of public opinion was traditional media sources, which are bound by a code of ethics that insists journalists attempt balance and accuracy within each story. However, a curated News Feed is not held to the same ethical guidelines and is understood to curate content based on each individual's perceived wants and beliefs rather than balance and truth. Despite the influence of media exposure on the perception of public opinion, an individual is not passive in the creation of their beliefs. In fact, there are three logical stages that create belief, these being: reporting, reception, and acceptance (Goldman, 2008). These stages may be disrupted by the personalized curation of Facebook News Feeds because for people to believe truth instead of falsehoods choices need to be made at one or more stages where content filtering is involved. If filtering happens at the reporting stage, the gatekeeper removes some sources or specific types of information that will be sent to the receiver. If filtering happens at the reception stage, all the information is sent to the receiver and they can decide which messages to receive, read and digest. Finally, during the acceptance stage the receiver, after reading media about a certain topic, decides which of the messages to believe. According to Goldman, the gatekeeper's ability to filter information impacts the individual's outcome at the reporting level and therefore the final determined `truth' may not be reliable (Goldman, 2008). With the advent of personalized news feeds, these stages are

IT'S ALL ABOUT YOU

"

19"

disrupted as the information is filtered to agree with one ideology and, thus, during the acceptance stage, the receiver is presented a uniform message and therefore lacks the ability to base their selection on a balanced profile of information, including contradictive media. Despite the possibility of Facebook News Feeds impacting users' perception of public opinion, the argument may be dismissed by critics who assume users have alternate means (than social media) to observe the daily activities of the news. However, this assumption requires the user to be engaged with information outside their online bubble, and the user would need to be accepting of information and opinions that compete with the media which they have already been exposed to online. A number of scholars (Rader & Gray, 2015; Yang, Barnidge, & Rojas, 2016) noted that users are not always fully aware as to the extent of recommender systems' role in curating news media content and how each person's actions influence what news they are exposed to. This may be a problem as users who are not aware that algorithms and recommender systems are curating their exposure to news media on Facebook may interpret their New Feeds as analogous with public opinion, while, in reality, they may only be exposed to uniform one-sided media that is being shared by those whom they most often interact with on the platform. A highly curated exposure to news media can result in the creation of a pseudo-environment where the ideologically similar news articles and headlines influence how users perceive public opinion (Lippmann, 1965); and an ideologically uniform pseudo-environment has the potential to eliminate valuable

IT'S ALL ABOUT YOU

"

20"

communication that includes argumentation, thus threatening the ability to have a shared appreciation for finding the truest information (Habermas, 1984). 3.2 Echo Chambers and Filter Bubbles Applying the term echo chamber, before it was coined, American Scholar, Cass Sunstein (2002; 2007), argued that the increasing power for selective exposure to like-minded news media and opinions online may lead to the fragmentation of public opinion and polarization of individuals. According to Sunstein (2007), Internet users may isolate themselves from information that challenges their beliefs, and this would ultimately have a damaging impact on public discourse. He posits that to restore the public sphere elements of the Internet there should be deliberation and exposure to reasonable competing views to, "...ensure that people are exposed, not to softer or louder echoes of their own voices, but to a range of reasonable alternatives," (Sunstein, 2002). Following Sunstein, in 2008, two of the foremost experts on politics and media, Kathleen Hall Jamieson and Joseph Cappella, published their novel studying the narrowing of information in favour of exposure to ideologically similar news media. They conducted an analysis of conservative media and found that Limbaugh, Fox News and The Wall Street Journal's opinion pages allowed for an environment where information, ideas, and beliefs are amplified or reinforced, they referred to these insular environments as echo chambers. The authors defined echo chambers using two meanings: 1) "A bounded enclosed media space that has the potential to both magnify the messages delivered within it and insulate them from rebuttal." (2008, p.76); 2) "Each outlet legitimizes the

IT'S ALL ABOUT YOU

"

21"

other" (2008, p.76). They posit that within an echo chamber, official sources often go unquestioned and competing ideologies are underrepresented or completely absent. Based on this argument, it can be assumed that echo chambers exist, insulating users from ideologically varied media. Jamieson and Cappella (2008) found that the audiences of conservative media likely hold ideology and attitudes that agree with the media sources to which they are choosing to be exposed. This finding agreed with Slater's (2007) propositions in his work Reinforcing Spirals: The Mutual Influence of Media Selectivity and Media Effects and Their Impact on Individual Behavior and Social Identity. Slater's proposition suggests that media exposure is sought to explain individual's attitudes rather than create them and that people's social identities and opinions influence their selective media exposure. He combines these two theories to raise the possibility of spirals of effect where prior opinion leads to selective media exposure, which ultimately reinforces the individual's original attitude or opinion. Slater also focuses on how closed or open an individual's media consumption is, he wonders if individuals consume media that reflects competing ideologies. If not, then in a closed system (such as a highly ideologically uniform Facebook News Feed) the spiral of effect would be maximized and individuals would be self-affirming their assumptions of public opinion as reflecting their individual ideologies (Slater, 2007). In Cappella and Jamieson's work, Echo Chamber, they questioned whether the conservative media allows for ideologically uncongenial media sources or if the media amplifies users

IT'S ALL ABOUT YOU

"

22"

originally held beliefs through a reverberation of ideas, a theory they coined as an echo chamber (2008). In their book, Jamieson and Cappella draw on Slater's propositions, specifically that if an individual's media selection reinforces their opinions and attitudes, it leads to their social identity being more salient and accessible to them resulting in self-affirmation created by news media that reverberates existing beliefs and ideologies (2008). Their analysis and findings are not generalized to social media or recommender systems' role in creating news consumption environments. Despite this lack of connection, online social media accounts provide the necessary conditions for echo chambers as they are environments where ideologically similar networks exist and articles are curated based on individual users' social identities. Researchers following Jamieson and Cappella also recognized this connection and often adopt the terminology to describe insular existences created online ­ specifically on Facebook News Feeds. Four years after Jamieson and Cappella's book (2008), Internet activist and Upworthy chief executive, Eli Pariser, wrote the New York Times best seller, Filter Bubble: How the New Personalized Web is Changing What We Read and How We Think. Pariser used the term filter bubble to explain how the monetization of the Internet is suppressing the flow of ideas through the use of recommender algorithms (Pariser, 2012). Drawing on interviews with both cyberskeptics and cyber-optimists, including the co-founder of OK Cupid, an algorithmically-driven dating website, to explore the consequences of commercial power in the digital age. According to Pariser (2012), users receive different

IT'S ALL ABOUT YOU

"

23"

search results for the same key words, including those with similar networks and background, he uses Facebook News Feeds as a prime example of the aggregation of personalized content. The emergence of the term filter bubble happened just one year after Facebook publically expressed its plans to experiment with the existing curator algorithm, EdgeRank (McGree, 2013). During an interview with the New York Times, Pariser quoted Mark Zuckerberg, Facebook's chief executive, saying that he once told colleagues that, "a squirrel dying in your front yard may be more relevant to your interests right now than people dying in Africa." (Pariser, 2011) This quote metaphorically illustrates the problem with the insulation enabled by Facebook News Feeds, that is, perceived personal relevance can insulate people. Furthermore, the use of recommender algorithms to determine the salience of issues and ideologies for individuals is enabling people to focus on the squirrel rather than relevant public issues. In his book, Pariser echoes this point and posits that filter bubbles are, "closing us off to new ideas new subjects and important information," (Pariser, 2011). In relation to the effect filter bubbles have on the perception of public opinion, Pariser interviews John Rendon, an American self-proclaimed perception manager with high ranking Washington D.C. security clearance, who suggests that filter bubbles provide new ways of managing perceptions (2011). The theory is based on the assumption that a content creator could use the algorithm to ensure that only their content would be selected, which would improve the creator's ability to set the user's belief (2011). Further to this, self-sorting and

IT'S ALL ABOUT YOU

"

24"

personalization removes a layer of context from the shared information because to make good, informed decisions context is crucial (2011). However, the bubble is not experienced equally by everyone and may have different impacts on different groups. For instance, Bozdag and van der Hoven (2015) indicate that the extent to which a user experiences the problem of a filter bubble varies dependent on a variety of factors, such as the individual's interpretation of democracy. Although similar in definition, the key difference between filter bubbles and echo chambers is that the former occurs without the autonomy of the user (Bozdag & Timmermans, 2001). An echo chamber is produced by the active consumption of specific news sources and shared ideas between people with the same values and within an echo chamber, individual's ideas are replicated and amplified by close social groups, while filter bubbles curate algorithmically without the knowing participation of the user and typically for an online corporation's monetary gain. In both instances, users are confined to a curated digital existence free of competing ideas. Throughout the literature, the terms filter bubble and echo chamber are often used interchangeably, specifically when discussing Facebook as users are exposed to the elements for both theories. On Facebook, individuals can work to actively craft their social identities and networks enabling them to create a personalized echo chamber of like-minded users and media. However, this use of Facebook requires the user to have a strong understanding of the algorithms. As uncovered by Rader and Gray, users are often unaware of how algorithms on Facebook curate the media they are exposed to (2015), resulting in users passively

IT'S ALL ABOUT YOU

"

25"

consuming information from within a filter bubble that has been algorithmically chosen to suit them. Regardless of the terminology, both result in the user being exposed to ideologically uniform news media and content, which, in the absence of competing ideologies, may ultimately impact their perception of public opinion.

4. Detailed Method of Literature Collection and Analysis This MRP uses a critical literature review to explore current research relating to insular information exposure on Facebook News Feeds. Currently, there exists a large body of empirical research on recommender systems, algorithms and exposure to information online, and a growing body of research exists focusing specifically on social media and its role as an information intermediary; however, very little research has been conducted to determine the effects of recommender systems used for the aggregation of News Feeds and their effect on individual users. This MRP conducts a thorough literature review while collecting iterative data, sampling existing theories and comparing the findings against the selected supplementary literature to critically analyze the academic studies selected during a targeted search pertaining to social media networks', specifically Facebook News Feeds', ability to create insular environments. It will also identify any relations, gaps, contradictions or inconsistencies in the literature as they relate to the existing theories discussed in the literature review, as well as provide recommendations for future research. To identify relevant literature for the analysis, a search was conducted

IT'S ALL ABOUT YOU

"

26"

using two databases, these being Google Scholar and Science Direct (ideally, more databases would be consulted, however that is beyond the scope of this MRP). The literature collected to answer the research questions used the key terms: [Facebook] + [News Feed] + [Social Media Curation] + [Recommender | Recommendation] + [System | Systems] + [Echo Chamber] + [Public Opinion] + [Filter Bubbles]. The key terms were searched together during one search and then in two smaller groupings: 1) [Facebook News Feed] + [Public Opinion] + [Echo Chamber], 2) [Facebook News Feed] + [Public Opinion] + [Filter Bubble]. Additional searches were done based on the references cited by the selected studies, including journalism articles that were classified under supplementary literature (Appendix B). All articles are selected based on a relevance judgment for the purpose of determining overlapping interests, which is conducted by considering the title, the article's abstract and in some cases a first reading of the text. The body of literature was restricted to articles from 2008 and later. The rationale for this limitation is that Facebook was launched in 2004 and the term echo chamber was not coined until 2008, with the term Filter Bubble being first defined in 2011. To analyze the collected information, this MRP uses a qualitative approach (Bryman, Bell & Teevan, 2012). The information extracted from each piece of literature includes the researcher(s)' conclusions, methodologies, limitations, similarities, and differences from other articles, implications, and connections to the theoretical literature analyzed in the literature review. Once analyzed, the relevant information was included in this MRP.

IT'S ALL ABOUT YOU 5. Data Analysis 5.1 Appraising the Literature

"

27"

The literature search was completed in July 2017, and 125 articles were discovered (Appendix A). After conducting a relevance assessment, 19 articles remained for further analysis. The relevance assessment considered each article's title, publisher, and abstract. Next, a backward search (Levy & Ellis, 2006) of each relevant article's references was done to extract new relevant literature (including supplementary literature). To discover more supplementary literature, a forward search (Levy & Ellis, 2006) was conducted, this found three articles that cited Bakshy et al.'s (2015) study. Excluded articles are those that are a summation of the existing literature, published in a language other than English, lacked empirical study (there were exceptions made for literature analyses that closely aligned with this MRP's research interests), were written for a Master level program, or those outside the project's focus, such as, Agile PR: Expert Messaging in a Hyper-Connected, Always-On World (Salzman, 2017), which was an article designed for public relations experts to determine how they could amplify their messages online. After the initial relevance assessment literature tables were created to categorize, describe and evaluate the 19 found publications (Appendix B). After the completion of the literary tables, a second relevance assessment was done and any articles that focused on an intermediary other than Facebook or had research objectives outside the scope of this MRP were eliminated. For instance, some of the chosen studies only analyzed Twitter data. Despite both Twitter and Facebook being social networking sites, the findings

IT'S ALL ABOUT YOU

"

28"

were not generalizable across platforms as the sites have different users, applications, and algorithms. For example, Twitter has a reputation for allowing users to determine filtering within the platform, allowing users seeing most of their friends' Tweets, whereas Facebook users curate their network similarly to Twitter, but then further algorithmic filtering is done based on trend and personalization. After the second relevance assessment, seven academic and seven supplementary articles remained for further critical analysis. The remaining literature includes articles that gathered data sets both directly from Facebook or analyzed insular environments on a variety of social media platforms, including Facebook.

5.2!Synthesis and Critique of the Literature The following section is a critique of the existing empirical, theoretical, and review literature relevant to the idea: Facebook News Feeds have the ability to create insular environments where Facebook users are only exposed to ideologically similar media. A critical analysis of the research is presented with an overview of all the researches' designs and objectives. Next an individual critique of each article in relation to their subject(s), data collection, measures and results is provided. This section satisfies Objective C by critically analyzing the academic studies selected during the keyword database search.

IT'S ALL ABOUT YOU i)!

"

29"

Critical Analysis of the Academic Literature Research Design: A total of seven relevant publications were identified

wherein the researchers sought to determine the extent to which media curation affected social media users' content experience. This body of literature contains one literature review (Zuiderveen Borgesius, Trilling, Moller, Bodo, Vresses, & Helberger, 2016), and six empirical studies (Bakshy, Messing & Adamic, 2015; Dylko, Dolgov, Hoffman, Eckhart, Molina, & Aaziz, 2017; Flaxman, Goel & Rao, 2016; Goel, Mason, & Watts, 2010; Jacobson, Myung & Johnson, 2015; Kim, 2011). The collection of empirical studies includes one experiment, three surveys, one analysis of secondary data and two observational studies. Research Objective: Several researchers aimed to find out how online networks influence exposure to cross-cutting ideologies (Bakshy et al., 2015; Flaxman, Goel & Rao, 2016; Jacobson, Myung & Johnson, 2015; Kim, 2011). Dylko et al.'s study (2017) considers the relationship between personalization and selective exposure. Goel et al.'s research (2010) sought to understand if Facebook users' friend networks were hemophilic or not. Individual Article Critique: Flaxman et al. (2016) analyzed the Internet Explorer web-browsing behavior (including Facebook browsing history) of 1.2 million users located in the U.S. over a three-month period. Collecting data from Internet Explorer raises questions about the representativeness of the sample as the platform is dated and participants are more likely to be older and less Internet literate (Bursztein, 2012). However, the representativeness of the participants cannot be confirmed as demographics were not included in this article. Of the 1.2

IT'S ALL ABOUT YOU

"

30"

million users, 50,000 were selected who actively read at least 10 practical articles a month and two opinion pieces in three months. This study was restricted to individuals who voluntarily shared their data, which creates selection issues, as individuals who are more private about their online activities would be less likely to participate. The data set is large, 2.3 billion distinct page views were collected, with a median of 991 page views per individual. The size of the dataset causes issues as the 7,923 distinct domains were hand-labeled into a classification hierarchy and the viewed articles within those websites were automatically assigned the polarity score of the outlet in which they were published. This classification assumes that all of the articles written have a political affiliation, which causes neutral articles (like the reporting of breaking news events) to be assigned a polarization scores. It could also result in articles having the wrong polarity score; for instance, a liberal op-ed on a conservative news site would be assigned a conservative polarity score. This could skew the data as a liberal reading a neutral article from a conservation news source would be moving the needle in favour of being exposed to cross-cutting opinions when they were, in fact, reading ideologically aligned content. The results showed that most users' polarity scores were 0.11, meaning there is a degree of ideological segregation, however, it seems to be relatively moderate. This polarity score reflects users' activity on social media (MySpace, Facebook, and email) as well as direct browsing. The platforms were also evaluated independently to determine that social media users' exposure to ideological cross-cutting opinions is higher than when users are direct browsing.

IT'S ALL ABOUT YOU

"

31"

Jacobson et al.'s study (2015) conducted a network analysis and reviewed Facebook audience discussions that included links on the pages of O'Reilly (Conservative figure with 1,222 links) and Maddow (Liberal figure with 2,220) from May 23 to June 4, 2011 to determine how links on Facebook pages contribute to echo chambers. The researchers coded for base URL, type of information resource (hard versus soft news), political leaning of the information resource and the context in which the Facebook user shared the link. Two researchers identified the base URL and coded the link categories and an independent coder tested the reliability of the categories. Each links' website of origin was coded by political orientation as either liberal or conservative. This was done by checking if the website explicitly stated a political point of view or if one could be easily recognized from the site's content. Otherwise, the site was coded as neutral. It was found that both the O'Reilly and Maddow audiences linked most frequently to neutral media sources. It was also noted that in comparison, the conservative O'Reilly audience more often linked to conservative sources and the liberal Maddow audience more often linked to liberal sources. Furthermore, the most frequently linked to base URL was Facebook, with seven percent of the links, this is an interesting finding that the researchers do not fully explore. If seven percent of links being shared on Facebook pages are sourced from within the Facebook platform then the platform may be serving as a closed news system for some users, where information is both sourced and shared. Bakshy et al.'s study (2015) used a de-identified data set of 10.1 million active U.S. users who self- report their ideological affiliation and seven million

IT'S ALL ABOUT YOU

"

32"

distinct Web links (URLs) shared by U.S. users over a six-month period between July 7, 2014, and January 7, 2015. Results found that, in the context of Facebook, individual choices matter more than algorithms when limiting exposure to attitude-challenging content. However, the weakness of this finding is that individual choices are made based on what content is made available by the recommender algorithm, therefore the recommender algorithms dictate the content available for individual selection. There are also sampling issues, primarily that only users who volunteered their ideological affiliation were chosen, also by exclusively looking at data collected from American users the findings are most likely impacted by western ideological biases. Similar to Flaxman et al. (2016), the individual articles were assigned the same political value as the news sources which produce them, creating issues when articles without polarization were valued and measured as such. Furthermore, the researchers are Facebook Data Scientists and could be thought to have a conflict of interest when collecting and analyzing the data creating issues of confirmation bias, for this reason, it may be assumed that the researchers were not balanced in their extrapolations and interpretations the data. Furthermore, the study is not reproducible as the raw data was not made public and Facebook News Feed data is proprietary. The study's results found that individual choice has more of an effect on what users see than algorithms, however the data shows that exposure to different ideological views is eight percent reduced by the algorithm than it is by a user's personal choice; this inadvertently provides proof of filter bubbles as cross-cutting content is algorithmically reduced.

IT'S ALL ABOUT YOU

"

33"

Kim's study (2011) sought to find support for the hypothesis that social networking sites are positively related to exposure to cross-cutting political viewpoints. The study drew on secondary survey data conducted by The Pew Internet & American Life Project, between November 20, and December 4, 2008. The year of data collection, 2008, is when EdgeRank is understood to have been the main tool for Facebook News Feed aggregation. EdgeRank is known to have been a rudimentary recommender system that had since been updated meaning the findings may not be generalizable to the current Facebook user experience. The data was collected by using a random-digit sample of U.S. telephone numbers. A sample of 2,254 respondents, 18 years-old and older, were contacted to complete an interview. The response rate was 23% and respondents were 53% female and 79.5% were white; the median age group was 35 to 44 years-old. For this study, social media network use was measured by asking the respondents about their use of network sites such as Facebook and MySpace. They were asked about their activities and exposure to political material on these platforms. For example, had they received any campaign or candidate information from these sites? These questions require the participant to recall and self-report, which leaves the data open to misinformation and misremembering. Furthermore, a telephone interview, although a typical data collection method during 2008, is currently understood as a less appropriate medium for conducting interviews about online and social media use, as those who are understood to have a home telephone number are more advanced in age than those who are most active online. Therefore, researchers testing the findings or replicating this study would

IT'S ALL ABOUT YOU

"

34"

need to keep this limitation in mind and most likely choose different methods. During the interview, respondents were asked to indicate whether most of the sites they visit to get political or campaign information online challenge their own point of view, share their point of view, or do not have a particular point of view. A third option of `do not recall' or `not sure' should have been made available as, again, respondents are expected to rely on recall and memory to accurately complete these questions. A key problem with this study is that it relies on respondents' self-reports to measure exposure to cross-cutting political views online. An experimental setting would be a stronger methodology as it would enable researchers to measure participants' actual activities and exposure to polarizing information. Since this study was conducted in 2008, it would be interesting to re-conduct it with recent data and then compare the findings to determine if the new more sophisticated recommender algorithms have disrupted the early finding of prevalent exposure to cross-cutting ideologies. Dylko et al.'s study (2017) investigated if there was a causal relationship between recommender systems and political selective exposure. The results found that customizability technology has a strong effect on minimizing exposure to ideologically opposed information and increasing exposure to pro-attitudinal content. Subjects were recruited from communication and psychology classes at a southwest United States university and received course credit. Offering incentives for students who participate is both a strength and weakness as receiving a credit is likely to make students more apt to take the experiment seriously, however, students are known to be more open to diverse perspectives and may internalize

IT'S ALL ABOUT YOU

"

35"

the values presented in the articles they read. Internalizing the values would cause them to underestimate how often the technology exposed them to selective articles. For this study, there were 93 subjects, 66.3% were female and on average 22-years-old. Forty percent self-identified as liberal in political ideology, 29.6% as conservative, and 30.4% as moderate. Ideally, the study would have an equal split between men and women participating as well as include other gender identifications to avoid gender bias, which makes the findings less generalizable. Despite the critiques, this study has many strengths, including two rounds of experimentation, with the first measuring subjects' attitudes towards polarizing topics using a five point Likert scale, which allows for users to rate and choose an option that best aligns with their views instead of simply requesting that they categorize themselves as absolutes by answering `yes' or `no'. The subjects' attitudes were then tested for certainty by using a five-point scale to indicate how certain they were of their attitudes toward each polarizing issue. Once the users' attitudes were determined, they were asked to participate in an experiment and told it was unrelated. During part two, users were exposed to two webpages developed to look like a news magazine website. Participants were randomly assigned versions of the website. Some versions were personalized based on their answers to the part one questionnaire and some were not. The participants were then asked to browse the webpage as they typically would and a computer program monitored their activities. The approach of participants not knowing why they are being asked to read content could impact how they approach the webpage, for instance, users' may read the articles in sequence whereas in a

IT'S ALL ABOUT YOU

"

36"

typical scenario one may browse all the headlines first and select what most interests them based on keywords. After analyzing the data, the experiments showed that the present customizability technology increased political selective exposure, with users more often clicking on articles that were ideologically similar to their existing attitudes. Goel et al.'s research (2010) used a snowball survey to measure the difference between real and perceived agreement to determine how homophilic users' networks are. A survey application was launched through Facebook in January 2008, the survey application was added by 2,504 individuals and completed by varying numbers of participants depending on the questions. For instance, 900 subjects answered the political questions whereas only 872 completed the light-hearted questions. The population was reported as relatively diverse in age, gender and geographically (varied states across the U.S.). However, the population diversity numbers were based on the information gathered from the 2,504 users who initially downloaded the application and does not necessarily represent the subjects who actually completed the questions. By launching the survey using Facebook traditional biases of network related surveys were eliminated because respondents were not required to self-identify as friends as Facebook provides an existing social-network map that is understood to be representative of offline relationships. Subjects were asked binary questions that focused on political issues about their own attitudes, as well as about their friends' attitudes. To limit potential persecution caused by their political attitudes, subjects could choose to skip questions or request that their answers not be made public.

IT'S ALL ABOUT YOU

"

37"

The survey consisted of many questions, forty-seven questions were asked about politics alone. The number of questions raises the issue of respondent fatigue and possibly weakens the respondents' answers to the questions which appear later in the survey. Results showed that randomly matched pairs agreed 63% of the time and friends tend to agree 75% of the time. However, the overall U.S. population is reported to agree at levels close to 50%, meaning that the collected sample may be biased showing higher levels of agreement than the overall U.S. population. By conducting a synthesis of the empirical research, Zuiderveen Borgesius, Trilling, Moller, Bodo, Vresses and Helberger (2016) compare the literature related to self-selected personalization and pre-selected personalization. The latter is described as when algorithms personalize content for users without the active user choice. The researchers found that there is little empirical evidence that warrants worry about filter bubbles. However, this finding is based on a small body of new and emerging literature and assumes that the sparsity of academic research into the effects of pre-selected personalization directly correlates to a lower necessity of worry, which is questionable as there are many other assumptions that could be made and the sparsity of literature may be connected to a variety of variables, such as a lack of funding to support large scale studies on the topic or a lack of access to necessary data. ii)! Critical Analysis of the Supplementary Literature Given the newness of algorithmically personalized News Feeds and social media platforms as primary information intermediaries, research is emerging and most academic studies draw on a limited pool of existing literature and data. For

IT'S ALL ABOUT YOU

"

38"

this reason, the chosen supplementary literature consists of journalism articles from leading news outlets, as they are understood to report on public sentiment and conduct leading industry expert interviews. Overall, the articles offer anecdotal and circumstantial information, which defends the theory of users being exposed to ideologically uniform media on Facebook News Feeds. The journalism articles included as supplementary literature provide a lens to further consider the scholarly work, however, it is important to note that both are held to different mission statements and ethical codes, as such the journalism articles are used to compliment the academic literature and further consider public sentiment in relation to the academic findings. Article design: The body of supplementary literature includes a total of seven articles published by leading journalistic websites including New York Times (Hess, 2017), The Guardian (Wong, Levin & Solon, 2015), New Scientist (Adee, 2016), Fast Company (Lumb, 2015), Fortune (Ingram, 2015), The Onion ("Horrible Facebook algorithm accident," 2017), and Wired Magazine (Pariser, 2015). Most conduct interviews (Adee, 2016; Hess, 2017; Ingram, 2015; Lumb, 2015), one is an opinion piece (Pariser, 2015), one is satirical ("Horrible Facebook algorithm accident," 2017), and one conducts a case study (Wong, Levin & Solon, 2015). Article objective: In response to Bakshy et al.'s article (2015) several journalists wrote articles to defend the existence of Facebook filter bubbles (Pariser, 2015; Ingram, 2015; Lumb, 2015;), two report on Facebook News Feeds' role in political polarization (Wong, Levin & Solon, 2015; Hess, 2017),

IT'S ALL ABOUT YOU

"

39"

one was designed to prove Facebook user experience filter bubbles by satirically imagining a situation where bubbles did not exist ("Horrible Facebook algorithm accident," 2017) and one shared tools and techniques for users to `pop' their filter bubble (Adee, 2016). Individual Article Critique: Wong et al.'s study's (2015), gave 10 U.S. voters (five self-identified liberals and five self-identified conservatives) login information for a Facebook avatar account that was created to represent `the other side'. The participants were encouraged to login several times over the course of a month and were regularly interviewed about their experiences. All participants found the `other side's' News Feed to be made up of aggressive, hurtful and shocking content that represented ideologies that were in contrast to their own. In response to the recent U.S. election, Adee's study (2016) reports on the existence of Facebook News Feed filter bubbles and the problem of sourcing political news from a News Feed aggregated using recommender systems. Within the article Adee (2016) interviews Martin Moore at King's College London, Cesar Hidalgo at the Massachusetts Institute of Technology (MIT), Nathan Matias at MIT's Center for Civic Media and Philip Howard from the Oxford Internet Institute. One of the interviewees suggests that News Feeds can be controlled by users maintaining a network that includes ideologically opposed friends. This puts the pressure on users to try and `game the system' and pressure the algorithms into exposing them to varied information, which is not how most users' approach their Facebook activities. It was also reported that often users with moderate opinions are lost in the mix as extremist views exact more attention, thus ranking

IT'S ALL ABOUT YOU

"

40"

extreme content higher in the algorithmic aggregation. Overall this article represents good journalism techniques, with a balanced approach offering varied opinions through thoughtful selection of interviewees and independent research. Hess' (2017) article is a round-up of current technologies that help users escape their Facebook and Twitter bubbles. Hess (2017) identifies five web applications designed to expose users to more varied information: PolitEcho, Flip-Feed, Read Across the Aisle, Escape Your Bubble and Outside Your Bubble. The existence of these applications suggests that there is a want to a need being observed from online users to have access to more ideologically varied or moderate content. Ingram's article (2015) reports on the scientific debate about the legitimacy of Bakshy et al.'s (2015) study. Ingram (2015) quotes sociologist Nathan Jurgenson and social-media expert Zeynep Tufekci and Christian Sandvig, an associate professor at the University of Michigan, who all dismiss the study as biased and not proving the absence of filter bubbles. The Onion's satirical report ("Horrible Facebook algorithm accident," 2017) is about a fake press release that apologizes for `a glitch' in the Facebook algorithm which accidentally exposed users' to `new concepts'. Through satire, the article implies that Facebook staff is aware of the platform being insular and personalized by chiefly showing users headlines that have been algorithmically chosen to reinforce their existing ideologies. This article, although satirical, provides a record of public sentiment that shows, despite some opposing academic evidence, there is a sense of insulation among Facebook's users.

IT'S ALL ABOUT YOU

"

41"

Lumb (2016) reports on the backlash to Bakshy et al.'s study (2015). Lumb writes about Zeynep Tufekci and Christian Sandvig's independent posts citing failures of the Bakshy et al. (2015) study. Although multiple journalists quote Zeynep Tufekci and Christian Sandvig without crediting their original posts, Lumb is transparent in his article, crediting the quotes to their written source. Lumb adds value to his article by capturing the Twitter debate through a series of Tweets that criticize Bakshy et al.'s (2015) assertion that the recommender algorithms were not significantly diminishing users' exposure to ideologically varied content. However, Lumb (2016) only captures three Tweets, which is not enough to indicate an overwhelming sense of opposition to the study. Pariser (2015) writes an opinion piece defending the theory of filter bubbles and challenging Bakshy et al.'s study (2015). Pariser (2015) writes that the finding of friend groups mattering more than algorithms in the exposure to cross-cutting ideologies is an overstatement. He writes that his Filter Bubble theory was in part about algorithms helping users insulate themselves with media that support their already held beliefs, the second part asserts that algorithms tend to down-rank media that is challenging and necessary in a democracy. Pariser (2015) shares concerns about hard news is only seven percent of the content being clicked on meaning the remaining content is soft news or fake news. Pariser says that it is important to keep talking about algorithms and their effect on users' exposure to content as information guides action. This article is important as it is written in defense of the Filter Bubble theory by the researcher who developed it, however, it is open to confirmation bias.

IT'S ALL ABOUT YOU

"

42"

Summary of the key critiques: Journalism articles are written with the understanding that the journalists are held to high ethical standards and are researching and writing in a fair and balanced manner. However, journalism articles are not held to the same level of transparency as academic literature. For instance, journalists are not expected to include their method of data collection nor provide a sample of their questions. By not disclosing the methods used to gather information and write the articles the reports are not reproducible and open to confirmation bias. Also, some articles have overlapping data by drawing on similar experts for quotes and information. For instance, Ingram, (2015) and Lumb (2015) quote the same two American academics, one a teacher at the University of Maryland, the other at the University of North Carolina. Drawing on the same sources raises questions about the selection process for interviewees. Another important note is that none of the selected articles challenge the existence of filter bubbles, rather they champion it, again begging the question of confirmation bias. Despite these criticisms, the articles provide anecdotal evidence of filter bubbles, specifically Wong et al.'s case study (2015), which is experimental and conducts several interviews with the participants over time. Articles written in defense of the existence of filter bubbles and in response to Bakshy et al.'s study (2015) provide reactional evidence for the theory and underline the importance of further research as to how information is being curated on social media networks.

IT'S ALL ABOUT YOU 6. Discussion of Results

"

43"

The following section satisfies Objective D by making comparisons and connections regarding the academic literature findings and further connections to the supplementary literature. This section also provides a discussion of the findings that emerged from the critical review of the literature pertaining to Facebook News Feeds ability to create insular environments. Key similarities, connections, and differences among the various findings and approaches are presented followed by limitations and considerations for future research, which satisfies Objective E. The body of literature showed varied scholarly interest and debate about personalized Facebook News Feeds. Throughout the literature, researchers debate the effects and existence of uniform exposure to media on social networking sites, they also use varied methodologies and methods to defend or negate the existence of insular online environments. An overview of the methodologies shows that only one text used data from actual active Facebook News Feeds (Bakshy et al. 2015). This means that the remaining academic literature is weakened by their reliance on users' self-reporting and recall for surveys, questionaries' and interviews, or by their biases when re-creating a Facebook-like platform to conduct experiments. Of the articles analyzed, there appeared to be three standpoints concerning filter bubbles on social media platforms: 1.! Online social media platforms are insular; 2.! Online social media platforms are not insular;

IT'S ALL ABOUT YOU

"

44"

3.! The evidence does not strongly support either position. Based on a first reading of the academic literature, the second argument, that online social media platforms are not insular, is more persuasive as it draws on the largest studies (Bakshy et al., 2015; Goel et al., 2010; Kim, 2011), and includes the only study which collected data directly from users' Facebook News Feeds (Bakshy et al., 2015). However, upon further consideration, the critiques of these articles outweigh their persuasiveness. Furthermore, the supplementary literature provides anecdotal evidence and current expert interviews that, when partnered with the supporting academic studies (Dylko et al., 2017; Jacobson et al., 2015), is compelling in its support of online insular existences. Those who claim social network sites are not insular environments most often cite evidence showing that users experience exposure to cross-cutting opinions (Bakshy et al., 2015, 2016; Kim, 2011) which is typically caused by weak-ties (people who users may not align with ideologically and are less likely to have interactions with offline). The idea of maintaining varied networks that include weak-tie relationships to support an ideologically varied News Feeds was echoed in one of the articles from the supplementary literature (Hess, 2017). However, most of the supplementary literature challenged this finding on the basis that if Facebook decides what media appears on users' walls based on interaction frequency then posts from weak-ties will most likely disappear from the receiver's News Feed (Ingram, 2015; Lumb, 2016; Pariser, 2015). As a rebuttal to the supplementary literature, researchers found that even if users are blocked from content produced by weak-ties, they do disagree with their friends

IT'S ALL ABOUT YOU

"

45"

more often than is perceived and therefore even in a homogenous environment, they may be exposed to a difference of opinion (Goel et al., 2010). Researchers who found the data supports both or neither side of the argument assert that while social media networks and news aggregators are increasing the personalization of content and potentially creating filter bubbles or echo chambers, there is also the potential for increased choice and greater exposure to diverse ideas (Flaxman et al., 2016). Despite the feeling of concern that has been voiced throughout the supplementary literature, some of the academic research states that, "...at present, there is no empirical evidence that warrants any strong worries about filter bubbles." (Zuiderveen et al., 2016, p. 10). However, this assertion is weakened by the existing research drawing on a limited pool of existing topical literature and studies. During the analysis of collected literature, it was noted that, despite the targeted literature search being open to articles from 2008 until 2017, the earliest article found was published in 2010, with data being used from as early as 2008. During 2010, Facebook released their simplified version of EdgeRank and announced that, moving forward, they would be using a more sophisticated and experimental version of the algorithm (Goel, Mason, & Watts, 2010). This fact weakens earlier studies' findings because the new algorithm draws from over 100,000 elements to create each users' News Feed, whereas earlier versions aggregated based on only three variables (McGree, 2013), making it unlikely that the earlier findings would still be relevant and comparable to the most recent studies.

IT'S ALL ABOUT YOU

"

46"

The article most often cited by other researchers and contested by the supplementary literature is Bakshy et al.'s study (2015), which is also the only research to draw on big data directly from users' Facebook News Feeds. The study was conducted in response to Pariser's categorization of Facebook as a filter bubble and is arguably the most relevant study conducted about insular social media existences, specifically concerning Facebook News Feeds. For the abovementioned reasons, it is necessary that this MRP closely analyzes the Bakshy et al. study (2015). The study draws data from more than 10 million Facebook user profiles and news feeds to analyze the existence of cross-cutting opinions on Facebook. It categorizes `hard news' from `soft news' and assigns a political alignment to each article based on how often it was shared by users who identified as liberal versus conservative. The researchers consider how many times the scored articles appear on a self-identified liberal's News Feed versus that of a self-identified conservative. Results show that on average Facebook users are eight percent less likely to see content that the opposite political party favors. Another key finding is that an individual's friend network on Facebook has a greater impact on their exposure to ideologically varied news media ­ this is reminiscent of Jamieson and Cappella's echo chamber, where people's networks can be curated to disrupt or reflect their own ideologies, it also agrees with findings from Kim (2011) and Goel et al.'s (2010) studies. In their findings, Bakshy et al. (2015) write that Facebook News Feeds do have the potential to create ideologically uniform environments if the users choose to curate their networks by selecting friends who have similar beliefs and unfriending people

IT'S ALL ABOUT YOU

"

47"

who do not; although this use of Facebook is unlikely as most users curate their friend list through offline interactions and social connections, such as work, school, family, sports teams, which are made up of varied individuals rather than choosing friends based solely on their ideologies. Therefore, the ability to insulate one's News Feed is possible but does not align with the typical users' use of Facebook. Further bolstering Bakshy et al.'s findings, (2015), Goel et al. (2010) found that even when networks are composed entirely of friends, friends share ideologies approximately 75% of the time making the inference that 35% of the time friends will hold a competing viewpoint and therefore share ideologically varied content. However, this extrapolation does not consider that even if friends vary ideologically on issues they may not share their challenging or moderate opinions online meaning their competing beliefs never appear on each other's News Feeds. Bakshy et al.'s (2015) study is controversial and, as Pariser points out in his response piece, Did Facebook's Big New Study Kill My Filter Bubble Thesis, there are some major methodological hiccups (Pariser, 2015). In addition to Pariser's article (2015), various supplementary literature also directly criticizes Bakshy et al.'s (2015) findings (Ingram, 2015; Lumb, 2015), while the remaining supplementary journalism articles indirectly challenge the study (Adee, 2016; Hess, 2017; "Horrible Facebook algorithm accident," 2017; Wong, Levin & Solon, 2015). The first major critique is that the ideological scoring method does not measure how partisan-biased the news article is, rather it considers how often the news is shared by one ideologically similar group. For example, a story about

IT'S ALL ABOUT YOU

"

48"

leather shoes, which in its essence has no political leanings, may be assigned a liberal or conservative tag depending on how often it is shared by people who identify with the given political party. Secondly, this study does not offer a statistical analysis and measures just nine percent of Facebook users who report their political leanings online, which assumes that this group varies from Facebook's general population of users. Thirdly, the idea that users have the ability to disrupt their News Feeds more than the algorithm is flawed as users' choices are inextricably linked because their activities determine the algorithm's curation and the curated content influences the users' activities. Lastly, this study was conducted in-house by Facebook data scientists who may arguably have an invested interest in proving that Facebook does not insulate its users from media representing varied ideologies. Furthermore, the researchers used private proprietary data meaning this study is not reproducible. Therefore, this study's strength (number of subjects and access to data) is also its weakness as the dataset is a small niche group (those who identify their political leanings) and proprietary and therefore cannot be independently verified. After considering Bakshy et. al.'s (2015) study at length and in comparison to the existing academic literature and supplementary literature, it is concluded that, although it is an important contribution to the filter bubble debate, it does not entirely debunk the filter bubble theory and further research should be conducted to draw a firmer conclusion. Throughout the analyzed literature, researchers challenge and debate the notion that algorithms create insular environments. Despite the controversy, there

IT'S ALL ABOUT YOU

"

49"

was a consensus among authors that users' activities (likes, comments, friends, and shares) are interpreted algorithmically to determine what content appears on each users' News Feed. Where researchers find themselves divided is when they ask how the algorithm affects exposure to ideologically varied content. Ultimately, regardless of whether recommender systems are currently creating insular media environments, it must be acknowledged that they do have the potential, which leaves those who source information from social media sites at the discretion of proprietary companies who, unlike traditional news media, do not have an ingrained media ethics code that requires they provide fair, truthful and balanced information to the public.

6.4 Limitations This literature analysis was completed using two databases and three key term searches, ideally more databases (such as ISI Proceedings, JSTOR, and ProQuest), as well as more varied combinations of key terms would be used, however, it was outside the scope of this MRP. Furthermore, the choice to focus on Facebook News Feeds limited the quality and quantity of available research. The research was sparse as Facebook's data is proprietary and there are no existing tools that allow researchers to access individuals' News Feeds for the purpose of collecting data. In retrospect, a literature analysis that focuses on recommender systems as they apply to Twitter, Google or web-browser data would have yielded a more robust selection of research as the data from these platforms is more easily available to researchers.

IT'S ALL ABOUT YOU 6.5 Considerations for future research

"

50"

Moving forward, it is expected that academic literature examining the changing consumption of media online and the impact of uniform exposure on users' perception of public opinion will continue to expand. During this expansion it is suggested that the researchers conduct more empirical studies and that Facebook allow a third party analysis of its News Feed data; also more studies are needed that directly question the role of pre-selected curation, rather than looking at homophile among friend groups. This is because even if a users' network is diverse their aggregated content still relies on the algorithms interpretation of their social identities. An interesting study for future research could be a two-part study where users are interviewed to measure their attitudes towards polarizing topics and then asked to search key terms on their personal computers using different intermediaries (Facebook, Google, Twitter), the search results would then be captured and compared to other participants to analyze the variation of ideologies each user is exposed to based-on their algorithmically aggregated search feeds. Further research should also consider: ·! What elements a balanced algorithmic filter would need and how this could be implemented on intermediary website (i.e. Facebook, Twitter, Google, and Yahoo); ·! If social media networking sites should be held to the same standards and ethics as traditional news media websites; ·! The type of cross-cutting news that is shared. Are users' exposed to crosscutting news that is factual and sourced from vetted media outlets or are

IT'S ALL ABOUT YOU

"

51"

they being exposed to inaccurate content known as fake news? As the type of cross-cutting media has the potential to create an equally impactful effect on users' ability to perceive public opinion (Pariser, 2015); ·! The impact of shared insular environments on people with competing ideologies. For example, how much cross-cutting information are couples who use the same computer or social profiles, but who have different political ideologies, exposed too; ·! Finally, it is encouraged that researchers begin to close the gap between the academic literature and public sentiment by considering how users perceive and experience their filter bubbles.

7. Conclusion Surrounded by 24/7 access to timely news, people seem amazingly uniformed (Denning, 2006). The evidence supporting Facebook News Feeds as filter bubbles that perpetuate ignorance stemming from a lack of exposure to ideologically diverse media is limited, but expanding. The disconnect between academic and supplementary literature highlights the misinformation and mystery that remains around the existence and effect of insular online ecosystems. By analyzing the existing theories and literature, it has been determined that there is some evidence to suggest the lack of knowledge about current events may be attributed to filtering and insular online existences and that insular exposure may result in negative impacts on public opinion as well as discourse, engagement and democracy. However, the evidence is sparse and most of the studies rely on selfreporting, recall, and small (compared to the population of Facebook users)

IT'S ALL ABOUT YOU

"

52"

numbers of participants. Furthermore, the entire body of literature focuses on American subjects, specifically Facebook users located in the U.S., who only represent approximately 10% of the total Facebook population (Facebook users worldwide, 2017). Despite a lack of empirical research testing algorithmic media curation on Facebook News Feeds, it was noted repeatedly that the included supplementary literature (journalism articles) supported the idea that Facebook News Feeds are ideologically insular, which draws attention to the noticeable gap between the existing research and the public sentiment of a squeezed information environment, where exposure to competing opinions is limited.

IT'S ALL ABOUT YOU 8. Reference List

"

53"

Adee, S. (2016, November 18). How can Facebook and its users burst the `filter bubble'? New Scientist. Retrieved June 16, 2017 from https://www.newscientist.com/article/2113246how-can-facebook-and-its-users-burst-the-filter-bubble/ Anspach, N. M. (2017). The new personal influence: How our Facebook friends influence the news we read. Political Communication, 1-17. Baer, D. (2017, November 09). The 'filter bubble' explains why Trump won and you didn't see it coming. New York Magazine. Retrieved June 16, 2017 from http://nymag.com/scienceofus/2016/11/how facebook-and-the-filter-bubble-pushed-trump-to-victory.html Bakshy, E., Messing, S., & Adamic, L. A. (2015). Exposure to ideologically diverse news and opinion on Facebook. Science, 348(6239), 1130-1132. doi:10.1126/science.aaa1160 Bakshy, E., Messing, S., & Adamic, L. A. (2015). Supplementary material for exposure to ideologically diverse news and opinion on Facebook. Science, 348(6239), 1130-1132. doi:10.1126/science.aaa1160 Bell, R. M., Koren, Y., & Volinsky, C. (2010). All together now: A perspective on the Netflix Prize. Chance, 23(1), 24-24. doi:10.1007/s00144-010-0005-2 Berman, R., & Katona, Z. (2016). The impact of curation algorithms on social network content quality and structure. Bhargava, R. (2009). Manifesto for the content curator: The next big social media job of the future. Retrieved January 28, 2017 from http://www.rohitbhargava.com/2009/09/manifesto-for-the-content-curator-the-next-bigsocial-media-job-of-the-future-.html

IT'S ALL ABOUT YOU

"

54"

Birkbak, A., & Carlsen, H. B. (2016). The world of Edgerank: Rhetorical justifications of Facebook's News Feed algorithm. Bo-Anderson, and C.-O. Melen. "Lazarsfeld's two-step hypothesis: Data from some Swedish surveys. I." Acta Sociologica, 4.2, 1959, 20-34. Web. Bozdag, E. (2013). Bias in algorithmic filtering and personalization. Ethics and Information. Bozdag, E., & Timmermans, J. (2011, September). Values in the filter bubble ethics of personalization algorithms in cloud computing. 1st International Workshop on Values in Design­Building Bridges between RE, HCI and Ethics, (p. 7). Technology, 15(3), 209-227. doi:10.1007/s10676-013-9321-6 Broderick, R. (2017). I made a Facebook profile, started liking right-wing pages, and radicalized my News Feed in four days. BuzzFeed. Retrieved March 24, 2017 from https://www.buzzfeed.com/ryanhatesthis/i-made-a-facebook-profile-started-liking-rightwing-pages-an?utm_term=.rxo2LJADP#.ewQlnR0rK Bryman, A., Teevan, J., & Bell, E. (2009). Social Science Research Methods 2nd Canadian Edition. Bursztein, E. (2012, September 14). Survey: Internet Explorer users are older, Chrome seduces youth. Retrieved July 10, 2017, from https://www.elie.net/blog/web/survey-internetexplorer-users-are-older-chrome-seduces-youth Cisco Visual Networking Index predicts global annual IP traffic to exceed three zettabytes by 2021. (2017, June 08). Cisco. Retrieved June 12, 2017 from https://newsroom.cisco.com/press-release-content?type=webcontent&articleId=1853168 Comerasamy, H. (2012). Literature Based Research Methodology [PowerPoint Slides]. Retrieved from https://www.slideshare.net

IT'S ALL ABOUT YOU

"

55"

Conover, M. D., Gonçalves, B., Flammini, A., & Menczer, F. (2012). Partisan asymmetries in online political activity. EPJ Data Science, 1(1), 6. Colleoni, E., Rozza, A., & Arvidsson, A. (2014). Echo chamber or public sphere? Predicting political orientation and measuring political homophily in Twitter using big data. Journal of Communication, 64(2), 317-332. Corrado, A. & C.M. Firestone, eds. (1996). Elections in Cyberspace: Towards a New Era in American Politics. Aspen Inst Human Studies. Dale, S. (2014). Content curation: the future of relevance. Business Information Review, 31(4), 199-205. Datta, A., Tschantz, M. C., & Datta, A. (2015). Automated experiments on ad privacy settings. Proceedings on Privacy Enhancing Technologies, 2015(1), 92-112. DeGroot, M. H. (1974). Reaching a consensus. Journal of the American Statistical Association, 69(345), 118-121. Denning, P. J. (2006). Infoglut. Communications of the ACM, 49(7), 15­19. Devito, M. A. (2016). From editors to algorithms. Digital Journalism, 1-21. Dillahunt, T. R., Brooks, C. A., & Gulati, S. (2015, April). Detecting and visualizing filter bubbles in Google and Bing. In Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems (pp. 1851-1856). ACM. Dylko, I., Dolgov, I., Hoffman, W., Eckhart, N., Molina, M., & Aaziz, O. (2017). The dark side of technology: An experimental investigation of the influence of customizability technology on online political selective exposure. Computers in Human Behavior, 73, 181-190. El-Bermawy, M. M. (2017, June 03). Your echo chamber is destroying democracy. Retrieved

IT'S ALL ABOUT YOU

"

56"

June 16, 2017, from https://www.wired.com/2016/11/filter-bubble-destroyingdemocracy/ Facebook users worldwide 2017. Retrieved July 14, 2017 from https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-usersworldwide/ Farrell, H. (2012). The consequences of the internet for politics. Annual review of political science, 15, 35-52. Flaxman, S., Goel, S., & Rao, J. M. (2016). Filter bubbles, echo chambers, and online news consumption. Public Opinion Quarterly, 80(S1), 298-320. Flaxman, S., Goel, S., & Rao, J. M. (2013). Ideological segregation and the effects of social media on news consumption. Available at SSRN. Goel, S., Mason, W., & Watts, D. J. (2010). Real and perceived attitude agreement in social networks. Journal of personality and social psychology, 99(4), 611. Goldman, A. (2008). The social epistemology of blogging. Information technology and moral philosophy, 111-122. Gottfried, J., & Shearer, E. (2016). News use across social media platforms 2016. Pew Research Center, 26. Gunther, A. C. (1998). The persuasive press inference: Effects of mass media on perceived public opinion. Communication Research, 25(5), 486-504. Hallinan, B., & Striphas, T. (2016). Recommended for you: The Netflix Prize and the production of algorithmic culture. New Media & Society, 18(1), 117-137. Hess, A. (2017, March 03). How to escape your political bubble for a clearer view. Retrieved June 16, 2017, from https://www.nytimes.com/2017/03/03/arts/the-battle-over-your

IT'S ALL ABOUT YOU political-bubble.html?_r=0

"

57"

Horrible Facebook algorithm accident results in exposure to new ideas. (2016, September 06). The Onion. Retrieved June 15, 2017 from http://www.theonion.com/article/horriblefacebook-algorithm-accident-results-expos-53841 How News Feed Works | Facebook Help Center | Facebook. (n.d.). Retrieved January 25, 2017 from https://www.facebook.com/help/327131014036297/ Hilbert, M. (2012). Toward a synthesis of cognitive biases: How noisy information processing can bias human decision making. Psychological Bulletin, 138(2), 211-237. doi:10.1037/a0025940 Habermas, J. (1984). The theory of communicative action. Boston: Beacon Press. Habermas, J. (1987). The philosophical discourse of modernity. Cambridge, Mass.: MIT Press. Is your news feed a bubble? (n.d.). Retrieved March 20, 2017 from http://politecho.org/ Jamieson, K. H., & Cappella, J. N. (2010). Echo chamber Rush Limbaugh and the conservative media establishment. Oxford: Oxford University Press. Jacobson, S., Myung, E., & Johnson, S. L. (2015). Open media or echo chamber: the use of links in audience discussions on the Facebook Pages of partisan news organizations. Information, Communication & Society, 19(7), 875-891. doi:10.1080/1369118x.2015.1064461 Keegan, J. (2016, May 16). Blue Feed, Red Feed. Retrieved March 09, 2017 from http://graphics.wsj.com/blue-feed-red-feed/ Lazarsfeld, P., Berelson, B., & Gaudet, H. (1948). "The people's choice: How the voter makes up his mind in a presidential election (2nd ed."). New York, NY: Columbia University

IT'S ALL ABOUT YOU Press.

"

58"

Lippmann, W. (1965). Public Opinion. New York: Free Press. Lotan, G. (2014, August 04). Israel, Gaza, War & Data. Retrieved June 09, 2017 from https://medium.com/i-data/israel-gaza-war-data-a54969aeb23e Lowry, C. (2010). Content Curators and Twitter. Communications. Luckerson, V. (2015). Here's how your Facebook news feed actually works. Retrieved January 25, 2017 from http://time.com/3950525/facebook-news-feed-algorithm/ Lumb, D. (2015, May 08). Why scientists are upset about the Facebook filter bubble study. Retrieved June 15, 2017, from https://www.fastcompany.com/3046111/fast-feed/whyscientists-are-upset-over-the-facebook-filter-bubble-study McCombs, M. E., & Shaw, D. L. (1972). The agenda-setting function of mass media. Public opinion quarterly, 36(2), 176-187. McGee, M. (2014, July 22). EdgeRank is dead: Facebook's news feed algorithm now has close to 100K weight factors. Retrieved July 12, 2017 from http://marketingland.com/edgerank-is-dead-facebooks-news-feed-algorithm-now-hasclose-to-100k-weight-factors-55908 Messing, S., & Westwood, S. J. (2014). Selective exposure in the age of social media: Endorsements trump partisan source affiliation when selecting news online. Communication Research, 41(8), 1042-1063. Mill, J. (1885). Principles of political economy. Retrieved from http://www.gutenberg.org/etext Mitchell, A., Gottfried, J., Barthel, M., & Shearer, E. (2016). The modern news consumer. Pew Research Center, 7. Neubaum, G., & Krämer, N. C. (2017). Monitoring the opinion of the crowd: Psychological

IT'S ALL ABOUT YOU

"

59"

mechanisms underlying public opinion perceptions on social media. Media Psychology, 20(3), 502-531. Newman, N. (2017). Overview and key findings of the 2017 digital news report. Retrieved August 05, 2017, from http://www.bing.com/cr?IG=5833B5E102174BD5B5F07280B4E27A9E&CID=1D0670 470AA660FD13647A9C0BA061B9&rd=1&h=af3VO_yNSnuZhwGJrXWEt2CwDkru MKCYpDolGPZUGqk&v=1&r=http%3a%2f%2fwww.digitalnewsreport.org%2fsurvey %2f2017%2foverview-key-findings-2017%2f&p=DevEx,5064.1 Noelle-Neumann, E. (1979). Public Opiniona nd the Classical Tradition: A Re-evaluation. Public Opinion Quarterly, 43(2), 143-156. Pettit, P. (1997). Republicanism: a theory of freedom and government. OUP Oxford. Pew Research Center. (2012). The state of the news media 2012: An annual report on American journalism. Retrieved from http://stateofthemedia.org/files/2012/08/ 2012_sotm_annual_report.pdf Pariser, E. (2011). The filter bubble: What the Internet is hiding from you. Penguin UK. Pariser, E. (2011, May 22). When the Internet thinks it knows you. New York Times. Retrieved June 09, 2017 from http://www.nytimes.com/2011/05/23/opinion/23pariser.html Pariser, E. (2012). The filter bubble: How the new personalized web is changing what we read and how we think. Penguin. Pariser, E. (2015, May 07). Did Facebook's big study kill my filter bubble thesis? Wired Magazine. Retrieved June 10, 2017 from https://backchannel.com/facebook-published-abig-new-study-on-the-filter-bubble-here-s-what-it-says-ef31a292da95 Price, V., & Allen, S. (1990). Opinion Spirals, Silent and Otherwise: Applying Small-Group

IT'S ALL ABOUT YOU

"

60"

Research to Public Opinion Phenomena1. Communication Research, 17(3), 369-392. Rader, E., & Gray, R. (2015, April). Understanding user beliefs about algorithmic curation in the Facebook news feed. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (pp. 173-182). ACM. Rainie, L., & Andreson, J. (n.d.). Pew Research Center: Internet, science & tech. Retrieved March 07, 2017 from http://www.bing.com/cr?IG=A0687DC7A0DF4C94A602EACD1D86D3FC&CID=2B8 627A6532767B101622DE75216662D&rd=1&h=TFKBhq2S_vajFuktiowEZU_9Exo_B LitejROPo9r9to&v=1&r=http%3a%2f%2fwww.pewinternet.org%2ffeed%2f&p=DevEx ,5072.1 Ricci, F. (2011). Recommender systems handbook. New York, NY: Springer. Rihoux, B., & Ragin, C. C. (2009). Configurational comparative methods: Qualitative comparative analysis (QCA) and related techniques. Sage. Shirky, C. (2010). "Talk about Curation" Video at Curation Nation. Retrieved February 01, 2017 from http://curationnationvideo.magnify.net/video/Clay-Shirky6#c=1RMHLF18HLLBQ6WK&t=Talk%20about%20 Siddaway, A. (n.d.). What is a systematic literature review and how do I do one? Retrieved March 26, 2017 from https://www.bing.com/cr Slater, M. D. (2007). Reinforcing spirals: The mutual influence of media selectivity and media effects and their impact on individual behavior and social identity. Communication Theory, 17(3), 281-303. Solon, O., Levin, S., & Wong, J. C. (2016, November 16). Bursting the Facebook bubble: we

IT'S ALL ABOUT YOU

"

61"

asked voters on the left and right to swap feeds. The Guardian. Retrieved July 11, 2017 from https://www.theguardian.com/us-news/2016/nov/16/facebook-bias-bubble-uselection-conservative-liberal-news-feed Sunstein, C. R. (2002). The law of group polarization. Journal of political philosophy, 10(2), 175-195. Sunstein Cass, R. (2007). Republic. com 2.0. The Zettabyte Era: Trends and Analysis. (2017, June 07). Cisco. Retrieved June 12, 2017 from http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networkingindex-vni/vni-hyperconnectivity-wp.html Tsfati, Y., Stroud, N. J., & Chotiner, A. (2014). Exposure to ideological news and perceived opinion climate: Testing the media effects component of spiral-of-silence in a fragmented media landscape. The International Journal of Press/Politics, 19(1), 3-23. Van Hoboken, J. V. J. (2012). SOURCE (OR PART OF THE FOLLOWING SOURCE): Type PhD thesis Title Search engine freedom: on the implications of the right to freedom of expression for the legal governance of Web search engines. Wohn, D. Y., & Bowe, B. J. (2016). Micro agenda setters: The effect of social media on young adults' exposure to and attitude toward news. Social Media+ Society, 2(1), 2056305115626750. Zhang, Y., & Wildemuth, B. M. (2016). Qualitative Analysis of Content. Applications of Social Research Methods to Questions in Information and Library Science, 318. Zhong, C., Shah, S., Sundaravadivelan, K., & Sastry, N. (2013, July). Sharing the loves: Understanding the how and why of online content curation. In ICWSM. Zuiderveen Borgesius, F. J., Trilling, D., Moeller, J., Bodó, B., de Vreese, C. H., & Helberger,

IT'S ALL ABOUT YOU

"

62"

N. (2016). Should We Worry about Filter Bubbles?.

IT'S ALL ABOUT YOU 9. Appendices 9.1 Appendix A

!

63!

Based on a relevance assessment, the articles with red text were judged not relevant for this MRP. Blue highlighting represents the articles found using the terms: [Facebook] + [News Feed] + [Social Media Curation] + [Recommender | Recommendation] + [System | Systems] + [Echo Chamber] + [Public Opinion] + [Filter Bubbles] Orange highlighting represents the articles found using the terms: [Facebook News Feed] + [Public Opinion] + [Echo Chamber] Green: highlighting represents the articles found using the terms: [Facebook News Feed] + [Public Opinion] + [Filter Bubble]
Author(s) 1 2 3 4 5 6 7 8 9 10 11 Abrahams, A. S., Jiao, J., Wang, G. A., & Fan, W. Adar, E Adee, S. Ahmad, U., Zahid, A., Shoaib, M., & AlAmri, A. Allcott, H., & Gentzkow, M. An, J., Quercia, D., Cha, M., Gummadi, K., & Crowcroft, J. Ananny, M Andén, K Andres, S. G Andrew, L. P Anspach, N. M. Title Vehicle defect discovery from social media PersaLog: Personalization of News Article Content Burst the filter bubble HarVis: An integrated social media content analysis framework for YouTube platform Social media and fake news in the 2016 election Sharing political news: the balancing act of intimacy and socialization in selective exposure Creating proper distance through networked infrastructure The `Refugee Crisis' On Facebook Disposable Social Media Profiles The missing links: An archaeology of digital journalism The New Personal Influence: How Our Facebook Friends Influence the News We Read

IT'S ALL ABOUT YOU

!

64!
The Reputation Society: How Online Opinions Are Reshaping the Offline World The `Right to be Forgotten' ­ Worth remembering? Exposure to Ideologically diverse news and opinion on Facebook How social media reduces mass political polarization Oblivious trailblazers: Case studies of the role of recording technology in the music-making processes of amateur home studio users The Impact of Curation Algorithms on Social Network Content Quality and Structure The world of Edgerank: Rhetorical justifications of Facebook's News Feed algorithm Selective Exposure, Filter Bubbles and Echo Chambers on Facebook Skipping politics: Measuring avoidance of political content in social media Predictive algorithms and personalization services on social network sites Cartoons as interdiscourse: A quali-quantitative analysis of social representations based on collective imagination in cartoons produced after the Charlie Hebdo attack Bias in algorithmic filtering and personalization Bursting the filter bubble: Democracy, design, and ethics Breaking the filter bubble: democracy and design From laboratories to chambers of parliament and beyond: Producing bioethics in France and Romania The curious case of the right to be forgotten Principles of Information Neutrality and Counter Measures Against Biased Information Social Media: a reference handbook

12 13 14 15 16

Ashby, M. Ausloos, J. Bakshy, Messing & Adamie Barberá, P Bell, A. P

17 18 19 20 21 22

Berman, R., & Katona, Z Birkbak, A., & Carlsen, H. B Bobok, D Bode, L., Vraga, E. K., & Troller-Renfree, S Bodle, R. Bouko C., Calabrese L., De Clercq, O.

23 24 25 26 27 28 29

Bozdag, E Bozdag, E Bozdag, E., & van den Hoven, J. Bretonnière, S. Bunn, A Burger, V., Hirth, M., Hoûfeld, T., & Tran-Gia, P Burns, K. S.

IT'S ALL ABOUT YOU

!

65!
# thyghgapp: Instagram Content Moderation and Lexical Variation in Pro-Eating Disorder Communities Citizen satire in Malaysia and Singapore: why and how socio-political humour communicates dissent on Facebook The structured self: Authenticity, agency, and anonymity in social networking sites Who views online extremism? Individual attributes leading to exposure The Digital Gatekeeper: How Personalization Algorithms and Mobile Applications Have Changed News Forever Facebook Family Values: A News Feed Hierarchy Of Needs From Editors to Algorithms: A values-based approach to understanding story selection in the Facebook news feed. FrancoAngeli

30

Chancellor, S., Pater, J. A., Clear, T., Gilbert, E., & De Choudhury, M

31

Chen, K. W.

32 33 34

Cirucci, A. M Costello, M., Hawdon, J., Ratliff, T., & Grantham, T. Crain, M. G

35 36

DeVito, M. A DeVito, M. A

37

di Napoli, F. I., Pescatore, G., Ritzer, G., Sorrentino, C., & Spagnoletti, G. Ditrich, L., & Sassenberg, K. Dylko, I., Dolgov, I., Hoffman, W., Eckhart, N., Molina, M., & Aaziz, O

38 39

Kicking out the trolls­Antecedents of social exclusion intentions in Facebook groups The dark side of technology: An experimental investigation of the influence of customizability technology on online political selective exposure Big data, bigger dilemmas: A critical review

40

Ekbia, H., Mattioli, M., Kouper, I., Arave, G., Ghazinejad, A., Bowman, T., ... & Sugimoto, C Erdos, D

41

Beyond `having a domestic'? Regulatory interpretation of European Data Protection Law and individual publication An indie hype cycle built for two: A case study of the Pitchfork album reviews of Arcade Fire and Clap Your Hands Say Yeah Spiritual aspects of meat and nutritional security: Perspectives and responsibilities of the Abrahamic faiths

42

Ernst, S. R

43

Farouk, JRegenstein, Pirie, Najm, Bekhit & Knowles

IT'S ALL ABOUT YOU

!

66!
Web data extraction, applications and techniques: A survey Filter bubbles, echo chambers, and online news consumption Citizen's Right to the Digital City Citizens breaking out of filter bubbles: urban screens as civic media. Entr'acte: Performing Publics, Pervasive Media, and Architecture. A slippery and inconsistent slope: How Cambodia's draft cybercrime law exposed the dangerous drift away from international human rights standards The rule of law online: Treating data like the sale of goods: Lessons for the Internet from OECD and CISG and sacking Google as the regulator A network analysis of official Twitter accounts during the West Virginia water crisis The relevance of algorithms Autonomy Challenges in the Age of Big Data Ferguson on Facebook: Political persuasion in a new era of media effects The unstoppable march of the machines Social network services as tools for online and offline activism: The case of Egymillióan a Magyar Sajtószabadságért Cultural scene detection using reverse Louvain optimization Teaching grounded audiences: Burke's identification in Facebook and composition Regulating the new information intermediaries as gatekeepers of information diversity Verhaltenssteuerung durch Algorithmen­Eine Herausforderung für das Recht

44 45 46 47 48 49

Ferrara, E., De Meo, P., Fiumara, G., & Baumgartner, R Flaxman, Goel & Rao Foth, M., Brynskov, M., & Ojala, T Foth, M., Tomitsch, M., Forlano, L., Haeusler, M. H., & Satchell, C. Geiger, J Gerry F QC & Moore, C

50

Gerry F. QC & Berova, N

51 52 53 54 55 56

Getchell M. C., Sellnow T. L. Gillespie, T. Grafanaki, S. (2016). Greenwood, M. M., Sorenson, M. E., & Warner, B. R. Gross, M György, T

57 58 59 60

Hamdaqa, M., Tahvildari, L., LaChapelle, N., & Campbell, B Head, S. L. Helberger, N., Kleinen-von Königslöw, K., & van der Noll, R. Hoffmann-Riem, W

IT'S ALL ABOUT YOU

!

67!
The Future Political polarization on twitter: Implications for the use of social media in digital governments Big Data in Survey Research AAPOR Task Force Report

61 62 63

Holmberg, K Hong, S., & Kim, S. H. Japec, L., Kreuter, F., Berg, M., Biemer, P., Decker, P., Lampe, C., ... & Usher, A Jones, D Joyce, A. A Jun, N

64 65 66

Seeing reason The Paranoid Style of Tea Party Politics Contribution of Internet news use to reducing the influence of selective online exposure on political diversity Decentralized content sharing among tourists in visiting hotspots Giving the "viewser" a voice? Situating the individual in relation to personalization, narrowcasting, and public service broadcasting Can Twitter be an Effective Platform for Political Discourse in Malaysia? A Study of #PRU13 (Hyper) local news aggregation: Designing for social affordances

67 68

Kaisar S., Kamruzzaman J., Karmakar G., Gondal I. Kant, T

69 70

Kasmani M. F., Sabran R. & Ramle N Kavanaugh, A., Gad, S., Neidig, S., Pérez-Quiñones, M. A., Tedesco, J., Ahuja, A., & Ramakrishnan, N. Knight, M., & Cook, C Koene, A., Vallejos, E. P., Webb, H., Patel, M., Ceppi, S., Jirotka, M., & McAuley, D. Labrecque, L. I., vor dem Esche, J., Mathwick, C., Novak, T. P., & Hofacker, C. F. Lehtiniemi, A Liao, Q Lipizzi, C., Dessavre, D. G., Iandoli, L., & Marquez, J. E. R. (

71 72

Social media for journalists: Principles and practice Editorial responsibilities arising from personalization algorithms

73

Consumer Power: Evolution in the Digital Age

74 75 76

Novel music discovery concepts: User experience and design implications Designing technologies for exposure to diverse opinions Towards computational discourse analysis: A methodology for mining Twitter backchanneling

IT'S ALL ABOUT YOU

!
conversations

68!

77 78 79 80 81 82 83

Lomborg, S., & Mortensen, M. Loosen, W., & Scholl, A Marsh, I., & McLean, S. Marshall, L., & Laing, D Maynard, D., Roberts, I., Greenwood, M. A., Rout, D., & Bontcheva, K. Merakou, A Messing & Westwood

Users across media: An introduction Journalismus im Zeitalter algorithmischer Wirklichkeitskonstruktion Why the political system needs new media Popular Music Matters: Essays in Honour of Simon Frith A framework for real-time semantic social media analysis The Selective Exposure Hypothesis Revisited: Does Social Networking Make a Difference? Selective exposure in the age of social media: Endorsements trump partisan source affiliation when selecting news online How social media introduces biases in selecting and processing news content E-parliament: Opening the door Future of story: Transmedia journalism and National Geographic's Future of Food project Understanding Conspiracy Online: Social Media and the Spread of Suspicious Thinking The role of blogging in public deliberation and democracy Automated media: An institutional theory perspective on algorithmic media production and consumption Communicating science effectively: A research agenda Monitoring the Opinion of the Crowd: Psychological Mechanisms Underlying Public Opinion Perceptions on Social Media Polarized and liking it: How political polarization affects active avoidance behavior on Facebook

84 85 86 87 88 89

Messing, S., & Westwood Missingham, R Moloney, K. T Mortimer, K Mummery, J., & Rodan, D. Napoli, P. M.

90 91

National Academies of Sciences, Engineering, and Medicine Neubaum, G., & Krämer, N. C.

92

Newman, B. L

IT'S ALL ABOUT YOU

!

69!
Friends, followers and the future: How social media are changing politics, threatening big brands, and killing traditional media Joan Shorenstein Center on the Press, Politics and Public Policy Filterblasen-Ausgangspunkte einer neuen, fremdverschuldeten Unmündigkeit? From "information" to "knowing": Exploring the role of social media in contemporary news consumption User-Generated Censorship: Manipulating The Maps Of Social Media Who is reading whom now: privacy in education from books to MOOCs The plural iPod: A study of technology in action Examining user surprise as a symptom of algorithmic filtering Understanding User Beliefs About Algorithmic Curation in the Facebook News Feed A social media text analytics framework for double-loop learning for citizen-centric public services: A case study of a local government Facebook use Crime, Justice and Social Media Agile PR: Expert Messaging in a Hyper-Connected, Always-On World Expanding the presidential debate by tweeting: The 2012 presidential election debate in South Korea Facilitating personal deliberation online: Immediate effects of two ConsiderIt variations # Republic: Divided Democracy in the Age of Social Media Ask me anything: A digital (auto) ethnography of Reddit. Com

93

O'Connor, R

94 95 96

O'Connor, R., & Sagan Fellow, S. C. Ovens, C Pentina, I., & Tarafdar, M.

97 98 99 100 101 102

Peterson, C. E Polonetsky, J., & Tene, O Prior, N Rader, E Rader, E. & Gary Reddick C. G., Chatfield A. T., & Ojo, A

103 104 105 106 107 108

Salter, M. Salzman, M Se Jung Park, Ji Young Park, Yon Soo Lim, Han Woo Park Stiegler, H., & de Jong, M. D Sunstein, C. R Thompson, K

IT'S ALL ABOUT YOU

!

70!
Despite the current fascination with Facebook and other social media as spaces for the circulation of political information, the sociability of politics is nothing new. Social media as a new playing field for the governance of agro-food sustainability Together we are better: Professional learning networks for teachers. Tracking phantastic objects: A computer algorithmic investigation of narrative evolution in unstructured data sources New participation, new perspectives? Young adults' political engagement using Facebook Beyond self-reports: Using eye tracking to measure topic and style differences in attention to social media content A user-centered and group-based approach for social data filtering and sharing Statistical physics of vaccination

109

Thorson, K., Vraga, E., & Kligler-Vilenchik, N.

110 111 112

TM Stevens, N Aarts, CJAM Termeer, A Dewulf Trust, T., Krutka, D. G., & Carpenter, J. P Tuckett, D., Smith, R. E., & Nyman, R.

113 114

Van Wyngarden, K. E Vraga, E., Bode, L., & Troller-Renfree, S

115 116

Vu, X. T., Abel, M. H., & Morizet-Mahoudeaux, P Wang Z., Bauch C. T., Bhattacharyya S., d'Onofrio A., Manfredi P., Perc M., Perra N., Salathé M. & Zhao D. Warso, Z

117

There's more to it than data protection ­ Fundamental rights, privacy and the personal/household exemption in the digital age Chapter 11 - Politics and Civil Society Micro Agenda Setters: The Effect of Social Media on Young Adults' Exposure to and Attitude Toward News Privacy and Social Media Monitoring and Expressing Opinions on Social Networking Sites­Empirical Investigations based on the Spiral of Silence Theory Incidental exposure to online news The Politics of "Unfriending": User Filtration in Response to Political Disagreement on Social Media

118 119

Wassom B. D. Wohn & Bowe

120 121

Wolf, R. D., & Heyman, R. XX

122 123

Yadamsuren, B., & Erdelez, S. Yang, J., Barnidge, M., & Rojas, H

IT'S ALL ABOUT YOU

!

71!
Share This!: How You Will Change the World with Social Networking Should We Worry about Filter Bubbles?

124 125

Zandt, D Zuiderveen Borgesius, F. J., Trilling, D., Moeller, J., Bodó, B., de Vreese, C. H., & Helberger, N Goel, Mason, & Watts

126

Real and Perceived Attitude Agreement in Social Networks

IT'S ALL ABOUT YOU 9.2 Appendix B

!

72!

The following literature tables include the findings, use of the key terms (echo chamber, filter bubble and public opinion), limitations, subjects and methods for the 19 academic and seven supplementary articles chosen. After a second relevance assessment those articles with their title highlighted in red were not included in the data analysis or discussion section of this MRP.

Table 1 Literature examined based on a relevance assessment.
Date Title and Author Research purpose Subject(s) Methodology Theme 1: Filter Bubble Theme 2: Echo Chamber Theme 3: Public Opinion Does not use term. Key finding(s) Limitation(s)

2010

Real and Perceived Attitude Agreement in Social Networks (Goel, Mason, & Watts 2010)

To determine if Facebook networks are homophilic or not.

2,504 Facebook users (analyzed 900 respondent s' political answers).

Network survey conducted on Facebook.

Does not use term.

Individuals selfsort into likeminded communities that serve as echo chambers.

Although friends exist in mostly homogenous networks they are still exposed to disagreement and disagree more than they think they do. Friends agree 75% of the time, random strangers agree 63% of the time.

Survey, requires selfreporting.

IT'S ALL ABOUT YOU

!

73!
Does not use term. Does not use term. Does not use term. Social networking sites allow for exposure to cross cutting opinions and exposure to political difference, which is an optimistic outcome for enhancing democracy. This study relied on self-reporting and did not actually measure exposure to crosscutting political views online. This study focused on SNSs including Facebook and Myspace. These social media platforms operate differently and the findings should not be generalized across platforms.

2011

The contribution of social network sites to exposure to political difference: The relationships among SNSs, online political messaging, and 28 G. Neubaum and N. C. Krämer exposure to cross-cutting perspectives (Kim, 2011)

Seeks to understand how individuals' exposure to political difference is influenced by social network sites influence.

Random sample of U.S. telephone households totally 2254 respondent s ages 18 and older (response rate was 23%).

Secondary data, originally collected by Pew Internet & American Life Project.

More educated people are less exposed to crosscutting points of view than less educated people.

IT'S ALL ABOUT YOU

!

74!
Used interchangeabl y with echo chamber. Used interchangeably with filter bubble. Does not use term. Users can influence the information filtering process, even when recommender algorithms are being used. Based on literature (no empirical evidence presented).

2013

Bias in algorithmic filtering and personalizati on (Bozdag, 2013)

Analyze the role of recommende r algorithms as online gatekeepers.

Literature.

Existing literature to create a model of algorithmic gatekeeping.

"...democracy is most effective when citizens have accurate beliefs and to form such belies individuals must encounter information that will sometimes contradicts their preexisting views." (p.218).

If Facebook decides which updates appear on users' walls, interaction frequency may impact whose content you see, thus causing posts from weak-ties to disappear.

Search results can vary (specifically on Google), but more empirical research is needed.

Raises the question, is there enough choice of friends to vary their exposure?

IT'S ALL ABOUT YOU

!

75!
"Those attempting to overcome news information overload and to make better sense of the contemporize events, increasingly rely on information curated by like-minded others populating their virtual social networks. " (p.211). Niche advertising strategies may further contribute to the "filter bubble. (p.222). Does not use term. Does not use term. "Unintended consequence of social filtering may ultimately undermine civic discourse by confirming our preexisting views and limiting our exposure to challenging beliefs. " Social media is a platform where users' screen news and situate the news media in relation to familiar individuals, which allows for them to process and interpret news information. These are two coping mechanisms to reduce information overload. In an interview a respondent reported social media as making it impossible to avoid exposure to news. Interview based, although a strength in some ways, there is no opportunity to verify through collected data as Facebook News Feeds cannot be automatically scrapped.

2014

From "information " to "knowing": Exploring the role of social media in contemporar y news consu mption (Pentina & Tarafdar, 2014)

Addressing the information overload and devising strategies for news sensemaking and the resulting civic knowledge formation

112 diverse crosssection of US news consumers.

Interviews.

IT'S ALL ABOUT YOU

!

76!
Mention in the literature review, but no further study of the effects of echo chambers on opinion dynamics. Use the term echo chamber as individuals being exposed to likeminded views. However, posits that Jamieson and Cappella ignore that social media platforms create a space where users can be exposed to people whom they have weak -ties with and who may have information that individuals would not typically be exposed to. Does not use term. Does mention political moderation, which would allow for more balanced interpretation of public opinion. Found that social media reduces mass political polarization. Diverse networks become more moderate over time. Exposure to people whom you have weak-ties allows for a diversity of ideologies. Tests Twitter, but generalizes the results to apply to all social media. Shows individuals are exposed to people's opinions who they have weakties with (because they share a network), but doesn't consider how these opinions may be dismissed or hidden by recommender systems.

2014

How social media reduces mass political polarization (Barberá, 2014)

Argue that social media platforms like Facebook and Twitter increase incidental exposure to political news and increases exposure to people who users have weak-ties to (who will often have challenging views and ideas).

Twitter users from Germany, Spain and the United States. 50,000 Germany 50,000 Spain 94,441 user matched with voter files in U.S.

New method to estimate and measure dynamic ideal points for social media users. In this study the researchers matched Twitter profiles with voter files in several U.S. states.

IT'S ALL ABOUT YOU

!

77!
Term used interchangeabl y with echo chamber. Recommender algorithms are used as they may isolate individuals in a filter bubble or echo chamber Term used interchangeably with filter bubble. Recommender algorithms are used as they may isolate individuals in a filter bubble or echo chamber Public discourse. They write that their findings have implications for agenda setting as the world view is no longer traditional media, rather Facebook News Feeds. (Facebook is now our pseudoenvironment) Both experiments found that participants were more likely to read endorsed stories. Only tested undergraduates from the West Coast research university, thus choosing an educate and age uniform group, limiting the studies salience to other populations. Made a web application that was Facebook-like, not actual data observed or collected from Facebook.

2014

Selective exposure in the age of social media: Endorsement s trump partisan source affiliation when selecting news online (Messing & Westwood, 2014)

Seeks to understand the socialization of news online and how it changes the framework in which news reading occurs by providing a place that supports the exposure to news from politically dissimilar individuals.

739 recruited using Mechanica l Turk

Two incentivized experiments were conducted using a developed web based application with interfaces similar to Facebook.

IT'S ALL ABOUT YOU

!

78!
Does not use term. Echo chamber as described by Sunstein as a place where political orientation is reaffirmed. Does not use term. Public sphere where reasoning and public dialogue allows for deliberation. The public sphere allows for diverse opinions and information to interact. Democrats show higher levels of political homophily. However, Republicans who follow official Republican accounts have higher levels of homophily than Democrats. Furthermore, there is more homophily in networks of reciprocated followers than nonreciprocated networks. Twitter analysis cannot be generalized to Facebook.

2014

Echo Chamber or Public Sphere? Predicting Political Orientation and Measuring Political Homophily in Twitter Using Big Data. (Colleoni, Elanor, Alessandro Rozza & Adam Arvidsson, 2014)

Analyses the political homophily on Twitter.

2009 Twitter users (Big data allows for more generaliza ble statements)

Systematic big data analysis. Machine learning and social network analysis to classify users are Democrats or republicans. Measured homophily through the number of outbound ties that were politically same of different for each account.

IT'S ALL ABOUT YOU

!

79!
Used extensively, as defined by Pariser. Does not use term. Does not use term. Filter bubbles are higher in some searches than others. Also, some subjects had similar clustered results from their searches, while others were outliers, these outliers were more `bubbled' than their peers. Users seldom click beyond the first page, they believe the information closest to the top is understood as the best, even when the search results were randomly scrambled. Limited number of subjects (20).

2015

Detecting and Visualizing Filter Bubbles in Google and Bing (Dillahunt, T. R., Brooks, C. A., & Gulati, S., 2015)

Explores if filter bubbles can be measured as an initial investigation to identifying how users' can better understand how filter bubbles impact their search results.

20 users from Amazon's Mechanica l Turk.

Users conduct five unique search queries then researchers analyze the differences.

Shows their variation in search results, but ignores what those variations may be caused by. It would be interesting to do personality and political leanings surveys to see how similar the results are for likeminded subjects.

IT'S ALL ABOUT YOU

!

80!
Recommendat ions of decreasing diversity over time (previous studies found evidence, but it was weak and the studies were not empirical) Does not use term. Does not use term. The data showed that the majority of participants (73%) did not believe that they were exposed to all of the content their friends create. Data showed varied and competing reasons causing participants to act differently on Facebook depending on how they believe the system works. Study is based on data collected through selfidentification and reporting.

2015

Understandi ng User Beliefs About Algorithmic Curation in the Facebook News Feed (Rader & Gray, 2015)

Analyzing how individuals understand the influence of algorithms, and how awareness of algorithmic curation may impact their interactions with these systems.

464 Facebook recruited respondent s using Amazon's Mechanica l Turk users at least 18 years-old with minimum 20 Facebook friends.

Incentivized survey. Included open-ended questions.

IT'S ALL ABOUT YOU

!

81!
Does not challenge Pariser 's definition, rather discusses the varying criticisms as a product of varying stances on democracy. The filter bubble poses a problem because of the different models of democracy. Most researchers aim to fight the filter bubble, but do not define the filter bubble explicitly. Does not use term. "Deliberative democracy can be seen (1) as a matter of forming public opinion through open public discussion and translating that opinion into legitimate law." (Cohen, 2009)." The problem of the filter bubble varies dependent on the interpretation of democracy. Filter bubbles are a problem for the liberal democrats because of the restrictions on choice. Deliberative democracy attempts to increase information quality, and discover perspective and disagreements. Contestatory democracy focuses on channels that allows individuals to contest. None to note.

2015

Breaking the filter bubble: democracy and design (Bozdag & van der Hoven, 2015)

"Provide different models of democracy and discuss why the filter bubble poses a problem for these different models." (p.250) "Provide a list of tools and algorithms that designers have developed in order to fight filter bubbles." (p.250)

15 tools designed to disrupt the filter bubble.

Analysis of existing tools created to disrupt the filter bubble.

IT'S ALL ABOUT YOU

!

82!
Content is selected by algorithms based on a viewer's previous behaviors. Individuals are exposed only to information from like-minded individuals. Does not use term. The data showed that on average users' choices influenced their News Feeds more than algorithms. The ideological scoring method does not measure how partisan-biased the news article is, rather it considers how often the news is shared by one ideologically similar group. Measures 9% of Facebook users, and uses those who report their political leanings online. Not reproducible. It was conducted inhouse by Facebook data scientist.

2015

Exposure to Ideologically diverse news and opinion on Facebook (Bakshy, Messing & Adamic, 2015)

"how do online networks influence exposure to perspectives that cut across ideological line?" (para.1).

10.1 million U.S. Facebook users.

Systematic data analysis. (Case study?) The researchers quantified the degree to which users were exposed to more, or less, diverse news on their Facebook News Feed.

IT'S ALL ABOUT YOU

!

83!
Define filter bubbles as being created inadvertently by automatically recommending content based on perceived user preference Define echo chambers as in which individuals are largely exposed to conforming opinions. Does not use term. Data that supports both sides of the filter bubble debate, they found that, while social media networks and news aggregators are increasing the personalization of content and potentially creating filter bubbles or echo chambers, there is also the potential for increased choice and greater exposure to diverse ideas. Focused on ideological slant of news provider, would therefore misinterpret the news preferences of an individual who primarily reads liberal articles from conservative sites. Focus on news consumption and not the effect it may have on voting behaviors, perception of public opinions, etc. Does not consider those who have limited exposure to news and may exclusively or largely get this exposure through social media.

2016

Filter bubbles, echo chambers, and online news consumption (Flaxman, Goel & Rao, 2016)

Analyzing how ideological segregation and consumption of news manifests online.

1.2 million U.S. users.

Analyzed the web-browsing and social media browsing records over a three-month period.

IT'S ALL ABOUT YOU

!

84!
Uses Pariser 's definition (2011) Draws on Sunstein (2006), and Jamieson and Cappella (2010) Does not use term. Public discourse is a key element in democratic governance. The data supported echo chambers on partisan Facebook pages. Reproducible study (strength). For the purpose of this paper, the finding of an echo chamber within patrician Facebook groups is tangent to the appearance of an echo chamber on an individual's Facebook News Feeds.

2015

Open media or echo chamber: the use of links in audience discussions on the Facebook Pages of partisan news organization s (Jacobson, Myung, & Johnson, 2015)

Aims to determine if, the use of links within partisan Facebook groups allow competing ideas to break through the filter bubble.

Audience comments that included a link to an outside informatio n source on the O'Reilly and Maddow Facebook pages.

Reviewed audience discussions on partisan Facebook pages.

IT'S ALL ABOUT YOU

!

85!
Interchangeabl y used with echo chamber. Based on studies by Bakshy, Messing, & Adamic, 2015; Kim, 2011; Radinie & Smith, 2012, the researchers for this article brought forth the assumption that users are frequently confronted with crosscutting social networks. Interchangeably used with filter bubble. Credits Sunstien with the term echo chamber. Based on studies by Bakshy, Messing, & Adamic, 2015; Kim, 2011; Radinie & Smith, 2012, the researchers for this article brought forth the assumption that users are frequently confronted with cross-cutting social networks. Does not use term. Exposure to news media influences individuals' opinions. Users, "were found to use opinion cues on social networking sites to infer prevailing opinion climates on public issues." (p.25) Usergenerated comments were shown to effect on users' perception of public opinion. Facebook users', "fear of isolation sharpens their attention toward user-generated comments on Facebook which, in turn, affect recipients' public opinion perceptions. The latter influenced subjects' opinions and their willingness to participate in social media discussions." (p.1) Fictitiously generated Facebook News Feed creates questions about the findings, as users wouldn't recognize those who are producing the likes and comments as familiar. Familiarity of those sharing, commenting and liking the content is a key element to the aggregation of Facebook News Feed content.

2016

Monitoring the Opinion of the Crowd: Psychologica l Mechanisms Underlying P ublic Opinion Perc eptions on Social Media (Neubaum & Krämer, 2016)

Analyses in what way do the social cues on Facebook News Feed articles (likes and comments) affect user's interpretatio n of the correspondin g news article.

657 Facebook users who volunteere d using the SoSci panel to participate in online research. Aged ranged from 16 to 75 yearsold and 387 of the participant s were female. 85.1% used Facebook at minimum once a week.

A survey based on a fictitious Facebook News Feed.

IT'S ALL ABOUT YOU

!

86!
Uses Pariser and Sunstein's (2011; 2002) definition. Uses Pariser's (2011) definition. No mention of Jamieson and Cappella. Does not use term. Network sites are new gatekeepers and opinion influencers. Competing opinion to develop your own opinion and engage in good democracy. Facebook users are exposed to preselected personalized content and may or may not be aware of this. "... in spite of the serious concerns voiced ­ at present, there is no empirical evidence that warrants any strong worries about filter bubbles." (p.10). Engagement with news is positively associated with exposure to political disagreement. The amount of disagreement users are exposed to does not relate to responsive user filtration. Media literacy effects how users' interact on social media networks because they are more aware of the filtration capabilities. May not be generalizable to communities outside of Colombia. Does share the methods for collection of the literature.

2016

Should we worry about filter bubbles? (Zuiderveen, Borgesius, Trilling, Moller, Bodo, Vresse & Helberger, 2016)

Analyzing the existing literature to determine if we should worry about filter bubbles

Academic literature.

Model based literature review

2016

The Politics of "Unfriendin g": User Filtration in Response to Political Disagreemen t on Social Media (Yang, Barnidge, & Rojas, 2016)

Examine social media users' exposure to political disagreemen t and filtration.

Nationally representat ive sample of Colombian adults.

Survey data collected from August 29 to September 17, 2012.

Facebook algorithms may filter disagreeable and undesirable content. Draws on Pariser (2011).

People are disconnected from diverse others. This is drawing on Sunstein and Bennett and Ivengar (2007; 2008).

Political information on social media shapes users' perceptions of public opinion.

Self-reporting method.

IT'S ALL ABOUT YOU

!

87!
Filter Bubbles (Pariser , 2011). Does not use term. Does not use term. Not all algorithms are responsible for filter bubbles. Highly curated feeds could allow for the spread of "fake news" if it matches the beliefs of some readers. When platforms use curation, users are less likely to curate their networks. None to note.

2016

The Impact of Curation Algorithms on Social Network Content Quality and Structure (Berman, & Katona, 2016)

Seeks to understand the different types of algorithms to better understand filter bubbles on social media platforms.

Uses existing literature to reverse engineers different algorithms.

Model based on literature

Perfect Algorithm filters create filter bubbles, while Quality Algorithms vary exposure.

IT'S ALL ABOUT YOU

!

88!
Filter Bubbles (Pariser , 2011). Echo chambers (Sunstein, 2002). Does not use term. Exposure to diverse opinions for civil discourse. Customizable technology can undermine important foundations of deliberative democracy. Found that customizable technology increases political selective exposure, thus providing empirical evidence for echo chambers and filter bubbles. Students are known to be more open to diverse perspective and may internalize the values, underestimating the technologies exposure on selective exposure. Only liberal and conservative articles, what about nonpolitical?

2017

The dark side of technology: An experimental investigation of the influence of customizabili ty technology on online political selective exposure (Dylko, Dolgov., Hoffman, Eckhart, Molina, & Aaziz, 2017)

Analyze the relationship between personalized technology and political selective exposure.

93 students from a southwest U.S. university.

Psychological experiment. The aggregated customizability group (N=81) was created to test more information systems more similar to Facebook or Google.

System-driven customizability led to greater political selective exposure than user-driven customizability. (p. 188)

IT'S ALL ABOUT YOU

!

89!

Supplementary Literature
Date Title and Author Did Facebook's Big Study Kill My Filter Bubble Thesis? (Pariser, 2015) Published at: Back Channel (Wired Magazine's Blog) 2015 Why Scientists Are Upset About the Facebook Filter Bubble Study (Lumb, 2015) Drawing on academic blog posts and Tweets that are in retort to Baskshy, Messing & Adamies study (2015). Tech writer for Fast Company Journalism article. Purpose Author credentials The author of The filter bubble Type Filter Bubble Echo Chamber Public Opinion Does not use term. Key points:

2015

A response to Baskshy, Messing & Adamies study (2015).

Journalism article

Exists, but less about algorithms and more about your network (this is more aligned with Jamieson & Cappella) .

Does not use term.

The effect is smaller than Pariser originally thought (6% decrease in seeing cross-cutting content). Only 7% of the content clicked on is hard news. Pariser says Facebooks study does not `kill' his filter bubble theory.

Published at: Fast Company

Social media Filter bubble in which users assume most people agree with their perspective because they are not exposed to competing viewpoints.

Does not use term.

Does not use term.

Draws on quotes from academics such as, Zeynep Tufekci, a professor at the University of North Carolina, Chapel Hill, who says the study is not representative of Facebook as a whole and accuses the study minimizing the impact of Facebook algorithms in favour of pointing out that users create their own filter bubbles by selecting what to click on. Limitation: This article only represents researchers who are opposed to the results, presumably there are ones who are aligned with the findings.

IT'S ALL ABOUT YOU

!

90!
Facebook algorithms guess what you want to see on your timeline filtering out important content. Does not use term.

2015

Facebook `filter bubble' study raises more questions than it answers (Mathew Ingram, 2015) Published by: Fortune

A response to Baskshy, Messing & Adamies study (2015).

Senior writer at Fortune.

Journalism article: Bases on interviews with industry experts.

Quotes: Sociologist Nathan Jurgenson Sociologist and social-media expert Zeynep Tufekci. Christian Sandvig, an associate professor at the University of Michigan

Study is a way for Facebook to divert guilt and according to critics did nothing to help Facebook prove a point. "... there is no scenario in which user choices vs. the algorithm can be traded off, because they happen together." Users select from what the algorithms has selected for them.

IT'S ALL ABOUT YOU

!

91!
Highly personalized news feeds expose users to content that reinforces users' preexisting beliefs. Does not use term. Does not use term. Says exposure to the competing News Feeds caused some to change their opinions or further insulated them into their existing beliefs. Tens of millions of American voters get their news from Facebook. Five conservatives and 5 liberals were exposed to a fake Facebook profile that were created creating an avatar and liking news articles that represent each avatar (conservative versus liberal) Most participants were aware of the filter bubble, but still found it more intense than expected. ...the platform "seems to filter out credible news articles on both ends and feed sensationalist far left/far right things". All participants agree, the Facebook feed that represented the other side read largely wrong.

2016

Bursting the Facebook bubble: we asked voters on the left and right to swap feeds (Julia Carrie Wong, Sam Levin and Ol ivia Solon, 2016) Published by: The Guardian

Test the effects of political polarization on Facebook

Journalists for The Guardian in San Francisco, U.S.

Investigative journalism: Asked 10 U.S. voters to switch news feeds and self-report on the effects.

2016

Burst the filter bubble (Adee, 2016) Published by: New Scientist

Design tweaks and new habits could help pop users' Facebook filter bubbles.

New Scientist Tech writer and editor

Journalism article: Bases on interviews with industry experts.

Facebook is being criticized for trapping users in filter bubbles that reflect only their own views.

Does not use term.

Does not use term.

Facebook should be considered as a media firm and should have public editors and curate their content. Matias says. We don't know how much impact filter bubbles have on our views. In fact, the idea of the filter bubble has never been proved empirically. One expert is skeptical about popping the filter bubble as an unfiltered perspective may not exist. An easy fix is to, keep your network diverse.

IT'S ALL ABOUT YOU

!

92!
Does not use term. Does not use term. Does not use term. This article pokes fun at Facebook's insular News Feeds apologizing for accidently exposing users' to ideologically challenging or novel ideas. Shows that despite the lack of supporting empirical evidence, there is a feeling of insulation online, so much that it's a joke in the media for Facebook to not be insular. Implies that Facebook staff is aware of their platform being insular and the personalized headlines are chosen to reinforce users' existing ideologies.

2017

Horrible Facebook Algorithm Accident Results In Exposure To New Ideas ("Horrible Facebook algorithm accident," 2017) Published by The Onion

To draw attention to the ridiculous nature to insular Facebook News Feeds

Satirical publication

Satirical journalism

2017

How to Escape Your Political Bubble for a Clearer View (Hess, 2017) Published by: New York Times

Reflecting on Facebook News Feeds role in President Donald Trump's election

New York Times Internet culture writer and David Carr Fellow

Journalism article: Based on research and other news articles/blog posts.

When social networks lock users in personalized feedback loops.

'90s buzzword for partisan talk radio and newspapers. Echo chambers have been amplified through automation.

Does not use term. Does mention Facebook being blamed for the downfall of democracy.

Lists tech products users can use to better understand their own filter bubbles. Products include Politecho and FlipFeed, Read Across the Aisle.

