sensors
Article

Multispectral LiDAR Data for Land Cover Classification of Urban Areas
Salem Morsy *, Ahmed Shaker and Ahmed El-Rabbany
Department of Civil Engineering, Ryerson University, 350 Victoria Street, Toronto, ON M5B 2K3, Canada; ahmed.shaker@ryerson.ca (A.S.); rabbany@ryerson.ca (A.E.-R.) * Correspondence: salem.morsy@ryerson.ca; Tel.: +1-416-979-5000 (ext. 4623) Academic Editor: Vittorio M. N. Passaro Received: 19 February 2017; Accepted: 24 April 2017; Published: 26 April 2017

Abstract: Airborne Light Detection And Ranging (LiDAR) systems usually operate at a monochromatic wavelength measuring the range and the strength of the reflected energy (intensity) from objects. Recently, multispectral LiDAR sensors, which acquire data at different wavelengths, have emerged. This allows for recording of a diversity of spectral reflectance from objects. In this context, we aim to investigate the use of multispectral LiDAR data in land cover classification using two different techniques. The first is image-based classification, where intensity and height images are created from LiDAR points and then a maximum likelihood classifier is applied. The second is point-based classification, where ground filtering and Normalized Difference Vegetation Indices (NDVIs) computation are conducted. A dataset of an urban area located in Oshawa, Ontario, Canada, is classified into four classes: buildings, trees, roads and grass. An overall accuracy of up to 89.9% and 92.7% is achieved from image classification and 3D point classification, respectively. A radiometric correction model is also applied to the intensity data in order to remove the attenuation due to the system distortion and terrain height variation. The classification process is then repeated, and the results demonstrate that there are no significant improvements achieved in the overall accuracy. Keywords: multispectral LiDAR; land cover; ground filtering; NDVI; radiometric correction

1. Introduction With the evolution of airborne LiDAR technology, numerous studies have been conducted on the use of airborne LiDAR height and intensity data for land cover classification [1­4]. Initial studies have combined LiDAR-derived height surfaces in the format of the Digital Surface Model (DSM) or the normalized Digital Surface Model (nDSM) with multispectral aerial/satellite imagery [5,6]. Other investigations have combined multispectral aerial/satellite imagery with LiDAR height and intensity data [7­9]. Since most of the previous studies converted either LiDAR intensity or height data into 2D images, typical LiDAR images such as intensity [2­4,7,8], multiple returns [2,7], DSM, the Digital Terrain Model (DTM) [2,4] and nDSM [5,6,8] were created. Furthermore, when the LiDAR intensity data were combined with multispectral aerial/satellite imagery, the NDVI was used [5,6,8]. Traditional supervised pixel-based classification techniques such as maximum likelihood [5,9], rule-based classification [2,3,8] and the Gaussian mixture model [7] were applied. Other studies accounted for the spatial coherence of different objects to avoid the noises in the pixel-based classification results by using object-orientated classification techniques [4,6]. Brennan and Webster [2] used a rule-based classification approach for segmenting and classifying five bands, which were created from LiDAR data, into land cover classes. The five bands include DSM, digital terrain model, intensity, multiple returns and normalized height. The image pixels were first segmented into objects by applying threshold values on mean intensity, the standard deviation of intensity, mean DSM, mean normalized height and mean multiple returns. The segmentation
Sensors 2017, 17, 958; doi:10.3390/s17050958 www.mdpi.com/journal/sensors

Sensors 2017, 17, 958

2 of 21

process included four levels, where the image objects were separated in the first level into three classes, namely water, low land and elevated objects. The elevated objects were further discriminated into trees and buildings. The low land objects were separated into additional classes, such as low vegetation, roads and intertidal, until the image objects were segmented into ten classes by the fourth level. The overall classification accuracy was 94% and 98% for ten and seven classes, respectively. However, this study relied mainly on the height of LiDAR data. In addition, the threshold values, which were used in the classification rules, were applied to the object's mean values (e.g., mean intensity). That was a source of error, especially where building edges and ground returns formed one image object in the normalized height band. Furthermore, some dense coniferous trees exhibited single returns as they could not be penetrated by the laser beam. Those trees were misclassified as buildings because the separation of trees from buildings primarily relied on the multiple returns' band. In [3], a sensitive analysis on eight different LiDAR-derived features from height and intensity for three different sites was exhibited. First, image objects segmentation was conducted based on five generated bands from the LiDAR returns, namely bare soil, first returns, last returns, height and intensity. Second, six features based on height, including mean, standard deviation, homogeneity, contrast, entropy and correlation, in addition to mean intensity and compactness were computed. Finally, a decision tree was used to classify the image objects into five land cover classes and achieved more than 90% overall classification accuracy. It should be pointed out that the classification process relied on three features, mean height, height standard deviation and mean intensity, while the use of other features did not improve the land cover classifications. This might be due to that the tested sites were not complex landscapes (e.g., no interference between trees and buildings). Furthermore, the intensity band was assigned a weight of 0.1 in the segmentation process, while other bands were assigned an equal weight of one. This is because the recorded intensity was manually adjusted during the data acquisition. As a result, the intensity data were inconsistent along the different flight lines, meaning that the intensity data were not sufficiently used in this study. Antonarakis et al. [4] used a supervised object-orientated approach to classify the terrain into nine classes. Eight LiDAR-derived bands, namely the canopy surface model, terrain model, vegetation height model (VHM), intensity model, intensity difference model, skewness model, kurtosis model and percentage canopy model, were first created. A decision tree was then applied in order to classify three urban area datasets. The used approach brought out more than 93% overall accuracy for the three datasets. However, two essential classes were not considered in the presented approach (roads and buildings), even though some buildings were present in one of the three investigated sites. The VHM was calculated by subtracting the digital terrain model (created from the last return) from the canopy surface model (created from the first return). Some last return values had higher elevations than the first pulse return due to noise in the LiDAR receiver, which affected the calculation of VHM. As a result, this approach could not accurately distinguish between the ground and canopy tops. Another source of error is resulted from the triangulated irregular network interpolation of the LiDAR points to create images, whereas high elevations were recorded on the river surface. Other investigations have explored the use of LiDAR-derived height surfaces, such as the nDSM with multispectral imagery in land cover classification. Huang et al. [5] incorporated LiDAR-derived nDSM with high-resolution RGB aerial image and near-infrared band imagery. A pixel-based classification method, maximum likelihood, was used to obtain four land cover classes, namely buildings, trees, roads and grass, and achieved an overall accuracy of up to 88.3%. The classification accuracy was further improved up to 93.9% using a knowledge-based classification and correction systems. This technique was based on a set of threshold values applied to the height, height difference, smoothness, anisotropic smoothness, intensity, NDVI, transformed vegetation index, area and shape in order to detect the four land classes. Chen et al. [6] incorporated LiDAR-derived nDSM with Quick-Bird images in order to classify the terrain. First, two bands were derived from Quick-Bird imagery, namely the Normalized Difference Water Index and NDVI, and then combined with nDSM. Second, a hierarchical object-oriented classification method was used, which included image segmentation,

Sensors 2017, 17, 958

3 of 21

and then, threshold values to image objects were applied. The hierarchical classification process achieved an overall accuracy of 89.4% for nine land classes. However, this method could not separate road from vacant land, as these objects exhibit similar spectral and elevation characteristics. Both of the aforementioned studies did not incorporate LiDAR intensity data in their research work. The optical images were resampled to coarser resolution to be consistent with the created LiDAR images that resulted in mixed pixels. These pixels presented more than one land cover and caused classification errors. Other studies used multispectral imagery with LiDAR data (height and intensity) to take advantage of reflectivity variation from spectrum ranges (e.g., visible and Near Infrared (NIR)) of different land objects. Charaniya et al. [7] generated four LiDAR bands from height, intensity, height variation, multiple returns and the luminance band, measured in the visible spectrum, from aerial imagery. A Gaussian mixture model was then used to model the training data of four classes, including roads, grass, buildings and trees. The model parameters and the posterior probabilities were estimated using the expectation-maximization algorithm. Subsequently, those parameters were used to classify the tested dataset and resulted in overall accuracy of 85%. The results demonstrated that the height variation played an important role in classification, where the worst results were obtained by excluding the height band. Furthermore, the overall accuracy was decreased by excluding the aerial imagery. The use of the difference of multiple returns improved the classification of roads and buildings. However, it decreased the classification accuracy of other terrain covers, because of the misclassification of the grass patches. Hartfield et al. [8] combined LiDAR data with a 1-m resolution multispectral aerial image. Two LiDAR bands, namely intensity and nDSM, were generated from the LiDAR data, and NDVI was derived from the multispectral aerial image. Classification and regression tree were tested on the number of band combinations. The combination of LiDAR nDSM, multispectral image and NDVI produced the highest overall accuracy of 89.2% for eight land cover classes. The shadow in the aerial image affected significantly the classification results. In addition, misclassification between the bare ground and herbaceous (grass) classes occurred due to the use of the intensity data. This is because the intensity data needed to be calibrated as reported in [8]. Singh et al. [9] combined Landsat Thematic Mapper (TM) imagery with LiDAR-derived bands, which included intensity, the canopy height model and nDSM. The maximum likelihood classifier was applied to classify land cover into six classes. A number of band combinations was tested considering different resolutions of TM imagery such as 1 m, 5 m, 10 m, 15 m and 30 m. Classification of 1-m resolution TM imagery combined with the three LiDAR bands brought out the highest overall accuracy of 85%. The classification results were affected by two main sources of errors; first, the LiDAR data gaps that contributed to misinterpretation when creating 2D LiDAR images; second, the LiDAR intensity data were not normalized to a standard range. The effects of radiometric correction of LiDAR intensity data on land cover classification accuracies have been recently demonstrated. Radiometric correction aims to convert the recorded intensity data into the spectral reflectance of the land objects. Based on the radar (range) equation, system and environmental parameters were studied such as flying height, range, incidence angle, sensor aperture size and atmospheric attenuation, to correct the LiDAR intensity data [10,11]. By using the radiometrically-corrected LiDAR intensity data, the overall classification accuracies of urban areas were improved by 7.4% [12], 9.4­12.8% [11] and 3.8­16.5% [13]. 2. Historical Development of Multispectral LiDAR Systems In the past few years, numerous attempts have been conducted towards multispectral LiDAR systems. Laboratory-based multispectral LiDAR systems have been developed to collect data at different wavelengths [14­16]. Analysis of multispectral LiDAR data, collected from Terrestrial Laser Scanning (TLS) platforms, was conducted in order to retrieve the biophysical and/or biochemical vegetation parameters [17­21]. A few attempts have been reported on multispectral airborne LiDAR,

Sensors 2017, 17, 958

4 of 21

which uses various airborne LiDAR systems and combines different flight missions of the same study area [22­24]. Laboratory-based multispectral LiDAR systems have been developed to collect data at wavelengths of 531, 550, 660 and 780 nm [14] and 556, 670, 700 and 780 nm [15], in order to measure the 3D structure of forest canopies. Shi et al. [16] developed a calibration method for the backscatter intensity from a laboratory-based multispectral LiDAR systems operating at wavelengths of 556, 670, 700 and 780 nm. This method accounted for the incidence angle and surface roughness. After that, different vegetation indices were defined and explored, in order to improve the classification accuracy. Other investigations used TLS platforms to collect multispectral LiDAR data. For instance, a dual-wavelength full-waveform TLS platform was developed by [18], operating at two wavelengths (NIR: 1063 nm; mid-infrared (MIR): 1545 nm). The platform was used to record the full-waveform returned from the forest canopies to measure their three-dimensional structure. The Finnish Geodetic Institute developed a Hyperspectral LiDAR (HSL) system transmitting a continuous spectrum of 400­2500 nm [19]. An outdoor experiment was performed using seven wavelength bands ranging from 500­980 nm in order to discriminate between man-made targets and vegetation based on their spectral response [20]. Douglas et al. [21] designed a portable ground-based full-waveform TLS operating at 1064- and 1548-nm wavelengths. The system was used to collect data in the Sierra Nevada National Forest. Subsequently, and based on that the leaves absorb more strongly at 1548 nm compared to stems, the leaves were discriminated from the woody materials. For multispectral airborne LiDAR attempts, Briese et al. [22] proposed a practical radiometric calibration workflow of multi-wavelength airborne LiDAR data. Their approach was based on full waveform observations (range, amplitude and echo width), flight trajectory and in situ reference targets. The datasets used in this study were acquired by three flight missions based on the same flight plan within three months. Three RIEGL sensors, namely VQ-820-G (532 nm), VQ-580 (1064 nm) and LMS-Q680i (1550 nm), were utilized as one sensor for each mission. Important observations related to this study can be summarized as follows. First, the RIEGL VQ-820-G was mainly designed to survey seabeds, rivers or lakes, where its scan pattern is an arc-like pattern on the ground. As a result, the data collected using this sensor covered a smaller area with a curved boundary, compared to the other two sensors, which produced linear and parallel scan lines. Second, the in situ measurements of reference targets, which were used in the radiometric calibration, were performed using different sensors under specific conditions (i.e., dry condition at zero angle of incidence). Third, the LiDAR data and the in situ measurements of reference targets were collected at different times (in different seasons from August­December). Thus, the surface conditions at the individual flight missions were not identical. Consequently, the calibrated intensity values were affected, such as the calibrated reflectance 1064-nm wavelength showing higher values than other sensors. Briese et al. [23] calibrated multi-wavelength airborne LiDAR data acquired using the aforementioned three RIEGL sensors. The LiDAR data were acquired by two flight missions (both with an aircraft equipped with two sensors) within a short time period (i.e., four days) to ensure more stable reflectance behavior of the study site at all wavelengths. The calibrated intensity data collected at 532 nm were quite dark, and also, the data acquired at 1064 nm was brighter compared to the other wavelengths. In this study, no classification process is reported. In addition, the different viewing angle of the RIEGL VQ-820-G with respect to the other two nadir-looking sensors produced LiDAR data with different boundaries. Generally, the surface conditions at the individual flight mission were not identical due to temporal surface changes, atmospheric conditions and the influence of moisture content [23]. Wang et al. [24] demonstrated the potential use of dual-wavelength full waveform LiDAR data for land cover classification. The LiDAR data were acquired by two laser sensors, Optech ALTM Pegasus HD400 (Teledyne Optech, Vaughan, ON, Canada) and RIEGL LMS-Q680i operating (RIEGL Laser Measurment Systems, Horn, Austria) at 1064 nm and 1550 nm, respectively. A radiometric correction model was first applied to the LiDAR data acquired from both sensors. The LiDAR

Sensors 2017, 17, 958

5 of 21

points were then converted into spectral images with 1-m resolution and combined for subsequent processing. Three features were then derived from Optech and RIEGL sensors' data, namely amplitude (intensity), echo width and surface height. Finally, a supervised classification algorithm, the support vector machine, was used to classify the terrain into six classes, including soil, low vegetation, road and gravel, high vegetation, building roofs and water. Different feature combinations were tested, and overall accuracies of 84.3­97.4% were achieved. The conversion of the 3D LiDAR points into 2D spectral images affected the canopy reflectance information in the spectral images by the objects under the canopy, where the canopy could not be separated from the understory vegetation and soil. This study considered the first return only, extracted from each full waveform, for processing. However, land covers such as trees, building roofs or low vegetation may reflect more than one return. Furthermore, when the RIEGL and Optech amplitude information was tested, the building roofs were not completely separated from soil or low vegetation. One possible reason is that the intensity data came from different missions conducted at different times. Thus, the weather and/or surface conditions change over time, and hence, the same object exhibits different intensity values. As a result, the surface height and echo width were considered the major features for land cover discrimination, while the amplitude information was complementary information [24]. In 2014, Teledyne Optech (Vaughan, ON, Canada) developed the world's first commercial airborne multispectral LiDAR sensor, which is known as the "Optech Titan". The sensor offers the possibility of obtaining multispectral active data acquisition at day and night. This facilitates new applications and information extraction capabilities for LiDAR. The sensor operates simultaneously at three wavelengths and acquires point clouds in three channels with different looking angles, namely MIR (1550 nm) in C1 at 3.5 forward looking, NIR (1064 nm) in C2 at 0 nadir looking and green (532 nm) in C3 at 7 forward looking. Specifications of the Optech Titan sensor are provided in Table 1 [25].
Table 1. Optech Titan sensor specifications. Parameter Wavelength Specification Channel 1 = 1550 nm, Channel 2 = 1064 nm, Channel 3 = 532 nm Topographic: 300­2000 m above ground level (AGL), all channels Bathymetric: 300­600 m AGL, Channel 3 Programmable; 0­60 max Channels 1 and 2 = 0.35 mrad, Channel 3 = 0.7 mrad 50­300 kHz/channel; 900 kHz total Programmable; 0­210 Hz 0­115% of AGL Bathymetric: >15 points/m2 Topographic: >45 points/m2
1

Altitude Scan Angle (FOV) Beam Divergence Pulse Repetition Frequency Scan Frequency Swath Width Point Density 1

Assumes 400 m AGL, 60 m/s aircraft speed, 40 FOV.

Combining multispectral LiDAR data collected at three different wavelengths allows for a higher reliability and accuracy compared to the monochromatic wavelength LiDAR data. A few studies have been conducted on the use of multispectral LiDAR data collected by the Optech Titan for land cover classification. Wichmann et al. [26] studied the spectral patterns of different classes and showed that the intensity values could potentially be used in land cover classification. Raster images were created from the LiDAR intensity and height data, and image classification techniques were then applied [27]. In a previous work conducted by the authors, the maximum likelihood classifier was applied to single intensity image, combined three-intensity images and combined three-intensity images with DSM [28]. The overall accuracy of classifying the terrain into six classes was 65.5% when using

Sensors 2017, 17, 958

6 of 21

the combined three-intensity images, compared to a 17% improvement when using single-intensity images. In addition, the overall classification accuracy was improved to 72.5% when using the combined three-intensity images with DSM. Moreover, we derived three spectral indices from the intensity values recorded in the three channels. The spectral indices were tested in an urban area and achieved an overall accuracy of up to 96% for separating the low and high vegetation from built-up areas [29]. The combined use of LiDAR height and intensity data improved the results, in comparison with those obtained through either of the multispectral imagery alone or LiDAR height data (DSM) with high-resolution multispectral aerial/satellite imagery [30]. Although LiDAR systems acquire 3D dense and accurate point clouds, most of the previous studies converted the 3D point clouds into 2D intensity and/or height images, so that image classification techniques can be applied. However, with such conversion, the data lose the third dimension (i.e., the z component), which leads to incomplete and potentially incorrect classification results. In this research work, we aim to present the capability of using multispectral airborne LiDAR data for land cover classification. The objectives of this study are: (1) explore the use of existing image classification techniques in classifying multispectral LiDAR data; (2) develop a method for merging multi-wavelengths LiDAR data; (3) develop an automated method for land cover classification from 3D multispectral LiDAR points; and (4) assess the effect of radiometric correction of multispectral LiDAR data on the land cover classification results. This paper is organized as follows: the methodology, including image- and point-based classification techniques, is explained in Section 3; Section 4 presents the study area and dataset; the land cover classification results are illustrated in Section 5; Section 6 discusses and analyzes the results, and finally, the conclusion is summarized in Section 7. 3. Methodology Multispectral LiDAR data are used for land cover classification into four classes: buildings, trees, roads and grass. Two independent classification techniques are presented, namely image-based and point-based classification techniques. The workflow of the classification process is shown in Figure 1. The image-based classification technique is based on creating a number of bands from the height and intensity of the LiDAR data. Three intensity images are created from the intensity data collected at the three wavelengths. In addition, a DSM is created from the height data. The three-intensity images are combined together with the DSM. The maximum likelihood classifier is then applied to the combined three-intensity images and the combined three-intensity images with DSM. The point-based classification technique is applied directly to the 3D point clouds. LiDAR points from the three channels are first combined, and the three intensity values are assigned for each single LiDAR point. Subsequently, the ground filtering technique is used to separate non-ground from ground points. Three spectral indices are then computed based on the three intensity values to classify non-ground points into buildings and trees and ground points into roads and grass. These two techniques were applied to raw LiDAR intensity data and radiometrically-corrected LiDAR intensity data. The classification results are validated using an aerial image captured simultaneously with the acquisition of the LiDAR point clouds by the same system. Three accuracy measures are used in the validation, namely overall, producer's and user's accuracies, as well as the Kappa statistic.

Sensors 2017, 17, 958

7 of 21

Sensors 2017, 17, 958

7 of 21

Figure Figure 1. 1. Classification Classification workflow. workflow.

3.1. Technique 3.1. Image-Based Image-Based Classification Classification Technique The workflow of of the the proposed proposed image-based image-based classification classification technique technique starts creating raster raster The workflow starts by by creating images LiDAR point images from from LiDAR point cloud cloud data. data. Three Three intensity intensity images images are are created created from from the the recorded recorded intensity intensity values at the three wavelengths. In addition, the points' elevations are used to create a height height image image values at the three wavelengths. In addition, the points' elevations are used to create a (i.e., DSM). The pixel (cell) size equal to double average point spacing (i.e., 1 m) is selected to ensure (i.e., DSM). The pixel (cell) size equal to double average point spacing (i.e., 1 m) is selected to ensure a number of of points points within within the the cell. cell. The a sufficient sufficient number The mean mean intensity intensity values values or or elevations elevations were were calculated calculated from all points within a pixel and assigned to that pixel. A moving average window 3 is used then from all points within a pixel and assigned to that pixel. A moving average window 3 by 3 3 by is then used fill the voids between in the created [31]. Two band combinations are stacked, to fill to the voids between pixels pixels in the created imagesimages [31]. Two band combinations are stacked, namely namely intensity images from the three wavelengths (combined intensity bands) and intensity intensity images from the three wavelengths (combined intensity bands) and intensity images with images with DSM (combined intensity bands with DSM). After that, areas for four classes DSM (combined intensity bands with DSM). After that, training areas fortraining four classes are selected based are selected based on an aerial image of the study area to produce a spectral signature for each class. on an aerial image of the study area to produce a spectral signature for each class. Some classes are Some classes are composed of separately-sampled classes to account for the variety in the spectral composed of separately-sampled classes to account for the variety in the spectral attribute. For instance, attribute. instance, buildings class is comprised of samples fromas various roof colors, such as buildings For class is comprised of samples from various roof colors, such white roofs, grey roofs and white roofs, grey roofs samples and red from roofs.high Furthermore, from high vegetation with different red roofs. Furthermore, vegetationsamples with different greenness values are included greenness values are included in the trees class. Finally, a supervised classification, the maximum in the trees class. Finally, a supervised classification, the maximum likelihood classifier, is applied likelihood classifier, is applied The to the two band combinations. The maximum classifier to the two band combinations. maximum likelihood classifier accounts for likelihood the probability that accounts for the probability that a pixel/point belongs to a particular class and considers the a pixel/point belongs to a particular class and considers the variability of classes. variability of classes. 3.2. Point-Based Classification Technique 3.2. Point-Based Classification Technique The point-based classification technique is divided into four phases and applied directly to the 3D The point-based classification technique is divided four phases and applied directly to the point clouds. First, the collected point clouds in differentinto channels are combined, and three intensity 3D point clouds. First, the collected point clouds in different channels are combined, and three intensity values for each single point are estimated. Second, non-ground points are separated from

Sensors 2017, 17, 958

8 of 21

Sensors 2017, 17, 958

8 of 21

values for each single point are estimated. Second, non-ground points are separated from ground points ground points based on the elevation using ground filtering technique. Third,are NDVI values based on the elevation attribute usingattribute ground filtering technique. Third, NDVI values computed are computed for non-ground and ground points from intensity values recorded at different for non-ground and ground points from intensity values recorded at different channels. Fourth, channels. Fourth, breaks the Jenks natural breaks optimization is used to define threshold values the Jenks natural optimization method is used to method define threshold values and subsequently and subsequently to points clusterinto the different LiDAR points into different classes. details this technique to cluster the LiDAR classes. More details of this More technique areof explained in the are explained in the following sub-sections. following sub-sections. 3.2.1. Multi-Wavelength Multi-Wavelength LiDAR LiDAR Points Points Merging Merging As the new new multispectral multispectralsensor sensor acquires LiDAR data different wavelengths, point clouds acquires LiDAR data at at different wavelengths, point clouds are are collected same coverage area, but with differentintensity intensityvalues values relevant relevant to to different collected for for the the same coverage area, but with different wavelength. However, However,merging merging those point clouds predicting the intensity values forsingle each wavelength. those point clouds andand predicting the intensity values for each singleat point at all wavelengths make the available data dense more and dense and reliable. Although Optech point all wavelengths make the available data more reliable. Although Optech Titan Titan operates simultaneously at the three wavelengths, it acquires LiDAR in the three operates simultaneously at the three wavelengths, it acquires LiDAR points inpoints the three channels channels atangles. different angles. Consequently, collected from the in same object in different at different Consequently, collected points frompoints the same object different channels may channels may not coincide completely at the location. 3D spatial join technique could not coincide completely at the same location. Asame 3D spatial join A technique could provide a possible provide a possible solution merging points where from all channels, intensity value of a point solution for merging pointsfor from all channels, an intensitywhere value an of a point from one channel from one channel is assigned to the nearest point [26]. from However, another channel [26]. However, this is assigned to the nearest point from another channel this technique might lead to technique might lead to incorrect matching points, as shown in Figure 2 and explained incorrect matching between points, as shown inbetween Figure 2 and explained through the following scenarios. through the following scenarios. Case (1) indicates the perfect point from Channels C2 and Case (1) indicates the perfect point matching from Channels C2 andmatching C3. In Case (2), a point from C2 C3. In be Case (2), a point could be matched twiceC3, with points from C3, as this could matched twicefrom withC2 two different points from as two this different point is the nearest neighbor to pointpoints. is the nearest to possible both points. Case (3) points showsfrom two C2, possible neighboring points from both Case (3)neighbor shows two neighboring which have the same distance C2, haveC3. the Case same(4) distance to a point C3. Case (4) indicates that neighboring points to a which point from indicates that no from neighboring points from C2 to ano point from C3 within from C2 of to predefined a point from C3 within a sphere of predefined thebe intensity values of a sphere radius. Therefore, the intensity valuesradius. of eachTherefore, point cannot used the same as each point cannot the same as the intensity value of the nearest point. the intensity valuebe of used the nearest point.

C3. Figure 2. 3D spatial join between points from C2 and C3.

In [29], presented toto combine the LiDAR data from the three channels. The In [29], another anothermethod methodwas was presented combine the LiDAR data from the three channels. LiDAR data from each channel were divided into grids with a cell size of 1 m. The mean intensity The LiDAR data from each channel were divided into grids with a cell size of 1 m. The mean intensity value of ofall allpoints points within cell was assigned to the cell's center. Three spectral indices' grids value within aa cell was assigned to the cell's center. Three spectral indices' grids werewere then then calculated using the mean intensity values of the grid cells. The three spectral indices' values calculated using the mean intensity values of the grid cells. The three spectral indices' values were were interpolated then interpolated toLiDAR each LiDAR bilinear interpolation from the spectral indices' then to each point point using using bilinear interpolation from the spectral indices' grids grids based on the point's location, where the adjacent cell centers in the calculation. based on the point's location, where the adjacent cell centers were were used used in the calculation. AfterAfter that, that, the LiDAR points with the three spectral indices were for used for land/water discrimination and the LiDAR points with the three spectral indices were used land/water discrimination and land land cover classification [29]. process This process is summarized in Figure 3. cover classification [29]. This is summarized in Figure 3.

Sensors 2017, 17, 958 Sensors 2017, 17, 958

9 of 21 9 of 21

Figure 3. Spectral index calculation from 3D points.

This method is is acceptable acceptable when when land/water land/water discrimination to This method discrimination is is required, required, but but it it leads leads to misclassification terrain into different land covers. This is primarily due due to two misclassification when when classifying classifyingthe the terrain into different land covers. This is primarily to reasons. First, the mean intensity values of the points within a grid cell were used. Those two reasons. First, the mean intensity values of the points within a grid cell were used. Those points points could same land land cover cover or not, and and hence, hence, the the cell cell could could represent represent more more than than one one land land could belong belong to to the the same or not, cover. Second, bilinear interpolation was to used to the obtain thevalues spectral values 3D points. cover. Second, bilinear interpolation was used obtain spectral of 3D points.of Consequently, Consequently, the spectral values of points that have multiple returns were incorrectly assigned the the spectral values of points that have multiple returns were incorrectly assigned the same value, same value, such asbranches points from and bare soil underneath a tree the were assigned the same such as points from and branches bare soil underneath a tree were assigned same spectral values. spectral values. Therefore, in order to correctly predict the intensity value of a point, a median value Therefore, in order to correctly predict the intensity value of a point, a median value is calculated is calculated from its surrounding points from another The point merging of this research from its surrounding points from another channel. Thechannel. point merging of this research work can be work can be described as follows. described as follows. Let pi,, p j and ph represent points in C1, C2 and C3, respectively; where i = 1, 2, 3, ..., nC1; j = 1, 2, 3, Let p i pj and ph represent points in C1, C2 and C3, respectively; where i = 1, 2, 3, . . . , nC1 ; j = 1, 2, ..., n C2, h = 1, 2, 3, ..., nC3, and nC1, nC2 and nC3 are the total number of LiDAR points collected in C1, C2 3, . . . , nC 2 , h = 1, 2, 3, . . . , nC3 , and nC1 , nC2 and nC3 are the total number of LiDAR points collected in and C3, respectively. The LiDAR points in each channel areare first organized using a aK-d C1, C2 and C3, respectively. The LiDAR points in each channel first organized using K-dtree tree data data structure in order to efficiently apply a multidimensional range search. The neighboring points C2 structure in order to efficiently apply a multidimensional range search. The neighboring points Np i ofppifrom from C2 and C3, respectively, are obtained within a sphere of predefined search and C3 of and Np C2 and C3, respectively, are obtained within a sphere of predefined search radius i i radius (r) as follows: ( r) as follows:
C2 Np = i

=

xj, yj, zj : , , :

x j - xi + y j - yi + z j - zi  r - + - + - 

2

2

2

(1) (1) (2) (2)

C3 Np = = ( xh , ,yh , , zh ) :: i

- x i )2 - y i )2 + (zh - z i )2 ( xh - ++ (yh - + -  r

The r used asas 1m to fulfill two conditions; the first is to have sufficient number of points, r value valueis is used 1m to fulfill two conditions; the first is to a have a sufficient number of C2 C3 and theand second is not to contain any points different features. The Npi The and Npi and points are then points points, the second is not to contain anyfrom points from different features. C2 and I C3 of p arranged in ascending order according their intensity values. The values. intensity values Ip are then arranged in ascending order to according to their intensity The intensity values i pi i from C2 and are calculated, respectively, and of pi C3 from C2 and C3 are calculated,as: respectively, as:  C2 + 1 +1  Np  i    2 2      2      
C2 Np i 2 th

th

, , value 2
C2 Np i 2 +1 th

C2 Ip i =

C2 is an odd number i f Np i

=

value+

+

+1

value

(3) (3) ,
C2 is an even number i f Np i ,  C3 is an odd number i f Np i

2
C3 2 C3 Ip = Np i  i  =  2   2
C3 +1 Np i + 1 2 th

2

th

value, ,
value+

+ 2

C3 Np i 2 +1

th

value

22

+1

(4) (4) ,
C3 is an even number i f Np i , 

Sensors 2017, 17, 958

10 of 21

The median intensity value is used to avoid any intensity data noise. In case no neighboring points are found, the intensity value is assigned a zero value. Equations (1)­(4) are applied at any C1 and N C3 , as well as the intensity values I C1 and point pj in C2 to obtain the neighboring points Np pj pj j
C3 from C1 and C3, respectively. The same procedures are applied for any point p in C3, where the Ip h j C1 and N C2 , as well as the intensity values I C1 and I C2 are obtained from C1 and neighboring points Np ph ph ph h C2, respectively. The LiDAR points are combined, and the duplicated points (nd ) are removed using a MATLAB function "unique", whereas the unique xyz LiDAR points are detected and considered for the classification process; so that the total number of points (N) = nC1 + nC2 + nC3 - nd , and each LiDAR point has six attributes: x, y, z, IC1 , IC2 and IC3 .

3.2.2. Ground Filtering The ground filtering aims to separate non-ground points from ground points through the decision rules shown in Figure 4. A statistical analysis algorithm, skewness balancing, was applied to the elevation of points as a first step for ground filtering. The naturally measured data lead to a normal distribution [32]. Thus, the ground points collected within the LiDAR data are assumed to follow the normal distribution, while the other non-ground points (object) may disturb the distribution [33,34]. By removing those non-ground points from the LiDAR data, the ground points are obtained. The higher order moments (e.g., skewness) can characterize the distribution of LiDAR points. The skewness (Sk) is defined by: N 1 (5) · Sk = ( Zi - µ)3  N · S 3 i =1 where N is the total number of the LiDAR points, Zi is the elevation and i  {1, 2, . . . , N}, S and µ are the standard deviation and the arithmetic mean of elevation, defined by Equations (6) and (7) respectively: S=
N 1 ·  ( Zi - µ)2 N - 1 i =1

(6)

µ=

1 · N

i =1

 Zi

N

(7)

The elevations of the point clouds are first sorted in ascending order. The skewness is then calculated using Equation (5) from all points. If the skewness is greater than zero, the point with the highest elevation is removed and classified as a non-ground point. The remaining points are used to calculate the skewness, and the process is repeated until the skewness of the point clouds is balanced (Sk = 0). After the skewness balancing is performed, the remaining points are classified as potential ground points and assumed to be within a specific slope. As such, the output separation is refined based on the measurement of the slope changes of each LiDAR point with respect to its neighboring points. A threshold value (S_thrd) is applied to label the points with a higher slope as non-ground points. Moreover, the remaining ground points are divided into grids to filter out the points with higher elevation. For each grid, a minimum elevation is calculated (Z_min), and a threshold value to elevation (E_thrd) is applied.

Sensors 2017, 17, 958
Sensors 2017, 17, 958

11 of 21
11 of 21

Figure Ground filtering filtering workflow. Figure 4.4.Ground workflow.

3.2.3. NDVIs Computation

3.2.3. NDVIs Computation NDVI values are computed similarly as defined by [35] from the intensity data as follows: NIR - MIR NIR + MIR NIR - MIR NDVINIR-MIR = NIR - G = NIR + MIR NDVI NIR + G NIR- - MIR GG NDVI NDVI = NIR-G = NIR+ + MIR GG NDVI =

NDVI values are computed similarly as defined by [35] from the intensity data as follows:
(8) (9) (10)

(8) (9)

MIR G MIR, NIR and green wavelengths, where MIR, NIR and G are the recorded intensity at - the (10) NDVIMIR-G = respectively. The NDVIs values are between -1 and 1.MIR However, + G if a point has zero intensity value in two channels, the NDVI will be not a number. In this case, the point is labeled as an where MIR, NIR and G are the recorded intensity at the MIR, NIR and green wavelengths, respectively. unclassified point. The NDVIs values are between -1 and 1. However, if a point has zero intensity value in two channels, the NDVI not a number. In this case, the point is labeled as an unclassified point. 3.2.4. will Databe Clustering
The Jenks natural breaks optimization method is used to determine threshold values 3.2.4. Data Clustering (NDVI_thrd) in order to cluster LiDAR points based on NDVI values [36]. This optimization method

The Jenksdesigned natural breaks optimization method is used determine valuesvariance. (NDVI_thrd) has been to minimize within-class variances andto maximize thethreshold between-classes in order cluster pointsfrom based NDVI values This methodvalue has been Let to the NDVI LiDAR values range [a,on ..., b], where -1 [36].  a < b  optimization 1 and the threshold (NDVI_thrd )  [a, ... , b]. The (NDVI_thrd ) isand identified to cluster the non-ground points into buildings designed to minimize within-class variances maximize the between-classes variance. Let the NDVI and trees and [a ground into roads and by maximizing the between-classes sum of values range from , . . . , bpoints ], where -1 a< b grass 1 and the threshold value (NDVI_thrd)  [a, . . . , b]. squared differences as follows: The (NDVI_thrd) is identified to cluster the non-ground points into buildings and trees and ground points into roads and grass by maximizing the between-classes sum of squared differences as follows: NDV I _thrd = arg max ( M1 - M )2 + ( M2 - M )2
atb

(11)

Sensors 2017, 17, 958

12 of 21

where M is mean of NDVI values, M1 and M2 are the mean values of first and second class, respectively. The (M) is first calculated. Then, the points are divided into two classes with ranges [a, . . . , NDVI_thrd] and [NDVI_thrd, . . . , b]. The mean values M1 and M2 are calculated. Finally, the optimal threshold value (NDVI_thrd) is obtained from Equation (11). 3.3. Radiometric Correction Radiometric correction aims to remove the attenuation due to system- and environmentally-induced distortion. The relationship between the received laser power (Pr ) with respect to various system and environmental parameters is described by the radar equation [37]: Pr =
2 Pt Dr sys atm  4 4 R  t

(12) (13)

 = 4cos

where Pt is the transmitted laser pulse energy, Dr is the aperture diameter, R is the range, t is the laser beam width,  sys is the system factor and  atm is the atmospheric attenuation factor. The laser cross-section  consists of the projected target area A, the laser scan angle  and the spectral reflectance of the illuminated surface . In this research, a radiometric correction model based on the radar range equation is used to remove the system-dependent distortion by converting the (Pr ) into the () using R, A and  , considering that other parameters are constant. The angle  could be used as the incidence angle, which is defined as the angle between the incidence laser beam and the surface normal of any object [11], or a combination between the incidence angle and the scan angle controlled by the surface slope [13]. Further details on the radiometric correction model can be found in [11,13]. 4. Study Area and Dataset The study area is located in Oshawa, Ontario, Canada. The Optech Titan multispectral LiDAR sensor was used to acquire LiDAR point for a single strip during a flight mission on 3 September 2014. Optech Titan acquired LiDAR points in three channels at 1075 m altitude, ±20 scan angle, 200 kHz/channel Pulse Repetition Frequency and 40-Hz scan frequency. The mean point density for each channel is 3.6/m2 , with a point spacing of about 0.5 m. The acquired data consist of trajectory position data, as well as a time-tagged 3D point cloud with multiple returns (up to a maximum of 4 returns) in LASer file format (LAS) for each channel. The LAS data file contains xyz coordinates, raw intensity values, the scan angle and the GPS time of each LiDAR point. A subset from the LiDAR strip was clipped with a dimension of 550 m by 380 m for testing. The study area covers a variety of land cover features on the ground such as buildings, roads, parking lots, shrubs, trees and open spaces with grass covered. The tested subset has 712,171, 763,507 and 595,387 points from C1, C2 and C3, respectively. The variation in the number of points recorded at different channels depends on the interaction of land objects with different wavelengths (e.g., greenness of the vegetation). An aerial image, captured simultaneously with the acquisition of the LiDAR data, was geo-referenced with the LiDAR data and was used to validate the land cover classification results, as shown in Figure 5.

Sensors 2017, 17, 958
Sensors 2017, 17, 958

13 of 21
13 of 21

Figure 5. Ortho-rectified aerial image of the study area.

Figure 5. Ortho-rectified aerial image of the study area.

Since the 3D reference points are not available, a set of polygons was selected to extract the reference for each class (i.e., buildings, trees, roads grass). All was points within a polygon Since thepoints 3D reference points are not available, a setand of polygons selected to extract the drawn on classes, including roads, grass or buildings, were labeled as the same class, while the top reference points for each class (i.e., buildings, trees, roads and grass). All points within a polygon layer of the trees class was used as reference points. The total number of points for the four classes drawn on classes, including roads, grass or buildings, were labeled as the same class, while the top 45,618, distributed as shown in Table 2. layerwas of the trees class was used as reference points. The total number of points for the four classes was 45,618, distributed as shown in Table 2. Table 2. Reference points for the four classes. Class Buildings Trees Roads Grass Total Table 2. Reference points for the four classes. Number of Points 12,253 17,740 4566 11,059 45,618 Class Buildings Trees Roads Grass Total 5. Land Cover Classification Results Number of Points 12,253 17,740 4566 11,059 45,618 The study area was classified into four land covers: buildings, trees (includes trees and shrub), roads (asphalt surface, parking lots, and bare soil) and grass (includes green/dry grass and wetland). 5. Land Cover Classification Results The accuracy assessment was conducted using a number of reference points within predefined The study area was classified intodigitized four land covers: buildings, trees (includes trees and shrub), polygons. These polygons were first around the center of the objects to avoid confusion, which might be created by mixed when an image-based classification technique is used. Then, roads (asphalt surface, parking lots,pixels and bare soil) and grass (includes green/dry grass and wetland). all points within those polygons were labeled from the geo-referenced aerial image. The confusion The accuracy assessment was conducted using a number of reference points within predefined matrixThese was then created were and the accuracy measures (overall, producer's and user's accuracies), as polygons. polygons first digitized around the center of the objects to avoid confusion, well as the Kappa statistic were calculated.

which might be created by mixed pixels when an image-based classification technique is used. Then, all points within those polygons were labeled from the geo-referenced aerial image. The confusion 5.1. Image-Based Classification Results matrix was then created and the accuracy measures (overall, producer's and user's accuracies), as well LiDAR points were used to create three raster images from the intensity values (i.e., C1, C2 and as the Kappa statistic were calculated.
C3 from Channels 1, 2 and 3, respectively) and a raster image from the height data (i.e., DSM), with a spatial resolution of 1 m, Results as shown in Figure 6. The three raster intensity images were stacked 5.1. Image-Based Classification together to compose Combined Intensity Bands (CIBs) and with the DSM raster image (CIBs_DSM). LiDAR points were used tocomposite create three raster images the intensity values (i.e., C1, C2 and C3 Figure 7a shows a false color of the CIBs, whichfrom is visualized as C1 in red, C2 in green and C3 in blue; and Figure 7b shows false color composite of the CIBs_DSM, which is visualized as DSM from Channels 1, 2 and 3, respectively) and a raster image from the height data (i.e., DSM), with a spatial in red, of C31 in green and C2 in in blue. The likelihood classifierimages was applied tostacked these two band to resolution m, as shown Figure 6.maximum The three raster intensity were together combinations after identifying training signatures for different classes. Figure 7c,d shows the 7a compose Combined Intensity Bands (CIBs) and with the DSM raster image (CIBs_DSM). Figure classified images from combined intensity bands without/with the DSM. The confusion matrix, the shows a false color composite of the CIBs, which is visualized as C1 in red, C2 in green and C3 in blue; overall accuracy and the overall kappa statistics for the two cases are provided in Tables 3 and 4. The and Figure 7b shows false color composite of the CIBs_DSM, which is visualized as DSM in red, C3 in overall accuracy and kappa statistic are 77.3% and 0.675 from the CIBs and 89.9% and 0.855 from green and C2 in blue. The maximum likelihood classifier was applied to these two band combinations the CIBs_DSM.

after identifying training signatures for different classes. Figure 7c,d shows the classified images from combined intensity bands without/with the DSM. The confusion matrix, the overall accuracy and the overall kappa statistics for the two cases are provided in Tables 3 and 4. The overall accuracy and kappa statistic are 77.3% and 0.675 from the CIBs and 89.9% and 0.855 from the CIBs_DSM.

Sensors 2017, 17, 958
Sensors 2017, 17, 958 Sensors 2017, 17, 958

14 of 21
14 of 21 14 of 21

(a) (a)

(b) (b)

(c) (d) (c) (d) Figure 6. LiDAR raster images: (a) C1 intensity; (b) C2 intensity; (c) C3 intensity; (d) DSM. Figure 6. LiDAR raster images: (a) C1 intensity; (b) C2 intensity; (c) C3 intensity; (d) DSM. Figure 6. LiDAR raster images: (a) C1 intensity; (b) C2 intensity; (c) C3 intensity; (d) DSM.

(a) (a)

(b) (b)

(c) classified image from CIBs; (d) classified image from CIBs_DSM.

Figure 7. Combined and classified images: (a) Combined Intensity Bands (CIBs); (b) CIBs_DSM; Figure 7. Combined and classified images: (a) Combined Intensity Bands (CIBs); (b) CIBs_DSM; ( c) 7. classified imageand from CIBs; (d) classified from CIBs_DSM. Figure Combined classified images: image (a) Combined Intensity Bands (CIBs); (b) CIBs_DSM; (c) classified image from CIBs; (d) classified image from CIBs_DSM.

(c) (c)

(d) (d)

Sensors 2017, 17, 958

15 of 21

Table 3. Confusion matrix for CIBs.
Reference Data Buildings 7910 4153 157 33 12,253 64.56 Trees 359 15,346 1432 603 17,740 86.51 Roads 341 857 3319 49 4566 72.69 Grass 96 1895 370 8698 11,059 78.65 User's Accuracy (%) 90.86 68.97 62.88 92.70

Classification Data Buildings Trees Roads Grass Total column Producer's Accuracy (%)

Total Row 8706 22,251 5278 9383 45,618

Overall accuracy: 77.32%; overall Kappa statistic: 0.675.

Table 4. Confusion matrix for CIBs_DSM.
Reference Data Buildings 11,550 583 78 42 12,253 94.26 Trees 637 16,969 125 9 17,740 95.65 Roads 254 336 3926 50 4566 85.98 Grass 110 2236 154 8559 11,059 77.39 User's Accuracy (%) 92.02 84.32 91.66 98.83

Classification Data Buildings Trees Roads Grass Total column Producer's Accuracy (%)

Total Row 12,551 20,124 4283 8660 45,618

Overall accuracy: 89.89%; overall Kappa statistic: 0.855.

5.2. Point-Based Classification Results This technique involved ground filtering and NDVIs computation, which were applied on elevation and intensity attributes, respectively. Subsequently, the points were clustered into different classes. The ground filtering started with skewness balancing in order to separate ground points from non-ground points. The ground points were then refined based on the slope-based change mechanism. The slope of each point with respect to surrounding points was investigated; so that if the slope were greater than a threshold value (S_thrd = 10 ), the point was classified as a non-ground point. In addition, a few points with higher elevations were not classified as non-ground points. Therefore, the output ground points were divided into grids with a cell size of 25 m. For each grid, if the elevation was greater than 3 m (E_thrd = 3 m) above the minimum elevation, the point was classified as a non-ground point. The ground points include roads and grass classes, while the non-ground points include buildings and trees classes. The NDVIs were subsequently computed using Equations (8)­(10). Table 5 shows the threshold values obtained by applying the Jenks break optimization method to the NDVIs for both non-ground and ground points in order to separate buildings from trees and roads from grass, respectively. The vegetation (i.e., trees or grass) has high reflectance at the NIR, MIR and green wavelengths. As a result, the calculated NDVIs of the vegetation points exhibited higher values than the built-up areas (i.e., buildings or roads). Therefore, for a particular point, if NDVINIR-MIR , NDVINIR-G or NDVIMIR-G  NDVI_thrd, the cover type belongs to the buildings or roads class; otherwise, it belongs to the trees or grass class. Figure 8 shows the 3D classified point clouds based on the three NDVIs. The confusion matrix, overall accuracy and kappa statistics for the three cases are provided in Tables 6­8.
Table 5. Threshold values (NDVI_thrd). Non-Ground Points NDVINIR-MIR NDVINIR-G NDVIMIR-G Ground Points

-0.026 0.314 0.373

-0.035 0.288 0.354

Sensors 2017, 17, 958

16 of 21

Sensors 2017, 17, 958

16 of 21

(a)

(b)

(c)
Figure 8. LiDAR points based on:on: (a) NDVI NIR-MIR; (b) NDVINIR-G; (c) NDVIMIR-G; (left: (2D Figure 8. Classified Classified LiDAR points based (a) NDVI NIR-MIR ; (b) NDVINIR-G ; (c) NDVIMIR-G ; view); right: (3D view)). (left: (2D view); right: (3D view)). Table 6. Confusion matrix for point classification based on NDVINIR-MIR. Table 6. Confusion matrix for point classification based on NDVINIR-MIR .

Reference Data Total Row User's Accuracy (%) Reference Data Buildings Trees Roads Grass User's Accuracy (%) Classification Data Total Row Buildings Trees Roads Grass Unclassified 1 25 104 76 206 Unclassified 1 25 104 76 206 Buildings 11,013 6283 120 67 17,483 63.0 Buildings 11,013 6283 120 67 17,483 63.0 Trees 1212 11,432 19 155 12,818 89.2 19 155 12,818 89.2 Trees 1212 11,432 Roads 44 0 4175 1878 6057 68.9 Roads 0 4175 1878 6057 68.9 Grass 23 0 148 8883 9054 98.1 Grass 23 0 148 8883 9054 98.1 Total column 12,253 17,740 4566 11,059 45,618 Producer's 89.9 64.4 91.4 80.3 TotalAccuracy. column(%) 12,253 17,740 4566 11,059 45,618 77.8%; overall statistic: 0.695. Producer's Accuracy. (%) Overall 89.9accuracy:64.4 91.4Kappa80.3 Classification Data
Overall accuracy: 77.8%; overall Kappa statistic: 0.695.

Sensors 2017, 17, 958

17 of 21

Table 7. Confusion matrix for point classification based on NDVINIR-G .
Classification Data Unclassified Buildings Trees Roads Grass Total column Producer's Accuracy (%) Reference Data Buildings 8 11,212 1009 1 23 12,253 91.5 Trees 285 734 16,721 0 0 17,740 94.3 Roads 74 124 21 4200 147 4566 92.0 Grass 44 14 174 670 10,157 11,059 91.8 Total Row 411 12,084 17,925 4871 10,327 45,618 User's Accuracy (%)

92.8 93.3 86.2 98.4

Overall accuracy: 92.7%; overall Kappa statistic: 0.897.

Table 8. Confusion matrix for point classification based on NDVIMIR-G .
Classification Data Unclassified Buildings Trees Roads Grass Total column Producer's Accuracy (%) Reference Data Buildings 19 9722 2502 1 9 12,253 79.3 Trees 447 1016 16,277 0 0 17,740 91.8 Roads 22 118 82 4027 317 4566 88.2 Grass 59 35 160 680 10,125 11,059 91.6 Total Row 547 10,891 19,021 4708 10,451 45,618 User's Accuracy (%)

89.3 85.6 85.5 96.9

Overall accuracy: 88.0%; overall Kappa statistic: 0.831.

There are two sources of errors; the unclassified class and the ground filtering. During the intensity prediction and within the searching radius of any point, if no neighboring points were found, the intensity values were set to zero. As a result, the point's NDVI was not a number and labeled as an unclassified point. The classification errors due to the ground filtering are highlighted by gray color in Tables 6­8, whereas the ground points (roads or grass) were misclassified as non-ground points (buildings or trees) or vice versa. 5.3. Radiometric Correction Effect on the Classification Accuracy The LiDAR intensity data were corrected for the system attenuation (i.e., the range and scan angle). Using Equations (12) and (13), the  was estimated for each single LiDAR point. The corrected intensity images were then created from the three channels, and the classification process was repeated using the image-based classification technique. The radiometric correction improved the overall accuracy by about 1.7% when using the LiDAR intensity data and by only 0.6% when the DSM was considered. In contrast, the classification accuracy of the buildings class was significantly improved, reaching about 10.8%, and the trees class improved by 3.1%. The same procedures of the point-based classification technique were applied to the radiometrically-corrected LiDAR points; however, no significant improvements in the overall accuracy were recorded. This might be because of the fact that the LiDAR data were collected at a narrow scan angle. 6. Discussion Generally, the classification results have demonstrated the capability of using multispectral LiDAR data for classifying the terrain into four classes, namely buildings, trees, roads and grass. The image-based and point-based classification techniques achieved overall classification accuracies of up to 89.9% and 92.7%, respectively. Previous studies achieved overall classification accuracies from 85­89.5% for the same four land cover classes. They used multispectral aerial/satellite imagery combined with nDSM derived from LiDAR data [5,6] or combined with LiDAR height and intensity data [7­9], while the presented work in this research used the LiDAR data only. Furthermore, the availability of the multispectral LiDAR data eliminates the need for multispectral aerial/satellite

Sensors 2017, 17, 958 Sensors 2017, 17, 958

18 of 21 18 of 21

availability of the multispectral LiDAR data eliminates the need for multispectral aerial/satellite imagery imagery for for classification classification purposes. purposes. Figure Figure 9 9 summarizes summarizes the the achieved achieved overall overall accuracy accuracy from from the the two two classification classification techniques. techniques.

Figure Figure 9. 9. Overall Overallaccuracy accuracyfrom from the the two two classification classification techniques. techniques.

In particular, the the results demonstrated importance of combining LiDAR intensity and In particular, results demonstrated thethe importance of combining LiDAR intensity and height height data, where the overall accuracy increased by more than 12% after the DSM was incorporated data, where the overall accuracy increased by more than 12% after the DSM was incorporated with the with the intensity images. This is clearly notable where some pixels (marked by red intensity images. This is clearly notable in Figure in 7c,Figure where 7c, some pixels (marked by red rectangles) rectangles) in area the study were misclassified as trees (i.e., high vegetation). Furthermore, in the study were area misclassified as trees (i.e., high vegetation). Furthermore, many many pixels pixels that belong to the road surface were misclassified as buildings, while those pixels were that belong to the road surface were misclassified as buildings, while those pixels were correctly correctly classified as grass (i.e., low vegetation) or roads, after DSM was incorporated as shown in classified as grass (i.e., low vegetation) or roads, after DSM was incorporated as shown in Figure 7d Figure 7d (marked by black Significant rectangles). Significant improvements of the and user's (marked by black rectangles). improvements of the producer's andproducer's user's accuracies were accuracies were achieved for the individual land covers when DSM was considered. For instant, an achieved for the individual land covers when DSM was considered. For instant, an improvement was improvement was achieved accuracy for the producer's accuracy of roads buildings, trees and roads classes by achieved for the producer's of buildings, trees and classes by 29.7%, 9.1% and 13.3%, 29.7%, 9.1% and 13.3%, respectively. Furthermore, the user's accuracy of trees and roads classes was respectively. Furthermore, the user's accuracy of trees and roads classes was improved by 15.4% and improved by 15.4% and 28.8%, respectively. 28.8%, respectively. The The point-based point-based classification classification technique technique produced produced various various overall overall accuracies accuracies for for land land cover cover classification. Overall accuracies of 77.8%, 92.7% and 88.0% were achieved when using NDVI NIR-MIR, classification. Overall accuracies of 77.8%, 92.7% and 88.0% were achieved when using NDVINIR-MIR , NDVI NIR-G and NDVIMIR-G, respectively. As previously mentioned, the unclassified points and the NDVINIR-G and NDVIMIR-G , respectively. As previously mentioned, the unclassified points and the ground ground filtering filtering are are two two sources sources of of classification classification errors. errors. The The error error of of the the unclassified unclassified points points ranges ranges from 0.0­2.3%, while the ground filtering errors range from 0.0­4.4%. from 0.0­2.3%, while the ground filtering errors range from 0.0­4.4%. With With the the focus focus on on the the individual individual classes, classes, about about 35.6% 35.6% of of the the tree tree points points were were omitted omitted (64.4% (64.4% producer those points points were were wrongly wrongly classified classifiedas asbuildings buildingswhen whenNDVI NDVINIR-MIR was producer accuracy), accuracy), and and those NIR-MIR was used. used. This Thisomission omissioncaused causedaa misclassification misclassificationof of the the buildings buildings class class with with about about 37% 37% (63% (63% user user accuracy). This is primarily due to the moisture content of the vegetation, where dry vegetation accuracy). This is primarily due to the moisture content of the vegetation, where dry vegetation exhibits in C1 C1 [38]. [38]. As As a a result, result, the the NDVI NDVINIR-MIR yielded low values, and hence, exhibits high high intensity intensity values values in NIR-MIR yielded low values, and hence, the tree points were misclassified as buildings. Similarly, about 19.7% of thepoints grass were points were the tree points were misclassified as buildings. Similarly, about 19.7% of the grass omitted omitted (80.3% producer accuracy) andclassified mainly classified roads. Furthermore, about 31.1% and (80.3% producer accuracy) and mainly as roads.as Furthermore, about 31.1% and 14.5% of 14.5% of roads points were misclassified as grass when NDVI NIR-MIR and NDVIMIR-G, respectively, roads points were misclassified as grass when NDVINIR-MIR and NDVIMIR-G , respectively, were used. were In used. addition, about 20.7% (79.3% producer accuracy) of the buildings' points were wrongly In addition, about 20.7% (79.3% producer accuracy) of the buildings' points were wrongly classified as trees when NDVIMIR-G was used. This is mainly because some roof materials exhibit classified as trees when NDVIMIR-G was used. This is mainly because some roof materials exhibit high high intensity values in C1, and hence, the NDVIMIR-G increased, leading to the classification errors. intensity values in C1, and hence, the NDVIMIR-G increased, leading to the classification errors. As a As a result, this omission caused a misclassification of the tree class for about 14.4% (85.6% user accuracy). result, this omission caused a misclassification of the tree class for about 14.4% (85.6% user Although previous attempts showed that radiometric correction and normalization can lead accuracy). to the improvement of intensity homogeneity [11­13], such phenomena cannot be observed in this Although previous attempts showed that radiometric correction and normalization can lead to the improvement of intensity homogeneity [11­13], such phenomena cannot be observed in this

Sensors 2017, 17, 958

19 of 21

research. This might be attributed to the fact that the multispectral LiDAR data, used in this research, were collected with a narrow scan angle, whereas the LiDAR data of the aforementioned studies were collected with a wide scan angle that caused obvious energy loss especially close to the edge of the scan line. In addition, the target-related parameters, including the range to the target, the target size, the laser beam incident angle and the illumination of the target material should not necessarily have similar influence on the LiDAR intensity data to the previous studies. Another reason is that the environmental parameters related to the collected data, such as aerosol and Rayleigh scattering, aerosol absorption and atmospheric attenuation, are not changed over the study area, and hence, they do not have the same effect on the LiDAR intensity data as the previous studies. Furthermore, the three channels of the Optech Titan sensor are well controlled by its in-house transfer function, where the recorded signal strength is linear and stable. As such, there is no significant loss of energy due to the signal transmission function. Consequently, radiometric correction may not always be required under similar conditions. 7. Conclusions This research discussed the use of multispectral LiDAR data in land cover classification of an urban area. The multispectral data were collected by the Optech Titan sensor operating at three wavelengths of 1550 nm, 1064 nm and 532 nm. Two classification techniques were used to classify the multispectral LiDAR data into buildings, trees, roads and grass. The first technique is image-based classification, where the LiDAR intensity and height data were converted into images. Two band combinations were stacked including combined three-intensity images and combined three-intensity images with DSM. The maximum likelihood classifier was then applied to the two band combinations. This technique achieved an overall classification accuracy of about 77.32% from the LiDAR intensity data only. The classification accuracy was improved to 89.89% when DSM was incorporated with the LiDAR intensity data. The second technique is point-based classification, where the 3D LiDAR points in the three channels were combined and three intensity values were assigned to each single LiDAR point as a pre-processing step. Ground filtering, using skewness balancing and slope-based change, was then applied to separate the LiDAR data into ground and non-ground points. Subsequently, the NDVIs were computed and threshold values were estimated using Jenks break optimization method to cluster the ground points into roads and grass, and the non-ground points into buildings and trees. This technique achieved overall accuracies of 77.83%, 92.70% and 88.02% when NDVINIR-MIR , NDVINIR-G and NDVIMIR-G were used, respectively. A physical model based on the radar range equation was used for radiometric correction of the intensity data. The correction considered the system parameters and the topographic effect. It has been noticed that there is no significant effect on the land covers' classification results after applying the radiometric correction on this particular dataset. The presented work demonstrates the advantage of using multi-dimensional LiDAR data (intensity and height) over a single dimensional LiDAR data (intensity or height).
Acknowledgments: This research work is supported by the Discovery Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) (RGPIN-2015-03960) and the Ontario Trillium Scholarship (OTS). The authors also would like to thank Paul LaRocque and Teledyne Optech for providing the LiDAR data from the world's first multispectral LiDAR system, the Optech Titan. Author Contributions: Salem Morsy, Ahmed Shaker and Ahmed El-Rabbany conceived of the research idea and carried out the design. Salem Morsy implemented the experiment and produced and analyzed the results under the supervision of Ahmed Shaker and Ahmed El-Rabbany. Salem Morsy drafted the manuscript. Ahmed Shaker and Ahmed El-Rabbany reviewed the manuscript. Conflicts of Interest: The authors declare no conflict of interest.

Sensors 2017, 17, 958

20 of 21

References
1. 2. 3. 4. 5. 6. 7. Yan, W.Y.; Shaker, A.; El-Ashmawy, N. Urban land cover classification using airborne LiDAR data: A review. Remote Sens. Environ. 2015, 158, 295­310. [CrossRef] Brennan, R.; Webster, T.L. Object-oriented land cover classification of lidar-derived surfaces. Can. J. Remote Sens. 2006, 32, 162­172. [CrossRef] Im, J.; Jensen, J.R.; Hodgson, M.E. Object-based land cover classification using high-posting-density LiDAR data. GISci. Remote Sens. 2008, 45, 209­228. [CrossRef] Antonarakis, A.S.; Richards, K.S.; Brasington, J. Object-based land cover classification using airborne LiDAR. Remote Sens. Environ. 2008, 112, 2988­2998. [CrossRef] Huang, M.-J.; Shyue, S.-W.; Lee, L.-H.; Kao, C.-C. A knowledge-based approach to urban feature classification using aerial imagery with Lidar data. Photogramm. Eng. Remote Sens. 2008, 74, 1473­1485. [CrossRef] Chen, Y.; Su, W.; Li, J.; Sun, Z. Hierarchical object oriented classification using very high resolution imagery and LIDAR data over urban areas. Adv. Space Res. 2009, 43, 1101­1110. [CrossRef] Charaniya, A.P.; Manduchi, R.; Lodha, S.K. Supervised parametric classification of aerial LiDAR data. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, Washington, DC, USA, 27 June­2 July 2004. Hartfield, K.A.; Landau, K.I.; van Leeuwen, W.J.D. Fusion of high resolution aerial Multispectral and LiDAR data: Land cover in the context of urban mosquito habitat. Remote Sens. 2011, 3, 2364­2383. [CrossRef] Singh, K.K.; Vogler, J.B.; Shoemaker, D.A.; Meentemeyer, R.K. LiDAR-Landsat data fusion for large-area assessment of urban land cover: Balancing spatial resolution, data volume and mapping accuracy. ISPRS J. Photogramm. Remote Sens. 2012, 74, 110­121. [CrossRef] Höfle, B.; Pfeifer, N. Correction of laser scanning intensity data: Data and model-driven approaches. ISPRS J. Photogramm. Remote Sens. 2007, 62, 415­433. [CrossRef] Yan, W.Y.; Shaker, A.; Habib, A.; Kersting, A.P. Improving classification accuracy of airborne LiDAR intensity data by geometric calibration and radiometric correction. ISPRS J. Photogramm. Remote Sens. 2012, 67, 35­44. [CrossRef] Habib, A.F.; Kersting, A.P.; Shaker, A.; Yan, W.-Y. Geometric calibration and radiometric correction of LiDAR data and their impact on the quality of derived products. Sensors 2011, 11, 9069­9097. [CrossRef] [PubMed] Yan, W.Y.; Shaker, A. Radiometric correction and normalization of airborne LiDAR intensity data for improving land-cover classification. IEEE Trans. Geosci. Remote Sens. 2014, 52, 7658­7673. Woodhouse, I.H.; Nichol, C.; Sinclair, P.; Jack, J.; Morsdorf, F.; Malthus, T.J.; Patenaude, G. A Multispectral Canopy LiDAR Demonstrator Project. IEEE Geosci. Remote Sens. 2011, 8, 839­843. [CrossRef] Wei, G.; Shalei, S.; Bo, Z.; Shuo, S.; Faquan, L.; Xuewu, C. Multi-wavelength canopy LiDAR for remote sensing of vegetation: Design and system performance. ISPRS J. Photogramm. Remote Sens. 2012, 69, 1­9. [CrossRef] Shi, S.; Song, S.; Gong, W.; Du, L.; Zhu, B.; Huang, X. Improving Backscatter Intensity Calibration for Multispectral LiDAR. IEEE Geosci. Remote Sens. 2015, 12, 1421­1425. [CrossRef] Wallace, A.; Nichol, C.; Woodhouse, I. Recovery of Forest Canopy Parameters by Inversion of Multispectral LiDAR Data. Remote Sens. 2012, 4, 509­531. [CrossRef] Danson, F.M.; Gaulton, R.; Armitage, R.P.; Disney, M.; Gunawan, O.; Lewis, P.; Pearson, G.; Ramirez, A.F. Developing a dual-wavelength full-waveform terrestrial laser scanner to characterize forest canopy structure. Agric. For. Meteorol. 2014, 198­199, 7­14. [CrossRef] Hakala, T.; Suomalainen, J.; Kaasalainen, S.; Chen, Y. Full waveform hyperspectral LiDAR for terrestrial laser scanning. Opt. Express 2012, 20, 7119. [CrossRef] [PubMed] Puttonen, E.; Hakala, T.; Nevalainen, O.; Kaasalainen, S.; Krooks, A.; Karjalainen, M.; Anttila, K. Artificial target detection with a hyperspectral LiDAR over 26-h measurement. Opt. Eng. 2015, 54, 013105. [CrossRef] Douglas, E.S.; Martel, J.; Li, Z.; Howe, G.; Hewawasam, K.; Marshall, R.A.; Schaaf, C.L.; Cook, T.A.; Newnham, G.J.; Strahler, A.; et al. Finding leaves in the forest: The dual-wavelength Echidna Lidar. IEEE Geosci. Remote Sens. 2015, 12, 776­780. [CrossRef] Briese, C.; Pfennigbauer, M.; Lehner, H.; Ullrich, A.; Wagner, W.; Pfeifer, N. Radiometric calibration of multi-wavelength airborne laser scanning data. In Proceedings of the XXII ISPRS Congress, Melbourne, Australia, 25 August­1 September 2012.

8. 9.

10. 11.

12. 13. 14. 15.

16. 17. 18.

19. 20. 21.

22.

Sensors 2017, 17, 958

21 of 21

23.

24. 25.

26.

27. 28.

29.

30. 31. 32. 33.

34. 35.

36. 37. 38.

Briese, C.; Pfennigbauer, M.; Ullrich, A.; Doneus, M. Multi-wavelength airborne laser scanning for archaeological prospection. In Proceedings of the XXIV International CIPA Symposium, Strasbourg, France, 2­6 September 2013. Wang, C.-K.; Tseng, Y.-H.; Chu, H.-J. Airborne dual-wavelength LiDAR data for classifying land cover. Remote Sens. 2014, 6, 700­715. [CrossRef] Teledyne Optech--Titan Brochure and Specifications, 2015. Optech Titan Multispectral LiDAR System--High Precision Environmental Mapping. Available online: http://www.teledyneoptech.com/wp-content/ uploads/Titan-Specsheet-150515-WEB.pdf (accessed on 30 August 2016). Wichmann, V.; Bremer, M.; Lindenberger, J.; Rutzinger, M.; Georges, C.; Petrini-Monteferri, F. Evaluating the potential of multispectral airborne LiDAR for topographic mapping and land cover classification. In Proceedings of the ISPRS Geospatial Week, La Grande Motte, France, 28 September­3 October 2015. Bakula, K.; Kupidura, P.; Jelowicki, L. Testing of land cover classification from multispectral airborne laser scanning data. In Proceedings of the XXIII ISPRS Congress, Prague, Czech Republic, 12­19 July 2016. Morsy, S.; Shaker, A.; El-Rabbany, A. Potential use of multispectral airborne LiDAR data in land cover classification. In Proceedings of the 37th Asian conference on Remote Sensing, Colombo, Sri Lanka, 17­21 October 2016. Morsy, S.; Shaker, A.; El-Rabbany, A.; LaRocque, P.E. Airborne multispectral LiDAR data for land-cover classification and land/water mapping using different spectral indexes. In Proceedings of the XXIII ISPRS Congress, Prague, Czech Republic, 12­19 July 2016. Zhou, W. An object-based approach for urban land cover classification: Integrating LiDAR height and intensity data. IEEE Geosci. Remote Sens. 2013, 10, 928­931. [CrossRef] Reuter, H.I.; Nelson, A.; Jarvis, A. An evaluation of void-filling interpolation methods for SRTM data. Int. J. Geogr. Inf. Sci. 2007, 21, 983­1008. [CrossRef] Duda, R.O.; Hart, P.E.; Stork, D.G. Pattern Classification, 2nd ed.; Wiley: New York, NY, USA, 2001; p. 55. Bartels, M.; Wei, H.; Mason, D.C. DTM generation from LIDAR data using skewness balancing. In Proceedings of the 18th IEEE International Conference on Pattern Recognition (ICPR), Hong Kong, China, 20­24 August 2006. Yan, W.Y.; Morsy, S.; Shaker, A.; Tulloch, M. Automatic extraction of highway light poles and towers from mobile LiDAR data. Opt. Laser Technol. 2016, 77, 162­168. [CrossRef] Rouse, J., Jr.; Haas, R.H.; Schell, J.A.; Deering, D.W. Monitoring vegetation systems in the Great Plains with ERTS. In Third Earth Resources Technology Satellite-1 Symposium NASA SP-351; Freden, S.C., Mercanti, E.P., Becker, M.A., Eds.; NASA: Washington, DC, USA, 1974; Volume I, p. 309. Jenks, G.F. The data model concept in statistical mapping. In International Yearbook of Cartography; Frenzel, K., Ed.; Kirschbaum Verlag: Bonn, Germany, 1967; Volume 7, pp. 186­190. Jelalian, A.V. Laser radar systems. In Proceedings of the Electronics and Aerospace Systems Conference, Arlington, VA, USA, 29 September­1 October 1980. Gao, B.C. NDWI--A normalized difference water index for remote sensing of vegetation liquid water from space. Remote Sens. Environ. 1996, 58, 257­266. [CrossRef] © 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).


