AUTOMATIC DETECTION AND CLASSIFICATION OF DIABETIC RETINOPATHY FROM RETINAL FUNDUS IMAGES

by

Abdullah Biran, Bachelor of Science in the program of Electrical and Computer Engineering (Biomedical Engineering) Jeddah, Saudi Arabia, 2012
A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Applied Science in in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2017

Â©Abdullah Biran 2017

AUTHOR'S DECLARATION
I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my thesis may be made electronically available to the public.

Automatic Detection and Classification of Diabetic Retinopathy from Retinal Fundus Images by Abdullah Biran, Master of Applied Science, Electrical and Computer Engineering Department, Ryerson University, 2017.

Abstract
Automatic Detection and Classification of Diabetic Retinopathy from Retinal Fundus Images by Abdullah Biran, Master of Applied Science, lectrical and computer engineering Department, Ryerson University, 2017.

Diabetic Retinopathy (DR) is an eye disease that leads to blindness when it progresses to proliferative level. The earliest signs of DR are the appearance of red and yellow lesions on the retina called hemorrhages and exudates. Early diagnosis of DR prevents from blindness. In this thesis, an automatic algorithm for detecting diabetic retinopathy is presented. The algorithm is based on combination of several image processing techniques including Circular Hough Transform (CHT), Contrast Limited Adaptive Histogram Equalization (CLAHE), Gabor filter and thresholding. In addition, Support Vector Machine (SVM) classifier is used to classify retinal images into normal or abnormal cases of DR including non-proliferative (NPDR) or proliferative diabetic retinopathy (PDR). The proposed method has been tested on fundus images from Standard Diabetic Retinopathy Database (DIARETDB). The implementation of the presented methodology was done in MATLAB. The methodology is tested for sensitivity and accuracy.

Keywords: Diabetic retinopathy, Proliferative and non-Proliferative Diabetic retinopathy, Fundus images, DIARETDB, Fundus photography, Blood vessels extraction, Wavelet transform, Gabor filter, Thresholding, CLAHE, CHT, SVM.

iii

Acknowledgements
The thesis became a reality with the kind support and help of many individuals. I would like to extend my sincere thanks to all of them. Foremost, I would like to thank GOD for the wisdom he bestowed upon me, and the strength, peace of mind and good health in order to finish this research. I would like to express my special gratitude and thanks to SACB Saudi Arabian Cultural Bureau in Canada for its financial help and support. Without it, I would never have been able to pursue my higher education in the field of Electrical and Computer Engineering. I also would like to thank my supervisor, Dr. Kaamran Raahemifar, for his great continuous help and support in this work of Automatic Detection of Diabetic Retinopathy. I am very thankful for my supervisor as he was always motiving me. My thanks and appreciation also go to my English language teacher, Jennifer Harris who helped me to improve my writing skills.

iv

Dedication
I deeply dedicate this work to my father and my family for their love and support which gave me the strength to finish the research. I would like also to dedicate this thesis to my uncle, Bander, and my best friend, Tammem for encouraging me to make this thesis a reality.

v

Table of Contents
Author's Declaration................................................................................................ii Abstract..............................................................................................................iii Acknowledgments................................................................................................iv Dedication............................................................................................................v List of Figures.......................................................................................................ix List of Tables .....................................................................................................x List of Abbreviations..............................................................................................xii

Contents
Chapter 1 Introduction ........................................................................................................................... 1 1.1 Background .................................................................................................................................. 1 1.1.1 The Retina ............................................................................................................................. 1 1.1.2 Fundus Photography ............................................................................................................. 1 1.1.3 Diabetic Retinopathy............................................................................................................. 2 1.2 Literature Review......................................................................................................................... 4 1.2.1 Hemorrhages segmentation ................................................................................................... 4 1.2.2 Exudates Detection ............................................................................................................... 6 1.2.3 Blood Vessels Detection and Segmentation ......................................................................... 7 1.2.4 Optic Disc (OD) Detection and Filtering .............................................................................. 9 1.2.5 Diabetic Retinopathy Classification.................................................................................... 10 1.3 Challenges and Facts.................................................................................................................. 11 1.4 Conclusion ................................................................................................................................. 13 Chapter 2 System Overview ................................................................................................................ 15 2.1 General Block Diagram ............................................................................................................. 15 2.2 DR Detection Methodology ....................................................................................................... 16 2.3 Contributions.............................................................................................................................. 17 2.3.1 Retinal Objects Detection ................................................................................................... 18 2.3.2 A Tool for Automatic Segmentation of DR Lesions by Physicians ................................... 19 vi

2.3.3 A Testing Tool for Automatic Classification of Retinal Images for Researchers. .............. 21 2.4 Conclusion .................................................................................................................................. 23 Chapter 3 An Approach for Automatic Detection of Diabetic Retinopathy ........................................ 24 3.1 Preprocessing.............................................................................................................................. 24 3.1.1 Background Removing ........................................................................................................ 25 3.1.2 OD Localization and Removing .......................................................................................... 27 3.2 Image Enhancement ................................................................................................................... 30 3.2.1 Alternative Sequential Filter (ASF) ..................................................................................... 30 3.2.2 Contrast Limited Adaptive Histogram Equalization (CLAHE) .......................................... 31 3.3 Objects Extraction ...................................................................................................................... 34 3.3.1 Continues Wavelet Transform (CWT) ................................................................................ 34 3.3.2 The Two Dimensional Gabor Filter..................................................................................... 35 3.3.3 Combined Gabor Filter (CGF) ............................................................................................ 37 3.4 Lesions Segmentation................................................................................................................. 39 3.4.1 Thresholding ........................................................................................................................ 39 3.5 Classification .............................................................................................................................. 42 3.5.1 Feature Extraction ............................................................................................................... 42 3.5.2 Support Vector Machine...................................................................................................... 44 3.5.3 Performance Measurement .................................................................................................. 46 3.6 Conclusion .................................................................................................................................. 47 Chapter 4 Results.................................................................................................................................. 48 4.1 Processing Experiment using the Proposed CGF ....................................................................... 49 4.2 Segmentation Experiment .......................................................................................................... 54 4.3 Classification Experiment .......................................................................................................... 60 4.4 More details on the results .......................................................................................................... 62 4.4.1 Signal to Noise Ratio (SNR) ............................................................................................... 62 4.4.2 Fourier Transform of the Results......................................................................................... 63 4.4.3 Effect of Train/Test data percentage in Classifier Performance .......................................... 69 4.4.4 Choice of different kernels in Classifier Performance ........................................................ 71 4.5 Conclusion .................................................................................................................................. 72 Chapter 5 Conclusions.......................................................................................................................... 73 5.1 Discussion .................................................................................................................................. 73 vii

5.2 Future Work ............................................................................................................................... 74 5.3 Summary .................................................................................................................................... 74 5.4 References .................................................................................................................................. 76

viii

List of Tables
Table 3.1 Range of Gabor Filter Values for Retinal Images..................................................37 Table 3.2 Used Values.............................................................................................39 Table 3.3 Selected Images for Features Extraction............................................................43 Table 3.4 Features Descriptions.................................................................................44 Table 3.5 SVM classes............................................................................................46 Table 4.1 Selected Images.......................................................................................48 Table 4.2 Results of SVM test...................................................................................61 Table 4.3 Image Classification Results.........................................................................62 Table 4.4 The average SNR value of Normal / Abnormal Fundus Images in dB (decibles)............62 Table 4.5 The effect of training/testing data percentage..................................................... 69 Table 4.6 Table 4.6 Kernel effect on the performance of SVM classifier.................................71

Table 5.1 Sensitivity of other research.........................................................................73 Table 5.2 Accuracy of other research...........................................................................73

ix

List of Figures
Figure  1.1. Retinal Fundus Image with DR Lesions .............................................................................. 3 Figure  1.2. Normal Retinal Fundus Images. .......................................................................................... 4 Figure  2.1. General block diagram for DR detection ........................................................................... 16 Figure  2.2. The flowchart of the objects detection algorithm .............................................................. 19 Figure  2.3. The flowchart for DR lesions segmentation algorithm...................................................... 21 Figure  2.4. The flowchart of Classification Process ............................................................................ 22 Figure  3.1. CHT algorithm theory ....................................................................................................... 27 Figure  3.2.The process of OD removing.............................................................................................. 29 Figure  3.3. Histogram enhancement ................................................................................................... 33 Figure 3.4. The process of image enhancement using CLAHE. .......................................................... 33 Figure  3.5. CWT .................................................................................................................................. 35 Figure  3.6. Gabor wavelet generated at 18 different orientations ........................................................ 38 Figure  3.7. An example of using different thresholds to segment exudates.. ...................................... 41 Figure  3.8. An example of classification using SVM. T ...................................................................... 46 Figure  4.1. Resulted images using CGF on a normal retinal image from DIARETDB. ...................... 50 Figure  4.2. Resulted images using CGF on an abnormal retinal image from DIARETDB. ................ 51 Figure  4.3. Applying CGF on an abnormal retinal image from STARE ............................................. 52 Figure  4.4. Applying CGF on a normal retinal image from STARE ................................................... 52 Figure  4.5. Using CGF technique on a normal image from DRIVE. ................................................... 53 Figure  4.6. The process of segmenting exudates using NPDR image from DIARETDB. .................. 55 Figure  4.7. The process of segmenting exudates using NPDR image from STARE. .......................... 56 Figure  4.8. The process of segmenting exudates using PDR image from DIARETDB. ..................... 57 Figure  4.9. The process of segmenting bleeding regions using NPDR image from DIARETDB. ...... 58 Figure  4.10. The process of segmenting exudates using PDR image from DRIVE ............................ 59 Figure  4.11. A randomly selected normal fundus image ..................................................................... 65 Figure  4.12. The Fourier Transform of the normal fundus image ....................................................... 66 Figure  4.13. A randomly selected abnormal fundus Image ................................................................. 67 x

Figure  4.14. The Fourier Transform of the abnormal fundus image .................................................... 68 Figure  4.15. Changes in accuracy with changes in the training set. ..................................................... 70 Figure  4.16. Changes in sensitivity with changes in the training set. ................................................... 70 Figure 4.17. Changes in the accuracy using different kernels at three training sets. ............................ 71 Figure  4.18. The change in sensitivity using different kernels at three training sets. .......................... 72

xi

List of Abbreviations
OCT SNR DIARETDB STARE DRIVE DR MA NPDR PDR SVM OD CHT AHE CLAHE ASF HT PDF CDF Optical Coherent Tomography Signal to Noise Ratio Standard Diabetic Retinopathy Database Structured Analysis of the Retina Digital Retinal Images for Vessel Extraction Diabetic Retinopathy Microaneurysms Non-Proliferative Diabetic Retinopathy Proliferative Diabetic Retinopathy Support Vector Machine Optic Disc Circular Hough Transform Adaptive Histogram Equalization Contrast Limited Adaptive Histogram Equalization Alternative Sequential Filter Hough Transform Probability Distribution Function Cumulative Density Function

xii

Chapter 1 Introduction
1.1 Background The application of image processing and computer vision techniques in different fields of science and engineering is rapidly growing. In the medical scope, the existing developments of such techniques mainly aim at early diagnosis of medical disorders by reducing the time for disease detection to prevent the progress of the disease. Successful developments have been made in image processing, such as the automated diagnostic systems. These systems are based on a verity of algorithms that diagnose diseases invasively in a short period of time. For example, different automatic systems have been proposed to detect Diabetic Retinopathy (DR) from retinal fundus images. The following is brief background information about the retina, fundus imaging and diabetic retinopathy. 1.1.1 The Retina The retina is a multilayered sheet that consists of neurons, photoreceptors, and support cells. It is a metabolically active organ; therefore, it is very sensitive to nutrients and gases exchanges, diffusion. When rays of light enter the eye focusing on the retina, the retina reacts by sending them to the brain through the optic nerve. Then, the brain interprets them as a picture. The retina receives its blood supply from two sources: choriocapillaris and branches of the central retinal artery. The choriocapillaris supplies the outer one third of the retina, while the branches of the central retinal artery supply the inner two thirds of the retina [1]. 1.1.2 Fundus Photography There are different medical imaging techniques for capturing the internal structures of human eye including fundus photography and Optical Coherent Tomography (OCT). These two methods are non-invasive. OCT provides cross sectional images of ocular tissues. However, fundus photography 1

provides pictures of the internal structures of human eye including the retina, the optic disc, macula, fovea and blood vessels [2]. The OCT is helpful when the eye diseases are diagnosed at tissue level. On the other hand, fundus images are very efficient when the eye diseases are diagnosed at retinal level. In addition, fundus photography is the faster and easier technique. It observes larger field of retina, and it is widely used in diagnosis of eye-related diseases. Fundus images are either indexed or RGB (Red, Green, and Blue) images of different intensities [2]. These images can be found on a variety of publically available retinal image databases including Standard Diabetic Retinopathy Database (DIARETDB), Structured Analysis of the Retina (STARE) and Digital Retinal Images for Vessel Extraction (DRIVE). These databases provide both normal and abnormal retinal images. Although STARE database has 400 images of different eye diseases, most of the DR related images have high signal to noise ratio (SNR). On the other hand, DRIVE images are not considered because they are limited to 40 images. Thus, DIARETDB is selected to be the main database in this research as it has over 200 images with low signal to noise ratio. 1.1.3 Diabetic Retinopathy Diabetic Retinopathy (DR) is the influence of diabetes on the eye. It is the number one eye disease that causes blindness. Patients with diabetes are more likely to develop such disease as they grow older. Early indications of DR are the appearance of microaneurysms, hemorrhages and exudates. DR starts when diabetes mellitus damages small retinal blood vessels causing microaneurysms (MA), the small swellings that form on the side of tiny blood vessels. As the damage increases, it causes hemorrhages deep inside the retina. Retinal hemorrhage can cause severe vision problems. As the disease advances, the retina reacts by growing abnormally weak, leaky, fragile and misdirected blood vessels. The more these abnormal blood vessels grow, the more they bleed and pull 2

on the retina. In addition, the leaked fluid produces sediments composed of lipid byproducts called exudates. Exudates are yellow and appear in different sizes and locations within the retina [3]. There are two main types of DR: Non-Proliferative (NPDR) and Proliferative (PDR). When an abnormal blood vessel grows in the eye, the disease is proliferative, Neovascularization. When there is no abnormal blood vessel growth, the disease is non-proliferative and falls into categories of mild, moderate and severe [1]. Fig 1.1 shows a retinal fundus image with DR. Fig 1.2 Shows normal retinal images. There are no early symptoms for Diabetic Retinopathy as patients do not suffer from vision problems until the late stages of the disease when treatments could be inefficient. Therefore, an early diagnosis of DR is vital, and preventing the disease from developing is desirable. Additionally, it is very important that diabetic patients undergo an annual eye exam [1]. The human eye can detect DR from retinal fundus images with the help of manual blood vessels segmentation and lesion extraction. However, this is a very time-consuming process that requires training and could be affected by the observer's fatigue. Therefore, researchers have presented different solutions to detect DR automatically.

Figure  1.1. Retinal Fundus Image with DR Lesions

3

From DIARETDB

Authors Eye

Figure  1.2. Normal Retinal Fundus Images, the author's fundus image was taken during vision test in February 2016 at the Toronto health one clinic. 1.2 Literature Review In the last ten years, several image processing techniques for detecting DR lesions and classifying them into PDR or NPDR of different types have been proposed. These methods are based on more than one algorithm because DR has two different indications. In addition, the blood vessels and the optic disc need to be filtered from the image. After the detection process is complete, most researchers have classified DR using various algorithms. 1.2.1 Hemorrhages segmentation Retinal Hemorrhage is one of the major abnormalities in DR. Therefore, segmentation of hemorrhages has been the main objective of many previous research papers. For instance, Kleawsirikul et al. [4] presented an automated algorithm for detecting hemorrhages using morphological top hat transform, which is a filter to binarize the image by finding the difference between the image, closing of the image and opening of the image. White pixels represented blood vessels and hemorrhages, and black pixels represented the image background. Then, features of color, area, eccentricity and 4

compactness were extracted, and hemorrhages were classified using rule based classification. The hemorrhage classification has achieved accuracy of 99.12%. However, the sensitivity of this work is low, only 80.37%. Also, this work has the disadvantage of considering only 20 images. In [5], a different methodology for hemorrhages segmentation was proposed. The technique was based on removing the blood vessels from the image. Morphological opening with multi scale structuring elements were used to detect the blood vessels. Then, blood vessels were removed from the image. Thus, only hemorrhages were segmented. Such methodology has achieved sensitivity of 87.69%. Also, it has the advantages of segmenting MAs which are the early indication of DR. However, this study does not consider the accuracy test of the methodology. Accuracy is an important factor to test the algorithm performance. Athira and Ferlin [6] used splat future technique to segment large hemorrhages. The technique is based on dividing the image to segments called splat. The splat contains pixels of the same color and spatial location. A wide range of features were extracted from each splat. Then, the mean filter was used to select the best splat feature. In fact, this work has the advantage of segmenting large hemorrhages that overlap with blood vessels. However, Athira and Ferlin did not include image preprocessing techniques, sensitivity calculation or accuracy measurement. Inbarathi et al. have also used splat feature technique to detect DR [7]. In addition, the Support Vector Machine (SVM) classifier was used to segment hemorrhages. Features of contrast, correlation, energy and homogeneity were considered. Although the sensitivity and accuracy were not indicated in this study, the study achieved good results in terms of hemorrhages segmentation. However, the number of selected features was short. Reshma and Chavan [8] developed an automatic method to detect hemorrhages using template matching technique and region growing. The template matching was applied to provide information about the centers of the objects. The region growing technique was used to segment the 5

proper size of the hemorrhages. Reshma et al. methodology classifies NPDR fundus images into two classes of moderate or severe based on the number of detected hemorrhages. The sensitivity of classifying abnormal images was 90%. The main drawback of such method is that since it does not detect mild NPDR, it cannot be used for early detection of DR. Jadhav et al. [9] presented a methodology to remove blood vessels from retinal images using morphological closing technique with disc structuring element [9]. Images from DIARETDB were considered. Jadhav's work achieved 90% sensitivity. However, their work does not distinguish between NPDR and PDR. 1.2.2 Exudates Detection Retinal Exudates is another major abnormality in DR. Therefore, segmentation of bright objects in color fundus images has been the main objective of much research. For example, Reza et al. [10] presented an algorithm to segment exudates using an image processing technique based on average filtering, contrast adjustment, morphological opening, and watershed transformation. Their methodology for exudate segmentation achieved high sensitivity of 96.7%. In contrast, Tripathi et al. [11] proposed a new method for automatic segmentation of exudates based on differential morphological profile (DMP). The first stage in their work was Gaussian smoothing on the green channel of the input, followed by contrast enhancement. The DMP was used to detect bright regions which are mainly exudates. The exudates segmentation test achieved sensitivity of 97.63%. However, neither of the aforementioned two studies considered the accuracy of their methodologies [10, 11]. Also, Tripathi's work was done on STARE images that require more image enhancement techniques than contrast adjustment. Mansour et al. [12] presented a three-stage algorithm based on: Discrete Cosine Transform for feature extraction including color information, color histogram thresholding for exudate segmentation and SVM for DR classification. Akter and colleagues [13] proposed an improved method to segment exudates using nonlinear background elimination. The methodology used by Akter et al. depends on 6

converting grayscale image to binary image of bright objects using local thresholding and median filter. The optic disc (OD) was removed using erosion and dilation. Although Mansour et al. achieved 97% sensitivity, and Akter et al. achieved results with 93.55% sensitivity and 99.96% accuracy, it is not efficient to detect DR by only segmenting exudates. However, early segmentation of exudates can be used to prevent blindness. In [14], the authors have developed an automatic method to segment exudates based on their high gray level variation. The OD was removed from the image using K-means clustering technique. Features of area, intensity, entropy, smoothness and regions of exudates were extracted and used by the SVM to find the severity level of DR. Such work has the advantage to diagnose mild, moderate and severe NPDR. The classification test has achieved 94.17% accuracy. However, the sensitivity test was not recorded. On the Other Hand, Kuar and Mittal [15] developed an algorithm to segment exudates based on modified dynamic region growing method. Their work achieved 97.9% sensitivity for segmenting exudates. However, the study by Kuar and Mittal does not indicate the difference between NPDR and PDR, and lacks the accuracy record. 1.2.3 Blood Vessels Detection and Segmentation Blood vessels and hemorrhages are similar in color; however, they differ in size and dimensions. Therefore, many approaches have been proposed for automated segmentation of retinal blood vessels. For instance, Selvathi et al. [16] proposed a method for retinal blood vessel segmentation using Gabor wavelet transform. SVM and Relevance Vector Machine (RVM) classifiers were used to classify each pixel to vessel or non-vessel based on the pixel's feature vector. The drawback of such work is the long computational time for blood vessel segmentation where the best achievement was at 252 seconds.

7

Hou [17] presented a multi scale line detection methodology for segmenting blood vessels. A basic line detector produces vessel responses using morphological attributes of retinal blood vessels. Each pixel was identified as vessel or non-vessel. The author improved a multi scale line detector to produce vessel response images at varying scales. The line detector with more vessel responses was considered as the segmented vessel image. Such work achieved 93.36% accuracy. However, the sensitivity is low at 73.48%. On the other hand, Ricci et al. [18] proposed an algorithm which is also based on line detectors and SVM. They achieved better accuracy, 96.46%, than what was reported by Hou. In contrast, Pradeepa and Raja [19] applied SVM based method to segment blood vessels. Gabor filter and thresholding techniques were applied to extract different features. Pradeepa's work achieved 90% accuracy. However, sensitivity measurement was not recorded in [18] and [19]. An improved algorithm based on multi-structure elements has been proposed by Shajahan and Roy [20]. Multi-structure elements were used to detect vessel edges. The false detected edges were removed using thresholding. Each pixel was identified as vessel on non-vessel with the comparison to ground truth image. The main advantage of this work is the short computational time of 15 seconds. However, their methodology was tested on DRIVE images which include a small number of abnormal images with few hemorrhages. On the other hand, Raja et al. [21] presented a methodology which was also based on using multi-structure elements to detect blood vessels. SVM was used to test the performance of the methodology. Features of contrast, energy, homogeneity and correlation were considered. The study by Raja and colleagues achieved 95.1% accuracy, however, the sensitivity was low at 78%. Kaur and Talwar [22] presented a methodology to segment blood vessels using Fuzzy neural networks. Each pixel was classified as vessel or non-vessel. Such methodology achieved 87.4%

8

sensitivity. However, accuracy measurement was not evaluated. The main advantage of their work is that their methodology was tested on images of different eye diseases. Segmentation and filtering of the retinal vascular system help in better segmentation of hemorrhages. However, the shorter the computational time, the faster the detection of DR. In addition, it is important to find both accuracy and sensitivity measurements. 1.2.4 Optic Disc (OD) Detection and Filtering The Optic Disc (OD) and exudates appear yellow in retinal fundus images. Therefore, many algorithms have localized and removed OD in order to better detect exudates. Dehghani et al. [23] proposed an OD removal algorithm based on histogram matching. The average of histograms of each color was used as template to localize the center of the OD. The correlation and thresholding techniques were used to segment the OD. Such work has perfectly localized the OD. However, it only can segment 91.36% of the OD pixels. Foracchia et al. [24] also proposed a new method for OD localization based on detecting the main vessels as they originate from the OD in a similar directional pattern. Then, they proposed geometrical parametric models to describe any retinal vessel direction, and the OD was detected using two model parameters. However, this methodology is limited as it only localizes the OD. Sekhar et al. [25] presented a methodology to localize the OD using Circular Hough Transform (CHT). The OD was perfectly detected as the brightest yellowish circle. CHT has shown excellent results for localization of OD. In [26], a different methodology to locate the OD was proposed by using implicit active counters technique. The results of OD segmentation were accurate; however, it requires more computation than CHT. Wisaeng et al. developed a method to detect the OD in low quality retinal images that have non uniform illumination and poor contrast [27]. Their methodology was based on a combination of mathematical morphology operations and Otsu's algorithms. Wisaeng's et al. work achieved 91.35% 9

accuracy using images from STARE database. In addition, it has the advantages of segmenting the OD in blurred images. 1.2.5 Diabetic Retinopathy Classification Several algorithms have been proposed to classify retinal fundus images of DR type. For instance, Aravind et al. [28] developed a method to detect Microaneurysms which are the first clinical sign of DR using morphological operations. Features of area, entropy, correlation, energy, contrast, homogeneity and mean were extracted. Then, Support Vector Machine (SVM) was applied to classify each image to normal retina, mild or severe NPDR. The sensitivity and accuracy of the study by Aravind et al. were 92% and 90%, respectively. Such work has the advantage of classifying NPDR images to classes of mild or severe. However, cannot detect moderate NPDR. On the other hand, SujithKumar et al. [29] presented an automatic algorithm for detecting MAs using Adaptive Histogram Equalization (AHE) and thresholding. The algorithm classifies each input image to normal retina or NPDR. The NPDR images were classified to classes of mild, moderate or severe based on the number of detected MAs. Unlike Aravind et al., SujithKumar et al. were able to distinguish between moderate and severe NPDR. The accuracy of such work was 94.44%; however, sensitivity analysis was not considered. Tjandrasa et al. [30] focused on classifying NPDR to classes of moderate or severe. The exudates were segmented using K-Means clustering. Three different classifiers were used. The multilayer perception, which is one of the neural networks classifiers, has shown the best classification result at 91.07% accuracy. Moreover, Sundhar and Archana [31] proposed automatic screening to detect DR. Their developed method perfectly detected the size of exudates and MAs. The artificial neural networks were used to classify the fundus images to normal, NPDR, and PDR. The classification test achieved 92.5%

10

sensitivity. Although Sundhar and Archana were able to detect PDR, the drawback of their study was that they used only a small number of images, 40 images to be exact, in the classification test. Jagatheesh and Jenila [32] developed a method to detect DR lesions using the Bag of Visual Words (BOVW) model. The model transforms local image descriptors into image representations which are considered in the classification. These models aim to detect a large number of feature vectors around the points of interest in the image. The detected features are then assigned to visual words based on a visual dictionary. Jagatheesh and Jenila's work achieved 84.21% accuracy and 76.62% sensitivity for exudates detection. For hemorrhages detection, it achieved 75.2% accuracy and 74.57% sensitivity. Detection and classification of DR based on BOVW is still under development. Welikala et al. [33] proposed a methodology to detect and classify PDR using dual classification approach. The blood vessel was segmented in two images using the standard line operator and a modified line operator. Then, 21 features were extracted from each image and were used in classification stage. The final decision was based on combining the results of the two classifiers by removing the false new vessel responses in each classifier. PDR is a very advanced level of DR and the current treatments may not be effective for curing PDR. However, the findings of the study by Welikala and colleagues help ophthalmologists in finding new treatments.

1.3 Challenges and Facts
Research on early detection of diabetic retinopathy was inspired by the desire to save patient's vision. It also was inspired by the need to help doctors to make better decisions during the diagnosis process. However, several factors affect the results of detection. For example, hemorrhages and exudates are two different signs of DR which must be detected separately. Additionally, hemorrhages and exudates are similar in color to the retinal blood vessels and the OD, respectively. Thus, an efficient algorithm should be able to distinguish between the disease indicators and the normal features 11

of fundus images. For instance, blood vessels are different in shape than hemorrhages, and the OD is bigger than exudates. Furthermore, the research also focuses on classification of retinal images into normal or abnormal cases of DR. Here a number of features should be defined so that the algorithm can differentiate between the classes of DR. According to Jesse and Thomas [1], DR is a serious medical disorder which influences 3.4 percent of population. The duration of the disease, the type of diabetes mellitus and patient's age are the three main factors that increase the risk of developing DR. For example, for individuals with type 1 diabetes, the probability of DR is 50% for a 10 year old person; however, by age 30 the risk increases up to 90%. The probability of DR for individuals with type 2 diabetes is 5%. Moreover, according to the World Health Organization, after 15 years of diabetes, 2% of DR patients become blind and 10% suffer from sever vision problems. In this study, a novel approach for automatic detection and classification of DR from retinal fundus images is proposed. Images from the standard diabetic retinopathy database are considered. Different algorithms are developed and used to extract hemorrhages, exudates, blood vessels and OD. The blood vessels and OD are removed from the images. By applying these algorithms, DR is detected. On the classification part, ten features are defined. Then, the methodology is tested in terms of sensitivity and accuracy. Detection and classification of DR depend on the: 1. Database selection 2. Objects removing and lesion extraction 3. Algorithms 4. Feature definitions 5. Classifier selection.

12

Database Selection: There are several public databases for retinal fundus images. Therefore, the first challenge is to choose the suitable database of clear objects and DR lesions. Therefore, a well-known database has been chosen. Objects removing and lesion extraction: In order to detect DR, objects similar to the lesions must be removed from the image. For example, while blood vessels and hemorrhages have dark intensities, exudates and OD have bright intensities. The challenge here is to extract only exudates and hemorrhages, and to remove the other objects. Algorithms: Because DR lesions are similar to some objects in retinal images, a number of algorithms have been researched and modified in order to extract the lesions and filter the other objects in the image. Feature definitions: A number of features can be defined from the considered images and from the DR detection process. However, the challenge here is to select the suitable features in order to reduce detection time. Therefore, various features that explain the differences between normal and abnormal images have been defined and the most suitable ones have been selected. In addition, another goal here is to train the classifier based on the selected features Classifier Selection: Since there are several types of data classifiers, the most suitable one is selected. Also, the selected classifier must be trained to distinguish normal and abnormal images from each other.

1.4 Conclusion
The thesis has a total of five chapters. In this chapter, the background information and the literature review for automatic detection and classification of DR are provided. In addition, the challenges we face and the important facts about DR are explained. The rest of the thesis is organized as following: Chapter 2 presents a general block diagram of the DR detection and classification system followed by the contributions made on different stages of the system. Chapter 3 discusses the 13

proposed methodology starting with the preprocessing stage. Then, the process of object removing, lesion extraction and the details of selected algorithms are explained. Finally, the selected classifier and the defined features are introduced. Chapter 4 outlines the results of the proposed methodology in terms of sensitivity and accuracy and compares them with the results of other methods. In addition, it provides additional information regarding SNR, Fourier transform and SVM using different kernels. The thesis is concluded in Chapter 5. Directions for future work are also given in this chapter.

14

Chapter 2 System Overview
2.1 General Block Diagram
In the medical field, the computerized methodologies aim to provide automatic systems which can be used as diagnostic tools. For example, the DR detection system is a series of steps taken in order to achieve a particular result. Most of the current approaches related to DR detection consider different image processing techniques. However, they follow the same pattern shown in Fig 2.1. The main goals of this research could be summarized as following: 1- Improve the quality of fundus images to make it easier for medical doctors to recognize any possible soft or hard exudates and bleeding in fundus images; 2- Distinguish the exudates and hemorrhages from the OD and the eye vessels; 3- Analyze fundus images and provide guidelines for medical doctors to help them with distinguishing between normal images and the ones with DR. In order to achieve these goals, the first step is to apply preprocessing algorithms to select the green channel from the fundus image and to find the boundaries of the eye from background and to improve the image to make it easier for further processing. The next step is to detect different segments of the eye such as OD, blood vessels, soft and hard exudates and feasible area of bleedings or hemorrhages. Since each of these objects are in divergent shapes and properties, we have implemented and tested different image processing methods for better object differentiation and extraction. The third step is to analyze the detected objects to extract features such as area and cumulative intensity from every input fundus image. The last step is to use the extracted information to train and test a classifier to help medical doctors to better classify diseased eyes from normal ones. Although medical doctors will make the final decisions, our goal is to help them with diagnosis.

15

Retinal Fundus Images Preprocessing Objects Detection

Lesions Segmentaion
Disease Classification
Figure  2.1. General block diagram for DR detection

2.2 DR Detection Methodology
Detecting DR starts with preprocessing stage by applying several image processing techniques including green channel extraction, Circular Hough Transform (CHT), Contrast Limited Adaptive Histogram Equalization (CLAHE), and thresholding. Next, blood vessels, hemorrhages and exudates are detected using Gabor filter. In the next stage, hemorrhages and exudates are segmented separately using thresholding and CHT methods. Finally, ten features are defined as the input for the last stage, i.e., classification of input images into normal retina, NPDR or PDR. The classification process is done using SVM. The methodology for automatic detection of DR includes the following four main steps: 1. Image preprocessing which include:   Image enhancement Optic disc localization, segmentation and filtering

2. Image processing to detect boundaries of:  Retinal blood vessels 16

 

Hemorrhages Exudates

3. Lesion segmentation which provides images of:   Segmented exudates Segmented hemorrhages

4. Image Classification process which is based on:  Features computing and selection.

The four main categories and the applied algorithms are discussed in details in chapter 3. Here the contributions of this study to the automatic detection process of DR are explained.

2.3 Contributions
Most algorithms for automatic detection and classification of DR follow similar image processing stages. The performance of the algorithms in each stage highly affects the final result of the detecting methodology. For example, results of lesions segmentation step is affected by the results of the processing step. The clearer the image in processing step, the better results for the segmented exudates or hemorrhages. Therefore, this study contributes to different stages of image processing. In fact, each contribution is a separate tool that can be used by researchers and/or medical doctors. In this project, we first automated the detection of different segments of the eye from fundus images. These segments include optical disc, blood vessels, any hard and soft exudate, and possible hemorrhages. Using all this information in the process of DR recognition is one of the major contributions of the proposed method. In other studies, the classifiers are either based on the original fundus image and the intensity level of the pixels in the image, or based on only a few features extracted from the image, such as blood vessels and exudates. Since soft and hard exudates and the

17

hemorrhages could be all indications of DR, we used features that are used to train a classifier in order to sort fundus images into normal, PDR and NPDR classes. 2.3.1 Retinal Objects Detection In the processing stage the boundaries of all image objects need to be clearly detected. In this work, we present a modified Gabor filter algorithm for edge enhancement of the main objects in any retinal image. The modified algorithm perfectly detects boundaries of blood vessels, exudates and hemorrhages. The technique has been tested on images from different databases and the results were satisfactory. The results of this part of the study have been presemted in the 29th IEEE Canadian Conference of Electrical and Computer Engineering (Vancouver, 2016). Figure 2.2 shows the flowchart of the methodology for detecting image objects. Green Channel Extraction: the green channel image is extracted from the color image since image objects have higher contrast in color images. OD Removing: The CHT is used to detect the OD. Then, the OD is removed from the image to prevent from being incorrectly detected as exudate. Image Enhancement: The contrast is enhanced in order to have significant information about the image objects. Contrast Limited Adaptive Histogram Equalization (CLAHE) and Alternative Sequential Filter (ASF) algorithms are used. Gabor Filter 1 and 2: Edges of the image objects are detected using Gabor filter at two different input values. Object Detection: The two images are combined using multiplication and square root operations in order to increase image quality. Edge Enhancement: The edges are enhanced in order to have significant information about the boundaries of image objects.

18

Green Channel

OD Removing

Image Enhancement

Gabor Filter 1

Gabor Filter 2

Ã

Objects Detection

Edge Enhancement

Figure  2.2. The flowchart of the objects detection algorithm 2.3.2 A Tool for Automatic Segmentation of DR Lesions by Physicians Since manual segmentation of exudates and hemorrhages is time consuming, the automated methods have been proposed. This work presents a methodology for automatic segmentation of exudates and hemorrhage in any retinal image. It has been modified to turn it into a tool that can be used by physicians to segment the lesions automatically. The doctor can select different values from the range of segmentation thresholds to obtain the best possible segmentation of exudates and hemorrhages. The technique has been tested on images from various databases, and the segmentation 19

results were excellent. Figure 2.3 shows the flowchart of the proposed algorithm for DR lesions segmentation. Preprocessed Image: The output image of the objects detection algorithm was used as input image. Thresholding 1: The optimized range of thresholds was found. Then, thresholding algorithm was used to extract objects of bright intensities in the preprocessed image. Thresholding 2: Another threshold was used to extract objects of dark intensities in the preprocessed image. Segmented Soft and Hard Exudates: After applying the first threshold, exudates of different types were segmented. Segmented Blood Vessels and Hemorrhages: After applying the second threshold, an image including both blood vessels and hemorrhages were created. Thresholding 3: The last threshold was used to extract the brightest objects. Circular Hough Transform: The CHT was used to extract the dark circular structures. Segmented Hard Exudates: After applying the third threshold, hard exudates were segmented. Segmented Hemorrhages: After applying CHT, hemorrhages were segmented.

20

Preprocessed Image

Thresholding 1

Thresholding 2

Segmented Soft and Hard Exudates

Segmented Blood Vessels and Hemorrhages

Thresholding

Circular Hough Transform

Segmented Hard Exudates

Segmented Hemorrhages

Figure  2.3. The flowchart for DR lesions segmentation algorithm 2.3.3 A Testing Tool for Automatic Classification of Retinal Images for Researchers The classification process of retinal images is highly affected by the extracted features. Therefore, the focus here is on selection of features. In this study specific features are used for training the classifier. In addition, the defined features are computed using images from one of our previous studies. The classification methodology has been tested on images from two different databases. The results were satisfactory, specifically in terms of sensitivity.

21

Therefore, the proposed classification methodology is also considered as a testing tool for algorithms that segment DR lesions automatically. Figure 2.4 shows the flowchart of the classification algorithm.

Features Extraction and Selection

SVM Training

Images Classification using SVM

Performance Testing

Figure  2.4. The flowchart of Classification Process Features Extraction and Selection: Several features were computed using different images from the previous studies. The main features were extracted from Green, Red and Blue channels of the fundus images, as well as the gray scale image. However, only the best features, i.e., the total area and cumulative intensity of DR segments, were chosen. SVM Training: The selected futures were used to train the classifier. Image Classification using SVM: SVM was used to classify retinal images to normal or abnormal. Performance Testing: The performance of the SVM was measured in terms of accuracy and sensitivity.

22

2.4 Conclusion
This chapter presented the automatic detection and classification system for DR. In addition, it briefly introduced the contributions we have made to detecting system. In summary, we have selected suitable algorithms such as CLAHE, Gabor filter, CHT and thresholding and we modified them to achieve our goal of automatic detection of DR. To conclude, the major contribution of this work is to modify the selected algorithms to achieve clearer images that would allow better feature extraction.

23

Chapter 3 An Approach for Automatic Detection of Diabetic Retinopathy
As mentioned in the previous chapter, the methodology used in this study consists of four main stages: preprocessing, objects extraction (processing), lesions segmentation and classification. The proposed techniques were contributed in the algorithms used in these stages.

3.1 Preprocessing
Preprocessing is a primary step in working with retinal fundus images since these images inherently suffer from issues such as inadequate contrast, lighting variations and noise effect [2]. Therefore, preprocessing mainly aims to enhance the input images by attenuating intensity variations in order to have normalized images. Enhanced images are less noisy, hence they have better display and are easier to analyze. In this study, the primary purposes of the preprocessing step are: to extract the boundaries of the eye and distinguish between eye and the background; to detect the OD and remove it from the fundus image; and to enhance the image for use in the following stages. In fundus images, the imaging device normally records the eye image with a circular mask. This could help to utilize circle detection methods in order to distinguish the eye from the background of the fundus image. On the other hand, the OD normally has a circular shape. The Circular Hough Transform is a strong stable candidate to detect circles in an image; therefore, we utilized this method for both purposes of detecting the eye from the background and capturing the OD from the rest of the eye. The preprocessing stage includes two main steps:   Background Removing OD Localization and Removing

24

3.1.1 Background Removing Background removing is an essential step in image preprocessing because the proposed algorithms should be applied only on the eye pixels. The inputs of this stage are public RGB colored images from the DIARETDB database. In this study, the CHT detects the eye. Then, the background is removed. However, the image is first normalized. Image normalization is a process of changing the range of pixel intensities to simplify the analysis. The selected range is 0 to 1; while 0 refers to minimum and 1 refers to maximum intensity value in the image [34]. 3.1.1.1 Circular Hough Transform. Circular Hough Transform or CHT is a special case of Hough Transform (HT) used to find circles within an image. This approach has been chosen in this study because of its robustness to noise and its adaptability to different image contrasts. The eye is usually brighter than the background; therefore, to detect the eye, we utilized CHT algorithm to detect the largest bright circle in the image. Similarly, OD is usually much brighter than rest of the eye and is located in the left or right corner of the fundus image. This information was used for detecting the OD. Finding geometric shapes, e.g. a circle, in an image is one of the challenges in image processing since these objects may have different sizes. Normal retinal images have two main circular structures of different sizes, i.e., the eye and the OD. However, there might be additional circular structures in abnormal retinal images. There are many methods for locating circles in an image. Circular Hough Transform (CHT) is one of the most commonly used algorithms for locating circles. This is probably due to the high efficiency of this algorithm even when the image is noisy or has varying illumination [35]. In addition, CHT can detect low contrast objects [36]. 25

In 1962, Paul Hough presented a method for detecting lines in a binary image, known as the Hough transform algorithm. Its principle depends on the fact that all points of a line intersect in one point, the peak, at the corresponding parameter space of slope and intersection variables [37]. Then, HT is generalized to detect arbitrary shapes such as circles. CHT algorithm, a special form of Hough transform, is a feature extraction method for detecting circular shapes in an image. It converts a gray scale image to binary image using edge detection techniques such as Sobel or Canny. The equation of the circle is:  2 = ( - )2 + ( - )2 (1)

Here, a and b represent the center coordinates, and r is the radius where,  =  + ()  =  + () (2) (3)

In a circular shape, the intersection point at the corresponding parameter space is the center C (a,b) of the circle of radius R. Therefore, the challenge in implementing the CHT depends on the number of unknown dimensions as the parameter space consists of three variables: the coordinates of the center of the circle and the radius. Therefore, three steps must be taken when considering CHT implementation: the accumulator array creation, center estimation and radius estimation [36]. To illustrate, after detecting the edges of an image, the CHT creates a 3D accumulator array of zeros A (a,b,r) where a and b are the center of the circle and r is its radius. Then, each edge point is considered as the center of the circle, and different r-values at each edge point are taken into the accumulator array. However, all edge points will intersect only in one value at the parameter space, the center of the circle (Fig. 3.1). Such point is the peak value in the accumulator array [37]. In this project, the candidate points that are considered as potential edges of a circle are the ones with gradient value of greater than 30% of the maximum gradient within the whole image. Any

26

point with higher gradient value is considered a potential edge of a circle; then, the best circles are selected by choosing the peak maximum value in the accumulator array.

Figure  3.1. CHT algorithm theory. Each point (x, y) in the geometric space (left) creates a circle in parameter space (right). All circles created in the parameter space intersect at the (a, b) which is the center of the geometric space. In this study, the CHT is used twice. First, it is used to detect the eye as the largest circle in the image. The detection process considers a range value for the radius. Since eye sizes in retinal images vary, the maximum radius Rmax is chosen to be 50% of the image size. On the other hand, the minimum radius value Rmin is found to be 85% of half of the image. After detecting the large circle, the background was removed.  = 50%     =
  2

(4) (5)

 85%

3.1.2 OD Localization and Removing The optic disc (OD) is the bright yellowish region in the image. It appears in circular shape with outgoing blood vessels. Localization of OD in retinal images is very important since the OD is

27

similar in color to exudates. Therefore, OD has to be excluded so that it is not mistakenly detected as a lesion. Moreover, OD localization is an essential step in diagnosis of optic nerve diseases. The OD is a large and brightest circle located in a specific area of a retinal image. In order to detect such circle of known position, a search image relevant to the position of the OD was created. OD localization is the process of finding the largest circle in the region of the search using the CHT algorithm. An RGB image has three channels: red, green and blue. The green channel has the best objects/background contrast, and it is widely considered as the main input image. In the next step, the green channel is extracted. Then, since the OD is localized, it is extracted and moved to a separate image. In the last step, the OD is removed from the green channel image. For example, an RGB image of detected eye is shown in Fig 3.2. (A). The CHT was applied on the RGB image and the OD was detected (Fig 3.2. (B)). Next, the OD was extracted and moved to a separate image from the green channel image (Fig. 3.2 (C)). In the last stage, the OD was removed (Fig. 3.2. (D)).

28

(A) Eye Detection

(B) OD Localization

(C) OD Extraction

(D) OD Removing Figure  3.2.The process of OD removing 29

3.2 Image Enhancement
In different parts of this thesis project, we have utilized image enhancement techniques to improve the quality of the image or enhance its contrast. Depending on the need, we used Alternative Sequential Filtering, Contrast Limited Adaptive Histogram Equalization or thresholding techniques. Image enhancement is the process of increasing image quality before proceeding to further analyses. Such process is required for several reasons. For example, it might be necessary to highlight or suppress certain image objects. In addition, image enhancement improves the results of advanced image processing techniques. For instance, in case of retinal images, DR detection requires suppressing the retinal blood vessels and highlighting exudates and hemorrhages. This is because the clearer the objects are, the easier they will be detected. As mentioned before, the major techniques utilized in this project are ASF and CLAHE techniques. These techniques are used to enhance the removed OD image. 3.2.1 Alternative Sequential Filter (ASF) The ASF is a mathematical morphology technique that aims to represent and describe region shapes by testing the geometric structure of an image using small models named the structuring elements, a set of morphological algebraic arithmetic operators such as dilation, erosion, opening and closing. Morphological operators in image processing are a set of non-linear operators related to the shape or morphology of the image. In general, these operators rely on the relative ordering of the pixels in the image and they rely on definition of a small binary image called a structuring element. The structuring element scans through the whole image and based on the operator in use, it is compared with the corresponding neighborhood of pixels in different manners. The fundamental morphological operators are erosion and dilation. The erosion of the structuring element and the original image results into shrinkage of the objects in the image. It generates an image that shows the 30

area in the original image that the structuring element could be completely fit in the objects of the original image. On the other hand, dilation results into expansion of the objects. This operator produces an image that every structuring element could hit the object in the original image. The opening operator is defined as erosion followed by dilation. This process generates a new image from the original one in which the gaps among objects are opened up. On the other hand, closing operator is defined as dilation followed by erosion. This process generates a new image from the original one in which the gaps are filled up. The amount of opening or filling of the gaps depends on the size of the structuring element. The larger the element is, the more effect it would have on the image. Smaller structuring elements are more suitable to open or fill small gaps. The ASF is a sequential use of opening and closing. In each iteration, the size of the structuring element increases. This way, we could better remove the speckle noise from the image. We could also separate the larger objects in the image. In this study, the ASF aims to smooth some parts of the blood vessels using disk structuring element in order to remove speckle features of the image that may interfere with image objects. 3.2.2 Contrast Limited Adaptive Histogram Equalization (CLAHE) Histogram equalization is the process of enhancing image contrast through adjusting image intensities. The conventional histogram equalization method tries to equally distribute the intensity values in the whole image. Since in our proposed method by applying different extracted masks a large part of the image will be removed and converted to black area, the conventional histogram equalization does not benefit image enhancement and will change the removed black area with other intensity values. Therefore, we need to utilize an adaptive method in histogram equalization process. The Adaptive Histogram Equalization (AHE) is the process of mapping each pixel to an intensity value derived from the surrounding pixels using a transformation function. However, the AHE 31

method has the drawback of noise over enhancement in cases of homogenous regions, or poor noise to signal ratio [38]. The Probability Distribution Function (PDF) of a digital image (I) that has N pixels with intensity values distributed in (L) values where nk is the number of intensity pixels at intensity level ik is given by equation (6). The Cumulative Density Function (CDF) is given by equation (7).  ( ) =
 

(6) (7)

( ) =   ( ) =0 

The value of contrast enhancement for a specific intensity is proportional to the slope of the CDF at that intensity. Therefore, to reduce noise enhancement, the contrast enhancement has to be limited. This is achieved by limiting the slope of the CDF function. Here, the histogram at some threshold is cut, and equalization is applied [38]. Thus, the AHE has been modified to CLAHE. One of the main advantages of CLAHE in comparison with AHE is that it prevents over-amplification of noise in the homogeneous areas of the image. This is the main reason we have utilized CLAHE in histogram equalization of the image. The basic form of CLAHE algorithm enhances the contrast among small regions, i.e., tiles, of an image. The contrast of each tile is individually enhanced to obtain a uniform histogram. The enhanced images are then combined by interpolation. Figure 3.4 shows that in the CLAHE enhanced image (Figure 3.4.B) the objects can be seen with greater details than in the green channel image without OD (Figure 3.4.A). For instance, the boundaries of retinal blood vessels, the main objects in both normal and abnormal retinal images, are clearly visible in the CLAHE image.

32

Figure  3.3. Histogram enhancement, the histogram of green channel image without OD (left) is enhanced using CLAHE (right).

A

B

Figure 3.4. The process of image enhancement using CLAHE. The CLAHE enhanced Image (B) has better contrast than the green channel image (A).

33

3.3 Objects Extraction
Objects extraction is the process of extracting significant information from the image. The main goal of this stage is to detect the edges of the objects. The objects that have been segmented in this project are mainly blood vessels, soft and hard exudates and hemorrhages. Thus, the novel approach aims to detect edges of normal and abnormal objects using a proposed developed Gabor filter. Later, advanced algorithms will be applied to segment DR objects. Blood vessels are extracted using Gabor filter bank and by applying two different filters into the image. The results, shown in the next sections, show that this method is well capable of extracting the vessels. The exudates and bleedings are then segmented by applying thresholding and CHT techniques.

3.3.1 Continues Wavelet Transform (CWT) Continuous Wavelet Transform measures the similarity between a signal and analyzing function. Here, the analyzing function is a wavelet. The wavelet is a waveform of effectively limited duration that has an average value of zero. (t)dt = 0  -


(8)

CWT is the sum over all time of the signal multiplied by scaled, shifted versions of an analyzing wavelet [12]. It is defined as:

T  (b,,a)=  -Â½ -1   (a-1  -  (x - b))f(x)d
Where, C  : The normalizing constant, : The analyzing wavelet, b: The displacement factor, : The rotating (shifting) angle, a: The scaling parameter. 34

(9)

Continuously changing the scale and position parameters obtains a set of coefficients. Multiplication of each coefficient by the appropriately scaled and shifted wavelet returns the constituent wavelets of the original signal. CWT is a very effective tool in image processing due to its selectivity and flexibility in choosing scales and positions [16].

Figure  3.5. CWT using shifted and scaled versions of a wavelet (red) is applied to a signal (black) to return the constituent wavelets of the original signal (bold black). (Source: MATLAB help tool). There are several analyzing wavelets. The two-dimensional Gabor filter has shown excellent results in image processing. Therefore, in this study Gabor filter has been used to extract image objects including blood vessels, exudates and hemorrhages. 3.3.2 The Two Dimensional Gabor Filter The two-dimensional Gabor filter is a well-known edge detection algorithm. Gabor filter is able to detect image objects in any rotation, position and scale. It is a powerful tool with several advantages. One of the most important benefits of Gabor filter is its capability to detect and analyze directional structures. Such advantage is very helpful for edge detection of blood vessels since blood vessels appear in different directions all over the image. In addition, Gabor filter has strong edge

35

detecting features such as its spatial locality, orientation selectivity and frequency characteristics [16]. Gabor is defined as:  =   =
  2 2   2 + + ) )) cos ( 2 2   

(10) (11) (12)

(, ) = exp (-0.5 (

 = () + ()  = -() + () Where,

(13) (14)

x: The standard deviation of Gaussian in x direction along the filter, y: The standard deviation of Gaussian in y direction along the filter, : The orientation of the filter, : The wavelength of the cosine factor of the Gabor filter, : The phase offset. The range of input values for retinal images using Gabor filter are shown in Table 3.1. At each angle, Gabor wavelet detects some edges. Then, the maximum value over all possible orientations is considered. In this method, we first generate a bank of filters with different specifications. Each filter is responsible to better detect a predefined angle. In each Gabor filter, we used 18 angles starting from zero with the increments of 10o. By convolving a specified filter with a defined angle, we can detect the lines in the image aligned with that angle. Then, a single image is generated by adding all the results from all filters. In this resulted image, the straight lines are detected.

36

Depending on the values of  and  in Eq. (12), the length and strength of the lines in the Gabor filter are defined. We used two sets of values to generate two different filter banks. Gabor filter 1 is selected to detect more details within the image, whereas Gabor filter 2 is selected to detect less details but more of the thick straight lines through the input image. By combining the results from Gabor filter 1 and 2, we removed all the details and selected just the blood vessels of the fundus image. Table 3.1 Range of Gabor Filter Values for Retinal Images

Input
 

Best Value Range {0.2-1} {1.5-3.8} {3-6} {3-5} {0-180}

 


3.3.3 Combined Gabor Filter (CGF) CGF is a technique used for object extraction in different types of retinal images. It is also an edge enhancement technique for image objects. In this study, Gabor filter is applied with values shown in table 3.2. A total of 18 rotation angle were chosen at an increase rate of 10 degrees starting from 0 to 170. Two different values of the standard deviations of Gaussian have shown excellent results. Here, this study contributed in finding an edge enhancement technique. Therefore, Gabor filter images at  = 2 and  = 3 have been modified into a CGF image. It is defined as: CGF Image = (Imagein) 2*Ã (1- Image=2ÃImage=3) Where: Image=2: Gabor filter image at  = 2. Image=3: Gabor filter image at  = 3. 37 (15)

Imagein: Preprocessed image



7

Figure  3.6. Gabor wavelet generated at 18 different orientations using two values for the standard deviation of Gaussian. The upper 18 wavelets are generated at  = 2, the lower 18 wavelets are generated at  = 3. The first angle is 0, the 18th angle is 170. Image=2 and Image=3 are combined using multiplication operation to allow better edge detection. The resulting image is inverted. Next, the preprocessed image is enhanced using square root operation. In the next stage, multiplication operation combines the resulting images to enhance edges of the preprocessed image.
Fig .2 Images of a) Eye etection b) OD localization c) OD detection d) OD removing

38

Table 3.2 Used Values

Input
 

Used 1 2,3 6



As mentioned previously, each Gabor filter selects predefined details from the input image. The first filter shows more details since the size of the predefined lines in the filter bank is smaller. On the other hand, the second Gabor filter detects the longer and thicker lines in the image.

3.4 Lesions Segmentation
The signs of DR include MAs, hemorrhages and hard exudates. MAs are small swellings on the side of tiny blood vessels [5]. MAs are the first clinical signs of DR. Retinal hemorrhages (bleeding) could severely damage the person's vision [4]. Exudates are a yellowish fluid rich in protein that oozes out of blood vessels as a result of inflammation [11]. Exudates are the main causes of blindness in NPDR. MAs are considered a type of bleeding, therefore, in this study, the algorithm focuses on detecting exudates and hemorrhages. Hemorrhages and exudates must be segmented in order to detect DR. The obvious difference between these two abnormalities is the color intensity. While exudates are bright, hemorrhages are dark. Therefore, thresholding technique is used to segment them. 3.4.1 Thresholding Thresholding technique is widely used for image segmentation. It is a useful tool for distinguishing different pixels' intensities. Thresholding divides image pixels into object pixels or background pixels by choosing a proper threshold. In other words, it is the process of segmenting the 39

image based on the pixel intensity value [39]. Thresholding has shown excellent results when detecting bright objects on dark background. In this research, exudates and hemorrhages are segmented separately. Eexudates are the brightest parts of the image; thus, an optimized threshold is obtained to segment them from the CGF image. However, there are two types of exudates: hard and soft. While hard exudates have explicit boundaries, soft exudates have unclear boundaries. The obtained threshold intensity for hard exudates is in the range 0.6 to 0.65. Hard exudates, the brightest parts of the image, are segmented using equation (16): 0, CGF(x,y)= { f(x,y), Where, T is the threshold; CGF is the processed image, and f(x,y) is the gray level intensity for a pixel of coordinates x,y. Equation (16) is also used to segment all exudates. The optimized threshold intensity for all exudates is in the range of 0.45 to 0.5. This is a reasonable range of values for detection of exudates because the intensity of these lesions is the mid-level. In order to make sure that other parts of the fundus image in this range of intensities are not selected, we first combined the input image and the results from Gabor filters to remove any details from the OD and the blood vessels. Then, we apply the thresholding after image normalization. Fig. 3.7 shows the difference in using various thresholds. if f(x,y)<T if f(x,y)T

(16)

40

T = 0.42

T=0.5

T=0.55

T = 0.58

T=0.6

T=0.62

Figure  3.7. An example of using different thresholds to segment exudates. All the exudates are segmented at T=0.42. The exudates in the range of 0.5 to 0.6 are normally considered as soft where in some cases the doctor defines them as hard exudates. All hard exudates are segmented at T=0.6. On the other hand, hemorrhages are the darkest part of the image; therefore, another optimized threshold is determined to segment them from the CGF image. Since blood vessels are very similar to hemorrhages in intensity, they are also segmented. The threshold range of the dark intensities including blood vessels and hemorrhages are in the range of 0.01 to 0.4; where 0.01 is the minimum threshold that detects the deep hemorrhages in the very advanced cases of DR, and 0.4 is the maximum threshold that detects the early hemorrhages that are known as MAs that appear at the very beginning of DR. However, hemorrhages differ from blood vessels in shape; hemorrhages usually appear in circular or semicircular shapes. Therefore, CHT is used to differentiate hemorrhages from blood vessels. Here, the range of radii for finding bleeding regions depends on the amount of bleeding in the image.

41

It should be noted that the thresholding algorithms used in this section result into three different image masks: one for hard exudates, one for hard and soft exudates and one for hemorrhages. These masks are black and white images that show the location of exudates and hemorrhages. By multiplying the masks and the original image, we can show the details of exudates or hemorrhages and remove the rest of image parts. The resulting image shows bleeding regions in the eye including the MAs. However, unlike exudates where the exact boundaries are extracted, hemorrhages are detected as circles that represent their exact location. The proposed methodology can be used for detecting DR because it has the ability to detect exudates and localize regions of hemorrhage. Additionally, the same results were obtained when the proposed algorithm was used on different databases such as STARE and DRIVE.

3.5 Classification
Image classification is the process of grouping images into different classes. Image classification is an important process in the field of medical image processing. The outcome of the feature extraction process has a significant effect on the results of image classification. Moreover, image classification is based on the similarity of image items. To illustrate, the classification system consists of a set of predefined images of known features which are used to compare with an image of unknown class [40]. However, the classifier requires training in order to be able to perform its function. Therefore, extracting the appropriate features is an essential step in image classification. 3.5.1 Feature Extraction A feature is a piece of information that describes a specific item in an image. For example, the texture of hemorrhage or the shape of exudate is considered as a feature of a DR image. Therefore, a larger number of extracted features will lead to better image classification. However, as

42

the number of features increases, the computing time increases too. Therefore, it is important to select only the most appropriate and useful features. In this research, initially 31 features have been extracted; however, only the best ten features were selected (Table 3.4). The initial 31 features were extracted from Green, Red and Blue channels of the fundus images, as well as the gray scale image. By analyzing these features, we noticed that the Green channel has more details compared to the rest of the channels. Therefore, to have a faster training and avoid under-fitting problem in the training process, we only used the features extracted from the Green channel. In addition, only a selection of the 109 images in DIARETDB was used. For each image, the proposed algorithm computed and extracted the ten selected features using seven images. As per Table 3.3, the features were selected to ensure that they bring meaningful information to the classification process. We used the total area of exudates (hard and soft) and hemorrhages, their cumulative intensities, and the percentage of their cumulative intensity with respect to the cumulative intensity of the whole fundus image. Table 3.3 Selected Images for Feature Extraction Image Number Image 1 Image 2 Image 3 Image 4 Image 5 Image 6 Image 7 Description Green Channel Image without OD All Exudates Image Hard Exudates Image Bleeding regions Image Mask of All Exudates Mask of Hard Exudates Mask of Bleeding 43

Table 3.4 Features Description Feature Number Feature 1 Feature 2 Feature 3 Feature 4 Feature 5 Feature 6 Feature 7 Feature 8 Feature 9 Feature 10 Description Total area of hard exudates Total area of all exudates Total area of bleeding Cumulative Intensity of all hard exudates Cumulative Intensity of all exudates Cumulative Intensity of all bleeding Cumulative Intensity of the eye Feature 4 / Feature 7 Feature 5 / Feature 7 Feature 6 / Feature 7 Used Image Image 6 Image 5 Image 7 Image 3 Image 2 Image 4 Image 1

The first six features describe DR lesions in terms of total area and cumulative intensity. Features 8 to 10 calculate the ratio of the cumulative intensity of DR lesions to the cumulative intensity of the main image. The last three features are critical since they determine the associated class of the image. In order to calculate the area of exudates and hemorrhages, we used the white area in the defined masks from the thresholding steps. 3.5.2 Support Vector Machine SVM is a supervised machine learning method used for data classification including images. Based on the calculated features, the classification parameters are defined and every data is grouped into its most relevant class. Each data, i.e., image, is given to the SVM as point in n-dimensional space where n represents the number of extracted features. In n-dimensional space, the value of each 44

feature is the value of a particular coordinate. Here, SVM finds all the possible hyper planes that separate the classes linearly. Next, it finds the optimal hyper plane based on the maximum distance, margin, from the nearest data point to the hyper plane of interest. In case of nonlinear data, the training data are mapped into high dimensional feature apace using a nonlinear kernel function. Then, the data can be classified linearly [40]. Figure 3.8 illustrates an example of how SVM works. Although there are several image processing techniques for DR classification, SVM has the advantages of high performance in higher dimensional spaces, easier analysis of data, ability to deliver a unique solution, and the ability to perform non-liner classification. Therefore, it is used to classify retinal images. SVM is used to classify the input images to classes of normal retina, NPDR or PDR. However, SVM has to be trained in order to perform its duty. It is a learning process for the SVM. Here, the goal is to enable the SVM to distinguish among data classes. In this study, 75% to 80% of the images train the SVM using their extracted features. At this stage, SVM is familiar with selected classes (Table 3.5), and it also has the ability to differentiate DR features from normal features. Next, the remaining 20% to 25% of images are classified and are also used to measure the performance of the proposed method. It should be noted that the reason we used a high percentage of dataset in the training process is that we only had 109 images available for both training and testing. Since there are 10 defined features in the classifier, 75% of the total images were selected for training to ensure that the classifier is well trained. The rest of images were used for testing. We have also used 50% of the data for training and the results were the same.

45

Figure  3.8. An example of classification using SVM. The SVM finds all possible hyper planes to separate the data (left), SVM finds the optimal hyper plane by finding the maximum margin between the nearest data point and the hyper plane (right) [40]. Table 3.5 shows SVM classes and the number of images selected from DIARETDB for each class. The SVM classifies all images in two steps. First, it classifies all images to two classes of normal and DR. Then, the images in the DR class are classified to either NPDR or PDR. Table 3.5 SVM Classes

SVM Class
Normal Retina NPDR PDR

Total Number of Images
28 61 20

3.5.3 Performance Measurement The relation between the performance of SVM and the strength of extracted features is proportional. SVM has shown excellent results for data classification in many studies. Therefore, it is required to test the performance of the presented methodology through testing the performance of the 46

classification processes. Here, the performance of the proposed method is measured in terms of sensitivity and accuracy. Sensitivity is the ratio of abnormal images classified correctly. Accuracy however, is the ratio of the total number of correctly classified images to the total number of images. Sensitivity and accuracy are defined as:

Accuracy = +++ Sensitivity =
Where,
 +

+

(17)

(18)

TP is the number of abnormal images correctly classified as abnormal; TN is the number of normal images correctly classified as normal; FP is the number of normal images incorrectly classified as abnormal; FN is the number of abnormal images incorrectly classified as normal.

3.6 Conclusion
In summary, the DR detecting methodology presented in this thesis starts by removing the background and the OD from the green channel image followed by enhancing the image using ASF and CLAHE techniques. Next, we modified the Gabor filter to detect boundaries of the image objects. Then, only DR lesions were segmented using Thresholding and CHT techniques. After that, ten features based on area and cumulative intensity were extracted for training the SVM classifier to separate the fundus images into classes of normal, NPDR and PDR.

47

Chapter 4 Results
This study aims to develop an approach for automatic detection and classification of DR through developing the main three stages of detection, i.e., processing, segmentation and classification. Therefore, to test the performance of the used algorithms and contributions in retinal images, we performed three different experiments. Here, the goal is to quantify the algorithmic performance of each stage several times using different images. The processing and segmentation experiments were done on a total of 40 images from DIARETDB. The classification experiment was done using 109 images from DIARETDB. Next, it is important to perform the same experiments using images from other databases. Therefore, processing and segmentation experiments have also been done using images from STARE and DRIVE databases. Both normal and abnormal images were included in the experiments. Moreover, the classification experiment was done on the 33 best images of STARE. Table 4.1 Selected images Database Number of normal retina Number of DR images images DIARETDB DRIVE STARE 15 3 5 25 2 5

48

4.1 Processing Experiment using the Proposed CGF
As explained earlier in chapter 2, in this work we present a new processing algorithm called CGF. CGF is a modified version of Gabor filter and is used to enhance edges of image objects. This experiment indicates that using CGF for processing different types of retinal images has excellent results. Applying CGF in a normal retinal image Figure 4.1 shows the processing of an image of normal retina selected from the DIARETDB. Fig 4.1 (a) shows the preprocessed image which was extracted using CHT, ASF, CLAHE and square root algorithms. Next, Gabor filter of two different Gaussians were applied and the resulting images are shown in Figures 4.1 (b) and (c), respectively. These images have good results for objects' edges. However, the clearer the edges are, the better the advanced results are. Therefore, the resulting two images were combined to enhance edges of blood vessels as shown in Fig. 4.1 (d). Here, the pattern of blood vessels was clearly detected and blood vessels' edges were well enhanced. Next, the combined image was inverted (Fig 4.1 (e)) and multiplied by the original image. The resulting image is the CGF image (Fig 4.1 (f)). It has better details of the blood vessels than the original image. Applying CGF in abnormal retinal images The processing experiment has also shown excellent performance on abnormal retinal images. It has the ability to enhance edges of DR lesions. For example, The CGF processing technique was applied on an image of an abnormal retina from DIARETDB, and the results are shown in Fig 4.2. Boundaries of blood vessels, hemorrhages and exudates are clearer in the CGF image (Fig 4.2 (f)) than the preprocessed image (Fig 4.2 (b)).

49

(a) Removed OD image

(b) Gabor Image at  = 2

(c) Output Image at  = 2

(d) Combined Image

(e) Inversion Process

(f) CGF Image

Figure  4.1. Resulted images using CGF on a normal retinal image from DIARETDB.

50

(a) RGB image

(b) Preprocessed Image

(c) Gabor filter  = 2

(d) Gabor Filter  = 3

Figure  4.1 Resulted image using CGF on normal retinal image from STARE.

(e) Combined Image

(f) CGF Image

Figure  4.2. Resulted images using CGF on an abnormal retinal image from DIARETDB, The CFG image (f) shows clear details for edges of blood vessels and DR lesions than the main image (b).

51

Applying CGF in images from other databases The CGF algorithm has been presented in a conference [41]. It was applied on images from DRIVE database. The results were satisfactory (Fig. 4.5). Furthermore, CGF showed excellent results on normal (Fig. 4.4) and abnormal (Fig. 4.3) images from STARE database too. Thus, CGF is considered an object extraction and edge enhancement technique for any fundus image. It is a tool that can be used to process retinal images.

.

Figure  4.3. Applying CGF on an abnormal retinal image from STARE (left). The boundaries of blood vessels and exudates are clearly detected and well enhanced (right).

Figure  4.4. Applying CGF on a normal retinal image from STARE (left). The blood vessels pattern is clearly detected and well enhanced (right).

52

(a) RGB Image

(b) Gabor Image  = 2

(c) Gabor Image  = 3

(d) Combined Image

Figure  4.5. Using CGF technique on a normal image from DRIVE (a), the edges of blood vessels are well shown in the combined image (d) than in Gabor images (b) and (c) [41].

53

4.2 Segmentation Experiment
The segmentation experiment aims to test the algorithm's ability to segment DR lesions. The segmenting methodology is based on thresholding and CHT techniques. This experiment indicates that using the proposed algorithm to segment DR lesions from retinal images gives excellent results. It perfectly segments hard exudates and bleeding regions. The average segmentation time is 15 seconds. Exudates Segmentation Figure 4.6 shows the process of segmenting exudates using an image from DIARETDB. Thresholding technique was applied on the CGF image (Fig. 4.6 (b)) and all exudates were perfectly segmented with clear boundaries (Fig. 4.6 (c)). Next, another threshold was applied to segment hard exudates. Also, hard exudates were perfectly segmented (Fig. 4.6 (d)). The used methodology has shown the same performance using different DR images. However, for some images, the thresholding values were modified to reduce the effect of inadequate intensity of the retina. Some of the fundus images had the disadvantage of inadequate retina color. For example, the image shown in Fig. 4.8 (a) has a bright color intensity which is different than the normal intensity of the retina. This affects the segmentation process as such brightness will be detected by the algorithm as exudates. Fig. 4.8 (b) shows that the image of segmented hard exudates has some segmented part of the bright retina. This error in segmentation is due to inadequate color of the retina. Moreover, the used methodology has shown excellent performance on images from other databases. Fig. 4.7 (a) shows the RGB image from STARE data bases. The RGB image has both hard exudates. Thresholding methodology was applied and the exudates were clearly segmented (Fig. 4.7 (b) and (c)).

54

(a) RGB Image

(b) CGF Image

(c) All Segmented Exudates

(d) Segmented Hard Exudates Figure  4.6. The process of segmenting exudates using NPDR image from DIARETDB. 55

(a) RGB Image

(b) All Segmented Exudates

(c) Segmented Hard Exudates Figure  4.7. The process of segmenting exudates using NPDR image from STARE.

56

(a) RGB image

b) Segmented Hard Exudates Figure  4.8. The process of segmenting exudates using PDR image from DIARETDB. Hemorrhages Segmentation The presented methodology based on the thresholding and CHT algorithms segment regions of bleeding with satisfactory results. Fig. 4.9 shows the process of segmenting bleeding regions using NPDR image from DIARETDB. The CGF image (Fig. 4.9 (b)) was extracted from the RGB image (Fig 4.9 (b)). Next, the proposed methodology was applied and the bleeding regions were segmented (Fig 4.9 (c)). Beyond that, the proposed algorithm was applied on selected images from STARE and DRIVE databases and the results were satisfactory (Fig. 4.10). Therefore, the proposed algorithm can serve as a proper tool for DR lesions segmentation.

57

(a) RGB Image

(b) CGF Image

(c) Extracted Regains of Bleeding

Figure  4.9. The process of segmenting bleeding regions using NPDR image from DIARETDB. 58

(a) RGB Image

(b) Preprocessed Image

(

(c) Bleeding Regions Figure  4.10. The process of segmenting exudates using PDR image from DRIVE 59

4.3 Classification Experiment
The SVM was used to classify retinal images into two classes of normal and abnormal, or three classes of normal, NPDR and PDR. This experiment was done to examine the performance of the SVM using the extracted features. DIARETDB has a total of 219 images. STARE has a total of 400 images; however, the images in STARE are from different eye diseases. Therefore, the 109 images from the DIARETDB that have ground truth information were used in this test, and their features were extracted. The classification procedure included the following steps:  First, the SVM was trained using features of 82 images (75%). These images were from normal retina, PDR or NPDR.  Second, based on their features, the SVM classifies the remaining 27 images (25%) to either two classes (i.e., normal and abnormal) or three classes (i.e., normal, PDR, and NPDR).  Third, the accuracy and the sensitivity of the algorithm were calculated.

Since inadequate color intensity of the retina affects segmentation results, the features of the normal images were extracted separately to ensure that the SVM identifies normal images correctly (without exudates or hemorrhages). Then the test was repeated. As shown in Table 4.2, the accuracy of SVM (its ability to classify normal images as normal) is high in the third and fourth cases because features of normal images are well extracted. In contrast, the accuracy is low in the first and second cases of the classification test because some features of normal images have been extracted erroneously due to inadequate color of those images. However, the sensitivity of SVM (its ability to classify abnormal images) is high in all cases. Table 4.3 shows the image classification results for the third and fourth cases.

60

Table 4.2 Accuracy and Sensitivity of svm test Description SVM classes Accuracy 80.1 % Sensitivity. 93.33%

Case 1: Features of 109 images are Normal or abnormal extracted Case 2: Features of 109 images are Normal, NPDR, PDR extracted Case 3: independent Features extraction Normal or abnormal for normal images Case 4: independent Features extraction Normal, NPDR, PDR for normal images

77.77%

93.33%

92.5%

90%

92.5%

90%

Table 4.3 Image Classification Results Classification measurements Total number of classified images Abnormal images classified correctly (TP) Normal images classified correctly (TN) Normal images classified incorrectly (FP) Abnormal images classified incorrectly (FN) 18 7 0 2

Moreover, the first case of classification test is applied on the best 33 DR images from STARE. The accuracy and sensitivity were 94.4% and 93.2%, respectively. However, these results are after applying the proposed methodology over 6 times. The reason is that the classifier is trained with 23 images.

61

4.4 More details on the results
In this section, we provide more details on the results of image processing and the classification stages of the project. Here, we calculate the SNR value of the images in each step and how the proposed methodology changes the SNR value. We also show Fourier transform of the images in each step and explain how the algorithms changed the frequency spectrum of the images. We will also use different percentages of data as for training, and show how it changes the accuracy and sensitivity of classification. 4.4.1 Signal to Noise Ratio (SNR) The Signal-to-Noise Ratio (SNR) of an image is a measurement that shows the signal level versus the noise level in the image. It compares the level of informative signal to the level of background noise. The SNR is defined as the ratio of signal power to the noise power and is normally expressed in dB. We used the following equation to calculate the image SNR in each step: SNR (dB) = 20 log ((max intensity Â­ min intensity) / (intensity standard deviation)) (19)

By comparing the SNR value of the images resulted in each step, we could illustrate the effect of algorithms in removing noise from the image and extracting more information. We have summarized the average SNR values of the proposed algorithms in Table 4.4. Table 4.4 The average SNR value of Normal / Abnormal Fundus Images in dB (decibles) Image Original Image (Green Channel) Removing OD Resulted image from Gabor Filter 1 Resulted image from Gabor Filter 2 Resulted image from Combined Gabor Filters Improved Image for Exudate detection Normal Images 15.7 12.6 16.5 26.0 26.1 18.2 Abnormal Images 15.6 14.9 16.9 26.5 25.5 19.1

The SNR level slightly decreases when we remove OD from the original image. This is resulted from the mask that we apply during OD removal. The OD is normally the brighter part of the 62

image and after removing the maximum value of image intensity reduces. This reduces the SNR. After applying the Gabor filters, the SNR increases significantly. This is an indication that the Gabor filters, especially Gabor filter 2 and the combined Gabor filter, are successful in segmenting the blood vessels. The images used for exudate detection have lower SNR values. This is due to the face that usually exudates form a small portion of the whole image. The SNR value for other parts of the algorithms, such as thresholding and bleeding detection is meaningless because in these images we convert the intensities into zero and one in order to create masks. 4.4.2 Fourier Transform of the Results In this section, we explain the step by step changes in the frequency contents of the images through all processing steps. The 2D Fourier transform of randomly selected normal and abnormal images will be shown and studied. The next four figures show the effect of each algorithm on a randomly selected normal and a randomly selected abnormal fundus image followed by their Fourier transforms. The Fourier transform has been done using FFT and FFT shift functions in MATLAB. In the frequency domain images, the center of the image is f = 0. The upside, downside, left and right sides of the frequency domain are the maximum frequencies. As illustrated in these images, the OD removal adds some high frequency noises to the image. This is because we introduce a mask into the image that generates a sudden change in the fundus image in the OD region. The CLAHE algorithm has a smoothing effect on the results and reduces the amount of high frequency noise. This result is predictable since the effect of smoothing is like a lowpass filter.

63

The effect of Gabor filters is similar to a band-pass filter which reduces the high and low levels of frequencies. Therefore, we can see more detailed mid-range frequencies after the Gabor filters. Multiplying the two Gabor filters in space is the same as convolving them in frequency. As could be illustrated from the FFT results, Gabor filter 2 has a higher band-pass frequency than Gabor filter 1. This can be seen in the results as Gabor filter 2 is brighter in higher levels of frequencies. The reason behind this effect is that Gabor Filter 2 is designed to capture the brighter, thicker and straighter blood vessels. By multiplying the Gabor filters results in space domain, they are convoluted in frequency domain. Therefore, we see higher ranges of frequencies in the resulted CGF image. This can introduce more noise on the image; however, the Gabor filters are precisely designed to make sure that the noise level does not increase significantly. We examined the SNR level in the previous sections.

64

Figure  4.11. A randomly selected normal fundus image

65

Figure  4.12. The Fourier Transform of the normal fundus image

66

Figure  4.13. A randomly selected abnormal fundus Image

67

Figure  4.14. The Fourier Transform of the abnormal fundus image

68

4.4.3 Effect of Train/Test data percentage in Classifier Performance As mentioned before, we used 75% of the data for training purpose and the rest for testing the SVM classifier performance. In this section we study how the accuracy and sensitivity of the SVM classifier changes when we use different percentages of training/testing datasets. As shown in Table 4.5, we reduced the training percentage from 80% of the total data to 25% with the decrements of 5%. In each run, we selected almost similar percentage from each class. The best accuracy was obtained from 75% of training. This could be justified based on the number of samples we had in our database. However, the classifier still has a good performance when the training set is reduced to 25%. As shown in the table, the accuracy and sensitivity obtained from testing the classifier is in the range of 75% and 83% for low percentage of training data. Table 4.5 The effect of training/testing data percentage Three-class classification Normal/PDR/NPDR Accuracy (%) Sensitivity (%) 89.74 97.14 92.5 90 82 88.64 76.37 83.3 82 88.7 81.8 87.7 79.5 85.5 75.9 83.6 76.2 84.5 75.6 84.2 77.9 86.2 75.2 83.5

Training Percentage (%) 80 75 70 65 60 55 50 45 40 35 30 25

69

Accuracy
100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80%

Figure  4.15. Changes in accuracy with changes in the training set. The best accuracy is at 75% training set.

Sensitivity
100% 95% 90% 85% 80% 75% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80%

Figure  4.16. Changes in sensitivity with changes in the training set. The best sensitivity is at 80% training set.

70

4.4.4 Choice of different kernels in Classifier Performance In this section, the effect of using different kernels in SVM classifier is studied. We have studied Linear, Quadratic, Polynomial (order:3) and RBF kernels. The classifier performance has been studied when the training percentage is 75%, 50% and 25% of all available data. Table 4.6 Kernel effect on the performance of SVM classifier
Linear Kernel Accuracy Sensitivity 92.5 79.5 75.2 90 85.5 83.5 Quadratic Kernel Accuracy Sensitivity 71.8 66.2 63.9 74.1 74.1 71.2 Polynomial Kernel Accuracy Sensitivity 78.1 72.3 71.1 77.8 83.3 76.5 RBF Kernel Accuracy Sensitivity 68.8 72.3 67 74.1 81.5 74.1

Training Percentage 75 50 25

Accuracy
100% 80% 60% 40% 20% 0% 25% 50% 75% Linear Kernel Quadratic Kernel Polynomial Kernel RBF Kernel

Figure 4.17. Changes in the accuracy using different kernels at three training sets. The linear kernel showed the best results.

The best results are from linear kernel (Figure 4.17 and figure 4.18). This shows that the feature space defined in this project results in linear differentiation of Normal, PDR and NPDR data samples. After linear kernel, the best choice of kernel is Polynomial kernel with the order of 3. The 71

reason is that there are more similarities between linear kernel and polynomial curve with order 3. This is another indication that the feature space is a linear space and the best kernel would be the simplest one which is linear kernel.

Sensitivity
100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% 25% 50% 75% Linear Kernel Quadratic Kernel Polynomial Kernel RBF Kernel

Figure  4.18. The change in sensitivity using different kernels at three training sets. The linear kernel showed the best results.

4.5 Conclusion
The methodology used in this thesis can detect and classify DR with excellent results. The results showed that the detecting methodology perfectly segments DR lesions in different types of fundus images from DIARETDB. The proposed methodology can also classify fundus images to different classes with excellent accuracy and sensitivity using linear kernel. Moreover, the presented methodology showed excellent results on images from other databases such as STARE and DRIVE. .

72

Chapter 5 Conclusions
5.1 Discussion
Several algorithms have been proposed for automatic detection and classification of DR. However, this work is unique in different aspects as it has application for image processing, lesion segmentation and image classification. Unlike other algorithms that focus on detecting DR by segmenting hemorrhages [5,8,] or exudates [11,12], the proposed approach can segment both hemorrhages and exudates. This work also has its own features for the SVM classifier. In addition, the sensitivity and accuracy of this algorithm is better than the sensitivity and accuracy of some of the previously reported algorithms (Tables 5.1 and 5.2).

Table 5.1 Sensitivty of other research Sensitivity Gupta et al. [42] AbrÃ moff et al. [43] Presented Methodology 87% 84% 90%

Table 5.2 Accuracy of other research

Accuracy
Tjandrasa et al. [30] Gupta et al. [42] Mamatha et al.[44] Presented Methodology 91.07% 86% 88% 92.5%

73

5.2 Future Work
Future research based on this work can be done in several aspects in three main areas:  The output image of the CGF can be used for more advanced image processing techniques including classification of image pixels into vessels or non-vessels; this can be helpful in filtering blood vessels [45].  After segmenting regions of bleeding as circles, more research is needed to segment hemorrhages with clear boundaries using the location of extracted circles.  The classifier has shown better results when features of normal images were extracted separately. Although the defined features are enough to distinguish between normal and abnormal cases, future research might be needed to better extract features of DIARETDB images to solve the problem of inadequate color.

5.3 Summary
Early detection of DR prevents blindness [46]. In the past years, many researchers have

developed algorithms for detecting DR [47]. In this paper, a novel approach for automatic detection and classification of DR has been presented. The approach is based on a combination of several image processing techniques including CHT, CLAHE, thresholding, Gabor filter and SVM. The methodology has shown perfect results on images from DIARETDB. Furthermore, the proposed methodology also showed excellent results on selected images from DRIVE and STARE. In summary, the major contributions of this study are: (1) Proposing a new method called CGF which can be used for edge enhancement and objects extraction in fundus images. The CGF performs well using different types of retinal fundus images.

74

(2) An algorithm for automatic segmentation of DR lesions has been proposed. The results prove that such methodology accurately segments hard exudates, soft exudates and bleeding regions from images of different databases. (3) In this work, ten features of fundus images are presented. The SVM uses the features of total area of DR lesions and the cumulative intensity to classify retinal images. The results show that the selected features are perfect for this purpose. Using these features SVM could classify fundus images with 90% sensitivity and 92.5% accuracy. To conclude, both researchers and physicians can benefit from the results of this study. For researchers, the CGF can be used as a separate tool for processing an image before applying advanced algorithms. On the other hand, doctors can use the algorithm for automatic segmentation of DR lesions as a tool to locate hemorrhages and exudates.

75

5.4 References
[1]

J. Vislisel and T. Oetting, "Diabetic Retinopathy: from one medical student to another," Department of Ophthalmology and Visual Sciences, University of Iowa, September, 2010.

[2]

H. Li, and O. Chutatape, "Fundus image features extraction," 22nd Annual EMBS International Conference, Chicago, Vol. 4, pp. 3071-3073, July 2000.

[3]

K. Narasimhan, V. Neha and K. Vjayarekha, "A review of Automated Diabetic Retinopathy Diagnosis from Fundus Image," Journal of Theoretical and Applied Information Technology, Vol. 39, pp. 225,238, May 2012.

[4]

N. Kleawsirikul, S. Gulati and B. Uyyanonvara, "Automated Retinal Hemorrhage Detection using Morphological Top Hat and Rule-based Classification," 3rd International Conference on Intelligent Computational Systems, April 2013.

[5]

S. B. Junior and D. Walfer, "Automatic Detection of Microaneurysms and Hemorrhages in Color Eye Fundus Images," International Journal of Computer Science and Information Technology, Vol. 5, October 2013.

[6]

R. V. Athira and D. F. Ferlin, "Detection of Retinal Hemorrhage using Splat Feature Classification Technique," International Journal Of Engineering Research and Applications, Vol. 4, pp. 327-330, January 2014.

[7]

R. Inbarathi and R. Karthikeyan, "Detection of Retinal Hemorrhage in Fundus Images by classifying the Splat Features using SVM," International Journal of Innovative research in Science, Engineering and Technology, Vol. 3, pp. 1979-1986, March 2014.

[8]

M. Reshma and M. Chavan, "Detection of Hemorrhages from Fundus Images using Hybrid Method," International Journal of Computer Applications, Vol. 12, December 2014.

[9]

A. Jadhav and P. Patil, "Classification of Diabetes Retina Images using Blood Vessel Area," International Journal of Cybernetics, Vol. 4, April 2015. 76

[10]

A. W. Reza, C. Eswaran and S. Hari, "Automatic Tracing of Optic Disc and Exudates from color Fundus Images Using Fixed and Variables Thresholds," Journal of Medical Systems, Vol. 33, pp. 73-80, April 2008.

[11]

A. Tripathi, K. K. Singh, B. K. Singh and A. Mehrotra, "Automatic Detection of Exudates in Retinal Fundus Images using Differential Morphological Profile," International Journal of Engineering and Technology, Vol. 3, pp. 2024-2029, July 2013.

[12]

R. F. Mansour, E. Md. Abdelrahim and A. S. Al-Johani, "Identification of Diabetic Retinal Exudates in Digital Color Images Using Support Vector Machine," Journal of Intelligent Learning Systems and Applications, pp. 134-142, August 2013.

[13]

M. Akter, A. Rahman and A.K.M. Islam, "An improved method of automatic exudates detection in retinal images," International Journal Innovative Research in Electrical, Electronics, Instrumentation and Control Engineering, Vol. 2, May 2014.

[14]

N. Prakash, G. Hemalakshmi and M. Mary, "Automated Grading of Diabetic Retinopathy Stages in Fundus Images using SVM Classifier," Journal of Chemical and Pharmaceutical Research, Vol. 8, 2016.

[15]

J. Kuar and D. Mittal, "Segmentation and Measurement of Exudates in Fundus Images of the Retina for Detection of Retinal Disease," Journal of Biomedical and Medical Imaging, Vol. 2, February 2015.

[16]

D. Selvathi, and P. Lalitha Vaishnavi, "Gabor wavelet based blood vessel segmentation in retinal images using kernel classifiers," International Conference on Signal Processing, Communication, Computing and Networking Technologies, pp. 830-835, 2011.

[17]

Y. Hou, "Automatic Segmentation of Retinal Blood Vessels Based on Improved Multiscale Line Detection," Journal of Computing Science and Engineering, Vol. 8, pp. 119 -128, June 2014. 77

[18]

E. Ricci, and R. Perfetti, "Retinal Blood Vessel Segmentation Using Li ne Operators and Support Vector Classification," IEEE Transactions on Medical Imaging, Vol. 26, pp. 1357 1365, October 2007.

[19]

B. Pradeepa, and J. Raja, "Automated Blood Vessel Segmentation in Retinal Image," International Journal of Engineering and Computer Science, vol. 3, pp. 5886-5890, May 2014.

[20]

S. Shajahan, and C. Roy, "An Improved Retinal Blood Vessel Segmentation Algorithm based on Multistructure Elements Morphology," International Journal of Computer

Applications, vol. 57, November 2012.
[21]

D. Raja, S. Vasuki, and R. Kumar, "Performance analysis of retinal image blood vessel segmentation," Advanced Computing, vol. 5, May 2014.

[22]

M. Kaur and R. Talwar, "Automatic Extraction of Blood Vessels and Eye Retinopathy Detection," European Journal of Advances in Engineering and Technology, Vol. 2, 2015.

[23]

A. Dehghani, H.A. Moghaddam and M.S. Moin, "Optic disc localization in retinal images using Histogram matching," EURASIP Journal on image and Video Processing, December 2012.

[24]

M. Foracchia, E. Grisan and A. Ruggeri, "Detection of Optic Disc in Retinal Images by Means of a Geometrical Model of Vessel Structure," IEEE Transactions on Medical Imaging, Vol. 23, pp. 1189-1195, October 2004.

[25]

S. Sekhar, W. AL Nuaimy and K. Nandi, "Automated Localization of Optic Disc and Fovea in Retinal Fundus Images," 16th European Signal Processing Conference, August 2008.

[26]

P. C. Siddalingeswamy and K. G. Prabhu, "Automatic Localization and Boundary Detection of Optic Disc Using Implicit Active counters," International Journal of Computer Applications, Vol. 1, February 2010. 78

[27]

K. Wisaeng, N. Hiransakolwong and E. Pothiruk, "Automatic Detection of Optic Disc in Digital Retinal Images," International Journal of Computer Applications, Vol. 90, March 2014.

[28]

C. Aravind, M. PonniBala and S. Vijayachitra, "Automatic Detection of Microaneurysms and Classifications of Diabetic Retinopathy Images using SVM Technique," International Journal of Computer Applications, 2013.

[29]

S. SujithKumar and S. Vipula, "Automatic Detection of Diabetic Retinopathy in non-detailed RGB Retinal Fundus Images," International Journal of Computer Applications, vol. 47, pp. 26-32, June 2012.

[30]

H. Tjandrasa and I. Anggoro, "Classification of Non-Proliferative Diabetic Retinopathy Based on Segmented Exudates using K-Means Clustering," International Journal on Image, Graphics and Signal Processing, pp. 1-8, 2015.

[31]

C. Sundhar and D. Archana, "Automatic Screening of Fundus Images for Detection of Diabetic Retinopathy, "International Journal of Communication and Computer Technologies, vol. 2, April 2014.

[32]

C. Jagatheesh and C. Jenila, "Automatic Detection and Classification of Diabetic Retinopathy Lesion Using Bags of Visual Words Model," International Journal of Scientific and Research Publications, Vol. 5, September 2015.

[33]

R. Welikala, M. Fraz, T. Williamson and S. Barman, "The Automated Detection of Proliferative Diabetic Retinopathy using Dual Ensemble Classification," International Journal of Diagnostic Imaging, Vol. 2, 2015.

[34]

E. Bart and S. Ullman, "Image Normalization by Mutual Information," Department of Computer Science and Applied Mathematics, Weizmann Institute of Science.

79

[35]

T.J. Atherton, D.J. Kerbyson. "Size invariant circle detection." Image and Vision Computing, Volume 17, Number 11, 1999, pp. 795-803.

[36]

T. Atherton and D. Kerbyson, "The Coherent Circular Hough Transform," Department of Computer Science, University of Warwick.

[37]

A. Hassanein, S. Mohammad, M. Sameer and M. Ragab, "A survey on Hough Transform, Theory, Techniques and Application, Informatics Department, Electronic Research Institute, Egypt.

[38]

K. Zuiderveld "Contrast Limited Adaptive Histogram Equalization." Graphic Gems IV. San Diego: Academic Press Professional, 1994. 474Â­485.

[39]

S. Al-amri, N. Kalyankar and S.Khamitkar, "Image Segmentation by Using Threshold Technique," Journal of Computing, V. 2, May 2010.

[40]

C. Dhaware and K. Wanjale, "Survey on Image Classification Methods in Image Processing, "International Journal of Computer Science Trends and Technology," Vol. 4, June, 2011.

[41]

A. Biran, P. Bidari, A. Almazroa, V. Lakshminarayanan and K. Raahemifar, "Blood Vessels Extraction from Retinal Images Using Combined 2D Gabor Wavelet Transform with Local Entropy Thresholding and Alternative Sequential Filter," 29th Canadian Conference in Electrical and Computer Engineering, May, 2016.

[42]

S. Gupta and A.M. Karandikar, "Diagnosis of Diabetic Retinopathy using Machine Learning," Journal of Research and Development, Vol. 3, August 2015.

[43]

M. D. Abramoff, M. Niemeijer, M. S. Suttorp-Schulten, M. A. Viergever, S. R. Russell and B. Van Ginneken, "Evaluation of a system for automatic detection of diabetic retinopathy from color fundus photographs in a large population of patients with diabetes," Diabetes Care, Vol. 2, pp.193-198, February, 2008.

80

[44]

B.V Mamatha, R.Srilatha, D. Deepashree and K. Prasanna "A survey on Different Classifiers for Medical diagnosis and Grading: Application to Diabetic Retinopathy," International Journal of Health Care Sciences, Vol .2, pp. 210-216, October 2015.

[45]

M. Garg and S. Gupta, "Retinal Blood Vessel Segmentation Algorithms: A comparative Survey," International Journal of Bio-science and Bio-Technology, Vol. 8, 2016.

[46]

K.mahesh, "A survey on Automated Techniques for Retinal Disease Identification in Diabetic Retinopathy," International journal of Advancements on Research and Technology, Vol. 2, May 2013.

[47]

M. Akter and M. Shorif Uddin, "A review on Automated Diagnosis of Diabetic Retinopathy," International Journal on Advanced Computer Technology, Vol. 3, October 2014.

81

