applied sciences
Article

Using Multispectral Airborne LiDAR Data for Land/Water Discrimination: A Case Study at Lake Ontario, Canada
Salem Morsy *
ID

, Ahmed Shaker and Ahmed El-Rabbany

Department of Civil Engineering, Ryerson University, 350 Victoria Street, Toronto, ON M5B 2K3, Canada; ahmed.shaker@ryerson.ca (A.S.); rabbany@ryerson.ca (A.E.-R.) * Correspondence: salem.morsy@ryerson.ca; Tel.: +1-41-6979-5000 (ext. 4623) Received: 13 January 2018; Accepted: 24 February 2018; Published: 28 February 2018

Abstract: Coastal areas are environmentally sensitive and are affected by nature events and human activities. Land/water interaction in coastal areas changes over time and, therefore, requires accurate detection and frequent monitoring. Multispectral Light Detection and Ranging (LiDAR) systems, which operate at different wavelengths, have become available. This new technology can provide an effective and accurate solution for the determination of the land/water interface. In this context, we aim to investigate a set of point features based on elevation, intensity, and geometry for this application, followed by a presentation of an unsupervised land/water discrimination method based on seeded region growing algorithm. The multispectral airborne LiDAR sensor, the Optech Titan, was used to acquire LiDAR data at three wavelengths (1550, 1064, and 532 nm) of a study area covering part of Lake Ontario in Scarborough, Canada for testing the discrimination methods. The elevationand geometry-based features achieved an average overall accuracy of 75.1% and 74.2%, respectively, while the intensity-based features achieved 63.9% accuracy. The region growing method succeeded in discriminating water from land with more than 99% overall accuracy, and the land/water boundary was delineated with an average root mean square error of 0.51 m. The automation of this method is restricted by having double returns from water bodies at the 532 nm wavelength. Keywords: multispectral LiDAR; discrimination; region growing; point density; height variation

1. Introduction Water bodies are one of the irreplaceable natural resources, which are subject to intense exploitation and require monitoring for their sustainable management. Land/water discrimination is a valuable topic in terms of the services it provides to decision makers such as water area estimation [1,2], flood monitoring [3,4], flood disaster assessment [5­7], hydrological regulation and erosion control [8], and water resources management [9­11]. 1.1. Related Work Over the past decades, remotely sensed data have been employed for bathymetric applications. Airborne Light Detection and Ranging (LiDAR) Bathymetry (ALB) systems have been developed to measure water depth in coastal zones [12]. ALB systems typically operate at dual-wavelengths, namely near infrared (1064 nm) and green (532 nm), in order to detect water surfaces and benthic layers. Allouis et al. [13] presented a combination of mathematical and heuristic methods to estimate the water depth in very shallow water areas. The LiDAR waveforms, recorded at the near infrared and green wavelengths, were first synchronized, and the peak of the waveform at near infrared wavelength was amplified. The approximate positions of the water surface and benthic layer were then performed by simple maxima detection algorithm. After that, a non-linear least square fitting of a sum of two
Appl. Sci. 2018, 8, 349; doi:10.3390/app8030349 www.mdpi.com/journal/applsci

Appl. Sci. 2018, 8, 349

2 of 21

Gaussian functions on the green signal were conducted. Finally, the water depth was calculated from the accurate estimation of the water surface and benthic layer positions, and the minimum detectable depth was around 1 m. This approach is found to be computationally expensive and requires many processing steps, as well, the delineation of land/water interface was not reported in that study. When the green laser contacts water molecules, a small portion of energy is returned at a wavelength of 647 nm, which is called the red (Raman) channel [14]. Pe'eri and Philpot [15] analyzed the recorded waveform at the Raman channel, which represents the received intensity (digital number) relative to time (in nanoseconds), and divided it into 41 bins. Then, a normalized difference index (NDI) was created between bin 11 and bin 27 to discriminate water from land by applying a threshold value to the NDI. The LiDAR measurements were manually classified into land, water, and suspected as water. Despite the good achieved results (97%), there is a transition class "suspected as water" that cannot be identified as land or water. In addition, the manual processing of the waveform is required for each tested dataset. Over the past years, land/water discrimination has been investigated using LiDAR data collected by monochromatic-wavelength airborne LiDAR systems. LiDAR point features such as point density [16­18], roughness [18,19], and intensity variation [19] were extracted and used in the discrimination process. However, data-driven threshold values were manually selected to make those point features fit the tested data, which means the existing methods are ad hoc. In addition, the existing methods require prior knowledge of the land/water interface to aid in the supervised discrimination process. Antonarakis et al. [20] introduced a supervised object-orientated approach to separate the water bodies from land objects in urban areas. The water surfaces were assumed to have low elevation and intensity values so that the LiDAR points within a specific height range and average intensity values of less than specific threshold values were classified as water points. The manual setting of such threshold values leads to more than a 95% accuracy index for water class. Smeeckaert et al. [17] used a set of features to train a Support Vector Machine (SVM) algorithm in order to classify water areas. The results were then refined by incorporating contextual knowledge to remove pixel-wise misclassification. Their proposed work achieved an overall accuracy of more than 95% in coastal areas. Crasto et al. [18] developed a decision tree to classify water areas based on point density, elevation standard deviation, and intensity standard deviation of the LiDAR data. The results revealed that the water areas were classified with more than 95% accuracy compared to manually interpreted data. Brzank et al. [16] classified the water surface in the Wadden Sea using a supervised fuzzy classifier based on height, intensity, and 2D point density. For each feature, a membership value (between 0 and 1) was assigned to each LiDAR point, where the values "0" and "1" indicate the mudflat and water class, respectively. Two study areas were tested, and the results yielded a completeness and correctness value up to 99.2% and 98.5%, respectively. Schmidt et al. [21] introduced a point-wise supervised classification method based on geometric and intensity features. Eight features were extracted and used to build a classifier based on conditional random fields to distinguish between water, mudflat, and mussel bed classes at the Wadden Sea. The water class was classified with a completeness of 82.4% and a correctness of 66.3%. Höfle et al. [19] proposed a workflow for water surface classification and land/water boundary delineation based on a region growing algorithm. The seed points were first selected based on a minimum intensity density threshold value. The growing criteria were conducted by applying threshold values on the height difference, intensity density, and intensity variation in order to create segments. All segments were then classified into land and water segments by applying a threshold on minimum segment size and mean segment roughness. The algorithm demonstrated an overall classification accuracy of 97% with a root mean square error of 0.45 m for the land/water boundaries. Despite achieving a higher overall accuracy, this method requires a significant pre-processing step, which is the model of LiDAR data dropouts. This step cannot be conducted without the knowledge of the pulse repetition frequency, Global Positioning System (GPS) timestamps, and the scan direction.

Appl. Sci. 2018, 8, 349

3 of 21

In addition, many threshold values were manually selected to fit the tested data. Nevertheless, this method could not discriminate water surface from asphalt because they both exhibited low intensity and smooth surface. Hooshyar et al. [22] delineated wet channels using LiDAR intensity and elevation data. The vegetation canopy was first filtered out by applying a threshold value on elevations, and a digital elevation model (DEM) was created. Potential wet channels extents were then extracted based on their geomorphologic characteristics. In parallel, an intensity image was created from ground returns, and a Gaussian mixture model was used to decompose the intensity image into three classes, namely wet, transition and dry classes. The edges of wet channels were then extracted from the gradient of the intensity image. Finally, the networks, extracted from the LiDAR DEM and the intensity image were overlaid. However, the final network was disconnected because of the mismatch between the two networks. This is mainly due to distortion of the DEM through the generation process or error in the DEM itself. Another possible reason for disconnection is that the low quality of data and missing intensity data. The supervised machine learning techniques such as SVM [17], decision trees [18], or random forest are attributed to the amount of available field sample data. These samples are used as training data, which are usually limited, and affect the achieved accuracy by those methods [23]. In addition, the output results of those methods are produced using data-driven threshold values, which cannot be generalized for any dataset under different conditions. We aim in this research to present an unsupervised and fully automated land/water discrimination method using LiDAR data acquired at different wavelengths. In addition, we use the seeded region growing algorithm that considers the spatial coherence of the LiDAR data by using the neighboring points of each LiDAR point. 1.2. Multispectral LiDAR Systems Recently, several attempts have been conducted to use multispectral LiDAR data collected from either laboratory-based multispectral LiDAR systems or terrestrial laser scanning (TLS) for vegetation applications. Woodhouse et al. [24] used laboratory-based measurements from a tunable laser operating at four wavelengths of 531, 550, 660, and 780 nm. The normalized difference vegetation index and photochemical reflectance index [25] were then used to measure plant structure and its interaction with the environment. Shi et al. [26] used a laboratory-based prototype of multispectral LiDAR, designed by [27], to record the reflectance values at four different wavelengths (556, 670, 700 and 780 nm). The physiology of the canopy was then studied using three vegetation indexes, and the reported overall classification accuracy yielded above 90%. Regarding the TLS platforms, Puttonen et al. [28] used a TLS developed by the Finnish Geodetic Institute, called the Hyperspectral LiDAR system. The system was tested in an outdoor experiment using seven wavelength bands ranging from 500 to 980 nm to discriminate man-made targets from vegetation based on their spectral response. Four vegetation indexes were used in the classification process and achieved an overall accuracy of up to 91%. Other studies used the TLS system for measuring the three-dimensional structure of forest canopies by two lasers with wavelengths of 1063 nm and 1545 nm [29] and discriminating leaves from woody material in forestry areas by using data collected at dual-wavelengths (i.e., 1064 and 1548 nm) [30]. A few attempts have been conducted to combine different flight missions from airborne LiDAR systems of the same study area [31,32]. However, it was difficult to use the intensity data from different missions because those flight missions were conducted at different times. Thus, the surface conditions were not identical and, hence, intensity values were affected. Teledyne Optech has recently launched the first commercial multispectral airborne LiDAR sensor, the Optech Titan. Optech Titan's typical configuration includes three active laser beams operating simultaneously at three wavelengths, collecting data in three channels. The order of the channels according to their wavelengths are as follows: C1 with wavelength of 1550 nm at 3.5 forward looking, C2 with wavelength of 1064 nm at 0 nadir looking, and C3 with wavelength of 532 nm at 7 forward

Appl. Sci. 2018, 8, 349

4 of 21

looking. The beam divergence of C1 and C2 is 0.35 mrad, while the beam divergence of C3 is 0.7 mrad. The preferable flying height for topographic applications is between 300 and 2000 m above ground level (AGL), while the flying height is between 300 and 600 m AGL for bathymetric applications. The pulse repetition frequency of each laser beam varies from 50 to 300 kHz. The scan frequency can be programmed to take values between 0­210 Hz. The scan angle (field of view) is programmable in the range from 0 to 60 [33,34]. A few studies have been conducted to explore multispectral LiDAR data collected by the Optech Titan in land cover classification. Raster images were created from the LiDAR intensity and height data, and image classification techniques were then applied [35­38]. As well, the multispectral LiDAR data were explored for 3D point classification in urban areas [39­42]. Fernandez-Diaz et al. [34] presented an overview on the use of the multispectral Titan data in different applications, such as land cover classification, measuring water depths in shallow water areas, and forestry mapping, however, nothing was reported on land/water discrimination. In a previous work conducted by the authors on land/water discrimination, we defined three normalized difference feature indexes (NDFIs) based on the intensity data acquired at the three wavelengths by the Optech Titan sensor [43] as follows: NDFIC3-C2 NDFIC3-C1 NDFIC2-C1 I C3 - I C2 I C3 + I C2 I C3 - I C1 I C3 + I C1 I C2 - I C1 I C2 + I C1 (1)

(2) (3)

where IC1 , IC2 , and IC3 are the intensity from the C1, C2, and C3, respectively. The results showed that the NDFIs could not be directly used for land/water discrimination. This is because of the recorded intensity values of vegetation and water are very close in each channel. Consequently, water points are misclassified as vegetation points [43]. The green wavelength has the ability to penetrate water bodies, allowing it to detect water surfaces and/or benthic layers [12­15], while the infrared wavelengths can detect the water surface. This adds a new capability to the LiDAR system, especially at the land/water interface. Therefore, the dual-wavelength or multispectral airborne LiDAR system, including the green wavelength, is expected to aid in the automation of the land/water discrimination process. Also, to the best of the authors' knowledge, there is a lack of studies using dual-wavelength or multispectral LiDAR data in discrete returns format for land/water discrimination. In this research, we aim to (1) evaluate a set of point features, derived from the LiDAR data based on geometric and radiometric attributes, for discriminating water body from land objects; and (2) present an unsupervised land/water discrimination method based on a seeded region growing algorithm using discrete return multispectral LiDAR data. 2. Study Area and Dataset A study area located near Lake Ontario in Scarborough, Ontario, Canada was tested in this research. The study area covers a variety of land features, objects, including bare soil, roads, grass, shrubs, trees, and buildings, as well as part of Lake Ontario (Figure 1), and it includes a land/water interface with a gentle slope. A flight mission was conducted on 3 September 2014 to acquire multispectral LiDAR data using the Optech Titan. The acquired data were received from the supplier as a time-tagged 3D point cloud file with multiple returns in LASer file format (LAS) for each channel. The LAS data files contain xyz coordinates, raw intensity values, scan angle, return number, number of returns, and the GPS time of each LiDAR point.

Appl. Sci. 2018, 8, 349 Appl. Sci. 2018, 8, x FOR PEER REVIEW Appl. Sci. 2018, 8, x FOR PEER REVIEW

5 of 21 5 of 21 5 of 21

Figure 1. An image of the study area. The red rectangle indicates the LiDAR LiDAR data data coverage. coverage. Figure 1. 1. An Animage imageof ofthe thestudy studyarea. area. The Thered redrectangle rectangle indicates indicates the the Figure LiDAR data coverage.

A 400 m by 200 m subset of LiDAR data was clipped for the experimental testing. Table 1 A 400 400 m m by by 200 m subset of LiDAR data was clipped for the experimental testing. Table Table 1 1 A summarizes the specifications of the data subset. The LiDAR data of the three channels are shown in summarizes the the specifications specifications of of the the data data subset. subset. The The LiDAR LiDAR data data of of the the three three channels channels are are shown shown in in summarizes Figure 2. The difference in the number of points between channels is attributed to the interaction of Figure 2. 2. The The difference difference in in the the number number of of points points between between channels channels is is attributed attributed to to the the interaction interaction of of Figure each feature with different wavelengths (e.g., reflection from the water surface and/or water benthic each feature feature with with different different wavelengths (e.g., reflection reflection from the water surface and/or water benthic benthic each and/or water layer, and greenness of the vegetation). layer, and greenness of the vegetation). layer,
Table 1. Data Data specifications. specifications. Table 1. 1. Table Data specifications. Specification Specification Specification Channel 1 = 1550 nm; Channel 2 = 1064 nm; Channel 3 = 532 nm Channel 1 = 1550 nm; Channel 2 = 1064 nm; Channel 3 = 532 nm ~4302m Channel 1 = 1550 nm; Channel = 1064 nm; Channel 3 = 532 nm ~430 m ~430 m ±15° ±15° ±15 Channel 3 = 0.7 mrad Channel 1 and 2 = 0.35 mrad,; Channel 1 and 2 = 0.35 mrad,; Channel 3 3 = 0.7 mrad Channel200 1 and 2 = 0.35 mrad; kHz/channel; 600Channel kHz total = 0.7 mrad 200 kHz/channel; 600 kHz total 200 kHz/channel; 600 kHz total 40 Hz 4040 Hz Hz Up to 4 returns Up 4 returns Up toto 4 returns Channel 1 = 971,490; Channel 2 = 1,123,418; Channel 3 = 1,403,386 Channel 1= 971,490; Channel 2 1,123,418; = 1,123,418; Channel 3 1,403,386 = 1,403,386 Channel 1= 971,490; Channel 2= Channel 3=

Parameter Parameter Parameter Wavelength Wavelength Altitude Wavelength (Above Ground Level) Altitude (Above Ground Level) Altitude (Above Ground Level) Scan Angle ScanAngle Angle Scan Beam Divergence BeamDivergence Divergence Beam Pulse Repetition Frequency PulseRepetition RepetitionFrequency Frequency Pulse Scan Frequency ScanFrequency Frequency Scan Number of Returns/Pulse Number of Returns/Pulse Number of Returns/Pulse Number of Points Number Numberof ofPoints Points

As shown in Figure 2, the highest elevation in the three channels is very close, while the lowest As in Figure 2, highest elevation in the channels is while the As shown shown Figure 2, the the highest the three three channels is very very close, close, the lowest lowest elevation in C3 in is lower than that in C1 elevation and C2 byin about 5 m. This is attributed to thewhile recorded points elevation in C3 is lower than that in C1 and C2 by about 5 m. This is attributed to the recorded points elevation in C3 is lower than that in C1 and C2 by about 5 m. This is attributed to the recorded points from the water benthic layer. The intermediate part of the water body has high intensity variation in from the water benthic layer. The intermediate part of the water body has high intensity variation in from the water layer. The intermediate part ofa the water body high intensity variation in all channels. In benthic C2, the high vegetation (i.e., trees) has relatively highhas intensity variation. The water all channels. In vegetation (i.e., trees) has high intensity variation. The all channels. InC2, C2,the thehigh high vegetation (i.e., trees) has a a relatively relatively high intensity variation. The water water points have single returns in C1 and C2, while most of them have double returns in C3 due to the points have single returns in C1 and C2, while most of them have double returns in C3 due to the points have single returns in C1 and C2, while most of them have double returns in C3 due to the reflectance from the water surface as well as water benthic layer as it is a shallow water area. reflectance reflectance from from the water surface as well as water benthic layer as it is a shallow water area. C1 C1 Elevation Elevation C2 C2 C3 C3

91.60 91.60 38.68 38.68

89.48 89.48 38.64 38.64
Figure 2. Cont.

92.43 92.43 33.17 33.17

Appl. Sci.2018 2018 x FOR PEER REVIEW Appl. Sci. ,, 88 ,, 349 Appl. Sci. 2018, 8, x FOR PEER REVIEW

6 of 6 of 2121 6 of 21

Intensity Intensity

4090 4090 1 1

4085 4085 1 1

4086 4086 1 1

# of Returns # of Returns
Figure 2. Light Detection and Ranging (LiDAR) point clouds from the three channels for a subset of Figure 2. Light Detection and Ranging (LiDAR) point clouds from the three channels for a subset of the study area colored by elevation; intensity; and number of returns. Figure 2. Light Detection Ranging (LiDAR) point clouds from the three channels for a subset of the study area colored by and elevation; intensity; and number of returns. the study area colored by elevation; intensity; and number of returns.

3. Methodology 3. Methodology 3. Methodology 3.1. Point Features Extraction 3.1. Point Features Extraction 3.1. Point Features Extraction set point features could be extracted for each LiDAR point.These Thesepoint pointfeatures featuresare are AA set ofof point features could be extracted for each LiDAR point. divided into three categories, namely elevation-based features including height variation ( HV ) and A set of point features could be extracted for each LiDAR point. These point features are divided into three categories, namely elevation-based features including height variation (HV ) and height standard deviation (HSD intensity-based features including intensity coefficient variation divided into three categories, namely elevation-based features including height variation (variation HV ) and height standard deviation (HSD ); ); intensity-based features including intensity coefficient ofof ( ICOV ) and intensity density ( ID ); and geometry-based features including point density ( PD and height standard deviation ( HSD ); intensity-based features including intensity coefficient of variation (ICOV ) and intensity density (ID); and geometry-based features including point density (PD) ) and number of returns ( NOR ). Figure 3 shows the workflow of land/water discrimination using different ( ICOV) and intensity density (ID); and geometry-based features including point density ) and number of returns (NOR ). Figure 3 shows the workflow of land/water discrimination using(PD different point features. number of returns ( NOR ). Figure 3 shows the workflow of land/water discrimination using different point features. point features.

Figure 3. Workflow of land/water discrimination based on LiDAR point features and seeded region Figure algorithm. 3. Workflow of land/water discrimination based on LiDAR pointICOV features and seeded region growing HV : Height Variation; HSD: Height Standard Deviation; : Intensity Coefficient growing algorithm. HV : Height Variation; HSD : Height Standard Deviation; ICOV : Intensity Figure 3. Workflow of land/water discrimination based on LiDAR point features and seeded region of Variation; ID: Intensity Density; PD: Point Density; and NOR: Number of Returns. Coefficient of Variation; : Intensity Density; PD: Point Density; and NOR : Number of Returns. growing algorithm. HV: ID Height Variation; HSD Height Standard Deviation; ICOV : Intensity

Coefficient of Variation; ID: Intensity Density; PD: Point Density; and NOR: Number of Returns.

Appl. Sci. 2018, 8, 349

7 of 21

The water points were labeled based on the following assumptions. First, the water points were assumed to have the lowest intensity in the scene [16,20]. Second, the intensity variation was higher in water bodies than in the land areas [18,19]. This is attributed to the laser pulse angle of incidence with respect to the water surface, which relies on the scan angles, water surface roughness (i.e., waves), the and aircraft's attitude. Third, the C1 and C2 were used to acquire LiDAR data from water surfaces, while the green wavelength was used to acquire LiDAR data from both water surfaces and benthic layers due to its ability to penetrate water bodies. Thus, these characteristics increased the usefulness of the LiDAR data in land/water discrimination, when the C3 was combined with C1 or C2. The LiDAR point features were extracted based on a local neighborhood of each point, where neighboring points were considered using a fixed searching radius in 2D space. A searching radius of 1 m was used in order to define the land/water interface with high resolution and to ensure a sufficient number of points in the neighborhood [44]. The details of those point features are presented in the following subsections, assuming that a LiDAR point pi and its neighboring points are j, where j = 1, 2, 3, ..., n, and n is the total number of neighboring points of pi . 3.1.1. Height Variation (HV ) Height Variation is the difference between the maximum and the minimum elevations of neighboring points of a LiDAR point. The water points were assumed to have high HV C3 values when using the green wavelength in C3 due to the recorded returns from water surfaces and benthic layers. Also, the water points were assumed to have low HV C1 or C2 values when using the infrared wavelengths in C1 or C2 due to the recorded returns from water surfaces only. 3.1.2. Height Standard Deviation (HSD) Height Standard Deviation is the standard deviation of a LiDAR point's elevation in relation to neighboring points. The water points were assumed to create a horizontal surface (i.e., low HSDC1 or C2 ) when using the infrared wavelengths in C1 or C2. The HSDC3 of water points should be increased when using the green wavelength due to the recorded returns from the water surface and benthic layer. The HSD can be calculated from Equation (4), where Zj is the point's elevation and Z is the mean elevation. 1 n 2 or C2 or C3 ( Zj - Z ) (4) HSDC1 =  pi n - 1 j =1 3.1.3. Intensity Coefficient of Variation (ICOV ) Intensity Coefficient of Variation is calculated by dividing the standard deviation by the mean of the intensity values. The water points were assumed to have ICOV C1 or C2 or C3 greater than those of land objects. The reason for this is the intensity variation in water bodies is relatively high as a result of different incidence angles. For each LiDAR point in any channel, the ICOV C1 or C2 or C3 can be calculated from Equation (5), where Ij is the point's intensity value and I is the mean of the intensity values.
1 n -1 C1 or C2 or C3 ICOVp = i j =1 1 n

 ( Ij - I )  Ij
n

n

2

(5)

j =1

3.1.4. Intensity Density (ID) Within the searching radius, the IDC1 or C2 or C3 is the percentage of LiDAR points that have intensity values below a threshold value (Ithresh ) as shown in Equation (6). The Ithresh was identified using the Jenks natural breaks optimization method as explained in detail in Section 3.2. The water

Appl. Sci. 2018, 8, 349

8 of 21

points were assumed to have high IDC1 or C2 or C3 . This is due to the water points usually having the lowest intensity values in the scene.
or C2 or C3 IDC1 = pi C1 or C2 or C3 no. of points with I  Ithresh · 100 n

(6)

3.1.5. Point Density (PD) Point density refers to the number of LiDAR points per square meter. The water points were assumed to have low PDC1 or C2 when using the infrared wavelengths in C1 or C2, and high PDC3 when using the green wavelength in C3 due to the presence of additional points from benthic layers. 3.1.6. Number of Returns (NOR) The NOR is defined as the recorded number of returns for each laser pulse. Table 2 shows the number of possible returns that can be recorded using the infrared and green wavelengths. The water point should have a single return when using the infrared wavelengths in C1 or C2 due to the recorded returns from the water surface only, and the water points should have double returns when using the green wavelength in C3 due to the recorded returns from water surfaces and benthic layers.
Table 2. Recorded returns from different classes at infrared and green wavelengths. Classes Built-up areas Water Vegetation Infrared Wavelengths single/multiple single single/multiple Visible Wavelengths single/multiple single/double single/multiple

The point feature was extracted considering single or double channels. For instance, the evaluation of ICOV and ID features were conducted using the data of a single channel (C1, C2, or C3), thus, different results were obtained from the three channels for each point feature. The HV, HSD, PD, and NOR features were conducted using the combined data from two channels (i.e., C3 with C1 or C3 with C2) and, hence, two different results were obtained. 3.2. Land/Water Points Labeling Based on LiDAR Point Features We used different methods and/or criteria to automatically define the threshold values and then apply them to each LiDAR point feature in order to label water and land points as follows. For NOR, double returns were recorded from the water body in C3, while single returns were recorded in C1 or C2. Based on these characteristics, the water and land points were labeled. Previously, the water points were labeled from single wavelength LiDAR data based on HSD [18,19] or PD [16­18] by applying manually selected threshold values. In this research, a combination of C3 with C1 or C2 was used to automatically label water points. The values of HV, HSD, and PD are higher for C3 than C1 or C2. Therefore, a LiDAR point was labeled as land or water as follows: C3 > HV C1 or C2 pi is a water point, if HVp pi i (7) pi is a land point, otherwise
C1 or C2 pi is a water point, if HSDC3 pi > HSD pi

pi is a land point,

otherwise

(8)

C1 or C2 pi is a water point, if PDC3 pi > PD pi

pi is a land point,

otherwise

(9)

Appl. Sci. 2018, 8, 349

9 of 21

The Jenks natural breaks optimization method was used to determine threshold values for ICOV and ID. This optimization method has been designed to minimize within-class variances and maximize the between-classes variance [45]. Let any of the aforementioned LiDAR point feature values range from [a, ..., b], and the threshold values t  [a, ..., b]. The t was identified in order to separate land from water by maximizing the between-classes sum of the squared mean differences. This was done as follows: t = argmaxatb {( M1 - M )2 + ( M2 - M)2 } (10) where M is the mean of point feature values, M1 and M2 are the mean values of the first and second classes, respectively. M was first calculated. Then, the points were divided into two classes with ranges [a, ..., t] and [t, ..., b]. The mean values M1 and M2 were calculated. Finally, the optimal threshold value t was obtained from Equation (10). The tested data was assumed to consist of three main classes: water, vegetation, and built up areas. Therefore, two threshold values were identified using the Jenks optimization based on intensity. As the water points were assumed to have the lowest intensity in the scene, the first threshold value (Ithresh ) was used to label water points. If the point's intensity is lower than the Ithresh , the point was labeled as water; otherwise it was labeled as land. The Ithresh was used to calculate the ID from Equation (6), and a percentage IDthresh was used. For ICOV, the LiDAR point clouds were separated into land and water using a threshold value (ICOVthresh ). This was done as follows:
C1 or C2 or C3 > ICOV pi is a water point, if ICOVp thresh i

pi is a land point,

otherwise

(11)

or C2 or C3 > ID pi is a water point, if IDC1 thresh pi

pi is a land point,

otherwise

(12)

3.3. Seeded Region Growing for Labeling Water Points Seeded region growing is a pixel-based image segmentation algorithm [19,46­48]. In this research, the seeded region growing algorithm was applied on the 3D LiDAR points. The algorithm is divided into two main steps: selection of seed points and criterion for region growing (Figure 3). Previous studies mainly relied on manually selecting seed points or applying data-driven threshold values on the LiDAR data. Also, in region growing criteria, threshold values were manually selected to fit the tested data. We aimed to present a fully automated and unsupervised method for labeling water points based on seeded region growing using the automated extracted point features NOR and HV. In seed points selection, the point feature "NOR" was used to find the possible seed points. As mentioned previously, the water points have single and double returns when the data are collected at the infrared and green wavelengths, respectively. First, the points that have single returns (points_single) at the infrared wavelengths (e.g., 1064 nm) were extracted, which included points from built-up areas (building roofs and road surfaces) and vegetation as well as water surfaces. Second, the first points of the double returns (points_first_of_double) at the green wavelength (i.e., 532 nm) were extracted. These points included points from vegetation and may represent built-up areas (from power lines or fences) as well as water surfaces. For each point in points_single, the nearest point within the footprint of the green wavelength from points_first_of_double was found. For simplicity's sake, the footprint was assumed to be a circle with radius "rG ", where rG = 0.5 × altitude × beam divergence ( ). The footprint was used to ensure that the point belonged to the same object. This step is essential for removing points that belong to built-up areas and vegetation with single returns. Thus, all points in points_single that have nearest points from points_first_of_double were considered as the possible seed points. It should be pointed out that vegetation could be recorded with single or more returns at various wavelengths due to different interaction between vegetation and those wavelengths as well as the change of the scan angles. Therefore, the point feature "HV " was used to refine the possible seed points.

Appl. Sci. 2018, 8, x FOR PEER REVIEW
Appl. Sci. 2018, 8, 349

10 of 21
10 of 21

as the change of the scan angles. Therefore, the point feature "HV" was used to refine the possible seed points. The height difference within a searching radius for each point was checked against the variations the point's elevations at the infrared wavelengths inchecked C1 or C2. These variations are The heightin difference within a searching radius for each point was against the variations in because of the presence waves on wavelengths the water surface the These fact that the Titan channels a the point's elevations at of the infrared in C1and or C2. variations are becausescan of the given area slightly different time periods. threshold of 0.5 m was scan usedain order toin preserve presence of in waves on the water surface and the A fact that the Titan channels given area slightly horizontal surfaces and, hence, filter out points. Thus, the remaining points were different time periods. A threshold of 0.5 m non-water was used in order to preserve horizontal surfaces and, considered water seed points. points. Thus, In the region growing criterion, all pointsas in the scene were hence, filter as out non-water the remaining points were considered water seed points. considered asgrowing neighboring points of each water point. points within the neighborhood of In the region criterion, all points in the seed scene were Then, considered as neighboring points of each each point Then, were points arranged according to their of distance to point the seed point. The HV was water seed seed point. within the neighborhood each seed were arranged according to subsequently tested point by point against the variations in the point's' elevations in order to label their distance to the seed point. The HV was subsequently tested point by point against the variations all water points in the scene. in the point's' elevations in order to label all water points in the scene.
3.4. Evaluation Evaluation 3.4. Since aerial aerial images images during during the the LiDAR LiDAR data data acquisition acquisition were were not not available available and and the the study study area area is is Since a coastal coastal area, area, the the elevation elevation was was used used to label the LiDAR points as references. In In coastal coastal areas, areas, the the a water part has the largest area withwith the lowest elevations in the scene. an elevation part usually usually has the largest area the lowest elevations in theTherefore, scene. Therefore, an histogram was first constructed with a bin size of m,size where the range of elevations was divided elevation histogram was first constructed with a1 bin of 1 m,entire where the entire range of elevations into a series of equal intervals (i.e., bins), as shown in Figure 4a­c. Then, the highest was divided into a series of equal intervals (i.e., bins), as shown in Figure 4a­c. peak Then,was the detected highest and considered the and average water surface elevation. elevation of the first peak represents the peak was detected considered the average waterThe surface elevation. The elevation of the first average water level C1 orwater C2. The second peak represents the average water level C3, whereas peak represents the for average level for C1 or C2. The second peak represents thefor average water the first is for returns from theis benthic layer.from A threshold of 0.5 m (half bin size) of was added to level for peak C3, whereas the first peak for returns the benthic layer. A threshold 0.5 m (half the size) average water surface and all points with elevation than or equal to that elevation bin was added to the elevation, average water surface elevation, and allless points with elevation less than or (i.e., 39.5 m) were labeled as water (Figureas 4d­f). equal to that elevation (i.e., 39.5 m)points were labeled water points (Figure 4d­f).
2.5 2 1.5
X: 39 Y: 1.428e+05

10 5

C1 Elevation Histogram

2.5 2

10 5

C2 Elevation Histogram

X: 39 Y: 1.977e+05

1.5 1 0.5 0 30

1 0.5 0 30

40

50

60

70

80

90

100

40

50

60

70

80

90

100

Elevation (m)

Elevation (m)

(a)

(b)

Number of Points

(c)

(d)

(e)

( f)

Figure 4. Elevation Elevationhistogram histogram a) C1; (b) and C2; ( and (cand ) C3; and labeled points on Figure 4. ofof (a)(C1; (b) C2; c) C3; labeled LiDAR LiDAR points based onbased elevation elevation using ( d ) C1; ( e ) C2; and ( f ) C3. using (d) C1; (e) C2; and (f) C3.

To evaluate the success of using the different features and the region growing algorithm in To evaluate the success of using the different features and the region growing algorithm in labeling labeling water points, the confusion matrix was constructed as shown in Table 3. The completeness, water points, the confusion matrix was constructed as shown in Table 3. The completeness, correctness, correctness, and overall accuracy were then calculated for the evaluation of the results between the and overall accuracy were then calculated for the evaluation of the results between the extracted water extracted water points from different LiDAR point features or seeded region growing algorithm and points from different LiDAR point features or seeded region growing algorithm and the labeled water the labeled water points based on elevation. points based on elevation.

Appl. Sci. 2018, 8, 349

11 of 21

Table 3. Confusion matrix elements. Reference Water True Positive (TP) False Negative (FN) Land False Positive (FP) True Negative (TN)

Classified as Water Land

where TP: the point was labeled as water in both classification process and reference data TN: the point was labeled as land in both classification process and reference data FP: the point was labeled as water in classification process and as land in reference data FN: the point was labeled as land in classification process and as water in reference data. The completeness (or recall) indicates how complete were the extracted water points, while the correctness (or precision) indicates how correct were the extracted water points. The overall accuracy indicates how successful was the classification process. The three quality measures are defined by [49] as follows: TP Completeness (%) = × 100 (13) TP + FN Correctness (%) = Overall Accuracy (%) = TP × 100 TP + FP (14) (15)

TP + TN × 100 TP + FP + TN + FN

Since the land/water interface is very important to monitor, the land/water boundary was delineated and compared with the digitized boundary from the reference points. Ten transects were then drawn perpendicular to the land/water boundary. The 2D distances between the reference boundary and the delineated boundary were measured and the root mean square error (RMSE) was calculated. 4. Results and Discussion In this section, we provide the extraction of each LiDAR point feature and its usage in land/water discrimination with the evaluation of the results. As aforementioned, the point features were extracted from the neighboring points within a 2D searching radius of 1 m, and threshold values were applied to each feature to discriminate water from land. The results of the land/water discrimination using elevation-, intensity-, and geometry-based features are presented. Also, the results of using the seeded region growing algorithm and the delineation of land/water boundaries are presented at the end of this section. The LAS data files were converted into ASCII files using lastools (lastools; rapidlasso GmbH; Gilching, Germany) so that they could be processed. The LiDAR point features extraction, the seeded region algorithm, and point labeling were implemented using Matrix Laboratory (MATLAB) (MATLAB2016a; MathWorks; Natick, MA, USA). The threshold values, which were identified using Jenks break optimization method, were selected using the embedded function in ArcGIS (ArcGIS; Esri; Redlands, CA, USA). The accuracy assessment and the LiDAR data visualization were both conducted using ArcGIS as well. 4.1. Land/Water Discrimination from Elevation-Based Features The HV and HSD features were calculated and Equations (7) and (8) were used to label water points. The HV and HSD of water points in C3 were higher than of water points in C1 or C2. Therefore, the water points were labeled using a combination of C1 with C3 or C2 with C3. The labeled points

Appl. Sci. 2018, 8, 349
Appl. Sci. 2018, 8, x FOR PEER REVIEW

12 of 21
12 of 21

based on HVpoints and HSD are inHSD Figure The completeness, correctness and correctness overall accuracy labeled based onshown HV and are5. shown in Figure 5. The completeness, and of the labeled are in Tableare 4. provided in Table 4. overall points accuracy ofprovided the labeled points
C1 with C3 C2 with C3

HSD

HV

Water Land

Figure 5. Labeled LiDAR points based on HV and HSD.
Figure 5. Labeled LiDAR points based on HV and HSD.

TableTable 4. Accuracy measures of of labeled point HVand and HSD . The best results are highlighted 4. Accuracy measures labeled pointbased based on on HV HSD . The best results are highlighted in bold. in bold.
Point Feature Channels Completeness (%) Completeness (%) Land Water 97.5 62.9 Land Water 67.3 96.8 62.9 97.5 97.4 57.5 67.3 96.8 59.8 97.0

Point Feature
HV HSD

Channels
C1C3 C2C3 C1C3 C1C3 C2C3 C2C3

Correctness (%) Land Water
97.7 Land 97.3 97.7 97.4 97.3 97.1 61.1 Water 63.9 61.1 57.8 63.9 59.0

Correctness (%)

Overall Accuracy (%)

Overall Accuracy (%)
75.8 78.3 75.8 72.4 78.3 73.7

HV

C1C3 57.5 97.4 97.4 57.8 72.4 HSD 97.0 97.1 between 59.0the two used 73.7 The discriminationC2C3 based on HV 59.8 and HSD features varies combinations. The reason of this is the objects (e.g., vegetation) that have double or more returns in C3, but have single returns in C1 or C2 were misclassified as water points. On the contrary, water areas that have The discrimination based on HV and HSD features varies between the two used combinations. single returns in C3 were incorrectly classified as land. As a result, the correctness of the The reason of this is the objects (e.g., vegetation) that have double or more returns in C3, but have combination of C1 with C3 is lower than that of C2 with C3 because the classification errors of land single returns in C1 or C2 were misclassified as water points. On the contrary, water areas that have points as water points are much higher in the first combination. In general, the HV and HSD features single returns in C3 discrimination were incorrectly classified as land. As a returns result, the of the combination could enhance capacity wherever benthic are correctness present, which reflects the of C1potential with C3 is of lower than thatthat of C2 with C3 because the classification errors of land points as use multi-channels have various characteristics in land and water regions.

water points are much higher in the first combination. In general, the HV and HSD features could 4.2. Land/Water Discrimination Intensity-Based enhance discrimination capacity from wherever benthic Features returns are present, which reflects the potential use of multi-channels that have various characteristics in land and water regions. The Jenks break optimization was used to identify Ithresh and ICOV thresh in order to label water 4.2. Land/Water from Features selected as Discrimination 0.7 to minimize the Intensity-Based type I (FP) and II (FN) errors. The labeled LiDAR points from
points. Table 5 presents the threshold values of intensity (Ithresh) and ICOV (ICOVthresh). The IDthresh was

different channels based on ICOVwas and used ID areto shown in Figure while the accuracy measures of the The Jenks break optimization identify Ithresh6, and ICOV thresh in order to label water labeled points are provided in Table 6. points. Table 5 presents the threshold values of intensity (Ithresh ) and ICOV (ICOVthresh ). The IDthresh was selected as 0.7 to minimize the type I (FP ) and II (FN ) errors. The labeled LiDAR points from Table 5. Threshold values Ithresh and ICOV thresh for the three channels. different channels based on ICOV and ID are shown in Figure 6, while the accuracy measures of the Channel (S) ICOVthresh Ithresh labeled points are provided in Table 6.
C1 164 0.71 C2 194 0.73 Table 5. Threshold values ICOVthresh0.78 for the three channels. C3 Ithresh and57

Channel (S) C1 C2 C3

Ithresh 164 194 57

ICOVthresh 0.71 0.73 0.78

Appl. Sci. 2018, 8, 349 Appl. Sci. 2018, 8, x FOR PEER REVIEW

13 of 21 13 of 21

C1

C2

C3

ID

ICOV

Figure 6. Labeled LiDAR points based on ICOV and ID. Figure 6. Labeled LiDAR points based on ICOV and ID. Table 6. Accuracy measures of labeled points based on ICOV and ID. The best results are highlighted Table 6. Accuracy measures of labeled points based on ICOV and ID. The best results are highlighted in bold. in bold. Point Feature Channel Channel Point Feature Completeness (%) Completeness (%) Land Water Water Land 51.651.6 93.5 93.5 93.6 38.838.8 93.6 54.0 76.776.7 54.0 72.372.3 60.9 60.9 76.376.3 57.5 57.5 88.7 51.751.7 88.7 Correctness (%) Correctness (%) Overall Accuracy (%) Overall Accuracy (%) Land Water Land Water 97.9 25.0 97.9 25.0 57.857.8 96.6 24.6 96.6 24.6 48.448.4 73.6 58.0 68.268.2 58.0 73.6 91.5 27.5 91.5 27.5 70.670.6 89.4 34.2 89.4 34.2 73.073.0 52.3 88.5 88.5 52.3 65.565.5

ICOV ICOV

ID

ID

C1 C2 C3 C1 C2 C3

C1 C2 C3 C1 C2 C3

Generally, the accuracy measures using intensity-based features are relatively low. The Generally, the accuracy measures using intensity-based features are relatively low. The intermediate intermediate part of the water body had high intensity variation due to different angles of incidence part of the water body had high intensity variation due to different angles of incidence of the laser of the laser pulse with respect to the water surface. Therefore, this characteristic was used as the pulse with respect to the water surface. Therefore, this characteristic was used as the basis of ICOV, basis of ICOV, where higher ICOV values represented water points. However, some water points in where higher ICOV values represented water points. However, some water points in C3 exhibited low C3 exhibited low ICOV and were misclassified as land. As a result, the completeness of the water ICOV and were misclassified as land. As a result, the completeness of the water class from C1 (93.5%) class from C1 (93.5%) and C2 3.6%) was higher than from C3 (54.0%). On the other hand, the and C2 3.6%) was higher than from C3 (54.0%). On the other hand, the correctness of the water class correctness of the water class from C1 (25.0 %) and C2 (24.6%) was lower than from C3 (58.0%). This from C1 (25.0 %) and C2 (24.6%) was lower than from C3 (58.0%). This is because some vegetation is because some vegetation areas had high intensity variation in C1 and C2, so they exhibited high areas had high intensity variation in C1 and C2, so they exhibited high ICOV and were misclassified as ICOV and were misclassified as water points. water points. For ID feature, the water points were assumed to have the lowest intensity values in the scene. For ID feature, the water points were assumed to have the lowest intensity values in the scene. However, based on this assumption, the results revealed low discrimination rates. This is mainly However, based on this assumption, the results revealed low discrimination rates. This is mainly because the intensity variations of the water points as a result of low altitude and looking angles of because the intensity variations of the water points as a result of low altitude and looking angles of C1 and C2. Thus, most returns of the water area were recorded at normal incidence angle and, hence, C1 and C2. Thus, most returns of the water area were recorded at normal incidence angle and, hence, caused intensity variations. On the other hand, the discrimination results demonstrated the highest caused intensity variations. On the other hand, the discrimination results demonstrated the highest completeness (88.7%) and correctness (52.3%) of water class in C3. This is due to the returns from the completeness (88.7%) and correctness (52.3%) of water class in C3. This is due to the returns from the water surface and benthic layer that exhibited lower intensity values. The intermediate part of the water surface and benthic layer that exhibited lower intensity values. The intermediate part of the water body was misclassified as land due to high intensity variation. In land area, points that had water body was misclassified as land due to high intensity variation. In land area, points that had low low intensity values (i.e., vegetation points) were incorrectly classified as water. intensity values (i.e., vegetation points) were incorrectly classified as water. 4.3. 4.3. Land/Water Land/Water Discrimination Discrimination from from Geometry-Based Geometry-Based Features Features As As mentioned mentioned earlier, earlier, the the labeling labeling of of water water points points using using geometry-based geometry-based features features depends depends on on the the returns returns from from the the benthic benthic layer. layer. Consequently, Consequently, the the combination combination of of C1/C2 C1/C2and andC3 C3was wasrequired. required. The The water water points points were were labeled labeled as as such such if if the the PD PD of of a a LiDAR LiDAR point point in in C3 C3 was was higher higher than than the the PD PD in inC1 C1 or C2. For NOR feature, a LiDAR point with double returns in C3 and a single return in either C1 or

Appl. Sci. 2018, 8, 349

14 of 21

Appl. Sci. 2018, 8, x FOR PEER REVIEW

14 of 21

or C2. For NOR feature, a LiDAR point with double returns in C3 and a single return in either C1 or C2, .. C2, was was labeled labeled as as a a water water point. point. Figure Figure 7 7 shows shows the the labeled labeled LiDAR LiDAR points points based based on on PD PDand andNOR NOR The completeness, correctness, and overall accuracy of the labeled points are provided provided in in Table Table 7. 7.

C1 with C3

C2 with C3

NOR

PD

Water Land
Figure 7. Labeled LiDAR points based on PD and NOR. Figure 7. Labeled LiDAR points based on PD and NOR. Table 7. Accuracy measures of labeled points based on PD and NOR. The best results are highlighted Table 7. Accuracy measures of labeled points based on PD and NOR. The best results are highlighted in bold. in bold. Feature Channels Channels Point Point Feature Completeness (%) Completeness (%) Land Water Water Land 97.1 27.427.4 97.1 44.144.1 95.3 95.3 97.797.7 75.8 75.8 98.398.3 78.2 78.2 Correctness (%) Correctness (%) Overall Accuracy (%) Overall Accuracy (%) Land Water Land Water 94.1 44.4 94.1 44.4 53.4 53.4 50.4 94.0 94.0 50.4 63.2 63.2 87.1 95.2 87.1 95.2 89.5 89.5 88.3 96.5 88.3 96.5 90.8 90.8

PD

PD

NOR NOR

C1C3 C1C3 C2C3 C2C3 C1C3 C1C3 C2C3 C2C3

The PD feature shows lower overall accuracy, and most land points were misclassified as water. The PD feature shows lower overall accuracy, and most land points were misclassified as water. This is mainly due to the land area reflecting more returns in C3 than in C1 and C2 because the land This is mainly due to the land area reflecting more returns in C3 than in C1 and C2 because the land objects exhibited various characteristics at different wavelengths. Therefore, the condition that the objects exhibited various characteristics at different wavelengths. Therefore, the condition that the PD PD in C3 is higher than the PD in C1 or C2 was not achieved. The NOR feature demonstrated in C3 is higher than the PD in C1 or C2 was not achieved. The NOR feature demonstrated promising promising results. However, this is attributed to the presence of benthic returns from C3. Despite the results. However, this is attributed to the presence of benthic returns from C3. Despite the relatively relatively high overall classification accuracies obtained using NOR, there are misclassified points in high overall classification accuracies obtained using NOR, there are misclassified points in water water bodies. This is mainly due to single returns from water surfaces or benthic layers in C3 and, bodies. This is mainly due to single returns from water surfaces or benthic layers in C3 and, hence, hence, the necessary condition for labeling water points was not achieved. the necessary condition for labeling water points was not achieved. 4.4. Land/Water Discrimination from Seeded Region Growing 4.4. Land/Water Discrimination from Seeded Region Growing The point feature "NOR" was used to find the possible seed points by detecting the points that The point feature "NOR" was used to find the possible seed points by detecting the points that had had single returns in C1 or C2 (points_single) and the first of the double returns in C3 single returns in C1 or C2 (points_single) and the first of the double returns in C3 (points_first_of_double), (points_first_of_double), as shown in Figure 8a­c, respectively. The NOR was used rather than other as shown in Figure 8a­c, respectively. The NOR was used rather than other features (e.g., HV or features (e.g., HV or HSD) because it achieved a higher correctness of 95.2% (C1C3) and 96.5% HSD) because it achieved a higher correctness of 95.2% (C1C3) and 96.5% (C2C3) for the water body (C2C3) for the water body discrimination. For each point in points_single, the nearest point from and discrimination. For each point in points_single, the nearest point from and within the circle footprint within the circle footprint of points_first_of_double was found. The searching radius was set to 0.15 m, of points_first_of_double was found. The searching radius was set to 0.15 m, which was calculated by which was calculated by multiplying the flying height (430 m) by half of the beam divergence of C3 multiplying the flying height (430 m) by half of the beam divergence of C3 (0.35 mrad). (0.35 mrad). The selected points might represent other land objects, such as vegetation, which could have the The selected points might represent other land objects, such as vegetation, which could have the same characteristics. The extracted points_single and points_first_of_double are shown in Figure 8d,e. same characteristics. The extracted points_single and points_first_of_double are shown in Figure 8d,e. Therefore, the HV was checked within a 10 m searching radius to be less 0.5 m in order to preserve Therefore, the HV was checked within a 10 m searching radius to be less 0.5 m in order to preserve horizontal surfaces (i.e., water surface). Figure 8f,g shows the output that represents the water seed points. These points were used as an input for region growing, where the HV was checked point by

Appl. Sci. 2018, 8, 349

15 of 21

horizontal surfaces (i.e., water surface). Figure 8f,g shows the output that represents the water seed Appl. Sci. 2018 , 8, x FOR PEER REVIEW 15 point of 21 by points. These points were used as an input for region growing, where the HV was checked point to be less than 0.5 m so that all water surface points could be labeled accurately. Figure 8h,i point to be less than 0.5 m so that all water surface points could be labeled accurately. Figure 8h,i shows the labeled LiDAR points using the combination of C3 with either C1 or C2, while Table 8 shows the labeled LiDAR points using the combination of C3 with either C1 or C2, while Table 8 provides the accuracy measures for the two cases. provides the accuracy measures for the two cases.

(a)

(b)

(c)

(d)

(e)

( f)

( g)

Water Land

(h)

(i)

Figure 8. LiDAR points based on seeded region growing algorithm. (a­c) Single Returns of C1, C2 and and C3; (d­e) Possible Seed Points using C1 with C3 and C2 with C3; (f­g) Water Seed Points using C3; (d ,e)with Possible Seed using with C3 and C2 with C3; ( f,gwith ) Water Seed Points using C1 with C1 C3 and C2 Points with C3; (h­i) C1 Seeded Region Growing using C1 C3 and C2 with C3. C3 and C2 with C3; (h,i) Seeded Region Growing using C1 with C3 and C2 with C3.
Table 8. Accuracy measures of labeled points based on seeded region growing. The best results are in bold. Tablehighlighted 8. Accuracy measures of labeled points based on seeded region growing. The best results are highlighted in bold. Completeness (%) Correctness (%) Channels C1C3 Land Water Completeness (%) 99.9 94.0 99.9 95.7 Land Water Land Water Correctness (%) 99.0 99.2 99.1 99.4 Land Water Overall Accuracy (%) 99.0 99.1

Figure 8. LiDAR points based on seeded region growing algorithm. (a­c) Single Returns of C1, C2

Channels C2C3

Overall Accuracy (%)

C1C3 C1 or C2 99.9together 94.0 99.0 99.0 of more than 99% Using either with C3 achieved a 99.2 high overall accuracy C2C3 99.9 95.7 99.1 99.4 99.1 when using the automatic seeded region growing algorithm. The completeness and correctness of the water class is also high at more than 94%. The completeness of the water class was affected by the misclassification small pond at the C3 edge of the dataset. completeness combination of Using either C1 of or a C2 together with achieved a highThe overall accuracyof ofthe more than 99% when C1 with C3 (94%) was smaller than that of C2 with C3 (95.7%) because the misclassified part at the using the automatic seeded region growing algorithm. The completeness and correctness of the water theat first combination than the partwas in the second water class edges is alsoin high more than 94%. was The higher completeness ofmisclassified the water class affected by the combination. misclassification of a small pond at the edge of the dataset. The completeness of the combination of C1 This high accuracy rate shows the benefit of using multi-channel in such applications. with C3 (94%) was smaller than that of C2 with C3 (95.7%) because the misclassified part at the water However, this approach requires recorded returns at the green wavelength from the surface and

edges in the first combination was higher than the misclassified part in the second combination.

Appl. Sci. 2018, 8, 349

16 of 21

Appl. Sci. 2018, 8, x FOR PEER REVIEW

This high accuracy rate shows the benefit of using multi-channel in such applications. However, this approach requires recorded the green wavelength from the surface and benthic layer benthic layer of water bodies in returns order toat fully automate the discrimination process. At water body of water bodies order tomisclassified fully automate the discrimination process. At water edges, there edges, there are in still a few points due to low point density or high body variation in point are still a few misclassified points due to low point density or high variation in point elevation. elevation. Figure 9 summarizes the overall accuracies obtained from all point features and the Figure 9 summarizes the overall accuracies obtained from all point features and the seeded region seeded region growing algorithm. growing algorithm.

16 of 21

Figure 9. Average overall accuracies based on different point features and seeded region growing. Figure 9. Average overall accuracies based on different point features and seeded region growing.

4.5. Land/Water Boundary Delineation 4.5. Land/Water Boundary Delineation Land/water discrimination based on the seeded region growing algorithm achieved the highest Land/water discrimination based on the seeded region growing algorithm achieved the highest overall accuracy of more than 99% with clear visualized land/water boundaries. The land/ overall accuracy of more than 99% with clear visualized land/water boundaries. The land/water water boundary was delineated from region growing results produced by combining C1 with C3 boundary was delineated from region growing results produced by combining C1 with C3 (RG_C1C3_bound) and C2 with C3 (RG_C2C3_bound). The labeled points were converted into (RG_C1C3_bound) and C2 with C3 (RG_C2C3_bound). The labeled points were converted into raster images with a pixel size of 1 m, and contour lines were automatically drawn using the raster images with a pixel size of 1 m, and contour lines were automatically drawn using the embedded function in ArcGIS "Contour list". The output contour lines were compared with the embedded function in ArcGIS "Contour list". The output contour lines were compared with the contour line produced from labeled point clouds of C3 based on elevation (H_C3_bound) as shown in contour line produced from labeled point clouds of C3 based on elevation (H_C3_bound) as shown Figure 10. The misclassification at the dataset edges caused disconnection and delineation errors of the in Figure 10. The misclassification at the dataset edges caused disconnection and delineation errors land/water boundary. of the land/water boundary. The RMSE was calculated from nine transects distributed perpendicular to the land/water The RMSE was calculated from nine transects distributed perpendicular to the land/water boundary as shown in Figure 10. The tenth transect was excluded from calculations as the land/water boundary as shown in Figure 10. The tenth transect was excluded from calculations as the boundary was not delineated from RG_C2C3_bound. If nine transects are considered, the RMSE of land/water boundary was not delineated from RG_C2C3_bound. If nine transects are considered, the RG_C1C3_bound and RG_C2C3_bound are 4.81 m and 0.87 m, respectively. The high values are due RMSE of RG_C1C3_bound and RG_C2C3_bound are 4.81 m and 0.87 m, respectively. The high to the discrimination errors at the land/water interface at the edges. Therefore, we also excluded values are due to the discrimination errors at the land/water interface at the edges. Therefore, we transects at the edges (the first, second, and ninth transects). If only six transects are considered, the also excluded transects at the edges (the first, second, and ninth transects). If only six transects are RMSE of RG_C1C3_bound and RG_C2C3_bound improved to 0.42 m and 0.60 m, respectively. considered, the RMSE of RG_C1C3_bound and RG_C2C3_bound improved to 0.42 m and 0.60 m, respectively.

Appl. Sci. 2018, 8, 349 Appl. Sci. 2018, 8, x FOR PEER REVIEW

17 of 21

17 of 21

Figure 10. boundarydelineation. delineation. Figure 10.Land/water Land/water boundary

4.6. Limitations Land/Water Discrimination 4.6. Limitations of of Land/Water Discrimination The water body might be lake,aacoastal coastal zone zone with terrain, or a river in in urban The water body might be aa lake, with gradually graduallysloped sloped terrain, or a river urban or mountain areas. In addition, the LiDAR data are collected from different missions. Thus, the data or mountain areas. In addition, the LiDAR data are collected from different missions. Thus, the specifications depend on the flying height, aircraft's attitude, scan angles, and water surface data specifications depend on the flying height, aircraft's attitude, scan angles, and water surface roughness (i.e., waves). Therefore,we wediscuss discuss in in this this section of the proposed methods roughness (i.e., waves). Therefore, section the thelimitations limitations of the proposed methods for land/water discrimination considering different LiDAR data specifications. for land/water discrimination considering different LiDAR data specifications. The Titan sensor is designed to detect water surfaces in C1 (1550 nm) and C2 (1064 nm) and The Titan sensor is designed to detect water surfaces in C1 (1550 nm) and C2 (1064 nm) and detect water surfaces and/or benthic layers in C3 (532 nm). The arrangement of looking angles of C2 detect water surfaces and/or benthic layers in C3 (532 nm). The arrangement of looking angles of at nadir and C1 at 3.5° forward looking increases the reflection from the water surface, while the C2 at nadir and C1 at 3.5 forward looking increases the reflection from the water surface, while looking angle of C3 at 7° forward looking increases the probability of water surface penetration. the looking angle of C3 at 7 forward looking increases the probability of water surface penetration. Although the 1550 nm wavelength is less reflected than the 1064 nm wavelength from the water Although the 1550 nm wavelength is less than the is 1064 nmidentical. wavelength the bodies [34], the results demonstrated that reflected their performance almost Thisfrom is due to water the bodies [34], the results demonstrated performance is almost identical. This is due to the fact that the flying height is relativelythat low.their A higher altitude might provide different results. Water fact that the flying height relatively low. A higher altitude might provide different results. Water surface penetration and is benthic layer detection at the 532 nm wavelength depend on the water surface penetration and benthic layer detection at the 532 nm wavelength depend on the water surface surface roughness, incidence angles of the laser pulses, water clarity, and water depth. roughness, incidence angles of thethe laser pulses, water clarity, and water depth. As mentioned previously, interaction between the 532 nm wavelength and the water As mentioned previously, the interaction between the 532 nm wavelength and the water molecules molecules produces returns at a red wavelength of 647 nm [14]. Similarly, when the laser at the used wavelengths contacts the land/water interface, they Similarly, could produce at at a specific wavelength, produces returns at a red wavelength of 647 nm [14]. whenreturns the laser the used wavelengths which is required to be further explored. contacts the land/water interface, they could produce returns at a specific wavelength, which is Water coastal areas usually occupy the lowest elevation in the scene so that an required to bebodies furtherin explored. elevation threshold might be enough for discriminating water from in land. thean elevation Water bodies in coastal areas usually occupy the lowest elevation the However, scene so that elevation threshold would not help in the discrimination process for elevated water areas. The NOR feature is threshold might be enough for discriminating water from land. However, the elevation threshold expected to achieve promising results for elevated water areas if and only if LiDAR returns from to would not help in the discrimination process for elevated water areas. The NOR feature is expected water surfaces and benthic layers are recorded at the green wavelength. However, this is attributed achieve promising results for elevated water areas if and only if LiDAR returns from water surfaces to the data specifications and environment conditions. For instance, higher altitude and/or water and benthic layers are recorded at the green wavelength. However, this is attributed to the data turbidity may cause non-returns from the benthic layer. Other LiDAR point features such as HV, specifications and environment conditions. For instance, higher altitude and/or water turbidity may

Appl. Sci. 2018, 8, 349

18 of 21

cause non-returns from the benthic layer. Other LiDAR point features such as HV, HSD and PD require double returns from the water body at the green wavelength as well, but the results are also affected by the recorded returns from land objects at different wavelengths. The intensity-based features (ICOV and ID) rely on the recorded signal strength from the water body. As aforementioned, this is attributed to the aircraft's altitude, laser pulse angle of incidence, and water surface roughness. The results of intensity-based features also change according to the interaction of land objects and water bodies with different wavelengths. The performance of those features will differ according to the tested environment and the data specifications. In addition, intensity correction might maximize the usefulness of the intensity data. The automation of seeded region growing method is also restricted by the presence of double returns from the water body at the green wavelength. Another point feature could be used, then, to automatically detect the water seed points. In general, more datasets representing different complexity at the land/water interface will be further investigated once the reference data are available. 5. Conclusions This research presented the capability of multispectral LiDAR data, in the format of discrete returns, for land/water discrimination in coastal areas. The multispectral data of a study area at Lake Ontario were collected using the Optech Titan sensor operating at three channels with wavelengths of 1550, 1064 and 532 nm. The interaction of the water bodies at the C1 and C2 is almost identical, whereas the water surface is detected. The green wavelength can penetrate the water surface and detect both the water surface and benthic layer. A set of LiDAR point features based on elevation, intensity, and geometry were extracted and evaluated for land/water discrimination. The elevation-based features (HV and HSD) revealed an average overall accuracy of 75.1%, while the PD feature revealed an average overall accuracy of 58.3%. Those features showed various overall accuracies due to the different interaction of the wavelengths with land objects and water bodies. The NOR feature achieved relatively high overall accuracy of 90.2% due to the large number of points that have double returns from the water bodies in C3. The intensity-based features show relatively low water detection rates with an average overall accuracy of 63.9%, because the water points have the same intensity range of other land features (e.g., vegetation). A fully automated and unsupervised land/water discrimination method, based on seeded region growing algorithm, was also presented. This method started with the automatic selection of water seed points using two point features, NOR and HV. Subsequently, a criterion based on HV was grown point by point in order to label all water points in the scene. It has been shown that our method achieves high land/water discrimination with an overall accuracy of more than 99% and an average RMSE of 0.51 m for the land/water boundary. The use of multispectral LiDAR systems is required to fully automate the land/water discrimination process. However, dual-wavelength LiDAR systems are suitable for this application if the green wavelength (532 nm) is used. The multispectral data could be more effective in land/water discrimination if more LiDAR point features are explored and combined. Consequently, features selection and the assessment of their relevance to land/water discrimination will be required [50,51]. The land/water discrimination could be extended to consider shorelines with submerged vegetation and debris and inland water areas such as rivers in urban or mountain areas.
Acknowledgments: This research work is supported by the Discovery Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) [RGPIN-2015-03960] and the Ontario Trillium Scholarship (OTS). The authors also would like to thank Paul LaRocque and Teledyne Optech for providing the LiDAR data from the world's first multispectral LiDAR system, Optech Titan. Author Contributions: Salem Morsy, Ahmed Shaker, and Ahmed El-Rabbany conceived the research idea and carried out the design. Salem Morsy implemented the experiment and produced and analyzed the results under the supervision of Ahmed Shaker and Ahmed El-Rabbany. Salem Morsy drafted the manuscript. Ahmed Shaker and Ahmed El-Rabbany reviewed the manuscript. Conflicts of Interest: The authors declare no conflict of interest.

Appl. Sci. 2018, 8, 349

19 of 21

References
1. 2. Ma, M.; Wang, X.; Veroustraete, F.; Dong, L. Change in area of Ebinur Lake during the 1998­2005 period. ISPRS J. Photogramm. Remote Sens. 2007, 28, 5523­5533. [CrossRef] Du, Z.; Linghu, B.; Ling, F.; Li, W.; Tian, W.; Wang, H.; Gui, Y.; Sun, B.; Zhang, X. Estimating surface water area changes using time-series Landsat data in the Qingjiang River Basin, China. J. Appl. Remote Sens. 2012, 6, 063609. [CrossRef] Schumann, G.; Neal, J.; Mason, D.; Bates, P. The accuracy of sequential aerial photography and SAR data for observing urban flood dynamics, a case study of the UK summer 2007 floods. Remote Sens. Environ. 2011, 115, 2536­2546. [CrossRef] Mueller, N.; Lewis, A.; Roberts, D.; Ring, S.; Melrose, R.; Sixsmith, J.; Lymburner, L.; McIntyre, A.; Tan, P.; Curnow, S.; et al. Water observations from space: Mapping surface water from 25 years of Landsat imagery across Australia. Remote Sens. Environ. 2016, 174, 341­352. [CrossRef] Qi, H.; Altinakar, M. Simulation-based decision support system for flood damage assessment under uncertainty using remote sensing and census block information. Nat. Hazards 2011, 59, 1125­1143. [CrossRef] Stephens, E.; Bates, P.; Freer, J.; Mason, D. The impact of uncertainty in satellite data on the assessment of flood inundation models. J. Hydrol. 2012, 414, 162­173. [CrossRef] Kuenzer, C.; Guo, H.; Huth, J.; Leinenkugel, P.; Li, X.; Dech, S. Flood mapping and flood dynamics of the Mekong Delta: ENVISAT-ASAR-WSM Based Time Series analyses. Remote Sens. 2013, 5, 687­715. [CrossRef] Wang, L.; Huang, J.; Du, Y.; Hu, Y.; Han, P. Dynamic assessment of soil erosion risk using Landsat TM and HJ satellite data in Danjiangkou Reservoir Area, China. Remote Sens. 2013, 5, 3826­3848. [CrossRef] Giardino, C.; Bresciani, M.; Villa, P.; Martinelli, A. Application of remote sensing in water resource management: The case study of Lake Trasimeno, Italy. Water Resour. Manag. 2010, 24, 3885­3899. [CrossRef] Ding, X.; Li, X. Monitoring of the water-area variations of Lake Dongting in China with ENVISAT ASAR images. Int. J. Appl. Earth Obs. 2011, 13, 894­901. [CrossRef] Van Dijk, A.; Renzullo, L. Water resource monitoring systems and the role of satellite observations. Hydrol. Earth Syst. Sci. 2011, 15, 39­55. [CrossRef] Guenther, G. Airborne Laser Hydrography, 1st ed.; U.S. Department of Commerce, National Oceanic and Atmospheric Administration, National Ocean Service, Charting and Geodetic Services: Rockville, MD, USA, 1985. Allouis, T.; Bailly, J.; Pastol, Y.; Le Roux, C. Comparison of LiDAR waveform processing methods for very shallow water bathymetry using Raman, near-infrared and green signals. Earth Surf. Process. Landf. 2010, 35, 640­650. [CrossRef] Guenther, G.; Brooks, M.; LaRocque, P. New Capabilities of the "SHOALS" Airborne LiDAR Bathymeter. Remote Sens. Environ. 2000, 73, 247­255. [CrossRef] Pe'eri, S.; Philpot, W. Increasing the Existence of Very Shallow-Water LiDAR Measurements Using the Red-Channel Waveforms. IEEE Trans. Geosci. Remote Sens. 2007, 45, 1217­1223. [CrossRef] Brzank, A.; Heipke, C.; Goepfert, J.; Soergel, U. Aspects of generating precise digital terrain models in the Wadden Sea from LiDAR­water classification and structure line extraction. ISPRS J. Photogramm. Remote Sens. 2008, 63, 510­528. [CrossRef] Smeeckaert, J.; Mallet, C.; David, N.; Chehata, N.; Ferraz, A. Large-scale classification of water areas using airborne topographic LiDAR data. Remote Sens. Environ. 2013, 138, 134­148. [CrossRef] Crasto, N.; Hopkinson, C.; Forbes, D.L.; Lesack, L.; Marsh, P.; Spooner, I.; Van Der Sanden, J.J. A LiDAR-based decision-tree classification of open water surfaces in an Arctic delta. Remote Sens. Environ. 2015, 164, 90­102. [CrossRef] Höfle, B.; Vetter, M.; Pfeifer, N.; Mandlburger, G.; Stötter, J. Water surface mapping from airborne laser scanning using signal intensity and elevation data. Earth Surf. Process. Landf. 2009, 34, 1635­1649. [CrossRef] Antonarakis, A.; Richards, K.; Brasington, J. Object-based land cover classification using airborne LiDAR. Remote Sens. Environ. 2008, 112, 2988­2998. [CrossRef] Schmidt, A.; Rottensteiner, F.; Sörgel, U. Classification of airborne laser scanning data in Wadden sea areas using conditional random fields. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2012, 39-B3, 161­166. [CrossRef]

3.

4.

5. 6. 7. 8. 9. 10. 11. 12.

13.

14. 15. 16.

17. 18.

19. 20. 21.

Appl. Sci. 2018, 8, 349

20 of 21

22. 23.

24. 25. 26. 27.

28. 29.

30.

31.

32. 33.

34.

35.

36. 37. 38.

39.

40. 41. 42.

Hooshyar, M.; Kim, S.; Wang, D.; Medeiros, S.C. Wet channel network extraction by integrating LiDAR intensity and elevation data. Water Resour. Res. 2015, 51, 10029­10046. [CrossRef] Acevedo, M.A.; Corrada-Bravo, C.J.; Corrada-Bravo, H.; Villanueva-Rivera, L.J.; Aide, T.M. Automated classification of bird and amphibian calls using machine learning: A comparison of methods. Ecol. Inform. 2009, 4, 206­214. [CrossRef] Woodhouse, I.H.; Nichol, C.; Sinclair, P.; Jack, J.; Morsdorf, F.; Malthus, T.J.; Patenaude, G. A Multispectral canopy LiDAR Demonstrator project. IEEE Geosci. Remote Sens. 2011, 8, 839­843. [CrossRef] Gamon, J.; Peñuelas, J.; Field, C. A narrow-waveband spectral index that tracks diurnal changes in photosynthetic efficiency. Remote Sens. Environ. 1992, 41, 35­44. [CrossRef] Shi, S.; Song, S.; Gong, W.; Du, L.; Zhu, B.; Huang, X. Improving Backscatter intensity calibration for Multispectral LiDAR. IEEE Geosci. Remote Sens. 2015, 12, 1421­1425. [CrossRef] Wei, G.; Shalei, S.; Bo, Z.; Shuo, S.; Faquan, L.; Xuewu, C. Multi-wavelength canopy LiDAR for remote sensing of vegetation: Design and system performance. ISPRS J. Photogramm. Remote Sens. 2012, 69, 1­9. [CrossRef] Puttonen, E.; Hakala, T.; Nevalainen, O.; Kaasalainen, S.; Krooks, A.; Karjalainen, M.; Anttila, K. Artificial target detection with a hyperspectral LiDAR over 26-h measurement. Opt. Eng. 2015, 54, 013105. [CrossRef] Danson, F.M.; Gaulton, R.; Armitage, R.P.; Disney, M.; Gunawan, O.; Lewis, P.; Pearson, G.; Ramirez, A.F. Developing a dual-wavelength full-waveform terrestrial laser scanner to characterize forest canopy structure. Agric. For. Meteorol. 2014, 198­199, 7­14. [CrossRef] Douglas, E.S.; Martel, J.; Li, Z.; Howe, G.; Hewawasam, K.; Marshall, R.A.; Schaaf, C.L.; Cook, T.A.; Newnham, G.J.; Strahler, A.; et al. Finding leaves in the forest: The dual-wavelength Echidna LiDAR. IEEE Geosci. Remote Sens. 2015, 12, 776­780. [CrossRef] Briese, C.; Pfennigbauer, M.; Lehner, H.; Ullrich, A.; Wagner, W.; Pfeifer, N. Radiometric calibration of multi-wavelength airborne laser scanning data. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2012, I-7, 335­340. [CrossRef] Wang, C.-K.; Tseng, Y.-H.; Chu, H.-J. Airborne dual-wavelength LiDAR data for classifying land cover. Remote Sens. 2014, 6, 700­715. [CrossRef] Teledyne Optech--Titan Brochure and Specifications, 2015. Optech Titan Multispectral LiDAR System--High Precision Environmental Mapping. Available online: http://www.teledyneoptech.com/wp-content/ uploads/Titan-Specsheet-150515-WEB.pdf (accessed on 30 August 2016). Fernandez-Diaz, J.C.; Carter, W.; Glennie, C.; Shrestha, R.; Pan, Z.; Ekhtari, N.; Singhania, A.; Hauser, D.; Sartori, M. Capability assessment and performance metrics for the Titan multispectral mapping LiDAR. Remote Sens. 2016, 8, 936. [CrossRef] Morsy, S.; Shaker, A.; El-Rabbany, A. Potential use of multispectral airborne LiDAR data in land cover classification. In Proceedings of the 37th Asian Conference on Remote Sensing, Colombo, Sri Lanka, 17­21 October 2016; p. 296. Bakula, K.; Kupidura, P.; Jelowicki, L. Testing of land cover classification from multispectral airborne laser scanning data. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2016, XLI-B7, 161­169. Zou, X.; Zhao, G.; Li, J.; Yang, Y.; Fang, Y. 3D land cover classification based on multispectral LiDAR point clouds. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2016, XLI-B1, 741­747. [CrossRef] Matikainen, L.; Karila, K.; Hyyppä, J.; Litkey, P.; Puttonen, E.; Ahokas, E. Object-based analysis of multispectral airborne laser scanner data for land cover classification and map updating. ISPRS J. Photogramm. Remote Sens. 2017, 128, 298­313. [CrossRef] Wichmann, V.; Bremer, M.; Lindenberger, J.; Rutzinger, M.; Georges, C.; Petrini-Monteferri, F. Evaluating the potential of multispectral airborne LiDAR for topographic mapping and land cover classification. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2015, II-3/W5, 113­119. [CrossRef] Morsy, S.; Shaker, A.; El-Rabbany, A. Multispectral LiDAR data for land cover classification of urban areas. Sensors 2017, 17, 958. [CrossRef] [PubMed] Morsy, S.; Shaker, A.; El-Rabbany, A. Clustering of multispectral airborne laser scanning data using Gaussian decomposition. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, 42-2/W7, 269­276. [CrossRef] Ekhtari, N.; Glennie, C.; Fernandez-Diaz, J.C. Classification of multispectral LiDAR point clouds. In Proceedings of the IEEE International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth, TX, USA, 23­28 July 2017; pp. 2756­2759.

Appl. Sci. 2018, 8, 349

21 of 21

43.

44.

45. 46. 47. 48. 49. 50. 51.

Morsy, S.; Shaker, A.; El-Rabbany, A.; LaRocque, P.E. Airborne multispectral LiDAR data for land-cover classification and land/water mapping using different spectral indexes. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2016, III-3, 217­224. [CrossRef] Morsy, S.; Shaker, A.; El-Rabbany, A. Evaluation of distinctive features for land/water classification from multispectral airborne LiDAR data at Lake Ontario. In Proceedings of the 10th International Conference on Mobile Mapping Technology (MMT), Cairo, Egypt, 6­8 May 2017; pp. 280­286. Jenks, G.F. The data model concept in statistical mapping. In International Yearbook of Cartography; Frenzel, K., Ed.; George Philip: England, UK, 1967; Volume 7, pp. 186­190. Zhu, S.C.; Yuille, A. Region Competition: Unifying Snakes, Region Growing, and Bayes/MDL for Multiband Image Segmentatio. IEEE Trans. Pattern Anal. Mach. Intell. 1996, 18, 884­900. Tremeau, A.; Borel, B. A region growing and merging algorithm to color segmentation. Pattern Recognit. 1997, 30, 1191­1203. [CrossRef] Yu, P.; Qin, A.K.; Clausi, D.A. Unsupervised polarimetric SAR image segmentation and classification using region growing with edge penalty. IEEE Trans. Geosci. Remote Sens. 2012, 50, 1302­1317. [CrossRef] Heipke, C.; Mayer, H.; Wiedemann, C.; Jamet, O. Evaluation of automatic road extraction. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 1997, 32, 151­160. Chehata, N.; Guo, L.; Mallet, C. Airborne LiDAR feature selection for urban classification using random forests. ISPRS Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2009, 38-3/W8, 207­212. Mallet, C.; Bretar, F.; Roux, M.; Soergel, U.; Heipke, C. Relevance assessment of full-waveform LiDAR data for urban area classification. ISPRS J. Photogramm. Remote Sens. 2011, 66, S71­S84. [CrossRef] © 2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).


