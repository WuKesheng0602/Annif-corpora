Speeding up calibration of Latent Dirichlet Allocation model to improve topic analysis in Software Engineering

by

Jorge Arturo L´ opez Bachelor of Applied Mathematics and Computer Science, Universidad Nacional Aut´ onoma de M´ exico, Mexico, 1991

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science

Toronto, Ontario, Canada, 2017 c Jorge Arturo L´ opez 2017

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS IherebydeclarethatIamthesoleauthorofthisthesis.Thisisatruecopyofthethesis, includinganyrequiredfinalrevisions,asacceptedbymyexaminers. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose ofscholarly research. I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, intotal or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

JJ

Speeding up calibration of Latent Dirichlet Allocation model to improve topic analysis in SoftwareEngineering Master of Science 2017 Jorge Arturo L´ opez Computer Science Ryerson University

Abstract Extraction of topics from large text corpuses helps improve Software Engineering (SE) processes. Latent Dirichlet Allocation (LDA) represents one of the algorithmic tools to understand, search, exploit, and summarize a large corpus of data (documents), and it is often used to perform such analysis. However, calibration of the models is computationallyexpensive,especiallyifiteratingoveralargenumberoftopics.Ourgoal is to create a simple formula allowing analysts to estimate the number of topics, so that the top X topics include the desired proportion of documents under study. We derived the formula from the empirical analysis of three SE-related text corpuses. We believe that practitioners can use our formula to expedite LDA analysis. The formula is also of interest to theoreticians, as it suggests that different SE text corpuses have similar underlying properties.

JJJ

Acknowledgements I would like to express my gratitude to whom made possible the making of this thesis. Foremost, to my supervisor Dr. Andriy Miranskyy. During my studies at Ryerson, Dr. Miranskyy was an exemplary teacher and advisor. I also thank the examination committee for their valuable comments andsuggestions. Thanks to NSERC for the financial support received through my supervisor. My gratitude extends to RC4 for using the computer hardware that they provide to the program. Also I would like to thank IBM Canada for the opportunity to showcase my research.  Finally, I would like to thank my family for all their support and comprehension to Bchieve this goal.

JW

Dedication To my family.

v

Contents
Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction 1.1 Motivation . . . . . . . . 1.2 Research Statement . . . 1.3 Novelty and contribution . 1.4 Organization . . . . . . . v ix x 1 1 2 3 3 5 5 7 8 10

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

2 Background 2.1 Document Representation . . 2.2 Document clustering . . . . . 2.3 Mixture models . . . . . . . . 2.4 Mixed Membership Modeling 2.4.1 2.4.2

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

Bag of words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Prior probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

3 Literature Review 12 3.1 Probabilistic Topic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 3.3 3.4 3.5 3.6 3.7 Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Evolution of Topic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 LDA model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Dynamic Topic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Selection of the number of topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Applications of LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.7.1 Bug localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.7.2 Software traceability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 vJ

3.8 3.9

3.7.3 Feature Location . . . 3.7.4 Source code labeling . 3.7.5 Test case prioritization Prerequisites of LDA . . . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

. . . .

25 25 26 26

Limitations of LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 28

4 Methodology 4.1 4.2 4.3

Notation and terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 From pLSI to LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Graphical Representation of Latent Dirichlet Allocation (LDA) and Dynamic Topic Models (DTM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.3.1 LDA graphical model . . 4.3.2 DTM graphical model . . Dirichlet distribution . . . . . . . The Simplex . . . . . . . . . . . . Posterior calculation for LDA . . Approximation techniques . . . . 4.7.1 Collapsed Gibbs Sampling 4.7.2 Variational inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 31 31 35 39 41 41 43 46 46 48 54 54 55 56 57 68 69

4.4 4.5 4.6 4.7

5 Implementation and Calibration 5.1 Processing of datasets . . . . . . . . . . . . . . . . 5.2 Mining of Q&A forums . . . . . . . . . . . . . . . . 5.3 Calibration . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Datasets . . . . . . . . . . . . . . . . . . . . 5.3.2 Processing of datasets for model calibration 5.3.3 Time consumption to iterate over K . . . . 5.3.4 Creating fitted values . . . . . . . . . . . . 5.4 Analysis of results . . . . . . . . . . . . . . . . . . 5.4.1 Selection of the best performing model . . . 5.4.2 5.5

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

. . . . . . . . .

Analysis of the best performing model . . . . . . . . . . . . . . . . . . . . . . . . . 71

Threats to Validity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 82 84

6 Conclusion and Future Work Appendices

A Sample Plots 85 A.1 android dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 A.2 dba quarterly dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 A.3 dba monthly dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

vJi

A.4 salesforce dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 B Code listings B.1 Program: xml2csv.pl . . . . . . . . B.2 Program: do analysis db2 idf.R . . B.3 Program: utilsdb2.R . . . . . . . . B.4 Program: do lda analysis topics.R B.5 Program: do lda quarter inc.R . . B.6 Program: utils.R . . . . . . . . . . B.7 Program: convVar k200.R . . . . . B.8 Program: verifyFit k200.R . . . . . B.9 Program: verifyFit k200 analysis.R B.10 Program: validate fit.R . . . . . . 3FGFSFODFT (MPTTBSZ "DSPOZNT 115 115 118 123 127 129 131 136 140 144 148 151 163 165

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . . . . . . .

vJii

List of Tables
3.1 Example of obtaining 5 topics and 20 keywords using LDA from the dba.stackexchange.com corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Topic 1 of the LDA inference of 5 topics and 20 keywords . . . . . . . . . . . . . . . . . Datasets considered . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Expanded list of number of subsets processed by dataset. Year refers to the yearly data that is available to be extracted from the corpus. . . . . . . . . . . . . . . . . . . . . . . Linear regression models for intercept term a ^ (flexible model: Equations 5.6 and 5.7). The values in brackets represent 90% confidence interval of the coefficients. . . . . . . . Linear regression models for slope term ^ b (flexible model:Equations 5.8 and 5.9). The values in brackets represent 90% confidence interval of the coefficients. . . . . . . . . . . Linear regression models for ^ b (constrained model: Equations 5.12 and 5.13). The values in brackets represent 90% confidence interval of the coefficients. . . . . . . . . . . . . . . Summary statistics for the RMSE: Dataset 1 . . . . . . . . . . . . . . . . . . . . . . . . Summary statistics for the RMSE: Dataset 2 . . . . . . . . . . . . . . . . . . . . . . . . . 49 . 55 . 55 . 64 . 65 . 67 . 68 . 68

5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8

A.1 Sample plots included in this appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 B.1 List of programs included in this appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

JY

List of Figures
2.1 2.2 2.3 2.4 2.5 3.1 3.2 3.3 3.4 Representation of a document as a vector of term frequency (tf)'s . . . . . . . . . . . . . Clustering of three documents with a vocabulary of two words, adapted from [34] . . . . 6 8

Overlapping Clusters, adapted from [34] . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Mixture of Gaussian probabilities representing topic proportions, adapted from [34] . . . 9 Mixture of topics in a document, adapted from [34] . . . . . . . . . . . . . . . . . . . . . 10 A fitting of a small piece of text processed with a Topic Model, adapted from [14] . . . Output of an LDA inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Representation of a generative process and the problem of statistical inference, adapted from [78] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Example of one topic extracted from the issues of Science journal published between 1880 and 2000 (every 10 years as shown in [22], but shown every 40 years here). Top panel shows the tracking of ten top words for this topic for a given year; and bottom panel shows how the inferred probability of terms associated with this topic (that was labeled with "neuroscience"by an analyst) varies throughout the years. Adapted from [22]. . . . . 13 . 15 . 16

. 20

4.1 4.2 4.3 4.4

Diagram of the Probabilistic Latent Semantic Analysis (pLSI) model without using plates notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Diagram of the pLSI model using plates notation . . . . . . . . . . . . . . . . . . . . . . . 30 LDA graphical model without using plates notation. . . . . . . . . . . . . . . . . . . . . . 32 LDA graphical model using plates notation, adapted from [14]. The components with the dashed lines refers to the smoothed LDA model.  is a multinomial distribution that is sampled from a Dirichlet distribution with parameter  . This sampling occurs repeatedly for each topic until K topics have been produced. . . . . . . . . . . . . . . . . . . . . . . 33 Graphical model representation of the DTM, adapted from [13]. . . . . . . . . . . . . . Contour plots of different probability density (pd)'s when the weight of the distribution (dark big point) is centred in a particular point. Adapted from [16] . . . . . . . . . . . . Dirichlet distribution with variations of  when weight of the distribution is distributed on the plane between the three words, adapted from [16] . . . . . . . . . . . . . . . . . Dirichlet distribution with  < 1, increased pd at the corners of the simplex . . . . . . . 34 . 37 . 38 . 39

4.5 4.6 4.7 4.8

x

4.9

Word and topic simplex embedded, adapted from [14]. This geometrical representation of the simplex considers 3 words and 3 topics. Looked perpendicularly from above, the contoured lines represent a smooth distribution placed by LDA. The vertices of the word simplex represent distributions with p = 1 while the ones of the topic simplex mean

different distributions over words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 4.10 Diagram of the LDA variational model, adapted from [1] . . . . . . . . . . . . . . . . . . 44 5.1 5.2 5.3 5.4 5.5 5.6 Process of data for performing a LDA inference . . . . . . . . . . . . . . . . . . . . . . . . 47 Probability per term for Topic 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 Document frequency per term for topic 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Keywords vs idf for topic 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 . 53 . 61 . 62 . 63

LDA inference for topic 1 showing terms plotted against its idf and probability. . . . . . Time needed to compute (calibrate) LDA model for a given number of topics K (the input data are Android subset, January 2014). . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.7 Fitted empirical data for X = 5, 10, 25, 50 . . . . . . . . . . . . . . . . . . . . . . . . . . 5.8 Linear relation between log (F ) and log (K ) breaks approximately at K > 200, example for X=5,10,25, and 50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.9 Summary statistics for the RMSE: Dataset 1. `Flex - simple' uses Equations 5.6 and 5.8; `Flex - complex' uses Equations 5.7 and 5.9. `Constr - simple' uses Equation 5.12; `Constr - complex' uses Equation 5.13. `Ind. fit - flex' shows RMSE for flexible linear model (Eq. 5.4 fitted individually to every data subset), while `Ind. fit - constr' depicts RMSE for constraint linear model (Eq. 5.10 fitted individually to every data subset). . . . . . 5.10 Summary statistics for the RMSE: Dataset 2. Flex - simple' uses Equations 5.6 and 5.8; `Flex - complex' uses Equations 5.7 and 5.9. `Constr - simple' uses Equation 5.12; `Constr - complex' uses Equation 5.13. `Ind. fit - flex' shows RMSE for flexible linear model (Eq. 5.4 fitted individually to every data subset), while `Ind. fit - constr' depicts RMSE for constraint linear model (Eq. 5.10 fitted individually to every data subset). . . . . . . 5.11 10-fold cross validation of the fit of the `Constrained - complex' model (Equation 5.13). 5.12 5.13 5.14 5.15 5.16 5.17 Performance Performance Performance Performance Performance Performance of of of of of of

. 69

. 70 . 72 73 74 75 76 77 78

the constrained complex model for different values of X : Dataset 1. . . . the constrained complex model model for different values of X : Dataset 2. the constrained complex model for different values of N : Dataset 1. . . . the constrained complex model model for different values of N : Dataset 2. the constrained complex mode per dataset: Dataset 1. . . . . . . . . . . . the constrained complex model per dataset: Dataset 2. . . . . . . . . . .

5.18 Fitting for a selection of top X using the constrained-complex model for Dataset 1 . . . . 79 5.19 Decision tree for selecting the best fitting model. The path to the best-performing model is given by the dashed line. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

xJ

Chapter 1

Introduction
Latent Dirichlet Allocation (LDA) is a probabilistic model that can uncover common topics within a group of documents (such as support tickets1 or defect descriptions), based on the analysis of words extracted from these documents. LDA is employed in a wide variety of areas, ranging from music [38] to robot learning [32]. LDA is gaining popularity in the Software Engineering (SE) community, where it is used in a number of areas including software maintenance [1], traceability [8, 49], a posteriori requirements identification [38], and program comprehension [38]. LDA is attractive because it is unsupervised, requiring no a priori annotations.

1.1

Motivation

Analysis of various SE-related text corpuses aids in Software (SW) development. For example, analysis of artifacts created during development can improve SW traceability [9], while mining text of defects and support tickets can help to understand which particular features are causing users the most grief [1]. The former improves code comprehension; the latter enhances business risk analysis, and prioritization of maintenance tasks. My
1 Reports on specific problems in an issue tracking system, including their statuses and additional significant data that are logged in a call center or help desk.

1

CHAPTER 1. INTRODUCTION

1.2. RESEARCH STATEMENT

work is motivated by my industrial experience in SW maintenance, resolving product issues. A representative example is as follows. The software product was returning a cryptic error message if it could not connect to the central data repository (which happened frequently). This error message triggered more than a 1,000 calls from confused clients, overloading support personnel. This problem came to attention a month later, when an analyst read (mined) thousands of support tickets opened within the last month and identified a cluster of tickets associated with the error message. Fixing the problem (thereby satisfying customers and relieving support personnel) was simple: the message text was improved, and documentation associated with the message was enhanced. However, identifying the problem was laborious and time consuming. Other practitioners report similar challenges [1]. To diagnose such problems, the analyst often needs to identify a small set of topics containing a large portion of the documents, so that s/he can recognize "key" topics: e.g., the analyst from our example sifted through thousands of support tickets to find priority problem areas. Fortunately, automatic topic analysis techniques, such as LDA [14] can expedite the process.

1.2

Research Statement

Selecting the optimal number of topics to be identified in the LDA model determines greatly its performance and outcome expected [39]. Thus, the problem can be described as follows: an analyst needs to pick K , such that top X topics would contain a certain fraction F of the documents. For example, pick the value of K , such that top 5 topics would contain 80% of all the support tickets under study. This outcome can be achieved by iterating over the values of K until reaching the desired outcome. This process, however, is resource-intensive. Probabilistic Topic Models (TMs), such as LDA, require to be provided with the value

2

CHAPTER 1. INTRODUCTION

1.3. NOVELTY AND CONTRIBUTION

of number of topics K . Internally, the model also needs two additional parameters:  and  , that are calculated automatically for each parameter K [8]. It is known that the processing time, necessary to find the optimal values of K by iterating, grows exponentially [85]. Therefore, this thesis addresses the following research question: How can we quickly select the number of topics K so that the top X topics include a certain fraction F of the N documents under study?

1.3

Novelty and contribution

We are proposing a formula to quickly estimate K in LDA by means of X , F , and N , which would be invaluable in the realm of software system development and maintenance. For example, practitioners can use the formula to determine a short-list of the most frequently reported product field failures, which, in turn, can inform their decisions regarding corrective product maintenance; businesses can identify the most frequently reported user issues in order to inform their decisions regarding product evolution, and risk analysis; management can obtain a more definitive answer to the question "how many problematic areas does the product contain?", rather than an answer such as "somewhere between 5 and 100 depending on the granularity".

1.4

Organization

The chapters are organized as follows: 1. In Chapter 2, we provide the background for progressing into the subject matter of this thesis. 2. In Chapter 3, we perform a review of the available information that we deem to be more relevant to embrace TMs and LDA.

3

CHAPTER 1. INTRODUCTION

1.4. ORGANIZATION

3. In Chapter 4, we present the foundations of the LDA, its evolution and inner workings. 4. In Chapter 5, we show how the LDA model works using data from a Q&A repository, and we also propose the development of a formula to calibrate it. This involves readily calculating K (number of topics to be extracted with LDA) using this formula. 5. Finally, in Chapter 6, we outline the ideas of this thesis and identify directions of future work.

4

Chapter 2

Background
This chapter serves as a preamble for further discussing TMs. Here, we will review basic but important concepts that will allow us to better understand the focus of this thesis. The review does not intend to be exhaustive, but only at a level of detail that help us to understand further discussions. These concepts vary from the term frequency - inverse document frequency (tf-idf) scheme to mixture models.

2.1

Document Representation

One method for representing a document is simply as a vector of word counts, for example, let us suppose that we have a document containing just one phrase. After we perform stop word removal and word stemming, we annotate underneath of each word its count (tf) (See Figure 2.1). This concept was introduced in the 50's [56] followed by a term weighting function called (inverse document frequency (idf)), introduced in the 70's [69]. Commonly, documents are described in terms of the topics they represent. For example, documents on databases would contain several words, such as "table", "create", or "select". The purpose of the tf-idf scheme is to find important words in the text corpora. Salton and McGill introduced this scheme in 1983 [72]. They defined tf-idf as the measure of how significant a term in a text corpus is. This significance can be understood as
5

CHAPTER 2. BACKGROUND

2.1. DOCUMENT REPRESENTATION

Figure 2.1: Representation of a document as a vector of tf's

the concentration rate of the term into relatively short quantity of documents. The idf [25] is defined by equation (2.1), where N is the total number of documents in the corpus with N = |D|. The term {d  D : t  d} represents the number of documents where the term t appears (i.e., tf (t, d) = 0). The tf-idf is given by tf×idf. The higher the tf-idf score, the more important this word is for describing the document. N . {d  D : t  d}

idf (t, D) = log

(2.1)

The applications of the tf-idf scheme are numerous. Ramos [67], exemplifies the use of tf-idf for deciding which terms are more favorable to include in a query. Terms that possesses a high tf-idf indicate that they have a strong relationship with the document where they are located. Therefore, this may imply that if those terms are shown in the query would catch the attention of the user. Berger et al. [11] proposed several tf-idf based algorithms. For instance, one includes enhancements to improve tf-idf performance and another one implements statistical translation. Even though it was widely used, tf-idf didn't include sufficient reduction in description of terms length and provided limited document information on statistical structure.

6

CHAPTER 2. BACKGROUND

2.2. DOCUMENT CLUSTERING

2.2

Document clustering

Clustering textual documents refers to grouping related documents. Unlike a multiclass classification problem (which is a supervised learning task, where we would like to label what a new article is about, and labels are provided in the training examples), the problem of clustering is an unsupervised learning task that consists in uncovering the cluster structure from data input alone [34]. One of the simplest algorithms for clustering is K -means. Here a dataset is divided in K different and non-overlapping clusters. Let us consider C1 . . . CK clusters, then two conditions must be met: 1. C1  C2  . . .  CK = 1, . . . , n meaning that each observation belongs to at least one of the CK , 2. CK  CK = 0, K = K . In this algorithm, the data points are scored by determining its distance from the center of the cluster in question [50]. As documents can be represented as vectors (Xi ) we can consider to cluster a collection of them (Zj ). Supposing that our vocabulary consists of only two words for simplicity, then the collection of documents can be clustered as Figure 2.2 indicates. Let us call these clusters, topics. I.e., we can say that Topic 1 is cluster 1, Topic 2 ­ cluster 2, and so on. Moreover, we may label them according to the semantic contents of the topic; in this case we may say (not the algorithm) that topic 1 is about "Computer Science", topic 2 is about "Genetics", etc.

7

CHAPTER 2. BACKGROUND

2.3. MIXTURE MODELS

Figure 2.2: Clustering of three documents with a vocabulary of two words, adapted from [34]

2.3

Mixture models

In order to illustrate the reason for using a probabilistic model, let us exemplify the concept of uncertainty in cluster assignments using the K -means algorithm. In Figure 2.3, it is unclear if observation xi belongs to cluster 1 or cluster 3. The overlapping among the clusters represent uncertainty areas, where we are not positive about the assignment of an observation xi to a particular cluster. A mixture model is a probabilistic model that in our case represents the topics of a document as a combination of weighted Gaussian distributions. For instance, let us create a mixture of Gaussian distributions with the respective topic probabilities: 1 =0.55, 2 =0.27, and 3 =0.18 (chosen arbitrarily), represented by the vector  = [0.55, 0.27, 0.18] for K =3, where 0  i  1 and in Figure 2.4.
K i=1

i = 1 [34, 42]. This mixture is shown graphically

8

CHAPTER 2. BACKGROUND

2.3. MIXTURE MODELS

Figure 2.3: Overlapping Clusters, adapted from [34]

Figure 2.4: Mixture of Gaussian probabilities representing topic proportions, adapted from [34]

9

CHAPTER 2. BACKGROUND

2.4. MIXED MEMBERSHIP MODELING

Figure 2.5: Mixture of topics in a document, adapted from [34]

2.4

Mixed Membership Modeling

In the context of document analysis, we have seen that clustering captures the topics that permeate a text corpus in which every document is assigned only to one topic (hard assignment). However, very often, the case is that a document represents more than one topic. We need models that are able to capture the uncertainty when a document may belong to several topics (soft assignment), see Figure 2.5. In that case, the document has membership in topics 1,2,3 and 4, also the relative proportion of occurrence for each topic is captured. Such models that allow to associate any document with a set of topics are known as mixed membership models [18]. 2.4.1 Bag of words

Let us consider the case where we extract all the words that are present in a given document. The order of these words are unimportant. Given that this collection of
10

CHAPTER 2. BACKGROUND

2.4. MIXED MEMBERSHIP MODELING

words may contain multiple occurrences of a unique word we call this a multiset. This representation of a document is called a Bag-of-words (BoW) [88]. 2.4.2 Prior probability

We need a clustering model for this BoW document representation. This is where the prior probability (prior) comes into play, as we do need to specify the prior that a given document is related to a specific topic. We express the prior in Equation 2.2 below. It poses the question: what is the probability that the observed word is from topic K (without observing the document content)?

p( z i = K ) = K ,

(2.2)

where zi is the topic assignment vector, K is the topic number, and  = [1 , 2 , . . . , K ] represents the corpus-wide topic prevalence. In the BoW model representation the words of each document in the collection are going to be assigned a probability of occurrence within each uncovered topic. In this way, we will obtain topic probability vectors over words. For example, if K =4, z1 = (0.6, 0.25, 0.15), z2 = (0.8, 0.12, 0.08), z3 = (0.9, 0.08, 0.02) and, z4 = (0.5, 0.33, 0.17). Each vector component corresponds to the probability of occurrence of the wi in zK . Note that the vectors elements are ordered in decreasing order of probability. For details see [34]. Moreover, we can ask: what is the likelihood of seeing xi , given that the observation xi  topic K ? This can be expressed as: p(xi |zi = K, K , K ) = N (xi |K , K ), where N (·) is the Normal distribution with parameters K (mean) and K (covariance matrix). For more information on statistical and probability concepts, see [71].

11

Chapter 3

Literature Review
Information Retrieval (IR) using Probabilistic Topic Models, and in particular LDA, is a relative new area of research in machine learning [17]. In the following sections, we review the concept of TMs and their evolution from the tf-idf scheme to the LDA model. We also discuss the selection of number of topics in LDA and take a look into its applications, inside and outside the realm of SE.

3.1

Probabilistic Topic Models

The basic idea behind TMs is that, given a large unstructured text, the hidden topics are discovered. TMs are directed to answer the question: "what is this text talking about?". This concept is illustrated in Figure 3.1 using a toy example (as the text analyzed may contain billions of words [74]). Here, each of the words from the text is assigned to a topic, i.e., a list of related words on the same subject. The output of a TM is the most likely words per each of the most likely topics. We will see later that the number of topics and the number of words are parameterizable. A more formal definition of TMs, as per [14, 78], is that they are a set of machine learning techniques aimed to extract the thematic structure in a massive collection of documents. These are IR / Data Mining (DM) techniques that use the texts latent
12

CHAPTER 3. LITERATURE REVIEW

3.1. PROBABILISTIC TOPIC MODELS

Figure 3.1: A fitting of a small piece of text processed with a Topic Model, adapted from [14]

information for clustering the documents; i.e., grouping them according to the similarity found amongst them. The above mentioned techniques are algorithms that represent statistical methods to discover, and annotate hidden thematic structure of high volumes of information. All of them follow the same essential concept: a document is a mixture of topics and a topic is a probability distribution over words. A TM represents a generative model for documents [78], given that it stipulates a probabilistic procedure that generates documents. In order to generate a new document, we select a distribution over topics. Following this action, for each of the terms in that document, we select a topic randomly in accordance with this distribution, and then we choose a term from that topic. By applying statistical methods (such as statistical inference) we are able to reverse this process, and deduce the collection of topics that generated the set of documents. In Table 3.1, we show a real inference of five topics that we extracted from the DBA Q&A forum in stackexchange.com using the LDA TM. At the time of extraction, this collection consisted of over 50,000
13

CHAPTER 3. LITERATURE REVIEW

3.1. PROBABILISTIC TOPIC MODELS

Table 3.1: Example of obtaining 5 topics and 20 keywords using LDA from the dba.stackexchange.com corpus TOPIC 1 TOPIC 2 TOPIC 3 TOPIC 4 TOPIC 5 keyword server databas mysql user use sql connect log error can file master replic set run creat oracl slave tri data prob. 0.022 0.021 0.018 0.013 0.013 0.012 0.012 0.011 0.011 0.010 0.009 0.009 0.008 0.008 0.007 0.007 0.007 0.006 0.006 0.006 keyword index row queri tabl select time use order join column read product can data plan cost date scan one updat prob. 0.021 0.021 0.018 0.015 0.014 0.012 0.011 0.008 0.008 0.007 0.007 0.006 0.006 0.005 0.005 0.005 0.005 0.005 0.005 0.005 keyword tabl null key select creat name column valu use user insert row int can data queri default varchar type primari prob. 0.044 0.025 0.017 0.014 0.013 0.013 0.013 0.012 0.011 0.010 0.010 0.009 0.009 0.008 0.008 0.007 0.007 0.007 0.007 0.007 keyword select date name dbo end valu sys sql join error set object null declar type count procedur varchar max order prob. 0.023 0.016 0.014 0.014 0.011 0.011 0.009 0.008 0.008 0.008 0.007 0.007 0.006 0.006 0.006 0.005 0.005 0.005 0.005 0.004 keyword server databas sql use tabl file data innodb log size can queri mysql run will transact page buffer read time prob. 0.018 0.016 0.014 0.013 0.013 0.013 0.012 0.012 0.011 0.010 0.010 0.009 0.008 0.007 0.007 0.006 0.006 0.006 0.006 0.005

Q&A entries. Here we selected the first 20 keywords (terms) for 5 topics in which they have the highest probability of occurrence under each topic. The keywords of the topics are all related to databases. For instance, for topic 1, the arrangement of the output keywords suggests that the main issue expressed by the users, which posted in this forum (approximately 3,500 posts/ documents in topic 1 for year 2014), is related to how they can connect to the Database Management System (DBMS). With topic models, we are able to represent a text corpus as a collection of topics that are individually interpretable. These topics provide a probability distribution over terms that discerns clusters of coherent words [20, 78]. Figure 3.2 shows the general output of an inference performed with a TM (such as LDA), which is a matrix Mk,t , where K are the keywords (rows) and t are the topics

14

CHAPTER 3. LITERATURE REVIEW

3.2. GENERATIVE MODELS

Figure 3.2: Output of an LDA inference

(columns), where p(Ki )  p(Ki+1 ) and p(ti )  p(ti+1 ).

3.2

Generative Models

A probabilistic topic model is a stochastic model that indicates how it is possible to generate keywords in documents based on latent (i.e. random) variables [78]. The purpose of performing an inference is to uncover the hidden variables that are able to explain the observed ones. In Figure 3.3 (a) our toy example explains how documents can be generated using a TM. We have only two topics related to 1 (birds) and 2 (machinery). The containers represent BoWs [31]. The first drawing represents three documents that express a different distribution over words. For instance, the documents generated (1 and 3) were sampled exclusively from topic 1 and topic 2 and then the keywords from these documents have p = 1. 0
1 The

1

of coming from these topics, however, document 2 was generated from a mix-

value of p is chosen at random, and for illustration purposes

15

CHAPTER 3. LITERATURE REVIEW

3.2. GENERATIVE MODELS

Figure 3.3: Representation of a generative process and the problem of statistical inference, adapted from [78]

ture of topic 1 and topic 2. This means that the keywords generated in each document had p = 0.5 footnote[1] of coming from any of the two topics. In Figure 3.3 (b), we are concerned with knowing what are the topics that may have generated the documents, given the observed terms. This is the problem of statistical inference: how to derive the probability distribution over words for each topic, the distribution over topics with each topic, and additionally determining the topics that generated each of the terms? Note that the numbers above the words in the documents indicate what is the topic that sampled the word. TMs are able to capture polysemy. For example in Figure 3.3, the term crane can refer to a bird or to a machinery, and both topics can assign high probability to them.
16

CHAPTER 3. LITERATURE REVIEW

3.3. EVOLUTION OF TOPIC MODELS

3.3

Evolution of Topic Models

In this section we discuss the progression of TMs from the tf-idf scheme, to the Latent Semantic Analysis (LSI), to the pLSI, and to the LDA model. · Latent Semantic Analysis (LSI) was presented by S. Deerwester [28] in 1990. It is a well-known method for indexing and retrieving the hidden latent semantic structure of terms in a text corpus. LSI was created as a response to deal with the deficiencies of the tf-idf scheme. This model handles the problem of synonymy and polysemy, by collecting information on the semantic content of the documents, not solely on their words. LSI achieves this by implementing the Singular Value Decomposition (SVD) [53, 83] of the large matrix containing term-by-document elements that represents a document collection in an Information Retrieval system. The latent space of topics in the corpus (i.e., the vector space where the topics are found by the TM) is discovered by using word co-occurrence (i.e., the degree of semantic similarity in documents of the text corpus) [52]. For instance, consider a fragment of the Internet indexed by Google where the term "Database Design" will be close in the latent space for a pair of documents that contain related terms on the subject. However, within an intranet of a software development company two documents will be related exclusively if they share several terms. Even though LSI represents an advance in the probabilistic modeling of text, it still does not take into account the probabilistic model at the level of corpus. · Probabilistic Latent Semantic Indexing (pLSI) was introduced by Hoffman [24, 46, 47, 64] in 19992 . The pLSI differs from the LSI in that it offers a sound
2 pLSI

model is also known as the aspect model.

17

CHAPTER 3. LITERATURE REVIEW

3.4. LDA MODEL

statistical foundation. This means that it is based on the likelihood principle [11], which details a generative model for generating the words in the documents of the text corpora. pLSI models each term in a document as a sample that comes from a mixture model. The mixture is represented by the topics (random multinomial variables) modelled as a probability distribution of a fixed collection of topics. pLSI represents a good method for text analysis. However, it possesses two deficiencies [14]: 1) it consists of a large number of parameters that grow with the size of the corpus (and may lead to another major problem: overfitting), 2) it is not well known how to assign probability to a document that is not contained in the training set.

3.4

LDA model

LDA is the most popular modern TM and is the focus of our discussion. LDA was developed by Blei, Ng, and Jordan in 2002 [6, 13, 14]. LDA is a generative probabilistic model. The ideas behind it are as follows: · Treat the data from observations arriving from some sort of general probabilistic process, including the structure that we want to find in the data (i.e., hidden variables) that for documents reflect the thematic structure of the collection to which we do not have access. · Infer the hidden structure by using posterior inference. This refers to the calculation of the conditional distribution of the hidden variables, given the documents under study. · Allows situating new data into the estimated model.
18

CHAPTER 3. LITERATURE REVIEW

3.5. DYNAMIC TOPIC MODELS

Therefore, topic models such as LDA, help us to discover topics in a corpus. A topic is a distribution over terms. For example, let us consider an article (i.e. corpus), the different words that compose this corpus are related to a set of topics. Each topic represents a distribution over terms in the vocabulary, and different topics have different words with different probabilities. LDA is used in a broad range of applications covering audio, imaging, computer code, and social networks (please see section 3.7 for more information). In addition, to supply the corpus that we want to analyze with the model, we also need to provide K , which is the number of top topics to be identified.

3.5

Dynamic Topic Models

Unlike static LDA, where it is assumed that the order of the documents to be modeled does not matter, the Dynamic Topic Model (DTM) approach assumes that topics change over time. In this model, the order of the documents is respected, and instead of being a single distribution over words, topics become a sequence of distributions over words. With DTM it is possible to track how a particular topic changes over time [17, 22]. An illustration of a DTM can be seen in Figure 3.4.

3.6

Selection of the number of topics

Selecting the number of topics (K ) in the LDA model determines greatly its performance and outcome expected [39], hence the problem identified is as follows: An IT practitioner needs to discover a minor collection of issues including a broad fraction of documents. This need can be formally expressed as determining the value K , in such a way that the top X topics include a portion of the documents. This is challenging, because optimal K values may vary by research domain, and topic specificity increases with K . Probabilistic topic models, such as LDA, require to be provided with the value of
19

CHAPTER 3. LITERATURE REVIEW

3.6. SELECTION OF THE NUMBER OF TOPICS

Figure 3.4: Example of one topic extracted from the issues of Science journal published between 1880 and 2000 (every 10 years as shown in [22], but shown every 40 years here). Top panel shows the tracking of ten top words for this topic for a given year; and bottom panel shows how the inferred probability of terms associated with this topic (that was labeled with "neuroscience"by an analyst) varies throughout the years. Adapted from [22].

20

CHAPTER 3. LITERATURE REVIEW

3.6. SELECTION OF THE NUMBER OF TOPICS

number of topics K . Internally the model also needs two additional parameters  and  that need to be calculated for each parameter K [14]. There is limited work made on the selection of the number of topics. Many authors, such as Grant and Cordy [38] and Taddy [79], recognize this challenge. Wang et al. [85] point out that the processing time necessary to find the optimal values of this parameter by iterating may be very large. Taddy [79] identifies three methods for learning the number of topics from data. The first one is Cross-Validation (CV), the second one is non-parametric model such as Hierarchical Dirichlet Process (HDP), the third one is marginal likelihood. Cross Validation (CV) is considered to be the most common method selected by researchers. More specifically, n-Fold CV is a technique to evaluate the performance of the prediction model. Here, the data is split into n segments of about the same size for training while reserving a portion of data (held-out data) for testing purposes. Then the model is repeatedly learned on the training sets data, and evaluated based on their goodness-of-fit to held-out test data, where fitness is based upon a statistical measure, such as perplexity or likelihood. Models are evaluated for several K values, and the one with the best fit is selected [2, 45]. However, CV is not a large scale method and the statistical/error information that it provides may be difficult to understand [42]. In the non-parametric technique, hierarchical Dirichlet processes [80], infer the number of topics automatically [45]. In marginal likelihood, the problem of selecting the optimal K becomes an optimization problem to maximize the log likelihood of the data, addressed by using a variational expectation-maximization procedure [14]. Additional techniques that researchers have developed include: Arun et al. [7] who propose a measure for selecting the correct K based on viewing LDA as a matrix factorization mechanism and computing the Kullback-Leibler divergence (KL), and Greene et al. [40] who propose a term-centric stability analysis strategy.

21

CHAPTER 3. LITERATURE REVIEW

3.6. SELECTION OF THE NUMBER OF TOPICS

Heuristics is another way to determine the number of topics. Here, researchers identify K with "prior knowledge" of the domain [19]. For example, Zhao et al. [92] proposes a method based on the analysis of variation of statistical perplexity [51] during topic modeling. In practice, researchers typically employ prior knowledge or CV to select K . Some using prior knowledge do not elaborate on their rationale for choosing K , as in [86], where K = 5 topics is used to categorize developer interactions. Others provide some justification; for example, to study topics in the Q&A Website Stack Overflow, Barua et al. [10] chose K = 40 to provide topic sets with medium granularity which implies that the topics represent the general tendency in the data set, while staying different from each other. Yet others report using a similar rationale, but experimenting with several values for K before settling on the optimal value. Some determine the optimal value based on manual inspection from among the experimental K values, as having the best quality topic set [1]. Others employ CV to determine the optimal K . For example, to predict customer behaviours, values of K from 5 to 18 are used on two datasets, with 3- and 4fold CV, respectively [59]. Each fold's optimal K is identified using the log-likelihood, and one final K for a dataset is calculated as the average over its folds. In some situations, the optimal K is identified using an oracle. This occurs when the topic model itself is being studied by evaluating its ability to uncover topics and relationships which are known in advance. In such work, e.g. [37, 63, 82], models are created for various values of K , and an optimal model is selected as the one being most accurate according to the oracle. CV is the most advocated means of model selection, and thus determination of optimal K value [45]. In fact, Arcuri and Fraser argue that n-fold CV is a precondition of achieving scientifically-sound results [5]. For the researcher wishing to use LDA, perform-

22

CHAPTER 3. LITERATURE REVIEW

3.7. APPLICATIONS OF LDA

ing n-fold CV is often impractical: it is resource-intensive, and the functionality is not available in any toolkit. Acruri and Frasier note that few SE researchers attempt CV; most simply select the model (and thus K value) based on the entire pool of data, calling into question the validity of the results [5] . Without a practical means of performing CV, the researcher must rely on guidelines and heuristics. Such guidance for selecting K is crucial for LDA, since its performance is particularly sensitive to the value of K chosen [45], and there is no single value of K that is appropriate in all situations and all datasets [38, 84]. Selecting quickly the value for K is the objective that we would like to tackle in this thesis.

3.7

Applications of LDA

LDA is ubiquitous and is used in many research fields. For example, for finding patterns in music [44]. In population genetics it is leveraged to discover ancestral populations [17, 33]. In computer vision, for classification and organization of images, where they are analyzed to became collections of "visual words" [17] including analysis of video [87, 88]. In bioinformatics, where the job of the TM is focused on three major tasks: biological data clustering analysis, biological data classification, and biological feature extraction [54]. We will now focus on applications of LDA in the area of SE. We shall note that the application categories explained may overlap between each other (i.e., both Feature Location Technique and Software Traceability have a goal to automate the comprehension of SW artifacts).

23

CHAPTER 3. LITERATURE REVIEW

3.7. APPLICATIONS OF LDA

3.7.1

Bug localization

This is one of the most common tasks in SE. Here the developers use information about a bug in order to identify the faulty code where the bug resides. Once this have been achieved, they can work on its resolution. There have been previous efforts in IR-based bug localization. For instance LSI-based techniques have been used in source code retrieval that include clustering and feature location [60], however it has been proved that LDA-based technique is more effective [57]. Lukins et al. [58] have proposed a LDAbased technique that performs automated bug localization on Agile software. Essentially, a model is created using LDA by inputting the issue descriptions in the Issue Tracking System (ITS). With the model created, a query (made by the developer debugging the code) based on the issue description of the bug that needs to be fixed, inquires the model and provides the probable location of the piece of code that needs to be fixed. We should note that the query for finding the bug is as effective as the developer debugging the code that creates it. An experienced developer will yield to better results that will outline the bug in a query. Bug localization automation may decrease the costs associated with maintaining SW (i.e., developer effort). 3.7.2 Software traceability

With the growing complexity of SW systems, automated SW traceability has become an essential function in SE. In large companies SW projects often comprise of thousands of SW artifacts (i.e, source code, analysis and design documents, test plans and test cases, bug reports, etc.). The objective of Software Traceability is to identify the relationships amongst these artifacts. Basically, SW traceability creates a cross-reference (by means of links) between the SW and the artifact (e.g., linking source code to use cases). Asuncion et al. [9] identify two categories of SW traceability: retrospective traceability (that

24

CHAPTER 3. LITERATURE REVIEW

3.7. APPLICATIONS OF LDA

takes place retroactively in static sets of artifacts) and prospective traceability (that takes place on-line, at the moment the artifacts are being produced). They propose a prospective traceability technique based on LDA. Gethers et al. [37] propose an approach that combines orthogonal Information Recovery techniques to recover traceability links in software artifacts. 3.7.3 Feature Location

LDA-based Feature Location Technique (FLT) has the purpose of finding the source code that implements certain functionality. SW systems suffer continuous code changes to adapt to new functionality. Typically, a developer -- who is new to the SW -- will struggle trying to comprehend it, so s/he can identify the part of the source code where the change needs to be implemented. Given that by using FLT the effort that the developer spent in this task is decreased, the SW cost will also do so. A study of LDA-based FLT is provided by Biggers et al. [12], in which they measure the performance effects of implementing FLT on several Java systems. For a detailed analysis and categorization study of FLTs see [30]. 3.7.4 Source code labeling

It is intended to assist the developer in understanding source code with labels, for instance, software visualization tools or a collection of representative terms. Labels provide developers with a "quick look" information to focus on the software component areas which they need to analyze in details to make a decision. These labels can be gathered using an IR method, such as LDA. Panichella et al. [65] investigates the way in which the source code artifact labelling is implemented using LDA.

25

CHAPTER 3. LITERATURE REVIEW

3.8. PREREQUISITES OF LDA

3.7.5

Test case prioritization

It is recognized that software developers are not always able to execute large sets of tests for every single source code change they have, considering the amount of time and effort that this action would involve. Then, test case prioritization becomes a solution to save time and effort in this process. Thomas et al. [81] propose a technique that uses linguistic data in the test cases (i.e., identifier names, string literals, and comments) to map functionality to each test case. The test cases can then be prioritized for execution.

3.8

Prerequisites of LDA

In order to implement LDA, it is necessary to perform a preparation over the dataset under study. This preparation refers to basically removing "stop-words" (such as "the", "of", or "and"). Otherwise they will pervade the topics that have been learned and may cover semantic word patterns that can be important for us [27].

3.9

Limitations of LDA

Despite being the most popular Topic Model in use, LDA presents the following disadvantages, that some authors have tried to mitigate: 1. Number of topics (K ). As discussed above, it is a pre-requisite to provide K by the user in order to implement LDA. As we have mentioned, with a K too small the topics produced would be very wide, whereas a too large K would produce topics that are difficult to differentiate. To overcome this problem, a non-parametric model called Hierarchical Dirichlet processes [80] can be used to learn the optimal K . However, this process is very onerous computationally. Choosing K is a central discussion in this thesis.

26

CHAPTER 3. LITERATURE REVIEW

3.9. LIMITATIONS OF LDA

2. Labeling. LDA does not provide labelling for the topics inferred. It is up to the user to describe what is the label of the topic that the keywords are referring to. For example, in the inferred topic keywords "school, teacher, blackboard, president, book", it should be labeled as "education". In this regard there are efforts that try to provide labels to topics automatically [61]. 3. BoW assumption. Words that need to be inferred together in the same topic (as one word), such as "Royal Canadian Mounted Police", are disaggregated in their component words and assigned to different topics. This is because the BoW assumption allows it. This assumption can be relaxed using LDA extension implementation, such as [15].

27

Chapter 4

Methodology
We have reviewed the basic concept of topic models including LDA. In this chapter we will develop the LDA model by explaining its mathematical foundations.

4.1

Notation and terminology

In order to start explaining the models, we need to formally define a common terminology, such as words, documents, and text corpus [14]. Our discussion will involve the following: w ­ will refer to a term or word in the vocabulary (V ). It is the basic unit of discrete data, and wi  V . d ­ will refer to a document defined as a sequence of N words, described by d = {w1 , w2 , . . . , wN }, where wn represents the nth word in the sequence. M ­ will refer to the number of documents in the text corpus, denoted by D, consisting of M documents: D = {d1 , d2 , . . . , dM }. K ­ will refer to the selected number of topics. z - will refer to an unobserved topic that pervades the corpus (there would be K unobserved topics in total).  ­ it is a multinomial distribution used to model the topic proportions.  ­ it is a multinomial distribution of z .
28

CHAPTER 4. METHODOLOGY

4.2. FROM PLSI TO LDA

Figure 4.1: Diagram of the pLSI model without using plates notation

 and  are the hyperparameters of the model;  controls the topic distributions per document, while  controls the terms' distribution per topic.

4.2

From pLSI to LDA

The starting point, in order to understand how LDA works, is to continue discussing pLSI. We present in Figure 4.1 the pLSI model in graphical form. Here the nodes represent random variables, the solid ones are observed variables, because we know their outcome. The arrows symbolize dependency, this means that the outcome of the target variable depends on the value of the origin variable [24]. To simplify representation of the models we are going to introduce the concept of rectangular plates to denote multiple repetition in our models. With them we can discard the use of repeating nodes and use indexes, greatly simplifying diagramming of the models. For additional information on plate notation, see [90]. In Figure 4.2 we present the same diagram of pLSI using "plate notation". The pLSI model establishes that a document d and a word wn are conditionally inde-

29

CHAPTER 4. METHODOLOGY

4.3. GRAPHICAL REPRESENTATION OF LDA AND DTM

Figure 4.2: Diagram of the pLSI model using plates notation

pendent, given an unobserved topic z , and can be represented as: p(d, wn ) = p(d)
z

p( w n | z ) p( z | d ) .

(4.1)

pLSI has two major issues [14]: · Lacks of clarity on how to generate probability of not yet seen documents. · The number of parameters grows linearly in M , which may lead to overfitting. LDA addresses these two problems by introducing a K -parameter (K -topic), instead of using a large collection of parameters related to the training set, therefore, eliminating the possibility of overfitting.

4.3

Graphical Representation of LDA and DTM

Now, let us illustrate the graphical models for LDA and DTM. 4.3.1 LDA graphical model

Figure 4.3 illustrates the LDA model in the graphical form. As in the pLSI model, the nodes represent random variables and the solid ones are observed variables. The arcs
30

CHAPTER 4. METHODOLOGY

4.4. DIRICHLET DISTRIBUTION

also represent dependency. 's are topic proportions, the z 's are the topic assignments and the w's, are the observed words. Note that  and  are called hyperparameters.  governs the amount of smoothing of the topic distributions per document [78], and  determines the amount of smoothing of the word distributions in every topic [78]. The same graphical model can be rewritten in plates notation, as illustrated in Figure 4.4. 4.3.2 DTM graphical model

In Figure 4.5 we can appreciate the DTM graphical model, here as we have reviewed, the model keeps track of topics changing over time [22]. Contrary to static LDA that makes the assumption that documents are exchangable (i.e., their order is unimportant for determining the topics that pervade the documents, and the disaggregation of every document into those topics), it is a sin equa non condition for a DTM that the corpus be organized as a sequential collection of documents. In this Figure each LDA model represents a slice of time and K,t represents a vector for topic K in slice t. In order to make the model work for sequential modeling, the static LDA models are connected (i.e., K,1  K,2  . . .  K,T ) and each slice t depends on the previous slice t - 1.

4.4

Dirichlet distribution

As described in Chapter 3, Blei et al. [14] broaden the pLSI model by bringing in the LDA model and expanding it with a Dirichlet prior on . The probability density of a K -dimensional Dirichlet distribution is: Dir(1 , 2 , . . . , K ) = ( i ) i (i )
i K i=1  i -1 i ,

(4.2)

31

CHAPTER 4. METHODOLOGY

4.4. DIRICHLET DISTRIBUTION

Figure 4.3: LDA graphical model without using plates notation.

32

CHAPTER 4. METHODOLOGY

4.4. DIRICHLET DISTRIBUTION

Figure 4.4: LDA graphical model using plates notation, adapted from [14]. The components with the dashed lines refers to the smoothed LDA model.  is a multinomial distribution that is sampled from a Dirichlet distribution with parameter  . This sampling occurs repeatedly for each topic until K topics have been produced.

33

CHAPTER 4. METHODOLOGY

4.4. DIRICHLET DISTRIBUTION

Figure 4.5: Graphical model representation of the DTM, adapted from [13].

34

CHAPTER 4. METHODOLOGY

4.5. THE SIMPLEX

where  denotes the Gamma function and the  is a K -dimensional vector with elements i > 0. Moreover, the generative process of the LDA model can be described in terms of the following joint distribution that includes a topic mixture , a set of N topics z , and a collection of N words w [14, 16, 17]:
K

p(, z, w|,  ) = p(|)
n=1

p( z n |  ) p( w n | z n ,  ) .

(4.3)

As we have seen, LDA is a generative probabilistic model, where the documents are shown as random mixtures over latent topics and each document is a distribution over words. Generating each document w in a corpus D is done as per Algorithm 1 [14, 89].
Data: words w  document d Result: topic assignments z begin foreach topic z from a Dirichlet distribution with parameter  do Choose a multinomial distribution z end foreach document w do Pick a multinomial distribution d from the Dirichlet distribution with parameter  end foreach of the N words wn do Choose a topic z  1, 2, . . . , K from the multinomial distribution d ; Choose a word w from the multinomial distribution z ; end end Algorithm 1: LDA algorithm, adapted from [14].

4.5

The Simplex

The Latent Space where the Dirichlet distribution "lives" is also known as the simplex1 . The Dirichlet variable  is defined in the (K - 1)-simplex,  exists in this space if the following conditions are met: i  0,
1A

K i=1 i

= 1 [15].

In order to explain this concept, we will start by exemplifying using only words, and
generalization of the concept of triangles to an arbitrary number of dimensions.

35

CHAPTER 4. METHODOLOGY

4.5. THE SIMPLEX

then using both words and topics. Considering only three words (e.g., a, b, and c). For convenience, we will call the (K - 1)-simplex as the simplex; and in this case it would be a 2-dimensional simplex (or 2-D triangle), with each of the vertices representing each of the words2 . The outcomes of this multinomial distribution are represented by a vector P of probabilities of the elements a, b and c that sum to 1. We can start by looking at a probability distribution that occurs in this 2-D Simplex. Let us suppose that a pd of a word situates at vertex of word a (which will denote by A to differentiate a word from a vertex), then P = {1, 0, 0}. If we situate the pd at the , 1 , 0}, if the pd is situated at the centroid of the middle of the edge A - B , then P = { 1 2 2 triangle, then P = { 1 , 1 , 1 }, which implies that all three words have the same probability 3 3 3 of occurrence. Figure 4.6 illustrates this concept3 . One of the parameters of the Dirichlet distribution is , in Figure 4.7 we present several variations of the shape of the distribution, based on different values of this parameter. We can see that in Figure 4.7(a) the distribution places some probability in words B and C , in Figure 4.7(b) it places probability on words A and C , while in Figure 4.7(c) places less probability in A, C and in figure 4.7(d) places probability in A, B . Observe that the peakiness of the distribution is determined by the value of , if its value is small it means that the Dirichlet is spread out, while if its value is greater means that the distribution is more peaky. Values of  < 1 imply increased sparsity, placing pd at the corners of the simplex [16] (see example in Figure 4.8). Now that the concept of word simplex has been illustrated, we proceed to exhibit the combined representation of probabilities of words and topics in the simplex. LDA contemplates that the word simplex contains K points that constitutes a sub-simplex
2 If all of the probability density resides in one of the corners, say corner A, then almost certainly (i.e., with probability 1.0) the outcome would be A. 3 It is not possible for a term in a Dirichlet distribution to have p = 0 or p = 1, however, for illustration purposes we will allow that.

36

CHAPTER 4. METHODOLOGY

4.5. THE SIMPLEX

Figure 4.6: Contour plots of different pd's when the weight of the distribution (dark big point) is centred in a particular point. Adapted from [16]

37

CHAPTER 4. METHODOLOGY

4.5. THE SIMPLEX

Figure 4.7: Dirichlet distribution with variations of  when weight of the distribution is distributed on the plane between the three words, adapted from [16]

38

CHAPTER 4. METHODOLOGY

4.6. POSTERIOR CALCULATION FOR LDA

Figure 4.8: Dirichlet distribution with  < 1, increased pd at the corners of the simplex

which is known as the topic simplex. Therefore any point in the topic simplex represents a point in the word simplex too. In LDA the words of the observed and unseen documents are generated at random by selecting a topic that is sampled from the smooth distribution with a parameter selected at random in the topic simplex. The LDA states that every word is represented as a distribution of words over a distribution of topics. This is portrayed in Figure 4.9.

4.6

Posterior calculation for LDA

By applying the Bayes' rule to the LDA model, the direct approach for calculating the topic inference equation becomes: p(, z |w, ,  ) = p(, z, w|,  ) , p(w|,  ) (4.4)

39

CHAPTER 4. METHODOLOGY

4.6. POSTERIOR CALCULATION FOR LDA

Figure 4.9: Word and topic simplex embedded, adapted from [14]. This geometrical representation of the simplex considers 3 words and 3 topics. Looked perpendicularly from above, the contoured lines represent a smooth distribution placed by LDA. The vertices of the word simplex represent distributions with p = 1 while the ones of the topic simplex mean different distributions over words.

40

CHAPTER 4. METHODOLOGY

4.7. APPROXIMATION TECHNIQUES

where p(w|,  ) = ( i ) i (j )
i

K

(
i=1

 i -1 i )(

N

K

V

(i i j )wn d

j

(4.5)

n=1 i=1 j =1

is the marginal probability of the observations. Equation 4.5 is defined as the posterior distribution (posterior), and resolving it has become a fundamental problem of statistical inference. However, there is a limitation for computing this distribution: the denominator is intractable. Given this constraint, it is possible to approximate the resolution by using sampling based algorithms and variational algorithms. In the following section we will discuss these techniques.

4.7

Approximation techniques

Here we will discuss two of the available methods4 for approximating the posterior: CGS and variational inference. 4.7.1 Collapsed Gibbs Sampling

CGS [1, 8, 27, 41, 66] represents a sampling based technique for approximating the posterior of LDA. CGS belongs to the family of algorithms of Markov Chain Monte Carlo (MCMC) [3, 36], and it is straightforward to be implemented, being able to extract from a large corpus a collection of topics. Building a Markov chain using MCMC algorithms, CGS will converge after a number of iterations are executed [27]. It is called "collapsed" because  and  are marginalized out, and only the latent variables z is sampled [66]. This has the effect of reducing drastically the space that is being explored by the algorithm, and therefore it will have a better performance. The multinomial parameter for topics in a document j means which topics emerge in document j , while the multinomial parameter for words in a topic K indicates which
4 Note

that the LDA R package (that we will discuss in Chapter 5) implements Collapsed Gibbs Sampling (CGS).

41

CHAPTER 4. METHODOLOGY

4.7. APPROXIMATION TECHNIQUES

words are important for topic K [41]. We would like to use CGS to approximate the variables  and , however it is not a direct approximation. These variables can be estimated by using approximations of z (Equation 4.6) [66]. CGS is an iterative process: the start of the process is called "burning" phase, where the Gibbs samples are poor estimates. In this phase, the samples are rejected and the consecutive ones start to estimate the distribution of interest. For more details see [23, 68, 78]. 1 aKj bwK , Z

p(zij = K |z ¬ij , x, ,  ) =
¬ij where aKj = NKj + , bwK = K
¬ij NK +V  ¬ij NwK +

(4.6)

, and Z is the normalization constant Z =

aKj bwK .
j

The superscript ¬ij means that it is omitted from NwK =
j

N wKj . NKj =

w

NwKj and

NwKj are counts of the number of times a word w has been assigned to

topic K , and the number of times a word in document j has been assigned to topic K , respectively. One iteration of the CGS consists in choosing a sample for zij as stated by (4.6) for each term i in each document j . Taking into account the value sampled for zij the counts NKj , NK , NwK are brought ^j and  ^K : up to date, and with this sample we are able to approximate  ^w K = NwK +   NK + V  ^K j = NKj +   Nj + K

(4.7)

(4.8)

Implementation of CGS for LDA is shown in pseudocode of Porteous et al. [66] in Al-

42

CHAPTER 4. METHODOLOGY

4.7. APPROXIMATION TECHNIQUES

gorithm 2. After the CGS algorithm is done, the variables  and  can be approximated. In the algorithm, N is a count variable for keeping track of the number of times any word is assigned to topic K ; w is the representation of the text corpus; and z is used to handle the current topic assignment for each of the N words in w. An additional representation of CGS is given by Gao et al. in [35].
begin foreach i = 1 to N do u  draw from Uniform[0,1]; foreach k = 1 to K do P [k ]  P [k - 1] +
¬ij ¬ij (Nkj +)(Nx k + ) ¬ij Nk +V  ij

;

end foreach k = 1 to K do if u < P [k ]/P [K ] then zij = k , STOP end end end end Algorithm 2: LDA Gibbs sampling, adapted from [66].

4.7.2

Variational inference

Variational inference is another available technique to infer the posterior [1, 14, 21]. In this case this inference is performed through optimization (which we will discuss below). Equation 4.9 is often called the variational model, which assumes independence among all the latent variables and it is the result of simplifying the graphical model of LDA, where only the nodes  and z have been left, and nodes  and  (variational parameters) have been inserted (see Figure 4.10).
N

q (Z, |, ) = q (| )
n=1

q ( Zn |  n ) ,

(4.9)

where  is a Dirichlet parameter and  = (1 , . . . , n ) are the multinomial parameters. The problem of variational inference becomes a problem of optimizing the variational
43

CHAPTER 4. METHODOLOGY

4.7. APPROXIMATION TECHNIQUES

Figure 4.10: Diagram of the LDA variational model, adapted from [1]

parameters  and . This can be achieved by minimizing the distance between the true model, p(, Z |,  ), and the variational model (for instance using the KL [43]): min KL[q (, Z |, )
,

p(, Z |,  )].

(4.10)

This optimization problem is not tractable, however it can be estimated iteratively [1] by using the following two equations: i = i + i =1 K ni , (4.11)

ni  iwn exp[(i )], where  is the bi-gamma function. We present a variational inference procedure in Algorithm 3 as per [14].

(4.12)

44

CHAPTER 4. METHODOLOGY

4.7. APPROXIMATION TECHNIQUES

initialize 0 ni - 1/K for all i and n; initialize i  i + N/K for all i; repeat for n = 1 to N do for i = 1 to K do +1 t ni - iwn exp((i )); end +1 to sum to 1; normalize t n end N +1  t+1 -  + n=1 t n ; until convergence ; Algorithm 3: Variational inference algorithm, adapted from [14].

45

Chapter 5

Implementation and Calibration
In this chapter we will perform analysis in the following freely available datasets from the Questions and Answers (Q&A) forums of stackoverflow.com: android, dba and salesforce. StackOverflow consists of the StackExchange network of Q&A sites, which are forums for professional and enthusiast programmers that can post questions and answers on a forum associated with a particular product(s). First, we will start by exemplifying the usage of LDA in the dba dataset only, for the sake of brevity. Second, we will assume the problem of finding a formula to quickly select the number of topics K for implementing LDA. The programming was done using the LDA implementation in the R language package, with the exception of one script programmed in Perl.

5.1

Processing of datasets

All our datasets will undergo a common processing comprised by the following steps (see Figure 5.1) which include:

46

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.1. PROCESSING OF DATASETS

Figure 5.1: Process of data for performing a LDA inference

1. Data Extraction. The text corpus used was the dba forum of StackExchange (https: //dba.stackexchange.com). The dataset was extracted from https://archive. org/download/stackexchange [4]. 2. Data Conversion. The file from the repository was in Extensible Markup Language (XML) format and for performing the LDA inference we needed the data in Comma Separated Value file (CSV) format; therefore, a Perl script for doing the conversion was created: "xml2csv.pl" (see Appendix B.1). This script keeps the text for all the questions but eliminates those answers that have less than three votes and were not officially accepted as an answer (by the person who asked the question). 3. Data Cleansing. This step involved the following actions: (a) Converting to lowercase all words. (b) Removal of punctuation signs. (c) Removing stopwords (i.e., most common words in English, such as "and", "the", or "a"). (d) Stemming applicable words (i.e., reducing words to their word stem, such as "computers" and "computing" to "comput"). (e) White-space char elimination.

47

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

(f) Removing terms which have tf-idf smaller than the median of all the tf-idf values. I.e., we eliminate the terms with low "importance". 4. LDA inference. Here is where the LDA is applied to the data. This processing is particular to what our objective is. In our case, we have two types of processing, the first one is to illustrate the usage of LDA by generating a matrix of topic-keywords (similar to Figure 3.2), and the second one is to generate term frequencies to perform analysis and deduce the formula for calibrating the model.

5.2

Mining of Q&A forums

The volume of Q&A in stackexchange.com forums varies from forum to forum. In the case of database-administrators-site (extracted from https://dba.stackexchange.com) [76], there were approximately 50,000 entries at the time we downloaded the data. The file that we extracted included several periods by year and month. This repository contains unstructured data for both questions and answers. One of the managers in charge of the product might be wondering what the users are talking about it. Then, LDA can be applied and its output provide the necessary analytics to help responding this question. All LDA-related computations are performed using R-package `topicmodels' [48] (which in turn uses the C code implementation of LDA by Blei et al. [14]). For performing the analysis to illustrate the usage of LDA, we follow the processing of the dataset as depicted in Figure 5.1. In this case the "LDA Inference" step was accomplished by the developed program "analysis db2 idf.R" (see Appendix B.2) which involves these specific tasks: 1. Extraction of data will be by year (2015) and by their respective months. 2. The number of topics to be extracted will be 5, and the number of keywords per topic will be 20 (specifically for this example).
48

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

3. The probability per topic/keyword will be outputted. 4. The idf will be calculated. The output of this LDA inference for each of the five topics would produce a list of keywords, their probability of belonging to a given topic, number of documents, and idf. Example of an output for topic 1 is given in Table 5.1.
Table 5.1: Topic 1 of the LDA inference of 5 topics and 20 keywords KEYWORD PROBABILITY #doc idf server databas mysql user use sql connect log error can file master replic set run creat oracl slave tri data 0.022 0.021 0.018 0.013 0.013 0.012 0.012 0.011 0.011 0.010 0.009 0.009 0.008 0.008 0.007 0.007 0.007 0.006 0.006 0.006 1582 1497 710 948 1631 1151 874 714 959 1582 829 407 481 921 1036 902 393 288 1018 847 0.214 0.238 0.562 0.436 0.200 0.352 0.471 0.559 0.431 0.214 0.494 0.803 0.731 0.449 0.397 0.458 0.818 0.953 0.405 0.485

For better appreciation of the data, let us render the data in this table in the graphical format. Figure 5.2 shows the probability of occurrence in descending order for each of the keywords in topic 1. Figure 5.3 shows the number of documents1 , where each of the keywords appears. The last plot in this sequence is Figure 5.4; it shows that the closer the value of idf to 0 is for a term, the more widespread the term is, and vice versa. Now, let us plot the keywords of topic 1 to show its idf and probability as Figure 5.5
1 The

overall number of documents for topic 1 is 2587 (N ).

49

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

Figure 5.2: Probability per term for Topic 1

indicates. The figure shows that the most important and widespread words are `server` and `databas` and the the least widespread ones are `master`, `oracl`, `slave`. Eyeballing the overall set of keywords, we may conjecture that this topic aggregates Q&A related to master/slave ­ replication connect/error ­ how to connect to db? etc. We would like to mention that a manual validation of the subsets of documents has taken place.

50

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

Figure 5.3: Document frequency per term for topic 1

51

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

Figure 5.4: Keywords vs idf for topic 1.

52

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.2. MINING OF Q&A FORUMS

TOPIC 1 2015 - db2
0.025 0.020
server databas

mysql

probability

0.015

use

user sql connect error file run set creat data replic log master oracl slave

0.010 0.005

can

tri

0.000 0.2

0.4

0.6 idf N= 2587

0.8

Figure 5.5: LDA inference for topic 1 showing terms plotted against its idf and probability.

53

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

5.3

Calibration

In this section we will develop our proposed model for calculating in a straightforward way the number of topics for implementing LDA. Calibration of LDA can consume a large amount of computing resources. Steyvers and Griffits [78], agree on the idea that selecting the number of topics influences greatly the understandability of the results produced by LDA. A K which is too small can produce very broad topics, and one which is too large may affect the interpretability of the topics. Our aim is to create a simple formula for readily calculating the number of topics, therefore addressing our research question: How can we quickly select the number of topics K so that the top X topics include a certain fraction F of the N documents under study? Note that we do not speed up computation of a particular LDA model. Rather, we drastically reduce the number of LDA models that have to be computed and calibrated. In the worst-case scenario, one will need to compute up to N models, while usage of our formula reduces the number of models to, at best, one. In practice, given that our approximation is not perfect, one may have to compute a couple ofLDA models to obtain the desired value of F . 5.3.1 Datasets

We will use for our study the three downloaded datasets from stackoverflow.com. They are: 1. android (android.stackoverflow.com)[75] 2. database administration(dba.stackoverflow.com)[76] 3. salesforce (salesforce.stackoverflow.com)[77]
54

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

All of them can be downloaded at https://archive.org/download/stackexchange [4]. In Table 5.2 we show a summary of these datasets.
Table 5.2: Datasets considered Dataset android dba salesforce Selected data from-to (period) 2014-2015 (monthly) 2012-2014 (monthly and quarterly) 2014-2015 (monthly) Number of questions 15765 39174 25965

As we plan to process several periods for these datasets, the expanded table of processed datasets is as Table 5.3 illustrates.
Table 5.3: Expanded list of number of subsets processed by dataset. Year refers to the yearly data that is available to be extracted from the corpus. Dataset android android dba dba dba dba dba dba salesforce salesforce Periodicity monthly monthly monthly monthly monthly quarterly quarterly quarterly monthly monthly Year 2014 2015 2012 2013 2014 2012 2013 2014 2014 2015 Totals Num. of files 12 12 12 12 12 4 4 4 12 12 96 Num. of subsets 588 588 588 588 588 196 196 196 588 588 4704

5.3.2

Processing of datasets for model calibration

In order to generate the information to perform our analysis for deducing the formula to calibrate the model, we will execute the processing of datasets as depicted in Section 5.1. The specific step corresponds to the LDA inference consists in processing each considered dataset (android, dba, and salesforce) for several periods: years-months/quarters, comprising 96 data files. Moreover we consider each of these yearly files to be divided into
55

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

temporal subsets (i.e., 12 for monthly and 4 for quarterly subsets) times the number of top X we computed (49, i.e. X = 2, 3, . . . , 50). We will produce for each dataset/period the topic frequency for the words contained in them. For each dataset, we will calculate the distribution of the number of posts per topic, with K = 2, 3, . . . , N . For each K and for each top topics X = 2, 3, . . . , 50, we will calculate the empirical value of F as follows:

F = P/N,

(5.1)

where P is the overall number of documents in top X topics. We achieve this, by running the developed programs: "do lda analysis topics.R" (See Appendix B.4) to process by year-monthly and the program "do lda quarter inc.R" (See Appendix B.5) to process by year-quarterly. Once we have generated the frequency files for each dataset-year-period (monthly/quarterly), we proceed to bind all of these files in one file, we achieve this by running the developed program "convVar k200.R" (See Appendix B.7) After doing this we commence our analysis for inferring our formula. The developed scripts for supporting this analysis are: "verifyFit k200.R" (See Appendix B.8) and ­ "verifyFit k200.R analysis.R" (See Appendix B.9) for generating the performance plots. 5.3.3 Time consumption to iterate over K

Figure 5.6 illustrates the processing time to iterate over K in order to determine the number of topics for the model. Here, we have plotted the times needed to compute LDA model for a given K , while K = 2, 3, . . . , N . We arbitrarily chose the Android dataset, for January of 2014. This data subset has N = 614. Notice that time to compute the LDA model grows (non-monotonically) with the increase of K . The maximum time for computing the model (when K = 602) is  6690 seconds; the overall time to compute

56

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

all 613 models for this subset is  1276576 seconds (or  15 days). The times shown here are for one subset; however, the behaviour is similar for all the subsets under study. Thus, it is beneficial to reduce the number of calibrations of LDA models. 5.3.4 Creating fitted values

We notice that the graph of log(F ) against log(K ) for a given X forms a straight line (e.g., see Figure 5.7), suggesting Power Law relationship between F and K [55], [62]. The law takes form F = aK b , where a and b are some constants. Log transformation of Equation 5.2 yields: log(F ) = log(a) + b log(K ). (5.3) (5.2)

Eyeballing Figure 5.7 suggests that the values of log(a) and b vary (at least with the change in X ). The question then becomes: can we estimate the values of a and b for a given dataset based on some attributes of the dataset? Given that N and X are (almost) readily available to us, let us assume that a and b are governed by N and X . In order to find a(X, N ) and b(X, N ), we will compute empirical values of a and b, denoted by a ^ and ^ b, respectively, by fitting linear regression model (Equation 5.3) to the data points for each dataset and for each value of X (we set X = 2, 3, . . . , 50). We will consider two models: 1) a more flexible one, where a and b are independent of each other and 2) a more constrained one, where a = X -b . The second model is obtained via ansatz (an educated guess). Further exploratory analysis shows that if K > 0.75N or if K > 200, then the linear relation between log(F ) against log(K ) breaks (example is shown in Figure 5.8). From

57

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

practical perspective, an analyst would rarely be interested in a large number of topics K . Thus, if we filter out data for large values of K , then we may improve the fit of linear models while preserving the models' practicality. We will try two different filters, hence the two datasets: 1. Dataset 1: retain the data if K  0.75N ; 2. Dataset 2: retain the data if K  0.75N and K  200. We discuss in the following sections the fit of the flexible and the constraint models. Comparison of the models is given in Section 5.4.
Flexible Model

In the flexible case, Equation 5.2 becomes F = a(X, N )K b(X,N ) . Solving it for K yields: F K= a(X, N ) (5.4)

1/b(X,N )

.

(5.5)

The value of N can be easily extracted from the dataset. The values of F and X are provided by an analyst. However, we still need to define the functional form of a(X, N ) and b(X, N ) to compute Equation 5.5. To compute a(X, N ) we examined a number of relations between a ^ and X, N . A regression of the form a ^ = 1 + 2 ln(X ) + 3 ln(N ) (5.6)

yields good results. For both datasets, the regression model is valid and statistically significant. In the case of Datasets 1 and 2 it explains most of the variability (R2  0.87 and R2  0.96, respectively). As expected, the fit to Dataset 2 is better than to Dataset
58

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

1, as we filter larger number of "anomalous" data points. Details of validity and the values of coefficients are given in Table 5.4. We also defined a more complex (yet statistically significant relation between a ^ and X, N , namely a ^ = 1 + 2 ln(X ) + 3 ln(N ) + 4 ln(X )2 + 5 ln(N )2 . (5.7)

The model is valid and statistically significant as shown in Table 5.4. In the case of Datasets 1 and 2 the model explains more variability than Equation 5.6 (R2  0.91 and R2  0.98, respectively). However, the improvement is not dramatic. Similarly, we explored relations between ^ b and X, N to compute b(X, N ). A regression of the form ^ b = 1 + 2 X + 3 N (5.8)

yields adequate results. As in the above case (Eq. 5.6), the model is valid and statistically significant, but it cannot explain as much variability: R2  0.54 for Datasets 1 and R2  0.78 Dataset 2. Details of validity and the values of coefficients are given in Table 5.5. A more complex relation between ^ b and X, N is given by ^ b = 1 + 2 X + 3 N + 4 N 2 + 5 ln(N ) + 6 ln(X ). (5.9)

Table 5.5 shows that the model is valid and statistically significant2 . In the case of the Datasets 1 and 2, the model explains more variability than Equation 5.8 (R2  0.60 and R2  0.83, respectively). However, it still cannot explain as much variability as the
2 In the case of Dataset 1, p-value of the N 2 term is > 0.05. We keep it for consistency ­ addition of the term does not degrade the R2 value. To make sure that inclusion of the N 2 term does not bias the final result, we also executed the final validation (discussed in Section 5.4) while using the formula with and without the N 2 term. For Dataset 1, we obtained the same result for both formulas; for Dataset 2, the result was marginally better for the formula without the N 2 term. However, the models calibrated on Dataset 1 prevailed over the models calibrated on Dataset 2. Thus, from practical perspective, it is acceptable to keep the N 2 term (assuming that one will favor better performing models).

59

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

models for the intercept term (defined in Equations 5.6 and 5.7).

60

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      

Time (seconds)

0

1000

2000

3000

4000

5000

6000

0

100

200

300 K

400

500

600

Figure 5.6: Time needed to compute (calibrate) LDA model for a given number of topics K (the input data are Android subset, January 2014).

61

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

Figure 5.7: Fitted empirical data for X = 5, 10, 25, 50

62

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.3. CALIBRATION

Figure 5.8: Linear relation between log (F ) and log (K ) breaks approximately at K > 200, example for X=5,10,25, and 50

63

Table 5.4: Linear regression models for intercept term a ^ (flexible model: Equations 5.6 and 5.7). The values in brackets represent 90% confidence interval of the coefficients.
Dataset 1 Eq. 5.6 0.613 (0.607, 0.618) 0.053 (0.043, 0.063) -0.086 (-0.091, -0.080) 0.319 (0.302, 0.337) -0.144 (-0.215, -0.073) 4,704 0.874 0.874 0.182 (df = 4701) 16,277.930 (df = 2; 4701) 4,704 0.906 0.906 0.157 (df = 4699) 11,341.730 (df = 4; 4699) 14.716 (13.875, 15.557) 1.171 (1.128, 1.214) -4.386 (-4.628, -4.143) -0.116 (-0.122, -0.110) 1.057 (1.029, 1.085) 0.649 (0.646, 0.652) 1.081 (1.067, 1.095) 2.694 (2.569, 2.818) -0.083 (-0.086, -0.081) -0.202 (-0.211, -0.193) -9.047 (-9.480, -8.615) 4,704 0.976 0.976 0.081 (df = 4699) 47,647.760 (df = 4; 4699)
 p<0.1;  p<0.05;  p<0.01

Dataset 2 Eq. 5.7 Eq. 5.6 Eq. 5.7

Coefficient at

ln(X )

ln(N )

ln(X )2

ln(N )2

Constant

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

64

Observations R2 Adjusted R2 Residual Std. Error F Statistic

4,704 0.956 0.956 0.109 (df = 4701) 51,006.570 (df = 2; 4701)

Note:

5.3. CALIBRATION

Table 5.5: Linear regression models for slope term ^ b (flexible model:Equations 5.8 and 5.9). The values in brackets represent 90% confidence interval of the coefficients.
Dataset 1 Eq. 5.8 0.002 (0.002, 0.002) -0.00001 (-0.00002, -0.00001) 5.21e-09 (-2.44e-09, 1.29e-08) 0.014 (0.011, 0.016) 0.116 (0.095, 0.137) -0.744 (-0.746, -0.741) 4,704 0.538 0.537 0.032 (df = 4701) 2,732.760 (df = 2; 4701) 4,704 0.599 0.598 0.030 (df = 4698) 1,401.956 (df = 5; 4698) -1.450 (-1.567, -1.334) -0.841 (-0.842, -0.840) 4,704 0.782 0.782 0.018 (df = 4701) 8,443.119 (df = 2; 4701) -0.0001 (-0.0002, -0.0001) 0.00003 (0.00003, 0.00003) 0.002 (0.002, 0.002) 0.002 (0.002, 0.002) 0.002 (0.002, 0.002) 0.0003 (0.0003, 0.0004) -4.83e-08 (-5.25e-08, -4.41e-08) 0.007 (0.006, 0.009) -0.202 (-0.213, -0.190) 0.276 (0.212, 0.340) 4,704 0.825 0.824 0.016 (df = 4698) 4,419.474 (df = 5; 4698)
 p<0.1;  p<0.05;  p<0.01

Dataset 2 Eq. 5.9 Eq. 5.8 Eq. 5.9

Coefficient at

X

N

N2

ln(X )

ln(N )

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

Constant

65

Observations R2 Adjusted R2 Residual Std. Error F Statistic

Note:

5.3. CALIBRATION

CHAPTER 5. IMPLEMENTATION AND CALIBRATION Constrained Model

5.3. CALIBRATION

In the constrained case, Equation 5.2 becomes F = a(X, N )K b(X,N ) = X -b(X,N ) K b(X,N ) . Solving it for K yields: K= F X b(X,N ) (5.10)

1/b(X,N )

.

(5.11)

To find a relation between ^ b and X, N we explore two regression models. A simple regression model is given by: b(X, N ) = 1 + 2 X + 3 N, and a more complex one by: b(X, N ) = 1 + 2 X + 3 N + 4 ln(X ). (5.13) (5.12)

Both models are valid and statistically significant. Model's details are given in Table 5.6. Simple model (Eq. 5.12) explains 79% of variability (R2  0.79), while a more complex model, given by Eq. 5.13, explains 83% of variability (R2  0.83).

66

Table 5.6: Linear regression models for ^ b (constrained model: Equations 5.12 and 5.13). The values in brackets represent 90% confidence interval of the coefficients.
Dataset 1 Eq. 5.12 0.002 (0.002, 0.002) 0.00002 (0.00001, 0.00002) 0.031 (0.030, 0.032) -0.767 (-0.768, -0.766) -0.819 (-0.821, -0.817) -0.777 (-0.778, -0.775) 0.00002 (0.00001, 0.00002) 0.00003 (0.00003, 0.00003) 0.001 (0.001, 0.001) 0.003 (0.003, 0.003) 0.001 (0.001, 0.001) 0.00003 (0.00003, 0.00003) 0.035 (0.033, 0.037) -0.836 (-0.839, -0.833) Eq. 5.13 Eq. 5.12 Eq. 5.13 Dataset 2

Coefficient at

X

N

ln(X )

Constant

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

67

Observations R2 Adjusted R2 Residual Std. Error F Statistic

4,704 0.817 0.817 0.016 (df = 4701) 10,510.100 (df = 2; 4701)

4,704 0.873 0.873 0.013 (df = 4700) 10,802.240 (df = 3; 4700)

4,704 0.787 0.787 0.021 (df = 4701) 8,695.582 (df = 2; 4701)

4,704 0.834 0.834 0.019 (df = 4700) 7,864.944 (df = 3; 4700)
 p<0.1;  p<0.05;  p<0.01

Note:

5.3. CALIBRATION

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

5.4

Analysis of results

In order to estimate goodness of fit of our approximation of a and b, we compute the Root-Mean-Square Error (RMSE) (defined by Equation 5.14) between the actual value of F and the ones given to us by Equation 5.4 for each dataset and for each value of X (where X = 2, 3, . . . , 50). Summary statistics for various models is given in Tables 5.7 and 5.8 and Figures 5.9 and 5.10. (y - y ^)2 , n

RM SE =

(5.14)

where y is an actual value of F , y ^ is the predicted value of F , and n is the number of values to predict. By looking at the Tables 5.7 and 5.8 we can appreciate that the performance of the model varies based on various factors, discussed in the following paragraphs.
Table 5.7: Summary statistics for the RMSE: Dataset 1 Statistic Flex-simple Flex-complex Constrained-simple Constrained-complex Individual fit-flex Individual fit-constrained n 4,704 4,704 4,704 4,704 4,704 4,704 Mean 0.028 0.019 0.018 0.018 0.018 0.017 St. Dev. 0.013 0.005 0.006 0.006 0.005 0.006 Min 0.006 0.006 0.006 0.004 0.005 0.004 Max 0.072 0.042 0.040 0.039 0.032 0.038

Table 5.8: Summary statistics for the RMSE: Dataset 2 Statistic Flex-simple Flex-complex Constrained-simple Constrained-complex Individual fit-flex Individual fit-constrained n 4,704 4,704 4,704 4,704 4,704 4,704 Mean 0.034 0.022 0.023 0.023 0.018 0.021 St. Dev. 0.020 0.007 0.006 0.006 0.005 0.006 Min 0.010 0.009 0.009 0.007 0.006 0.005 Max 0.226 0.071 0.046 0.044 0.038 0.041

68

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

0.050

                                                

 

          

RMSE

0.020

0.010

0.005

  

 

    

Constr. - simple

Flex - complex

Figure 5.9: Summary statistics for the RMSE: Dataset 1. `Flex - simple' uses Equations 5.6 and 5.8; `Flex - complex' uses Equations 5.7 and 5.9. `Constr - simple' uses Equation 5.12; `Constr - complex' uses Equation 5.13. `Ind. fit - flex' shows RMSE for flexible linear model (Eq. 5.4 fitted individually to every data subset), while `Ind. fit - constr' depicts RMSE for constraint linear model (Eq. 5.10 fitted individually to every data subset).

5.4.1

Selection of the best performing model

In order to select the best performance model, we focus on the RMSE stats (provided in Tables 5.7 and 5.8) and select the model that minimizes the mean RMSE. For Dataset 1 (DS1), among four models, constrained-complex model yields the lowest mean, min, and max RMSE values, comparable with the performance of individually69

Constr. - complex

Ind. fit - constr

Flex - simple

Ind. fit - flex

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

0.200

0.100

                                  

0.050 RMSE

                      

     

     

        

 

0.020

0.010
  

0.005 Constr. - simple Flex - complex Constr. - complex Ind. fit - constr Flex - simple Ind. fit - flex

Figure 5.10: Summary statistics for the RMSE: Dataset 2. Flex - simple' uses Equations 5.6 and 5.8; `Flex - complex' uses Equations 5.7 and 5.9. `Constr - simple' uses Equation 5.12; `Constr - complex' uses Equation 5.13. `Ind. fit - flex' shows RMSE for flexible linear model (Eq. 5.4 fitted individually to every data subset), while `Ind. fit - constr' depicts RMSE for constraint linear model (Eq. 5.10 fitted individually to every data subset).

fitted lines. Thus, constrained-complex model is the winner for DS1. In the case of Dataset 2 (DS2), RMSE of the flex-complex model is slightly lower than that of the constrained-complex model (0.022 vs. 0.032). However, the min and max values of the flex-complex are worse than that of the constrained-complex model (0.009 vs. 0.007 and 0.071 vs. 0.044, respectively). Thus, the constrained-complex can

70

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

be considered a winner for DS2 too. 5.4.2 Analysis of the best performing model

To verify robustness of the results of the the best performing (constrained-complex) model and make sure that we are not overfitting, we performed 10-fold cross validation3 . We split 96 data subsets into ten partitions. We then fit Equation 5.13 to nine out of ten partitions. We then fit the constrained model (Equation 5.10) to the raw data of the tenth partition and compute the RMSE value. The process is repeated nine more times, alternating partitions of the test and train sets. The resulting ten values of the RMSE are shown in Figure 5.11. As we can see, for both Dataset 1 (mean  0.018) and Dataset 2 (mean  0.024) the RMSE values are comparable to those reported in Tables 5.7 and 5.8. Thus, we are not overfitting. In Figures 5.12 and 5.13 we can observe that for small values of X the RMSE is smaller and it increases as X gets larger. The X grows fast until approximately X = 20 and then it starts to slow down in growth. In the other hand, in Figures 5.14 and 5.15 we can see that the relation between N and the RMSE varies in a similar manner to X for small values of N and large values of N . Therefore, we can say that the quality of RMSE is not influenced significantly by N , but it is influenced by X in the sense that the smaller the value of X the better the predictions are. In the case of the datasets analyzed, for the Dataset 1 the fitting from higher to lower is android, salesforce, dba monthly, and dba quarterly (see Figure 5.16) and for the Dataset 2 is android, dba quarterly, salesforce monthly and dba monthly (see Figure 5.17). Finally, Figure 5.18 shows one plot that demonstrates the fitting achieved with the
3 The

listing for performing the validation is given in Appendix B.10

71

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

RMSE

0.018

0.020

0.022

0.024

0.026

Dataset 1 Constr. - complex

Dataset 2

Figure 5.11: 10-fold cross validation of the fit of the `Constrained - complex' model (Equation 5.13).

constrained-complex model for Dataset 1. By eyeballing this plot we can appreciate that starting at the left corner the fit is very good, then it goes above the data and when it gets the right side, it starts to deteriorate. The same behaviour applies to all X 's shown (X = 5, 10, 25, and 50, these values of X were selected at random). For a larger selection of plots that show this fitting please see Appendix A. In the case of Dataset 1, the model yields better results (i.e. lower RMSE) for data splits per-month in comparison with the data splits per-quarter, as can be seen in Fig-

72

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex
                                                                                                                                                                           

RMSE 0.005 0.010

0.020

2

5

8

11 14 17 20 23 26 29 32 35 38 41 44 47 50 X

Figure 5.12: Performance of the constrained complex model for different values of X : Dataset 1.

ure 5.16. We conjecture that the larger number of documents in the quarterly subsets plays a role here: Figure 5.14 shows that RMSE increases slightly for large values of N . However, in the case of Dataset 2 the RMSE for per-quarter data splits is on par or better than in the case of per-month splits (as can be seen in Figure 5.17). We also
73

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex
                                                                      

0.03

0.04

0.02

 

RMSE





0.01

2

5

8

11 14 17 20 23 26 29 32 35 38 41 44 47 50 X

Figure 5.13: Performance of the constrained complex model model for different values of X : Dataset 2.

see that for small and large values of N the performance is comparable as shown in Figure 5.15. We conjecture that this can be explained by the fact that we truncate our K values at 200 and get rid of the error factor contributed by the larger values of K .

74

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex

0.020

 

     

                                                                                               

RMSE

  

                                         

     

          

       



       

0.010



0.005

  

467

560

592

617

710

799

886 N

968

1071 1295 1802 2936

Figure 5.14: Performance of the constrained complex model for different values of N : Dataset 1. The case when K > 0.75N

Based on our experiments, Equation 5.2 holds for K  0.75N . As K  N , the Power Law approximation deteriorates (see Figure 5.7 for an example). Likely, this is because

75

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex

0.04

0.03

                                                                                                                         

0.02

                    

          

 

                                                       

                        

RMSE



       

                                      

0.01

467

560

592

617

710

799

886 N

968

1071 1295 1802 2936

Figure 5.15: Performance of the constrained complex model model for different values of N : Dataset 2.

the average number of documents per topic becomes too small for LDA to process. It seems, empirically, that the average number of posts per topic should be 4 or greater, hence the K  0.75N constraint.

76

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex

   

           

0.020 RMSE 0.010
                            

0.005

android - monthly

dba - monthly

dba - quarterly

Figure 5.16: Performance of the constrained complex mode per dataset: Dataset 1. Summary

In conclusion, we can appreciate graphically (in Figure 5.19) the "path" that leads to the best-performing model F = X -b(X,N ) K b(X,N ) with the more complex prediction variable

77

salesforce - monthly

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Model - constrained, complex
                           

0.04 0.03

RMSE

0.02
                                  

0.01
  

android - monthly

dba - monthly

dba - quarterly

Figure 5.17: Performance of the constrained complex model per dataset: Dataset 2.

b(X, N ) = 1 + 2 X + 3 N + 4 ln(X ). The model performs well with the K  0.75N (Dataset 1) filter.

78

salesforce - monthly

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.4. ANALYSIS OF RESULTS

Dataset: android monthly 2014-1-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 K Filter name: DS1 100 200 500

Figure 5.18: Fitting for a selection of top X using the constrained-complex model for Dataset 1

79

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.5. THREATS TO VALIDITY

Figure 5.19: Decision tree for selecting the best fitting model. The path to the best-performing model is given by the dashed line.

5.5

Threats to Validity

In the following paragraphs we outline some of the limitations of our study and their impacts as per [26, 70]. Threats to conclusion validity are about the degree to which conclusions attained about relationships in our data are reasonable. To verify the absence of overfitting, we

80

CHAPTER 5. IMPLEMENTATION AND CALIBRATION

5.5. THREATS TO VALIDITY

performed 10-fold cross validation analysis of our best model. Threats to construct validity involves the relationship among the concepts and theories that backs the experiment and what is measured and affected. In this regard, the experiments that we have carried out were designed based on formal knowledge. We have also used the data from three different sources, partitioning the data either monthly or quarterly. Threats to internal validity [29] comprise potential errors in our execution of the study process, these errors may influence the accuracy of our results and the conclusions we deduce from them. In order to avoid introducing bias we used an automated procedure for data extraction and processing, that included the creation of scripts in Perl and R languages. Threats to external validity [73] include the degree to which we can generalize our results. The subjects of our study contain three datasets for several periods comprising thousands of data points. The results cannot be generalized to other datasets, rather the design of this study is based on the concept of the critical case [91]. If the model does not work "out-of-the-box", one can re-calibrate it by following our methodology.

81

Chapter 6

Conclusion and Future Work
LDA is a widespread IR probabilistic topic model technique for performing analysis of text corpora. We have reviewed its roots, its basic functioning, its applications in the SE arena, exemplified its usage, and derived a formula from SE-related text corpuses to calibrate the model. The problem that we have addressed in this thesis, was to find in an expedited manner the number of topics (K ), which the LDA model requires to be implemented. To answer the research question "How can we quickly select the number of topics K so that the top X topics include a certain fraction F of the N documents under study?", we created a simple, closed-form Power Law expression (5.11), estimating K with X , F , and N as input. Although Power Law occurs frequently in SE [55], to the best of our knowledge, this is the first appearance of the power law in LDA parameter calibration. Moreover, we established that LDA models and (5.11) become unstable if K > 0.75N . Practitioners can accelerate LDA analysis by using (5.11) with (5.13) to suggest the number of topics required to answer a particular question (e.g., to identify customers' pain-points and to prioritize maintainers tasks). They can forego the often computationally-prohibitive current practice of iterating over all values of K to identify the optimal value. Formula (5.11) is also of interest to theoreticians as it suggests that dif82

CHAPTER 6. CONCLUSION AND FUTURE WORK

ferent SE-related text corpuses (in our case a technical Q&A forum and an issue-tracking system) might have similar underlying properties. The formula is validated on three datasets and, as discussed in Section 5.5, cannot be generalized to other datasets. However the results do hint the existence of underlying structural similarities in the datasets. In the future, we plan to analyze additional SE-related text corpuses and extend our work to non-SE-related text corpuses . Also, we would like to find ways for improving the formula for K .

83

Appendices

84

Appendix A

Sample Plots
We are including in this section a selection of sample plots that show the fitting for the datasets considered (Table A.1).

Table A.1: Sample plots included in this appendix Dataset name Period Year Month or Quarter number android android dba dba dba dba salesforce salesforce monthly monthly quarterly quarterly monthly monthly monthly monthly 2015 2015 2012 2012 2014 2014 2015 2015 2,4,6,8,10,12 2,12 1,2,3,4 2,4 2,4,6,8,10,12 2,12 2,4,6,8,10 2,12

Filter DS1 DS2 DS1 DS2 DS1 DS2 DS1 DS2

85

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

A.1

android dataset
Dataset: android monthly 2015-2-
1.00

F

0.05

0.10

0.20

0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 K Filter name: DS1 100 200 500

86

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-4-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

87

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-6-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

88

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-8-
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

89

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-10
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

90

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-12
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

91

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-2-
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

92

APPENDIX A. SAMPLE PLOTS

A.1. ANDROID DATASET

Dataset: android monthly 2015-12
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

93

APPENDIX A. SAMPLE PLOTS

A.2. DBA QUARTERLY DATASET

A.2

dba quarterly dataset
Dataset: dba quarterly 2012-q1
1.00

F 0.05 0.10

0.20

0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

0.02

K Filter name: DS1

94

APPENDIX A. SAMPLE PLOTS

A.2. DBA QUARTERLY DATASET

Dataset: dba quarterly 2012-q2
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

0.02

K Filter name: DS1

95

APPENDIX A. SAMPLE PLOTS

A.2. DBA QUARTERLY DATASET

Dataset: dba quarterly 2012-q3
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

0.02

K Filter name: DS1

96

APPENDIX A. SAMPLE PLOTS

A.2. DBA QUARTERLY DATASET

Dataset: dba quarterly 2012-q4
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

0.02

K Filter name: DS1

97

APPENDIX A. SAMPLE PLOTS

A.2. DBA QUARTERLY DATASET

Dataset: dba quarterly 2012-q1
1.0 F 0.1 0.2 0.5

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

98

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

A.3

dba monthly dataset
Dataset: dba monthly 2014-2-
1.00

F 0.05 0.10

0.20

0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

99

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-4-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

100

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-6-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

101

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-8-
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

102

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-10
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

103

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-12
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500

K Filter name: DS1

104

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-2-
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

105

APPENDIX A. SAMPLE PLOTS

A.3. DBA MONTHLY DATASET

Dataset: dba monthly 2014-12
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

106

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

A.4

salesforce dataset
Dataset: salesforce monthly 2015-2-
1.00

F 0.05 0.10

0.20

0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

K Filter name: DS1

107

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-4-
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

K Filter name: DS1

108

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-6-
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

K Filter name: DS1

109

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-8-
1.00 F 0.05 0.10 0.20 0.50

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

0.02

K Filter name: DS1

110

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-10
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

K Filter name: DS1

111

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-12
1.00 F 0.05 0.10 0.20 0.50

0.02

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200 500 1000

K Filter name: DS1

112

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-2-
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

113

APPENDIX A. SAMPLE PLOTS

A.4. SALESFORCE DATASET

Dataset: salesforce monthly 2015-12
1.00 F 0.10 0.20 0.50

0.05

X=5 X = 10 X = 25 X = 50 5 10 20 50 100 200

K Filter name: DS2

114

Appendix B

Code listings
The following Table (B.1) is the list of scripts developed for the implementation and calibration of LDA.
Table B.1: List of programs included in this appendix Language Objective Perl R " " " " " " " " Convert format of corpus from XML to CSV LDA inference for dba corpus with idf calculation Utility functions for dba LDA inference LDA inference for monthly datasets LDA inference for quarterly dataset Utility functions for dba LDA inference (monthly and quarterly) Binds topic frequency files to be analyzed with the regression model Iterates over datasets, and computes the RMSE of the approx. formulas Plots the RMSE boxplots per model and per dataset name Performs validation using 10 folds

Name xml2csv.pl do analysis db2 idf.R utilsdb2.R do lda analysis topics.R do lda quarter inc.R utils.R convVar k200.R verifyFit k200.R

verifyFit k200 analysis.R validate fit.R

B.1
1 2 3 4

Program: xml2csv.pl

# ------------------------------------------------------------------------------ #This s c r i p t r e a d s i n XML d a t a from S t a c k O v e r f l o w and r e t u r n s i t i n t e x t f o r m a t #Output " t i t l e \ t body " , one l i n e p e r p o s t #

115

APPENDIX B. CODE LISTINGS
5 6 7 8 9 10 11 12 13 14 15 16 my $minVoteCount = 3 ; 17 18 19 20 21 22 23 24 25 27 28 29 30 31 my $hs = HTML: : S t r i p ->new ( ) ; 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 my $ t i t l e my $body = t r a n s f o r m ( $ ->{ T i t l e } ) ; $ t a g s =~ s /></^&^/g ; $ t a g s =~ s /^ < //g ; $ t a g s =~ s /> $ // g ; #remove "<" and r e p l a c e ">" w i t h " " open ( my $ f i d , ">" . $ARGV [ 1 ] ) o r die ( " Cannot open $ARGV [ 1 ] : $ ! \ n" ) ; #r e a d XML f i l e i n t o memory 26 my $ r e f = XMLin( $ARGV [ 0 ] ) ; } i f ( s c a l a r (@ARGV) != 2 ) { sub t r a n s f o r m ; use u t f 8 ; use Text : : Unidecode ; #t r a n s l a t e s u n i c o d e i n t o a s c i i use XML: : Simple ; use HTML: : S t r i p ; # u s e XML: : Twig f o r b i g f i l e s

B.1. PROGRAM: XML2CSV.PL

#Usage example : p e r l x m l 2 c s v . p l P o s t s . xml P o s t s . xml . c s v # ------------------------------------------------------------------------------ use w a r n i n g s ; use s t r i c t ;

#minimum number o f v o t e s r e q u i r e d t o s a v e an answer

print " u s a g e : xml2csv . p l i n f i l e n a m e o u t f i l e n a m e \ n" ; exit ;

#e x p o r t t o CSV

print $ f i d " c r e a t e t s \ t i d \ t t a g s \ t t i t l e \ tbody \ t a n s w e r s \ n" ; foreach ( @{ $ r e f ->{row } } ) { i f ( $ ->{PostTypeId } == " 1 " ) { my $ i d my $ t a g s = $ ->{ Id } ; = $ ->{Tags } ;

#h e a d e r

#i f t h e p o s t i s t h e o r i g i n a l q u e s t i o n

my $ c r e a t i o n D a t e = ( $ ->{ C r e a t i o n D a t e } ) ;

my $bestAnswerId = $ ->{ AcceptedAnswerId } ;

= t r a n s f o r m ( $hs -> p a r s e ( $ ->{Body } ) ) ; #s t r i p HTML t a g s

116

APPENDIX B. CODE LISTINGS
48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 } } return $ a n s w e r s ; } { ) ) ) or ( foreach ( @{ $ r e f ->{row } } ) { if ( ( ( $ ->{PostTypeId } eq " 2 " ) sub getAnswersText { my ( $ q u e s t i o n I d , $minVote , $AcceptedAnswerId ) = @ ; my $ a n s w e r s = " " ; } #c o n v e r t u n i c o d e t o a s c i i and make i t l o w e r c a s e sub t r a n s f o r m { my ( $ t x t ) = @ ; $ t x t =~ s / \ n | \ t | \ r / / g ; $ t x t =~ s / [ [ : punct : ] ] / / g ; words t o l o w e r c a s e } close $ f i d ; $hs ->eof ; }

B.1. PROGRAM: XML2CSV.PL

my $ a n s w e r s = getAnswersText ( $id , $minVoteCount , $bestAn swerId ) ; print $ f i d j o i n ( " \ t " , ( $ c r e a t i o n D a t e , $id , $ t a g s , $ t i t l e , $body , $ a n s w e r s ) ) . " \ n" ;

#r e p l a c e new l i n e or t a b w i t h s p a c e #r e p l a c e p u n c t u a t i o n w i t h s p a c e

return l c ( u n i d e c o d e ( $ t x t ) ) ; #c o n v e r t #c o n v e r t u n i c o d e t o l a t i n and change

#r e c o r d t y p e = answer #answer t o q u e s t i o n $ q u e s t i o n I d #minimal v o t e #a l w a y s g e t a c c e p t e d answer

and ( $ ->{ P a r e n t I d } eq $ q u e s t i o n I d ) and ( ( $ ->{ S c o r e } + 0 ) >= $minVote )

defined ( $AcceptedAnswerId ) and ( $ ->{ Id } eq $AcceptedAnswerId )

$ a n s w e r s .= t r a n s f o r m ( $hs -> p a r s e ( $ ->{Body } ) ) . " " ;

117

APPENDIX B. CODE LISTINGS

B.2. PROGRAM: DO ANALYSIS DB2 IDF.R

B.2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39

Program: do analysis db2 idf.R

# #---------------------------------------------------------------------------- # # The s c r i p t r e a d s p o s t s i n d e l i m i t e d format , and g e n e r a t e s a l i s t o f keyword # # Usage : R s c r i p t my S c r i p t name .R o u t d i r e c t o r y / i n f i l e name yyyy # # This program i n c l u d e s t h e p r o b a b i l i t y p e r keyword # # a l s o i n c l u d e s t h e s p e c i f i c i t y by keyword by t o p i c # #---------------------------------------------------------------------------- l i b r a r y ( tm ) library ( f o r e a c h ) library ( d o P a r a l l e l ) library ( s t r i n g r ) # # added t o h a n d l e s t r i n g f u n c t i o n s memory . l i m i t ( 2 4 3 5 6 ) memory . s i z e (max = TRUE) source ( " u t i l s d b 2 .R" ) xURL <- " h t t p : // dba . s t a c k e x c h a n g e . com/ q u e s t i o n s / " -% M -%S %Y" ) xSystime <- format ( Sys . time ( ) , "%a-%b-%d %H # # f o l l o w i n g l i n e s commented o u t t o run i n PC ## # #a r g s <- commandArgs ( t r a i l i n g O n l y = TRUE) # #o u t D i r <- a r g s [ 1 ] # #readFrom <- p a s t e ( a r g s [ 1 ] , a r g s [ 2 ] , s e p ="") # #y e a r <- as . i n t e g e r ( a r g s [ 3 ] )

# # f o l l o w i n g l i n e s h a r d c o d e d t o run i n PC ## readFrom <- " nondb2 . awk" ##"P o s t s . xml . f i l t e r e d . c s v . new" ##"db2 . awk" y e a r <- 2015 # # # #month <- as . i n t e g e r ( a r g s [ 3 ] )

topKeywordCount <- 20 # # SET TO 20 FOR THIS RUN # # c o r p <- c r e a t e C o r p ( readFrom ) c o r p <- c r e a t e C o r p ( readFrom , y e a r ) # # added y e a r # To a c c e s s o r i g i n a l i d o f a document i n VCorp run c o r p [ [ i n d e x o f document 1 t o N ] ] $ meta $ i d # To a c c e s s o r i g i n a l i d o f a document i n DocumentTermMatrix run dtm $ dimnames $ Docs [ i n d e x o f document 1 t o N ] cat ( "DEBUG a f t e r c a l l i n g c r e a t e C o r p ! \ n" )

118

APPENDIX B. CODE LISTINGS
40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 T t o p i c . keyword [ i , j ] <- } ) %do% { #change t o %dopar% f o r m u l t i e x e c u t i o n cat ( " RUNNING SINGLE PROCESSING \ n" ) #s e t u p p a r a l l e l b a c k e n d t o u s e 8 p r o c e s s o r s # # B u i l d a Document-Term Matrix

B.2. PROGRAM: DO ANALYSIS DB2 IDF.R

dtm <- DocumentTermMatrix ( corp , control = l i s t ( minWordLength = 2 ) ) #k e e p words o f l e n g h t 2 or l o n g e r cat ( " B e f o r e t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" )# dtm <- removeFrequentWords ( dtm ) #removing b a s e d on median t f - i d f v a l u e cat ( " A f t e r t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" n l i k e l y event ) cat ( " A f t e r removing terms a p p e a r i n g o n l y i n 1 document : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" )

c l<-makeCluster ( 8 ) # # change t o 8 f o r m u l t i p r o c e s s i n g registerDoParallel ( cl ) f o r e a c h ( t o p i c C o u n t = c ( 5 ) #max = 1 t o p i c p e r document ## changed t o p i c c o u n t=5 , . packages= ' t o p i c m o d e l s ' #i n c l u d e p a c k a g e

cat ( " TopicCount : " , topicCount , " \ n" ) #t o s c r e e n ( no s c r e e n o u t p u t i s p a r a l l e l mode ) mdl <- LDA( dtm , t o p i c C o u n t ) #LDA model t o p i c . keyword <- terms ( mdl , topKeywordCount ) mdl . p o s t <- p o s t e r i o r ( mdl ) #g e t p o s t e r i o r d a t a

# # t h e f o l l o w i n g n e s t e d l o o p was added t o append t h e p- v a l u e p e r t o p i c . keyword T t o p i c . keyword <- t o p i c . keyword # # make a copy # # t h e f o l l o w i n g n e s t e d l o o p was added t o w r i t e t h e p- v a l u e s o f t o p i c s by column ou tPval <- paste ( " p v a l u e s " , xSystime , " " , readFrom , " " , year , "-" , topicCount , " . txt " , sep = "" ) for ( j in 1 : topicCount ) { cat ( "TOPIC " , j , " \ n" , append = T, f i l e = outPval ) cat ( "KEYWORD, PROBABILITY\ n" , append = T, f i l e = outPval ) f o r ( i i n 1 : topKeywordCount ) {

T t o p i c . keyword [ i , j ] <- paste ( t o p i c . keyword [ i , j ] , " , " , mdl . p o s t $terms [ [ j , t o p i c . keyword [ i , j ] ] ] )

119

APPENDIX B. CODE LISTINGS
76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 # # code added t o c a l c u l a t e f r e q u e n c i e s # # t i s number o f terms i n dtm # # tC i s t h e i n d e x o f t o p i c C o u n t # # kC i n t h e i n d e x o f topKeywordCount } # # W i l l r e t r i e v e a l l i d s and q u e s t i o n s t <- t o p i c s ( mdl ) l e n <- length ( t o p i c s ( mdl ) ) # # code added t o s u p p o r t r e p o r t c r e a t i o n write . csv ( T t o p i c . keyword , } }

B.2. PROGRAM: DO ANALYSIS DB2 IDF.R

cat ( T t o p i c . keyword [ [ i , j ] ] , " \ n" , append = T, f i l e = ou tPval )

f i l e = paste ( " p v a l u e s " , xSystime , " " , readFrom , " " , year , "-" , topicCount , " . c s v " , s e p = " " ) )

saveTo <- paste ( " Report -" , xSystime , " " , readFrom , "-" , year , " . t x t " ) cat ( " q u e s t i o n i d \ t q u e s t i o n d e s c r i p t i o n \ t t o p i c \ t \ n" , s e p=" \ t " , append = T, f i l e = saveTo ) # t o f i l e for ( i in 1 : l e n ) { r i <- c o r p [ [ i ] ] $ meta $ i d rq <- c o r p [ [ i ] ] $ meta $ q u e s t rt <- t [ i ] r <- paste (xURL, r i , " \ t " , rq , " \ t " , rt , s e p=" " ) cat ( r , " \ n" , append = T, f i l e = saveTo ) # t o f i l e

# # The f o l l o w i n g code was added f o r g e n e r a t i n g t h e s p e c i f i c i t y p e r keyword

# # topKeywordCount i s t h e number o f k e y w o r d s # # dC i s t h e i n d e x o f number o f t o p i c s # # countKw i s t h e c o u n t e r o f t h e keyword t h a t a p p e a r s i n t h e t o p i c s a l l D oc u m e n t s <- nrow ( dtm ) # # S i z e i n rows o f dtm a l l K e y w o r d s <- ncol ( dtm ) countKw <- matrix ( 0 , topKeywordCount , t o p i c C o u n t ) # # i n i t i a l i z e c o u n t keyword matrix with 0

# intermediate array to countKw <- array ( 0 , dim=c ( topKeywordCount , t o p i c C o u n t ) ) # h a n d l e t h e c o u n t i n g o f kW i n a l l D o c u m e n t s t <- t o p i c s ( mdl ) # # documents and t h e t o p i c s i t b e l o n g s t o

120

APPENDIX B. CODE LISTINGS
115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 } } } } i f ( ! i s . null (acumKw) ) { countKw [ i , j ] <- acumKw acumKw <-0 } } } } i f ( tK==j ) { i f ( dfz [ [ colZ ] ] [ k ] > 0) { acumKw <- acumKw + 1 for ( k in 1 : allDocuments ) { i f ( dfz [ [ colZ ] ] [ k ] > 0) { tK <- t [ [ k ] ] z <- as . matrix ( dtm ) # # dtm as m a t r i x

B.2. PROGRAM: DO ANALYSIS DB2 IDF.R

d f z <- as . data . frame ( z ) # # c o n v e r t s z t o d a t a frame tK <- 0 # # a r r a y ( 0 , dim=c ( topKeywordCount , t o p i c C o u n t ) ) acumTo <- rep ( 0 , t o p i c C o u n t ) # # i n i t i a l i z e acum o f t o p i c s f o r ( i i n 1 : topKeywordCount ) { acumKw <-0 for ( j in 1 : topicCount ) { xlookup <- t o p i c . keyword [ i , j ] c o l Z <- which ( colnames ( z )==t o p i c . keyword [ i , j ] ) # # f i n d s t h e column i n z t h a t t h e kW i s i n

# # i n d e k t k i s t h e t o p i c number

# # C a l c u l a t i o n o f N as t h e number o f documents p e r t o p i c f o r ( i i n 1 : nrow ( dtm ) ) { t o p i c<-t [ [ i ] ] acumTo [ t o p i c ]<-acumTo [ t o p i c ]+1

# # t h e f o l l o w i n g n e s t e d l o o p was added t o w r i t e t h e t o p i c s by column o u t I d f <- paste ( " i d f " , xSystime , " " , readFrom , " " , year , "-" , topicCount , " . txt " , sep = "" ) for ( j in 1 : topicCount ) { cat ( "TOPIC " , j , " \ n" , append = T,

121

APPENDIX B. CODE LISTINGS
156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 cat ( "Done \ n" ) } } i d f<-log10 ( acumTo [ j ] / countKw [ i , j ] ) f i l e = outIdf )

B.2. PROGRAM: DO ANALYSIS DB2 IDF.R

cat ( "KEYWORD, PROBABILITY, N, d , i d f \ n" , append = T, f i l e = outIdf ) f o r ( i i n 1 : topKeywordCount ) {

cat ( " i j " , i , " " , j , " i d f= l o g ( " , acumTo [ j ] , " / " , countKw [ i , j ] , " ) \ n" ) T t o p i c . keyword [ i , j ] <- paste ( t o p i c . keyword [ i , j ] , " , " , mdl . p o s t $terms [ [ j , t o p i c . keyword [ i , j ] ] ] , " , " , acumTo [ j ] , " , " , countKw [ i , j ] , " , " , i d f ) # # g o i n g t o work w i t h a copy o f t o p i c . keywords i n s t e a d cat ( T t o p i c . keyword [ [ i , j ] ] , " \ n" , append = T, f i l e = outIdf )

122

APPENDIX B. CODE LISTINGS

B.3. PROGRAM: UTILSDB2.R

B.3
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 } }

Program: utilsdb2.R

library ( topicmodels ) # #-------------------------------------------------------------------------- # # This f u n c t i o n r e a d s t h e d a t a from c s v f i l e , c l e a n s i t and r e t u r n s tm t e x t Corpus # # readFromfileName - name o f t h e f i l e t o r e a d from # # # # year f i l t e r year - o p t i o n a l : y e a r when d o c s were c r e a t e d , f o r m a t = YYYY

# #-------------------------------------------------------------------------- c r e a t e C o r p <- function ( readFromfileName , y e a r ) { cat ( "@ c r e a t e C o r p \ n" ) l i b r a r y ( tm ) # # Load t h e d a t a P o s t s <- read . d e l i m ( f i l e = readFromfileName , h e a d e r = T, quote = " " , s e p = " \ t " ) cat ( "Read" , nrow ( P o s t s ) , " rows from " , readFromfileName , " \ n" ) cat ( " y e a r t o p r o c e s s =" , year , " \ n" ) # # t h e f o l l o w i n g l i n e s t o f i l t e r by y e a r i f ( ! missing ( y e a r ) ) { P o s t s $create t s <- as . POSIXlt ( P o s t s $create t s ) P o s t s <- subset ( Posts , P o s t s $create t s $ y e a r == ( y e a r - 1 9 0 0 ) ) cat ( "Kept" , nrow ( P o s t s ) , " rows \ n" )

return ( doCorpCreation ( P o s t s ) )

# # t h i s i s a p r i v a t e function f or corpus creation doCorpCreation <- function ( P o s t s ) { P o s t s <- data . frame ( doc i d = P o s t s $ id , q u e s t i o n = P o s t s $ t i t l e , t x t = paste ( P o s t s $ t i t l e , P o s t s $body , P o s t s $ answers , s e p =" " ) ) # # added q u e s t i o n # # Build a corpus p o s t s . r e a d e r <- r e a d T a b u l a r ( mapping= l i s t ( c o n t e n t=" t x t " , i d=" doc i d " , q u e s t=" question ") ) # # added q u e s t i o n c o r p <- VCorpus ( DataframeSource ( P o s t s ) , r e a d e r C o n t r o l= l i s t ( r e a d e r=p o s t s . r e a d e r ) ) # # Transform d a t a

123

APPENDIX B. CODE LISTINGS
37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 } } } # # mc . c o r e s = 1 t o f i x run t i m e e r r o r

B.3. PROGRAM: UTILSDB2.R

c o r p = tm map( corp , c o n t e n t t r a n s f o r m e r ( t o l o w e r ) , mc . c o r e s =1) #c o n v e r t i n g t o lower case c o r p = tm map( corp , removeWords , s t o p w o r d s ( ' e n g l i s h ' ) , mc . c o r e s =1) # Remove Stopwords c o r p = tm map( corp , stemDocument , mc . c o r e s =1) # Stemming c o r p = tm map( corp , s t r i p W h i t e s p a c e , mc . c o r e s =1) # E l i m i n a t e w h i t e s p a c e c h a r cat ( " Created c o r p u s from " , length ( c o r p ) , " documents \ n" ) return ( c o r p )

# # Removes t o p i c s from t h e Document Term Matrix u s i n g t f - i d f approach # # dtm - Document Term Matrix t o p r o c e s s # # t h r e s h o l d - remove words w i t h t f - i d f v a l u e s s m a l l e r t h a n t h r e s h o l d # # # # # # t h i s parameter i s o p t i o n a l : i f t h r e s h o l d i s n o t d e f i n e d , we w i l l remove most f r e q u e n t words , b a s e d on t h e median v a l u e o f t f - i d f ( a t l e a s t 50%) S e t t i n g t h r e s h o l d = 0 w i l l e l i m i n a t e words a p p e a r i n g i n e v e r y document

removeFrequentWords <- function ( dtm , t h r e s h o l d ) { l i b r a r y ( " slam " ) t e r m T f i d f <- tapply ( dtm$ v /row sums ( dtm ) [ dtm$ i ] , dtm$ j , mean )  log2 ( nDocs ( dtm ) / c o l sums ( dtm > 0 ) ) # i f no t h e s h o l d i s p r o v i d e d t h e n s e t t h e t h r s h o l d v a l u e t o median o f t f - i d f # d i s t r i b u t i o n , removing a t l e a s t 50% o f t h e words i f ( missing ( t h r e s h o l d ) ) { t h r e s h o l d <- median ( t e r m T f i d f )

# remove terms which have t f - i d f s m a l l e r t h an t h e median o f a l l t h e t f - i d f values # t h i s w i l l s h r i n k t h e d i c t i o n a y by a p p r o x i m a t e l y 50% dtm <- dtm [ , t e r m T f i d f > t h r e s h o l d ] dtm <- dtm [ row sums ( dtm ) > 0 , ] u n l i k e l y event ) return ( dtm ) #remove d o c s t h a t have no terms r e m a i n i n g (

# # This f u n c t i o n r e t u r n s f r e q u e n c y o f t o p i c s f o r a g i v e n LDA model # # d a t - Document Term Matrix t o f e e d t o t h e LDA model # # t o p i c C o u n t - Number o f t o p i c s i n t h e LDA model g e t T o p i c s F r e q u e n c y <- function ( dat , t o p i c C o u n t ) {

124

APPENDIX B. CODE LISTINGS
76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 cat ( "p=" , p , " \ n" ) p <- i cat ( " i n d I=" , i n d I , " i n d J=" , indJ , " i=" , i , " \ n" ) MRanges [ i , 1 ] <- i n d I MRanges [ i , 2 ] <- i n d J maxCount <- 1 cat ( "numRanges=" , numRanges , " \ n" ) f o r ( i i n 1 : numRanges ) { i n d I <- ( 1 0 0  i ) -100+1 i n d J <- 100  i i n c C a l c <- function ( totCount ) { } )) return ( l i s t ( mdl . a l p h a = mdl . alpha , mdl . beta . mean = mdl . beta . mean , mdl . beta . sd = mdl . beta . sd , t o p i c . frequency = as . vector ( table ( t o p i c s ( mdl ) ) ) mdl . a l p h a <- mdl@alpha mdl . beta . mean <- mean ( mdl@beta ) mdl . beta . sd <- sd ( mdl@beta ) mdl <- LDA( dat , t o p i c C o u n t ) #LDA model

B.3. PROGRAM: UTILSDB2.R

# # F u n c t i o n t o c a l c u l a t e i n c r e m e n t s , 2 . . 1 0 0 , 1 1 0 1 . . 2 0 0 5 , 2 0 1 . . 3 0 0 10 . . . capped t o 25 a f t e r t o t C o u n t >= 600 t o 25 numRanges <- c e i l i n g ( totCount / 1 0 0 ) # # t h i s i s t o e n s u r e t h a t i f g o e s beyond 100 ' s i t i s t a k e n i n t o a c c o u n t vecRanges = matrix ( nrow = 1 , ncol = numRanges ) vecCount <- c ( ) MRanges <- matrix ( nrow = numRanges , ncol = 2 ) i x <- 1

cat ( " i=" , i , " from=" , MRanges [ i , 1 ] , " t o " , MRanges [ i , 2 ] )

i f (p > 1 & & p < 7 ) { p <- 5  ( i - 1) } e l s e { i f ( p !=1 ) { p <-25 }}

125

APPENDIX B. CODE LISTINGS
117 118 119 120 121 122 123 124 125 126 127 128 129 130 } } return ( vecCount ) } cat ( "maxCount=" , maxCount , " \ n" ) }

B.3. PROGRAM: UTILSDB2.R

f o r ( j i n seq ( from=MRanges [ i , 1 ] , t o=MRanges [ i , 2 ] , by=p ) ) { i f ( j < totCount ) { # # This i s t o l i m i t t h e c a l c u l a t i o n o f i n t e r v a l s up to totCount vecCount [ i x ] <- j maxCount <- i x cat ( " j =" , j , " \ n" ) cat ( " vecCount [ " , i x , " ]= " , vecCount [ i x ] , " \ n" ) i x <- i x + 1

126

APPENDIX B. CODE LISTINGS

B.4. PROGRAM: DO LDA ANALYSIS TOPICS.R

B.4
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Program: do lda analysis topics.R

# #------------------------------------------------------------------------------- # # The s c r i p t r e a d s p o s t s i n d e l i m i t e d format , and g e n e r a t e s # # d i s t r i b u t i o n o f LDA t o p i c s f o r a g i v e n month year , # # s a v i n g t h e o u t p u t i n i n f i l e name + " . year -month . t o p i c f r e q u e n c y " # # Usage : R s c r i p t do l d a a n a l y s i s .R i n f i l e name y e a r month # # # # This v e r s i o n adds a Semaphore i n o r d e r t o slowdown t h e p r o c e s s o f w r i t i n g i n a p a r a l l e l processing # # i n c l u d e s r e f a c t o r i n g c h a n g e s ( i . e . t a k i n g o u t semaphore code from f o r l o o p . . ) # #------------------------------------------------------------------------------ l i b r a r y ( tm ) library ( f o r e a c h ) library ( d o P a r a l l e l ) source ( " u t i l s .R" ) args <- commandArgs ( t r a i l i n g O n l y = TRUE) readFrom <- args [ 1 ] year <- as . integer ( args [ 2 ] ) as . integer ( args [ 3 ] ) month <-

semaphoreFileName <- paste ( " semaf " , Sys . g e t p i d ( ) , s e p= ' ' ) # append R' s p r o c e s s i d t o semaphore t o a v o i d c o n f l i c t i f ( f i l e . e x i s t s ( semaphoreFileName ) ) { f i l e . remove ( semaphoreFileName ) } saveTo <- paste ( readFrom , " . " , year , "-" , month , " . t o p i c f r e q u e n c y " , s e p =" " ) c o r p <- c r e a t e C o r p ( readFrom , year , month ) # # B u i l d a Document-Term Matrix dtm <- DocumentTermMatrix ( corp , control = l i s t ( minWordLength = 2 ) ) #k e e p words o f l e n g h t 2 or l o n g e r cat ( " B e f o r e t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" ) dtm <- removeFrequentWords ( dtm ) #removing b a s e d on median t f - i d f v a l u e cat ( " A f t e r t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" ) # # t h e f o l l o w i n g l i n e commented o u t t o a v o i d s p a r s e e r r o r dtm <- removeSparseTerms ( dtm , 1 - ( 1 . 1 /nrow ( dtm ) ) ) i n 1 document dtm <- dtm [ row sums ( dtm ) > 0 , ] u n l i k e l y event ) cat ( " A f t e r removing terms a p p e a r i n g o n l y i n 1 document : term count =" , ncol ( dtm ) , #remove d o c s t h a t have no terms r e m a i n i n g ( #remove terms a p p e a r i n g o n l y

127

APPENDIX B. CODE LISTINGS
" , doc count =" , nrow ( dtm ) , " \ n" ) 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 } cat ( " Saved data t o " , saveTo , " \ n" ) cat ( "Done \ n" ) } }

B.4. PROGRAM: DO LDA ANALYSIS TOPICS.R

#s e t u p p a r a l l e l b a c k e n d t o u s e 8 p r o c e s s o r s c l<-makeCluster ( 8 ) registerDoParallel ( cl ) cat ( " t o p i c C o u n t \ tmdl . a l p h a \ tmdl . b e t a . mean \ tmdl . b e t a . sd \ t t i m e ( s e c ) \ t t o p i c . f r e q u e n c y \ n" , s e p=" \ t " , append = T , f i l e = saveTo ) # t o f i l e f o r e a c h ( t o p i c C o u n t = 2 : nrow ( dtm ) #max = 1 t o p i c p e r document , . packages= ' t o p i c m o d e l s ' #i n c l u d e p a c k a g e ) %dopar% { #change t o %do% f o r s e q u e n t i a l e x e c u t i o n s t a r t R u n <- Sys . time ( ) v a l <- g e t T o p i c s F r e q u e n c y ( dtm , t o p i c C o u n t ) p r e f i x <- paste ( topicCount , v a l $ mdl . alpha , v a l $ mdl . beta . mean , v a l $ mdl . beta . sd , d i f f t i m e ( Sys . time ( ) , startRun , u n i t s = " s e c s " ) , s e p=" \ t " ) cat ( " TopicCount : " , topicCount , " \ n" ) #t o s c r e e n ( no s c r e e n o u t p u t i s p a r a l l e l mode ) while ( f i l e . e x i s t s ( semaphoreFileName )== TRUE) { Sys . s l e e p ( 1 ) ; cat ( " i n w h i l e s l e e p i n g 1 s e c \ n" ) ; f i l e . create ( semaphoreFileName ) # # lock f i l e f o r ( i i n 1 : length ( v a l $ t o p i c . frequency ) ) { #c a t (" i n c a t f o r " , i , Sys . t i m e ( ) , f i l e . e x i s t s ( semaphoreFileName ) , " \ n ") cat ( p r e f i x , v a l $ t o p i c . frequency [ i ] , " \ n" , s e p=" \ t " , append = T , f i l e = saveTo ) # t o f i l e f i l e . remove ( semaphoreFileName ) # # unlock f i l e #c a t (" Semaphore i s " , f i l e . e x i s t s ( semaphoreFileName ) , " \ n ")

128

APPENDIX B. CODE LISTINGS

B.5. PROGRAM: DO LDA QUARTER INC.R

B.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35

Program: do lda quarter inc.R

# #-------------------------------------------------------------- # # The s c r i p t r e a d s p o s t s i n d e l i m i t e d format , and g e n e r a t e s # # d i s t r i b u t i o n o f LDA t o p i c s f o r a g i v e n q u a r t e r -year , # # s a v i n g t h e o u t p u t i n i n f i l e name + " . " + y e a r "- q " + q u a r t e r " . t o p i c frequency " # # Usage : R s c r i p t do l d a q u a r t e r i n c .R i n f i l e name y e a r q u a r t e r # #-------------------------------------------------------------- l i b r a r y ( tm ) library ( f o r e a c h ) library ( d o P a r a l l e l ) source ( " u t i l s .R" ) args <- commandArgs ( t r a i l i n g O n l y = TRUE) readFrom <- args [ 1 ] year <- as . integer ( args [ 2 ] ) as . integer ( args [ 3 ] ) q u a r t e r <-

semaphoreFileName <- paste ( " semaf " , Sys . g e t p i d ( ) , s e p= ' ' ) # append R' s p r o c e s s i d t o semaphore t o a v o i d c o n f l i c t i f ( f i l e . e x i s t s ( semaphoreFileName ) ) { f i l e . remove ( semaphoreFileName ) } saveTo <- paste ( readFrom , " . " , year , "-q" , q u a r t e r , " . t o p i c f r e q u e n c y " , s e p =" " ) c o r p <- c r e a t e C o r p Q u a r t e r ( readFrom , year , q u a r t e r ) # # B u i l d a Document-Term Matrix dtm <- DocumentTermMatrix ( corp , control = l i s t ( minWordLength = 2 ) ) #k e e p words o f l e n g h t 2 or l o n g e r cat ( " B e f o r e t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" ) dtm <- removeFrequentWords ( dtm ) #removing b a s e d on median t f - i d f v a l u e cat ( " A f t e r t f - i d f : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" ) # # t h e f o l l o w i n g l i n e commented o u t t o a v o i d s p a r s e e r r o r dtm <- removeSparseTerms ( dtm , 1 - ( 1 . 1 /nrow ( dtm ) ) ) i n 1 document dtm <- dtm [ row sums ( dtm ) > 0 , ] u n l i k e l y event ) cat ( " A f t e r removing terms a p p e a r i n g o n l y i n 1 document : term count =" , ncol ( dtm ) , " , doc count =" , nrow ( dtm ) , " \ n" ) #s e t u p p a r a l l e l b a c k e n d t o u s e 8 p r o c e s s o r s #remove d o c s t h a t have no terms r e m a i n i n g ( #remove terms a p p e a r i n g o n l y

129

APPENDIX B. CODE LISTINGS
36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 } cat ( " Saved data t o " , saveTo , " \ n" ) cat ( "Done \ n" ) } f i l e . remove ( semaphoreFileName ) } s t a r t R u n <- Sys . time ( ) t o p i c C o u n t <- vecCount [ i x ] vecCount <- i n c C a l c ( nrow ( dtm ) ) c l<-makeCluster ( 8 ) registerDoParallel ( cl )

B.5. PROGRAM: DO LDA QUARTER INC.R

cat ( " t o p i c C o u n t \ tmdl . a l p h a \ tmdl . b e t a . mean \ tmdl . b e t a . sd \ t t i m e ( s e c ) \ t t o p i c . f r e q u e n c y \ n" , s e p=" \ t " , append = T , f i l e = saveTo ) # t o f i l e

# # f o r e a c h ( t o p i c C o u n t = 2 : nrow ( dtm ) #max = 1 t o p i c p e r document cat ( " l e n g h t ( vecCount )=" , length ( vecCount ) , " \ n" ) f o r e a c h ( i x = 2 : length ( vecCount ) , . packages= ' t o p i c m o d e l s ' #i n c l u d e p a c k a g e ) %do% { #change t o %dopar% f o r m u l t i p r o c e s s e x e c u t i o n

v a l <- g e t T o p i c s F r e q u e n c y ( dtm , t o p i c C o u n t ) p r e f i x <- paste ( topicCount , v a l $ mdl . alpha , v a l $ mdl . beta . mean , v a l $ mdl . beta . sd , d i f f t i m e ( Sys . time ( ) , startRun , u n i t s = " s e c s " ) , s e p=" \ t " ) cat ( " TopicCount : " , topicCount , " \ n" ) #t o s c r e e n ( no s c r e e n o u t p u t i s p a r a l l e l mode ) while ( f i l e . e x i s t s ( semaphoreFileName )== TRUE) { Sys . s l e e p ( 1 ) ; cat ( " i n w h i l e s l e e p i n g 1 s e c \ n" ) ; f i l e . create ( semaphoreFileName ) # # lock f i l e f o r ( i i n 1 : length ( v a l $ t o p i c . frequency ) ) { cat ( p r e f i x , v a l $ t o p i c . frequency [ i ] , " \ n" , s e p=" \ t " , append = T , f i l e = saveTo ) # t o f i l e

130

APPENDIX B. CODE LISTINGS

B.6. PROGRAM: UTILS.R

B.6
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34

Program: utils.R

library ( topicmodels ) # #------------------------------------------------------------------------------ # # The f u n c t i o n t a k e s term document m a t r i x ( s p a r s e r e p r e s e n t a t i o n ) , tdm , and exports i t to f i l e # # f i l e N a m e i n t h e f o l l o w i n g f o r m a t " d i s t i n c t W o r d C o u n t wordId : WordCount . . . " , one document p e r # # l i n e . For example , "2 1 5 : 7 2 1 : 4 " means t h a t a document c o n t a i n s two d i s t i n c t words # # w i t h i d s 15 and 21 , a p p e a r i n g 7 and 4 ti me s , r e s p e c t i v e l y . # # This f o r m a t i s u s e d by LDA and HDP -LDA code p r o v i d e d by B l e i ' s team . # # # # The f u n c t i o n s a v e s d i c t i o n a r y o f words , one word p e r l i n e t o f i l e fileName , s u f f i x e d by " . d i c " . # # I d o f a g i v e n word i s g i v e n by word ' s l i n e number i n t h e f i l e f i l e N a m e . d i c ; # # l i n e number c o u n t s t a r t s from 1 . # # # # The f u n c t i o n s a v e s document "names " , one name p e r l i n e t o f i l e fileName , s u f f i x e d by " . doc " . # # I d o f a g i v e n document i s e q u i v a l e n t t o t h e document ' s l i n e number i n t h e f i l e f i l e N a m e . doc ; # # l i n e number c o u n t s t a r t s from 1 . # # # # Usage example : e x p o r t 2 l d a c ( myTermDocMatrix , " ~/ f o o . t x t ") # # Note t h a t t h e e x i s t i n g file w i l l be o v e r w r i t t e n # #---------------------------------------------------------------------------- e x p o r t 2 l d a c <- function ( tdm , f i l e N a m e ) { # We assume t h a t # j - term ( a . k . a . word ) i d s # i - doc i d s # v - word f r e q u e n c y f i d <- f i l e ( fil eNa me , "w" ) # o v e r w r i t e e x i s t i n g d o c I d s <- unique ( tdm$ i ) for ( docId in docIds ) { # I am n o t c e r t a i n i f $ j i s g u a r a n t e e d t o be c o n s e c u t i v e ; # hence t h e i n e f f i c i e n t s e a r c h u s i n g which wordIndexes <- which ( tdm$ i == d o c I d ) cat ( length ( wordIndexes ) , f i l e = f i d , s e p = " " , append = T) f o r ( wordIndex i n append = T) wordIndexes ) { cat ( " " , tdm$ j [ wordIndex ] , " : " , tdm$ v [ wordIndex ] , f i l e = f i d , s e p = " " , file

131

APPENDIX B. CODE LISTINGS
35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 } return ( doCorpCreation ( P o s t s ) ) } } else { stop ( " P r o v i d e y e a r and q u a r t e r " ) i f ( ! missing ( y e a r ) & ! missing ( q u a r t e r ) ) { P o s t s $create t s <- as . POSIXlt ( P o s t s $create t s ) } # s a v e o r i g i n a l document names # s a v e d i c t i o n a r y o f words } close ( f i d ) } cat ( " \ n" , f i l e = f i d , s e p = " " , append = T)

B.6. PROGRAM: UTILS.R

write ( tdm$dimnames$ Terms , f i l e = paste ( fil eNa me , " . d i c " , s e p=" " ) , s e p = " \ n" )

write ( tdm$dimnames$ Docs , f i l e = paste ( fil eNam e , " . doc " , s e p=" " ) , s e p = " \ n" )

# # This f u n c t i o n r e a d s t h e d a t a from c s v f i l e , c l e a n s i t and r e t u r n s tm t e x t Corpus # # readFromfileName - name o f t h e f i l e t o r e a d from # # Q u a r t e r l y F i l t e r ( mandatory ) : # # # # year - mandatory : y e a r when d o c s were c r e a t e d , f o r m a t = YYYY month - mandatory : month when d o c s were c r e a t e d

c r e a t e C o r p Q u a r t e r <- function ( readFromfileName , year , q u a r t e r ) { l i b r a r y ( tm ) # # Load t h e d a t a P o s t s <- read . d e l i m ( f i l e = readFromfileName , h e a d e r = T, quote = " " , s e p = " \ t " ) cat ( "Read" , nrow ( P o s t s ) , " rows from " , readFromfileName , " \ n" )

P o s t s <- subset ( Posts , q u a r t e r s ( P o s t s $create t s ) == paste ( "Q" , q u a r t e r , s e p = "" ) & P o s t s $create t s $ y e a r == ( y e a r - 1 9 0 0 ) ) cat ( "Kept" , nrow ( P o s t s ) , " rows \ n" )

# # This f u n c t i o n r e a d s t h e d a t a from c s v f i l e , c l e a n s i t and r e t u r n s tm t e x t Corpus # # readFromfileName - name o f t h e f i l e t o r e a d from

132

APPENDIX B. CODE LISTINGS
74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 } cat ( " Created c o r p u s from " , length ( c o r p ) , " documents \ n" ) return ( c o r p ) # # Transform d a t a # # Build a corpus c o r p <- Corpus ( DataframeSource ( P o s t s ) ) # # t h i s i s a p r i v a t e function f or corpus creation doCorpCreation <- function ( P o s t s ) { } return ( doCorpCreation ( P o s t s ) ) } i f ( ! missing ( y e a r ) & ! missing ( month ) ) { P o s t s $create t s <- as . POSIXlt ( P o s t s $create t s ) # # Monthly F i l t e r ( o p t i o n a l ) : # # # # year

B.6. PROGRAM: UTILS.R

- o p t i o n a l : y e a r when d o c s were c r e a t e d , f o r m a t = YYYY

month - o p t i o n a l : month when d o c s were c r e a t e d

c r e a t e C o r p <- function ( readFromfileName , year , month ) { l i b r a r y ( tm ) # # Load t h e d a t a P o s t s <- read . d e l i m ( f i l e = readFromfileName , h e a d e r = T, quote = " " , s e p = " \ t " ) cat ( "Read" , nrow ( P o s t s ) , " rows from " , readFromfileName , " \ n" )

P o s t s <- subset ( Posts , P o s t s $create t s $mon == ( month - 1 ) & P o s t s $create t s $ y e a r == ( y e a r - 1 9 0 0 ) ) cat ( "Kept" , nrow ( P o s t s ) , " rows \ n" )

# C o n c a t e n a t e columns , o t h e r w i s e DataframeSource g e t s c o n f u s e d P o s t s <- data . frame ( paste ( P o s t s $ t i t l e , P o s t s $body , P o s t s $ answers , s e p =" " ) )

c o r p = tm map( corp , c o n t e n t t r a n s f o r m e r ( t o l o w e r ) ) #c o n v e r t i n g t o l o w e r c a s e c o r p = tm map( corp , removeWords , s t o p w o r d s ( ' e n g l i s h ' ) ) # Remove S t o p w o r d s c o r p = tm map( corp , stemDocument ) # Stemming c o r p = tm map( corp , s t r i p W h i t e s p a c e ) # E l i m i n a t e w h i t e s p a c e c h a r

# # Removes t o p i c s from t h e Document Term Matrix u s i n g t f - i d f approach # # dtm - Document Term Matrix t o p r o c e s s # # t h r e s h o l d - remove words w i t h t f - i d f v a l u e s s m a l l e r t h a n t h r e s h o l d # # t h i s parameter i s o p t i o n a l : i f t h r e s h o l d i s n o t d e f i n e d , we w i l l remove

133

APPENDIX B. CODE LISTINGS
116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 } i n c C a l c <- function ( totCount ) { return ( l i s t ( mdl . a l p h a = mdl . alpha , mdl . beta . mean = mdl . beta . mean , mdl . beta . sd = mdl . beta . sd , t o p i c . frequency = as . vector ( table ( t o p i c s ( mdl ) ) ) )) mdl . a l p h a <- mdl@alpha mdl . beta . mean <- mean ( mdl@beta ) mdl . beta . sd <- sd ( mdl@beta ) } } # # # #

B.6. PROGRAM: UTILS.R

most f r e q u e n t words , b a s e d on t h e median v a l u e o f t f - i d f ( a t l e a s t 50%) S e t t i n g t h r e s h o l d = 0 w i l l e l i m i n a t e words a p p e a r i n g i n e v e r y document

removeFrequentWords <- function ( dtm , t h r e s h o l d ) { l i b r a r y ( " slam " ) t e r m T f i d f <- tapply ( dtm$ v /row sums ( dtm ) [ dtm$ i ] , dtm$ j , mean )  log2 ( nDocs ( dtm ) / c o l sums ( dtm > 0 ) ) # i f no t h e s h o l d i s p r o v i d e d t h e n s e t t h e t h r s h o l d v a l u e t o median o f t f - i d f # d i s t r i b u t i o n , removing a t l e a s t 50% o f t h e words i f ( missing ( t h r e s h o l d ) ) { t h r e s h o l d <- median ( t e r m T f i d f )

# remove terms which have t f - i d f s m a l l e r t ha n t h e median o f a l l t h e t f - i d f values # t h i s w i l l s h r i n k t h e d i c t i o n a y by a p p r o x i m a t e l y 50% dtm <- dtm [ , t e r m T f i d f > t h r e s h o l d ] dtm <- dtm [ row sums ( dtm ) > 0 , ] u n l i k e l y event ) return ( dtm ) #remove d o c s t h a t have no terms r e m a i n i n g (

# # This f u n c t i o n r e t u r n s f r e q u e n c y o f t o p i c s f o r a g i v e n LDA model # # d a t - Document Term Matrix t o f e e d t o t h e LDA model # # t o p i c C o u n t - Number o f t o p i c s i n t h e LDA model g e t T o p i c s F r e q u e n c y <- function ( dat , t o p i c C o u n t ) { mdl <- LDA( dat , t o p i c C o u n t ) #LDA model

# # F u n c t i o n t o c a l c u l a t e i n c r e m e n t s , 2 . . 1 0 0 , 1 1 0 1 . . 2 0 0 5 , 2 0 1 . . 3 0 0 10 . . . capped t o 25 a f t e r t o t C o u n t >= 600 t o 25 numRanges <- c e i l i n g ( totCount / 1 0 0 ) # # t h i s i s t o e n s u r e t h a t i f g o e s beyond

134

APPENDIX B. CODE LISTINGS
100 ' s i t i s t a k e n i n t o a c c o u n t 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 } } return ( vecCount ) } cat ( "maxCount=" , maxCount , " \ n" ) } cat ( "p=" , p , " \ n" ) p <- i cat ( " i n d I=" , i n d I , " i n d J=" , indJ , " i=" , i , " \ n" ) MRanges [ i , 1 ] <- i n d I MRanges [ i , 2 ] <- i n d J maxCount <- 1 cat ( "numRanges=" , numRanges , " \ n" ) f o r ( i i n 1 : numRanges ) { i n d I <- ( 1 0 0  i ) -100+1 i n d J <- 100  i vecRanges = matrix ( nrow = 1 , ncol = numRanges ) vecCount <- c ( ) MRanges <- matrix ( nrow = numRanges , ncol = 2 ) i x <- 1

B.6. PROGRAM: UTILS.R

cat ( " i=" , i , " from=" , MRanges [ i , 1 ] , " t o " , MRanges [ i , 2 ] )

i f (p > 1 & & p < 7 ) { p <- 5  ( i - 1) } e l s e { i f ( p !=1 ) { p <-25 }}

f o r ( j i n seq ( from=MRanges [ i , 1 ] , t o=MRanges [ i , 2 ] , by=p ) ) { i f ( j < totCount ) { # # This i s t o l i m i t t h e c a l c u l a t i o n o f i n t e r v a l s up to totCount vecCount [ i x ] <- j maxCount <- i x cat ( " j =" , j , " \ n" ) cat ( " vecCount [ " , i x , " ]= " , vecCount [ i x ] , " \ n" ) i x <- i x + 1

135

APPENDIX B. CODE LISTINGS

B.7. PROGRAM: CONVVAR K200.R

B.7
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 }

Program: convVar k200.R

# #------------------------------------------------------------------------------ # ## This program r e a d s t h e  . t o p i c . f r e q u e n c y f i l e s f o r each d a t a s e t g e t s t h e t o p f r e q u e n c y f o r x = 2 . . 5 0 , and merges them up t o g e t h e r # ## f o r them t o be a n a l y z e d l a t e r w i t h a r e g r e s s i o n model # #------------------------------------------------------------------------------ r e a d F i l e <- function ( fName ) { dat <- read . d e l i m ( f i l e = fName , h e a d e r = T, row . names=NULL) colnames ( dat )<-c ( colnames ( dat ) [ - 1 ] , "x" ) dat $ x<-NULL dat<-as . data . frame ( dat ) return ( dat ) f i l l =T, s e p = " \ t " ,

p r o c e s s F i l e <- function ( dat ) { t o p i c C o u n t L i s t <- sort ( unique ( s t r t o i ( dat $ t o p i c C o u n t ) ) ) # # making s u r e t h e l i s t i s ordered t i m e f r a m e type = vector ( ) t i m e f r a m e = vector ( ) d a t a s e t name = vector ( ) t o p i c C o u n t = vector ( ) p o s t F r a c t i o n = vector ( ) topXX = vector ( ) documentCount = vector ( ) topX <- data . frame ( t i m e f r a m e t yp e = vector ( ) , t i m e f r a m e = vector ( ) , d a t a s e t name = vector ( ) , t o p i c C o u n t = vector ( ) , p o s t F r a c t i o n = vector ( ) , topXX = vector ( ) , documentCount = vector ( ) )

26 27 28 29 30 31 32 33 34 35 36 37 } else { d <- dat [ which ( dat $ t o p i c C o u n t == t ) , ] f r e q <- sort ( d $ t o p i c . frequency , d e c r e a s i n g = T) documentCount <- sum ( f r e q ) #number o f d o c s i n c o r p u s p o s t F r a c t i o n = sum ( f r e q [ 1 : x ] ) / documentCount # t h i s i s an F v a l u e f o r a f o r ( x i n 2 : 5 0 ) {#t o p X t o p i c s for ( t in topicCountList ) { # # s t r t o i ( topicCount ) if (t < x){ # # t o a v o i d w r i t i n g NA' s next

136

APPENDIX B. CODE LISTINGS
given top X 38 39 40 41 42 43 } i f ( i s . na ( p o s t F r a c t i o n ) ) {

B.7. PROGRAM: CONVVAR K200.R

cat ( "====================== p o s t f r a c t i o n i s NA program s k i p s i n t e r a c t i o n t=" , t , "===============\n" ) cat ( "x=" , x , " documentCount=" , documentCount , " \ n" ) next topX <- rbind ( topX , data . frame ( t i m e f r a m e ty p e = Xtimeframe t yp e [ i dsName ] , t i m e f r a m e = Xtimeframe [ i l i s t f ] , d a t a s e t name = X d a t a s e t name [ i dsName ] , t o p i c C o u n t = t , p o s t F r a c t i o n = p o s t F r a c t i o n , topXX = x , documentCount = documentCount ) )

44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 } } }

}

# # w r i t e s o u t p u t f i l e e v e r y t i m e f i n i s h e s p r o c e s s i n g one f r e q u e n c y f i l e fName <- paste ( outPath , "topX" , " . c s v " , s e p = " " ) if ( firstime ){ cat ( "  WRITING OUTPUT FILE FIRSTIME " , f i r s t i m e , " \ n" ) cat ( dir ( outPath ) , " \ n" ) # # c a t (" FILE EXIST NOT NEGATED: " , fName , " i s " , f i l e . e x i s t s ( fName ) , " \ n ") # # c a t (" FILE EXIST NEGATED: " , fName , " i s " , ! f i l e . e x i s t s ( fName ) , " \ n ") write . table ( topX , f i l e = fName , s e p = " , " , row . names=FALSE, c o l . names=TRUE, append = T) firstime < <- FALSE print ( f i l e . i n f o ( fName ) ) } else { cat ( "  WRITING OUTPUT FILE SUCCESIVE  " , f i r s t i m e , " \ n" ) write . table ( topX , f i l e = fName , s e p = " , " , row . names=FALSE, c o l . names=FALSE, append = T) print ( f i l e . i n f o ( fName ) ) } # # EOF

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### main ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # -% M -%S %Y" ) XSystime < <- format ( Sys . time ( ) , "%a-%b-%d %H dsName <- c ( " s a l e s f o r c e -m -" , "dba-m -" , "dba-q-" , " android -m -" ) # # to handle the ds names

137

APPENDIX B. CODE LISTINGS
73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 for ( i l i s t f i n 1 : length ( l i s t f ) ) { listf ] readFromfileName <- l i s t f [ i Xtimeframe < <- vector ( ) Xtimeframe [ i l i s t f <- l i s t . f i l e s ( p a t t e r n=" P o s t s . xml . c s v " ) setwd ( f f D i r [ i dsName ] ) FKdir <- c ( )

B.7. PROGRAM: CONVVAR K200.R

# ## t e s t dsName <- c (" t e s t -q -") ## t o h a n d l e t h e ds names <- c ( " monthly " , " monthly " , " q u a r t e r l y " , " monthly " ) Xtimeframe type < <- c ( " s a l e s f o r c e " , " dba " , " dba " , " a n d r o i d " ) X d a t a s e t name <

o u t L i s t f <- c ( ) # ## t h i s i s t h e o u t p u t d i r e c t o r y o f t h e f i l e s t o be p l o t t e d , may n o t be u s e d f f D i r <- c ( ) <- 1 i dsName < firstime < <- TRUE # # t h i s f l a g was p l a c e d a f t e r f i r s t , f o r so i t w r o t e t h e h d r s each tim e ds changed ! f o r ( i dsName i n 1 : length ( dsName ) ) { cat ( "DEBUG: P r o c e s s i n g d a t a s e t=" , dsName [ i dsName ] , " a t : " , XSystime , " \ n" ) # # former FKdir [ i dsName ] <- f i l e . p a t h (" ~ " , " T h e s i s " , " F i t t i n g " , p a s t e ( dsName [ i dsName ] , "FK" , s e p ="") , " f i l e s -FK") # # former f f D i r [ i dsName ] <- f i l e . p a t h (" ~ " , " T h e s i s " , " F i t t i n g " , p a s t e ( dsName [ i dsName ] , "FK" , s e p ="") , " f i l e s - f r e q u e n c i e s ") FKdir [ i dsName ] <- f i l e . path ( " / media " , " data " , " t h e s i s " , " T h e s i s " , " F i t t i n g " , paste ( dsName [ i dsName ] , "FK" , s e p=" " ) , " f i l e s -FK" ) f f D i r [ i dsName ] <- f i l e . path ( " / media " , " data " , " t h e s i s " , " T h e s i s " , " F i t t i n g " , paste ( dsName [ i dsName ] , "FK" , s e p=" " ) , " f i l e s - f r e q u e n c i e s " )

cat ( "DEBUG: f f D i r [ " , i dsName , " ]= " , f f D i r [ i dsName ] , " \ n" )

# # former o u t P a t h < <- f i l e . p a t h (" ~ " , " T h e s i s " , " F i t t i n g " ," / ") outPath < <- f i l e . path ( " / media " , " data " , " t h e s i s " , " T h e s i s " , " F i t t i n g " , " / " ) ##} ## ends h e r e f o r d e b u g g i n g p u r p o s e s n e x t code d e a c t i v a t e d : # # f o r ( i dsName i n 1 : l e n g t h ( dsName ) ) {

cat ( " l i s t f [ " , i l i s t f , " ]= " , l i s t f [ i

l i s t f ] , " \ n" )

l i s t f ] <- substr ( gsub ( " \ \ . " , "-" , readFromfileName ) , 1 5 , 2 1 )

readFromfileName <- gsub ( " " , " " , readFromfileName , f i x e d = TRUE)

138

APPENDIX B. CODE LISTINGS
110 111 112 113 114 115 } cat ( "  END OF EXECUTION  \ n" ) }

B.7. PROGRAM: CONVVAR K200.R
listf ] ,

cat ( "DEBUG: Reading=" , readFromfileName , " Xtimeframe=" , Xtimeframe [ i " \ n" ) dat <- r e a d F i l e ( readFromfileName ) p r o c e s s F i l e ( dat )

139

APPENDIX B. CODE LISTINGS

B.8. PROGRAM: VERIFYFIT K200.R

B.8
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 } exp ( a }

Program: verifyFit k200.R

# #---------------------------------------------------------------------------- # # This program i t e r a t e s o v e r t h e s p e c i f i e d d a t a s e t s , and computes t h e RMSE o f the approximation formulas # #---------------------------------------------------------------------------- #s e t t o True i f you want t o e n a b l e f i l t e r i n g o f K > 200 , e l s e s e t t o F a l s e data f i l t e r 200 <- T readFromfileName = "topX . c s v . o r i g i n a l . z i p " i f ( data f i l t e r 2 0 0 ) { model f i l e name <- " . / models / models f i t t i n g . remove top 25 p e r c e n t and v a l u e s g t 2 0 0 . rda " models p e r f o r m a n c e f i l e name <- " . / models / models p e r f o r m a n c e . remove top 25 p e r c e n t and v a l u e s g t 2 0 0 . c s v " } else { model f i l e name <- " . / models / models f i t t i n g . remove top 25 p e r c e n t . rda " models p e r f o r m a n c e f i l e name <- " . / models / models p e r f o r m a n c e . remove top 25 percent . csv "

load ( model f i l e name ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### d e f i n e approximation formulas ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # The " f l e x i b l e " model a (X, N)  K^ b (X,N) # K == t o p i c c o u n t # X == t h e top -X # N == number o f documents model two param s i m p l e <- function (K, X, N) { v a l <- data . frame (X = X, N = N) a <- predict ( dat . lm . i n t e r c e p t , v a l ) # 1 . 1 7 1 + 0 . 6 4 9  l o g (X) - 0 . 1 1 6  l o g (N) b <- predict ( dat . lm . s l o p e , v a l ) + b  log (K) ) # - 0.841 + 0 . 0 0 2  X + 0 . 0 0 0 0 3  N

model two param complex <- function (K, X, N) {

140

APPENDIX B. CODE LISTINGS
38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 } # r o o t -mean-s q u a r e e r r o r rmse <- function ( a c t u a l , e x p e c t e d ) { sqrt ( mean ( ( e x p e c t e d - a c t u a l ) ^ 2 ) ) } X^-b  K^b model one param complex <- function (K, X, N) { v a l <- data . frame (X = X, N = N) b <- predict ( dat . lm . b . complex , v a l ) } X^-b  K^b model one param s i m p l e <- function (K, X, N) { v a l <- data . frame (X = X, N = N) b <- predict ( dat . lm . b , v a l ) N # The " c o n s t r a i n e d " model X^- b (X, N)  K^ b (X,N) # K == t o p i c c o u n t # X == t h e top -X # N == number o f documents } exp ( a + b  log (K) ) v a l <- data . frame (X = X, N = N) a <- predict ( dat . lm . i n t e r c e p t . complex , v a l ) b <- predict ( dat . lm . s l o p e . complex , v a l )

B.8. PROGRAM: VERIFYFIT K200.R

#- 8.408351E-01 + 2 . 1 1 0 6 6 7E-03  X + 3 . 3 9 7 9 0 1E-05 

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### helpers ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### main ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # dat <- read . csv ( unz ( readFromfileName , "topX . c s v . o r i g i n a l " ) , h e a d e r = T, s e p = " , " , row . names = NULL) # # NOT EXCEL

141

APPENDIX B. CODE LISTINGS
79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 # f i t i n d v i d u a l c o n s t r a i n e d model # f i t i n d v i d u a l f l e x model } dat . s t a t s <- data . frame ( ) f o r ( iG i n 1 : length ( v a l i d G r o u p s ) ) { s <- s t r s p l i t ( v a l i d G r o u p s [ iG ] , " " ) s d f <- as . data . frame ( s ) p1 <- as . character ( s d f [ 1 , ] ) # # t i m e fr a m e t y p e p2 <- as . character ( s d f [ 2 , ] ) # # t i m e fr a m e p3 <- as . character ( s d f [ 3 , ] ) # # d a t a s e t name

B.8. PROGRAM: VERIFYFIT K200.R

v a l i d G r o u p s <- unique ( paste ( dat $ t i m e f r a m e type , dat $ timeframe , dat $ d a t a s e t name , sep = " " ) )

cat ( " p1=" , p1 , " p2=" , p2 , " p3=" , p3 , " iG=" , iG , " \ n" ) f o r ( itopXX i n 2 : 5 0 ) { ds <- dat [ dat $ t i m e f r a m e type == == p3 & dat $topXX == itopXX , ] #c a t (" p1 =", p1 , " p2 =", p2 , " p3 =", p3 , " itopXX =", itopXX , " iG=", iG , " \ n ") i f ( data f i l t e r 2 0 0 ) { #i f True -- k e e p o n l y v a l u e s o f K <= 200 ds <- ds [ ds $ t o p i c C o u n t <= 2 0 0 , ] ds <- ds [ ds $ t o p i c C o u n t < 0 . 7 5  max( ds $ documentCount ) , ] p1 & dat $ t i m e f r a m e == p2 & dat $ d a t a s e t name

ds . lm . f l e x <- lm ( log ( p o s t F r a c t i o n ) ~ log ( t o p i c C o u n t ) , data = ds )

ds . lm . c o n s t r <- n l s ( ds $ p o s t F r a c t i o n ~ ds $topXX^( - b )  ds $ t o p i c C o u n t ^b , data = ds , s t a r t = l i s t ( b = -1) )

F . ds . lm . f l e x <- exp ( predict ( ds . lm . f l e x ) ) #k e e p i n mind t h a t we a r e o p e r a t i n g on l o g -t r a n s f o r m d a t a here , need t o c o n v e r t i t b a c k F . ds . lm . c o n s t r <- predict ( ds . lm . c o n s t r ) F . model two param s i m p l e <- model two param s i m p l e ( ds $ topicCount , ds $topXX [ 1 ] , ds $ documentCount [ 1 ] ) F . model one param s i m p l e <- model one param s i m p l e ( ds $ topicCount , ds $topXX [ 1 ] , ds $ documentCount [ 1 ] ) F . model two param complex <- model two param complex ( ds $ topicCount , ds $topXX [ 1 ] , ds $ documentCount [ 1 ] ) F . model one param complex <- model one param complex ( ds $ topicCount , ds $topXX [ 1 ] , ds $ documentCount [ 1 ] )

142

APPENDIX B. CODE LISTINGS
114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 } } ) ) #compute rmse and s a v e s t a t s dat . s t a t s <- rbind ( dat . s t a t s , data . frame ( dt p a r t i t i o n = p1 , time frame = p2 , d a t a s e t name = p3 , X = ds $topXX [ 1 ] , N = ds $ documentCount [ 1 ] ,

B.8. PROGRAM: VERIFYFIT K200.R

rmse f l e x model s i m p l e = rmse ( ds $ p o s t F r a c t i o n , F . model two param s i m p l e ) , rmse c o n s t r a i n t model s i m p l e = rmse ( ds $ p o s t F r a c t i o n , F . model one param simple ) , rmse f l e x model complex = rmse ( ds $ p o s t F r a c t i o n , F . model two param complex ), rmse c o n s t r a i n t model complex = rmse ( ds $ p o s t F r a c t i o n , F . model one param complex ) , rmse t a i l o r e d f l e x lm = rmse ( ds $ p o s t F r a c t i o n , F . ds . lm . f l e x ) , rmse t a i l o r e d c o n s t r lm = rmse ( ds $ p o s t F r a c t i o n , F . ds . lm . c o n s t r )

write . csv ( dat . s t a t s , models p e r f o r m a n c e f i l e name , row . names = FALSE)

143

APPENDIX B. CODE LISTINGS

B.9. PROGRAM: VERIFYFIT K200 ANALYSIS.R

B.9
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 }

Program: verifyFit k200 analysis.R

# ------------------------------------------------------------------------- #s e t t o True i f you want t o e n a b l e f i l t e r i n g o f K > 200 , e l s e s e t t o F a l s e #p l o t s t h e RMSE b o x p l o t s p e r model and p e r d a t a s e t name # ------------------------------------------------------------------------- data f i l t e r 200 <- F i f ( data f i l t e r 2 0 0 ) { model f i l e name <- " . / models / models f i t t i n g . remove top 25 p e r c e n t and v a l u e s g t 2 0 0 . rda " models p e r f o r m a n c e f i l e name <- " . / models / models p e r f o r m a n c e . remove top 25 p e r c e n t and v a l u e s g t 2 0 0 . c s v " rmse p e r model f i l e name <- " . / models / models p e r f o r m a n c e and v a l u e s g t 2 0 0 . pdf " rmse p e r d a t a s e t f i l e name <- " . / models / models p e r f o r m a n c e top 25 p e r c e n t and v a l u e s g t 2 0 0 . p df " rmse p e r x f i l e name <- " . / models / models p e r f o r m a n c e p e r c e n t and v a l u e s g t 2 0 0 . p df " rmse p e r n f i l e name <- " . / models / models p e r f o r m a n c e p e r c e n t and v a l u e s g t 2 0 0 . p df " } else { model f i l e name <- " . / models / models f i t t i n g . remove top 25 p e r c e n t . rda " models p e r f o r m a n c e f i l e name <- " . / models / models p e r f o r m a n c e . remove top 25 percent . csv " rmse p e r model f i l e name <- " . / models / models p e r f o r m a n c e . pdf " rmse p e r d a t a s e t f i l e name <- " . / models / models p e r f o r m a n c e top 25 p e r c e n t . pdf " rmse p e r x f i l e name <- " . / models / models p e r f o r m a n c e p e r c e n t . pdf " rmse p e r n f i l e name <- " . / models / models p e r f o r m a n c e p e r c e n t . pdf " per n remove top 25 per x remove top 25 per dataset remove remove top 25 p e r c e n t per n remove top 25 per x remove top 25 per dataset remove remove top 25 p e r c e n t

dat . s t a t s <- read . csv ( models p e r f o r m a n c e f i l e name ) #l e t ' s v i s u a l i z e p er f or ma n ce o f t h e models library ( t i d y r ) dat . s t a t s . l o n g <- g a t h e r ( dat . s t a t s , model name , rmse , rmse f l e x model s i m p l e : rmse t a i l o r e d c o n s t r lm , f a c t o r key=TRUE)

144

APPENDIX B. CODE LISTINGS
30 31 32 #r e o r d e r f o r box - p l o t

B.9. PROGRAM: VERIFYFIT K200 ANALYSIS.R

dat . s t a t s . l o n g $model name <- f a c t o r ( dat . s t a t s . l o n g $model name , l e v e l s =c ( " rmse f l e x model s i m p l e " , " rmse f l e x model complex " , " rmse c o n s t r a i n t model s i m p l e " , " rmse c o n s t r a i n t model complex " , c o n s t r lm" ) ) " rmse t a i l o r e d f l e x lm" , " rmse t a i l o r e d

33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 pd f ( rmse p e r d a t a s e t f i l e name ) dat . s t a t s $ ds merged name <- as . f a c t o r ( paste ( dat . s t a t s $ d a t a s e t name , "-" , dat . s t a t s $dt p a r t i t i o n ) ) boxplot ( rmse c o n s t r a i n t model complex ~ ds merged name , data = dat . s t a t s , log = "y" , ) grid ( ) dev . o f f ( ) pd f ( rmse p e r d a t a s e t f i l e name ) dat . s t a t s $ ds merged name <- as . f a c t o r ( paste ( dat . s t a t s $ d a t a s e t name , "-" , dat . s t a t s $dt p a r t i t i o n ) ) boxplot ( rmse c o n s t r a i n t model complex ~ ds merged name , data = dat . s t a t s , log = "y" , #x l a b = " Model Name" , las = 2 , y l a b = "RMSE" , par ( mar = c ( 1 2 , 5 , 4 , 2 )+ 0 . 1 ) #names = c (" F l e x - s i m p l e " , " F l e x - complex " , " Constr . - s i m p l e " , " Constr . - complex " , " Ind . f i t - f l e x " , " Ind . f i t - c o n s t r ") ) grid ( ) dev . o f f ( ) pd f ( rmse p e r model f i l e name ) boxplot ( rmse ~ model name , data = dat . s t a t s . long , log = "y" , #x l a b = " Model Name" , las = 2 , y l a b = "RMSE" , par ( mar = c ( 1 2 , 5 , 4 , 2 )+ 0 . 1 ) , names = c ( " F l e x - s i m p l e " , " F l e x - complex " , " Constr . - s i m p l e " , " Constr . - complex " , " Ind . f i t - f l e x " , " Ind . f i t - c o n s t r " )

145

APPENDIX B. CODE LISTINGS
66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 #summary s t a t s ) grid ( ) dev . o f f ( ) pd f ( rmse p e r n f i l e name ) ) grid ( ) dev . o f f ( ) pd f ( rmse p e r x f i l e name ) ) grid ( ) dev . o f f ( ) #x l a b = " Model Name" ,

B.9. PROGRAM: VERIFYFIT K200 ANALYSIS.R

main = " Model - c o n s t r a i n e d , complex " , las = 2 , y l a b = "RMSE" , par ( mar = c ( 1 2 , 5 , 4 , 2 )+ 0 . 1 ) #names = c (" F l e x - s i m p l e " , " F l e x - complex " , " Constr . - s i m p l e " , " Constr . - complex " , " Ind . f i t - f l e x " , " Ind . f i t - c o n s t r ")

boxplot ( rmse c o n s t r a i n t model complex ~ X, data = dat . s t a t s , log = "y" , x l a b = "X" , main = " Model - c o n s t r a i n e d , complex " , #l a s = 2 , y l a b = "RMSE" #par ( mar = c ( 1 2 , 5 , 4 , 2)+ 0 . 1 ) #names = c (" F l e x - s i m p l e " , " F l e x - complex " , " Constr . - s i m p l e " , " Constr . - complex " , " Ind . f i t - f l e x " , " Ind . f i t - c o n s t r ")

boxplot ( rmse c o n s t r a i n t model complex ~ N, data = dat . s t a t s , log = "y" , x l a b = "N" , main = " Model - c o n s t r a i n e d , complex " , #l a s = 2 , y l a b = "RMSE" #par ( mar = c ( 1 2 , 5 , 4 , 2)+ 0 . 1 ) #names = c (" F l e x - s i m p l e " , " F l e x - complex " , " Constr . - s i m p l e " , " Constr . - complex " , " Ind . f i t - f l e x " , " Ind . f i t - c o n s t r ")

146

APPENDIX B. CODE LISTINGS
106 107 library ( s t a r g a z e r ) s t a r g a z e r ( dat . s t a t s [ , 6 : 1 1 ] )

B.9. PROGRAM: VERIFYFIT K200 ANALYSIS.R

147

APPENDIX B. CODE LISTINGS

B.10. PROGRAM: VALIDATE FIT.R

B.10
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 } } }

Program: validate fit.R

# -------------------------------------------------------------------------- # s e t t o True i f you want t o e n a b l e f i l t e r i n g o f K > 200 , e l s e s e t t o F a l s e # p e r f o r m s v a l i d a t i o n u s i n g 10 f o l d s # -------------------------------------------------------------------------- data f i l t e r 200 <- F # r o o t -mean-s q u a r e e r r o r rmse <- function ( a c t u a l , e x p e c t e d ) { sqrt ( mean ( ( e x p e c t e d - a c t u a l ) ^ 2 ) )

# one param complex p r e d i c t i o n model model one param complex <- function (K, X, N) { v a l <- data . frame (X = X, N = N) b <- predict ( dat . lm . b . complex , v a l ) X^-b  K^b

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### g e t raw d a t a ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # readFromfileName = "topX . c s v . o r i g i n a l . z i p " dat . raw <- read . csv ( unz ( readFromfileName , "topX . c s v . o r i g i n a l " ) , h e a d e r = T, s e p = " , " , row . names = NULL) #add k e y column dat . raw $data subset name <- paste ( dat . raw $ t i m e f r a m e type , dat . raw $ timeframe , dat . raw $ d a t a s e t name ) # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### g e t t h e i n d i v i d u a l f i t o f b- v a l u e s ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # i f ( data f i l t e r 2 0 0 ) { to f i t } else { to f i t f i l e name <- " t o f i t . remove top 25 p e r c e n t . c s v " f i l e name <- " t o f i t . remove top 25 p e r c e n t and v a l u e s g t 2 0 0 . c s v "

148

APPENDIX B. CODE LISTINGS
40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 } #a p p l y DS1 or DS2 f i l t e r rmse p e r f o l d <- c ( ) for ( f in 1 : 1 0 ) { #s p l i t 96 d a t a s e t s i n t o 10 f o l d s #add k e y column dat <- read . csv ( t o f i t f i l e name )

B.10. PROGRAM: VALIDATE FIT.R

dat $data subset name <- paste ( dat $time i n t e r v a l , dat $time frame , dat $ d a t a s e t name )

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### do run 10- f o l d v a l i d a t i o n ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

unique data s u b s e t s <- unique ( dat $data subset name ) require ( c a r e t ) f o l d s <- c r e a t e F o l d s ( c ( 1 : length ( unique data s u b s e t s ) ) , k = 1 0 , l i s t = TRUE, r e t u r n T r a i n = FALSE)

t r a i n s u b s e t s <- unique data s u b s e t s [ - f o l d s [ [ f ] ] ] test subsets <- unique data s u b s e t s [ f o l d s [ [ f ] ] ]

#l e t ' s s e l e c t t h e b v a l u e s a s s o c i a t e d w i t h t h e d a t a s u b s e t s i n t h e t r a i n s e t t r a i n data <- dat [ dat $data subset name %i n% t r a i n s u b s e t s , ] #complex c o a n s t r a i n e d model : compute t h e model f o r t h e v a l u e o f b dat . lm . b . complex <- lm ( b ~ X + N + log (X) , data = t r a i n data ) #l e t us s e e how t h e model f o r b v a l u e f i t s t h e raw d a t a for the t e s t set

dat . raw . t e s t <- dat . raw [ dat . raw $data subset name %i n% t e s t s u b s e t s , ]

i f ( data f i l t e r 2 0 0 ) { #i f True -- k e e p o n l y v a l u e s o f K <= 200 dat . raw . t e s t <- dat . raw . t e s t [ dat . raw . t e s t $ t o p i c C o u n t <= 2 0 0 , ] dat . raw . t e s t <- dat . raw . t e s t [ dat . raw . t e s t $ t o p i c C o u n t < 0 . 7 5  dat . raw . t e s t $ documentCount , ]

f i t t e d v a l u e s <- model one param complex ( dat . raw . t e s t $ topicCount , dat . raw . t e s t $ topXX , dat . raw . t e s t $ documentCount )

149

APPENDIX B. CODE LISTINGS
79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 ) grid ( ) dev . o f f ( ) grid ( ) dev . o f f ( ) pd f ( f i g u r e f i l e name ) boxplot ( rmse p e r f o l d , y l a b = "RMSE" , x l a b = " Constr . - complex " ) } i f ( data f i l t e r 2 0 0 ) { summary ( rmse p e r f o l d ) } #compute and s a v e rmse p e r f o l d

B.10. PROGRAM: VALIDATE FIT.R

rmse p e r f o l d <- c ( rmse p e r f o l d , rmse ( dat . raw . t e s t $ p o s t F r a c t i o n , f i t t e d v a l u e s ))

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #### show summary s t a t s ##### # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

f i g u r e f i l e name <- " . / f i g u r e s / one param complex 10- f o l d v a l i d a t i o n DS2 . pdf " rmse . df . ds2 <- data . frame ( f i t e r name = " D a t a s e t 2 " , rmse = rmse p e r f o l d ) } else { f i g u r e f i l e name <- " . / f i g u r e s / one param complex 10- f o l d v a l i d a t i o n DS1 . pdf " rmse . df . ds1 <- data . frame ( f i t e r name = " D a t a s e t 1 " , rmse = rmse p e r f o l d )

#need t o run t h e s c r i p t t w i c e w i t h d a t a f i l t e r 200 = T and d a t a f i l t e r 200 = F t o get the f i g u r e below c o r r e c t l y pd f ( " . / f i g u r e s / one param complex 10- f o l d v a l i d a t i o n DS both . p df " ) rmse . df . ds both <- rbind ( rmse . df . ds1 , rmse . df . ds2 ) plot ( rmse . df . ds both $ f i t e r name , rmse . df . ds both $ rmse , y l a b = "RMSE" , x l a b = " Constr . - complex "

150

References
[1] Ayushi Aggarwal, Gajendra Waghmare, and Ashish Sureka. Mining issue tracking systems using topic models for trend analysis, corpus exploration, and understanding evolution. In Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014, pages 52­58, New York, NY, USA, 2014. ACM. [2] Edoardo M Airoldi, Elena A Erosheva, Stephen E Fienberg, Cyrille Joutard, Tanzy Love, and Suyash Shringarpure. Reconceptualizing the classification of PNAS articles. Proc. Natl. Acad. Sci. U. S. A., 107(49):20899­20904, 7 December 2010. [3] Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to mcmc for machine learning. Machine learning, 50(1-2):5­43, 2003. [4] archive.org. Internet archive. https://archive.org/download/stackexchange. Accessed: 2015-9-NA. [5] Andrea Arcuri and Gordon Fraser. On parameter tuning in search based software engineering. In Search Based Software Engineering, pages 33­47. Springer, Berlin, Heidelberg, 10 September 2011. [6] Sanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models­going beyond svd. In Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on, pages 1­10. IEEE, 2012.
151

REFERENCES

REFERENCES

[7] Rajkumar

Arun,

Vommina

Suresh,

C

E

Veni

Madhavan,

and

M

N

Narasimha Murthy. On finding the natural number of topics with latent dirichlet allocation: Some observations. In Advances in Knowledge Discovery and Data Mining, pages 391­402. Springer, Berlin, Heidelberg, 21 June 2010. [8] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference for topic models. Uncertainty in Articial Intelligence, 2009. [9] Hazeline U Asuncion, Arthur U Asuncion, and Richard N Taylor. Software traceability with topic modeling. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1, pages 95­104. ACM, 1 May 2010. [10] Anton Barua, Stephen W Thomas, and Ahmed E Hassan. What are developers talking about? an analysis of topics and trends in stack overflow. Empir. Softw. Eng., 19(3):619­654, 1 June 2014. [11] James O Berger, Robert L Wolpert, M J Bayarri, M H DeGroot, Bruce M Hill, David A Lane, and Lucien LeCam. The likelihood principle. Lect. Notes Monogr. Ser., 6:iii­199, 1988. [12] Lauren R Biggers, Cecylia Bocovich, Riley Capshaw, Brian P Eddy, Letha H Etzkorn, and Nicholas A Kraft. Configuring latent dirichlet allocation based feature location. Empir. Softw. Eng., 19(3):465­500, 1 June 2014. [13] David Blei, Lawrence Carin, and David Dunson. Probabilistic topic models: A focus on graphical model design and applications to document and image analysis. IEEE Signal Process. Mag., 27(6):55­65, 1 November 2010. [14] David M Blei. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993­1022, 2003.

152

REFERENCES

REFERENCES

[15] David M Blei. Probabilistic models of text and images. PhD thesis, University of California, Berkeley, 2004. [16] David M Blei. Topic models, machine learning summer school. Lecture, 2009. [17] David M Blei. Introduction to probabilistic topic models. Princeton University, 2011. [18] David M Blei. Mixed membership models. Princeton University, 2011. [19] David M Blei and Jonathan Chang. Hierarchical Relational Models For Document Networks. Ann. Appl. Stat., 4(1):124­150, 2010. [20] David M Blei, Sean Gerrish, Chong Wang, Jordan L Boyd-graber, and Jonathan Chang. Reading tea leaves: How humans interpret topic models. In Y Bengio, D Schuurmans, J D Lafferty, C K I Williams, and A Culotta, editors, Advances in Neural Information Processing Systems 22, pages 288­296. Curran Associates, Inc., 2009. [21] David M Blei and Michael I Jordan. Variational inference for dirichlet process mixtures. Bayesian Anal., 1(1):121­143, March 2006. [22] David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine Learning, ICML '06, pages 113­120, New York, NY, USA, 2006. ACM. [23] George Casella and Edward I George. Explaining the gibbs sampler. Am. Stat., 46(3):167­174, 1 August 1992. [24] Aggarwat Charu and Chengxiang Zhai. Dimensionality reduction and topic modeling. In Mining Text Data, pages 140­148. Springer, 2012.

153

REFERENCES

REFERENCES

[25] Kenneth Church and William Gale. Inverse document frequency (IDF): A measure of deviations from poisson. In Susan Armstrong, Kenneth Church, Pierre Isabelle, Sandra Manzi, Evelyne Tzoukermann, and David Yarowsky, editors, Natural Language Processing Using Very Large Corpora, Text, Speech and Language Technology, pages 283­295. Springer Netherlands, 1999. [26] Reidar Conradi and Alf Inge Wang. Empirical methods and studies in software engineering: experiences from ESERNET, volume 2765. Springer, 2003. [27] William M Darling. A theoretical and practical implementation tutorial on topic modeling and gibbs sampling. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 642­ 647. ailab.chonbuk.ac.kr, 2011. [28] Scott Deerwester, Susan Dumais, Thomas Landauer, and George Furnas. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391­407, 1990. [29] Oscar Dieste, Anna Grim, Natalia Juristo, Himanshu Saxena, et al. Quantitative determination of the relationship between internal validity and bias in software engineering experiments: Consequences for systematic literature reviews. In Empirical Software Engineering and Measurement (ESEM), 2011 International Symposium on, pages 285­294. IEEE, 2011. [30] Bogdan Dit, Meghan Revelle, Malcom Gethers, and Denys Poshyvanyk. Feature location in source code: a taxonomy and survey. J. Softw. Evol. and Proc., 25(1):53­ 95, 1 January 2013. [31] Charles Elkan. Text mining and topic models. University of California at San Diego (Lecture notes), pages 1­13, 2014.
154

REFERENCES

REFERENCES

[32] Felix Endres, Christian Plagemann, Cyrill Stachniss, and Wolfram Burgard. Unsupervised discovery of object classes from range data using latent dirichlet allocation. In Robotics: Science and Systems, volume 2, pages 113­120.

pdfs.semanticscholar.org, 2009. [33] Daniel Falush, Matthew Stephens, and Jonathan K Pritchard. Inference of population structure using multilocus genotype data: linked loci and correlated allele frequencies. Genetics, 164(4):1567­1587, August 2003. [34] Emily B. Fox and Carlos Guestrin. Machine learning: Clustering & retrieval on-line course, university of washington. https://coursera.org, 2016. Accessed: 2017-521. [35] Yang Gao, Jianfei Chen, and Jun Zhu. Streaming gibbs sampling for LDA model. 6 January 2016. [36] James E Gentle. Computational Statistics. Springer Science & Business Media, 28 July 2009. [37] Malcom Gethers, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. On integrating orthogonal information retrieval methods to improve traceability recovery. In 2011 27th IEEE International Conference on Software Maintenance (ICSM), pages 133­142. ieeexplore.ieee.org, 2011. [38] Scott Grant and James R Cordy. Estimating the optimal number of latent concepts in source code analysis. In 2010 10th IEEE Working Conference on Source Code Analysis and Manipulation, pages 65­74, 2010. [39] Scott Grant, James R Cordy, and David B Skillicorn. Using heuristics to estimate an appropriate number of latent topics in source code analysis. Science of Computer Programming, pages 1673­1678, 2013.
155

REFERENCES

REFERENCES

[40] Derek Greene, Derek OCallaghan, and P´ adraig Cunningham. How many topics? stability analysis for topic models. In Machine Learning and Knowledge Discovery in Databases, pages 498­513. Springer, Berlin, Heidelberg, 15 September 2014. [41] Thomas L Griffiths and Mark Steyvers. Finding scientific topics. Proc. Natl. Acad. Sci. U. S. A., 101 Suppl 1:5228­5235, 6 April 2004. [42] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Data Mining, Inference, and Prediction. Springer, 2009. [43] John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between gaussian mixture models. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07, volume 4, pages IV­317­IV­320. ieeexplore.ieee.org, April 2007. [44] Matthew D Hoffman, David M Blei, and Perry R Cook. Content-Based musical similarity computation using the hierarchical dirichlet process. In ISMIR, pages 349­354. books.google.com, 2008. [45] Matthew D Hoffman, David M Blei, Chong Wang, and John William Paisley. Stochastic variational inference. J. Mach. Learn. Res., 14(1):1303­1347, 2013. [46] Thomas Hoffman. Probabilistic latent semantic indexing. in Proceedings of the Twenty-Second Annual International SIGIR Conference, 1999. [47] Thomas Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Mach. Learn., 42:177­196, 2001. [48] Kurt Hornik and Bettina Gr¨ un. topicmodels: An R package for fitting topic models. J. Stat. Softw., 40(13):1­30, 2011.

156

REFERENCES

REFERENCES

[49] Ritika Jain, Smita Ghaisas, and Ashish Sureka. SANAYOJAN: A framework for traceability link recovery between use-cases in software requirement specification and regulatory documents. In Proceedings of the 3rd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2014, pages 12­18, New York, NY, USA, 2014. ACM. [50] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning with applications in R. Springer, 2015. [51] Hayato Kobayashi. Perplexity on reduced corpora. In ACL (1), pages 797­806, 2014. [52] Franco Lancia. Word co-occurrence and theory of meaning. Retrieved August, 18:2007, 2005. [53] Kenneth Lange. Singular value decomposition. In Numerical Analysis for Statisticians, Statistics and Computing, pages 129­142. Springer New York, 2010. [54] Lin Liu, Lin Tang, Wen Dong, Shaowen Yao, and Wei Zhou. An overview of topic modeling and its current applications in bioinformatics. Springerplus, 5(1):1608, 20 September 2016. [55] Panagiotis Louridas, Diomidis Spinellis, and Vasileios Vlachos. Power laws in software. ACM Trans. Softw. Eng. Methodol., 18(1):2:1­2:26, October 2008. [56] Hans Peter Luhn. The automatic creation of literature abstracts. IBM Journal of research and development, 2(2):159­165, 1958. [57] Stacy K Lukins. Source Code Retrieval for Bug Localization using Latent Dirichlet Allocation and its relationship to stability of Agilely developed Software. PhD thesis, The University of Alabama in Huntsville, 2009.

157

REFERENCES

REFERENCES

[58] Stacy K Lukins, Nicholas A Kraft, and Letha H Etzkorn. Bug localization using latent dirichlet allocation. Information and Software Technology, 52(9):972­990, 2010. [59] Duc Minh Luu, Ee-Peng Lim, and Freddy Chong Tat Chua. On modeling brand preferences in item adoptions. In ICWSM. pdfs.semanticscholar.org, 2014. [60] Adrian Marcus, Andrey Sergeyev, Vaclav Rajlich, and others. An information retrieval approach to concept location in source code. 2004. Proceedings. 11th , 2004. [61] Qiaozhu Mei, Xuchua Shen, and Chengxiang X Zhai. Automatic labeling of multinomial topic models. Proceedings of the 13th ACM SIGKDD, 2007. [62] Mej Newman. Power laws, pareto distributions and zipf's law. Contemporary

Physics, 46(5):323­351, 1 September 2005. [63] Rocco Oliveto, Malcom Gethers, Denys Poshyvanyk, and Andrea De Lucia. On the equivalence of information retrieval methods for automated traceability link recovery. In 2010 IEEE 18th International Conference on Program Comprehension, pages 68­ 71. ieeexplore.ieee.org, June 2010. [64] Dan Oneata. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncertainty, pages 1­7, 1999. [65] Annibale Panichella, Bogdan Dit, Rocco Oliveto, Massimilano Di Penta, Denys Poshynanyk, and Andrea De Lucia. How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms. In 2013 35th International Conference on Software Engineering (ICSE), 2013. [66] Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic Smyth, and Max Welling. Fast collapsed gibbs sampling for latent dirichlet allocation. In
158

REFERENCES

REFERENCES

Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 569­577. ACM, 24 August 2008. [67] Juan Ramos et al. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on machine learning, Rutgers University, volume 242, pages 133­142, 2003. [68] Philip Resnik and Eric Hardisty. Gibbs sampling for the uninitiated. Technical report, University of Maryland, 2010. [69] Stephen Robertson. Understanding inverse document frequency: on theoretical arguments for IDF. Journal of Documentation, 60(5):503­520, 2004. [70] Per Runeson and Martin H¨ ost. Guidelines for conducting and reporting case study research in software engineering. Empirical software engineering, 14(2):131­164, 2009. [71] Prasanna Sahoo. Probability and Mathemaical Statistics. pages 214­221. University of Louisville, 2015. [72] Gerard Salton and Michael Mcgill. Introduction to Modern Information Retrieval. McGraw-Hill, 1983. [73] Janet Siegmund, Norbert Siegmund, and Sven Apel. Views on internal and external validity in empirical software engineering. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, volume 1, pages 9­19. ieeexplore.ieee.org, May 2015. [74] Vivek Kumar Rangarajan Sridhar. Unsupervised topic modeling for short texts using distributed representations of words. In Proceedings of NAACL-HLT, pages 192­200, 2015.
159

REFERENCES

REFERENCES

[75] stackexchange.com. Android enthusiasts questions and answers. http://android. stackexchange.com. Accessed: 2015-9-NA. [76] stackexchange.com. Database administrators questions and answers. http://dba. stackexchange.com. Accessed: 2015-9-NA. [77] stackexchange.com. Salesforce questions and answers. https://salesforce.

stackexchange.com. Accessed: 2015-9-NA. [78] Mark Steyvers and Tom Griffiths. Handbook of latent semantic analysis, chapter 21: Probabilistic topic models, 2007. [79] Matthew Taddy. On estimation and selection for topic models. In AISTATS, pages 1184­1193, 2012. [80] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet processes. J. Am. Stat. Assoc., 101(476):1566­1581, 1 December 2006. [81] Stephen W Thomas, Hadi Hemmati, Ahmed E Hassan, and Dorothea Blostein. Static test case prioritization using topic models. Empir. Softw. Eng., 19(1):182­ 212, 1 February 2014. [82] Kai Tian, Meghan Revelle, and Denys Poshyvanyk. Using latent dirichlet allocation for automatic categorization of software. In 2009 6th IEEE International Working Conference on Mining Software Repositories, pages 163­166. ieeexplore.ieee.org, May 2009. [83] Michael E Wall, Andreas Rechtsteiner, and Luis M Rocha. Singular value decomposition and principal component analysis. In Daniel P Berrar, Werner Dubitzky, and Martin Granzow, editors, A Practical Approach to Microarray Data Analysis, pages 91­109. Springer US, 2003.
160

REFERENCES

REFERENCES

[84] Hanna M Wallach, David M Mimno, and Andrew McCallum. Rethinking LDA: Why priors matter. In Y Bengio, D Schuurmans, J D Lafferty, C K I Williams, and A Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1973­1981. Curran Associates, Inc., 2009. [85] Biao Wang, Yang Liu, Zelong Liu, and Maozhen Li. Topic selection in latent dirichlet allocation. In 2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), pages 756­760. ieeexplore.ieee.org, August 2014. [86] Shaowei Wang, David Lo, and Lingxiao Jiang. An empirical study on developer interactions in StackOverflow. In Proceedings of the 28th Annual ACM Symposium on Applied Computing, SAC '13, pages 1019­1024, New York, NY, USA, 2013. ACM. [87] Xiaogang Wang, Xiaxu Ma, and Eric Grimson. Unsupervised activity perception by hierarchical bayesian models. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1­8. ieeexplore.ieee.org, June 2007. [88] Yang Wang, Payam Sabzmeydani, and Greg Mori. Semi-Latent dirichlet allocation: A hierarchical model for human action recognition. In Human Motion ­ Understanding, Modeling, Capture and Animation, pages 240­254. Springer, Berlin, Heidelberg, 2007. [89] Xing Wei and W Bruce Croft. LDA-based document models for ad-hoc retrieval. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '06, pages 178­185, New York, NY, USA, 2006. ACM. [90] Ian H Witten, Eibe Frank, Mark A Hall, and Christopher J Pal. Data mining: Practical machine learning tools and techniques. page 371. Morgan Kaufmann, 2016.

161

REFERENCES

REFERENCES

[91] Robert K. Yin. Case study research: Design and methods. Sage publications, 5th edition, 2013. [92] Weizhong Zhao, James J Chen, Roger Perkins, Zhichao Liu, Weigong Ge, Yijun Ding, and Wen Zou. A heuristic approach to determine an appropriate number of topics in topic modeling. BMC Bioinformatics, 16(13):S8, 1 December 2015.

162

Glossary
F word frequency. 3, 54, 56­58, 68 K Refers to the selected number of topics. vi, ix, x, 2­4, 7, 8, 11, 14, 19, 21­23, 26, 28, 30, 31, 33, 35, 36, 42, 43, 46, 54, 56, 58, 61, 66, 82, 83 M Refers to the number of documents in the text corpus, denoted by D, consisting of M documents: D = {d1 , d2 , . . . , dM }. 28 N total number of documents in corpus. 3, 6, 54, 57, 58 X top X topics. 3, 54, 56­58, 68  Hyperparameter of the model, controls the topic distributions per document. 29  Hyperparameter of the model, controls the term distributions. 29  Multinomial distribution on z . 28  Multinomial distribution used to model the topic proportions. 28 d Refers to a document defined as a sequence of N words, described by d = {w1 , w2 , . . . , wN } where wN represents the nth word in the sequence. 28 w Refers to a term or word in the vocabulary (V ). It is the basic unit of discrete data, and wi  V . 11, 28

163

Glossary

Glossary

SE-related text corpuses Any text corpus associated with software, processes, products and projects. 1, 82, 83 perplexity measurement of how well a probability distribution or probability model predicts a sample. 21, 22 polysemy when a word may have more than one meaning (i.e., crane, bank, etc.). 16, 17 synonymy when one word meaning can be expressed by many words (i.e., defect and bug). 17

164

Acronyms
BoW Bag-of-words. 11, 15, 27 CGS Collapsed Gibbs Sampling. 41­43 CSV Comma Separated Value file. 47 CV Cross-Validation. 21­23 DBMS Database Management System. 14 DM Data Mining. 12 DTM Dynamic Topic Models. vi, ix, 19, 30, 31, 34 FLT Feature Location Technique. 25 HDP Hierarchical Dirichlet Process. 21 idf inverse document frequency. 5, 6, 49 IR Information Retrieval. 12, 24, 82 ITS Issue Tracking System. 24 KL Kullback-Leibler divergence. 21, 44

165

Acronyms

Acronyms

LDA Latent Dirichlet Allocation. vi, viii­x, 1, 3, 4, 14, 18, 19, 23­31, 33, 35, 36, 39­42, 46­49, 54, 55, 76, 82, 115 LSI Latent Semantic Analysis. 17, 24 MCMC Markov Chain Monte Carlo. 41 pd probability density. ix, 36, 37, 39 pLSI Probabilistic Latent Semantic Analysis. ix, 17, 18, 29­31 posterior posterior distribution. 41, 43 prior prior probability. 11 Q&A Questions and Answers. 46, 48, 50 RMSE Root-Mean-Square Error. 68, 69, 71 SE Software Engineering. 1, 23, 24, 82 SVD Singular Value Decomposition. 17 SW Software. 1, 2, 23­25 tf term frequency. ix, 5, 6 tf-idf term frequency - inverse document frequency. 5, 6, 12, 17, 48 TMs Topic Models. 2, 3, 5, 16 XML Extensible Markup Language. 47

166


