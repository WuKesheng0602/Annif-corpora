Uncertainty in Risk Modelling
by

Harjas Singh Bachelor of Science, Ryerson, 2017

A thesis presented to Ryerson University

in partial fulfillment of the requirements for the degree of Master of Science in the program of Applied Mathematics

Toronto, Ontario, Canada, 2018 ©Harjas Singh, 2018

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS

I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

Uncertainty in Risk Modelling Master of Science, 2018 Harjas Singh Applied Mathematics Ryerson University

Abstract
In this thesis, we explore the uncertainty issues in risk modelling arising from the different approaches proposed in the literature and currently being used in the industry. The first type of methods that we discuss assume that the returns of the stocks follows a generalized hyperbolic distribution. Data is calibrated by the Expectation-Maximization (EM) algorithm in order to estimate the parameters in the underlying distribution. Once we have the parameters, we estimate the Value at Risk (VaR) and Expected Shortfall (ES) by using Monte Carlo simulations. Furthermore, we calibrate data to different copulas, including the Gauss Copula, the t Copula and the Gumbel Copula for estimation of VaR and ES using these copula structures. The results from both methods are then compared. It can be concluded that uncertainty issues in risk modelling are very significant and can be troublesome as the values of the same risk measure computed using different methods demonstrate great oscillations. iii

Acknowledgements
I would first like to thank my thesis advisor Dr. Niushan Gao of the Department of Mathematics at Ryerson University. The door to Dr. Gao's office was always open whenever I ran into a trouble spot or had a question about my research or writing. Our discussions were always fruitful and constructive feedback was constantly provided. I would also like to acknowledge Dr. Foivos Xanthos of the Department of Mathematics at Ryerson University as the co-supervisor of this thesis. I am gratefully indebted to his support.

I must express my very profound gratitude to my parents and my sister for providing me with unfailing support and continuous encouragement throughout my years of study and through the process of researching and writing this thesis. This accomplishment would not have been possible without them. Thank you.

Finally, I would like to thank God, who has always showered me with His blessings.

Harjas Singh

iv

Dedication
In every age There exist quiet heroes Who, in ther selfless devotion, lay aside their own needs and comforts and consecrate their energies and talents to healing the wounds of a troubled humanity. To these rare and often unhonored souls this thesis is humbly dedicated.

v

Table of Contents
Declaration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii iii iv v ix x xi 1 1 1 3 6 7 8 10 11 11 13 17

List of Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Introduction 1.1 Risk Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 1.1.2 1.2 Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples of Risk Measures . . . . . . . . . . . . . . . . . . . . .

Risk Measure Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2.1 1.2.2 Methods to calculate Risk Measures . . . . . . . . . . . . . . . . . Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Risk modelling via joint distribution 2.1 Algorithmic Setup 2.1.1 2.1.2 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Generalized Hyperbolic Distributions . . . . . . . . . . . . . . . . The Expectation-Maximization Algorithm . . . . . . . . . . . . .

A Modified EM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . vi

2.2.1 2.2.2 2.3

Modified MCECM Algorithm . . . . . . . . . . . . . . . . . . . . A further CM split of , ,  . . . . . . . . . . . . . . . . . . . . .

17 19 21 21 22 23 24 24 24 25 26 26 27 28 30 32 34 34 36 37 39 39 40 41 42

Computational considerations . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 2.3.2 2.3.3 2.3.4 Initial values Computing ci . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(m)

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

Maximizing Q1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . Maximizing Q2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4

Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 2.4.2 EM Estimates of Parameters . . . . . . . . . . . . . . . . . . . . . Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Risk Modelling via Marginal Distributions 3.1 Basic Copula Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 3.1.2 3.1.3 3.2 3.3 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples of Copulas . . . . . . . . . . . . . . . . . . . . . . . . .

Marginal Distributions of stocks . . . . . . . . . . . . . . . . . . . . . . . Empirical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 3.3.2 3.3.3 Gauss Copula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . t Copula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gumbel Copula . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Conclusions and Inferences 4.1 4.2 4.3 Numerical Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . Inferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Future Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Appendices

vii

References

66

viii

List of Tables
2.1 3.1 ES and VaR based on Generalized Hyperbolic Distribution . . . . . . . . Mean and standard deviation of log returns of marginal distributions of stocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 3.3 3.4 3.5 4.1 ES and VaR based on marginal distributions of stocks . . . . . . . . . . . ES and VaR based on Gauss Copula . . . . . . . . . . . . . . . . . . . . ES and VaR based on t Copula . . . . . . . . . . . . . . . . . . . . . . . ES and VaR based on Gumbel Copula . . . . . . . . . . . . . . . . . . . ES and VaR based on various Copulas . . . . . . . . . . . . . . . . . . . 34 34 35 37 38 39 25

ix

List of Figures
1.1 1.2 1.3 3.1 Graph highlighting Value-at-Risk. . . . . . . . . . . . . . . . . . . . . . . Graph highlighting shortcomings of VaR (does not capture tail-risk). . . Graph highlighting Expected Shortfall. . . . . . . . . . . . . . . . . . . . Marginal distributions of individual stocks. . . . . . . . . . . . . . . . . . 4 5 6 33

x

List of Appendices
A Codes for Chapter 2 A.1 EM Codes: Complete . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 EM Codes: W testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 42 50 54 58 58

A.3 EM Codes: W fixed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B Codes for Chapter 3 B.1 Copula Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

xi

Chapter 1 Introduction
According to the Cambridge dictionary, risk is"the possibility of something bad happening." In financial terms, risk is defined as "the potential for financial loss and uncertainty about its extent" [15]. The crucial part for companies and regulators is precisely the extent of risk. Let us first begin with some definitions in order to quantify this risk.

1.1
1.1.1

Risk Measure
Basic Properties

Mathematically speaking, a risk measure is defined as a mapping from a set of random variables to the real numbers. In practice, the random variables are generally representative of portfolio returns. We represent a risk measure associated with a random variable X as (X ). A risk measure  : L  R  {+} has the following properties each with its own financial interpretation [3]: · Normalized, i.e. (0) = 0. From a financial perspective, normalization is equivalent to saying that a portfolio 1

1.1. RISK MEASURE with no holding has zero risk. · Translation invariance, i.e. If a  R and X  L, then (X + a) = (X ) - a. From a financial perspective, translation invariance says that adding a fixed amount of capital to the portfolio will decrease the risk of the portfolio by the same amount. · Monotonicity, i.e. If X1 , X2  L and X1  X2 , then (X2 )  (X1 ). Viewed from the lens of finance, monotonicity has the interpretation that if a portfolio performs better than another one almost surely, then the first portfolio has less risk. · Law invariance under P, i.e. If X1 , X2  L have the same distribution with respect to P, then (X1 ) = (X2 ). Law-invariant risk measures allot the same level of risk to financial portfolios that have an identical distribution with respect to P a priori. In reality, it is the most pervasive type of risk measures that are used. Principally, any risk measure whose computation relies on statistical methods and thus depends on distributions of random variables must be law-invariant. · Sub-additivity, i.e. If Z1 , Z2  L, then (Z1 + Z2 )  (Z1 ) + (Z2 ).

2

1.1. RISK MEASURE This alludes to the principle commonly known as the diversification effect in finance, i.e. a portfolio that are well diversified carry lower risk than their counterparts. The idea is that the bad events for different assets in the portfolio may not happen simultaneously. · Positive homogeneity, i.e. If   0 and Z  L, then (Z ) = (Z ). This implies that the risk scaling is linear, i.e. doubling your financial position doubles risk. · Convexity, i.e. If Z1 , Z2  L and   [0, 1] then (Z1 + (1 - )Z2 )  (Z1 ) + (1 - )(Z2 ). Convexity incorporates both sub-additivity and positive homogeneity and states that a linear combination (with non-negative weights) of two financial positions has less risk than the corresponding linear combination of risks associated with the financial positions. Not all properties are satisfied by all risk measures. In particular, a risk measure  that satisfies monotonicity, sub-additivity, homogeneity, and translation invariance is called a coherent risk measure [3].

1.1.2

Examples of Risk Measures

A commonly used example of a risk measure is Value-at-Risk or VaR. Widely believed to be developed by JP Morgan during the late 1980's [2], VaR estimates the loss of a set of investments, given a certain probability and time interval. Mathematically speaking, 3

1.1. RISK MEASURE

Figure 1.1: Graph highlighting Value-at-Risk.

if   (0, 1) represents the level of confidence that we seek, then VaR (X ) = - inf x  R : FX (x) > 
 =F- X (1 - ),

(1.1) (1.2) (1.3)

= inf {m  R : P(X + m  0)  },

where FX is the cumulative distribution function (cdf) associated with the random
 variable X and F- X is the generalized inverse of the cumulative distribution function

associated with -X. Hence, VaR (X ) is the minimal amount of cash that needs to be raised and added to the position X to make it "acceptable".

VaR has remained the industry standard for Basel regulators till recently, but it is 4

1.1. RISK MEASURE

Figure 1.2: Graph highlighting shortcomings of VaR (does not capture tail-risk).

gradually being phased out in favour of Expected Shortfall (ES), which is another risk measure. Over the next few paragraphs, we discuss shortcomings of VaR and how ES elegantly takes care of those limitations. We begin by observing that VaR is not a coherent risk measure. Indeed, it does not satisfy the sub-additivity property in some scenarios. In addition to this, it does not capture tail-risk, i.e. even if the distribution is heavy-tailed with a lot of large losses beyond the quantile level, the VaR does not change.

To account for these shortcomings, Expected Shortfall was introduced. Expected shortfall at  % level is the expected return on the portfolio in the worst % of cases. Mathematically speaking, if X is the payoff of a portfolio at some future time and 0 <  < 1 then we define the expected shortfall as ES (X ) = - 1 


VaR (X )d
0

(1.4)

Finally, we define the entropic risk measure, which uses the exponential utility func5

1.2. RISK MEASURE CALCULATION

Figure 1.3: Graph highlighting Expected Shortfall.

tion in order to quantify risk. The entropic risk measure with the risk aversion parameter  > 0 is defined as ent (X ) = 1 log E[e-X ] .  (1.5)

Different from Expected Shortfall, the entropic risk measure is convex but not coherent. We refer to [6] to a more comprehensive treatment of risk measures.

1.2

Calculating Risk Measures for Portfolios

There are several approaches to calculate risk measures. We highlight a couple of them in this section. In addition we discuss the data set that we will use to model returns and

6

1.2. RISK MEASURE CALCULATION subsequently estimate corresponding risk of the portfolio.

1.2.1

Methods to calculate Risk Measures

Of the few approaches that can be used to estimate risk measures, two are prominent in existing literature and practice. The first one is a direct approach, involving knowledge of the underlying joint distribution. The other one is an indirect approach, and models dependence using copulas. We discuss them one by one. Direct approach: Modelling Joint Distribution Generally speaking, the marginal distributions (probability density functions) of the constituents of the portfolio are known (or can be safely assumed) a priori and can be calibrated using well known univariate techniques. A simple example is the return distribution underlying the Black-Scholes option pricing model. In the Black-Scholes framework, an asset is assumed to have log returns that are normally distributed [4], thereby allowing pricing of options on the asset. In such a framework, once the return distribution is established beforehand, it is simply a question of calibrating the empirical data set for the unknown parameters, i.e. the mean and the variance. Following this, we can also check if our assumptions about the distribution were valid using, for example, tests of normality [8]. Finally, once we are convinced that the return distribution is indeed as expected, we may plug in the calibrated parameters to obtain the required answer which, in this case, is the option price.

On the other hand, the joint distribution of the portfolio may be sufficiently complex to preclude such an exercise. However, under certain assumptions, we can calibrate the joint distribution of the portfolio and subsequently estimate risk measures. This is the approach that we follow in Chapter 2, where we assume that the joint distribution is a Generalized Hyperbolic Distribution and is subsequently calibrated using the Expecta7

1.2. RISK MEASURE CALCULATION tion Maximization (EM) algorithm.

Once we know the underlying joint distribution, and have calibrated it, we may then begin calculating risk measures. For this, we simulate a return from the distribution n times, where n is a sufficiently large number (in practice, n > 1, 000, 000). These numbers are then stored in a vector, following which Value-at-Risk and Expected Shortfall are calculated using quantiles [7]. Indirect approach: Modelling Dependence via Copulas Sometimes, it may be difficult to directly model the joint distribution of the portfolio. In this case, we rely on an implicit measure of dependence, i.e. the copula associated with that portfolio. Copulas capture the dependence structure of the joint distribution, and using this information we can evaluate risk measures. This is explained in greater detail in Chapter 3.

1.2.2

Data

The data that we use to perform analyses in this thesis consists of weekly returns of four stocks that trade on NASDAQ. These four stocks are Apple Inc. (ticker: AAPL), Ford Motor Company (ticker: F), Suncor Energy Inc. (ticker: SU) and Bank of America (ticker: BAC). The reason these particular stocks were chosen is that they represent four different sectors of the stock market, and thus their returns should, in general, be different over time. A portfolio consisting of these stocks will also be fairly representative of the entire stock market since the sectors are different. The reason weekly returns are chosen is to avoid the issue of volatility clustering that would be present in noisy daily return data.

We use the last 20 years of data (from May 1998 to May 2018), in order to have a sufficient number of weeks to conduct our analyses (1045 data points per stock). Instead 8

1.2. RISK MEASURE CALCULATION of prices, we employ log returns of closing prices to quantify movements allowing us to compare stocks at different price levels. Adjusted prices are used to account for stock splits and dividends. We also assume an equal investment of $10,000 in each of the stocks at inception and track the movement of the portfolio with this as the reference point.

9

Chapter 2 Risk modelling via joint distribution
Generalized hyperbolic distributions are a large class of normal mixture distributions, containing as special and limiting cases many well-known classes of distributions, such as Student's t-distributions, variance-gamma distributions, normal-inverse Gaussian distributions, and hyperbolic distributions. More importantly, compared to normal distributions, generalized hyperbolic distributions generally have semi-heavy tails and can accommodate skewness. Thus it has been very appealing to use them to model various real-life phenomena. Their use in financial modelling started in the 1990s. See, for example, [11, 16]. As for the case of finite Gaussian mixture models, it is conventional to use the Expectation-Maximization Algorithm or its variants to estimate the parameters in generalized hyperbolic distributions. See, for example, [9, 15]. In this chapter, we fit a generalized hyperbolic distribution to return data of four stocks to model their joint distribution. After estimating the parameters using our modified MCECM algorithm, we use the Monte Carlo methods to evaluate various risk measures of a portfolio of these four stocks.

10

2.1. ALGORITHMIC SETUP

2.1
2.1.1

Algorithmic Setup
Generalized Hyperbolic Distributions

Earlier risk models were often based on multivariate normal distributions. In recent decades, more advanced distributions are demanded, among which normal mixture models have become increasingly popular. Pioneering mixture models introduce mixture to variance only, resulting in, e.g., a already very large classes of spherical and elliptical distributions. More generally, one can introduce mixtures to both mean and variance, which produces in particular generalized hyperbolic distributions. We refer to [15] for basic properties of generalized hyperbolic distributions. A ddimensional random vector X is said to follow a generalized hyperbolic distribution if X = (µ + W  ) +
def



W  2 Z,

1

(2.1)

where µ,   Rd ,  is a positive-definite d × d matrix, Z  Nd (0, Id ) follows the standard d-dimensional multivariate normal distribution, and W  N- (, ,  ) follows a generalized inverse Gaussian (GIG) distribution and is independent of Z . Following the convention, we write X  GHd (, , , µ, ,  ). To be specific, W has the following density: - 2  2
1 2   1 2

fW (w) =

2K (  )

w-1 exp -

1  - w , 2w 2

w > 0,

(2.2)

11

2.1. ALGORITHMIC SETUP where K is a modified Bessel function of the second kind with index . In some literature, K is also called modified Bessel function of the third kind. It can be explicitly written for example as follows. For any   R and x > 0, K (x) = 1 x   x2 dt exp - t - 2 2 4t t+1 0  x 1 t-1 exp - t + t-1 dt. = 2 0 2

(2.3)

We refer to [1, 12] for basic facts on Bessel functions. The appearance of K in fW is due to normalization: words, it holds that
  0

fW (w)dw = 1. In other

w-1 exp -
0

1   - w dw = 2 2w 2 

 2

K (  2  2 ) ,

1

1

(2.4)

which follows easily from (2.3) by a change of variable. Thus, by a simple computation using (2.2) and (2.4), one easily sees that


E[ W ] =

- 2  2
1 2  2



 1 2



2K (  )

w+-1 exp -
0
1 1

1  - w dw 2w 2

(2.5) (2.6)

 = 

K+ ( 2  2 ) K ( 2  2 )
1 1

.

By the definition of X , it is clear that X |W is normally distributed with the following density: fX |W (x|w) = 1 (2 ) 2 det(w) 2 1
d 1

exp -

(x - µ - w ) (w)-1 (x - µ - w ) 2

(2.7)

(x - µ) -1 (x - µ) 1  -1  = - - w + (x - µ) -1  . d 1 d exp 2 w 2 (2 ) 2 det() 2 w 2

12

2.1. ALGORITHMIC SETUP Therefore, by (2.4), X has the following density:


fX (x) =
0

fX |W (x|w)fW (w) dw exp((x - µ) -1  ) (2 ) 2 det() 2 × 2K- d
2 d 1

=

·

- 2  2
1



 1

2K ( 2  2 )
1 2

(2.8) ( +  -1  ) 2
- d 2 2 1

 + (x - µ) -1 (x - µ) -1 (x - µ)
-
- d 2 2

 + ( x - µ)

( +  -1  )

Consequently, W |X has the following density: fW |X (w|x) = fX |W (x|w)fW (w) f X ( x)
-
- d 2 2

=

 + (x - µ) -1 (x - µ) 2K- d
2

( +  -1  )
1 2

- d 2 2 1

(2.9)

 + (x - µ) -1 (x - µ)
d

( +  -1  ) 2

× w- 2 -1 exp - and therefore,

 + (x - µ) -1 (x - µ) 1  +  -1  - w , 2 w 2

d W |X = x  N-  - ,  + (x - µ) -1 (x - µ),  +  -1  ) . 2

(2.10)

2.1.2

The Expectation-Maximization Algorithm

A comprehensive treatment in the EM algorithm can be found in [9, 14]. We now use it to fit a multivariate generalized hyperbolic distribution GHd (, , , µ, ,  ) to n observations x = (x1 , . . . , xn )

13

2.1. ALGORITHMIC SETUP that are independently produced from an iid experiment. For this purpose, let Xi be the random outcome at the i-th observation, and write X = (X1 , . . . , Xn ). Let W = (W1 , . . . , Wn ) be the corresponding mixtures. Then Xi  GHd (, , , µ, ,  ), Wi  N- (, ,  ), i = 1, . . . , n.

Moreover, (Xi , Wi ), i = 1, . . . , n, are independent random pairs. In the EM algorithm, we regard (X, W) as the complete information, X as the observable information, and W as the missing information. Put  = (, , , µ, ,  ), the parameter collection in the model. Given an estimate (m) of  at the m-th step, the estimate of  at the (m + 1)-th step is given by the argument in maximizing the following Q-function: Q(|(m) ) = EX,W|x,(m) [ln fX,W (X, W|)]
n

=
i=1 n

EXi ,Wi |xi ,(m) ln fXi ,Wi (Xi , Wi |)
n

(2.11) EXi ,Wi |xi ,(m) ln fWi (Wi |, ,  )
i=1 n

=
i=1 n

EXi ,Wi |xi ,(m) ln fXi |Wi (Xi |Wi , µ, ,  ) + EWi |xi ,(m) ln fXi |Wi (xi |Wi , µ, ,  ) +
i=1 i=1

=

EWi |xi ,(m) ln fWi (Wi |, ,  )

:= Q1 (µ, ,  |(m) ) + Q2 (, ,  |(m) ),

14

2.1. ALGORITHMIC SETUP where the second equality is due to independence of the pairs (Xi , Wi )'s and [9, Proposition 1.1]. The E-step. Here E stands for expectation. This step deals with computing Q. Recall from (2.10) that d Wi |xi , (m)  N- (m) - , (m) +(xi -µ(m) ) ((m) )-1 (xi -µ(m) ),  (m) +( (m) ) ((m) )-1  (m) . 2 Thus by (2.6), we have the following ai
(m)

:=EWi |xi ,(m) Wi ] = (m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) )  (m) + ( (m) ) ((m) )-1  (m) ×
2 1 2

(2.12)

K(m) - d +1 ( ((m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ))( (m) + ( (m) ) ((m) )-1  (m) )) K(m) - d (
2

((m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ))( (m) + ( (m) ) ((m) )-1  (m) ))

bi

(m)

:=EWi |xi ,(m) Wi-1 ] (m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ) =  (m) + ( (m) ) ((m) )-1  (m) ×
2

(2.13)
-1 2

K(m) - d -1 ( ((m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ))( (m) + ( (m) ) ((m) )-1  (m) )) K(m) - d (
2

((m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ))( (m) + ( (m) ) ((m) )-1  (m) ))

Also, put ci The computation of ci
(m) (m)

= EWi |xi ,(m) ln Wi ].

is more sophisticated, and we will deal with it in the next

15

2.1. ALGORITHMIC SETUP section. By (2.7), we have Q1 (µ, ,  |(m) )
n

=
i=1 n

EWi |xi ,(m) ln fXi |Wi (xi |Wi , µ, ,  )
n (m) ai i=1 n -1

 -1  = (xi - µ)   - 2 i=1

-
i=1

(xi - µ) -1 (xi - µ) (m) n bi - ln det() + C, 2 2

where C is a constant independent of µ, ,  . By (2.2), we have Q2 (, ,  |(m) )
n

=
i=1

EWi |xi ,(m) ln fWi (Wi |, ,  )
n (m) ai i=1

1 1  n n = -  ln  +  ln  - n ln 2K ( 2  2 ) - 2 2 2

 - 2

n (m) bi i=1

n

+ ( - 1)
i=1

ci .

(m)

The M-step. This step deals with finding the argument when maximizing Q. Namely, (µ(m+1) , (m+1) ,  (m+1) ) = arg max Q1 (µ, ,  |(m) )
µ,,

((m+1) , (m+1) ,  (m+1) ) = arg max Q2 (, ,  |(m) ).
,,

Monotonicity of Likelihood. Define the likelihood function as follows: l() = log fX (x|). [9, Theorem 2.1] assets that each step in the EM algorithm increases the likelihood: l()  l((m) ) whenever Q(|(m) )  Q((m) |(m) ). (2.14)

16

2.2. A MODIFIED EM ALGORITHM We refer to [10] and [14, Chapters 3 & 4] for convergence properties of the EM algorithm.

2.2
2.2.1

A Modified EM Algorithm
Modified MCECM Algorithm

Many variants of the EM algorithm have been introduced. See, e.g., [14, Chapters 5 & 6]. Since the maximization of Q is split into two components Q1 and Q2 which separate  into two independent components (µ, ,  ) and (, ,  ), one sees that the EM algorithm coincides with the Expectation-Conditional Maximization algorithm (ECM), in which following each E-step the M-step (now called the CM-step) consists of a few consecutive maximizations subject to updated constraints on prescribed subsets of the parameter collection. If one also updates the E-step whenever a conditional maximization in the CM-step is conducted, we come to the MCECM algorithm (It should be alerted that MCECM also refers to Monte Carlo ECM in some literature). In what follows, we describe a modified version of the MCECM algorithm. Suppose we have finished the m-th step and obtain (m) . After updating the E-step of computing Q(|(m) ), we have the option to first maximize Q1 to obtain (µ(m+1,1) , (m+1,1) ,  (m+1,1) ) , and then partially update the known parameters from (m) to (m,0,1) = ((m) , (m) ,  (m) , µ(m+1,1) , (m+1,1) ,  (m+1,1) ). Then we continue to update the E-step, namely, calculate Q(|(m,0,1) ), and then maxi-

17

2.2. A MODIFIED EM ALGORITHM mize Q1 again to partially update the known parameters from (m,0,1) to (m,0,2) = ((m) , (m) ,  (m) , µ(m+1,2) , (m+1,2) ,  (m+1,2) ). Repeating this process, one obtains (m,0,k) = ((m) , (m) ,  (m) , µ(m+1,k) , (m+1,k) ,  (m+1,k) ), Intuitively, (m,0,) := lim (m,0,k) = ((m) , (m) ,  (m) , µ(m+1,) , (m+1,) ,  (m+1,) )
k

k  N.

gives the best fit of the model to the n observations, when ((m) , (m) ,  (m) is fixed, subject to the initial value (µ(m) , (m) ,  (m) ) of (µ, ,  ). Next, we update the E-step, calculating Q(|(m,0,) ), and maximize Q2 (, ,  |(m,0,) ) to update the parameters from (m,0,) to (m,1,) = ((m+1,1) , (m+1,1) ,  (m+1,1) , µ(m+1,) , (m+1,) ,  (m+1,) ). We continue to update the E-step, calculating Q(|(m,1,) ), and maximize Q2 (, ,  |(m,1,) ) to update the parameters from (m,1,) to (m,2,) = ((m+1,2) , (m+1,2) ,  (m+1,2) , µ(m+1,) , (m+1,) ,  (m+1,) ). Repeating the process, we obtain (m,k,) = ((m+1,k) , (m+1,k) ,  (m+1,k) , µ(m+1,) , (m+1,) ,  (m+1,) ), k  N,

18

2.2. A MODIFIED EM ALGORITHM and (m,,) := lim (m,k,) = ((m+1,) , (m+1,) ,  (m+1,) , µ(m+1,) , (m+1,) ,  (m+1,) ).
k

Finally, we complete the (m + 1)-th step by setting (m+1) := (m,,) . In reality, it may be unfeasible or unnecessary to obtain (m,0,) ) and then (m,,) . Instead, one may stop first at (m,0,k1 ) and turn to work on Q2 and stop at (m,k2 ,k1 ) . In practice, k1 = k2 = 1 is used. We will stick to k1 = k2 = 3. Repeatedly using (2.14), one sees that l((m+1) )  l((m) ). It deserves mentioning that, alternatively, one can first work on Q2 and then on the partially updated Q1 . However, we will not take this alternative.

2.2.2

A further CM split of , , 

We will use the MATLAB tool "fmincon" to optimize Q2 . In Section 2.1.2, one sees that Q2 is highly nonlinear and has three variables , ,  . These two factors add a significant level of difficulties, instability and inaccuracy into the optimization task. Indeed, if one assumes the true values of µ, ,  are known (and thus never update them in the EM algorithm), then simulation studies often reveal that the estimates of , ,  produced by the EM algorithms may be wrong, in particular, when the true values of , ,  are somewhat extreme. We include the MATLAB codes for these simulation studies on EM estimates of W , i.e., estimates of , ,  in Appendix A.2. One reason for such failures is that "fmincon" can only find local minimums and has reduced efficiency over highdimensional regions. 19

2.2. A MODIFIED EM ALGORITHM On the other hand, if one assumes the true values of all parameters but  (respectively, ,  ) are known (and thus never update them in the EM algorithm), then simulation studies show that the EM algorithm produces expected estimates of  (respectively, ,  ). We include the MATLAB codes for these simulation studies on EM estimates of W , i.e., estimates of , ,  in Appendix A.3. Due to the above reasons, we modify the CM-step in the maximization of Q2 . Namely, we optimize Q2 (, 
(m,k) (m,k)

|((m+1,k) , 

), where

(m,k)  = ((m+1,k) ,  (m+1,k) , µ(m+1,) , (m+1,) ,  (m+1,) ),

over  to update the parameters to the following: ((m+1,k+1) , (m+1,k) ,  (m+1,k) , µ(m+1,) , (m+1,) ,  (m+1,) ). Then we optimize Q2 (, 
(m,k)

|((m+1,k) , 

(m,k)

), where

(m,k)  = ((m+1,k+1) ,  (m+1,k) , µ(m+1,) , (m+1,) ,  (m+1,) ),

over  to update the parameters to the following: ((m+1,k+1) , (m+1,k+1) ,  (m+1,k) , µ(m+1,) , (m+1,) ,  (m+1,) ) Finally, we optimize Q2 (,  |( (m+1,k) ,  ), where
(m,k)  = ((m+1,k+1) , (m+1,k+1) , µ(m+1,) , (m+1,) ,  (m+1,) ), (m,k) (m,k)

over  to update the parameters to the following: ((m+1,k+1) , (m+1,k+1) ,  (m+1,k+1) , µ(m+1,) , (m+1,) ,  (m+1,) ) 20

2.3. COMPUTATIONAL CONSIDERATIONS

2.3
2.3.1

Computational considerations
Initial values

For X  GHd (, , , µ, ,  ) as defined in (2.1), we have E[X ] = µ + E[W ], V[X ] = V[W ] + E[W ]. In practice, one usually starts with  (0) = 0, which corresponds to the special class of symmetric generalized hyperbolic distributions. Thus by (2.15), we set µ and by (2.16), we set (0) = S, where S is the sample covariance matrix. Now a second application of (2.16) inspires to restrict  E[W ] =  We set (0) =  (0) = 1, and set (0) by solving K(0) +1 (1) = K(0) (1), which by inspection has the solution (0) = -0.5.
1 2 1

(2.15) (2.16)

(0)

1 = n

n

xi ,
i=1

K+1 ( 2  2 ) K ( 2  2 )
1

1

1

= 1.

21

2.3. COMPUTATIONAL CONSIDERATIONS

2.3.2

Computing ci
(m)

(m)

The computation of ci

involves evaluating E[ln W ], where W  N- (, ,  ). We have

at least three methods to do this. The first one is to use Monte Carlo simulation, namely, simulating iid Wi  N- (, ,  ), i = 1, . . . , N , and estimating 1 E[ln W ]  N
N

Wi .
i=1
1

A drawback is that the convergence is quite slow, with rate O(N - 2 ). Alternatively, by (2.5), - 2  2
1 2   1 2



E[ln W ] =

2K (  )  E[W  ] (0). = 

(ln w)w-1 exp -
0

1  - w dw 2w 2

(2.17) (2.18)

Thus one may compute E[ln W ] via differentiation in (2.18) or via integration via improper integration in (2.17). To avoid numerical instability of differentiation, we will use integration, which can be easily done by implementing the integral command in MATLAB. Therefore, since d Wi |xi , (m)  N- (m) - , (m) +(xi -µ(m) ) ((m) )-1 (xi -µ(m) ),  (m) +( (m) ) ((m) )-1  (m) , 2

22

2.3. COMPUTATIONAL CONSIDERATIONS we have
(m) ci

 (m) + ( (m) ) ((m) )-1  (m) = (m)  + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ) × ×
0

(m) - d 2 2

1 2K(m) - d (
2

((m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ))( (m) + ( (m) ) ((m) )-1  (m) ))
(m) - d -1 2



(ln w)w

exp -

(m) + (xi - µ(m) ) ((m) )-1 (xi - µ(m) ) 1 2 w

 (m) + ( (m) ) ((m) )-1  (m) w dw - 2

2.3.3

Maximizing Q1

We refer to [19] for basics on vector and matrix differentiation. Differentiating Q1 we have Q1 = - n-1  + µ Q1 =  Q1 =  -1
n -1 n

-1 (xi - µ)bi ,
i=1 n -1

(m)

 (xi - µ) -  
i=1 n i=1 n

ai , ai
(m)

(m)

1 (xi - µ) -  2 i=1

-

i=1

1 2

n

(xi - µ)(xi - µ) bi
i=1

(m)

+

n . 2

Setting the partial derivatives to 0 and solving the equations, we obtain
(m+1) (m) n i=1 bi (xi (m) 1 n - ·n i=1 bi n n i=1 xi i=1 ai 1 n 1 n



= 1

- x)
n i=1

ai

(m)

,

µ(m+1) = 1 = n

n
n i=1

-

n
(m+1)

,
(m+1)

(xi - µ

)(xi - µ

)

(m) bi

1 - n

n

ai
i=1

(m)

·  (m+1) ( (m+1) )

23

2.4. NUMERICAL RESULTS

2.3.4

Maximizing Q2

In practice, one shall maximize 2Q2 /n to remove the magnitude n in case the data size is very large and unnecessary constants. Namely, we need to find the argument in minimizng the following function: Q2 (, ,  |(m) ) =  ln  -  ln  + 2 ln K (  ) + 
1 2 1 2

n i=1

ai

(m)

n

+

(m) n i=1 bi

n

- 2

(m) n i=1 ci

n

.

We will use the MATLAB tool "fmincon" to solve this nonlinear optimization problem with constraints:   0 and   0. Note that the precise constraints of the parameters , ,  are as follows:   0,  > 0 if  > 0;  > 0,  > 0 if  = 0;  > 0,   0 if  < 0.

2.4

Numerical Results

In this section, we discuss the results from calibrating and simulating Generalized Hyberbolic Distributions with our sample data.

2.4.1

EM Estimates of Parameters

Running the code in Appendix A.1 enables us to calculate the best estimates of the parameters involved in the Generalized Hyperbolic Distribution. We list them here.  = -1.8552  = 2.2326  = 5.1476  10-5

24

2.4. NUMERICAL RESULTS   -3 3.5261  10     2.4177  10-3    =   1.0796  10-3    -3 2.6350  10   -1.0028  10-2     - 3  -3.739  10    µ =  - 3 -1.2105  10    -3 -6.1779  10  2.9068  10-3   6.0378  10-4 =  6.8643  10-4  5.7658  10-4 6.0378  10
-4 -3 -4 -4

6.8643  10

-4 -4 -3 -4

5.7658  10

-4

   

2.1498  10

9.8082  10

6.0630  10

-4   -4   -3

9.8082  10

2.4551  10

6.4051  10

6.0630  10

6.4051  10

2.1930  10

2.4.2

Simulation Results

The parameters are then used in conjunction with Monte Carlo simulations to generate 1,000,000 simulations of the stock return values. Following this, we take the rowsums to evaluate the loss for the portfolio. For this, we assume that we have invested $10,000 in each of the four stocks. The results are listed in Table 2.1 Expected Shortfall $4,793 Value at Risk $4,352

Table 2.1: ES and VaR based on Generalized Hyperbolic Distribution We will come back to these numbers in Chapter 4, where we will discuss results along with conclusions and inferences.

25

Chapter 3 Risk Modelling via Marginal Distributions
If we have a joint distribution function corresponding to a risk factor random vector, we have a dependence structure that is implicit in it. Copulas help us in the isolation of this dependence structure. Dependence and correlation may seem synonymous at first glance, however, a deeper look reveals that linear correlations are inadequate to measure dependence. To this end, we can discuss alternative dependence measures including coefficients of tail dependence using copulas. The latter is of particular interest in the study of financial market crashes, since there may be a high degree of dependence in the tails which represent extreme events.

3.1

Basic Copula Theory

In this section we review the definition of a copula, some basic properties that a copula must satisfy and some examples of copulas. We refer to [18] for a comprehensive account of copulas. Throughout this chapter, all distributions are continuous, unless specified 26

3.1. BASIC COPULA THEORY otherwise.

3.1.1

Definitions

Definition 1. A d-dimensional copula is a function C : [0, 1]d  [0, 1] satisfying the following conditions: (i) C (u1 , . . . , ud ) is increasing in each component ui ; (ii) C (1, . . . , 1, ui , 1, . . . , 1) = ui for all i  {1, . . . , d} and ui  [0, 1]; (iii) For any (a1 , . . . , ad ), (b1 , . . . , bd )  [0, 1]d with ai  bi for all 1  i  d, it holds that
2 2

···
i1 =1

(-1)i1 +
id =1

...+id

C (u1i1 , . . . , udid )  0,

where uj 1 = aj and uj 2 = bj for all j  {1, . . . , d}. As is well-known from distribution theory, copulas can be equivalently defined as follows. Definition 2. A function C : [0, 1]d  [0, 1] is a copula if and only if there exists a random vector U = (U1 , . . . , Ud ) such that Ui is a standard uniform distribution on [0, 1] for each 1  i  d and that C (u1 , . . . , ud ) = P(U1  u1 , . . . , Ud  ud ), (u1 , . . . , ud )  [0, 1]d . We call this random vector U , although not unique, a generating random vector of C . It often plays a useful role in simulations.

27

3.1. BASIC COPULA THEORY

3.1.2

Basic Properties

Next we proceed to discuss some properties of copulas. Before we do that, we observe the following properties for distribution functions. Let F be a continuous distribution function and let F - denote the left continuous quantile of F , i.e., F - (y ) = inf {x : F (x)  y }. Then the following hold: (a) If U is a random variable following standard uniform distribution, then the distribution function of F - (U ) is F ; (b) If X is a random variable with distribution function F , then F (X ) follows the standard uniform distribution. We refer to [6] for detailed discussions of these properties. The primary importance of copulas in isolating dependence structure from marginal distributions is illustrated in the celebrated theorem of Sklar. Theorem 3 (Sklar). Let F be a joint distribution function with marginals F1 , . . . , Fd . Then there exists a unique copula C : [0, 1]d  [0, 1] such that, for all x1 , . . . , xd  R, F (x1 , . . . , xd ) = C (F1 (x1 ), . . . , Fd (xd )). Clearly, by (3.1), for any u1 , . . . , ud  [0, 1],
- - C (u1 , . . . , ud ) = F (F1 (u1 ), . . . , Fd (ud )).

(3.1)

(3.2)

Definition 4. We call C in Theorem 3 the copula of F . If F is the distribution of a random vector (X1 , . . . , Xd ), we also call C the copula of (X1 , . . . , Xd ), or (X1 , . . . , Xd ) an associated random vector of C . 28

3.1. BASIC COPULA THEORY We now make two useful remarks regarding generating and associated random vectors, combining which gives the standard procedures to simulate random vectors with a given copula and given marginals. Remark 5. Let C be a copula with associated random vector (X1 , . . . , Xd ). Let Fi be the distribution of Xi . Note that each Fi (Xi ) is a standard uniform distribution and that P F1 (X1 )  u1 , . . . , Fd (Xd )  ud
- - - - =P F1  F1 (X1 )  F1 (u1 ), . . . , Fd  Fd (Xd )  Fd (ud ) - - =P(X1  F1 (u1 ), . . . , Xd  Fd (ud )) - - =F (F1 (u1 ), . . . , Fd (ud ))

=C (u1 , . . . , ud ). Therefore, (F1 (X1 ), . . . , Fd (Xd )) is a generating random vector of C Remark 6. Let C be a copula with a generating random vector (U1 , . . . , Ud ). Let F1 , . . . , Fd be given marginals. Note that each Fi- (Ui ) has distribution function Fi and
- - P F1 (U1 )  x1 , . . . , Fd (Ud )  xd

=P U1  F1 (x1 ), . . . , Ud  Fd (xd ) =C (F1 (x1 ), . . . , Fd (xd )).
- - (U1 ), . . . , Fd (Ud )) is an associated random vector of C with Therefore, by (3.1), (F1

prescribed marginals Fi 's. We end this subsection with a useful result that allows us to simplify the associated random vector when determining its copula. This result demonstrates again the fact that copulas capture the essential dependence structure in a joint distribution.

29

3.1. BASIC COPULA THEORY Proposition 7. Let (X1 , . . . , Xd ) be a random vector with copula C and let T1 , . . . , Td be strictly increasing functions on R. Then (T1 (X1 ), . . . , Td (Xd )) also has copula C .

3.1.3

Examples of Copulas

We provide a number of examples of copulas in this section and these are subdivided into three categories: fundamental copulas represent a number of important special dependence structures; implicit copulas are extracted from well-known multivariate distributions using Sklar's Theorem, but do not necessarily possess simple closed- form expressions; explicit copulas have simple closed-form expressions and follow general mathematical constructions known to yield copulas. Independence Copula As a first example of fundamental copulas, the independence copula is defined by
d

(u, ., u) =
i=1

ui .

(3.3)

By Sklar's Theorem, it is clear that a random vector has the independence copula if and only if its marginals are independent. Comonotonicity Copula The comonotonicity copula is another basic example of fundamental copulas. It is defined by M (u1 , . . . , ud ) = min{u1 , . . . , ud }. (3.4)

Observe that this copula has a generating random vector (U, . . . , U ), where U follows a standard uniform distribution on [0, 1]. Let X be a random variable and Ti , i = 1, . . . , d, be strictly increasing functions 30

3.1. BASIC COPULA THEORY on R. Then by Proposition 7, the copula of (T1 (X ), . . . , Td (X )) is the same as that of (X, . . . , X ), which in turn is the same as that of (U, . . . , U ), where U = F (X ). Hence, the copula is in fact just the comonotonicity copula. Gauss Copula Gauss copula is one of the most important implicit copulas. By Proposition 7, X  Nd (µ, ) and Y  Nd (0, P ) have the same copula, where P is the corresponding correlation matrix of . Their copula is called a Gauss copula. Specifically, by Remark 5, we have
Ga CP (u1 , . . . , ud )

= P((Y1 )  u1 , . . . , (Xd )  ud ) = P (-1 (u1 ), . . . , -1 (ud ))
-1 (u1 ) -1 (ud )

=
-

···
-

det(2P )- 2 exp

1

1 - y P -1 y dy. 2

Gauss copulas were most popular in risk modelling. But some academics and practitioners attribute the 2007-2008 crisis to its wide use in credit risk modelling and its weakness to capture dependence, and since then its use has been greatly reduced. t Copula In the same way that we can extract a copula from the multivariate normal distribution, we can extract an implicit copula from any other distribution with continuous marginal

31

3.2. MARGINAL DISTRIBUTIONS OF STOCKS dfs. For example, the d-dimensional t copula takes the form
t Cv,P (u1 , . . . , ud ) 1 -1 = tv,P (t-  (u1 ), . . . , t (ud )) -1 (u1 ) -1 (ud )

=
-

···
-

 [( + d)/2] (/2) d/2  d/2 ||1/2

1 1 + (x - µ)T -1 (x - µ) 

-( +d)/2

dx.

t copulas capture both upper and lower tail dependence. Their capture of dependence are stronger than Gauss copula but weaker than the following two copulas each of which only capture one-sided tail dependence for the upper and lower tail dependence. Gumbel Copula The Gumbel copula is a typical explicit copula, which captures upper tail-dependence.
d Gu C (u1 , . . . , ud )
1 

= exp -
i=1

(- ln ui )



,

1   < .

(3.5)

Clayton Copula The last example is the Clayton copula, which is also an explicit copula but captures lower tail-dependence.
-  -1/ Cl C (u1 , . . . , ud ) = (u- , 0 <  < . 1 + · · · + ud - d + 1)

(3.6)

3.2

Marginal Distributions of stocks

We will work with 20 years of data for 4 stocks, which are Apple (AAPL), Bank of America (BAC), Ford (F) and Suncor Energy (SU). The reason for choosing these particular stocks was that they are among the largest in their respective sectors and hail from 32

3.2. MARGINAL DISTRIBUTIONS OF STOCKS

Figure 3.1: Marginal distributions of individual stocks.

different sectors, thereby allowing for some diversification. We assume that the marginal distributions of the stocks are normal. This assumption is chosen for simplicity, but may be relaxed to include any other distribution with no change in the procedure. Under this assumption, we calibrate the data set and obtain the distributions for the four stocks in Figure 3.1. The corresponding values are listed in Table 3.1. Next, we assume that we have invested $10,000 in each of the four stocks. In accordance with Basel regulations, we use the 99th percentile for VaR and 97.5th percentile

33

3.3. EMPIRICAL RESULTS Stock AAPL BAC F SU µ -0.0053 -0.0005 0.0002 -0.0027  0.0585 0.0629 0.0618 0.0500

Table 3.1: Mean and standard deviation of log returns of marginal distributions of stocks onwards for ES. The ES and VaR values based on 1,000,000 simulations are tabulated in Table 3.2.

Stock AAPL BAC F SU Sum

Expected Shortfall $1,312 $1,415 $1,378 $1,076 $5,178

Value at Risk $1,311 $1,310 $1,323 $1,056 $4,999

Table 3.2: ES and VaR based on marginal distributions of stocks

3.3

Empirical Results

For each of the four copulas defined in Section 2.1.3, and for the empirical marginal distributions determined in Section 2.2, we evaluate the results of plugging in the marginal distributions into the copulas.

3.3.1

Gauss Copula

We start with the Gauss Copula. For this, we need a correlation matrix. As suggested in [15], the following is a close approximation to the calibrated correlation matrix: 1 S (Xi , Xj ) = (6/ ) arcsin ij  ij , 2 34

3.3. EMPIRICAL RESULTS where S is the matrix formed by taking the pairwise Spearman's rank correlation coefficients between the stock return vectors. The matrix  then takes the following form based on the empirical data: 1.0000   0.2713 =  0.2843  0.2329  0.2713 0.2843 1.0000 0.4137 0.4137 1.0000 0.2439 0.2650  0.2329   0.2439   0.2650  1.0000

We then simulate the copula by the following steps: Algorithm 2.1 (simulation of Gauss copula). (1) Generate Z  Nd (0, P ); (2) Return U = ((Z1 ), . . . , (Zd )) , where  is the standard normal df. The random
Ga vector U has df CP ;

(3) Apply Remark 6 with the marginal distributions of the stocks as calculated in
Ga . section 3.2 to obtain a random vector with CP

Once we have the copula, and plugged in the marginals, we take the rowsums to evaluate the loss for the portfolio. For this, we assume that we have invested $10,000 in each of the four stocks. The results are listed in Table 3.3 Expected Shortfall $3,388 Value at Risk $3,303

Table 3.3: ES and VaR based on Gauss Copula As expected, both VaR and ES decrease when the stocks are put in a portfolio. The number for Value at Risk is $3,303 and for Expected Shortfall is $3,388, down from $4,999 and $5,178 respectively. 35

3.3. EMPIRICAL RESULTS

3.3.2

t Copula

The next copula that we discuss is the t Copula. For this, we need a correlation matrix and degrees of freedom. We may choose any arbitrary correlation matrix, however, as suggested in [15] the following relation holds:  (Xi , Xj ) = (2/ ) sin-1 ij , where  is the matrix formed by taking the pairwise Kendall's rank relation coefficients between the stock return vectors. It then follows, that our matrix may be estimated by multiplying both sides by /2 and subsequently taking the sine of both sides leading to the following for the purposes of this simulation: 1.0000   0.2866 =  0.3007  0.2477  0.2866 0.3007 1.0000 0.4415 0.4415 1.0000 0.2612 0.2858  0.2477   0.2612   0.2858  1.0000

We also choose  to be 5. The reasoning behind this is that we want to differentiate this from the Gauss Copula (since t Copula converges asymptotically to the Gauss Copula as degrees of freedom increase) and in order to observe the strong tail dependence present in the t Copula by design.

We then simulate the copula by the following steps: Algorithm 2.2 (simulation of t copula). (1) Generate X  td (, 0, P ). (2) Return U = (t (X1 ), . . . , t (Xd )) , where t denotes the df of a standard univariate
t t distribution. The random vector U has df Cv,P .

36

3.3. EMPIRICAL RESULTS (3) Apply Remark 6 with the marginal distributions of the stocks as calculated in
t section 3.2 to obtain a random vector with Cv,P .

Once we have the copula, and plugged in the marginals, we take the rowsums to evaluate the loss for the portfolio. For this, we assume that we have invested $10,000 in each of the four stocks. The results are listed in Table 3.4. Expected Shortfall $3,700 Value at Risk $3,660

Table 3.4: ES and VaR based on t Copula As expected, both VaR and ES decrease when the stocks are put in a portfolio even for the t Copula. The number for Value at Risk is $3,660 and for Expected Shortfall is $3,700, down from $4,999 and $5,178 respectively. Also note that while the Value at Risk increasing by using a t Copula as opposed to a Gauss Copula, the Expected Shortfall actually goes down. This is due to the fact that for the same correlation matrix , the t Distribution has heavier tails than the Normal Distribution.

3.3.3

Gumbel Copula

Another copula that we discuss in greater detail is the Gumbel copula. The interesting feature about this copula is that it captures upper tail-dependence. A Gumbel copula is uniquely characterized by the parameter   1. Calibrating  in the two-dimensional case boils down to a one-to-one mapping between the Kendall's rank correlation coefficient and , however, it is not trivial to calibrate in the d-dimensional case. For the purpose of our simulations, we choose  = 3, and then run the following algorithm to simulate the Gumbel Copula: Algorithm 2.3 (simulation of Gumbel copula).

37

3.3. EMPIRICAL RESULTS ^ , the Laplace-Stieltjes transform (1) First, generate a variate V with df G such that G of G, will be the inverse of the generator  of the required copula. (2) Next, generate independent uniform variates X1 , . . . , Xd . ^ (- ln(X1 )/V ), . . . , G ^ (- ln(Xd )/V where V is a positive stable (3) Return U = (G variate V  St(1/, 1, , 0) , where  = (cos(/(2))) and  > 1. The resultant ^ (t) = exp(-t1/ ) which is what we wanted. df has Laplace transform G (4) Apply Remark 6 with the marginal distributions of the stocks as calculated in
Gu section 3.2 to obtain a random vector with C .

Once we have the copula, and plugged in the marginals, we take the rowsums to evaluate the loss for the portfolio. For this, we assume that we have invested $10,000 in each of the four stocks. The results are listed in Table 3.5. Expected Shortfall $5,613 Value at Risk $5,544

Table 3.5: ES and VaR based on Gumbel Copula Due to high dependence in the upper tails, we have that the Value at Risk and Expected Shortfall increase from $4,999 and $5,178 to $5,544 and $5,613 respectively. This can also partly be explained by the instability of the random number generator for Positive Stable Variates in MATLAB. We will come back to these numbers in Chapter 4, where we will discuss results along with conclusions and inferences.

38

Chapter 4 Conclusions and Inferences
This chapter is dedicated to summarizing and dissecting the results obtained in Chapters 2 and 3, and attempting to explain them using both Mathematics and Finance. We will begin with numerical comparisons, and then proceed to inferences. Finally, we will finish by discussing potential for future research.

4.1

Numerical Comparisons

First, we summarize the results obtained by the various copulas in Table 4.1. Copula Name Gauss Copula t Copula Gumbel Copula Expected Shortfall $3,388 $3,700 $5,613 Value at Risk $3,303 $3,660 $5,544

Table 4.1: ES and VaR based on various Copulas It is evident from the table that both Gauss Copula and t Copula are able to capture the concept of risk reduction due to diversification [13], whereas the Gumbel Copula amplifies upper tail risk. 39

4.2. INFERENCES Next, we compare this with the numbers obtained by using the Generalized Hyperbolic Distribution. Recall that the values for the Value at Risk and Expected Shortfall were $4,352 and $4,793 respectively. While these are higher than those obtained by using the Gauss and t Copulas, they are significantly lower than the values of the Gumbel Copula. It is worth stressing again that this is the most generic version of the Generalized Hyperbolic Distribution, with all parameters being free. Another comparison can be drawn between the values obtained by using the sum of the individual Value at Risk and Expected Shortfall for the various stocks versus the values obtained by putting them in a portfolio first and subsequently applying either Copulas or Generalized Hyperbolic Distributions to the complete portfolio. Recall that the sum of the individual values was $5,178 and $4,999 for the Expected Shortfall and Value at Risk respectively. While the Generalized Hyperbolic Distribution, and the Gauss and t Copulas result in a reduction of risk, the Gumbel Copula results in an increase in risk.

4.2

Inferences

The class of Generalized Hyperbolic Distributions encompasses a wide range of distributions due to the fact that there are six parameters in total. This inherently allows for tweaking and fitting according to real life needs. This customization is useful when a simple explanatory model (for example, the Multivariate Normal Model), may be insufficient to capture the intricacies of the data set.

Copulas are an elegant way of measuring and allowing for dependence in portfolios that may otherwise be difficult to capture. Different copulas have different properties, and we have to find one that suits our needs. Often, the most extreme case (in this case, the Gumbel Copula) can be taken from the point of view of a conservative risk-adverse investor. For other practical purposes (for example, internal and external reporting),

40

4.3. FUTURE RESEARCH other copulas like the Gauss and the t Copulas may be better fits.

4.3

Future Research

The following are questions that can be considered extensions to the work done in this thesis and may be suitable for researching further: 1. Is it worth defining the Generalized Hyperbolic Distribution in its most generic form (using all six parameters), or is there merit to simplifying the problem by fixing one or more parameters, thereby specifying a subset of Generalized Hyperbolic Distributions (several of which are named distributions)? 2. How do these two methods, namely using Copulas or Generalized Hyperbolic Distributions, stack against other known methods of estimation in literature, for example, the Rearrangement Algorithm proposed in [17]? 3. How well do these methods capture risk in real life? This is a question that is quite open-ended and subjective, but having a metric that quantifies and compares the performance of different methods in the existing financial markets can be useful. 4. What other risk measures (for example, the Entropic Risk measure [5]) can be suitable for quantifying financial risk? What properties do these risk measures satisfy when applied to individual stocks? More importantly, what properties still hold (or come into existence) while discussing portfolio risk?

41

Appendix A Codes for Chapter 2
A.1
1

EM algorithm: Calibration and Simulation

% c l e a r v a r s -e x c e p t r e t u r n m a t r i x

2

3

X = returnmatrix ; [ n , d ] = s i z e ( returnmatrix ) ; % c a l c u l a t e dimensions of return matrix

4

5

6

n s i m u l a t i o n s = 1 0 0 0 0 0 0 ; % number o f s i m u l a t i o n s t o run c o n f i d e n c e l e v e l V a R = 0 . 9 9 ; % c o n f i d e n c e l e v e l f o r VaR c o n f i d e n c e l e v e l E S = 0 . 9 7 5 ; % c o n f i d e n c e l e v e l f o r ES d o l l a r v a l u e = 1 0 0 0 0 ; % amount i n v e s t e d i n each s t o c k

7

8

9

10

11

% i n i t i a l values

12

13

gamma = z e r o s ( 1 , d ) ; mu = mean (X) ; 42

14

A.1. EM CODES: COMPLETE Sigma = cov (X) ; psi = 1; chi = 1; lambda = - 0.5; i t e r a t i o n s = 100; klambda = 5 ; kchi = 5; kpsi = 5; kW = 5 ; for iter = 1: iterations % lambda o p t i m i z a t i o n chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix
27

15

16

17

18

19

20

21

22

23

24

25

26

psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ;

28

29

f o r j =1: klambda

30

31

%compute th e d e n s i t y o f W i | x i lambdawx = lambda-d / 2 ;

32

33

34

%compute c c = z e r o s ( n , 1 ) ;% t h i s i s nx1 matrix : E [ l n W] f o r i =1:1:n fun1 = @(w) l o g (w) .  w. ^ ( lambdawx -1) .  exp ( - 0.5  psiwx w) .  exp ( - 0.5  chiwx ( i , 1 ) w.^( - 1) ) ; c ( i , 1 ) = 0 . 5  ( psiwx / chiwx ( i , 1 ) ) ^ ( 0 . 5  lambdawx ) / b e s s e l k ( lambdawx , s q r t ( chiwx ( i , 1 )  psiwx ) )  43

35

36

37

38

A.1. EM CODES: COMPLETE i n t e g r a l ( fun1 , 0 , I n f ) ;
39

c l e a r fun1 ; end

40

41

42

%update lambda , c h i , p s i o p t i o n s = o p t i m o p t i o n s ( ' fmincon ' , ' Algorithm ' , ' sqp ' , ' Display ' , ' o f f ' ) ; fun2 = @( t ) t  l o g ( c h i )- t  l o g ( p s i ) +2 l o g ( b e s s e l k ( t , s q r t ( c h i  p s i ) ) ) -2 t  mean ( c ) ;

43

44

45

A = [ ]; B = [ ]; Aeq = [ ] ; beq = [ ] ; lb = [ ] ; ub = [ ] ; nonlcon = [ ] ; t 0 = lambda ; t = fmincon ( fun2 , t0 , A, B, Aeq , beq , lb , ub , nonlcon , options ) ;

46

47

48

49

50

51

52

53

54

c l e a r fun2 ;

55

56

%p r i n t out t h e r e s u l t step = j ; lambda = t ; end % chi optimization lambdawx = lambda-d / 2 ; 44

57

58

59

60

61

A.1. EM CODES: COMPLETE psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ; f o r j =1: k c h i %work on Q2 and p a r t i a l l y update lambda , c h i , p s i f o r k times
64

62

63

65

%compute th e d e n s i t y o f W i | x i chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix

66

67

68

%compute a , b , c b = s q r t ( psiwx . / chiwx ) .  b e s s e l k ( lambdawx - 1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W^( - 1) ]

69

70

71

72

%update lambda , c h i , p s i o p t i o n s = o p t i m o p t i o n s ( ' fmincon ' , ' Algorithm ' , ' sqp ' , ' Display ' , ' o f f ' ) ; fun2 = @( t ) lambda  l o g ( t ) +2 l o g ( b e s s e l k ( lambda , s q r t ( t  p s i ) ) )+t  mean ( b ) ;

73

74

75

A = - 1; B = 0; Aeq = [ ] ; beq = [ ] ; lb = [ ] ; ub = [ ] ; nonlcon = [ ] ; t0 = chi ; 45

76

77

78

79

80

81

82

A.1. EM CODES: COMPLETE t = fmincon ( fun2 , t0 , A, B, Aeq , beq , lb , ub , nonlcon , o p t i o n s ) ;

83

84

85

%p r i n t out t h e r e s u l t step = j ; chi = t ;

86

87

88

89

end % psi optimization lambdawx = lambda-d / 2 ; chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix

90

91

92

93

f o r j =1: k p s i

94

95

%compute th e d e n s i t y o f W i | x i psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ;

96

97

98

99

%compute a , b , c a = s q r t ( chiwx / psiwx ) .  b e s s e l k ( lambdawx+1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W]

100

101

102

%update lambda , c h i , p s i o p t i o n s = o p t i m o p t i o n s ( ' fmincon ' , ' Algorithm ' , ' sqp ' , ' Display ' , ' o f f ' ) ; fun2 = @( t ) -lambda  l o g ( t ) +2 l o g ( b e s s e l k ( lambda , s q r t ( c h i  t ) ) )+t  mean ( a ) ; 46

103

104

A.1. EM CODES: COMPLETE A = - 1; B = 0; Aeq = [ ] ; beq = [ ] ; lb = [ ] ; ub = [ ] ; nonlcon = [ ] ; t0 = p s i ; t = fmincon ( fun2 , t0 , A, B, Aeq , beq , lb , ub , nonlcon , o p t i o n s ) ;

105

106

107

108

109

110

111

112

113

114

115

%p r i n t out t h e r e s u l t step = j ; psi = t ; end % mu, sigma , gamma o p t i m i z a t i o n f o r i = 1 :kW lambdawx = lambda-d / 2 ; chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ; j; %compute th e d e n s i t y o f W i | x i chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ;

116

117

118

119

120

121

122

123

124

125

126

127

128

129

%compute a , b 47

A.1. EM CODES: COMPLETE a = s q r t ( chiwx / psiwx ) .  b e s s e l k ( lambdawx+1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W]
131

130

b = s q r t ( psiwx . / chiwx ) .  b e s s e l k ( lambdawx - 1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W^( - 1) ]

132

133

%update mu, Sigma , gammas gamma = mean ( b .  ( X-mean (X) ) ) /(1 - mean ( b )  mean ( a ) ) ; mu = mean (X)-mean ( a ) gamma ; Sigma = t r a n s p o s e (X-mu)  ( b .  ( X-mu) ) /n-mean ( a )  t r a n s p o s e ( gamma) gamma ;

134

135

136

137

138

end i f i t e r == i t e r a t i o n s - 1 gamma n 1 = gamma ; mu n 1 = mu; Sigma n 1 = Sigma ; chi n 1 = chi ; psi n 1 = psi ; lambda n 1 = lambda ; end i f i t e r == i t e r a t i o n s gamma n = gamma ; mu n = mu; Sigma n = Sigma ; chi n = chi ; 48

139

140

141

142

143

144

145

146

147

148

149

150

151

A.1. EM CODES: COMPLETE psi n = psi ; lambda n = lambda ; end end % e r r o r sanity checks error gamma = norm ( ( gamma n . / gamma n 1 - 1 ) ) ; e r r o r m u = norm ( ( mu n . / mu n 1 - 1 ) ) ; e r r o r S i g m a = norm ( ( Sigma n . / Sigma n 1 - 1 ) ) ; e r r o r c h i = norm ( ( c h i n . / c h i n 1 - 1 ) ) ; e r r o r p s i = norm ( ( p s i n . / p s i n 1 - 1 ) ) ; e r r o r l a m b d a = norm ( ( lambda n . / lambda n 1 - 1 ) ) ;

152

153

154

155

156

157

158

159

160

161

162

163

164

%s i m u l a t i o n o f f i n a l d i s t r i b u t i o n

165

166

W = g i g r n d ( lambda , p s i , c h i , n s i m u l a t i o n s ) ; % g e n e r a t e W Z = mvnrnd ( z e r o s ( 1 , d ) , Sigma , n s i m u l a t i o n s ) ; % g e n e r a t e Z with d e s i g n e d Sigma Xsim = mu+ Wgamma+s q r t (W) .  Z ; % combine t o g i v e X

167

168

169

170

Xsimsum = sum ( Xsim , 2 ) ; % f i n d rowsums t o c a l c u l a t e p o r t f o l i o losses

171

s o r t e d l o s s e s X s i m = s o r t ( Xsimsum , ' descend ' ) ; % s o r t p o r t f o l i o losses

172

n u m l o s s e s = numel ( s o r t e d l o s s e s X s i m ) ; % count p o r t f o l i o l o s s e s VaR index = f l o o r ((1 - c o n f i d e n c e l e v e l V a R )  n u m l o s s e s ) +1; % C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l be VaR ES index = f l o o r ((1 - c o n f i d e n c e l e v e l E S )  n u m l o s s e s ) +1; % 49

173

174

A.2. EM CODES: W TESTING C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l c o n t r i b u t e t o ES
175

VaRsumXsim = s o r t e d l o s s e s X s i m ( VaR index )  d o l l a r v a l u e ; % Use th e i n d e x t o e x t r a c t VaR from s o r t e d l o s s e s

176

ES1Xsim =( s o r t e d l o s s e s X s i m ( 1 : ES index -1) ) ; % E x t r a c t t he l o s s tail ESsumXsim = mean ( ES1Xsim )  d o l l a r v a l u e ; % a v e r a g e o f l o s s t a i l t o c a l c u l a t e ES

177

A.2
1

EM algorithm: testing on W

%This i s th e EM a l g o r i t h m f o r e s t i m a t i n g lambda , c h i and p s i w h i l e mu, Sigma , gamma a r e f i x e d .

2

3

format l o n g ;

4

5

6

%S i m u l a t i n g X f o r data

7

8

d = 4 ; %number o f s t o c k s n = 1 0 0 0 ; % number o f data p o i n t s % Important : a l a r g e r n a l l o w s t o s e e th e e s t i m a t e d p a r a m e t e r s c l o s e r to true values !

9

10

11

12

gamma = [ 0 , 1 , 4 , - 2 ] ; mu = [ 2 , 1 , - 1 , 0 ] ; A = [2 0 0 1; 0 1 0 0; 0 0 4 1; 1 0 1 4 ] ;

13

14

50

A.2. EM CODES: W TESTING Sigma = A/ n t h r o o t ( d e t (A) , d ) ; %f o r purpose o f comparison , we adopt d et ( Sigma )=1
16

15

chi = 2; psi = 0.5; lambda = - 1; W = g i g r n d ( lambda , p s i , c h i , n ) ; % g e n e r a t e W Z = mvnrnd ( z e r o s ( 1 , d ) , Sigma , n ) ; % g e n e r a t e Z with d e s i g n e d Sigma X = mu+ Wgamma+s q r t (W) .  Z ; % combine t o g i v e X c l e a r W; clear Z;

17

18

19

20

21

22

23

24

25

26

%i n i t i a l v a l u e s

27

28

chi = 1; psi = 1; %s e t t l e i n i t i a l v a l u e o f lambda fun = @( lambda ) b e s s e l k ( lambda +1 ,1)- b e s s e l k ( lambda , 1 )  n t h r o o t ( d e t ( cov (X) ) , d ) ;

29

30

31

32

lambda = f z e r o ( fun , - 0 . 5 ) ; c l e a r fun ;

33

34

35

%Excute th e EM a l g o r i t h m

36

37

38

k = 1 0 0 0 ; % number o f p a r t i a l updates on Q2 51

A.2. EM CODES: W TESTING % Important : a l a r g e k w i l l a l l o w us t o s e e whether t h e algorithm converges !
40

39

41

f o r j =1:k %work on Q2 and p a r t i a l l y update lambda , c h i , p s i f o r k times

42

43

%compute th e d e n s i t y o f W i | x i lambdawx = lambda-d / 2 ; chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ;

44

45

46

47

48

%compute a , b , c a = s q r t ( chiwx / psiwx ) .  b e s s e l k ( lambdawx+1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W] b = s q r t ( psiwx . / chiwx ) .  b e s s e l k ( lambdawx - 1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W^( - 1) ]

49

50

51

c = z e r o s ( n , 1 ) ;% t h i s i s nx1 matrix : E [ l n W] f o r i =1:1:n fun1 = @(w) l o g (w) .  w. ^ ( lambdawx -1) .  exp ( - 0.5  psiwx  w) .  exp ( - 0.5  chiwx ( i , 1 ) w.^( - 1) ) ; c ( i , 1 ) = 0 . 5  ( psiwx / chiwx ( i , 1 ) ) ^ ( 0 . 5  lambdawx ) / b e s s e l k ( lambdawx , s q r t ( chiwx ( i , 1 )  psiwx ) )  i n t e g r a l ( fun1 , 0 , I n f ) ;

52

53

54

55

c l e a r fun1 ; 52

A.2. EM CODES: W TESTING end

56

57

58

%update lambda , c h i , p s i o p t i o n s = o p t i m o p t i o n s ( ' fmincon ' , ' Algorithm ' , ' sqp ' ) ; fun2 = @( t ) t ( 1 )  l o g ( t ( 2 ) )- t ( 1 )  l o g ( t ( 3 ) ) +2 l o g ( b e s s e l k ( t ( 1 ) , s q r t ( t ( 2 )  t ( 3 ) ) ) )+t ( 3 )  mean ( a )+t ( 2 )  mean ( b ) -2 t ( 1 )  mean ( c ) ;

59

60

61

A = [0 , -1 ,0;0 ,0 , -1]; B = [0;0]; Aeq = [ ] ; beq = [ ] ; lb = [ ] ; ub = [ ] ; nonlcon = [ ] ; t 0 = [ lambda , c h i , p s i ] ; t = fmincon ( fun2 , t0 , A, B, Aeq , beq , lb , ub , nonlcon , o p t i o n s ) ; c l e a r fun2 ; clear a ; clear b; clear c ; c l e a r lambdawx ; c l e a r chiwx ; c l e a r psiwx ;

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

%p r i n t out t h e r e s u l t f p r i n t f ( 2 , ' \ n s t e p \n ' ) step = j 53

79

80

A.3. EM CODES: W FIXED lambda = t ( 1 ) chi = t (2) psi = t (3) end

81

82

83

84

85

86

87

%p r i n t t r u e v a l u e s o f p a r a m e t e r s f o r comparison %f p r i n t f ( 2 , ' \ n t r u e v a l u e s \ n ' ) %c h i =3 %p s i =2 %lambda =-4

88

89

90

91

A.3
1

EM algorithm: testing with W as fixed

%This i s th e EM a l g o r i t h m f o r e s t i m a t i n g mu, Sigma , gamma w h i l e lambda , c h i and p s i a r e f i x e d .

2

3

format l o n g

4

5

%S i m u l a t i n g X f o r data

6

7

d = 4 ; %number o f s t o c k s n = 1 0 0 0 0 0 ; % number o f data p o i n t s . % Important : a l a r g e r n a l l o w s t o s e e th e e s t i m a t e d p a r a m e t e r s c l o s e r to true values !

8

9

10

11

gamma = [ 0 , 4 , 0 , 4 ] ;

54

A.3. EM CODES: W FIXED mu = [ 2 , 1 0 , - 1 , 0 ] ; A = [2 0 0 1; 0 1 0 0; 0 0 4 1; 1 0 1 4 ] ; Sigma = A/ n t h r o o t ( d e t (A) , d ) ; %f o r purpose o f comparison , we adopt d et ( Sigma )=1
15

12

13

14

c h i =2; p s i =5; lambda =-3; %I t seems some combination o f v a l u e s o f lambda , c h i , p s i can c a u s e problems f o r th e a l g o r i t h m .

16

17

18

19

W = g i g r n d ( lambda , p s i , c h i , n ) ; % g e n e r a t e W Z = mvnrnd ( z e r o s ( 1 , d ) , Sigma , n ) ; % g e n e r a t e Z with d e s i g n e d Sigma X = mu+ Wgamma+s q r t (W) .  Z ; % combine t o g i v e X

20

21

22

23

24

% i n i t i a l values

25

26

gamma = z e r o s ( 1 , d ) ; mu = mean (X) ; Sigma = cov (X) / n t h r o o t ( de t ( cov (X) ) , d ) ; %s m a l l i n f l u e n c e on i n i t i a l v a l u e s

27

28

29

30

31

%Excute th e EM a l g o r i t h m

32

33

%compute th e d e n s i t y o f W i | x i lambdawx = lambda-d / 2 ;

34

35

55

A.3. EM CODES: W FIXED k = 1 0 0 0 0 ; % number o f p a r t i a l updates on Q1 ; % Important : a l a r g e k w i l l a l l o w us t o s e e whether t h e algorithm converges !
38

36

37

39

f o r j = 1 : k % work on Q 1 and p a r t i a l l y update mu, Sigma , gamma f o r k times

40

f p r i n t f ( 2 , ' \ n s t e p \n ' ) j %compute th e d e n s i t y o f W i | x i chiwx = c h i+sum((( -mu+X) / Sigma ) .  ( -mu+X) , 2 ) ; % t h i s i s nx1 matrix psiwx = p s i+gamma/ Sigma  t r a n s p o s e (gamma) ;

41

42

43

44

45

46

%compute a , b a = s q r t ( chiwx / psiwx ) .  b e s s e l k ( lambdawx+1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W] b = s q r t ( psiwx . / chiwx ) .  b e s s e l k ( lambdawx - 1, s q r t ( chiwx  psiwx ) ) . / b e s s e l k ( lambdawx , s q r t ( chiwx  psiwx ) ) ;% t h i s i s nx1 matrix : E [W^( - 1) ]

47

48

49

50

%update mu, Sigma , gammas gamma = mean ( b .  ( X-mean (X) ) ) /(1 - mean ( b )  mean ( a ) ) mu = mean (X)-mean ( a ) gamma Sigma = t r a n s p o s e (X-mu)  ( b .  ( X-mu) ) /n-mean ( a )  t r a n s p o s e ( gamma) gamma ;

51

52

53

54

Sigma = Sigma / n t h r o o t ( d et ( Sigma ) , d ) 56

A.3. EM CODES: W FIXED

55

56

end

57

58

%p r i n t t r u e v a l u e s o f p a r a m e t e r s f o r comparison f p r i n t f ( 2 , ' \ n t r u e v a l u e s \n ' ) gamma = [ 0 , 4 , 0 , 4 ] mu = [ 2 , 1 0 , - 1 , 0 ] A = [2 0 0 1; 0 1 0 0; 0 0 4 1; 1 0 1 4 ] ; Sigma = A/ n t h r o o t ( d e t (A) , d ) adopt d et ( Sigma )=1s %f o r purpose o f comparison , we

59

60

61

62

63

57

Appendix B Codes for Chapter 3
B.1
1

Code for Marginals and Copulas

close all p r i c e m a t r i x = [AAPL. AdjClose BAC. AdjClose F . AdjClose SU . AdjClose ] ; % l o a d p r i c e matrix

2

3

[ n , d ] = s i z e ( pricematrix ) ; % c a l c u l a t e dimensions of p r i c e matrix

4

r e t u r n m a t r i x = z e r o s ( n - 1,d ) ; % i n i t i a l i z i n g r e t u r n matrix names = { 'AAPL ' , 'BAC ' , 'F ' , 'SU ' } ; % names o f s t o c k s n s i m u l a t i o n s = 1 0 0 0 0 ; % number o f s i m u l a t i o n s t o run c o n f i d e n c e l e v e l V a R = 0 . 9 9 ; % c o n f i d e n c e l e v e l f o r VaR c o n f i d e n c e l e v e l E S = 0 . 9 7 5 ; % c o n f i d e n c e l e v e l f o r ES d o l l a r a m o u n t = 1 0 0 0 0 ; % d o l l a r amount f o r each s t o c k t h e t a = 3 ; % f o r Gumbel Copula

5

6

7

8

9

10

11

12

f o r i = 1 : n-1 % l o o p o v e r time r e t u r n m a t r i x ( i , : ) = -l o g ( p r i c e m a t r i x ( i + 1 , : ) . / p r i c e m a t r i x ( i 58

13

B.1. COPULA CODES , : ) ) ; % calculate losses
14

end meanvec = mean ( r e t u r n m a t r i x )  d o l l a r a m o u n t ; % f i n d mean o f d o l l a r losses stdvec = std ( returnmatrix )  dollaramount ; % f i n d standard deviation of dollar losses

15

16

17

figure counter = 0; % i n i t i a l i z e d counter f o r i = 1 : d % loop over stocks x = [ - 3  s t d v e c ( i )+meanvec ( i ) : 0 . 0 0 1 : 3  s t d v e c ( i )+meanvec ( i ) ] ; % marginals

18

19

20

21

norm = normpdf ( x , meanvec ( i ) , s t d v e c ( i ) ) ; % c a l c u l a t e d e n s i t y c o u n t e r = c o u n t e r +1; % i n c r e a s e c o u n t e r subplot (2 ,2 , counter ) % i n i t i a l i z e subplot p l o t ( x , norm ) % p l o t m a r g i n a l x l a b e l ( ' Returns ' ) ; y l a b e l ( ' Density ' ) ; m y t i t l e T e x t = [ ' Normal D i s t r i b u t i o n p l o t f o r ' , names ( i ) ] ; t i t l e ( mytitleText , ' I n t e r p r e t e r ' , ' tex ' ) ; y = meanvec ( i ) + randn ( n s i m u l a t i o n s , 1 )  s t d v e c ( i ) ; % s i m u l a t e marginals

22

23

24

25

26

27

28

29

30

s o r t e d l o s s e s = s o r t ( y , ' descend ' ) ; % s o r t m a r g i n a l l o s s e s n u m l o s s e s = numel ( s o r t e d l o s s e s ) ; % count m a r g i n a l l o s s e s VaR index = f l o o r ((1 - c o n f i d e n c e l e v e l V a R )  n u m l o s s e s ) +1; % C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l be VaR ES index = f l o o r ((1 - c o n f i d e n c e l e v e l E S )  n u m l o s s e s ) +1; % C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l 59

31

32

33

B.1. COPULA CODES c o n t r i b u t e t o ES
34

VaR( i ) = s o r t e d l o s s e s ( VaR index ) ; % Use t h e i n d e x t o e x t r a c t VaR from s o r t e d l o s s e s

35

ES1 =( s o r t e d l o s s e s ( 1 : ES index -1) ) ; % E x t r a c t t he l o s s t a i l ES( i ) = mean ( ES1 ) ; % a v e r a g e o f l o s s t a i l t o c a l c u l a t e ES end

36

37

38

39

c o r r e l a t i o n m a t r i x Z = c o r r ( r e t u r n m a t r i x , ' type ' , ' Spearman ' ) ; % c o r r e l a t i o n matrix f o r Gauss Copula c o r r e l a t i o n m a t r i x T = s i n ( 0 . 5  p i  c o r r ( r e t u r n m a t r i x , ' type ' , ' Ke ndall ' ) ) ; % c o r r e l a t i o n matrix f o r t Copula gamma = ( c o s ( p i /(2  t h e t a ) ) ) ^ t h e t a ; % gamma v a l u e f o r Gumbel Copula

40

41

42

43

Z = mvnrnd ( z e r o s ( 1 , d ) , c o r r e l a t i o n m a t r i x Z , n s i m u l a t i o n s ) ; % s i m u l a t e m u l t i v a r i a t e random normal f o r Gauss c o p u l a

44

T = mvtrnd ( c o r r e l a t i o n m a t r i x T , 5 , n s i m u l a t i o n s ) ; % s i m u l a t e m u l t i v a r i a t e random t f o r t c o p u l a

45

U = rand ( n s i m u l a t i o n s , 4 ) ; Gu = [ ] ; w h i l e l e n g t h (Gu) < n s i m u l a t i o n s Gurand = random ( ' s t a b l e ' , 1 / t h e t a , 1 , gamma , 0 , 1 ) ; %s i m u l a t e s t a b l e random v a r i a t e s f o r Gumbel c o p u l a

46

47

48

49

i f Gurand > 0 Gu = [ Gu ; Gurand ] ; end end 60

50

51

52

B.1. COPULA CODES

53

54

s i m u l a t e d m a t r i x Z = normcdf (Z) ; % e v a l u a t e normal c u m u l a t i v e d i s t r i b u t i o n f u n c t i o n t o g e n e r a t e Gauss c o p u l a

55

s i mu l atedmatrixT = t c d f (T, 5 ) ; % e v a l u a t e t c u m u l a t i v e d i s t r i b u t i o n f u n c t i o n to generate t copula

56

simulatedmatrixGu = exp( -(( - l o g (U) . /Gu) . ^ ( 1 / t h e t a ) ) ) ; % g e t g e n e r a t i n g v e c t o r f o r Gumbel c o p u l a

57

58

Xz = [ ] ; % i n i t i a l i z e l o s s matrix f o r Gauss c o p u l a Xt = [ ] ; % i n i t i a l i z e l o s s matrix f o r t c o p u l a Xgu = [ ] ; % i n i t i a l i z e l o s s matrix f o r Gumbel c o p u l a for i = 1:d Xz = [ Xz norminv ( s i m u l a t e d m a t r i x Z ( : , i ) , meanvec ( i ) , s t d v e c ( i ) ) ] ; % plug i n m a r g i n a l s t o g e t l o s s e s f o r Gauss c o p u l a

59

60

61

62

63

Xt = [ Xt norminv ( simulatedmatrixT ( : , i ) , meanvec ( i ) , s t d v e c ( i ) ) ] ; % plug i n m a r g i n a l s t o g e t l o s s e s f o r t c o p u l a

64

Xgu = [ Xgu norminv ( simulatedmatrixGu ( : , i ) , meanvec ( i ) , s t d v e c ( i ) ) ] ; % plug i n m a r g i n a l s t o g e t l o s s e s f o r Gumbel c o p u l a

65

end Xzsum = sum ( Xz , 2 ) ; % f i n d rowsums t o c a l c u l a t e p o r t f o l i o l o s s e s f o r Gauss c o p u l a

66

67

Xtsum = sum ( Xt , 2 ) ; % f i n d rowsums t o c a l c u l a t e p o r t f o l i o l o s s e s f o r t copula

68

Xgusum = sum ( Xgu , 2 ) ; % f i n d rowsums t o c a l c u l a t e p o r t f o l i o l o s s e s f o r Gumbel c o p u l a

69

s o r t e d l o s s e s z = s o r t ( Xzsum , ' descend ' ) ; % s o r t p o r t f o l i o l o s s e s f o r Gauss c o p u l a 61

B.1. COPULA CODES s o r t e d l o s s e s t = s o r t ( Xtsum , ' descend ' ) ; % s o r t p o r t f o l i o l o s s e s f o r t copula
71

70

s o r t e d l o s s e s g u = s o r t ( Xgusum , ' descend ' ) ; % s o r t p o r t f o l i o l o s s e s f o r Gumbel c o p u l a

72

n u m l o s s e s = numel ( s o r t e d l o s s e s z ) ; % count p o r t f o l i o l o s s e s VaR index = f l o o r ((1 - c o n f i d e n c e l e v e l V a R )  n u m l o s s e s ) +1; % C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l be VaR ES index = f l o o r ((1 - c o n f i d e n c e l e v e l E S )  n u m l o s s e s ) +1; % C a l c u l a t e th e i n d e x o f t h e s o r t e d l o s s e s t h a t w i l l c o n t r i b u t e t o ES

73

74

75

VaRsumz = s o r t e d l o s s e s z ( VaR index ) ; % Use t he i n d e x t o e x t r a c t VaR from s o r t e d l o s s e s f o r Gauss c o p u l a

76

ES1z =( s o r t e d l o s s e s z ( 1 : ES index -1) ) ; % E x t r a c t t he l o s s t a i l f o r Gauss c o p u l a

77

ESsumz = mean ( ES1z ) ; % a v e r a g e o f l o s s t a i l t o c a l c u l a t e ES f o r Gauss c o p u l a

78

VaRsumt = s o r t e d l o s s e s t ( VaR index ) ; % Use t he i n d e x t o e x t r a c t VaR from s o r t e d l o s s e s f o r t c o p u l a

79

ES1t =( s o r t e d l o s s e s t ( 1 : ES index -1) ) ; % E x t r a c t t he l o s s t a i l f o r t copula

80

ESsumt = mean ( ES1t ) ; % a v e r a g e o f l o s s t a i l t o c a l c u l a t e ES f o r t copula

81

VaRsumgu = s o r t e d l o s s e s g u ( VaR index ) ; % Use t h e i n d e x t o e x t r a c t VaR from s o r t e d l o s s e s f o r Gumbel c o p u l a

82

ES1gu =( s o r t e d l o s s e s g u ( 1 : ES index -1) ) ; % E x t r a c t t he l o s s t a i l f o r Gumbel c o p u l a

83

ESsumgu = mean ( ES1gu ) ; % a v e r a g e o f l o s s t a i l t o c a l c u l a t e ES 62

B.1. COPULA CODES f o r Gumbel c o p u l a

63

References
[1] Abramowitz, M., and Stegun, I. Handbook of Mathematical Functions. Dover Publications INC., 1972. ´ kova ´ , E., and Vala ´ ´ b, K. The history and ideas [2] Adamko, P., Spuchl'a skova behind VaR. Procedia Economics and Finance 24 (2015), 218­24. [3] Artzner, P., Delbaen, F., Eber, J.-M., and Heath, D. Coherent measures of risk. Mathematical Finance 9(3) (1999), 203­228. [4] Black, F., and Scholes, M. The pricing of options and corporate liabilities. The Journal of Political Economy 81(3) (1973), 637­654. ¨ rsten, W., and Rischau, R. Entropic risk measures and [5] Brandtner, M., Ku their comparative statics in portfolio selection: Coherence vs. convexity. European Journal of Operational Research 264(2) (2018), 707­716. ¨ llmer, H., and Schied, A. Stochastic Finance An Introduction in Discrete [6] Fo Time. Walter de Gruyter GmbH & Co. KG, Berlin/New York, 2011. [7] Gaglianone, W. P., Lima, L. R., Linton, O., and Smith, D. R. Evaluating value-at-risk models via quantile regression. Journal of Business & Economic Statistics 29(1) (2011), 150­160.

64

REFERENCES [8] Ghasemi, A., and Zahediasl, S. Normality tests for statistical analysis: A guide for non-statisticians. International Journal of Endocrinology & Metabolism 10(2) (2012), 486­489. [9] Gupta, M., and Chen, Y. Theory and use of the em algorithm. Foundations and Trends in Signal Processing 4(3) (2010), 223­296. [10] Jeff Wu, C. On the convergence properties of the em algorithm. Annals of Statistics 11(1) (1983), 95­103. [11] Keller, U., and Eberlein, E. Hyperbolic distributions in finance. Bernoulli 1 (3) (1995), 281­299. [12] Koreney, B. Bessel Functions and Their Applications. CRC Press, 2002. [13] Lintner, J. Security prices, risk, and maximal gains from diversification. The Journal of Finance 20(4) (1965), 587­615. [14] McLachlan, G., and Krishnan, T. The EM Algorithm and Extensions, 2 ed. Wiley-Interscience, 2008. [15] McNeil, A. J., Frey, R., and Embrechts, P. Quantitative Risk Management. Princeton University Press, 2005. [16] Prause, K. The Generalized Hyperbolic Model: Estimation Financial Derivatives and Risk Measures. PhD thesis, Albert-Ludwigs-Universit¨ at Freiburg i. Br., Germany, 1999. ¨ schendorf, L. Computation of sharp bounds on the [17] Puccetti, G., and Ru distribution of a function of dependent risks. Journal of Computational and Applied Mathematics 236(7) (2012), 1833­1840.

65

REFERENCES [18] Rachev, S., Stoyanov, S., and Fabozzi, F. A Probability Metrics Approach to Financial Risk Measures. Wiley-Blackwell, 2011. [19] Schott, J. Matrix Analyis for Statistics. Wiley, 1996.

66

