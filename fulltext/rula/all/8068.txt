FEELING THE BEAT: AN INVESTIGATION INTO THE NEURAL CORRELATES OF VIBROTACTILE BEAT PERCEPTION

by Sean A. Gilmore, Bachelor of Arts (Hons), Western University 2016 A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Arts in the program of Psychology

Toronto, Ontario, Canada, 2018. © Sean A. Gilmore, 2018

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

Feeling the Beat: An investigation into the neural correlates of vibrotactile beat perception MA Psychology, October 2018 Sean A. Gilmore (MA candidate) Masters of Psychology Ryerson University

The current study investigates our ability to perceive and synchronize movements to the beat of rhythms presented through vibrations to the skin. I compared EEG recordings and tapping accuracy to rhythms that varied in modality: auditory-only, vibrotactile, multimodal (vibrotactile and auditory) and complexity: metronome and simple rhythms. The neural data showed that signals localized to the primary auditory cortex showed a larger spike in power at beat frequencies presentation of auditory compared to vibrotactile rhythms. Tapping ability was found to be lowest in vibrotactile compared to auditory and multimodal rhythms. Auditory only and multimodal rhythms did not show a statistical difference in the neural or tapping data. Tapping variability predicted neural entrainment, such that more variable tapping elicited a more entrained neural signal in primary auditory cortex, and less in pre-motor regions. In conclusion, these results show how the temporal processing of rhythm is superior in auditory modalities.

iii

Acknowledgments First and foremost I want to acknowledge my friends, family and all the people in the SMART lab for their ongoing support over the course of my MA. Specifically, I want to acknowledge Frank Russo for being one of the best supervisors someone could ask for. Throughout my MA, Frank has been the rational mind to all my ideas, ensuring they are grounded in theory and hold the upmost scientific integrity. He is always open for a casual chats about life and has been there supporting me during some emotional hardship. I also want to thank Gabriel Napoli. Over the course of the last two years Gabriel has been such a role model for me. He is always willing to put his work aside in order to teach someone in need, or simply lend an ear and couple of kind words to an anxious graduate student. For all the time you have helped me both emotionally and physically this one is for you Gabe.

iv

Dedication I want to dedicate this to my brother Kevin Gilmore. Over the course of my life Kevin has always been someone I looked up. This became immediately apparent when I skirted my dream of being a musician to join him on the path of empirical sciences. He has been there for late night conversations from a stressed younger brother and persistently showed love to me in some of the times I needed it the most. For all the love and support you have given me over the many years I dedicate this to you Kevin.

v

Table of Contents Lists of figures Introduction Methods Results Discussion Limitations Conclusions References vii 1 17 25 30 35 37 46

vi

List of Figures Figure 1. A complex wave form with periodic fluctuations at various frequencies and amplitudes. Bottom: The same signal as but only the frequency domain shown by applying a Fourier transformation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Figure 2. Example of simple and metronomic rhythms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Figure 3. Differences in neural entrainment in the primary auditory cortex across conditions. . 25 Figure 4. FFT power spectrum across all participants for each condition localized to signals in primary auditory cortex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Figure 5. Neural entrainment in primary auditory cortex between each modality. . . . . . . . . . . 27 Figure 6. Tapping variability across conditions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Figure 7. Tapping variability between rhythm complexity collapsing across modality. . . . . . . 28 Figure 8. Tapping variability between modalities collapsing across rhythm complexity. . . . . . 29 Figure 9. Regression lines of tapping variation predicting neural entrainment. . . . . . . . . . . . . . 30

vii

Feeling the music From modern pop lyrics to the embodied feelings described by jazz musicians, music has often been portrayed as something that is not only heard, but also felt. Although these descriptions are anecdotal and often romanticised, their pervasiveness invites closer consideration. Can music be experienced through the sensation of touch? The current thesis will investigate the idea that music is more than just an auditory experience, but also potentially something that is experienced through our tactile sensory system. Where do we start? Music is a complex stimulus with many elements (beat, tempo, pitch, timbre), and it is the interaction of each of these elements that provides us with the rich experience of music (Lerdahl & Jackendoff, 1983). Although many musicians object to the deconstruction of music into a set of constituent parts, doing so allows us to systematically examine psychological phenomena related to music. Music theorists have argued that temporal properties organize the basic structure of music (Honing, 2012; London, 2012). Expanding on this idea, the study of how humans perceive the beat in music seems like a logical place to start. What is the Beat? Most people who have listened to music have had some kind of experience of hearing the beat. However, the beat is usually confused with the rhythm. The rhythm of a musical piece refers to the temporal organization of musical events ­ i.e., the sequence of events and the spacing between the events (McAuley, 2010). This spacing is commonly referred to as the inter-onset interval (IOI). The beat of a musical piece pertains to the feeling of a pulse occurring at isochronous time intervals across a given rhythm (Cooper & Meyer, 1960). The tempo refers to

1

the rate of the beat and can be quantified in terms of a frequency (or beat rate) value, such as milliseconds (ms), beats per minute (BPM), or Hertz (Hz). The beat is a critical structural element in music as it allows for the perception of regularity and provides an underpinning for melody and harmony. Although the beat is viewed as an inherent component of musical composition, it is useful to point out that the beat is actually a psychological construct and not necessarily a physical property of the music (London, 2012; Palmer & Krumhansl, 1990). This can be shown by the fact that we can still perceive and follow along with the beat during silent portions of music (rests). Our perception of the beat in a rhythm sequence also leads to a perceptual enhancement of musical events that fall on temporal positions of the beat. For example, when listening to a metronome where no physical accents exist, it is common for listeners to perceive accents on the first of every second or every fourth event (referred to as the down beat) (Povel & Okkerman, 1981). This phenomenon has been referred to as subjective accenting (Temperley, 1963). Synchronization to the Beat From an audience clapping in unison at a concert, to a group of friends dancing in a club, to an individual tapping their foot in private, there is no shortage of evidence that humans are good at synchronizing movement to music. In fact, research suggests that humans are one of the few species that is able to readily synchronize motor movement to the timing of music (Marchant et al. 2015; Honing et al. 2012) making it a specialized cognitive process to study. The term for this form of motor synchronization to an external stimulus is called sensory-motor synchronization (SMS) (Repp & Su, 2013). Studies that investigate SMS typically use a finger tapping paradigm where a participant is asked to tap to the perceived beat underlying a rhythm. In these paradigms the dependent

2

measure is the discrepancy between the inter-tap interval (ITI) and the IOI of the beats contained in the rhythm (Repp & Su, 2013). This discrepancy can be referred to as asynchrony or synchronization errors. The average of these discrepancies provides a overall measure of the asynchrony ­ termed mean asynchrony. The standard deviation of the mean asynchrony is also of interest as it provides a measure of variability in tapping. Tapping variability is commonly used as a metric of SMS to an external rhythm (Patel, Iversen & Chen, 2005; Ammirante et al. 2016). What Modulates our Perception and Synchronization to the Beat? Although the perception of the beat is a psychological construct, the salience of it is driven by temporal characteristics of the external rhythm as some rhythms elicit a stronger sense of the beat. Povel & Essens' (1985) theory of temporal pattern perception provides predictions about beat salience. One of the predictions from the theory is that the proportion of equallyspaced events in a rhythm will influence the salience of the beat. Behavioural research corroborates these predictions and has revealed that the temporal encoding of a rhythm significantly improves for rhythms in which there are musical events at consistently spaced intervals (Grube & Griffiths, 2009; Patel, Iversen, Chen, & Repp, 2005; D.-J. Povel & Essens, 1985; D. J. Povel, 1981). The fact that the salience of the beat is predicted by the degree to which the musical events are equally spaced suggests a beat hierarchy. For the purposes of this thesis, rhythms that possess a higher ratio of events in an equally spaced manner - making the beat easier to perceive and synchronize movement to - are referred to as simple rhythms and rhythms that contain few to no events in an equally spaced manner are referred to as complex rhythms.

3

Auditory Advantage Some evidence suggests that SMS is possible using non-auditory modalities. For example, some research has demonstrated that participants are able to synchronize to rhythmic presentations of light flashes (Kolers & Brewster, 1985). Other studies have demonstrated that SMS is possible for rhythms presented through vibrotactile stimulation (Ammirante, Patel, & Russo, 2016). However, there is an abundance of research suggesting that there may be an auditory advantage for SMS to external rhythms (for review see Repp & Su, 2013). For example, it has been shown that compared to visual rhythms (presented via flashes), SMS to auditory rhythms show a significantly lower level of mean asynchrony, less variability and a smaller threshold with regards size of the IOI (Patel et al., 2005, Repp 2003). Moreover, when participants are presented with a visual rhythm alongside an auditory distractor rhythm, tapping synchronization tends to veer towards the beat rate of the auditory stimuli (Repp, 2005). Evolutionary Hypothesis for Beat Synchronization Evolutionary explanations for an auditory SMS advantage emphasize the role of action simulation. A prominent theory proposed by Patel & Iversen (2014), entitled the Action Simulation for Auditory Perception, suggests that motor plans responsible for periodic bodily movement (e.g., locomotion) provide a framework from which we perceive and synchronize to beats. They propose that these motor plans provide an excellent context for beat perception and synchronization, as they exist in a frequency range that is comparable to the beat rate in music (0.5 ­ 4 Hz, see London, 2012). A second postulate of the theory is that beat perception and synchronization evolved from the same evolutionary mechanisms that support vocal learning. Here vocal learning refers to the ability to "produce vocal signals based on auditory experience and sensory feedback" (Patel,

4

2006, p. 101). Evidence for this hypothesis can be seen in previously cited studies that show that non-human primates - not classified as vocal learners - do not possess the ability to track a beat (Marchant et al. 2015, Honing et al., 2012) whereas some songbirds ­ who are vocal learners ­ clearly possess this ability (Patel, Iversen, Bregman, & Schulz, 2009). As vocalization is strictly auditory, the vocal learning hypothesis provides an evolutionary explanation for the auditory advantage. Like SMS, vocal learning requires the integration of auditory input and motor plans. The auditory-motor connectivity that underpins vocal learning could thus be used in support of SMS. In vocal learning the motor plans would be articulatory, whereas in SMS the motor plans would be related to periodic body movement. The seat of these vocal motor and periodic body movement plans is likely premotor cortex and supplementary motor area, respectively (Halsband et al. 1993). In SMS, the periodic activity in supplementary motor area is likely further supported by subcortical connectivity with oscillatory timing neurons in the basal ganglia (Merchant et al., 2015). Neurological coupling of Motor and Auditory cortices Studies investigating the neural correlates of beat perception and sensory-motor synchronization have focused on cortical and sub-cortical regions related to motor timing processes, such as premotor cortex, supplementary motor cortex, prefrontal cortex, basal ganglia and cerebellum (Penhune, Zatorre, & Evans, 1998; Roux, Coulmance, & Riehle, 2003; Schubotz, Friederici, & Yves von Cramon, 2000) and how their activation is coupled with cortices related to auditory perception. In a seminal study by Grahn & Brett (2007), neural activation patterns using functional magnetic resonance imaging (fMRI) were contrasted during passive listening to simple, complex and non- rhythms (rhythms that elicited no sense of a beat). Their findings showed that compared to a resting condition, all listening conditions showed an increase in

5

activation within motor cortices such as the premotor cortex, supplementary motor area, and cerebellum . These results show that cortices responsible for motor planning and coordination are intimately involved in beat perception devoid of explicit motor movement. Further contrast analyses compared activation in the simple rhythm conditions with complex and non- rhythm conditions, on the basis that simple rhythms elicited a perception of the beat based on piloting results. In this contrast they observed specific increases of activation in the simple rhythm condition? in bilateral regions of the basal ganglia, primary motor cortex, and pre-motor cortex. Further functional connectivity analyses have investigated the cortical network related to beat perception. This work has mapped a neuronal network between cortical and sub-cortical regions in motor and auditory cortices from which the strength of connectivity is modulated by perceptual salience of a beat (Grahn & Rowe, 2009). These neuroimaging studies have provided insight into the role of motor cortices in mere perception of the beat, as shown by activation in motor related regions in conditions that were devoid of overt movement. Furthermore, the work has revealed that neural activation patterns are modulated by the salience of the beat, wherein simple rhythms are associated with increased activation in sub-cortical and pre-motor regions. As these regions are associated with motor planning and timing this corroborates the Action simulation for auditory prediction hypothesis proposed by Patel & Iversen, (2014) and places motor planning and timing mechanisms as an important part of beat perception. The previous sections have provided an overview of the neural imaging correlates and theoretical origins of beat perception. Cognitive models for how beat perception is processed has yet to be discussed. The following sections will overview two theoretical frameworks which are debated in the literature. The first being the duration based model, which models beat perception

6

as an information processing based process, and the second being the entrainment model which posits a model based on the phase locking of dynamic endogenous neural oscillations. Model for an Internal Timing Mechanism The perception of a beat in a rhythmic stimulus requires mechanisms which encode the incoming information and output a temporal framework which is used to organize the contents of the rhythm (Povel & Essens, 1984; Large & Jones, 1999). Differences in perceptual salience of the beat between different rhythmic complexities suggest the presence of an internal timing mechanism that tracks periodicity. Further, as some beats are perceived as subjectively accented, this mechanism must possess a framework that structures the rhythmic stimulus in a way in which events that align with the internal timing mechanism are perceptually enhanced. Generally, models of this cognitive process have fallen into two frameworks: (1) duration based timing; and (2) entrainment based timing. The following sections will expand on these two models and contrast the predictions of each model. Duration-based Perspectives One of the most prevalent examples of a duration-based model is scalar expectancy theory (SET) (Gibbons, Church & Meck, 1984). In SET models the temporal information of the rhythm is passed through a 3-step process where the temporal information of the stimulus is passed through a time-keeper stage, memory stage and then a final decision stage (Staddon & Higa, 1999). The time keeper-stage involves an internal pacemaker - which pulses at a fixed rate - and an attentional controlled accumulator. During the time keeper stage, an attention gated switch will open during the start of a to-be-determined interval (rhythmic phrase). Once this switch is opened the accumulator will record the number of pulses from the pacemaker. The switch will then close and the amount of pulses accumulated - during the time between the opening and

7

closing of the switch - will be used to estimate the timing interval and duration information. Importantly, the closing of the switch resets the current model and allows for multiple estimates over the course of a rhythmic stimulus. The estimates from the time-keeper stage are then passed to a memory stage where new estimates are stored in working memory and compared to previous estimates, stored in a longer-term memory. These comparisons provide information for temporal judgments (i.e. shorter, longer intervals). The final stage is the decision stage where all comparisons are taken into account to make a final decision on the temporal structure of the rhythm. Importantly, interval based timing models require a system to assess ongoing positive or negative feedback concerning the accuracy of the model, in order to account for our ability to dynamically perceive different tempi from rhythmic stimuli. Interval based models have been prevalent in the rhythm based literature for some time, and provide a cohesive framework for beat based timing assessments (Gibbon et al., 1984; Meck, 2003). However, a downfall of these models is that they are phasic and thus require a listener to run through each stage before a temporal estimate is acquired. As will be shown, entrainment models are able to provide a model that is able to accommodate real-time processing and dynamic adaptation to rhythmic stimuli. Entrainment Perspectives Foundational to the entrainment perspective is the observation that it is in the nature of many physiological/neurophysiological processes to be rhythmic. Generally speaking, entrainment theories posit that physiological processes dynamically synchronize to external rhythms found in our environment. An example of this being present in the canonical circadian rhythm ­ where hormonal and neurochemical fluctuations entrain to day/night cycles (Oster et al., 2006). In this

8

example, it is the neurons in the suprachiasmatic nucleus that entrain to the day/night cycles to create a trajectory of chemical fluctuations in our body. It is known that populations of neurons are intrinsically oscillatory as they fluctuate between peaks and troughs representing increases and decreases in excitability (Llinás, 2014) . Using signal processing algorithms neuronal oscillatory signals can be decomposed into two components, frequency (rate of oscillation) and phase (position of oscillation at specific time point) (Elliot & Rao, 1985). Variances in frequency and phase allow for neural oscillations to adopt different properties that may aid in the processing of external stimuli. For example, oscillations have been shown to align its phase with speech stimuli as to allow for optimal processing of high-level features of the stimuli (Zoeful & VanRullen, 2015). Thus, neuronal oscillations have a certain amount of variance related to their frequency and phase that allow for dynamic adjustments to accommodate the optimal processing of a stimuli at a given time point. Related to rhythms found in music, entrainment perspectives suggest that beat perception is accounted for by real-time entrainment (synchronization) of neuronal oscillations to the frequency of the beat. In this case, the mechanism for entrainment of musical rhythms are selfsustaining oscillations found in neuronal populations in the auditory cortices observed using magnetoencephalography (MEG), and electroencephalography (EEG) (Fujioka, Trainor, Large, & Ross, 2012; Henry & Obleser, 2012; Nozaradan, 2014). The essential concept underpinning entrainment perspectives of beat perception is that these periodic neuronal oscillations act as a referent from which the musical tones in a rhythm are compared to. In this sense, a comparison is made between relative position of a musical event and the peak of an internal oscillator in order to determine the relative extent of phase alignment (McAuley, 2010). It is suggested that from this information, a perceiver is afforded the perception of the tempo. This affords a more

9

succinct model than the interval model, as there is no need for a clock and memory mechanism, as the entrainment model is a dynamic system that works in real time (McAuley, 2010). In order for the entrainment model to work in real time, researchers have proposed two critical processes: phase correction and period correction (Large & Jones, 1999). Phase correction is the process where neural oscillation dynamically adjusts amplitude peaks to the onsets of musical events. This matching of amplitude peaks creates a perceptual salience for stimulus events that align with peaks. Period correction is the process where the frequency of the peaks is adjusted as to match the prominent time periods demarcated by musical events (McAuley, 2010). To account for perceptions of beats at multiple frequencies, multiple oscillators are needed, each of which map on to a specific frequency of the beat. It has been proposed that this explains our ability to perceive the beat at multiple harmonics of the beat (double and triple timing) (Large & Jones, 1999). The idea of multiple oscillators has been supported by numerous observations of neuronal frequency peaks found at the harmonic levels of the beat frequency as well as the base frequency (Henry, Herrmann, Kunke, & Obleser, 2017; Henry & Obleser, 2012; Large, Herrera, & Velasco, 2015; S. Nozaradan, Peretz, Missal, & Mouraux, 2011; Nozaradan, 2014) Steady-State Evoked Potentials and Entrainment Research investigating neural entrainment utilizes non-invasive magnetoencephalographic (MEG) or electroencephalographic (EEG) measurement devices to record fluctuations in magnetic and electrical fields recorded over the surface of the scalp. The data obtained from these devices takes the form of complex waveforms, which can be decomposed into frequency and phase spectra using Fourier transformations (see Fig. 1). These decompositions allow researchers to investigate the distribution of frequencies contained in the complex waveform,

10

which in turn is used to observe how stimuli can modulate increases at a specific frequency. For example, if a participant is exposed to a rhythm with a beat at 2Hz, an increase in neuronal oscillation at a similar frequency would be expected. If there was an increase in power at said frequency, then it may be concluded that a population of neurons is entraining/synchronizing their rate of fire to the stimulus. Moreover, the phase of the signal can also be used to infer entrainment. To do this the phase is assessed to determine the point in an oscillation at which a specific event occurs ­ say a beat. If the phase synchronized with the stimulus ­ the peaks of the oscillation fall in line with the onset of a beat ­ then it can be said with further confidence that the oscillations were entraining to the stimuli. These stimulus evoked increases have been referred to as steady-state evoked potentials (SSEP).

Fig. 1: Top: A complex wave form with periodic fluctuations at various frequencies and amplitudes. Bottom: The same signal as but only the frequency domain shown by applying a Fourier transformation. It is shown that the waveform above consists of a summation of cosine waves at 10, 20, 30, 40, 50 Hz.

11

Empirical support for entrainment models is shown by studies that have shown increases in the spectral power of EEG that corresponded to the same temporal frequency of beat of the rhythm (Nozaradan et al., 2011; Nozaradan, 2014; Snyder & Large, 2005). Interestingly, this increase in spectral power has been subject to change, dependent on the imagined position of the of the beat (double or triple time) (Takako Fujioka, Fidali, & Ross, 2014). Behavioural improvements also support entrainment models in work that suggests that detection of stimuli perturbations is increased when perturbations are placed at peak phase of recorded EEG signal (Henry & Obleser, 2012). Entrainment studies have also furthered support for auditory-motor coupling previously observed in fMRI research. In a study conducted by Cameron et al. (2012), it was shown that exciting neuronal populations using rapid transcranial magnetic stimulation (TMS) lead to an increase in motor evoked potential when stimulation pulses were presented at beat frequencies. These results show phase locking of neural oscillations at beat frequencies, such that the phase alignment of peaks increased the amplitude of motor evoked potentials. Together this empirical evidence provides a neurological mechanism that accounts for the temporal precision needed to perceive beats. The Vibrations of Sound Sound is generally defined as a physical stimulus that involves a source vibration that is propagated through a medium such as air. However, sound can also be defined as a psychological percept. In this latter respect, sound can be defined as the perception of vibrations, especially distal ones that are not proximal to the body. The conversion of a mechanical signal (vibrations) to an electrical signal (action potentials) is through a process called transduction, which for sound takes place through the displacement of hair cells lining the cochlea. These hair

12

cells are organized in a tonotopic fashion such that different sections of the cochlea are responsible for the transduction of specific frequencies/tones (Holton & Hudspeth, 1983). Moreover, the primary auditory cortex - a region which receives the signal from the cochlea ­ is also organized in a similar fashion (Formisano et al., 2003; Merzenich, Knight, & Roth, 1975). In summary, our perception of sound is created by an external vibration which stimulates sensory organs that code the sounds based on the frequency of oscillations and transmit a coded signal to auditory cortices where further encoding takes place to create a percept of sound. In which case, our music ­ or rather our perception of music - is essentially just complex vibrations stimulating relevant pathways. However, if our perception of music is an emergent property that stems from the encoding of vibratory stimulation by sensory organs, then it is reasonable that if other sensory organs could encode vibratory signals in a similar fashion it may be possible to simulate the percept of sound or music from a non-auditory format. The tactile system may be a potential candidate for the induction of non-auditory music. In this sense, the possibility of inducing the percept of music using the vibro-tactile stimulation of the skin will be explored. Mechanoreceptors found in layers of the dermis and responsible for the transduction of tactile stimulation show similar organization as the tonotopic layout of the cochlea. Four main classes of mechanoreceptors are capable of responding to vibration, each with its own characteristic frequency that drives response. Research has shown that auditory information such as speech (Ammirante, Russo & Good, 2013), or musical timbre (Russo, Ammirante & Fels, 2012) can be processed and identified through the skin (Kaczmarek, Webster, Bach-y-Rita, & Tompkins, 1991), positioning mechanoreceptors as a potential mechanism to process music through non-auditory pathways. Moving beyond mechanisms of transduction and more toward cortical components, researches have proposed that the inferior colliculus ­ a subcortical region

13

that mainly receives input from the cochlea ­ is also related to somatosensory sensation (Gruters & Groh, 2012), placing it as another potential neural mechanism that could process a signal to produce the perception of non-auditory music. Furthermore, fMRI (Kayser, Petkov, Augath, & Logothetis, 2005) and MEG (Levänen, Jousmäki, & Hari, 1998) research has shown that regions of the auditory cortex are responsible for tactile-auditory integration. Considering that primary auditory cortex is involved our perception of the beat, these findings provide a neurophysiological account our perception of a beat through vibro-tactile rhythms. Together, there are sensory organs and cortical mechanisms in place to process a signal that could lead to the percept of music from non-auditory stimuli. To this date however, no research exists on the neuro-electrical responses to music in vibro-tactile modalities. Research on Vibro-tactile Music Recent findings by Russo and colleagues are consistent with the notion that music may be perceived through non-auditory stimuli. Russo et al. (2012) demonstrated that timbre ­ the instrument source of a musical note ­ can be detected on the basis of vibration alone. Ammirante, Russo, Good, & Fels (2013) demonstrated that frequency-matched voices could also be discriminated on the basis of vibration alone. In both studies, vibrotactile stimuli were presented to deaf and hearing participants over the lumbar region using voice coils embedded in a conforming chair. While these striking results may have been based solely on complex tactile perception, they may also have arisen due to auditory perception that was triggered by tactile vibration. In a more recent study, the same group of researchers examined participants' ability to perceive and synchronize motor movements to rhythms presented in auditory, vibro-tactile, and multi-modal conditions Ammirante, Patel, & Russo, (2016). Rhythms varied in complexity

14

(metronome or simple) which manipulated the salience of the beat according to principles provided by D.-J. Povel & Essens, 1985. Again, stimuli were presented through voice coils, and the magnitude of stimulation was also manipulated, whereas in some conditions more voice coils were active than others. Their results showed that for metronomic rhythms, tapping accuracy was comparable between vibro-tactile, auditory and multimodal conditions, however for simple rhythms, tapping accuracy in vibro-tactile conditions fell off. These results show that beat perception and further SMS is obtainable for metronomic vibro-tactile rhythms, however for simple rhythms SMS seems to dissipate. A potential explanation for this may be related to the cortical mechanism in place for beat perception. As described above, there are well established networks between motor and auditory cortices, which appear to underpin SMS to auditory rhythms. Further, there is evidence for the idea that beat perception emerges from simulated rhythmic motor plans. Additionally, cortices related to motor planning are more active for complex rhythms (Chen et al., 2008). Thus, it could be proposed that cortices related to motor planning act as a compensatory mechanism from which aid in our perception of beat when salience is reduced. For vibro-tactile rhythms, it may be the case where these networks are not as well established as ones for auditory rhythms. Whereas auditory rhythms have supportive mechanisms, vibro-tactile rhythms do not. This may however change as reliance on non-auditory stimuli increases. For example research by González-Garrido et al. (2017) has shown that after a period of training the ability to discriminate vibro-tactile tones was paired with an increase in the P-300 event related potentials (ERPs) ­ an ERP commonly related to updating memory processes (Polich, 2007) - was observed for deaf and normal hearing participants. Differences in the locality of the ERPs was observed between deaf and hearing participants as in deaf participants the ERP signal was

15

localized to the right parietal lobe (commonly associated with processing of tactile information). This not only provides evidence for training effect of vibro-tactile stimulation, but also proposes neural plasticity in deaf populations which may facilitate enhanced perceptual abilities for vibrotactile stimuli. The Current Study The current study investigates sensorimotor synchronization and neural entrainment to beats presented in auditory, vibro-tactile, and multimodal conditions. More specifically the aim of this study is to expand on findings from Ammirante et al. (2016) and investigate the neural correlates of beat perception from vibro-tactile rhythms. The study consists of a behavioural component where SMS will be compared across modalities and a neural component involving passive listening while EEG recordings will be obtained. To determine the location of cortical areas involved in beat perception, three regions of interest (ROI) will be examined: pre-motor and supplementary motor area (PMC), primary auditory (A1), and somatosensory (SOMA) cortex. Neural entrainment from each ROI will be computed and compared across rhythmic modalities. All rhythms used in the passive listening and tapping paradigm will be constructed using the frameworks outlined by Povel & Essens (1985). It is expected that findings from Ammirante et al. (2016) will be replicated where vibro-tactile tapping accuracy will be comparable with auditory in metronomic rhythms, but vibro-tactile accuracy will decrease with simple rhythms. Entrainment rates are hypothesized to follow similar trends as tapping data, such that tapping accuracy will be followed by peaks in power at beat frequencies. It is also hypothesized that lower tapping accuracy will predict a localized EEG signal in the pre-motor cortex, corroborating the role of motor cortex in SMS to beat.

16

In summary the aim of this project is to explore the idea that music elements can be perceived through non-auditory mechanisms. Replicating SMS trends from Ammirante et al. (2016) and discovering entrainment to beat frequencies in vibro-tactile rhythms will further provide evidence for the idea that music can be induced through a non-auditory, vibro-tactile medium. Methods Participants Nozaradan et al. 2011 observed a difference in entrainment between two different rhythm structures with a t-value of 2.55 and an estimated effect size of 0.85 (d = 2.8). These parameters were entered into a power analysis in order to determine the minimum sample size that would be required to achieve power of .8. The analysis provided an estimated sample size of 10.85. A total of 14 participants were recruited through SONA, an online platform that facilitates recruitment of undergraduate students and tested, 2 participants were recruited through word of mouth. A total of 5 participants were excluded for childhood brain trauma or equipment malfunctions. This left us with a final sample size of 11 participants (6 females, average age 22.2; age range 18 ­ 32 years of age), which means that the study is slightly underpowered. Participants were incentivized to participate through course credit. All participants reported having normal hearing. Musical ability. As musical abilities have been shown to modulate cortical recruitment in response to music (Chen et al., 2008), a questionnaire was used to determine music training and music experience. The average years of musical training was 2.87, and ranged from 0 ­ 6 years.

17

Stimuli Rhythms. All rhythmic stimuli used in the experiment were generated using the signal processing toolbox developed in MatLab (Matlab_2016b, version 9.1.0.441655). Tones used in rhythms were 20 ms. sinewaves at 196Hz and a linear on/off ramp of 0.05 ms. Rhythms were generated by placing these tones in different temporal sequences in order to elicit a sense of the beat. The pattern of sequences was obtained from the Ammirante et al. 2016 experiment, which is based on Povel & Essens 1985. Patterns were either simple rhythms or metronomic. Both simple and metronomic always contained an event at 1.25Hz, however simple rhythms also contained additional events at quarter, third and half sub-divisions of the beat. Simple trials started off with an 8 beat metronome. This acted as an inducer to target a participant's attention to the beat level of 1.25Hz. Following the inducer, simple rhythms were constructed by concatenating the 15 simple rhythms (Fig. 2) in random order. Metronome trials were 4 beat phrases repeated 17 times (15 + 2 inducer) leading both the simple and metronome trials to contain 60 events at the beat level of 1.25hz.

Fig. 2: Example of simple and metronomic rhythms. Solid vertical line reflect positions of tones, black dots represent positions of the beat. The ellipses reflect the end of a one of the 15 concatenated rhythmic sequence. Image retrieved from Ammirante, Patel, Russo 2016 (p. 5).

18

Rhythm Presentation. Rhythms were presented in three different sensory modalities (audio only, vibrotactile, and bi-modal). For audio-only condition, rhythms were presented using 3M EA-RTONE 3A insert earphones, a piezoelectric earphone which minimizes interference with EEG signal. In vibro-tactile only conditions, rhythms were presented using the Subpac S2, a backpack containing vocal coils that vibrate and stimulate the thoracic region. To control for external noise produced by the Subpac, white noise was presented via piezoelectric headphones at a fixed level. To ensure the white noise was played at an intensity high enough to mask the sound of the vibrations a pilot was carried out asking participants to adjust the signal level of white noise until they could no longer hear the vibrations. The final adjusted level was averaged across participants and used as the masking level for all test participants. To match the intensity of the vibrotactile stimuli with the auditory stimuli, a pilot study was carried out where participants were asked to perceptually match the intensity of vibrotacile and auditory stimuli. Bi-modal rhythms were presented using both EARtone headphones as well as Subpac S2. A pilot study was conducted to ensure the vibrotactile and auditory stimuli were matched for perceptual magnitude. In this study I administered auditory and vibrotactile stimuli in alternating sequence and asked participants to adjust the level of the auditory stimulus to perceptually match the magnitude of the vibrotactile stimulus. The final presentation level from each participant was then averaged giving us a final level for presentation of auditory rhythms Tapping Recording. For the tapping paradigm I asked participants to tap along to rhythms. Participants tapped on a Roland HPD 10 MIDI drum pad which was plugged into a FocusRite audio interface. All output from the drum pad was recorded to a Musical Interface Digital Interface (MIDI) track using ProTools.

19

EEG Recording. A 128-channel electrode (BioSemi ActiveTwo) system with a sampling rate of 512 Hz was used to collected neural recordings. Electrode placement followed the standard 10/20 montage. Additional electrodes were placed on mastoids and used as a reference and outer canthus of each eye and bilateral zygomatic bones in order to monitor eye blinks. EEG data was recorded using Actiview software. Procedure Before participants began the experiment, they filled out questionnaires that gathered demographic and musical training information. Following the completion of the questionnaires, participants began the tapping or passive listening tasks. These tasks were counterbalanced in order to minimize any order effects. Tapping Task. Participants completed a total of 36 trials; 3 modalities (vibro-tactile, audio, bimodal) x 2 rhythm complexity (metronome, simple) x 6 repetitions. Trials were blocked by modality and the order of modality was also counter balanced. Within each block participants were exposed to either simple rhythms or a metronome in a counter balanced order. Participants were instructed to tap along to the beat. The participants were told that they should follow the beat, which does not necessarily correspond to every onset in a rhythm. Before experimental trials began participants were exposed to 1 simple rhythm from each modality. This allowed them to become acquainted with what to expect from each modality as well as to ensure that each participant was indeed tapping to the beat and not tapping out the entire rhythmic pattern. Each simple trial contained a unique concatenation of the 15 simple rhythms so as to avoid familiarity effects. The total time required to complete the tapping task was approximately 45 minutes including instructions.

20

EEG/Passive Listening. Following capping, participants were seated in a sound attenuated chamber and were instructed to minimize the amount of overt movement during recording. Participants then begin the passive listening task. The passive listening task consisted of 36 trials; 3 modalities (vibro-tactile, audio, bi-modal) x 2 rhythm complexity (metronome, simple) x 6 repetitions. The amount of trials selected was based on a minimum requirement of data for independent component analysis (covered in Analysis section). Similar to the tapping task, trials were blocked by modality which was counterbalanced between participants. Within each block participants heard/felt either simple or metronomic rhythms in counterbalanced order. Total time required for the passive listening task was approximately 1 hour, including instructions and capping protocol. Data Analysis Tapping Data. Accuracy of sensorimotor synchronization was determined using a commonly used metric: mean asynchrony and tapping variability (Repp & Su, 2013). Tapping was recorded as a MIDI file, which registers the onset time and velocity of each tap. The MIDI files were subsequently processed using miditoolbox a Matlab based toolbox designed to process MIDI files in Matlab. Asynchrony of tapping was calculated by taking the mean time between time of tap and time of beat. It was expected that participants would require a few measures to internalize a sense of the beat and commence tapping, thus asynchronies were calculated from the start of the tapping to the end of the trial. Asynchrony and standard deviation (variability) of ITIs was calculated using the closest beat onset to the initial tap. Based on methods in Ammirante, Patel, Russo 2016, tapping variability was used as a measurement of SMS.

21

EEG Data. All processing and signal analysis of EEG data was done in EEGLAB (Delorme & Makeig, 2004), a Matlab based toolbox. Third party functions developed used for preprocessing are offered in brackets and can be found online (https://sccn.ucsd.edu/) Preprocessing. Data of each participant was initially rereferenced to the average activity across all channels. An average rereference is preferred here because it keeps data full-ranked which will optimize the segmentation of data into independent components. After referencing, data was then was subjected to a high pass filter at 1Hz to eliminate baseline drift and correct for slow drift which happens when channels are contaminated by perspiration (Winkler et al. 2015). A low pass filter was not needed as the frequencies of interest was below 5Hz. After filtering, bad channels were flagged and eliminated using detection algorithms developed in the clear_rawdata tool (Miyakoshi & Kothe, 2009). Data was then epoched into 30 sec. segments starting 7 second after the onset. Epoching 7 second following the onset allowed for us to 1) eliminate the transient audio-evoked spike in activity elicited by the onset of a sound (Saupe et al. 2009); and 2) account for the time delay in entrainment as research has suggested that SSEPs take a couple cycles to become entrained to the external signal (Regan, 1989). After epoching our data was segmented into 36 distinct epochs (12 each modality). Signals were re-referenced once again to account for the filtration and rejection of data. The re-referenced signals were then subjected to an independent component analysis (ICA) using the runica algorithm initially developed by Delorme & Makeig 2004. Finally, artifactual components were rejected using a machine based method, ADJUST, which rejects components based on artifactual spectral and temporal features (Magnon et al. 2011). Entrainment. Three regions of interest were specified: (1) SOMA = primary somatosensory cortex (BA3, BA2, BA1); (2) A1 = primary auditory cortex (BA41, BA42,

22

BA22); and (3) PMC = premotor cortex and supplementary motor area (BA6). The dipole models were extracted and dipole signals were mapped onto a (x,y,z) Talairach coordinate systems. The signals localized to the spatial coordinates of the regions of interest were then subjected to a fast Fourier transformation (FFT) which extracted the spectral power distribution of the localized dipoles. A hanning window was used to calculate the power spectrum across each epoch. Frequency bins were defined as 0.031Hz wide (512 Hz/214 ). EEG signals were assumed to contain a mix of the oscillatory signals induced from the beat, endogenous EEG activity, and noise related signals from muscle movements and other artifactual features. To isolate the neural signal corresponding to the beat, a noise floor was calculated as the average amplitude of two bins flanking each side of a five bin window surrounding the target frequency. This average was removed from each target frequency bin (after Nozaradan et al., 2011). Using the obtained power distribution from the FFT, neural entrainment values for each trial and region of interest was calculated using the average power across 5 frequency bins centered around the beat frequency (1.25 Hz.). As the focus of this research was to examine how the upper limits of entrainment in each region of interest varied between trials, in the case where there were multiple components selected to a particular region of interest, the component with the highest entrainment value was selected for the specific trial. This resulted in a total of 3(region of interest) x 36 (trials) entrainment values for each participant. Statistical Analysis The study was repeated measure design, therefore there is violation of the assumption of independence of observation - due to the clustering within a participants repeated exposure to the experimental conditions. To measure the amount of dependence in the data, the inter-class

23

correlation coefficient (ICC) and design effect (DEFF) was calculated for each region of interest and the tapping variability. The ICC is a measurement of the homogeneity of variance between participants, which is used to calculate the DFF which is measure of sampling variability due to the study design compared to if the variance expected from a random sample (McCoach & Adelson, 2011). DEFF coefficients of 1 indicate the lack of homogeneity of variance. Coefficients above 2 were marked as containing a large amount of homogeneity of variance. It was shown that A1 (ICC = 0.15, DEFF = 6.28), PMC (ICC = 0.13, DEFF = 5.47), SOMA (ICC = 0.25, DEFF = 9.9) and tapping variability (ICC = 0.55, DEFF = 20.57) all contained a high amount of homogeneity in variance. To account of the homogeneity in variance differences in entrainment and SMS between specific experimental conditions was modeled using a series of multilevel linear mixed effects models, where participants were used as a crossed random factors. This allowed for the intercept of each participant to be included as a random effect. All multilevel models were fitted using the toolbox lme4 (Bates, Mächler, Bolker, & Walker, 2015) developed for R. To examine the differences between conditions a series of orthogonal contrasts were used based on a priori hypotheses. First contrast was between rhythm complexity (metronome and simple) collapsing across modalities. Second and third contrasts examined differences between modalities collapsing across rhythm complexity. These contrasts compared auditory-only with vibrotactile trials and auditory-only with multimodal trials respectively. The last contrast examined differences in rhythm complexity in vibrotactile trials, and contrasted vibrotactile simple with vibrotactile metronome trials. Finally, to examine how tapping variability predicted neural entrainment, three linear regression models were used (1 for each ROI) where neural entrainment values were regressed onto tapping variability.

24

Results Neural Entrainment To determine if the experimental manipulation effected neural entrainment in premotor, primary auditory (Fig. 4) and somatosensory cortex (PMC, A1, SOMA) I modeled the entrainment values for each ROI in a linear mixed effect model using condition as a predictor (Fig 3). Results showed that the mixed-effects model did not explain the variance in entrainment found in A1, c2 (8) = 5.43, p = 0.36; PMC, c2 (8) = 0.9, p = 0.97; or SOMA, c2 (8) = 1.1, p = 0.95 compared to their respective intercept only model.

Fig. 3: Differences in neural entrainment in the primary auditory cortex across conditions. Each bar represents one of the six experimental conditions. Error bars reflect standard error.

As the Chi squared value was largest in the A1 model, differences in entrainment were more thoroughly examined using orthogonal contrasts. The first contrast revealed that entrainment was not different between simple and metronome rhythms, b = -0.0004, t(361) = -0.6, se = 0.0007, p = 0.54. The next contrasts compared entrainment differences between modalities collapsing across rhythm (Fig. 5). These contrasts revealed a significant increase in entrainment in auditory-only conditions compared to the vibrotactile conditions, b = 0.0025, t(361) =2.03, se = 0.0012, p = 0.04, and no difference between multimodal and auditory-only

25

conditions, b = 0.0008, t(361) = 0.85 , se = 0.0009, p = 0.39. The last comparison was between auditory-only and vibrotactile simple rhythms, which did not show a significant difference b = -0.001, t(361) = -0.65, se = 0.0016, p = 0.7.

Fig. 4: FFT power spectrum across all participants for each condition localized to signals in primary auditory cortex. Each plot represents a condition. The x-axis represents frequencies (Hz), and the y-axis is amplitude (µV).

26

Fig 5: Neural entrainment in primary auditory cortex between each modality. The each bar represents the three modalities (auditory-only, multimodal, vibrotactile). Error bars reflect the standard error of the mean.

Tapping Variability Differences in tapping variability between conditions was examined using a linear mixed effects model with tapping variability as the dependent measure and condition as the predictor (Fig. 6). Results showed that the model explained significantly more variance in tapping variability compared to the intercept only model, c2 (8) = 30.7, p > 0.001.

Fig. 6: Tapping variability across conditions. Each bar represents a different condition. Pink, green and blue bars are auditory-only, multimodal and vibrotactile respectively. Error bars reflect standard error of the mean.

27

Orthogonal contrasts revealed that metronome trials were significantly less variable than simple rhythm trials (Fig. 7), b = -0.014, t(367) = -5.44, se = 0.002, p < 0.001. No significant differences between auditory-only and vibrotactile, b = -0.003, t(367) = -0.64, se = 0.005, p = 0.52. Auditory-only and multimodal conditions did not significantly differ, b =-0.005, t(367) = 1.44, se =0.003, p = 0.15 (Fig. 8). Lastly, no differences were observed between auditory and vibrotactile simple rhythms, b = -0.002, t(367) = -0.27, se = 0.006, p = 0.78.

Fig 7: Tapping variability between rhythm complexity collapsing across modality. Each bar represents a different rhythm complexity, the pink bar represents metronome trials and turquoise is simple trials. Error bars represent standard error of the mean.

28

Fig 8: Tapping variability between modalities collapsing across rhythm complexity. Each bar represents a different modality. Pink, green and blue bars represent auditory-only, multimodal and vibrotactile respectively. Error bars represent standard error of the mean.

Tapping Variability and Entrainment I also investigated how neural entrainment predicted tapping variability (Fig. 9). Three separate linear models were used for each ROI and neural entrainment values were regressed onto tapping variability. Tapping variability did not predict entrainment in SOMA, b = -0.003, t(360) = -0.37, se = 0.008, p = 0.71. Tapping variability significantly predicted entrainment in PMC, b = -0.014, t(357) = -1.98, se = 0.007, p = 0.048, such that as variability increased entrainment in PMC decreased. Moreover, tapping variability also significantly predicted entrainment in A1, b = 0.035, t(349) = 3.7, se = 0.009, p < 0.001, showing that as tapping variability increased neural entrainment also increased in A1.

29

Fig. 9: Regression lines of tapping variation predicting neural entrainment. Pink, green and blue lines represent regression lines for primary auditory, pre-motor and somatosensory cortex respectively. Grey shadow surrounding each line reflects standard error.

Discussion The results contribute to our understanding of the neural mechanisms related to beat perception more generally. Specifically, I was able to show how levels locality of neural entrainment is modulated by the sensory modality but not the complexity of the external rhythm. I was also able to show how neural entrainment maps onto SMS. Regarding the effect of rhythmic complexity on neural entrainment, it was shown that the temporal structure of a rhythm does not affect the level to which neurons synchronize to the beat. However, consistent with findings that show an auditory advantage, modality was shown to affect neural entrainment, and it was observed that rhythms presented in the auditory modality elicited higher levels of neural entrainment then vibrotactile. Entrainment levels for multimodal rhythms were shown to be comparable to levels observed in the auditory-only condition.

30

The data suggests that these reported effects of modality and complexity on neural entrainment are only observed in the primary auditory cortex. I did not observe a significant amount of variance between conditions in the PMC which seems to contradict our predictions regarding involvement of the motor system in beat perception. The full implications of this result will be discussed in the following section. Results showed how SMS is moderated by modality and temporal changes in rhythm. It was observed that metronomic rhythms are easier to synchronize motor movement to compared to simple rhythms. This could be explained by the simplicity of the metronomic rhythms compared to the simple rhythms. As for the effect of modality, there was no statistically significant differences between auditory and vibrotactile rhythms. This finding suggests that SMS is aided when adding vibrotactile information to auditory stimuli, as shown in the lowest tapping variability in our multimodal condition. Mapping SMS onto neural entrainment levels the results of a regression analysis showed a positive relationship between neural entrainment in primary auditory cortex and tapping variability. This suggests that as our ability to synchronize motor movement to a periodic stimulus decreases, neuronal populations in the primary auditory cortex become more phase locked with the stimulus. A relationship between tapping variability and neural entrainment in pre-motor cortex was also observed, albeit opposite to the trend found in primary auditory cortex. Here, I observed that as tapping variability increases, neural entrainment in pre-motor cortex decreases. This suggests that as SMS worsens, neurons in pre-motor cortex become less phase locked with the stimulus. Overall, these findings contribute to our understanding of the neural mechanism related to beat perception. It also provides a better understanding of the potential of multimodal

31

enhancement, wherein temporal processing of auditory information may be aided by the presence of tactile stimuli. The following section will be used to unpack these findings and discuses some of the implication of the results. A comment on the future research on nonauditory beat perception will also be discussed. Trends in the Locality of Neural Entrainment Beta Band Desynchronization. Based on previous literature (Grahn & Brett, 2007; Chen et al., 2008) we expected to observe differences of neural entrainment localized to the PMC. Our results however showed that neural entrainment in the PMC was not influenced by modality or rhythm complexity. An explanation of this finding however may be related to the type of analysis conducted. Work conducted by Fujioka et al. (2010; 2012; 2014) has investigated how beat perception modulates beta band (~20Hz) frequencies. This work has shown that beta band modulations are localized to auditory and motor regions, including the supplementary motor area (SMA), which is implicated in motor planning (Cunningham et al., 2003). More specifically, the work of Fujioka has shown that desynchronies in power of beta band frequencies are predicted by the onset of beat. This type of analysis deviates from the frequency tagging method posited by Nozaradan et al., (2011) and may more accurately examine how SMA processes the temporal information of rhythm. In this regard it may be the case that SMA does not encode temporal periodicities through the entrainment of oscillators tuned to beat frequency. Instead, this work suggests that the power of oscillations in the beta band entrain the rate of desynchronization to the beat frequency. Using similar analyses to examine the differences in beta desynchronization may provide us with different results. It is likely that the trends in neural entrainment in A1 as predicted by

32

modality and complexity would also predict differences in beta band desynchronization in PMC. In future research, it would be worthwhile adding these analyses to the pipeline. Tapping variability and Primary Auditory Cortex. Prior research has implicated A1 as an important region to temporal processing in rhythmic auditory stimuli (Grahn & Brett 2007; Chen et al. 2008; Fujioka et al., 2014). However, the impact of rhythmic complexity on entrainment in this region is less clear. Theories of predicative coding (Vuust & Witek, 2014) suggest that a completely isochronous rhythm will contain the least amount of uncertainty, therefore requiring the least amount of effort for anticipation. As a result, it is to be expected that EEG components related to beat processing would show a lower amplitude when listening to isochronous rhythms. (Costa-Faidella et al., 2011). This may explain why we observed that rates of neural entrainment in A1 increased with tapping variability (Fig. 5). It may be the case that rhythms which are easier to tap along to exhibit less uncertainty and therefore lead to lower amplitude in the entrained EEG signals. Therefore, results provide an origin of temporal prediction for rhythmic stimuli and place A1 as a potential candidate for predictive mechanisms. According to this interpretation we should expect higher entrainment in simple rhythms compared to metronome rhythms. However, our results showed no significant difference between these two conditions in A1 - although the numeric trends show slightly higher entrainment in simple rhythms. This should not be taken as contrary evidence to our interpretation. Instead, it could be interpreted that tapping variability may be more indicative of the level of uncertainty experienced then the mere amount of isochrony in the stimuli. As such, we would expect that regardless of the level of isochrony, as SMS becomes more variable the level of uncertainty will also increase.

33

Modality and Multimodal Effect of Neural Entrainment Ammirante et al., 2016 observed that while levels of SMS between vibrotactile, auditory and multimodal were comparable for isochronous rhythms; auditory and multimodal rhythms were at an advantage for simple rhythms. They interpreted this pattern of results as suggesting that the temporal information used for SMS was mainly being provided by the auditory channel. The entrainment results of the current study (Fig. 4) provide additional support for this interpretation. We observed that rates of entrainment in A1 were higher for auditory-only and multimodal rhythms compared to vibrotactile rhythms. These results also support the idea that auditory modality possesses an advantage in temporal processing of rhythmic stimuli. This advantage may ultimately be based in specialized auditory-motor connectivity that has evolved to support vocal learning (Patel, 2006). The data extends these findings and shows that this advantage extends over tactile rhythms as well. The trends observed in tapping variability did not directly map onto findings of Ammirante et al., 2016, as we did not observe any statistically significant differences between modalities. However, the numeric trends in the data suggest lower variability in the multimodal conditions compared to auditory (Fig. 5). Not finding a difference in tapping between auditory only and vibrotactile conditions does not fit the hypothesis of an auditory advantage for SMS over other modalities (Repp & Penal, 2002; Patel et al., 2005; McAuley & Henry, 2010). As the sample of the current study is much smaller than other studies, it is possible that these numeric trends may become significant with the addition of a few additional participants. The numeric trends in the tapping data also suggest that the combination of auditory and vibrotactile information may provide the best SMS. This suggests a possible additive effect of multimodal stimuli for SMS. A similar trend was observed in a recent study by Hove et al. (in

34

prep.), wherein adding vibrotactile stimulation to musical clips increased synchronized postural sway to the beat of the music. Although the evidence for a multimodal enhancement is quite preliminary, it is worthy of further exploration. Investigating the Role of Cortical Plasticity and Beat Perception The functional organization of the brain is more plastic than once believed. In cases where the amount of sensory input received by a section of cortex is reduced ­ as is the case of peripheral hearing loss - the "unused" will be repurposed and adopt other sensory input. This repurposing of cortices is called compensatory plasticity (Lazzouni & Lepore, 2014). This phenomenon is well documented in the deaf and hard of hearing population, and can be seen as morphological and functional changes the auditory cortex (Emmorey et al., 2003; González-Garrido et al., 2017; Levänen & Hamdorf, 2001; Li et al., 2012). Specifically, an increase in activation in the auditory cortex during a tactile discrimination has been observed in the deaf/hearing impaired (Auer et al., 2007; Levänen, Jousmäki, & Hari, 1998). This is also met with an increase in vibro-tactile change detection sensitivity (Levänen & Hamdorf, 2001). This suggests a cognitive and neurological change in the sensation and perception of tactile information as a result of hearing impairment. Future directions of this research may benefit from investigating how these plastic changes in the brain effect perceptions of a beat in vibrotactile modalities. It is a point of interest to determine if SMS to vibrotactile rhythms is increased in a population that is effected by hearing loss due to an increased sensitivity to tactile information. Moreover, it would be interesting to explore how hearing impairment effects the locality of neural entrainment. Limitations Although the current results show trends in neural entrainment and SMS across different modalities, there are a few limitations to be noted. First, as mentioned prior the sample of the

35

current study was fairly small compared to other studies investigating modality effects of beat perception. We initially conducted a power analysis using the power estimates from Nozaradan et al. 2011 based on the similar type of analysis used to calculate neural entrainment. This may however have been an inappropriate choice, as there may be more similarities to other studies which have investigated modality differences in beat perceptions (Repp & Penal, 2002; Patel et al., 2005; McAuley & Henry, 2010, Ammirante et al., 2016). As a result, this study may benefit from a larger sample size, which could be calculated using the power estimate of a more similar study. Another possible limitation is related to monotony of the task. Both the passive listening and tapping task is fairly repetitive and many participants reported saying the task was very tiresome and tedious. This may be related to the simplified stimuli used in the experiment, a common crux of basic neuroscientific investigations. A similar point ­ and one that may be contribute to the monotony ­ is that our rhythms were quite unlike what people experience when listening to music. The rhythms used comprised of a series of pure tones which did not fluctuate across the rhythm and were very repetitive. The use of pure or flat tones in auditory perception research is very common, as a survey of 215 experiments conducted by Shultz & Gillard, 2017 showed that only 2.8% of experiments used naturalistic tones. Using stimuli that are unlike musical rhythms may be problematic as it effects our ability to generalize our findings to the perception of music. Future research on this topic may benefit from the use of stimuli that are more naturalistic. Such experiments may use drum sounds instead of pure tones, or actual musical clips.

36

Conclusion The aim of this study was to compare how we perceive and synchronize motor movements to the beat of rhythms presented through auditory, tactile or multimodal conditions. More thoroughly, we intended to examine the neural mechanisms underlining this process. We collected neural responses using EEG and behavioural data using a tapping paradigm. Beat perception was quantified as entrained neural responses and synchronised tapping to the frequency of the beat. We compared beat perception across three sensory modalities: auditory, vibrotactile, multimodal (auditory and vibrotactile) and two levels of rhythm complexity: metronomic and simple. Trends in the EEG data showed that neuronal signals in primary auditory cortex entrain to the beat of vibrotactile rhythms, albeit entrainment was much stronger in auditory and multimodal rhythms. Synchrony of tapping was also lowest in vibrotactile compared to auditory and multimodal rhythms. Neural entrainment and tapping synchrony did not significantly differ between auditory and multimodal rhythms. It was shown that tapping variability predicted neural entrainment, such that more variable tapping elicited a more entrained neural signal in primary auditory cortex. In conclusion, these results provide further evidence that temporal processing of the beat is superior in auditory modalities, and although beat perception was comparably lower, the results show a low level of neural entrainment to the beat for vibrotactile rhythms.

37

References Ammirante, P., Patel, A. D., & Russo, F. A. (2016). Synchronizing to auditory and tactile metronomes: a test of the auditory-motor enhancement hypothesis. Psychonomic Bulletin & Review, under revi, 0­34. http://doi.org/10.3758/s13423-016-1067-9 Ammirante, P., Russo, F. A., Good, A., & Fels, D. I. (2013). Feeling Voices. PLoS ONE, 8(1), 1­5. http://doi.org/10.1371/journal.pone.0053585 Brainard, D. H. (1997). The Psychophysics Toolbox. Spatial Vision, 10(4), 433­436. http://doi.org/10.1163/156856897X00357 Brochard, R., Touzalin, P., Després, O., & Dufour, A. (2008). Evidence of beat perception via purely tactile stimulation. Brain Research, 1223, 59­64. http://doi.org/10.1016/j.brainres.2008.05.050 Cameron, D. J., Stewart, L., Pearce, M. T., Grube, M., & Muggleton, N. G. (2012). Modulation of motor excitability by metricality of tone sequences. Psychomusicology: Music, Mind, and Brain, 22(2), 122­128. http://doi.org/10.1037/a0031229 Chen, J. L., Penhune, V. B., & Zatorre, R. J. (2008). Moving on time: brain network for auditory-motor synchronization is modulated by rhythm complexity and musical training. Journal of Cognitive Neuroscience, 20(2), 226­239. http://doi.org/10.1162/jocn.2008.20018 Cooper, G., & Meyer, L. B. (1960). The rhythmic structure ofmusic. Chicago: The University of Chicago Press. Costa-Faidella, J., Baldeweg, T., Grimm, S., and Escera, C. (2011). Interactions between "what" and "when" in the auditory system: temporal predictability enhances repetition suppression. J. Neurosci. 31, 18590­18597 Cunnington R, Windischberger C, Deecke L, Moser E (2003). "The preparation and readiness for

38

voluntary movement: a high-field event-related fMRI study of the Bereitschafts-BOLD response". NeuroImage. 20: 404­412. doi:10.1016/s1053-8119(03)00291-x Delorme, A., & Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis. Journal of Neuroscience Methods, 134(1), 9­21. http://doi.org/10.1016/j.jneumeth.2003.10.009 Elliott, D. F., & Rao, K. R. (1985). FAST TRANSFORMS, Algorithms, Analysis, Applications. Proceedings of the IEEE. http://doi.org/10.1109/PROC.1985.13175 Formisano, E., Kim, D. S., Di Salle, F., Van De Moortele, P. F., Ugurbil, K., & Goebel, R. (2003). Mirror-symmetric tonotopic maps in human primary auditory cortex. Neuron, 40(4), 859­869. http://doi.org/10.1016/S0896-6273(03)00669-X Fujioka, T., Fidali, B. C., & Ross, B. (2014). Neural correlates of intentional switching from ternary to binary meter in a musical hemiola pattern. Frontiers in Psychology, 5(NOV), 1­ 15. http://doi.org/10.3389/fpsyg.2014.01257 Fujioka, T., Trainor, L. J., Large, E. W., & Ross, B. (2012). Internalized Timing of Isochronous Sounds Is Represented in Neuromagnetic Beta Oscillations. Journal of Neuroscience, 32(5), 1791­1802. http://doi.org/10.1523/JNEUROSCI.4107-11.2012 Fujioka, T., Zendel, B. R., & Ross, B. (2010). Endogenous Neuromagnetic Activity for Mental Hierarchy of Timing. Journal of Neuroscience, 30(9), 3458­3466. http://doi.org/10.1523/JNEUROSCI.3086-09.2010 Gibbon, J., Church, R. M., & Meck, W. H. (1984). Scalar Timing in Memory. Annals of the New York Academy of Sciences, 423(1), 52­77. http://doi.org/10.1111/j.17496632.1984.tb23417.x González-Garrido, A. A., Ruiz-Stovel, V. D., Gómez-Velázquez, F. R., Vélez-Pérez, H., Romo-

39

Vázquez, R., Salido-Ruiz, R. A., ... Campos, L. R. (2017). Vibrotactile Discrimination Training Affects Brain Connectivity in Profoundly Deaf Individuals. Frontiers in Human Neuroscience, 11(February). http://doi.org/10.3389/fnhum.2017.00028 Grahn, J. A. (2012). Neural Mechanisms of Rhythm Perception: Current Findings and Future Perspectives. Topics in Cognitive Science, 4(4), 585­606. http://doi.org/10.1111/j.17568765.2012.01213.x Grahn, J. A., & Brett, M. (2007). Rhythm and Beat Perception in Motor Areas of the Brain. Journal of Cognitive Neuroscience, 19(5), 893­906. Retrieved from http://www.mitpressjournals.org/doi/abs/10.1162/jocn.2007.19.5.893 Grahn, J. A., & Rowe, J. B. (2009). Feeling the Beat: Premotor and Striatal Interactions in Musicians and Nonmusicians during Beat Perception. Journal of Neuroscience, 29(23), 7540­7548. http://doi.org/10.1523/JNEUROSCI.2018-08.2009 Grube, M., & Griffiths, T. D. (2009). Metricality-enhanced temporal encoding and the subjective perception of rhythmic sequences. Cortex, 45(1), 72­79. http://doi.org/10.1016/j.cortex.2008.01.006 Gruters, K. G., & Groh, J. M. (2012). Sounds and beyond: multisensory and other non-auditory signals in the inferior colliculus. Frontiers in Neural Circuits, 6(December), 96. http://doi.org/10.3389/fncir.2012.00096 Henry, M. J., Herrmann, B., Kunke, D., & Obleser, J. (2017). Aging affects the balance of neural entrainment and top-down neural modulation in the listening brain. Nature Communications, 8(May), 15801. http://doi.org/10.1038/ncomms15801 Henry, M. J., & Obleser, J. (2012). Frequency modulation entrains slow neural oscillations and optimizes human listening behavior. Proceedings of the National Academy of Sciences of

40

the United States of America, 109(49), 2009­100. http://doi.org/10.1073/pnas.1213390109 Holton, T., & Hudspeth, A. J. (1983). A micromechanical contribution to cochlear tuning and tonotopic organization. Science, 222(4623), 508­10. http://doi.org/10.1126/science.6623089 Honing, H. (2012). Without it no music: Beat induction as a funda- mental musical trait. Annals of the New York Academy of Sciences, 1252, 85­91. doi:10.1111/j.17496632.2011.06402.x Honing H, Merchant H, Heden G, Prado LA, Bartolo R. 2012 Rhesus monkeys (Macaca mulatta) can detect rhythmic groups in music, but not the beat. PLoS ONE, 7, e51369. doi:10.1371/journal.pone. 0051369 Hove, M. J., Martinez, S., & Stupacher, J. (in preparation). Feel the Bass: Effects of aural and bodily bass stimulation on groove ratings and movement induction. Ivry, R. B., & Hazeltine, R. E. (1995). Perception and production of temporal intervals across a range of durations: Evidence for a common timing mechanism. Journal of Experimental Psychology: Human Perception and Performance, 21(1), 3­18. http://doi.org/10.1037/0096-1523.21.1.3 Kaczmarek, K. A., Webster, J. G., Bach-y-Rita, P., & Tompkins, W. J. (1991). Electrotactile and Vibrotactile Displays for Sensory Substitution Systems. IEEE Transactions on Biomedical Engineering, 38(1), 1­16. http://doi.org/10.1109/10.68204 Kayser, C., Petkov, C. I., Augath, M., & Logothetis, N. K. (2005). Integration of touch and sound in auditory cortex. Neuron, 48(2), 373­384. http://doi.org/10.1016/j.neuron.2005.09.018 Kolers, P. A., & Brewster, J. M. (1985). Rhythms and responses. Journal of Experimental Psychology: Human Perception and Performance, 11(2), 150­167.

41

http://doi.org/10.1037/0096-1523.11.2.150 Large, E. W., Herrera, J. A., & Velasco, M. J. (2015). Neural Networks for Beat Perception in Musical Rhythm. Frontiers in Systems Neuroscience, 9(November), 1­14. http://doi.org/10.3389/fnsys.2015.00159 Large, E. W., & Jones, M. R. (1999). The dynamic of attending: How People Track TimeVarying Events. Psychological Research, 106(1), 119­159. http://doi.org/10.1037/0033295X.106.1.119 Lazzouni, L., & Lepore, F. (2014). Compensatory plasticity: Time matters. Frontiers in Human Neuroscience, 8. doi:10.3389/fnhum.2014.00340 Lerdahl F, Jackendoff RS (1983) A Generative Theory of Tonal Music (MIT Press, Cambridge, MA). Levänen, S., Jousmäki, V., & Hari, R. (1998). Vibration-induced auditory-cortex activation in a congenitally deaf adult. Current Biology: CB, 8(15), 869­872. http://doi.org/10.1016/S0960-9822(07)00348-X Llinás, R. R. (2014). Intrinsic electrical properties of mammalian neurons and CNS function: a historical perspective. Frontiers in Cellular Neuroscience, 8(November), 1­14. http://doi.org/10.3389/fncel.2014.00320 London, J. (2012). Hearing in Time: Psychological Aspects of Musical Meter. Hearing in Time: Psychological Aspects of Musical Meter. http://doi.org/10.1093/acprof:oso/9780199744374.001.0001 Matell, M. S., & Meck, W. H. (2000). Neuropsychological mechanisms of interval timing behavior. BioEssays, 22(1), 94­103. http://doi.org/10.1002/(SICI)15211878(200001)22:1<94::AID-BIES14>3.0.CO;2-E

42

McAuley, J. D. (2010). Music Perception (Vol. 36). http://doi.org/10.1007/978-1-4419-6114-3 McAuley, J., & Henry, M. J. (2010). Modality effects in rhythm processing: Auditory encoding of visual rhythms is neither obligatory nor automatic. Attention, Perception & Psychophysics, 72(5), 1377­1389. http://doi.org/10.3758/APP McCoach, D. B., & Adelson, J. L. (2010). Dealing with dependence (Part I): Understanding the effects of clustered data. Gifted Child Quarterly, 54(2), 152­155. http://doi.org/10.1177/0016986210363076 Meck, W. H. (2003). Functional and neural mechanisms of interval timing. Genetics. Retrieved from http://books.google.com/books?id=0vVhounnRJwC&pgis=1 Merchant, H., Grahn, J., Trainor, L., Rohrmeier, M., & Fitch, W. T. (2015). Finding the beat: a neural perspective across humans and non-human primates. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, 370, 20140093. http://doi.org/10.1098/rstb.2014.0093 Merzenich, M. M., Knight, P. L., & Roth, G. L. (1975). Representation of Cochlea within Primary Auditory Cortex in the Cat. Journal of Neurophysiology, 38(2), 231­249. http://doi.org/10.1121/1.1920046 Mognon, A., Jovicich, J., Bruzzone, L., & Buiatti, M. (2011). ADJUST: An automatic EEG artifact detector based on the joint use of spatial and temporal features. Psychophysiology, 48(2), 229­240. http://doi.org/10.1111/j.1469-8986.2010.01061.x Nozaradan, S. (2014). Exploring how musical rhythm entrains brain activity with electroencephalogram frequency-tagging. Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, 369(1658), 1­10. http://doi.org/10.1098/rstb.2013.0393

43

Nozaradan, S., Peretz, I., Missal, M., & Mouraux, A. (2011). Tagging the Neuronal Entrainment to Beat and Meter. Journal of Neuroscience, 31(28), 10234­10240. http://doi.org/10.1523/JNEUROSCI.0411-11.2011 Oster, H., Damerow, S., Kiessling, S., Jakubcakova, V., Abraham, D., Tian, J., ... Eichele, G. (2006). The circadian rhythm of glucocorticoids is regulated by a gating mechanism residing in the adrenal cortical clock. Cell Metabolism, 4(2), 163­173. http://doi.org/10.1016/j.cmet.2006.07.002 Palmer, C., & Krumhansl, C. L. (1990). Mental representations for musical meter. Journal of Experimental Psychology: Human Perception and Performance, 16(4), 728­741. http://doi.org/10.1037/0096-1523.16.4.728 Patel, A. D. (2006). Musical Rhythm, Linguistic Rhythm, and Human Evolution. Music Perception. 24 (1), 99-104. http://doi.org/10.1525/mp.2006.24.1.99 Patel, A. D., & Iversen, J. R. (2014). The evolutionary neuroscience of musical beat perception: the Action Simulation for Auditory Prediction (ASAP) hypothesis. Frontiers in Systems Neuroscience, 8(May), 1­14. http://doi.org/10.3389/fnsys.2014.00057 Patel, A. D., Iversen, J. R., Chen, Y., & Repp, B. H. (2005). The influence of metricality and modality on synchronization with a beat. Experimental Brain Research, 163(2), 226­238. http://doi.org/10.1007/s00221-004-2159-8 Penhune, V. B., Zatorre, R. J., & Evans, A. C. (1998). Cerebellar Contributions to Motor Timing: A PET Study of Auditory and Visual Rhythm Reproduction. Journal of Cognitive Neuroscience, 10(6), 752­765. http://doi.org/10.1162/089892998563149 Povel, D.-J., & Essens, P. J. (1985). Perception of temporal patterns. Music Perception: An Interdisciplinary Journal, 2(4), 411­440. http://doi.org/10.2307/40285311

44

Povel, D. J. (1981). Internal representation of simple temporal patterns. Journal of Experimental Psychology. Human Perception and Performance, 7(1), 3­18. http://doi.org/10.1037/00961523.7.1.3 Regan D (1989) Human brain electrophysiology: evoked potentials and evoked magnetic fields in science and medicine. New York: Elsevier. Repp, B. H. (2005). Sensorimotor synchronization: a review of the tapping literature. Psychonomic Bulletin & Review, 12(6), 969­992. http://doi.org/10.3758/BF03206433 Repp, B. H., & Su, Y.-H. (2013). Sensorimotor synchronization: A review of recent research (2006­2012). Psychonomic Bulletin & Review, 20(3), 403­452. http://doi.org/10.3758/s13423-012-0371-2 Roux, S., Coulmance, M., & Riehle, A. (2003). Context-related representation of timing processes in monkey motor cortex. European Journal of Neuroscience, 18(4), 1011­1016. http://doi.org/10.1046/j.1460-9568.2003.02792.x Russo, F. a, Ammirante, P., & Fels, D. I. (2012). Vibrotactile discrimination of musical timbre. Journal of Experimental Psychology. Human Perception and Performance, 38(4), 822­6. http://doi.org/10.1037/a0029046 Russo, F. A. (in press). Multisensory processing in music. In Handbook of Music and Brain Research (Eds. Michael Thaut and Don Hodges). Oxford University Press. Saupe K, Schro ¨ger E, Andersen SK,Mu ¨llerMM (2009) Neural mechanisms of intermodal sustained selective attention with concurrently presented auditory and visual stimuli. Front Hum Neurosci 3:58. Schubotz, R. I., Friederici, A. D., & Yves von Cramon, D. (2000). Time Perception and Motor Timing: A Common Cortical and Subcortical Basis Revealed by fMRI. NeuroImage, 11(1),

45

1­12. http://doi.org/10.1006/nimg.1999.0514 Schutz, M., & Gillard, J. (2017). Surveying the sounds used in auditory perception research: Journal of the Acoustical Society of America. Canadian Acoustics, 45(3). Retrieved from https://jcaa.caa-aca.ca/index.php/jcaa/article/view/3090 doi:10.1525/mp.2014.31.3.288 Snyder, J. S., & Large, E. W. (2005). Gamma-band activity reflects the metric structure of rhythmic tone sequences. Cognitive Brain Research, 24(1), 117­126. http://doi.org/10.1016/j.cogbrainres.2004.12.014 Teki, S., Grube, M., & Griffiths, T. D. (2012). A Unified Model of Time Perception Accounts for Duration-Based and Beat-Based Timing Mechanisms. Frontiers in Integrative Neuroscience, 5(January), 1­7. http://doi.org/10.3389/fnint.2011.00090 Teki, S., Grube, M., Kumar, S., & Griffiths, T. D. (2011). Distinct Neural Substrates of Duration-Based and Beat-Based Auditory Timing. Journal of Neuroscience, 31(10), 3805­ 3812. http://doi.org/10.1523/JNEUROSCI.5561-10.2011 Temperley, N. M. (1963). Personal tempo and subjective accentuation. The Journal of General Psychology. http://doi.org/10.1080/00221309.1963.9920534 Vuust, P., & Witek, M. A. G. (2014). Rhythmic complexity and predictive coding: A novel approach to modeling rhythm and meter perception in music. Frontiers in Psychology, 5(OCT), 1­14. http://doi.org/10.3389/fpsyg.2014.01111.

46


