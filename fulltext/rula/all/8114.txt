EXTENDING THE HUMAN UMWELT: REFRAMING OUR UNDERSTANDING OF AN ENVIRONMENT VIA MIXED REALITY AND DIGITAL SOUNDSCAPES / VISUALIZATIONS

MICHAEL JAMES CASTLEDINE Graphic Design Diploma, Sheridan College, 2001 Multimedia Certificate, University of Guelph, 2003

An MRP presented to Ryerson University in partial fulfilment of the requirements for the degree of Master of Digital Media in the program of Digital Media

Toronto, Ontario, Canada, 2018

© Michael James Castledine, 2018

AUTHOR'S DECLARATION I hereby declare that I am the sole author of this MRP. This is a true copy of the MRP, including any required final revisions. I authorize Ryerson University to lend this MRP to other institutions or individuals for the purpose of scholarly research. I further authorize Ryerson University to reproduce this MRP by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my MRP may be made electronically available to the public.

ii

EXTENDING THE HUMAN UMWELT: REFRAMING OUR UNDERSTANDING OF AN ENVIRONMENT VIA MIXED REALITY AND DIGITAL SOUNDSCAPES / VISUALIZATIONS Michael James Castledine, 2018 Master of Digital Media Digital Media Ryerson University ABSTRACT This project conceptualizes and creates a series of multi-sensory art installations for public and private musical / sound experiences, exploring conversations in augmented / generative art, synesthesia and digital trespassing. At the intersection of the digital and physical world, how can reframing and layering physical landscapes with digital soundscapes educate, inform, delight, and elicit conversations about a world that has not been experienced before? Making reference to contemporary technology and documenting the creative / constructive process, the direction of this inquiry explores how a multi-sensory and generative artistic approach can create strong crossovers from one sense to inform another.

iii

ACKNOWLEDGEMENTS On this new adventure, I would like to acknowledge the love and support from the following people who have helped me along the way. Close family and friends; MDM colleagues; Matt Marshall; Michael F Bergmann; Am Sagarwala; Jimmy Tran; Colaboratory / DME staff.

iv

DEDICATION I would like to dedicate this MRP to my loving wife, for all your support and patience over the years, to my daughter who continues to make me laugh and is a source of energy and imagination, and to my new son who has inspired me to pursue this new adventure.

v

TABLE OF CONTENTS AUTHOR'S DECLARATION ABSTRACT ACKNOWLEDGEMENTS DEDICATION LIST OF FIGURES INTRODUCTION REVIEW OF LITERATURE / EARLY RESEARCH PROJECT ONE: 1. INSPIRATION / RESEARCH 2. METHODOLOGY / CONCEPT SKETCHES 3. TOUCH THE MUSIC 4. SEE THE MUSIC 5. FEEL THE MUSIC 6. HEAR THE MUSIC 7. FINAL THOUGHTS / FINDINGS PROJECT TWO: 1. INSPIRATION / RESEARCH 2. METHODOLOGY / DESIGN 3. SENSORS / BODY TRACKING TESTS 4. INDETERMINACY MUSIC PROJECT THREE: 1. INSPIRATION / RESEARCH
vi

ii iii iv v viii 1 3 8 8 10 10 13 `16 17 18 20 20 22 22 24 27 27

2. METHODOLOGY / SOFTWARE SCHEMATIC DESIGN 3. THE TECHNOLOGY 4. TRIGGERS 5. GENERATIVE SOUND SCULPTURES 6. FINAL THOUGHTS 7. PSYCHOGEOGRAPHY / INCLUSIVITY REFERENCES

30 30 31 33 36 36 38

vii

LIST OF FIGURES Figure 1: Structure of a Digital Musical Instrument and Rouages approach (Berthaut, 2013) Figure 2: Screenshots from Los Angeles Philharmonic Orchestra VR web experience Figure 3: Touché modules on skin with control panel Figure 4: Noli Me Tangere installation with user interaction Figure 5: Project One: Process sketches Figure 6: Early capacitive tests with conductive ink, foil and acrylic paint Figure 7: Adafruit 12-Key Capacitive Touch Sensor Breakout board Figure 8: Micro controller with extended arm Figure 9: Exploration in interface design and sound shapes Figure 10: Graphical bars of Metastasies Figure 11: Testing simple button triggers to activate sound / light sequences Figure 12: Subpac M2X (www.subpac.com) 3 4 6 9 10 11 12 12 13 14 15 17

Figure 13: Construction process, back of canvas showing wiring, micro switches and Arduino 18 Figure 14: Final product with participant Figure 15: Floating in the Falling Universe of Flowers installation by teamLab Figure 16: Project Two: Schematic Software Design Figure 17: Spatial Sound Control Diagram Figure 18: Kinect depth mapping and processing tests Figure 19: Processing / MSA Fluid particle sketch showing movement and colour Figure 20: Participant engaging with particle animation and movement Figure 21: Active Theory early AR experiments (www.activetheory.net) Figure 22: World Brush app screenshots of AR drawings over Project One 19 21 22 22 23 25 26 27 28

viii

Figure 23: Marpi showcasing invisible AR exhibition Figure 24: Project Three: Schematic Design

29 30

Figure 25: Demonstration of Vuforia's trigger technology, 2D image spawns 3D unity model 31 Figure 26: Screenshot of Vuforia Target Manager and 3D object scan Figure 27: Screenshot of Vuforia Target Manager and image rating scan Figure 28: Phone screenshots of Union Station musical performance soundscape Figure 29: Live musical performance with generative sound animations Figure 30: Physical speaker triggers pre-recorded sound reactive animation Figure 31: Nigel John Stanford's Demo: Chladni Plate - Sound, Vibration and Sand 32 33 34 35 35 36

ix

INTRODUCTION "...a wish to perceive something that we think we can not perceive..." - Neil Harbisson "To most of us music suggests definite mental images of form and colour. The picture you are about to see is a novel scientific experiment - its object is to convey these mental images in visual form." - Oskar Fischinger, 1938 "It is now becoming clear through scanning technologies that the various senses also share higher order cerebral networks, or perceptual supramodalities that engage a crossover of sensory inputs from one sense to another" - Harry Francis Mallgrave, 2010

In Marshall McLuhan's "Visual and Acoustic Space" he explains that we hear from all directions and see from only one direction, making the auditory sense a more efficient model for mapping our domain. He explains that in Western culture, we attempt to understand our world through metaphors for visual representations of space: "Truth, we think, must be observed by the `eye,' then judged by the `I'" (McLuhan, 1997). Contingent upon which senses are engaged through a given medium, the subsequent environment will either produce a visual or acoustic depiction of the reality. This project attempts to bring sight, touch and sound to the same "playing level".

What if we could see and touch sound? It is through one's five senses that we experience, learn and understand the world around us. Can we extend the senses or overlap them in ways that have not been done before? For many artists and designers these complimentary senses are starting to interact in unimaginable and unique ways, in contradiction to McLuhan's idea that we

1

experience "the emotional and intellectual jag," resulting from our attempts to convey properties of acoustic spheres through visual representations.

Today, through many new tools such as mixed reality platforms, for example Microsoft's HoloLens and haptic sensors, these senses can come together to form unique artistic expressions, leading to public and private conversations around how we can educate and share our memories. Mixed reality (MR), is the merging of real and virtual worlds producing new settings and visualisations. Physical and digital elements co-exist and interact in real time. This new technology provides unique opportunities to create human "peripheral devices."

Our understanding of reality is framed by our biology. This suggests that our eyes, our ears and our fingers are separately handling data from our environment. Therefore, our brains are sampling limited slivers of this environment. In the animal kingdom, different organisms pick up on various spectrums of their surroundings. We can find various peripheral devices in the animal kingdom. For the deaf and blind tick, the imperative imputs are temperature; for the bat, its reality is developed out of sound waves. Snakes have heat pits to detect infrared, and sharks have electroreceptors. These are the slices of the ecosystem that they perceive. Their perception of their environment becomes their reality. "One thing about which fish know exactly nothing is water, since they have no anti-environment which would enable them to perceive the element they live in" (McLuhan, 2002).

2

The German word for any organisms experience of its environment is called the umwelt. This project researches and documents how we can extend the human umwelt through the artistic creation of a human peripheral device.

REVIEW OF LITERATURE / EARLY RESEARCH

AUGMENTED INCLUSIVE DESIGN Research aimed at enriching artistic performances by the use of MR is currently being conducted at the University of Lille. The "Rouages" project enables musicians and artists to augment their MIDI (Musical Instrument Digital Interface) control devices with graphical UIs of music programming. This project proposes augmented digital musical instruments. For example, the musician can interact with a 3D visual representation of his or her mixer board and reveal the music mechanisms to the audience. This approach is a unique augmented reality display demonstrating musical performances by layering both physical and digital content on stage.

Figure 1: Structure of a Digital Musical Instrument and Rouages approach (Berthaut, 2013)

3

In 2016, the Los Angeles Philharmonic under Gustavo Dudamel performed Beethoven's Symphony No.5, not to a full concert hall, but for a 360 degree camera. This is orchestra VR. This experience was enhanced further with the addition of colourful animated "spirits", each representing a different element and feeling of the music.

Figure 2: Screenshots from Los Angeles Philharmonic Orchestra VR web experience (Photo credit: Los Angeles Philharmonic Orchestra Association- Youtube)

Similarly Simon Russell, a designer and animator, explored the combination of audio and visual through a generative visualisation of the ballet composed in 1801 by Beethoven. Augmented aided tools read music notation and generate particles using the pitch of the music to drive their height and increase speed. As the volume of each note increases so does the intensity of the colours emitted. This is a strong cybernetic example of how generative and artistic creative coding are bringing new experiences to those who might not be able to sit in a concert hall or hear the music.

SENSORY TRANSFER / CONNECTIONS In unique situations when one sense is limited or not present at all, the potential for interesting conversations and environments unfold. A case in point: Neil Harbisson's "eyeborg" allows him

4

to "listen" to colour via a device that converts color into audible frequencies. This is an exciting example of a cybernetic conversation and relationship leading to further exploration and knowledge. Harbisson explains, "In extending our senses, we can extend our knowledge." Once this cyborg relationship was established, an interesting secondary or "flipped" affect unfolded which allowed Harbisson to see normal sounds as colour; ie. he heard the phone ring and "felt" green. This unique response to a particular sound is a result of the way his eyeborg program has reprogrammed this perception.

Some individuals have an exceptionally interesting reaction to colour and sound. Synesthesia is a neurological condition in which one sense or intellectual pathway triggers automatic encounters in an auxiliary sense. Synesthesia can occur between almost any two senses and has many classified types or forms. One of the most common forms of synesthesia is chromesthesia, the association of sounds with colors.

Artist, writer, curator, and synesthete Carol Steen is a researcher who has made her work and knowledge of synesthesia more available to the public. Her article "Visions Shared: A Firsthand Look into Synesthesia and Art" is a pioneering account of synesthesia and its role in art. Steen explains "art inspired by synesthesia may convey information of significance to everyone, observing that more general aspects of perception may be illuminated by the study of synesthesia" (Steen, 2000).

Research by Charles Spence at Oxford University has discovered that what we hear can directly affect our perception of what we are tasting. In a psychology lab, while eating the same piece of

5

toffee, subjects listened to a high pitch sound and then a low pitch sound. Interesting results found that the toffee tasted more sweet at a higher frequency. This is another unique example of how senses interact, influence and inform and suggests the potential for further exploration in sensory enhancement (Herz, 2017).

Haptic technologies are being used in virtual arts, for example in sound synthesis, graphic design and animation. Haptic devices allow the artist to have direct contact with a virtual instrument that produces real-time sound or images. For instance, the simulation of guitar strings creates realtime vibrations under the pressure of the fingers and fluency of the pick (a haptic device) held by the artist. This can be done with physical modeling synthesis and can be further explored visually with cymatic tools. An exciting example of this is Touché', a series of wearable modules that allow live music be performed as "vibration loops and temperature melodies" on one's skin.

Figure 3: Touché modules on skin with control panel (Photo credit: Marie Tricaud)

GENERATIVE ART / RELATIONSHIPS Generative art is creation inside a predetermined "controlled" system. Often this includes an element of chance, a chance of connection via disconnection. Although there are many new and exciting generative tools available to artists today, this approach is not new and has its roots in
6

the Dada movement. AARON, created by Harold Cohen, a pioneering artist of generative art, is a computer program that is able to draw both abstract and representational forms with a remarkably organic and human-like quality. Cohen claims AARON is not creative but asks "If what AARON is making is not art, what is it exactly, and in what ways, other than its origin, does it differ from the `real thing?' If it is not thinking, what exactly is it doing?" (Harold, 1995). In the twenty-first century, artists such as Joshua Davis and Jessica Rosenkrantz continue to push this question while using various generative models in an artist / computer symbiosis.

In 1960, J.C.R. Licklider wrote that there will be "a fairly long interim during which the main intellectual advances will be be made by men and computers working together in an intimate association... those years should be intellectually the most creative and exciting in the history of mankind" (Licklider, 1960).

The above examples of technologies and experiences have helped encourage my belief that the possibility of creating new learning and artistic experiences are both feasible and achievable. They have also shaped the direction of my approaches to multi-sensory art installations.

7

PROJECT ONE

Feel the Music, See the Music, Hear the Music is an interactive digital experience presented on an analog platform. I am interested in the tactility and immersiveness of sound intersecting at the crosshairs of the digital and physical world. Through touch technology and haptic sensors, music generates a visual response allowing a user to feel, see and hear the music.

As an introduction to building a multi-sensory platform, from a technology and research approach, this art installation aimed to evoke and transform an individual's perception of how they can experience sound. This piece further explored the research question: how can the sense of sound be amplified to inform and delight the other senses? Demonstrated through a classical art medium, a physical canvas plays on the juxtaposition to its digital core. The incongruity of the headphones and power cord is juxtaposed to the traditionally standalone canvas.

INSPIRATION / RESEARCH

Early research into building simple circuits lead to conductive inks and inspirational installations such as A "Touching" Story: Conductive Ink Revives A Classic Philippine Novel. Jose Rizal's novel Noli Me Tangere translates to touch me not. Designed to bring this novel to a wider audience, illustrations of the story's characters were drawn using conductive ink on a wall. Instead of not being touched, the illustrations, when touched, closed the circuit and triggered
8

audio quotes from the book.

Figure 4: Noli Me Tangere installation with user interaction (Photo credit: A Bigornia ­ Vimeo)

Along with the technology used in the piece, it was the conversation of this somatic experience bringing the novel to a wider audience that triggered my interest. Speaking of Art as Embodied Imagination: A Multisensory Approach to Understanding Aesthetic Experience examines the links between embodiment, movement, and multisensory experience in art installations. Arguing that "embodiment can be identified at two levels, the phenomenological and the cognitive unconscious, these multi-sensory installations help the understanding of the contours of art appreciation" (Joy & Sherry, 2003).

9

METHODOLOGY / CONCEPT SKETCHES

Figure 5: Process sketches

TOUCH THE MUSIC Conductive inks were chosen for their surface application flexibility. The form of the installation was not yet solidified and this ink medium allowed for applying a capacitive touch sensor onto almost any plane. Limitations from a technical and artistic standpoint were found during early testing on canvas.

10

Figure 6: Early capacitive tests with conductive ink, foil and acrylic paint (Photo credit: Michael Castledine)

While building a simple capacitive / LED Arduino circuit, the sensitivity level of the ink was deemed unreliable. While certain circuits became closed and the LED emitted light, proximity and greater distances from the button trigger to the Arduino failed. Further testing and research in capacitive materials led to button prototypes using copper foil. An Adafruit 12-Key Capacitive Touch Sensor Breakout board (Fig. 3) was also implemented to allow extended triggers. Although the copper foil worked well as a conductive material, applying a thin coat of paint over the foil caused it to become unstable. This meant that the graphical interface of the buttons and artwork was very limited.

11

Figure 7: Adafruit 12-Key Capacitive Touch Sensor Breakout board (Photo credit: Adafruit)

In consulting and researching various sources regarding responsive trigger inputs, a decision was made to use micro-switch buttons with extended levers. (Fig. 4) These switches provided a very accurate and sensitive control system via the Arduino circuit and coding configuration.

Figure 8: Micro controller with extended arm (Photo credit: Adafruit)

12

SEE THE MUSIC Research into areas of spectromorphology and work the of composer Iannis Xenakis led to sketching and generating free hand drawings, illustrating wavelengths of sound and graphical shapes which were influenced by listening to various musical pieces.

Figure 9: Exploration in interface design and sound shapes

Iannis Xenakis experimented with spatial representations of sounds to reveal visions within his music. His work Metastasies (Fig. 5) originated with a graphical score that maintained the conventional use of representational axis.

13

Figure 10: Graphical bars of Metastasies

Removing my sense of sight thus focusing on senses of hearing and touch, these sketches later became the stencil forms which were applied as a repeat pattern on the canvas.

Since the 1940s, research in spectromorphology has examined musical experiences in order to aid listening, and seeks to describe the features we hear and explain how they work in the context of the music. Spectromorphology is not a compositional process, rather a sequential tool based on sound perception (Smalley, 1995).

14

SOUND GENERATED / REACTIVE LIGHTS Experimental prototypes with sound reactive Neopixel LED rings added another visual and emotive layer to this installation.

Figure 11: Testing simple button triggers to activate sound / light sequences (Photo credit: Michael Castledine)

Through "controlled randomness" in the Arduino Javascript code, each soundtrack, when triggered, generated a different colour and animation sequence. I experimented with various sizes of LED rings while considering the colour modes and sequences.. Four rings were finally daisy-chained together allowing the animations to run across a total of 108 lights. This sequence

15

provided a strong visual response to the music.

FEEL THE MUSIC Through exploration and use of a mobile haptic product called the Subpac M2X, I was able to connect the music shield to the installation. The Subpac (Fig. 12) is a "tactile base unit that uses bone conduction technology to transfer the low end frequency into the body". With a line in and out, it also has a base intensity dial which allows the user to adjust the amount of feedback they receive through the music. Designed as a backpack, the device rests against the users back and vibrates sound between 5 and 130 hertz. While testing the haptic sound feedback, closing my eyes and plugging my ears resulted in an extraordinary and heightened sensory experience. This "heightened" experience was later observed again with a synesthesia triggered response by a participant.

16

Figure 12: Subpac M2X (Source- www.subpac.com)

HEAR THE MUSIC Project One implemented the use of an Arduino Mega single board microcontroller to handle inputs from the touch sensor micro-switch buttons with extend levers behind the canvas. Each button switch correlated with a particular mp3 music file and when engaged, triggered a unique sound generative Neopixel light animation. The smaller Arduino Uno model, when tested in the prototype, was not able to handle the number of digital output channels needed for the build.

17

Figure 13: Construction process, back of canvas showing wiring, micro switches and Arduino (Photo credit: Michael Castledine)

Many of the channels were taken up by the music shield. An Adafruit "Music Maker" shield for the Arduino stored the music files and allowed for audio output to headphones, external speakers and the haptic Subpac piece. The music tracks were personal favourites and chosen for their melodic and uplifting musical melodies.

FINAL THOUGHTS / FINDINGS

From local installations just as the Aga Khan Museum's "Listening to Art, Seeing Music" to London's Tate "Sensorium", multi-sensory experiences provide important platforms that engage the body and mind. They also compliment looking and enhance learning about works of art. Understanding how and why a work of art was made helps make it more relevant and engaging (Levent, 2017). Project One challenges the viewer and the artist to experience multiple senses in unfamiliar territory, leading to exciting learning conversations in technology and somatics.

18

Figure 14: Final product with participant (Photo credit: BDS Studios / Collaboratory)

19

PROJECT TWO

Project Two is titled Feeling space: Music, Movement and Sensory Extension. While Project One focused on the sense of sound and touch to drive the generative visuals and inform sight, Project Two explores the spatial movements of the body to inform and connect to the other senses. This installation explores and asks the question; can the body, through movement and dance, extend the senses in unique ways?

This multi-sensory and generative environment deploys the use of haptics and camera tracking technology, allowing body depth mapping to trigger and augment musical scores. While also using projection light as a medium, the immersive environment is ever morphing and mimicking the viewer / participant.

INSPIRATION / RESEARCH Exploration and inspiration in the use of projection mapping and interactive walls has lead to many conversations on how larger scale installations can delight and evoke strong sensory responses in a unique environment. Such examples are teamLab's Tokyo immersive art installation titled 'Floating in the Falling Universe of Flowers.' This 32,300 square foot exhibition inspired me to deploy a larger scale platform using projection light as a medium. This new approach provided an original method for my artistic creations.

20

Figure 15: Floating in the Falling Universe of Flowers installation by teamLab (Photo credit: teamlab)

Leman suggests that our perception of music is tied to other embodied experiences: "The multimodal aspect of musical interaction draws on the idea that the sensory systems ­ auditory, visual, haptic as well as movement perception ­ form a fully integrated part of the way the human subject is involved with music" (Leman, 2008). This project is not aimed at focusing on specific emotional processes involved in experiencing and listening to music. Rather, it speaks to the embodied interaction with our environment and develops different sensory perceptions of music and sound in space.

21

METHODOLOGY / DESIGN

Figure 16: Schematic Software Design

Figure 17: Spatial Sound Control Diagram

SENSORS / BODY TRACKING TESTS Previous work with Arduino and sensors first lead to conversations considering IR (Infrared) proximity sensors to trigger human motion in the environment. From a quick prototyping approach, the Microsoft Kinect was recommended and deployed, as its camera technology allows for easy access to map x, y and z coordinates.

22

Using the Processing.js (javascript) platform and Shiffmans OpenKinect/Processing library, I was quickly able to setup and test a series of depth mapping / body tracking exercises.

Figure 18: Kinect depth mapping and processing tests (Photo credit: Michael Castledine)

These tests helped to establish a range for the camera and users to interact with the Processing environment. Now that the "stage" was created, further exploration and experimentation to the "sketch" with various colour and fluid libraries were influenced through conversations with individuals who experience Chromesthesia or sound-to-color synesthesia.

23

INDETERMINACY MUSIC The work of John Cage, a pioneer of indeterminacy music, defined this methodology as "the ability of a piece to be performed in substantially different ways" (Cage, 2018). It is this composing approach in which `some aspects of a musical work are left open to chance or to the interpreter's free choice' that has inspired my connection to music from disconnection. This project aims to create a that unique `substantially different approach' through movement and free dance.

Minim is an audio library that uses the JavaSound API designed to connect sound to the processing environment. Using this library, I was able to play and experiment with its controls and generative properties. I was particularly interested and experimented with the `SoundEnergyBeatDetection' library sketch, which connected any drawn elements to become reactive to the audio track. Using John Cage's Dream (1948) piano composition, the drawn `particles' of the processing sketch became activated through the high notes.

24

Figure 19: Processing / MSA Fluid particle sketch showing movement and colour (Photo credit: Michael Castledine)

Although the Minim API created strong visuals and a generative connection to the music, a centralised dedicated platform was needed to control the quality of sound and allow more varied `generative distortion play'. Ableton Live is a professional software music sequencer and digital audio workstation which was further explored and implemented for this installation. This software with new Touch OSC (Open Source Control) plugin bridged the connection from Processing to Ableton Live and MAX. This allowed mapping elements in Live such as pitch, volume and reverb to the Processing / Kinect environment.

Research in music signification lead to conversations and experiments mapping the y coordinate of the stage. The vertical mapping of pitch in space was initially noticed by C.C Pratt in 1930
25

after observing that the specific succession of tones in a musical phrase can generate a feeling of vertical movement (Pratt, 1930). This notion have since been experimentally demonstrated by Lidji (2007) and (Rusconi, 2006) showing the potential that audible properties can be mapped successfully into spatial representations. While listening to John Cage's Dream (1948), participants observed their vertical arm movements controlled the auditory volume of the track.

Figure 20: Participant engaging with particle animation and movement (Photo credit: Michael Castledine)

26

PROJECT THREE

Project Three titled `Sound Graffiti' is the accumulation of technology and research learning to creating an art installation that uniquely reframes our environment via mixed reality and generative soundscapes. Evoking conversations on digital real estate ownership and trespassing, this platform also hints at a creation of a tool which would enable the transfer and/or extension of one or more senses to enhance another sense which is not active or fully functional.

INSPIRATION / RESEARCH California design studios, Active Theory, following the release of Google's ARCore and Apple's ARkit in 2017, began a series of AR experiments expanding on how the camera could be used as a mechanism for revealing new layers of content. (Fig. 21)

27

Figure 21: Active Theory early AR experiments (Photo credit: Active Theory) The World Brush mobile app was created out of these experiments, allowing users to digitally paint on top of the physical world for others to discover. Every augmented painting is anonymous and only visible where it was created. Geo location mapping now allows users to see other paintings in their area. A self regulating system, with the ability to like, dislike and report paintings, has created new frontiers and conversations in "locative or geospatial technology."

Figure 22: World Brush app screenshots of AR drawings over Project One (Photo credit: Michael Castledine)

Through the treatment and narrative of `geospatial technology', William Gibson's Spook Country tells the story of a character who proposes cyberspace as becoming an integral and indistinguishable part of the physical world, rather than a domain to be visited. The author sketches a picture of invisible (or virtual) geographic annotations--"spatially tagged hypermedia," and mixed reality, the blending of the real and the virtual such that neither can be stabilized as distinct.

28

In 2017, artist Marpi created an AR experiment (Fig. 23) curating an invisible exhibition, blurring the lines between what is real and what is not. The AR experience was live recorded at de Young Museum in San Francisco.

Figure 23: Marpi showcasing invisible AR exhibition (Photo credit: Marpi)

While challenging the idea of what is real, this exhibition was unknown and unannounced to the de Young Museum, provoking conversations and questions. Who has ownership of this digital real estate and how is it going to be monitored in the future? Project Three does not aim to solve or answer this question. Rather it aims to highlight that these new tools and technologies are imposing important conversations regarding this question.

29

METHODOLOGY / SOFTWARE SCHEMATIC DESIGN

Figure 24: Schematic Design

THE TECHNOLOGY Augmented and mixed reality (AR/MR) are some of the most significant digital technologies, especially given their features of immersiveness, engagement live-ness and mobility. The concept of AR in the art realm is not a new conversation, but MR is starting to see tools like the Microsoft Hololens and mobile cardboard based Holokit bring new interactive storytelling to life in unique ways. Mobile phones equipped with monocular cameras and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but lack direct metric distance measurements, posing challenges on the localization of the AR device. This has now been solved with the use of an image or object "trigger" to track and stabilise the augmented location plain. Vuforia is an Augmented Reality Software Development Kit (SDK) for mobile

30

devices. This tool enables artists to position and orient virtual objects in relation to real world images when they are viewed through the camera.

TRIGGERS Using the Unity software platform and Vuforia plugin, experiments with various images and objects were conducted to establish their sensitivity / interactivity as a trigger. The Target Manager for Vuforia's cloud based tool deploys the use of a five star rating system to scan and establish "crosshair points" of reference in the image / object. The more "points", the stronger the rating system and trigger.

Figure 25: Demonstration of Vuforia's trigger technology, 2D image spawns 3D unity model (Photo credit: Michael Castledine)

31

Figure 26: Screenshot of Vuforia Target Manager and 3D object scan (Photo credit: Michael Castledine)

32

Figure 27: Screenshot of Vuforia Target Manager and image rating scan (Photo credit: Michael Castledine)

During this exploration I documented various public installations in the city, making note of the sensory environment and recording sounds, colours, patterns, their position, as well as adjacent surroundings. This further led to exploring soundscape narratives in the those public spaces.

GENERATIVE SOUND SCULPTURES After testing various triggers via the desktop computer camera, new explorations were later deployed as a native iOS phone application. This allowed for more immersive and 360 degree viewing while experimenting with form.

33

Figure 28: Phone screenshots of Union Station musical performance soundscape (Photo credit: Michael Castledine)

Sound, once again, generated the visual form is this installation. The implementation and experimentation of various Unity plugins allowed for both mic input and pre recorded sound to filter through a generative controlled environment. From previous research and learnings, variables such as pitch and acceleration were controlled and allocated through a low, medium and high filter.

34

Figure 29: Live musical performance with generative sound animations (Photo credit: Michael Castledine)

Figure 30: Physical speaker triggers pre-recorded sound reactive animation (Photo credit: Michael Castledine)

35

PSYCHOGEOGRAPHY / INCLUSIVITY Envisioning sound is ordinarily and historically the territory of the creative ability. For centuries, individuals have been exploring different avenues for making sound and vibration visible by way of exciting media like liquids and particles through the phenomenon of cymatics, and the invention of the cymascope and Chladni Plates. (Fig. 31) Today, this research has further led to the creation of mathematical sound visualizers in 3D software such as Unity. Deploying this software on headsets and geolocation enabled phones now allows artists to consider and ask questions such as; how can visualizing the sound of a city contribute to its psychogeography? Can these new conversations and artistic technologies aid in wayfinding and inclusitity approaches? This project does not aim to address these questions, rather it aims to highlight that these new tools and technologies are imposing important conversations for further consideration and research.

Figure 31: Nigel John Stanford's Demo: Chladni Plate - Sound, Vibration and Sand (Photo credit: Nigel John Stanford)

FINAL THOUGHTS "When I hear, I forget. When I see, I remember. When I do, I understand." This old Chinese

36

proverb has more significance and relevance than ever before because to our new capabilities. In Robert Wortman's "Using All the Senses to Learn", he states that "The five senses of hearing, touch, sight, taste and smell are the primary means we use to gain new knowledge. We rarely experience with one sense alone. Our senses work together to give us a total picture of our experiences" (Wartman, 1998). If we are to extend our senses it follows that we extend our learning.

In creating a multi-sensory approach to art installations, the above projects have demonstrated strong crossovers of inputs. They have also, hopefully, initiated many new and important conversations. These conversations included learning, geospatial technology, inclusivity and the arts, psychogeography and sound, synesthesia, and digital trespassing. These projects have also demonstrated exciting new technology media for artists, enabling the reframing of both digital and physical landscapes. The above conversations have in part, been generated by serendipitous opportunity arising from the research for the augmented projects.

37

REFERENCES

Berthaut, F., Marshall, M., Subramanian, S., & Hachet, M. (2013, April 23). Rouages: Revealing the Mechanisms of Digital Musical Instruments to the Audience. Retrieved from https://hal.inria.fr/hal-00807049/en

Cage, John. (2018, July 03). Retrieved from https://en.wikipedia.org/wiki/John_Cage

Harold, Cohen (1995, July 22) The further exploits of AARON, Painter, volume 4, issue 2: Constructions of the Mind. Retrived from: https://web.stanford.edu/group/SHR/4-2/text/cohen.html

Herz, R. (2017, December 09). How a high-pitched tune alters the taste of your food. Retrieved December 11, 2017, from https://www.thestar.com/news/insight/2017/12/09/how-a-highPitched-tune-alters-the-taste-of-your-food.html

Joy, A., & Sherry, Jr., J. (2003). Speaking of Art as Embodied Imagination: A Multisensory Approach to Understanding Aesthetic Experience. Journal of Consumer Research, 30(2), 259-282. doi:10.1086/376802

Leman, M, (2008). Embodied Music Cognition and Mediation Technology. Cambridge MA: MIT Press

38

Levent, N. P. (2017). Multisensory Museum: Cross-disciplinary Perspectives on Touch, Sound, Smell, Memory,... and Space. S.l.: Rowman & Littlefield.

Licklider, J. C. R. (1960, March). Man-Computer Symbiosis. IRE Transactions on Human Factors in Electronics, volume HFE-1, pages 4-11. Retrived from: https://groups.csail.mit.edu/medg/people/psz/Licklider.html

Lidji, P., Kolinsky, R., Lochy, A., & Morais, J. (2007). Spatial associations for musical stimuli: A piano in the head?. Journal of Experimental Psychology: Human Perception and Performance, 33(5), 1189.

McLuhan, M., "Acoustic Space" and "Notes on Burroughs," Media Research: Technology, Art, Communication. Edited with commentary by Michel A. Moos. Australia: G & B Arts, 1997.

McLuhan, M., Agel, J., & Fiore, Q. (2003). War and peace in the global village. Toronto: Penguin Books.

Pratt, C.C (1930). The spatial character of high and low tones Journal of Experimental Psychology. 13 (1930), pp. 278­285

Rusconi E, Kwan B, Giordano B.L, Umilta C, Butterworth B (2006). Spatial representation of pitch height: The SMARC effect. Cognition, 99 (2), pp. 113­129

39

Smalley, Denis. (1995, January). Spectromorphology: explaining sound-shapes. Retrieved from https://www.cambridge.org/core/services/aop-cambridge-core/content/view/ S1355771897009059

Steen, C. (2000, January 19). Visions Shared: A Firsthand Look into Synesthesia and Art. Retrieved from https://www.mitpressjournals.org/doi/pdf/10.1162/002409401750286949

Wartman, Robert. (1998, January). Using All the Senses to Learn. Retrieved from https://www.sd43.bc.ca/District/Departments/LearningServices/SLPResources/ LanguageDevelopmentDisorders/UsingAlltheSensestoLearn.pdf

40

