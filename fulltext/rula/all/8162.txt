INVESTIGATING EMOTION RECOGNITION IN THE FIRST YEAR OF LIFE USING EYE-TRACKING METHODOLOGY by Shira Chava Morrison Segal Bachelor of Science Honours, Queen's University, 2016

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Arts in the program of Psychology Toronto, Ontario, Canada, 2018 Â© Shira C. Segal, 2018

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. I understand that my thesis may be made electronically available to the public.




ii

Investigating Emotion Recognition in the First Year of Life Using Eye-Tracking Methodology Master of Arts, 2018 Shira C. Segal Psychology Ryerson

The ability to recognize facial expressions of emotion is a critical part of human social interaction. Infants improve in this ability across the first year of life, but the mechanisms driving these changes and the origins of individual differences in this ability are largely unknown. This thesis used eye tracking to characterize infant scanning patterns of expressions. In study 1 (n = 40), I replicated the preference for fearful faces, and found that infants either allocated more attention to the eyes or the mouth across both happy and fearful expressions. In study 2 (n = 40), I found that infants differentially scanned the critical facial features of dynamic expressions. In study 3 (n = 38), I found that maternal depressive symptoms and positive and negative affect were related to individual differences in infants' scanning of emotional expressions. Implications for our understanding of the development of emotion recognition are discussed.

Key Words: emotion recognition, infancy eye tracking, socioemotional development



iii

Acknowledgments I would like to express my gratitude to a number of people who have provided support and guidance throughout the completion of this thesis. I would first like to thank my supervisor, Dr. Meg Moulson, for giving me the opportunity to conduct this research, and for advising me through each step of the process. Watching this project come to life over the past 2 years has been an extremely rewarding experience and has continued to foster my love and excitement for developmental research. Your guidance has been instrumental in my learning and growth as a researcher thus far. I would also like to thank my committee members, Dr. Karen Milligan, for her thoughtful insights regarding the design and implementation of this project, and Dr. Lili Ma, for serving as my reader. I would also like to express my thanks to the graduate student, postdoc, and research assistants of the Brain and Early Experiences Lab for their efforts in participant recruitment and data collection, and continued enthusiasm while working together on this project. Finally, I would like to thank my parents, Zindel Segal and Lisa Morrison, and my siblings, Ariel and Solomon. They have each provided unique assistance and encouragement throughout the completion of this thesis, and I am so grateful to have had each of their support.



iv

Table of Contents Abstract ......................................................................................................................... iii Acknowledgments .......................................................................................................... iv List of Tables ................................................................................................................ vii List of Figures ............................................................................................................... vii General Introduction ....................................................................................................... 1 Emotion Discrimination and Preference ....................................................................... 2 Emotion Recognition Categorization, Bimodal Matching, and Social Referencing ....... 5 Eye-Tracking Methodology and Current Studies ........................................................ 11 Study One: Happy/Fear Preference................................................................................ 14 Gaps in the Literature................................................................................................. 18 Contribution of Eye Tracking..................................................................................... 20 The Current Study...................................................................................................... 23 Hypotheses ................................................................................................................ 24 Method ...................................................................................................................... 24 Results ....................................................................................................................... 27 Discussion ................................................................................................................. 32 Study Two: Dynamic Facial Expressions ...................................................................... 38 Dynamic Stimuli in Emotion Recognition .................................................................. 41 Pupillometry in Emotion Processing .......................................................................... 44 The Current Study...................................................................................................... 45 Hypotheses ................................................................................................................ 46 Method ...................................................................................................................... 46 Results ....................................................................................................................... 48 Discussion ................................................................................................................. 55 Study 3: Individual Differences in Emotion Recognition and the Contribution of Experience .................................................................................................................... 61 Maternal Affect.......................................................................................................... 62 Gaps in the Literature................................................................................................. 68 The Current Study...................................................................................................... 69 Hypotheses ................................................................................................................ 69



v

Method ...................................................................................................................... 70 Results ....................................................................................................................... 73 Discussion ................................................................................................................. 78 General Discussion ....................................................................................................... 84 Limitations and Future Directions .............................................................................. 87 Conclusion ................................................................................................................. 89 Appendix A........................................................................................................................90 Appendix B........................................................................................................................92 References .................................................................................................................... 97




vi

List of Tables Study 3 Table 1. Depression Scales Descriptive Statistics..............................................74 Table 2. Positive and Negative Affect Schedule (PANAS) Descriptive Statistics ..........................................................................................................74 List of Figures Study 1 Figure 1. Visual Paired-Comparison Task ......................................................26 Figure 2. Fear Preference Dot Plot...............................................................28 Figure 3. Significant Main Effect of Feature in the VPC Task............................... 30 Figure 4. Fearful and Happy Eyes Scatterplot....................................................30 Figure 5. Fearful and Happy Mouth Scatterplot................................................31 Study 2 Figure 6. Dynamic Emotion Task Interest Areas ................................................48 Figure 7. Looking Time to the Face...............................................................50 Figure 8. Significant Interaction of Emotion and Feature.....................................52 Figure 9. Average Pupil Size.........................................................................53



vii



General Introduction The ability to process and recognize emotions is critical in guiding successful interactions with other individuals (Nelson, 1987). Given the importance of this skill, it is not surprising that its development begins within the first few days after birth and continues throughout infancy and beyond. Learning to orient attention to faces and to process facial expressions are all part of the skill set required by infants for communicating with caregivers and navigating their social environment (Wagner, Luyster, Yim, Tager-Flusberg, & Nelson, 2013). Awareness of others' emotions is also crucial for the development of intersubjectivity (i.e., shared mental experience between two or more individuals), which is a key component of socio-emotional development (Legerstee, 2009). Emotion recognition is a foundational skill that supports decoding social cues and understanding complex social interactions (Nelson & de Haan, 1996). The overarching goal of this thesis is to further our understanding of the mechanisms that support the development of emotion recognition in the first year of life, and to investigate how experience is related to individual differences in this ability. I conducted three studies to investigate these questions. Studies 1 and 2 used eye tracking to 1) examine whether infants' visual scanning of static facial features informs their preferences for different emotions, and 2) characterize infants' scanning patterns of ecologically valid, dynamic facial expressions. Study 3 explored if variables related to infants' emotion exposure (i.e., depressive and anxiety symptoms, and positive and negative affect of the primary caregiver), are related to these behavioural measures. A general introduction is first presented to situate the reader in the current status of the field by introducing a number of techniques used to measure emotion recognition in infancy. I



1

will then go on to describe gaps in our knowledge, before introducing the current studies in more detail. I then present Studies 1, 2, and 3, which are followed by a general discussion to draw connections between them, as well as final conclusions. The first section of this introduction will describe the general developmental progression of emotion processing in the first year of life, focusing on what we have learned about emotion discrimination, emotion preference, and emotion recognition. Emotion Discrimination and Preference Infants are attracted to faces and face-like patterns within hours of birth, suggesting that to a certain degree they are born with sensitivity to human face structure. Newborns prefer face-like patterns compared to scrambled facial features, as inferred by longer time spent tracking the former (Johnson, Dziurawiec, Ellis, & Morton, 1991; Morton & Johnson, 1991), and infants prefer animated, live faces compared to photographs of faces, schematic faces, and non-face geometric stimuli (Sherrod, 1979). They also prefer to look at faces accompanied by a voice compared to still, silent faces (Haith, Bergman, & Moore, 1977). These early preferences potentially set the framework for infants to attend to and learn from other social beings. Although these studies reveal that infants can discriminate and prefer faces to non-face objects, only a few studies have investigated whether newborns can discriminate facial expressions. Field, Woodson, Greenberg, and Cohen (1982) demonstrated that newborns could discriminate and imitate happy, sad, and surprised facial expressions. Farroni, Menon, Rigato, and Johnson (2007) found that newborns less than 3 days old showed a looking preference for a happy face compared to a fearful face, but no preference when a fearful and a neutral face were shown. Thus, infants are able to



2

discriminate some, but not all, emotions. Critically, they also prefer happy facial expressions, potentially reflecting more experience with this emotion during the first few days of life. However, studies with newborns, particularly those that employ photographs, have been critiqued by researchers who question whether newborn infants have the requisite visual abilities (e.g., contrast sensitivity and scanning) to process facial expressions presented in static, two-dimensional stimuli (Nelson, 1987; Walker-Andrews, 1997). Bearing this in mind, the ability of newborns to discriminate facial expressions likely reflects their ability to attend to individual feature information, rather than an integration and understanding of the underlying expression. Newborns' discrimination ability can be seen as a precursor to the recognition of facial expressions, which is an ability that continues to develop throughout infancy and childhood (Nelson, 1987). Newborns display a preference for happy facial expressions (Farroni et al., 2007), which continues through the first half of the first year of life. Kuchuk, Vibbert, and Bornstein (1986) found that by 3 months of age, infants looked longer at smiling compared to neutral faces. This preference was more robust for faces with a more pronounced smile, suggesting that the mouth provided influential visual information to the infants. La Barbera, Izard, Vietze, and Parisi (1976) found that 4-month-old infants looked longer at static expressions of joyful compared to angry and neutral faces when faces were presented sequentially. One recurrent trend in this line of research is that infants' emotion discrimination ability and preferences for specific facial expressions are stronger when emotions are expressed by familiar individuals. Barrera and Maurer (1981) found that 3-month-olds' ability to discriminate between expressions of frowning and smiling was more



3

pronounced when expressions were posed by their mothers rather than by female strangers. Montague and Walker-Andrews (2002) found that 3.5-month-old infants looked longer to facial expressions posed by their mothers compared to similar expressions posed by their fathers and strangers. This finding highlights an interesting link between general face perception and emotion recognition: experience with a face leads to enhanced emotion recognition for that specific face, but not others. Infants first gain experience with the emotional expressions of their primary caregivers and may be more sensitive initially to emotions expressed by those faces (Montague & WalkerAndrews, 2002). Together, the above studies have contributed to the prevailing conclusion that infants as young as 3 months old, and potentially even newborns, are able to discriminate among a limited number of emotional expressions (Walker-Andrews, 1997), and display a preference for happy, the expression that most infants will encounter most frequently. However, it is unclear whether emotion discrimination and preference reflect an understanding of the unique affective meaning of a given emotion, or simply reflect basic discrimination of and preference for facial features that are characteristic of particular, familiar emotions. More advanced processing of emotional expressions likely develops by 5 to 7 months, when infants begin to exhibit more sophisticated abilities with respect to decoding the underlying affective information being portrayed in various emotional expressions. For example, between 5 and 7 months, infants no longer exhibit a preference for happy faces. This preference is replaced by a preference for fearful faces, which has been repeatedly and robustly demonstrated through paired preference paradigms (Amso, Fitzgerald, Davidow, Gilhooly, & Tottenham, 2010; de Haan & Nelson, 1998; Kotsoni,



4

de Haan, & Johnson, 2001; Ludemann & Nelson, 1988; Nelson & Dolgin, 1985; Nelson, Morse, & Leavitt, 1979; Peltola, LeppÃ¤nen, MÃ¤ki, & Hietanen, 2009), in which infants exhibit longer looking time towards fearful faces compared to happy faces. The meaning of this attentional bias has been questioned. For example, studies have attempted to elucidate whether this preference can be reduced to a preference for the novelty of fearful expressions (Peltola, LeppÃ¤nen, Palokangas, & Hietanen, 2008), or whether infants can appreciate the referent nature of a fearful expression (Hoehl & Striano, 2010). While the mechanisms supporting the onset of this preference and its significance remain equivocal, the preference can still be viewed as an index of infants' developing ability to process emotions (LeppÃ¤nen & Nelson, 2012). Emotion Recognition Bimodal Matching, Categorization, and Social Referencing Although numerous studies reveal infants' ability to discriminate between different emotional expressions, it is unclear whether this discrimination ability reflects true recognition Â­ i.e., an understanding of the emotional meaning of the facial expression (Walker-Andrews, 1997). While recognition certainly requires the ability to differentiate between different emotions, it also entails a level of interpretation (i.e., what does that emotional expression mean with respect to me?). Three advanced skills that suggest increasing understanding of emotions (i.e., true emotion recognition) include bimodal matching, categorization, and social referencing. Bimodal matching requires infants to detect a common emotion across visual and auditory stimuli (i.e., faces and voices). In bimodal matching studies, infants view two different facial expressions while hearing one soundtrack that is congruent with only one of the expressions (Walker-Andrews, 1986, 1988; Walker, 1982). Two-month-old infants



5

spent the most time looking at happy expressions, regardless of the vocal expression, whereas 4-month-olds only looked more at the happy expression when the vocal information expressed happiness as well, compared to when the auditory information was incongruent (i.e., happy expression with a sad voice). At 5 and 7 months of age, infants looked longer at whichever facial expression corresponded with the vocal expression, thus demonstrating the ability to recognize and match emotional expressions presented via face and voice. This finding supports the view that towards the second half of the first year of life infants begin to have an increased understanding of the meaning of different emotional expressions. By 5 months of age, infants also demonstrate the ability to identify multiple representations of the same emotion as part of a cohesive category, an ability known as categorization (Bornstein & Arterberry, 2003). In a categorization paradigm, infants are first habituated to multiple exemplars of a particular emotion (e.g., five different females expressing happy), and are then presented with a novel exemplar from the familiar category (e.g., a new female expressing happy) and a different category (e.g., the same new female expressing fear). A looking preference to the different category suggests that infants recognize the novel exemplar from the familiar category as part of the same grouping. Using this paradigm, Bornstein and Arterberry (2003) found that 5-month-old infants could categorize a new happy expression as part of a cohesive category when tested against a fearful expression. This effect was robust even when infants were first familiarized to happy expressions across five different models. Thus, by 5 months, infants show an emerging understanding that the same individual can display a range of emotions and that neither the features unique to the emotional expression or the



6

individual are static and fixed; the infant is able to categorize multiple representations of the same emotion and recognize that they occur separately from the individual who poses them. Kestenbaum and Nelson (1990) demonstrated 7-month-old infants' ability to differentiate happiness from fear and anger using a similar paradigm, and further illustrated that this effect was specific to faces presented upright (whereas they could accomplish basic discrimination when faces were inverted). These findings suggest that infants require more specific information when categorizing emotional expressions than they do when discriminating, such that featural information presented in either orientation is sufficient for discrimination, but categorization requires access to affective information presented in an upright position. The results also demonstrate the increased difficulty of categorization compared to discrimination, such that infants may not be able to handle the cognitive load required to interpret an inverted face and successfully categorize emotions posed by different individuals at the same time. Between 9 and 12 months of age, infants exhibit progressively more sophisticated emotion recognition. Specifically, social referencing Â­ the ability to modify and regulate behaviour according to others' facial expression Â­ emerges around this time. In a classic demonstration, Klinnert (1984) presented 12-month-old infants with an unfamiliar toy and examined how their approach behaviour differed as their mothers posed expressions of happiness, fear, or neutrality. Infants approached their mothers to the closest degree when fear was expressed, moved away when she expressed joy, and stayed at a distance between these two when her expression was neutral. That infants can guide their approach behaviour based on their mothers' expressions can be taken as evidence of



7

further understanding of the meaning of various emotional expressions. In a similar study, 7- and 10-month-old infants were presented with a barking toy dog and placed in a condition where the experimenter either looked in the direction of the infant or looked away from them. Looking behaviour of the infants was recorded at 30-second intervals as the dog barked, revealing that 7-month-olds looked more towards the experimenter after the barks regardless of condition, whereas 10-month-olds only looked more towards the experimenter after the barks when the experimenter was attending to them. These results suggest that by 10 months of age, infants monitor others and refer to them in an ambiguous situation only when others' attention is geared toward the infant. This suggests that infants are selective in whom they refer to, perhaps choosing individuals who would be available to extend help (Striano & Rochat, 2000). More broadly, it also shows an ability to integrate social cues: emotion and caregiver availability. Taken together, bimodal matching, categorization, and social referencing provide evidence of a more sophisticated understanding of emotional expressions. This demonstrates that the way infants learn about and engage with emotions over the first year of life shifts from early displays of discrimination and attentional preference to later demonstrations of an emerging understanding of the affective meaning of these expressions. Methodological Limitations and Gaps in the Literature Summarizing across studies, we can see a general pattern of development that emerges across the first year of life. Newborns preferentially attend to faces and face-like stimuli, which likely prepares them to be attuned to socially-relevant information. Newborns can also discriminate between a limited range of emotional expressions, which



8

at this stage likely reflects attention to individual feature across different emotions. Across the first half of the first year of life, infants display a preference for happy faces -- the expression with which most infants will have the most experience-- until around 5 months of age, at which point this initial preference is replaced by an attentional bias for fearful faces. The fear bias signifies more sophisticated emotion processing, which is thought to develop over the second half of the first year of life, and also includes abilities such as bimodal matching and social referencing. This timeline provides us with a general framework of how emotion recognition progresses across the first year, but a number of questions remain unanswered. The mechanisms driving improvements in these abilities have been largely unexamined. For example, it is unclear whether increased attention to specific facial features operates as a potential mechanism underlying changes in this ability. The attentional bias for fearful faces has been studied using behavioural measures, but the meaning of this attentional shift continues to be debated. The reliance on looking time measures has also precluded a more detailed understanding of how infants learn about emotions, and what specific affective information they attend when presented with different expressions. Regarding our limited knowledge about the specific affective information infants use to learn about emotions, looking time is a measure that is susceptible to other influences of attentional allocation aside from emotion recognition (e.g., preference for specific facial features, novelty of an expression), and provides limited information about the online processing of emotional faces. Looking time paradigms may help us understand that infants prefer to look at certain emotional expressions over others, but we are not able to draw conclusions using this method about what affective information



9

infants are using to explore and learn about facial expressions. Although this method has been most prevalent in the infant emotion perception literature, it is limited in its ability to provide specific information regarding the visual patterns infants utilize when processing affective stimuli. Critically, we also have limited knowledge about how the infant's experience affects the development of emotion recognition. Different theoretical perspectives take different stances on how and when experience is required for the development of emotion recognition. Nelson (1987) put forth a prominent theory of the mechanisms guiding emotion recognition in infancy, which posits that the ability to recognize emotions operates via both experience-dependent and experience-expectant mechanisms. This theory suggests that humans have evolved specific brain regions for processing emotional information, which have their origins early in development, and are biased towards processing affective facial information. Under this viewpoint, early biases towards specific facial expressions (e.g., fear; Nelson & Dolgin, 1985) may represent a "prewired readiness" (LeppÃ¤nen & Nelson, 2009) to attend to these biologically-relevant cues. Thus, the early neural circuitry of the amygdala and related brain regions (e.g., orbitofrontal cortex) may be biased to process emotional expressions, but also requires species-specific exposure to this information to further develop. Atypical exposure to emotional information in childhood, in the form of both early deprivation and physical abuse, has been associated with disruptions in emotion-processing systems, deviations in the ability to recognize emotions, and heightened sensitivity to certain emotions (Fries & Pollak, 2004; Moulson, Fox, Zeanah, & Nelson, 2009; Parker & Nelson, 2005; Pollak, Cicchetti, Hornung, & Reed, 2000; Pollak, Klorman, Thatcher, & Cicchetti, 2001; Pollak



10

& Sinha, 2002). Although these studies reflect the extremes of atypical rearing environments, these findings highlight the role of the early emotional environment in the maturation of this system. Walker-Andrews (1997) also highlighted the role of experience in infants' development of this system, as she proposed that infants first begin to process and learn about emotions when they are presented across multiple modes (e.g., visual and auditory), and that they require experience with these complex presentations before they are able to form abstract representations of these emotions. This interpretation necessitates the role of experience in the maturation and reorganization of the neural systems supporting these functions as infants improve in their ability to recognize unimodal displays of emotion. To evaluate the validity of these different perspectives, a better understanding of the relation between natural variations in experience and emotion recognition ability is required. Eye-Tracking Methodology and Current Studies Eye-tracking methodology affords researchers the opportunity to examine more nuanced questions pertaining to emotion recognition in infancy by measuring specific infant scanning patterns across different emotional expressions. Beyond making global observations regarding infants' looking behaviour, eye tracking provides researchers the opportunity to gather information about gaze location, fixation duration, distribution of fixations, overall scanning patterns, and pupil dilation (GredebÃ¤ck, Eriksson, Schmitow, Laeng, & Stenberg, 2012). These are all measures that might offer important insights into infants' online processing of emotional expressions and allow us to further our understanding of how infants learn about emotions in the first year of life.



11

This thesis will address the above-mentioned limitations by using eye tracking to explore various questions pertaining to early infant emotion recognition, and to further our understanding of how the infants' exposure to emotion may moderate these relations. Study 1 examines infants' scanning patterns towards fearful and happy faces in a pairedpreference task, specifically exploring if the fear preference is driven by increased attention to specific facial features. From a theoretical basis, this question will help us uncover the mechanisms driving this attentional bias, and to examine whether infants' online scanning patterns are related to this outcome measure. Study 2 describes the way in which infants scan a broader range of emotional expressions (i.e., anger, fear, happiness, neutral, and sadness). I also extend this literature by examining dynamic rather than static emotional expressions. Much of our knowledge about emotion recognition has been derived from studies using highly controlled static stimuli, with few studies outlining the ways in which emotion recognition may differ when stimuli offer a more realistic depiction of real-world emotions. This study will describe the visual scanning patterns employed by infants when presented with dynamic emotional faces. Finally, study 3 will investigate the origin of individual differences in scanning patterns in studies 1 and 2, to better understand how experience contributes to the development of emotion recognition. Specifically, this study will examine whether factors that may relate, in part, to the availability or type of emotions infants are exposed to (e.g., maternal mood symptoms and affect) influence infants' scanning of emotional faces. Much of what we know about emotion recognition has been examined from a nomothetic approach; therefore, the ways in which individual differences in this process arise remain largely unknown. Together, the results from these studies will add to our knowledge about



12

infants' online processing of emotional expressions, the specific affective information to which they attend, and the role of experience in shaping these differences.




13

Study One: Happy/Fear Preference Within the field of emotion recognition in infancy, there has been a particular focus on the expression of fear; specifically, understanding how infants learn about fear over the first year of life and why this may be different than learning about other emotions. Fear is seen as biologically adaptive in that it provides information about potential threats in the environment (LeppÃ¤nen & Nelson, 2012). Around the same time that infants demonstrate a robust ability to visually discriminate and categorize facial expressions (i.e., the second half of the first year of life), an attentional bias for fearful faces emerges as well, replacing the earlier attentional bias for happy faces (LeppÃ¤nen & Nelson, 2012). Researchers have posited that the attentional bias may represent an increased understanding of the importance of this emotion (i.e., threat detection), while others have argued that the attentional bias may be attributable to the novelty of the fearful expression and lack of experience with this expression in infancy. A number of different paradigms have been used to examine the attentional bias for fear, which provide us with more information about how infants learn about this expression over the first year of life and can be used to answer questions about the significance of this attentional bias. Visual paired-comparison (VPC) and habituation tasks serve as one way to examine this phenomenon. Nelson and Dolgin (1985) presented 7-month-old infants with 45-second trials of happy and fearful expressions presented side-by-side, and found that infants consistently looked longer to the fearful expressions compared to the happy expressions. This finding was stable across varying models used in different trials; thus, the observed preference for fearful faces was not tied to one individual model's face. In a



14

second experiment by Nelson and Dolgin (1985), infants were presented with three familiarization trials in which identical happy or fearful expressions were presented sideby-side by three different models across trials, followed by a test trial where a fourth model posed the familiar expression on one side of the screen and the novel expression on the other side. Infants exhibited a novelty preference after familiarization to happy faces (i.e., when the fearful face was the novel expression), but not the reverse. The authors speculated that infants might not have looked longer at the novel happy face due to a tendency of infants to look longer at the fearful faces in all conditions, even if they were familiarized to this expression. In this sense, when infants were presented with a novel happy expression in the test trial, their preference to look towards the fearful face may have precluded them from attending to the happy expressions, thus demonstrating a preference for fearful expressions in a slightly different way. Longer looking to fearful than happy or neutral faces in VPC and familiarization/habituation tasks has been replicated numerous times (Amso et al., 2010; de Haan & Nelson, 1998; Kotsoni et al., 2001; Ludemann & Nelson, 1988; Nelson & Dolgin, 1985; Nelson et al., 1979; Peltola, LeppÃ¤nen, MÃ¤ki, et al., 2009), suggesting that this manifestation of an attentional bias for fearful faces is robust by 7 months of age. Another way that the attentional bias for fear has been conceptualized is a difficulty disengaging attention from such stimuli, as opposed to intentionally looking for a longer time. LeppÃ¤nen et al. (2010) used an overlap task to examine infants' ability to disengage attention from various stimuli. In an overlap task, a face is presented in the center of the screen and a target stimulus (e.g., checkerboard pattern) is presented elsewhere on the screen to cue attention towards its location. In the overlap trials, the face



15

stimulus remains on the screen when the target appears, requiring the infant to disengage their attention from the face to attend to the target. In this specific task, infants were presented with a non-face control stimulus, a neutral face, a happy face, and a fearful face. Infants exhibited significantly fewer saccades towards the distractor when presented with the fearful face compared to the other stimuli, demonstrating a difficulty disengaging attention from the fearful expression. Peltola et al. (2008) examined 7month-old infants' ability to disengage from fearful faces using a similar paradigm, finding that infants broke fixation significantly less from fearful faces compared to faceshaped images (used as a control stimulus) and happy faces. Furthermore, they did not find the same pattern of decreased disengagement when infants were presented with a novel face (puffed cheeks), demonstrating a degree of specificity of this finding to fearful faces (not novel expressions in general). Given that the general ability to disengage and shift attention is relatively well developed by this age, these findings support an emotionspecific attention bias towards fearful faces (LeppÃ¤nen & Nelson, 2012). In addition to methods that examine overt visual behaviour, examining infants' neural responses to emotional faces is another way to gain insight into this phenomenon. As mentioned above, Peltola et al. (2009) recorded 5- and 7-month-old infants' looking times to fearful and happy faces, but they also measured event-related potentials (ERPs). ERPs measure the electrophysiological response to the presentation of a specific stimulus. The negative central component (Nc), a negative deflection most prominent over the fronto-central electrodes, is often of interest when examining infant ERPs as it is thought to reflect some aspect of attentional processing (de Haan, 2013). In their study, Peltola et al. (2009) presented infants with happy and fearful faces in a randomized order.



16

Seven-month-old infants displayed a larger Nc component in response to fearful faces compared to happy faces, suggesting a greater allocation of attention to these faces. These findings replicated previous work finding a larger amplitude Nc when 7-month-old infants looked at fearful faces compared to happy faces (de Haan, Belsky, Reid, Volein, & Johnson, 2004; de Haan & Nelson, 1998; Nelson & de Haan, 1996). Furthermore, LeppÃ¤nen, Moulson, Vogel-Farley, and Nelson (2007) extended these findings to show that the differential ERP reponse to fearful and happy faces could be seen across multiple recording sites at different locations (i.e., not just at the central electrodes, but also at posterior and occipitotemporal sites). Overall, results from ERP studies suggest that infants allocate more attention to fearful faces compared to happy faces, dovetailing nicely with behavioural findings demonstrating increased visual attention for fearful faces. Physiological measures (e.g., changes in heart rate) have also been used to examine the attentional bias for fearful faces. Humans and animals both exhibit a rapid deceleration of heart rate when attention is allocated to external stimuli, which is thought to represent an orienting reflex towards the stimulus (Bradley, 2009; Reynolds & Richards, 2008). LeppÃ¤nen et al. (2010) presented 7-month-old infants with neutral, happy, and fearful expressions in the overlap task described above, while electrocardiogram (ECG) was recorded via electrodes placed on the infants' chests. They found cardiac deceleration in response to all expressions, but it was largest for the fearful expressions. These results suggest that emotional expression influences infants' modulation of attention, which may vary based on the specific emotional expression (i.e., stronger for fearful faces). Together, findings from behavioural, physiological, and



17

electrophysiological studies provide converging evidence for an attentional bias for fearful faces that manifests across multiple modes of responding that reflect different levels of processing. This attentional bias was initially viewed as emerging in the second half of the first year of life, since some studies found no evidence of the bias in 5-month-old infants (Bornstein & Arterberry, 2003; LeppÃ¤nen, Cataldo, Bosquet Enlow, & Nelson, 2018; Peltola, Hietanen, Forssman, & LeppÃ¤nen, 2013; Peltola, LeppÃ¤nen, MÃ¤ki, et al., 2009). However, recent studies have suggested that when more ecologically valid dynamic stimuli are used, an attentional bias for fearful faces can be observed at younger ages. For example, when presented with dynamic faces and a competing peripheral checkerboard pattern in an overlap task, 5-month-old infants exhibited enhanced attention (i.e., longer looking times) to fearful faces compared to happy and neutral faces, whereas 3.5-monthold infants did not (Heck, Hock, White, Jubran, & Bhatt, 2016). This suggests that the attentional bias towards fearful faces might emerge earlier than the previously reported window of 5-7 months. Gaps in the Literature When interpreting the meaning of the attentional bias for fearful expressions, it is imperative to keep in mind that this bias has not been established as representing a conceptual understanding of this particular emotion (LeppÃ¤nen & Nelson, 2012). The attentional shift from happy faces to fearful faces that occurs sometime between 3 and 7 months may represent the development of an enhanced emotional understanding of negative emotions, but we cannot conclude this definitively. LeppÃ¤nen and Nelson (2009) posited that the fear bias may represent an experience-expectant mechanism, such that



18

infants may be predisposed to some extent to attend to biologically salient information, but require specific input to develop. Certain emotion-related brain structures that begin to develop in early infancy (amygdala and orbitofrontal cortex) may possess a certain "readiness" to process biologically relevant cues, such as fearful faces, which denote threat. This bias may represent a sensitive period for emotional learning, during which time these brain systems undergo maturation and increase in connectivity with visual processing areas in the prefrontal cortex. Support for this theory comes from the observation that the bias for fearful faces emerges during a time when these expressions may increase in their prevalence in the infant's environment and become more relevant (e.g., as infants gain locomotor ability and more independence from the caregiver, caregivers are more likely to express different emotions). Infants would therefore be receiving increased exposure to fearful facial expressions at a time when their brain "expects" to receive this information. However, it remains controversial whether the fear bias is simply driven by the novelty of the expression, considering that infants generally have less experience with this facial expression (Malatesta & Haviland, 1982). Despite the uncertainty surrounding the interpretation of the attentional bias for fear, it is still a significant index of emotion processing, for two primary reasons. First, the bias might provide a basic framework for learning about this and other negative emotions; thus, although more research is required to investigate whether infants can extract and understand the signal of threat from these expressions, the bias is the first step towards learning about the emotional meaning of negative facial expressions generally, and fearful facial expressions specifically. Second, individual differences in this bias might lead to meaningful variations in more complex emotional processes later in



19

development. For example, differences in this observed bias have the capacity to influence the ways in which children respond to and interact with threat-related stimuli in their environment, alter their sensitivity to such cues, and in some cases, increase one's anxiety vulnerability (Bar-Haim, 2010). Contribution of Eye Tracking Eye-tracking methodology serves as a new avenue by which various questions pertaining to the attentional bias for fearful faces have been examined, as it allows for a more specific investigation into the influence of various facial features and scanning patterns on shaping this preference. Previous studies have delineated the ways in which adults typically scan emotional faces. While the eye region is generally heavily attended to in the decoding of faces (Vinette, Gosselin, & Schyns, 2004) and facial expressions (Scheller, Buchel, & Gamer, 2012; G J Walker-Smith, Gale, & Findlay, 1977), it is thought that facial features vary in their relative importance to recognition depending on the emotion. Studies with adults have found that the eye region is relied upon for the recognition of fearful, neutral, angry, and sad faces, while the mouth region is considered to be critical in the recognition of happy faces (Adolphs et al., 2005; Boucher & Ekman, 1975a; Eisenbarth & Alpers, 2011; Hanawalt, 1944; Scheller et al., 2012; Schurgin et al., 2014; Smith, Cottrell, Gosselin, & Schyns, 2005). Peltola, LeppÃ¤nen, Vogel-Farley, Hietanen, and Nelson (2009) used eye tracking to investigate whether 7-month-old infants' attentional bias for fearful faces is due to a focus on particular features such as the enlarged eyes of a fearful face. They examined infants' visual scanning patterns of images of faces expressing fear, happiness, neutrality,



20

and a neutral face with fearful eyes, finding that infants spent the most time scanning the eyes, regardless of emotional expression. They used an overlap task to compare the ease with which infants disengaged attention from fearful faces compared to a neutral face with fearful eyes. They found that infants were slower to disengage from fearful faces compared to the neutral face with fearful eyes, suggesting that the fear preference cannot be reduced to the presence of salient fearful eyes. In an examination of the specific scanning patterns employed by infants and adults towards threatening emotional expressions, Hunnius, de Wit, Vrins, and von Hofsten (2011) compared infants' scanning patterns to threat-related (anger, fear) and non-threat-related faces (happy, sad, neutral) at 4 and 7 months. They compared these patterns to those of adults, finding an avoidant looking pattern Â­ fewer fixations to inner facial features Â­ to threatening faces in all age groups, and a tendency to avoid eye contact that was specific to adults. Peltola, LeppÃ¤nen, and Hietanen (2011) used eye tracking to examine a classic overlap task with 7-month-olds where either a happy or fearful face (i.e., target) was flanked by a peripheral stimulus after 700ms (i.e., distractor), which remained on the screen for 2000ms. In addition to infants exhibiting a longer latency and less frequent looks to the distractor when the target was a fearful face (i.e., difficulty disengaging), the fear preference also manifested in a novel way: infants were faster to reengage attention towards fearful target faces compared to happy target faces. Speed of re-focusing attention is a more nuanced measure that would not be possible to capture with simpler measures of overall looking time, further highlighting the strength and importance of eye tracking in examining these questions.



21

In another innovative eye-tracking study, Jessen, Altvater-Mackensen, and Grossmann (2016) examined looking patterns when 7-month-old infants were presented with happy and fearful faces at both a supraliminal level and a subliminal level. Expressions in the subliminal condition were presented for a duration that was too short for conscious perception (50ms), whereas expressions in the supraliminal condition were presented for a duration that could be consciously perceived (900ms). They measured the duration of infants' fixations toward the eye and mouth regions, finding that infants spent the most time fixating on the eyes across both expressions and both conditions of consciousness, and the proportion of looking time to the eyes was the longest when faces were presented subliminally. It is possible that when infants have a limited amount of time to scan the available affective information the eye region is the most critical feature in helping to disambiguate the facial expression. Closely related to the current study, Amso et al. (2010) examined how attention towards specific facial features influences infants' learning about happy and fearful emotional expressions. Using eye tracking, they examined proportion of gaze duration to the eye and mouth region in a habituation/discrimination task. Infants were habituated to two alternating models posing the same expression (happy or fear), and then presented with test trials consisting of: 1) a familiar model posing the novel expression, 2) a novel model posing the novel expression, and 3) a novel model posing the familiar expression. They found that infants were only able to discriminate between emotional expressions when the model was held constant, and only when habituated to happy faces. While infants did not exhibit a novelty preference for happy faces when habituated to fearful faces, an examination of infants' gaze patterns revealed that greater looking time to the



22

eye region during habituation was positively correlated with the novelty preference for happy faces. These results suggest that scanning patterns are an active mechanism by which infants learn about emotional expressions, and shed light on the relationship between infants' real-time processing of emotional information and an outcome measure such as discrimination. The Current Study The current study aims to build on our understanding of the mechanisms by which infants learn about emotions, with specific attention to happy and fear. While the attentional bias for fearful faces that emerges during the first year of life has been well documented in numerous studies, our knowledge about the online processing of fear compared to other emotions is limited. The specific affective information infants attend to is still unclear, and it is relatively unknown how infants' visual scanning patterns relate to outcome measures, such as the spontaneous preference for fearful faces. I decided to test 7-month-old infants in this study because infants at this age are expected to reliably demonstrate a fear preference, which will allow us to examine attention to specific facial features in relation to this preference. The current study used the visual paired-comparison (VPC) task to examine infants' preferences when presented with happy and fearful faces. In this commonly used paradigm, infants are presented with two faces side-by-side, and looking time to each face is recorded (Fantz, 1961). I seek to replicate previous findings of a preference for fearful faces, while adding the novel contribution of eye-tracking measures to determine to which elements infants allocate attention when presented with these faces in real time. While numerous looking time studies have found that infants at 7 months of age



23

preferentially attend to fearful faces over happy faces, these studies have not been able to provide an online measure of how infants process these faces and how online processing relates to the overall preference for fearful faces. In addition to the total amount of time spent looking at each face, the current study will examine infants' fixations to various interest areas (IAs), such as the eyes and mouth, to determine which elements 7-monthold infants are using to learn about these emotions and to guide their preferences. Hypotheses I hypothesized that infants would look longer at fearful faces compared to happy faces overall, replicating previous findings. With regard to attention to specific facial features, I expected infants to fixate for a greater duration of time towards the eyes of fearful faces compared to the mouth, and for a greater duration of time towards the mouths of happy faces compared to the eyes. I also predicted that increased scanning of the eye region of the fearful face would be associated with a larger preference for the fearful face. Method Participants. A total of 40 infants participated in this study (M age = 219 days, SD = 31.24, 21 females). Infants were recruited from the Ryerson Infant and Child Database (RIC-D). The sample size was determined by a power analysis run using G*Power 3 software to detect a medium sized effect given the statistical significance criterion of .05 (Faul, ErdFelder, Lang, & Buchner, 2007). The families included in this database were recruited by Ryerson students from local library groups and from the BabyTime Shows in Toronto. Parents were contacted via phone and email, and a detailed script describing the study was used in all recruitment communications regarding



24

participation. Eligible participants were 5- to 12-month-old infants born full term (37-42 weeks gestation). Infants were eligible from the day they turned 5 months to the day before they turned 12 months; however, the oldest infant enrolled in the study was 9 months of age. An additional 10 participants were tested, but their data were excluded because they did not complete the task due to fussiness/inattention (n = 6) and technical difficulties with the eye tracking equipment (n = 4). Exclusion criteria included visual impairment (as reported by the parent). Parents or legal guardians provided written consent for their infants to participate in the study. Given the setting in which participants were recruited, this sample is considered to be low risk. Stimuli and apparatus. The stimuli consisted of eight colour images of four Caucasian female models from the validated NimStim Face Stimulus set expressing happy and fearful facial expressions (Tottenham et al., 2009). The kappa values between given label and intended expression for the models and expressions ranged between 0.73 and 0.97, and the reliability scores of expressions ranged between 1.00 and 0.74 (Tottenham et al., 2009). Half of the infants were randomly assigned to view two of the models (model numbers 3, 5) while the other half of the sample were presented with the other two models (model numbers 7, 9). Infants were presented with four 10-second trials. On each trial, infants saw one model posing fearful and happy expressions side-byside on the computer monitor. The first two trials consisted of one model presenting the expressions, with the left-right position of the expressions switched between trials 1 and 2. The second two trials consisted of the other model presenting the expressions, with the left-right position of the expressions switched between trials 3 and 4. The side on which each emotion was presented in the first trial in a pair, as well as the order in which infants



25

saw the two models was counterbalanced across infants. All images were 715Ã920 pixels. Eye-tracking data was recorded using the remote Eyelink 1000 Plus (Arm Mount, SR Research Ltd., Canada). The arm mount is an adjustable arm that holds a 22'' LCD monitor with the camera (16 mm lens) and illuminator held beneath it. The remote monocular mode was used, as it is designed for use when a chin rest of head mount are not possible, such as for use with infants. The remote mode allows for gaze position to be recorded at a sampling rate of 500 Hz without head stabilization. The gaze tracking range is 32Â° horizontally, and 25Â° vertically, with allowed head movements of 40x40cm at 70cm (horizontal x vertical x depth) without accuracy reduction. Procedure. The Ryerson Research Ethics Board gave approval for all procedures. Participants were instructed to come to the Psychology Research Training Centre (PRTC) at Ryerson University. A researcher accompanied the family to the Brain and Early Experiences Lab, where parental consent was obtained, and the infant was given time to become familiar with the new environment. When the infant and parent(s) indicated that they were ready to participate, the family was accompanied to the eye-tracking room for testing. Parents were instructed to place the infant on their lap, and were situated so that there was an approximate eye-screen distance of 60 cm. The researcher placed a sticker on the infant's forehead to signal a location for the eye tracker to focus. Calibration was conducted to establish the corneal reflection (CR) detection threshold, using a 3-point calibration model with animated calibration targets to focus infants' attention to the targets. Once calibration was successful, the researcher started the trial. Infants were presented with two faces on the screen, with a distance of 10 cm between them. Each face



26

subtended a visual angle of 11.89Â° horizontally, and 17.71Â° vertically, separated by a horizontal gap that subtended 9.52Â°. The images appeared on the screen for 10 seconds. Each infant saw four trials in total, with an animated attention-grabber appearing after each trial to re-focus infants' attention, and to ensure that the subsequent trial began with infants fixating the center of the screen. Results Creation of interest areas and report generation. Interest areas (IAs) were created in Eyelink DataViewer version 2.4. IAs were manually created using a combination of ovals and free hand ellipses to capture features that would not be neatly contained within one of the available shapes. IAs were created to break down each face into various facial features, leading to the creation of 11 different IAs: fear left eye, fear right eye, fear mouth, fear nose, fear face, happy left eye, happy right eye, happy mouth, happy nose, happy face, and screen (Figure 1).

Figure 1. Fearful and happy faces in the visual paired-comparison task showing facial feature interest areas (IAs).


27

For each feature, IAs were created to have the same area across expressions and models (i.e., the area for the mouth and eyes is consistent for fearful and happy faces across all trials). A listing of the areas of all IAs can be found in Appendix A. Once IAs were created, they were uploaded to the respective session trials, and IA reports were generated within DataViewer, which provided the raw dwell time (ms) towards each IA for each trial. To determine whether trials should be excluded on the basis of insufficient looking to either face, the sum of the raw dwell time towards both faces was calculated for each trial (M = 6400.19ms, SD = 1883.40). Trials in which the infant attended to either face for less than 1000ms were excluded from analysis. This led to the exclusion of one trial from one infant. Fear preference. For each trial, the amount of time spent fixating the fearful face was divided by the sum of the time spent fixating both the fearful and happy face in a given trial. This value served as the fear preference for a single trial.

Fearful face dwell time X 100 (Fearful face + happy face dwell time) The fear preference was calculated for each trial, then averaged across all four trials (M = 53.59, SD = 9.59). To calculate whether there was a preference for fearful faces across participants, a one-sample t-test was conducted against chance (50). The t-test was significant, t(39) = 2.36, p = .023, d = .37. Descriptive analyses revealed that 27 of 40 (68%) infants displayed a fear preference (percentage of looking time >50%; Figure 2). Further, a binomial test was conducted, which indicated that the proportion of infants who exhibited a fear preference of .68 was greater than the expected .50, p < .001.



28

100 90 Percentage of Looking to Fearful Face (%) 80 70 60 50 40 30 20 10 0 No (n=13) Fear Preference Figure 2. Groupings of infants who displayed a fear preference (percentage of looking to fearful face > 50% averaged across trials) and those who did not. Scanning of critical features. The percentage of dwell time towards the critical features (i.e., eyes and mouth) was calculated as the amount of time infants spent fixating to the critical feature divided by the amount of time spent fixating to the entire face. This percentage was first calculated for each trial before it was averaged across all four trials to provide a single percentage of dwell time for each feature for both fearful (M eyes = 46.74, SD = 19.42; M mouth = 8.38, SD = 10.27) and happy faces (M eyes = 47.22, SD = 23.74; M mouth = 9.36, SD = 12.02; Figure 3). Yes (n=27)



29

A 2x2 repeated measures ANOVA was conducted on the percentage of dwell time towards the primary critical facial features (eyes vs. mouth) for each emotion (fear vs. happiness). The main effect of feature was significant, Wilk's  = .332, F (1, 39) = 78.45, p <.001, 2 = .67, such that across both emotions, infants allocated a greater percentage of looking time to the eyes than the mouth. Contrary to hypotheses, the interaction between emotion and feature was not significant F(1, 39) = .021, p = .887, 2 = .001, such that infants exhibited similar attention to the eyes of fearful and happy faces. The hypothesized relationship between percentage of looking towards the fearful eyes and the fear preference was not significant when averaged across trials, r(38) = .165, p = .308, nor was it significant when examined on a trial-by-trial basis (ps > .40). To further characterize infants' allocation of attention to features across trials, the relationship between looking to the eyes and mouths of fearful and happy faces was examined. Looking to the eyes of both faces was significantly correlated, r(38) = .71, p <.001, and looking to the mouth of both faces was significantly correlated as well, r(38) = .62, p <.001 (Figures 4-5). These positive correlations suggest that infants who scanned the eyes of fearful faces more than the mouths also scanned the eyes of happy faces more than the mouths, and infants who scanned the mouths of fearful faces more than the eyes exhibited a similar pattern of scanning for happy faces as well.



30

Figure 3. Percentage of time spent scanning the eyes and mouth of fearful and happy faces (N = 40). *p < .05

Figure 4. Relationship between percentage of looking time towards the eyes of fearful and happy faces. Pearson's r = .71, N = 40.



31

Figure 5. Relationship between percentage of looking time towards the mouth of fearful and happy faces. Pearson's r = .62, N = 40. Discussion The goals of this study were to 1) replicate the attentional bias for fearful faces that has been previously documented in 7-month-old infants, and 2) examine whether infants' scanning of critical facial features was related to this outcome measure. As hypothesized, infants exhibited a greater percentage of looking time to the fearful face compared to the happy face, which is concordant with previous studies reporting this attentional bias (Amso et al., 2010; de Haan & Nelson, 1998; Kotsoni et al., 2001; Ludemann & Nelson, 1988; Nelson & Dolgin, 1985; Nelson et al., 1979; Peltola, LeppÃ¤nen, MÃ¤ki, et al., 2009). Multiple theories have been put forth to explain the emergence of this attentional bias. Some have speculated that infants may look longer at this expression because of its relative novelty in the infant's environment (Peltola et al.,



32

2008). Infants typically receive more exposure to happy expressions early in the first year, but may be more gradually exposed to fearful expressions as they gain motor skills, independence, and start to separate more from their caregivers (Malatesta & Haviland, 1982). Others have posited that this preference is related to the biological saliency of this expression, and the bias may reflect a sensitive period in the development of the more general emotion recognition system (LeppÃ¤nen & Nelson, 2009; Nelson, 1987). Specific emotion-related brain areas, like the amygdala, are biased towards processing emotional information with evolutionary relevance (e.g., fear signals threat and potential danger), which may lead to behavioural biases in attending to displays of this emotion. More recently, LeppÃ¤nen et al. (2018) investigated whether this attentional bias represents a larger "threat-oriented" bias by examining whether infants exhibited a similar bias towards angry faces. They found that infants between 7-12 months of age exhibited longer dwell times towards fearful faces compared to happy and angry faces, while 36month-old infants exhibited longer dwell times towards both fearful and angry faces compared to happy faces. This systematic examination of looking towards fearful and angry faces suggests that the bias for fearful faces is developmentally distinctive from a more general bias towards threat-alerting signals, or negative emotions. The other primary goal of this study was to examine the mechanisms that drive this attentional bias; specifically, whether infants' in-the-moment visual scanning of the fearful and happy faces were related their overall preference for the fearful face. I hypothesized that increased attention towards the eye region would be associated with an overall looking preference towards fearful faces, as the eyes have been identified as a critical feature in the recognition of this emotion (Adolphs et al., 2005; Boucher &



33

Ekman, 1975b; Scheller et al., 2012; Schurgin et al., 2014). This hypothesis was not supported. Although infants did exhibit increased looking time to the eyes compared to the mouths of fearful faces, increased looking towards the eye region of the fearful face was not reliably associated with increased looking towards the fearful face overall. While this finding was in contrast to my prediction, it is consistent with previous data suggesting that the attentional bias for fearful faces is not solely limited to visual attention to the fearful eye region. LeppÃ¤nen, Hietanen, and Koskinen (2008) found that adults displayed an enhanced ERP response towards fearful faces compared to neutral faces whether the eyes were presented separately (i.e., eyes without the face) or hidden (i.e., face without the eyes). Similarly, Asghar et al. (2008) reported comparable differential activation of the amygdala in response to fearful than neutral faces when the whole face was presented, the eye region was presented in isolation, and when the eyes were covered. As well, Peltola, LeppÃ¤nen, Vogel-Farley, et al. (2009) found that infants had increased difficulty disengaging attention from fearful faces compared to happy and neutral faces, and, critically, neutral faces with fearful eyes. These findings suggest that the processing of fearful faces is not reliant on the fearful eyes alone, as differential processing of fearful faces (compared to happy and neutral faces) occurs even in the absence of the eye region. The lack of association between attention to the eyes and exhibiting a fear preference may also be explained by a "vigilant" scanning pattern towards threatening faces: Hunnius et al. (2011) found that 4- to 7-month-old infants directed most of their fixations towards the whole face display rather than focusing attention on the inner facial features when scanning threat-related faces. In this sense, while infants may have



34

allocated more attention at the eyes compared to the mouth, their attentional bias may have been directed at the fearful face more generally, rather than driven by attention to a specific feature. Furthermore, I hypothesized that infants would direct their fixations differently towards the happy face: specifically, I predicted increased fixation on the mouth region compared to the eye region. This hypothesis was also not supported, because infants exhibited increased attention towards the eyes of the happy face. Therefore, across both expressions, infants allocated increased attention towards the eye region. This finding is inconsistent with other studies showing differential scanning of the eye and mouth region across different emotions, and specifically, happy and fearful expressions (Boucher & Ekman, 1975b; Scheller et al., 2012; Schurgin et al., 2014; Smith et al., 2005). However, this finding is consistent with results from previous studies with infants (e.g.,Peltola, LeppÃ¤nen, Vogel-Farley, et al., 2009) that reported increased attention towards the eye region across both fearful and happy faces within the same task. This pattern of attention may reflect the tendency to focus on the eyes to gather quick information about the face, regardless of emotional expression (Langten, Watt, & Bruce, 2000; Scheller et al., 2012). Given that my predictions of increased fixation on the fearful eyes than the happy eyes, and the happy mouth compared to the happy eyes were not supported, I sought to further characterize the distribution of fixations across emotions by examining if looking to the eyes and the mouth of the fearful face was correlated with looking to the corresponding features of the happy face. This analysis revealed that infants who looked more to the eyes of fearful faces were also more likely to look to the eyes of happy faces, and infants who looked more to the mouth of fearful faces were also more likely to look



35

at the mouth of happy faces. A similar scanning pattern was identified by Amso et al. (2010) who reported a negative correlation between scanning the eye and mouth regions, and subsequently classified infants as either "eye lookers" or "mouth lookers." This pattern of looking is consistent with previous data suggesting that it is not until 10 months of age that infants distribute their fixations in a "triangular" pattern of facial exploration, which is a scanning pattern typical of adults (Libertus, Needham, & Pelphry, 2007). The infants in this study were younger than 10 months, which may explain their tendency to fixate to a single feature across both expressions. In summary, infants allocated more attention to fearful faces compared to happy faces, which was not driven by increased attention at the fearful eyes. Infants scanned the eye region more than the mouth region in both expressions, but revealed an interesting pattern of scanning across expressions; infants who allocated more attention at the eyes did so across both expressions, and infants who allocated more attention at the mouth did so across both expressions as well. The fear preference is one measure of developing emotion recognition, but a sole focus on this index of emotion processing is a limitation of this study. First, this focus only allows for an examination of infants' scanning of fearful and happy faces. While these two emotions are significant to the fear preference literature, an exclusive focus on happiness and fear precludes our understanding of infants' processing of other emotions. Furthermore, this study used static images, which have been critiqued for their poor depiction of real-world emotional expressions, which are dynamic in nature, and subsequently underestimate infants' emotion processing abilities (Heck et al., 2016). To increase our understanding of emotion perception more generally, an investigation of



36

scanning patterns towards a broader collection of emotions is required, with the use of more ecologically valid dynamic face stimuli.




37

Study Two: Dynamic Facial Expressions Most of the studies to date on infants' processing of facial information and emotional expressions have used static images of faces (Heck et al., 2016; Quinn et al., 2011). However, facial expressions are dynamic in nature; thus, dynamic facial expression stimuli are more ecologically valid, and may produce more reliable results because they mimic infants' experience with emotions in the real world (Dollion, Soussignan, Durand, Schaal, & Baudouin, 2015; Geangu, Hauf, Bhardwaj, & Bentz, 2011; Kim & Johnson, 2013). In particular, the use of static images may lead researchers to underestimate infants' emotional processing abilities, as these images may relay less information than infants have when navigating emotions in their day-to-day experiences. Caron, Caron, and Myers (1985) stated that emotional expressions are behavioural events that unfold over time, and as such, important diagnostic information is likely transmitted in the patterns of change and movement in facial musculature, body, and voice (i.e., diagnostic for accurate emotion judgments). Drawing on the broader face perception literature, infants learn unfamiliar faces at a quicker rate when presented with dynamic stimuli compared to static. Otsuka et al., (2009) observed the ability of 3- to 4-month-old infants to become familiar with unfamiliar faces, finding that they could recognize an unfamiliar face after a 30s familiarization period in the motion (dynamic) condition, and only after 90s of familiarization in the static condition. As well, Quinn et al., (2011) found that infants were more likely to scan the internal vs. external features of the face when faces were presented dynamically and have more difficulty recognizing inverted faces (i.e., show an inversion effect) at younger ages when presented with dynamic faces compared to static



38

faces (3 months vs. 5-6 months). While not impacted on recognition of emotional expression, these studies suggest that motion facilitates aspects of facial emotion recognition and impacts the way infants scan faces. Additionally, 5-month-old infants display overall longer looking times to dynamic compared to static non-emotional faces (Wilcox & Clayton, 1968). Xiao et al., (2014) compared the spatial distribution of fixations towards static and dynamic neutral faces in 3-, 6-, and 9-month-olds, finding that the older infants' fixations were more expansive when viewing dynamic faces compared to static faces (i.e., spanned more of the facial features than for the static presentation, where fixations occurred mostly at the center of the face). They also observed more fixation shifts between various facial features for dynamic faces compared to static faces. Building on these findings, Xiao et al., (2015) also observed that infants who had increased fixation shifting between facial regions also had better facial recognition, but only for dynamic faces. There are two predominant theories regarding why facial motion has a facilitative effect on identity recognition: 1) the supplementary information hypothesis (Bruce & Valentine, 1988; Hill & Johnston, 2001; Knight & Johnston, 1997; Lander & Bruce, 2000; Lander, Bruce, & Hill, 2001; Lander, Christie, & Bruce, 1999) and 2) the representation enhancement hypothesis (Christie & Bruce, 1998; Pike, Kemp, Towell, & Phillips, 1997). The first theory posits that in addition to using the invariant structure of the human face to aid with identity recognition, facial movement also offers additional unique facial information. For example, two individuals' faces will move differently even when performing the same action (e.g., opening their mouth), and this idiosyncratic information provides another cue to aid with recognition. The second theory posits that



39

facial movement helps us build a more robust representation of a given face by facilitating encoding of the facial features. Leo, Angeli, Lunghi, Dalla Barba, and Simion (2018) conducted a recent study designed to compare these hypotheses with 1- to 3-day-old infants. In their first experiment, infants were familiarized to static happy and fearful faces. They were then presented with neutral expressions of the familiar identity (i.e., model from familiarization trials) and a novel identity (i.e., a different model) in the test phase, and demonstrated a preference for the familiar identity rather than a novel identity. In the second experiment, infants were exposed to a similar learning phase of happy and fearful faces, but the stimuli were dynamic (i.e., three frames presented in a loop of increasingly intense expression) instead of static. Thus, by providing faces in motion, infants were provided with more information than the static image alone (i.e., support for the supplementary information hypothesis). In the test phase (identical to experiment one), infants looked longer at the novel identity. The third experiment examined whether presenting the same additional pictorial information from the three dynamic frames, but presenting them such that there was no apparent motion, would contribute to the same recognition advantage. The authors presented infants with the same three frames from experiment two, but without motion. The test phase was identical to experiment two. They found that there was no significant difference in infants' looking times towards the familiar and novel identity. Thus, there was a significant effect of motion on identity recognition (experiment two) that could not be attributed only to the additional pictorial information (experiment three). This provides support for the representation enhancement hypothesis.



40

Dynamic Stimuli in Emotion Recognition Numerous studies have demonstrated more accurate emotion recognition in adults when they view dynamic stimuli (Calvo, Avero, FernÃ¡ndez-MartÃ­n, & Recio, 2016; Cunningham & Wallraven, 2009; Ehrlich, Schiano, & Sheridan, 2000; Harwood, Hall, & Shinkfield, 1999; Recio, Schacht, & Sommer, 2013; Wehrle, Kaiser, Schmidt, & Scherer, 2000. For a review, see Krumhuber, Kappas, & Manstead, 2013). One specific way this has been demonstrated is by showing that adults can recognize subtle facial expressions more easily when they are presented dynamically (Ambadar, 2002; Ambadar, Schooler, & Conn, 2005; Bould & Morris, 2008; KÃ¤tsyri & Sams, 2008). Additionally, there is evidence that dynamic expressions may be perceived as more realistic and of a higher intensity compared to static expressions (Biele & Grabowska, 2006; Cunningham & Wallraven, 2009; Weyers, MÃ¼hlberger, Hefele, & Pauli, 2006; Yoshikawa & Sato, 2008). Many of these studies appear to support the theory of enhanced representation because improvements in recognition are only observed on trials with motion, and not when participants are provided with the equivalent static information from the motion sequence (i.e., still frames of each sequence from a dynamic clip; Ambadar et al., 2005; Bould & Morris, 2008). The facilitative effect of facial motion in emotion recognition has been demonstrated with infant populations as well. Ichikawa, Kanazawa, and Yamaguchi (2014) familiarized 4- to 7-month old infants with a subtle expression of happiness in two different conditions, static and dynamic, and then presented infants with higher-intensity expressions of happiness and anger during the test phase. They found that 6- to 7-month old infants only showed a novelty preference for the anger expression in the dynamic



41

condition. That being said, when Ichikawa and Yamaguchi (2014) measured infants' recognition of a subtle angry expression, infants could not recognize this expression when presented dynamically, suggesting that the facilitative effect of motion may be emotion-specific and related to the infants' developmental stage (as infants could recognize the subtle expression when it was static). Specifically, facial movement may interfere with attending to the emotional information for emotions with which infants of this age have less experience (i.e., anger compared to happiness) thereby disrupting encoding of emotional information. As described in Study 1, Heck et al. (2016) found a facilitative effect of facial movement on the attentional bias for fear, which is generally only observed in infants 7 months and older. They presented infants of 3.5 and 5 months of age with neutral, happy, and fearful dynamic faces in an overlap task with a competing peripheral checkerboard, and measured how long infants looked towards the dynamic faces in the presence of the distractor, with longer looking times denoting an attentional bias. Five-month-old infants displayed increased attention to the dynamic fearful faces compared to the dynamic happy or neutral faces, whereas 3.5-month-old infants did not differentially attend to the three emotions. Thus, the use of dynamic stimuli revealed that the attentional bias for fearful faces emerges earlier than previous studies had reported. Another element of emotional processing that may have been underestimated using static stimuli is categorization, which is largely documented as emerging between 5 and 7 months of age (Bornstein & Arterberry, 2003). Angeli (2015) examined 3-monthold infants' ability to categorize happy and fearful expressions after being familiarized to four different intensities of these expressions. While infants were unable to categorize the



42

fearful faces, they did exhibit the ability to successfully categorize happy expressions, suggesting that dynamic presentation of emotion may facilitate basic emotion processing abilities. Although several studies have now demonstrated a facilitative effect of motion on emotion recognition in infancy, it is unclear how and why facial movement aids with recognition. Eye tracking is an especially appealing method to examine these questions, as it allows for a characterization of the scanning patterns employed by infants when viewing dynamic expressions. An examination of these scanning patterns may reveal important clues about the ways in which facial movement influences how these expressions are perceived. The current study uses this method to investigate these questions. Dollion et al., (2015) used eye tracking to examine developmental changes in infants' scanning of dynamic facial expressions. They presented 3-, 7-, and 12- monthold infants with short movie clips of 3D avatar faces displaying basic emotions (anger, disgust, fear, sadness, happiness, and neutrality). This study was more exploratory in nature, so there were no direct comparisons made between outcome measures using static and dynamic stimuli; rather, the authors compared scanning patterns between age groups. They found that infants exhibited more sophisticated scanning patterns in the second half of the first year of life: The older group of infants exhibited increased looking time towards relevant facial features (e.g., mouth of happy face, upper nose of disgust face), compared to the younger infants, who attended to facial features in a similar manner regardless of emotional expression, and spent more time fixating to external facial features compared to the 7- and 12-month-olds.



43

Pupillometry in Emotion Processing One eye-tracking measure that is of particular interest to the study of emotion processing is pupil diameter, which serves as a proxy for emotional arousal, information processing, and attention. Pupil diameter is considered to be indicative of changes in the autonomic nervous system, which may be mediated by the locus coeruleus (Jessen et al., 2016). The current study will utilize measures of pupilometry as another outcome measure to contribute to our understanding of infant emotion processing. Previous studies have also used eye tracking to examine pupil size in the context of emotion processing. GredebÃ¤ck, Eriksson, Schmitow, Laeng, and Stenberg, (2012) measured the pupil diameter of 14-month-old infants as they were shown images of various emotional faces, finding an increase in pupil diameter when infants looked at fearful faces compared to happy or neutral faces. Furthermore, infants who were primarily cared for by a single parent exhibited a larger pupil size when observing neutral expressions of the opposite parent. The authors suggested that larger pupil diameters indicate increased arousal and attention caused by enhanced emotional processing. Geangu et al., (2011) also measured changes in pupil dilation as a proxy for emotional response and arousal. Six- and 12-month-old infants were presented with video clips of other infants either in distress (e.g., crying), in a positive emotional state (e.g., laughing, smiling), or a neutral state (e.g., babbling). They found that infants of both age groups exhibited increased pupil diameters when exposed to video clips of other infants in a negative emotional state compared to smaller increases in pupil diameter when exposed to video clips of infants displaying positive emotion. Furthermore, the change in



44

pupil dilation lasted for a longer period of time in the negative emotion condition compared to the positive emotion condition. Finally, Jessen et al.'s (2016) examination of subliminal presentation of happy and fearful faces (i.e., 50ms), also included a measure of pupillary response. They found greater pupil dilation in response to happy faces independent of conscious perception, which was discrepant with their prediction that fearful faces would elicit increased arousal. The Current Study Most of our knowledge about the processing of facial expressions is derived from studies using static stimuli in habituation and discrimination paradigms. While these paradigms have been informative in understanding whether infants can distinguish expressions, our knowledge about the selective attention towards specific features that enables infants to recognize these emotional expressions remains unclear. Infants' exploratory scanning patterns of emotional expressions have not been well examined in the literature, especially when presented sequentially outside of a fixed paradigm. Thus, the current study will examine 7-month-old infants' exploratory scanning patterns when presented with dynamic expressions of happiness, fear, sadness, anger, and neutrality. Dynamic stimuli are believed to be more ecologically valid with respect to how well they resemble the emotions that infants encounter in the real world. This study will not examine any specific outcome measures; instead, it will focus on quantitatively describing scanning patterns employed by infants when presented with dynamic emotional expressions. Infants will view each emotional expression twice, presented by two different models, and presented sequentially in a randomized order within each



45

model identity. Pupil diameter, which serves as a proxy for arousal and attention, will also be measured in response to the emotional expressions. Hypotheses The second study was more exploratory in nature, as I aimed to compare the scanning patterns of infants when presented with dynamic expressions of happiness, fear, anger, sadness, and neutrality. I predicted that infants would fixate longer to the eye region of angry, fearful, neutral, and sad faces, and the mouth region of happy faces. With respect to pupil dilation, I hypothesized that infants would exhibit greater pupil dilation in response to the negative emotions (i.e., anger, fear, and sadness) compared to happiness. Method Participants. Participants were the same infants who were recruited to participate in Study 1. However, from the whole sample of 50 infants who were recruited, not all infants completed both studies. A total of 40 infants participated in study 2 (M age = 217 days, SD = 31.60, 25 females). An additional 10 participants were tested, but their data were excluded because they did not complete the task due to fussiness/inattention (n = 4), technical difficulties with the eye tracking equipment (n = 5), or they looked at the face for less than 1000ms for two or more trials (n = 1). The sample size was determined by a power analysis run using G*Power 3 software to detect a small effect size given the statistical significance criterion of .05, based on previous data (Dollion et al., 2015; Faul et al., 2007). Stimuli and apparatus. The stimuli consisted of validated dynamic videos of five Caucasian female models expressing neutral, happy, fearful, angry, and sad



46

emotional expressions from the Amsterdam Dynamic Facial Expression Set (ADFES; van der Schalk, Hawk, Fischer, & Doosje, 2011). Emotion recognition scores for the stimuli were examined in terms of raw accuracy (%) and unbiased hit rates (Hu) for each emotion collapsed across models: anger (92%, Hu=0.87), joy (95%, Hu=0.81), fear (87%, Hu=0.80), and sadness (90%, Hu=0.79). In each video, the model begins with a neutral expression, shifts to the peak intensity of the target expression by 500ms, and remains at this peak intensity for another 5000ms. Thus, each video lasted for 6 seconds. All models were facing forward in these videos. Infants were presented with two models displaying each of the five emotional expressions for a total of 10 trials. Expressions were presented in a randomized order within each model identity, and exposure to each model was counterbalanced across participants. Eye-tracking data were recorded using the remote Eyelink 1000 Plus (Arm Mount, SR Research Ltd., Canada) as described in Study 1. Pupil diameter was also measured by the Eyelink 100 Plus, which has the capacity to capture up to 2000 samples per second. An ellipse fitting pupil detection model is used in the remote mode. Pupil diameter measurements are in the range of 400-16000 units. This measurement is noise-limited, with pupil size resolution of 0.2% of the diameter, which corresponds to a resolution of 0.01mm for a pupil that is 5mm. Procedure. The Ryerson Research Ethics Board gave approval for all procedures. Infants participated in this study either before or after study 1, following a similar procedure. The order in which the two eye-tracking tasks was presented was counterbalanced across participants, such that the effects of fatigue and boredom on looking behaviour did not disproportionately affect one of the two tasks. Parents were asked whether they would like to take a short break between tasks (e.g., walk around with



47

their infant, play with a toy), but were otherwise instructed to remain seated with their infant on their lap while calibration was performed again (as per Study 1 procedure). Following calibration, infants were presented with one video on the screen at a time, which played for 6 seconds. Each face subtended a visual angle of 10.48Â° horizontally, and 12.84Â° vertically. Each infant was presented with 10 trials in total, with an animated attention-grabber appearing after each trial to re-focus infants' attention. To examine whether trials should be excluded on the basis of insufficient looking towards the face, the raw dwell time to the face IA was examined across each trial for each participant. Any trials where the infant spent less than 1000ms scanning the face region were excluded from analysis. This resulted in the exclusion of 9 trials across 8 participants. Results Creation of interest areas and report generation. Interest areas (IAs) were created in Eyelink DataViewer version 2.4. IAs were manually created using a combination of rectangles, ovals, and free hand ellipses, to capture features that would not be neatly contained within one of the available shapes. The 10 IAs created were: left eye, right eye, lower nose, upper nose, between the eyebrows, mouth, neck/shoulders, hair, face, and screen (Figure 6).



48

Figure 6. Interest areas (IAs) for model 1. IAs were created so that the area of space for each feature was the same across all expressions for the same model (e.g., area for the mouth region was consistent across all expressions for model 5, but differed from the IAs of model 3). A listing of the area of all IAs can be found in Appendix A. Once IAs were created, they were uploaded to the respective session trials, and IA reports were generated within DataViewer. These reports specified the raw dwell time of fixations (ms) for each IA within the trial period. Considering that raw dwell time is a less meaningful metric than a percentage of looking time compared to the total time spent scanning the face, a Matlab script was employed to divide the dwell time of each feature IA by the face IA (i.e., out of the total dwell time to the face, what percentage of that time was spent fixating various features?).



49

Looking to the face across trials and emotions. The subsequent statistical analyses were conducted using IBM's Statistical Package for the Social Sciences (SPSS) version 24. The average looking time to the face collapsed across all trials and emotions was 4429.38ms (SD = 1262.44). A one-way ANOVA was conducted to examine whether attention waned across the 10 trials. Most trials had comparable looking time to the face, with only one significant difference in looking time between trial 2 (M = 4622.19) and 10 (M = 4127.17; p = .047). In addition, I conducted a linear trend analysis, which was not significant (p > .15), indicating that looking to the face did not decrease linearly over time. I concluded that attention did not taper off during the later trials, and therefore did not include stimulus order as a factor in the following analyses. Trials were then sorted by emotion to examine whether there were differences in looking towards the face by emotion. Looking towards the face ranged between 4219.34ms and 4751.22ms across expressions. Infants spent significantly more time scanning fearful faces (M = 4751.22, SD = 1094.58) than neutral (M = 4219.34, SD = 1156.20, p = .001), sad (M = 4265.27, SD = 1361.03, p = .001), and angry faces (M = 4400.60, SD = 1180.02, p = .012), but not happy faces (M = 4612.65, SD = 1389.65, p > .40). They also scanned happy faces for significantly more time than neutral faces (p = .024; Figure 7).



50

Figure 7. The duration of time (ms) for which infants scanned the face region of the different emotional faces. *p < .05 Percentage of dwell time towards features. A 2x5 repeated measures ANOVA was conducted on the percentage of dwell time towards the critical facial features (eyes vs. mouth) for each emotion (anger vs. fear vs. happiness vs. neutral vs. sadness). Mauchly's test indicated that the assumption of sphericity had been violated, so the Greenhouse-Geisser correction was applied. The main effect of emotion was significant, F(3.05, 156) = 2.82, p = .041, 2 = .07. Across both features, infants exhibited a greater percentage of dwell time towards happy faces (M = 22.24, SD = 10.31) compared to angry (M = 18.12, SD = 7.92, p = .018), fearful (M = 19.06, SD = 7.30, p = .043), neutral (M = 18.61, SD = 5.74, p = .031), and sad faces (M =18.37, SD = 7.73, p = .019). There were no other differences between



51

emotions (ps > .40). Although infants exhibited the greatest dwell time towards fearful faces in general, as indicated above, they spent the greatest percentage of their dwell time on the critical features of happy faces. There was also a main effect of feature, F(1, 39) = 11.77, p = .001, 2 = .23. Across all five emotions, infants exhibited a greater percentage of dwell time to the eyes (M = 25.33, SD = 11.0) than the mouth (M = 13.22, SD = 13.76). The expected interaction between emotion and feature was also significant, F(2.87, 156) = 18.38, p <.001, 2 = .32. (Figure 8). Post-hoc analyses revealed that infants exhibited a greater percentage of dwell time to the eyes of angry (M = 28.21, SD = 14.19), fearful (M = 26.55, SD = 13.37), neutral (M = 29.46, SD = 11.59), and sad faces (M = 23.67, SD = 12.80) compared to happy faces (M = 18.78, SD = 12.65 ps <.05). Infants also exhibited a greater percentage of looking time to angry and neutral eyes compared to sad eyes (p = .030, and p = .001 respectively). For the mouth feature, infants exhibited a greater percentage of dwell time to the mouth of happy faces (M = 25.70, SD = 25.37) compared to angry (M = 8.04, SD = 15.12), fearful (M = 11.56, SD = 16.45), neutral (M = 7.75, SD = 12.89), and sad faces (M = 13.06, SD = 17.49, ps <.001). Infants also exhibited a greater percentage of looking time to fearful compared to neutral mouths (p = .049). Within each emotion, infants exhibited a greater percentage of dwell time to the eyes than the mouth for angry (p <.001), fearful (p = .001), neutral (p <.001), and sad (p = .015) faces. There were no significant differences in looking to the eyes or mouth for happy faces (p = .210).



52

35 * 30 Percenatage of looking time 25 20 15 10 5 0 Anger Fear Happiness Emotion Neutral Sadness Eyes Mouth * * *

Figure 8. Percentage of looking time towards the critical features (eyes, mouth) for each of the five emotions. (N = 40). *p < .05 Infants' duration of looking towards the region between the eyebrows across emotions was also examined. This feature was not included in the main ANOVA as this region is not considered to be critical for emotion recognition, but has still received attention in the literature for its importance to facial expressions (Ekman & Friesen, 1978) and face recognition (Sadr, Jarudi, & Sinha, 2003). Thus, this analysis was more exploratory and conducted separately to preserve the power of the primary analysis. I conducted a one-way ANOVA on the percentage of dwell time towards the between-theeyebrow region across all five emotions. Mauchly's test was not significant, indicating the assumption of sphericity had not been violated. The appropriate statistic for sphericity assumed was therefore used. The main effect of emotion was not significant, F(4, 156) =



53

.55, p = .702, 2 = .32. Infants exhibited the greatest percentage of dwell time towards the between-the-eyebrow region of angry faces (M = 4.21, SD = 6.41), but this was not significantly different than the percentage of dwell time towards this feature of fearful (M = 3.68, SD = 5.20), happy (M = 2.93, SD = 4.42), neutral (M = 3.75, SD = 5.41), or sad faces (M = 3.05 , SD = 5.39). Pupil size. A one-way ANOVA was conducted the compare pupil size in response to the different emotions (anger, fear, happy, neutral, and sad). Mauchly's test indicated that the assumption of sphericity had been violated, so the Greenhouse-Geisser correction was applied. Contrary to my hypotheses, the expected main effect of emotion was not significant F(4, 87.66) = 2.10, p = .124, = .05, indicating that infants' exhibited similar pupil sizes across expressions (Figure 9). 285 280 275 270 Mean Pupil Size 265 260 255 250 245 240 235 Anger Fear Happiness Emotion Neutral Sadness

Figure 9. Average pupil size collected during the trials of each of the five emotions (N = 40). There were no significant differences between emotions.



54

Discussion The goal of the current study was to investigate 7-month-old infants' scanning patterns of dynamic emotional expressions. Infants were sequentially presented with angry, fearful, happy, neutral, and sad dynamic faces, while an eye tracker recorded the duration of gaze towards various facial features. The main hypothesis was supported, such that infants exhibited differential scanning of emotional expressions in a manner that reflected preferential attention towards their critical features. Infants spent a greater percentage of time scanning the eye region than the mouth region of angry, fearful, neutral, and sad faces, and a greater percentage of time spent scanning the mouth region of happy faces than the other emotions. Although the extant literature on processing of dynamic facial expressions of emotion is limited, these findings are in line with previous data suggesting that by 7 months of age, infants exhibit scanning patterns that take into account the critical features of dynamic emotional expressions. Dollion et al. (2015) found that while 3-month-olds infants exhibited similar scanning patterns across multiple expressions, 7-month-olds exhibited differential scanning of facial features across expressions (e.g., increased fixations towards the eyes of fearful and angry faces, and increased fixations towards the mouth of happy faces). Similarly, adults typically allocate attention towards different facial features depending on the emotion. Previous data suggests that the eyes are a critical feature in the recognition of fearful, neutral, angry, and sad faces, while the mouth region is more critical in the recognition of happiness (Adolphs et al., 2005; Boucher & Ekman, 1975b; Eisenbarth & Alpers, 2011; Hanawalt, 1944; Scheller et al., 2012; Smith et al., 2005). While these studies have been informative in identifying general critical



55

features, most have relied on static facial images, and have been conducted with adults. Therefore, much of our knowledge about the scanning of dynamic emotional expressions in infancy remains unknown. The finding of differential allocation of attention towards these same features of dynamic expressions seems to suggest that by 7 months of age, infants are sensitive to critical facial features and scan dynamic expressions in a manner consistent with how adults scan static expressions. Previous studies using dynamic facial expressions have reported mixed evidence regarding whether facial movement facilitates infants' recognition ability. For example, Ichikawa et al. (2014) found that 6- to 7-month-old infants were only able to recognize a subtle happy expression when it was presented dynamically, rather than statically. In contrast, when Ichikawa and Yamaguchi (2014) measured infants' recognition of a subtle anger expression, they found that infants could not recognize this expression when presented dynamically. Thus, it is possible that facial movement sometimes enhances infants' recognition of emotion, and other times, disrupts encoding of emotional information, especially for emotions with which infants have less experience (e.g., anger). Although the current findings do not speak directly to the discourse surrounding emotion recognition, they suggest that infants display scanning patterns that are consistent with encoding the necessary information required for recognition when facial expressions are presented dynamically. However, future work is necessary to examine the relationship between these scanning patterns and emotion recognition. Collapsed across all emotions, infants in this study allocated more attention to the eye region compared to the mouth region of dynamic faces. These results are consistent with the literature on critical features of faces more generally, both in infancy and



56

adulthood. By 2 months of age, infants spend increased time attending to the eyes over all other facial features (Maurer, 1985) and infants prefer to look at pictures of people with open eyes compared to closed eyes (Batki, Baron-Cohen, Wheelwright, Connellan, & Ahluwalia, 2000). In adults, up to 70% of face fixations are directed towards the eyes, with the rest of their looks directed at the mouth and nose (Walker-Smith, Gale, & Findlay, 1977). Eyes are a salient facial feature, and have been suggested to provide critical information that requires quick and efficient processing (e.g., direction of social gaze can be used to decipher important cues related to object location, and may provide information regarding one's mental state; Emery, 2000; Langten et al., 2000), irrespective of the emotional expression portrayed (Scheller et al., 2012). The results suggest that sensitivity to the eyes as an important information source emerges early in development and is robust across dynamic expressions. Although I did not have specific predictions regarding fixations towards the eyebrow region, this facial feature has received attention in the face recognition literature. For example, Sadr et al. (2003) found that adults had poor performance recognizing familiar faces in the absence of eye brows, which was a greater disruption than having the absence of eyes. The eyebrows have also been identified as a critical feature in the emotional expressions of happiness, surprise, and anger (Ekman & Friesen 1978). In the current study, there were no differences in infants' attention towards this feature between different emotions. However, the interest area was defined as the region between the eyebrows, which differs from the eyebrow region itself; thus, the current study may not have defined the specific part of the eyebrow that is directly relevant to emotion recognition, which may account for the lack of observed differences.



57

Infants in this study also spent a greater percentage of time scanning both the eyes and the mouth of happy faces compared to these features in all other expressions. This result may be related to most infants' increased exposure to happy expressions in early infancy (Farroni et al., 2007; Malatesta & Haviland, 1982). Theories about the role of experience in the development of emotion recognition propose that greater exposure to a specific emotion category may enhance infants' perceptual representations of that expression, and may lead to heightened sensitivity to this expression (LeppÃ¤nen & Nelson, 2009). While this has been largely examined in the context of increased exposure to anger in the case of physically abused children (Pollak, Cicchetti, Hornung, & Reed, 2000; Pollak, Klorman, Thatcher, & Cicchetti, 2001; Pollak & Sinha, 2002), the current results may reflect a similar process in typical environments, whereby exposure to happiness is likely greater compared to other expressions, and may increase infants' sensitivity to this emotion. This result may also be related to the general finding that by early childhood, happiness is recognized more accurately than other expressions (Brechet, 2017; Durand, Gallay, Seigneuric, Robichon, & Baudouin, 2007; Gao & Maurer, 2010). Enhanced attention towards and processing of the features of a happy face may facilitate the development of the recognition bias observed in childhood, but the broader context in which infants are presented with this expression may be important to consider as well. Exposure to happy faces during face-to-face interactions with caregivers might allow infants to learn about this expression more readily, as happy expressions may be more often associated with positive consequences. However, this is speculative, and future studies are required to provide direct evidence of the relationship between scanning patterns of emotional faces in infancy and recognition of emotions in childhood.



58

In addition, the influence of emotion exposure in the infant's environment on subsequent emotion processing requires further study. Another goal of the study was to investigate differences in infants' arousal in response to various emotions, as measured by pupil dilation. Pupil diameter has been previously used as a measure of attention and arousal, as it is thought to signal changes in autonomic nervous system functioning, mediated by the locus coeruleus (Jessen et al., 2016). Previous studies of emotion recognition in infancy using this measure have generally reported greater pupil dilation in response to negative emotions compared to positive emotions (Geangu et al., 2011; GredebÃ¤ck et al., 2012; Hepach & Westermann, 2013), with the exception of Jessen et al. (2016) who reported that infants exhibited the greatest pupil size in response to happy faces compared to fearful faces. The current study, however, did not report any significant differences in average pupil size between expressions. There are a number of differences between the methodology of the current and previous studies, which may account for these differences. Most previous studies have dichotomized positive and negative emotions, such that infants were only presented with two different conditions (e.g., happy vs. fear, or positive vs. negative). In the current study, there were five different conditions, which included multiple different negative emotions (i.e., anger, fear, and sadness). One possibility is that the differences in sympathetic response and arousal between anger and fear, or anger and sadness may be too nuanced for the measure of pupillometry, which would account for the lack of differences observed in the current study between negative emotions. Furthermore, faces were presented dynamically in the current study, whereas most of the previous studies used static stimuli, with the exception of Geangu et al. (2011). In their study, 6- and 12-



59

month-old infants were presented with a 50-second video of general positive and negative affect of other infants, which included audio (e.g., of a baby crying in the negative condition). In contrast, infants in the current study were presented with 6-second video clips of adults posing emotional expressions without sound. The differences in duration of stimulus presentation, mode of presentation, and model (e.g., infants versus adult) further complicate comparisons across studies. Further research is required to elucidate the mixed findings present in the literature. To summarize, the results from this study suggest that by 7 months of age, infants differentially allocate attention to facial features depending on the expression. Infants scanned the eye region more when presented with angry, fearful, neutral, and sad expressions, and the mouth region of happy faces compared to other expressions. Across features, infants allocated the most attention to happy faces, and across emotions, infants allocated more attention to the eyes than the mouth. I did not find differences in infants' arousal in response to different emotions, as measured by pupil dilation. The results of the current study, combined with the results of Study 1, broaden our understanding of emotion processing by shedding light on the facial features that capture infants' attention, and how this differs across emotions. While these findings may be indicative of the general emotion processing abilities of a 7-month-old infant, they do not examine the magnitude of individual differences in these abilities or the origins of such differences. A more thorough understanding of the development of these abilities requires an examination of the observable differences between infants, and how these differences may relate to variations in the infants rearing environment.



60

Study 3: Individual Differences in Emotion Recognition and the Contribution of Experience Emotion processing in infancy has been largely examined through a nomothetic approach, which has asked questions related to at what age do infants generally exhibit particular skills relevant to emotion recognition (e.g., discrimination, bimodal matching, etc.). The focus has primarily been on examining and comparing groups of infants to examine general trends across age. While this approach has allowed us to delineate a general timeline of various emotional processing abilities in infancy, it has also occluded our understanding of individual differences in these abilities. There are a number of potential contributors that may help shape the development of emotion processing in infancy. Infants' exposure to different emotions, in their home environment and when interacting with their caregivers, may be one contributing factor to differences in infants' emotion recognition abilities. As mentioned in the general introduction, emotion recognition abilities are thought to develop through both experience-dependent and experience-expectant mechanisms (LeppÃ¤nen & Nelson, 2009; Nelson, 1987). While the foundations for emotion processing may be laid down in emotion-related brain regions that are prewired to attend to biologically relevant stimuli (e.g., the amygdala), continued exposure to emotional expression input is believed to aid with the maturation and shaping of this system. Evidence for the importance of experience can be drawn from studies of neglect and maltreatment, which examine how emotion recognition differs when children do not receive typical input. For example, children who are physically abused are more likely to falsely attribute anger to other emotional expressions (Pollak et al., 2000), exhibit larger



61

amplitude ERPs towards photographs of angry faces (Pollak et al., 2001) and are able to recognize anger more accurately than non-abused children in the absence of detailed perceptual information (Pollak & Sinha, 2002). These findings collectively suggest that atypical rearing environments may alter the ways in which children perceive certain expressions. Studies of previously and currently institutionalized children support this notion as well, as they provide evidence for decreased accuracy (Fries and Pollak, 2004) and differences in ERP responses (Moulson, Fox, Zeanah, & Nelson, 2009; Parker & Nelson, 2005) in emotion recognition tasks in these populations compared to familyreared children. Maternal Affect1 While the above adverse conditions highlight the ways in which experience can influence emotion recognition abilities, there is also evidence for altered emotion recognition in more commonly encountered contexts of atypical exposure, such as variations in maternal affect. A critical source of emotional exposure comes from interactions with caregivers. Infants spend 25% of their time exposed to faces (Sugden, Mohamed-Ali, & Moulson, 2014), which provides ample opportunity to view facial expressions of emotion. For example, Malatesta (1985) estimated that between 3 and 6 months of age, infants are


1

In much of the parent-infant interaction literature, there is a conflation between the terms "primary caregiver" and "mother/maternal," as a vast majority of these studies are on mothers exclusively. Acknowledging this problematic conflation, I will use these terms relatively interchangeably, noting that mothers may not always be the primary caregivers in certain family structures.



62

typically exposed to 32,000 contingent expressions of emotion. There is natural variation in the range of affect displayed by different caregivers with some tending towards more positive affect, and others displaying more negative affect (Belsky, Crnic, & Woodworth, 1995). At the extreme ends of the distribution would be mothers who are struggling with mental health difficulties, such as depression or anxiety. Disorders of this nature can impact the general emotional environment of the infant, as previous studies have reported differences in the quantity and quality of communication (via face, voice, and touch), as well as affective behaviour more generally, in mothers with depression compared to nondepressed mothers (Downey & Coyne, 1990; Field, 1995; Weinberg & Tronick, 1998). Considering that infants as young as 3 months old are sensitive to the affective states of their mother and are able to detect depression in their mothers (Weinberg & Tronick, 1998), it is important to understand the potential downstream effects of these more common variations in emotional exposure on the development of emotion recognition abilities. The influence of maternal depression on emotion recognition in childhood and adolescence has been examined previously, with one study finding that 9- to 14-year-old girls of mothers with recurrent depression have more difficulty recognizing subtle sad expressions and require greater intensities of the emotion for accurate identification (Joormann, Gilbert, & Gotlib, 2010). Findings like these highlight the importance of understanding whether the origins of such deficits are rooted in infancy. Depression. The national prevalence of postpartum depression among Canadian women has been estimated to be 8% (Lanes, Kuk, & Tamim, 2011), indicating its relevance for study. Infants of depressed mothers have been found to look away from



63

their mothers during face-to-face interactions, which may reflect a pattern of looking away from sad information (Boyd, Zayas, & McKee, 2006). This pattern has also been observed when non-depressed mothers have been instructed to make a sad expression (Termine & Izard, 1988). For example, Cohn and Tronick (1983) examined infants' behaviour while they interacted with their mothers, who were instructed to pose a depressed affect (i.e., monotone speech, flat affect, minimal body movement and touching the infant). In this simulation of an interaction with a depressed caregiver, infants exhibited more looks away from the caregiver compared to infants in the "normal interaction" condition. Termine and Izard (1988) similarly had non-depressed mothers interact with their infants while facially and vocally expressing sadness. Infants in this condition were also found to avert gaze more frequently in the sadness condition compared to a joy-induction condition, as well as exhibit more sadness and anger themselves. The results of these studies add further support to the observed trend of infants allocating attention away from sad stimuli. While this finding is indicative of the way infants respond to the specific expression of sadness, it may also shed light on differences in emotion exposure more generally in the case of infants with depressed parents. If these infants are allocating attention away from their caregivers, who are a primary source of emotional information, they may be missing out on other opportunities for emotion exposure, which may interfere with their learning about different expressions more generally. Hernandez-Reif, Field, Diego, Vera, and Pickens (2006) found that 3-month-old infants with depressed mothers tended to be slower at habituating to happy facial expressions, and were only able to discriminate happy from sad faces when sad expressions were used as the



64

habituation stimuli. Slow habituation may suggest that infants with depressed mothers have more difficulty paying attention to or perceiving facial expressions in general. This finding is in line with the previous research; if infants avert gaze from depressed caregivers, they may have less exposure to emotional expressions, and may therefore have difficulty perceiving different expressions. Furthermore, the discrimination order effect suggests that sadness may not be a novel expression for these infants. Similarly, Forssman et al. (2014) sought to examine the influence of maternal stress and depression on the development of 5- to 7-month-old infants' attention to emotional cues. Stress and depression were measured using self-report questionnaires, including the recent life events questionnaire (Brugha, Bebbington, Tennant, & Hurry, 1985) and the Edinburgh Postnatal Depression Scale (EPDS; Cox, Holden, & Sagovsky, 1987), while infant attention was measured using an attention disengagement paradigm designed by Peltola et al. (2013). Infants were tested on their ability to disengage attention from a central emotional stimulus (i.e., happy, fearful, or neutral face) to focus on a laterally presented checkerboard pattern. Greater maternal stress and depressive symptoms were associated with greater difficulty disengaging from fearful expressions, suggesting that the infant's emotional environment, and caregiver affect more specifically, may contribute to the development of attentional biases towards negative emotional information. This finding conflicts with the previously mentioned attentional bias away from negative stimuli, which may be related to differing negative expressions (i.e., sad vs. fearful). The parallel literature of children with depressed mothers also supports an attentional bias for sad faces, but with equivocal findings about the direction of this bias.



65

Previous studies have reported an attentional bias towards sad faces, such that children of depressed mothers exhibit selective attention for sad faces compared to other expressions (Joormann et al., 2010; Kujawa et al., 2011; Owens et al., 2016). In contrast, an eyetracking study of children with depression themselves found that these children exhibited an avoidance of sad stimuli, such that they exhibited increased looking towards happy faces and decreased looking towards sad faces (Harrison & Gibb, 2015). Variation in genes related to HPA axis reactivity has been found to moderate this relationship and offers one explanation of the mixed findings (Owens et al., 2016). To explain the differences in attentional allocation towards sad stimuli between infancy and adulthood, Owens et al. (2016) posited a sequence of emotion regulation strategies to explain the observed behavioural trends, based on Gross' (2014) theory of emotion regulation. He suggests that sad stimuli may be perceived as distressing to at-risk infants with a depressed mother, so they might avert attention away from these stimuli as a method of self-soothing. Similar patterns are observed in children with familial risk of depression, which may stem from a similar attempt to use attentional allocation to regulate their mood, but with less alleviation of distress (as the capacity for processes like rumination may be developed at this time, which could interfere with attention disengagement). During adulthood, gaze aversion may be too difficult to use as a strategy of emotion regulation, which accounts for the attentional bias towards negative stimuli that has been observed in adults with various forms of depression (for a review, see Peckham, McHugh, & Otto, 2010). The differences in attentional biases towards negative emotional stimuli, as related to maternal depression, among various stages of development further highlight the importance of examining this phenomenon in infancy.



66

It is important to understand the origins of these biases, as there is preliminary evidence that the observed biases in childhood may differ during infancy. Anxiety. Recognition of threat-related faces, such as fearful and angry faces, has been another avenue of interest in the context of emotion exposure, and has often been examined in the context of maternal anxiety. However, previous studies in different literatures have found conflicting results. As mentioned in study 1, the typically developing infant literature suggests that an attentional bias towards fearful faces develops between 5-7 months of age (LeppÃ¤nen et al., 2010, 2007; LeppÃ¤nen & Nelson, 2012; Peltola, LeppÃ¤nen, MÃ¤ki, et al., 2009; Peltola et al., 2008) and is considered to be developmentally normative. Further, the development of this bias has been associated with positive outcomes, such as greater biases predicting secure attachment at 14 months of age (Peltola, Forssman, Puura, van Ijzendoorn, & LeppÃ¤nen, 2015), and has been found to be correlated with maternal positive affect (de Haan et al., 2004). In contrast, a parallel clinical literature suggests that increased attention towards threatening faces is associated with exposure to maternal anxiety, and may be an early mechanism by which anxiety vulnerability may be transmitted (Morales et al., 2017). One stark difference between studies in these different fields is the stimuli used to denote threat. The developmental literature has largely used fearful faces, while the clinical literature in both children and adults has used angry faces to represent attention towards threat. In one of the few investigations of this nature, Morales et al. (2017) presented 424 month old infants with an overlap task including angry, happy, and neutral faces, and measured infant's attention disengagement from these faces using eye tracking (i.e., duration of looking when a checkerboard distractor was present). They found support for



67

a relationship between maternal anxiety and an attentional bias towards threat, such that infants whose mothers reported higher anxiety symptom scores exhibited greater difficulty disengaging attention from angry faces. The results of this study are in line with Forssman et al. (2014) who found an attention bias to threat in infants of mothers reporting general stress, as well as studies conducted with older children that have found a positive relationship between maternal anxiety and attentional bias for threat (for a review, see Roy, Dennis, & Warner, 2015). Considering the paucity of studies examining this relationship in infancy, and the potential importance of early processing biases to later socioemotional development, the current study provides an important contribution to this literature. Gaps in the Literature As described above, there has been recent progress in this area of research, with studies demonstrating that maternal depressive and anxiety symptoms are related to attentional biases for threat-related expressions observed in infancy (Forssman et al., 2014; Morales et al., 2017), which may differ from the direction of similar biases observed in childhood (Joormann et al., 2010; Kujawa et al., 2011; Owens et al., 2016), and previously reported associations between maternal positive affect and the attentional bias for fearful faces (de Haan et al., 2004). Taken together, these studies provide preliminary support for the role of maternal mood symptoms and affect in the development of emotion recognition. However, further research is required to address inconsistencies in the literature, to examine these influences from a longitudinal perspective in infancy, and to measure parental affect along a more general continuum of negative to positive affect. As well, examining these questions with methods such as eye



68

tracking might allow for a better understanding how these influences relate to individual differences in emotion recognition at the basic level of processing differences (e.g., how infants scan emotional faces). The Current Study The current study will explore the links between these various contributions (i.e., maternal mood symptoms and general positive and negative affect) and infants' preferences for emotional faces (study 1), as well as their allocation of attention towards critical facial features (study 2), to examine whether emotion exposure may be related to individual differences in infants' scanning patterns during these tasks. The current study will use parent-report data collected at 3.5 and 7 months to examine both the early and concurrent influences of these variables in a longitudinal framework. Broadly, this examination will help us fine-tune our understanding of the role of experience in the development of emotion recognition abilities. Hypotheses Happy-fear preference (study 1). Given the mixed findings within the literature, I did not have specific predictions regarding the direction of the association between maternal depression, anxiety, and the preference towards fearful faces. However, I predicted that greater maternal positive affect would be associated with increased duration of fixations towards the critical features of the preferred face, while greater negative affect and depressive symptom scores would be associated with decreased duration of fixations towards critical features. Dynamic emotions (study 2). Regarding infants' scanning patterns of dynamic faces, I predicted that maternal positive affect would be positively correlated with



69

duration of fixations to critical features, whereas negative affect would be negatively correlated with fixation to critical features. I also hypothesized that maternal-reported depressive symptoms would be negatively correlated with duration of fixations towards critical features. Method Participants. Participants were the same infants who participated in Studies 1 and 2. Thirty of the infants included in the final sample participated in a larger longitudinal study in the Brain and Early Experiences Laboratory, and I therefore had questionnaire data from the same participants at 3.5 months old in addition to the same questionnaire data at 7 months old. These data were included in the current study, as it allowed us to examine the relationship between early (3.5 months) and concurrent (7 months) emotion variables and later emotion recognition (7 months). All measures were completed by the primary caregiver, who was, in all cases, the mother. Measures. Maternal depression was measured with the Edinburgh Postnatal Depression Scale (EPDS; Cox et al., 1987) and the Center for Epidemiologic Studies Depression Scale (CES-D; Radloff, 1977). The fear subscale of the Positive and Negative Affect Schedule (PANAS; Watson, Clark, & Tellegen, 1988) was used as a proxy for maternal anxiety, as a specific measure of anxiety was not included in the current study. In addition, the above questions of emotional experience are also important to consider in regard to general variations in caregiver affect. Even in the absence of mood and anxiety disorders, parental affect can range between positive and negative, and may also impact infants' processing of emotional expressions. In the current study, I used the general positive and negative scales, and joviality and hostility subscales of the PANAS to gather



70

ratings of parental affect along a continuum of negative to positive affect, and linked this measure of emotion exposure to infants' performance across tasks. Edinburgh Postnatal Depression Scale (EPDS). The EPDS is a self-report screening tool for depressive symptoms that is intended for use during pregnancy and throughout the first year after childbirth (Cox et al., 1987). The scale was designed to be more sensitive to the conflation of somatic symptoms of depression observed on other validated measures of depression (e.g., weight gain and fatigue) that are commonly experienced during pregnancy and during the postpartum period outside of a depressive episode (Thomas et al., 2017). Questions ask mothers to rate how they have been feeling over the past seven days. Items are rated on a 4-point Likert scale (0-3) and a total score is created from a sum of the items, with scores above 12 indicating a high possibility of depression. The original validation study reported acceptable internal reliability (Cronbach's  = .87), sensitivity of true positives (i.e., had a diagnosis of major depressive disorder, 86%), and specificity of true negatives (78%), while subsequent studies have further validated the measure using standardized clinical interviews with large samples across different countries (Affonso, De, Horowitz, & Mayberry, 2000; Bergink et al., 2011). Center for Epidemiologic Studies Depression Scale (CES-D). The CES-D (Radloff, 1977) is a self-report screening measure for a major depressive episode, as defined by the Diagnostic and Statistical Manual of Mental Disorders (DSM; American Psychiatric Association, 2013). The scale includes 20 items measured from "1" = less than 1 day to "4" = most or all of the time, which are summed to produce a total score between 0-60. A total score equal to or above 16 indicates risk of clinical depression.



71

This instrument was originally developed for use in epidemiological studies of depression in the general population, and has demonstrated reliability in non-clinical samples (Cronbach's  = 0.85-0.90, test-retest reliability = 0.45-0.70; Radloff, 1977), and has demonstrated moderate correlations (0.49) with depression scores from clinical interviews and high correlations with measures of trait anxiety (0.71; Orme, Reis, & Herz, 1986). Positive and Negative Affect Schedule (PANAS). The PANAS is a self-report measure of affect, which was completed by mothers in the study about their own affect (i.e., it was not a report of their infants' affect). The PANAS contains 60 words related to feelings and emotions (e.g., "cheerful, "fearless," "angry at self"), which are rated on a 5point Likert scale of how much the individual has been feeling this way during the past few weeks (e.g., "1" = very slightly or not at all, "5" = extremely). Items are summed to create general positive and negative emotion scales, as well as separate scales for fear, hostility, guilt, sadness, joviality, self assurance, attentiveness, shyness, fatigue, serenity, and surprise. The original validation study included a sample of 586 individuals and reported good internal consistency reliability for the positive and negative affect scales ( = .87), as well as a negative intercorrelation between the two scales ( = -.22; Watson, Clark, & Tellegen). Test-retest reliability over an 8-week interval was reported as 0.58 for the positive affect scale, and 0.48 for the negative affect scale. The negative affect scale is also positively correlated with measures of other related constructs (e.g., state depression, anxiety, and general distress), and negative correlations have been found between these measures and the positive affect scale. For example, the Beck Depression



72

Inventory (negative = .58, positive = -.36), the State Anxiety Scale (negative = .51, positive = -.35), and the Hopkins Symptom Checklist (negative = .65, positive = -.29). Procedure. The Ryerson Research Ethics Board gave approval for all procedures. During the lab visit, the researcher told families that they would be sent an online link to a number of questionnaires to be completed after the visit. The researcher indicated that the primary caregiver should be the one to complete the questionnaires. Following the lab visit, parents were sent an email with a link to a number of questionnaires to be completed through Qualtrics, an online survey company. Parents were instructed to complete the questionnaires within a week of their visit, as I wanted their responses to be as reflective of the infant's age as close to the time of the visit as possible. Parents were sent a reminder email if they did not complete the questionnaires after five days, and were sent a thank-you email upon completion. Results Not all participants provided complete data for each of the questionnaires at both time points. The EPDS was completed by n = 18 participants at 3.5 months and n = 33 participants at 7 months. The CESD was completed by n = 25 participants at 3.5 months at n = 37 participants at 7 months. The PANAS was completed by n = 25 participants at 3.5 months and n = 38 participants at 7 months. Reliability and validity. Cronbach's  was calculated for each scale to assess internal consistency, and acceptable values were found for all scales (EPDS 3.5 months = .84, EPDS 7 months = .84, CESD 3.5 months = .79, CESD 7 months = .92, PANAS 3.5 months = .86, PANAS 7 months = .87).



73

Depression scales. To assess for concurrent validity between depression scales, correlations were run between both depression scales at 3.5 and 7 months. The CESD and EPDS were positively correlated at 3.5 months, r(16) = .63 p = .005, and 7 months, r(31)= .82, p <.001. Within scales, correlations were also positive across time: CESD: r(18) = .66, p=.002; EPDS: r(12) = .87, p<.001 (see Table 1 for CESD and EDPS descriptive statistics). At 3.5 months, n = 2 participants scored in the clinical range on the EPDS (score above 12), and n = 3 participants had scores in the clinical range on the CESD (scores above 16). At 7 months, n = 2 and n = 5 participants scored in the clinical range on the EPDS and the CESD respectively. Positive and negative affect. Correlations were run between the general positive affect and general negative affect scales, and the fear, joviality, and hostility subscales of the PANAS (at 3.5 and 7 months) and the behavioural variables of interest in Studies 1 and 2 (i.e., fear preference, and attention towards static and dynamic critical facial features). These scales were selected to represent general expression of positive and negative affect, as well as parent-reported anxiety (fear subscale; see table 2 for PANAS descriptive Statistics). The positive dimensions of parental affect (joviality and general positive affect) were positively correlated with each other at both 3.5 months, r(15) = .682, p = .001, and 7 months, r(37) = .906, p = <.001. At 3.5 months, fear was positively correlated with general negative affect, r(23) = .727, p <.001, while hostility was not (p = .175). At 7 months, all three dimensions of negative affect (fear, hostility, and general negative affect) were positively correlated, ps <.015.



74

Table 1 Mean and Standard Deviation of the Depression Scales. N Mean Std. Deviation CESD 3.5 months 25 10.08 6.06 CESD 7 months EPDS 3.5 months 37 18 10.11 7.28 9.09 4.24

EPDS 7 months 33 6.36 4.29 Note. The clinical cut off for mild depression is 16 for the CESD and 12 for the EPDS. Table 2 Mean and Standard Deviation of the Positive and Negative Affect Schedule (PANAS) N Mean Std. Deviation Joviality 3 months 25 29.76 5.04 Fear 3 months Hostility 3 months General Negative Emotion 3 Months General Positive Emotion 3 months Joviality 7 months Fear 7 months Hostility 7 months General Negative Emotion 7 Months General Positive Emotion 7 months 25 25 25 25 38 38 38 38 9.32 10.60 16.56 36.76 26.47 8.50 11.55 16.58 3.04 1.89 3.82 6.489 5.71 2.84 2.62 4.39

38

33.18

7.47

Note. General negative and positive emotion scales can range between 0-50. Joviality, fear, and hostility subscales can range between 0-30. Relations between emotion variables and behavioural measures of emotion



75

processing (fear preference and dynamic expressions). Depressive and anxiety symptoms. No significant correlations were found between maternal depressive symptoms at 3.5 or 7 months (EPDS, CESD) and the magnitude of the fear preference in the VPC task in study 1, nor were there associations between depressive symptom scores and percentage of looking towards the eyes and mouth of fearful faces (ps > .35). Additionally, the fear subscale of the PANAS (which served as a proxy for anxiety) was not significantly correlated with the magnitude of the fear preference at 3.5 or 7 months (ps > .45). The percentage of looking to the eyes and the mouth of dynamic expressions was also examined with respect to depressive and anxiety symptoms (study 2). Correlations were run between the EPDS (3.5 and 7 months), CESD (3.5 and 7 months), PANAS fear subscale (3.5 and 7 months) and percentage of looking time towards the eyes and mouth of all five emotional faces. The correlations between depressive symptom scores on the EPDS at 3.5 months and percentage of looking time to the mouth of dynamic sad faces at 7 months was marginally significant, r(13) = .509, p = .053. At 7 months, parent-reported depressive symptom scores on the EPDS were significantly correlated with percentage of looking time to the mouth of dynamic sad faces r(29) = .402, p = .025, and the correlation between symptom scores on the EPDS and percentage of looking time to the mouth of dynamic happy faces was marginally significant, r(31) = .347, p = .052., No other correlations between EPDS scores and duration of fixations to critical features were significant (ps > .1).



76

There were no significant correlations between parent-reported depressive symptom scores on the CESD, or parent-reported anxiety, and percentage of looking time to the critical features of dynamic faces at either 3.5 or 7 months (ps > .063) Positive and negative affect. Percentage of looking to the mouth of fearful faces within the VPC task was positively correlated with measures of positive parental affect at 3.5 months. Specifically, looking to the fearful mouth was significantly correlated with general positive affect and joviality at 3.5 months, r(19) = .487, p = .025, and r(19) = .456, p = .038 respectively. Negative correlations were observed between the percentage of looking to the eyes of fearful faces and these same dimensions of positive affect at 3.5 and 7 months. Specifically, looking to the fearful eyes was significantly negatively correlated with general positive affect at 3.5 and 7 months, r(19) = -.467, p = .033,and r(32) = -.351, p = .042, respectively, as well as joviality at 3.5 months, r(19) = -.474, p = .030. Correlations were also run between the general positive, general negative, joviality, and hostility scales of the PANAS (3.5 and 7 months) and the percentage of looking time to the eyes and mouth of each of the five emotional faces in the dynamic scanning task. Parent-reported joviality and hostility at 3.5 months were both significantly correlated with percentage of looking time to the mouth of dynamic happy faces, r(20) = .466, p = .029, and r(20) = .568, p = .006, respectively. As well, parent-reported joviality at 7 months was significantly negatively correlated with duration of fixations to the eyes of dynamic neutral faces, r(33) = -.357, p = .035, and significantly positively correlated with duration of fixations to the mouths of dynamic neutral faces, r(35) = .349, p = .040.



77

There were no significant correlations between percentage of looking time to the critical features (eyes and mouth) of dynamic expressions and the general positive affect scale, or general negative affect scale of the PANAS at either 3.5 or 7 months (ps > .06) Discussion The current study examined the relationship between emotional influences in the infant's environment and the outcome measures of studies 1 and 2 in an effort to elucidate the origin of individual differences in 1) the attentional bias for fearful faces, 2) the scanning of critical features, and 3) the scanning of dynamic expressions. The variables of interest were self-reported maternal depressive symptoms, anxiety, and general positive and negative affect. Depressive symptoms and anxiety. The results demonstrated that maternal depressive and anxiety symptoms, measured at 3.5 and 7 months, were not associated with infants' attentional bias towards fearful faces, nor were they related to increased attention towards the critical features of the fearful face. This finding is discordant with previous studies that have reported a relationship between maternal stress, anxiety, and depression, and an attentional bias towards threatening faces (Forssman et al., 2014; Morales et al., 2017). However, the results are in line with the findings of LeppÃ¤nen et al. (2018) who similarly reported that maternal depression and anxiety symptoms were not associated with the magnitude of the attentional bias towards fearful or angry faces. While the above studies examined a similar question, the methodologies differed from those of the current study. Previous studies examining this association have used an attention disengagement task (i.e., attentional bias represented by increased difficulty looking away from a face), while the current study used a visual paired-comparison task



78

(i.e., attentional bias represented by increased looking towards a face). Critically, the current sample was not a clinical sample, as a limited number of participants met the clinical cut off of our measures (for mild depression). It is possible that we may have found significant associations with more variability on our measures. Finally, each study measured maternal mood symptoms with different self-report questionnaires, which may have also contributed to the difference in results across studies, and might suggest that the previously reported associations are not replicable across different paradigms and scales. While associations between mood symptoms and increased attention towards fearful faces were not found, the current study found that greater maternal depressive symptoms at 3.5 and 7 months was associated with increased looking towards the mouth region of dynamic happy and sad expressions. The finding of increased attention towards the mouth of happy faces could be due to infants' tendency to attend to novel visual stimuli. Infants whose mothers reported greater depressive symptoms might see fewer positive expressions, and therefore allocate increased attention to the mouth of a happy face due to its relative novelty. Relatedly, there is evidence that infants with depressed mothers are slower to habituate to happy expressions (Hernandez-Reif et al., 2006), possibly because this expression is more novel, and infants therefore have more information to process. The results indicate that these infants are spending increased time scanning the mouth region of this expression compared to infants of non-depressed mothers, which may be a more specific mechanism underlying this processing deficit. The finding that infants of mothers with more depressive symptoms also allocate greater attention towards the mouth of sad faces does not support this novelty hypothesis.



79

It is also at odds with previous data suggesting an avoidant pattern of looking away from sad information in infants (Boyd et al., 2006) and children (Harrison & Gibb, 2015) with depressed mothers. However, previous studies have also reported that children of depressed mothers exhibit selective attention towards sad faces (Joormann et al., 2010; Kujawa et al., 2011; Owens et al., 2016), which is more consistent with my findings. Considering that the current study measured depressive symptoms among a typical sample, rather than a clinical sample, it is possible that the infants in the current study have not developed an avoidant looking pattern away from sad stimuli, as they have not required the same attentional self-soothing strategies required by "at risk" infants with clinically depressed mothers who may receive greater exposure to sadness (Gross, 2014; Owens et al., 2016). The small number of mothers meeting the clinical cut off range in the current study limited our ability to conduct separate analyses with this subgroup, but this is an important consideration for future study. Positive and negative affect. My next aim was to examine whether typical variations in positive and negative affect were related to infants' scanning of emotional faces. I hypothesized that the fear preference and attention towards critical features of both static and dynamic expressions would be associated with greater maternal positive affect. These predictions follow from the notion that displaying an attentional bias towards fearful faces around 7 months is developmentally normative and predicts positive outcomes later on (Peltola et al., 2015), and results suggesting that increased exposure to positive affect may facilitate typical emotional development (de Haan et al., 2004). My hypothesis regarding the fear preference was not supported, as I did not find that the preference was related to maternal positive affect (general positive affect and



80

joviality). These findings are contrary to the results of de Haan et al. (2004), who found that greater looking time towards fearful faces was associated with higher maternal positivity. While the overall fear preference was not related to maternal positive affect, further analyses revealed that looking to the mouth of the fearful face in the VPC task was positively correlated with general positive affect and joviality at 3.5 months, while looking to the eyes of fearful faces was negatively correlated with these measures of positive affect at 3.5 and 7 months. Eye tracking allows for a more nuanced investigation into this relationship than that of previous studies that relied solely on behavior measures of looking time. It is possible that de Haan et al.'s (2004) findings were driven by attention to a specific facial feature of the fearful face (e.g., the mouth, per my results), but they were limited in their ability to draw these types of conclusions due to their methodology. In addition, greater maternal joviality and hostility at 3.5 months were both associated with increased looking to the mouth of dynamic happy faces. In terms of hostility, one possible explanation is that looking to the mouth of happy faces is sensitive to different dimensions of negative affect (e.g., depression, see above). However, another explanation comes from the finding that my measures of hostility and joviality were highly positively correlated with each other, suggesting that these scales may represent greater emotional expressiveness in general. Following this interpretation, infants with mothers who are more emotionally expressive overall may be more sensitive to the mouth region of happy faces (i.e., smiles). A recent study conducted on the role on family expressiveness on individual differences in emotion matching provides support for this explanation. Ogren, Burling, and Johnson (2018) found that family expressiveness



81

was positively related to 9-month-old infants' ability to match a happy facial expression to corresponding vocal information. The authors speculated that this finding might be category learning in children, such that children often demonstrate superior learning when provided with exemplars that both fit and do not fit with the category being learned (Ankowski, Vlach, & Sandhofer, 2013; Gentner, Anggoro, & Klibanoff, 2011; Gentner & Namy, 1999). Infants may similarly learn best about emotions when provided with a range of categories (i.e., both positive and negative, such as joviality and hostility), rather than just increased positive emotion. The current results may further suggest an impact of exposure to a wide variety of emotions early on in development. Maternal joviality at 7 months was also related to decreased scanning of the eyes and increased scanning of the mouths of neutral faces, the least expressive face. This result may suggest that these infants, who are exposed to greater positive emotion, are spending more time trying to detect the emotional signal of neutral expressions and are looking to the mouth to do so. Support for this interpretation can be loosely drawn from work demonstrating that positive maternal affect is associated with infants' increased exploration of ambiguous toys (e.g., Gunnar & Stone, 1984), such that ambiguity may lead to increases in attention allocation. Although this finding is consistent with the above finding of increased scanning of the happy mouth, as opposed to the eyes, it remains unclear why the mouth region of a neutral face would draw infants' attention compared to the eyes. It is also crucial to consider the interrelations between these maternal variables of emotion exposure. While there appear to be unique effects of maternal depressive symptoms and general positive and negative affect on infants' scanning patterns, these



82

variables likely interact with each other as well. One such link is that depression is commonly associated with reduced emotional reactivity and expression (Bylsma, Morris, & Rottenberg, 2008). Thus, in addition to the potential for increased exposure to negative affective states, infants of depressed mothers may also receive less expressive input in general (Lundy, Field, & Pickens, 1996). This lack of expressiveness may be observed through multiple modalities: reduced time spent looking at the infant and making facial expressions, less time talking to the infant, and less time touching the infant (Cohn, Campbell, Matias, & Hopkins, 1990; Field, 1984). Depression has also been associated with deficits in the reward processing system (Pizzagalli et al., 2009), which may also impact the ability of some mothers with depression, or at-risk for depression, to interact with their infants in a way that is engaging and reciprocal (Fleming, Ruble, Flett, & Shaul, 1988). If some of these mothers do not receive the same positive stimulation from interacting with their infants, it is possible that their resulting affect may be flat and more restricted in range (Weinberg & Tronick, 1998). Furthermore, Ogren's (2018) findings highlight the importance of family expressiveness in the infants' early environment, which captured the expressiveness of all members of a household. The current study, and much of the previous literature, only measured the expressiveness of the primary caregiver. Therefore, less is known about the interrelation between maternal depression and family expressiveness (i.e., how the expressiveness of other family members contributes to the overall emotional environment of the infant, even in the case of maternal depression). Future studies are encouraged to measure emotion-exposure variables across both parents, to better elucidate the relative contributions of these factors to infants' emotion recognition.



83

General Discussion Across all three studies, the collective goal of this thesis was to develop a clearer and more precise picture of emotion processing abilities in 7-month-old infants and their developmental origins. This was accomplished through a series of related studies. In study 1, I sought to replicate the attentional bias for fearful faces and to investigate whether infants' online scanning patterns inform this preference. In study 2, I characterized infants' scanning patterns of dynamic expressions and examined their differential attention towards critical features across emotions. In study 3, I uncovered links between these behavioural variables and emotion-related influences in the infants' environment. There were a number of main findings across studies. First, I successfully replicated the attentional bias for fearful faces, by demonstrating that infants allocated increased attention towards the fearful face compared to the happy face in the VPC task. To add to the literature of previous studies reporting this visual preference (Amso et al., 2010; de Haan & Nelson, 1998; Kotsoni et al., 2001; Ludemann & Nelson, 1988; Nelson & Dolgin, 1985; Nelson et al., 1979; Peltola, LeppÃ¤nen, MÃ¤ki, et al., 2009), I examined whether this preference was informed by infants' online scanning of the two faces, as the mechanisms driving this preference are not well understood. I hypothesized that increased attention towards the eye region would be associated with the fear preference, but this hypothesis was not supported. While infants exhibited increased scanning of the eye region of fearful faces compared to the mouth region, attention towards the eyes was not associated with the fear preference. Instead, infants revealed a scanning pattern



84

demonstrating similar attention to corresponding critical features across both expressions. Infants who were "eye scanners" of the fearful face were also "eye scanners" of happy faces, and the same was true for scanning of the mouth region. This finding is consistent with previous evidence that infants continue to exhibit increased attention towards fearful faces even when the eyes are not visible (Asghar et al., 2008; LeppÃ¤nen et al., 2008; Peltola, LeppÃ¤nen, Vogel-Farley, et al., 2009), as well as previous data suggesting that it is not until 10 months of age that infants engage in mature triangular scanning of faces (i.e., taking both eyes and mouth into account; Libertus et al., 2007). While I did not find support for my original prediction, the current findings present a significant contribution to the literature, as few studies have related eye-tracking measures to the fear preference. The current findings strengthen the view that the preference for fearful faces is not merely due to attention towards a specific facial region, and likely reflects attention to the face more holistically. The second finding that emerged was that infants engaged in differential scanning of critical facial features (i.e., the eyes and mouth) across dynamic expressions. Specifically, infants spent more time scanning the eyes of angry, fearful, neutral, and sad faces compared to happy faces, and more time scanning the mouth region for happy faces compared to all other emotions. Within individual emotions, infants exhibited increased attention towards the eyes than the mouth for all expressions aside from happiness, where there was no difference. The results of this study provide a mechanism of understanding previously identified trends in infants' emotion recognition abilities. The first year of life is a time of significant improvement in emotion recognition abilities. Infants improve in their ability



85

to discriminate between different emotional expressions and develop the ability to categorize groupings of an emotional expressions as the same expression, whether presented by different identities, or across varying intensity of the emotion (Bornstein & Arterberry, 2003). Previous studies using the behavioural measure of looking time have mapped out the timeline of the emergence of these abilities, but the mechanisms driving changes in these abilities are unclear. As well, few studies have explored these questions using dynamic expressions, which are more representative of emotions that infants encounter in the real world. My results suggest that by 7 months of age, infants are sensitive to the critical features of emotional expressions, and preferentially allocate attention towards these features depending on the emotion. These findings suggest a link between lower-level emotion processing patterns and the observed improvements in emotion recognition occurring at a similar time in development. The last finding to emerge was that infants' visual scanning patterns of emotional expressions are influenced by variations in maternal depressive symptoms and general positive and negative affect, both predictive (at 3.5 months) and concurrent (7 months). Infants whose mothers reported greater depressive symptoms allocated more attention towards the mouths of both happy and sad dynamic expressions. Infants whose mothers were highly positive or more expressive in general allocated more attention to the mouth of happy expressions, the mouths of neutral expressions, and less attention to the eyes of neutral expressions. In the VPC task, greater maternal positive affect was associated with increased looking to the mouth of the fearful face and less looking to the fearful eyes. I did not find support for a relationship between the fear preference and maternal mood symptoms (depression and anxiety) or positive and negative affect. This finding is in



86

contrast to previous studies reporting that emotional factors influence infants' preference for threatening faces (Forssman et al., 2014; Morales et al., 2017), but is supported by LeppÃ¤nen et al. (2018), who also reported a lack of association. These mixed findings highlight an important inconsistency in the literature. There are conflicting results between the typical infant literature, which suggests that an attention bias towards fearful faces is developmentally normative (Peltola et al., 2015), and a parallel clinical literature, which suggests that increased attention towards threatening faces is more pronounced in infants who are exposed to greater maternal anxiety, stress, and depression. While the current results may be driven by methodological differences across studies, they may also provide support for the view that the fear preference is a robust developmental phenomenon that is more or less immune to environmental influences.  Limitations and Future Directions There are a number of limitations of this thesis that are necessary to acknowledge. The first pertains to the restricted range of depressive and anxiety symptom scores that follow from measuring these symptoms within a typical sample. This limitation directly pertains to study 3 and may contribute to the lack of associations reported between these variables and my behavioural variables of interest. Furthermore, the reliance on selfreport across all of the emotional variables leaves this study vulnerable to a number of response biases and may be a less reliable method of measurement for certain variables (e.g., mood symptoms). As well, caregiver anxiety was not measured with a scale that has been validated to measure this construct, so these results remain tentative. The stimuli used across studies also limit the conclusions of this thesis. While the photographs and videos were taken from validated stimulus sets (Tottenham et al., 2009;



87

van der Schalk et al., 2011), both studies only presented infants with female models. There is evidence suggesting that there may be differences in the ways that males and females express emotions (e.g., Hall, 1984; LaFrance, Hecht, & Paluck, 2003), and as such, the findings from this thesis can only be generalized to emotional expressions posed by females. Moreover, all models were Caucasian, which is another limiting factor, as infants in the current sample were of diverse ethnicities, and may vary on their exposure to Caucasian faces. It is important to consider that this might have influenced the way in which infants responded to the stimuli in this study. Furthermore, the expressions presented across studies were presented against a uniform background, which is not how facial expressions are encountered in the real world. While study 2 can be said to be more ecologically valid, as facial movement presents a more realistic depiction of expressions than static images, the stimuli across both studies were still posed and presented in a less natural context. Future studies are suggested to address these limitations. Modifying the current design to include males and females expressing emotions in more naturalistic contexts would greatly improve the generalizability of the current findings. Further, studies are encouraged to replicate and expand on the current findings by examining how visual scanning patterns and attention towards critical features relate to the attentional bias for fearful faces across other paradigms. The current study may provide support for a possible relationship between emotion exposure and infants' scanning patterns of critical facial features. Like much of the literature, however, the current results are mixed, with some findings in the opposite direction as predicted. These variables should continue to be investigated across studies using different scales to measure these variables, and



88

within clinical populations, to examine these relationships in the more extreme range of scores that I was unable to obtain in the current sample. Importantly, more work is required to examine whether infants' online scanning patterns, as outlined in this thesis, relate to other outcome variables, such as habituation, discrimination, and categorization. Broadening the current focus to include examinations of abilities that are more closely tied to emotion recognition will allow for an even deeper understanding of the mechanisms driving these abilities. Conclusion The current findings provide insight into infants' online processing of affective information in facial expressions and begin to shed light on the origins of individual differences in these scanning patterns. These results increase our understanding of the mechanisms underlying the attentional bias for fearful faces that emerges between 5 to 7 months by demonstrating that the preference to attend to fearful faces is not solely driven by increased attention to the fearful eyes. As well, the current results contribute to the small literature on infants' scanning of dynamic emotional expressions, and demonstrate that by 7 months, infants allocate differential attention to critical features across expressions. Individual differences in emotion processing abilities typically receive little attention within the literature, and this thesis suggests that maternal variables related to infants' emotion exposure (e.g., maternal mood symptoms and affect) may contribute to differences in infants' scanning patterns. Taken together, this thesis puts forth scanning patterns, and attention to critical features specifically, as a strategy for examining the mechanisms underlying the



89

development of emotion recognition abilities in infancy, and highlights the importance of considering the developmental antecedents of individual differences in these abilities.



90

Appendix A Table 3 Study 1 Size (pixels) of Interest Areas Feature Area Left Eye Right Eye Mouth Nose Face 32034.82 32034.82 32100.79 17904 271567.12

Screen 2286752 Note. Size of interest areas is consistent across all four models and across both emotions of fear and happiness.



91

Table 4 Study 2 Size (pixels) of Interest Areas Feature Model 1 Model 2 Right eye 10976 11760 Left Eye Mouth Lower Nose Upper Nose Between Eye Brows 10976 9380 4120.98 1595 1984 11760 8932 3883.01 1891 2046 41387.5 13528 75703 415582 Model 3 11160 11160 9246 4099.78 2035 2516 34688.5 29813 77829 415582 Model 4 10622 10622 9782 4206.59 1568 1932 28350.5 50242 74623 415582 Model 5 12100 12100 13695 5030.48 1881 2040 41334.5 37778 101509 415582

Neck/Shoulders 41379.5 Hair Face Screen 25234 72909 415582

Note. Size of interest areas is consistent across all emotions for each model



92

Appendix B Ryerson University Â­ Parental Consent Agreement Â­ 5-12 months Emotion recognition in the first year of life using eye-tracking methodology Your child is being asked to participate in a research study. Before you give your consent for your child to be a volunteer, it is important that you read the following information and ask as many questions as necessary to be sure you understand what you and your child will be asked to do. Investigators: Shira Segal, MA student in Psychology, Ryerson University Margaret Moulson, PhD, Department of Psychology, Ryerson University Purpose of the Study: Forty infants between 5 and 12 months of age are being recruited to participate in this study. This study is designed to look at the emotions infants see in their daily lives and how this relates to their early emotional development and understanding. We are particularly interested in examining how infants are exposed to emotional faces and how this exposure shapes their preference for emotions and their ability to recognize emotions. Description of the Study: To participate, you will visit us at Ryerson University for a 1hour visit during which your baby will complete two computer-based tasks where we show him/her a number of faces expressing different emotions and record how they respond to these faces. You will be with your infant at all times during these tasks and we will take breaks in between tasks. Following your visit to the lab, you will be asked to complete a series of questionnaires within 1 week of your visit. Computer-based tasks: 1) Face preference task: This task is only 1-2 minutes long. Your infant will see pairs of happy and fearful female faces while we record where your infant looks using eye-tracking technology. Your infant will be secured in a car seat. 2) Dynamic faces task: This task is only 5-7 minutes long. Your infant will see videos of female faces expressing different emotions while we record where your infant looks using eye-tracking technology. Your infant will be secured in a car seat. Questionnaires: You can complete the questionnaires at your own convenience using a smart phone, laptop/computer, or tablet. If you would prefer to complete the questionnaires on paper, we can provide you with paper copies that we will ask you to scan and send to us electronically or send through the mail. Some questionnaires ask about your infant's developing abilities and behaviours during daily activities (e.g., feeding, sleeping). Other


93

questionnaires are directed towards you as the parent, and ask questions about your expressiveness and your relationship with your baby. Finally, as we are interested in emotion, we will also be asking some questions about your mental well-being; this will include questions asking you to rate your current mood (e.g., I have felt sad or miserable), traumatic experience (e.g., experience of repeated dreams of stressful event), and coping behaviours, such as substance use (e.g., you have used alcohol or drugs weekly). The completion of these questionnaires will take approximately 1.5 hours. POTENTIAL BENEFITS: While there is no direct benefit to you or your child, this study has the potential to tell us more about how emotion recognition develops. This is very important because it helps us understand how people communicate and interact with each other. WHAT ARE THE POTENTIAL RISKS TO YOU/YOUR CHILD AS A PARTICIPANT? Although the faces we will show your baby have been presented to infants like yours in other research projects, it is possible that your infant may become distressed or upset during the presentation of negative faces We will be asking you to reflect on a number of potential challenges you may be facing at this time (e.g., mental health). As such, there is a potential risk that you may experience some distress when thinking about your challenges, and/or you may worry about being judged for substance use. Researchers will non-judgmentally provide referrals for additional support if desired or if concern is identified based on your responses on questionnaires we will contact you. You may contact Dr. Karen Milligan, clinical psychologist, who is a collaborator on this research at 416-979-5000, ext. 7054. CONFIDENTIALITY: Any information obtained in connection with this study will remain confidential unless release is required by law. All participants in this study will have their information protected using a participant ID number. Any written notes and data files will be identified only by this participant ID number, will be stored separately from the participant consent forms, and will be accessed only by those individuals directly involved in the study. Paper copies of the data will be stored in locked filing cabinets in the BEE Lab; electronic data files will be stored on password-protected media in the BEE Lab. The BEE Lab is locked and only accessible to members of the lab. All data will be pooled and published in aggregate form only. Since most journals require data to be stored for five years post-publication, this data will be stored for at least this period of time prior to being destroyed.



94

When completing the questionnaires online at home, you will be entering your responses into a secure computer program called Qualtrics, which is an American (USA) company. Consequently, Qualtrics or US authorities may access survey data in some forms (e.g., aggregate usage information) and under strict policies. Qualtrics employs a variety of security features to make sure that the data collected are not accessible by outside bodies. More information on Qualtrics security system can be viewed here: https://www.qualtrics.com/security-statement/. Information regarding their protective privacy is available here: https://www.qualtrics.com/privacy-statement/. Although Qualtrics usually stores IP address data, we have deactivated that function for this study. The researchers have a legal obligation to report to the appropriate authorities if they become aware of child abuse, neglect, or harm to a child. Researchers will make every attempt to maintain confidentiality (e.g., destroying protocols immediately after data is entered into a database, identifying you only with a number), where possible to minimize risk. INCENTIVES FOR PARTICIPATION: For participating in this study, your child will receive a small gift (e.g., a toy) at the end of the study session. You will receive $10 for your participation, whether or not your child completes all tasks in the study. VOLUNTARY PARTICIPATION AND WITHDRAWAL: Participation in this study is voluntary. This means that you can choose whether or not you and your child will be in this study. Your choice of whether or not to take part in this study will not influence future relations with Ryerson University. If you decide to participate, you are free to withdraw consent and to stop participation at any time without penalty or loss of benefits. In addition, your infant's success or interest in the task does not imply anything negative or positive about your child's development. At any particular point in the study, including while filling out the questionnaires at home, you may decline to answer any question(s) or stop participation altogether. QUESTIONS: If you have any questions about the research now, please ask. If you have questions later about the research, please contact: (416)979-5000 ext2189 shira.segal@psych.ryerson.ca (416)979-5000 ext2661 mmoulson@psych.ryerson.ca

Principal Investigator Supervisor and CoInvestigator

Shira Segal Dr. Maraget Moulson



95

If you have questions regarding your rights as a human subject and participant in this study, you may contact the Ryerson University Research Ethics Board for information: Research Ethics Board c/o Office of the Vice President, Research and Innovation Ryerson University 350 Victoria Street, Toronto, ON M5B 2K3 Email: REBChair@ryerson.ca Tel: 416-979-5042

Agreement: Your signature below indicates that you have read the information in this agreement and have had a chance to ask any questions you have about the study. Your signature also indicates that you are consenting for your child to be in the study and have been told that you can change your mind and withdraw your consent to participate at any time. You have been given a copy of this agreement. You have been told that by signing this consent agreement you are not giving up any of your legal rights. ___________________________________________ Name of Parent/Guardian of Participant (please print) _______________________ Name of Child (please print)

__________________________________________ Signature of Parent/Guardian

_______________________ Date

__________________________________________ Signature of Investigator

_______________________ Date

Â¨ I wish to receive information about the final results of this study Â¨ I do not wish to receive information about the final results of this study (Note: Results will be communicated in a group manner and we cannot give any details on individual infant performance).



96

References Adolphs, R., Gosselin, F., Buchanan, T. W., Tranel, D., Schyns, P., & Damasio, A. R. (2005). A mechanism for impaired fear recognition after amygdala damage. Nature, 433, 68Â­72. https://doi.org/10.1038/nature03086 Affonso, D., De, A. K., Horowitz, J. A., & Mayberry, L. J. (2000). An international study exploring levels of postpartum depressive symptomatology. Journal of Psychosomatic Research, 49(3), 207Â­216. https://doi.org/10.1016/S00223999(00)00176-8 Ambadar, Z. (2002). The effects of motion and orientation on perception of facial expressions and face recognition.(Unpublished doctoral dissertation). University of Pittsburgh, Pittsburgh, PA. Ambadar, Z., Schooler, J. W., & Conn, J. F. (2005). Deciphering the enigmatic face the importance of facial dynamics in interpreting subtle facial expressions. Psychological Science, 16(5), 403Â­410. https://doi.org/10.1111/j.09567976.2005.01548.x American Psychiatric Association. (2013). Diagnostic and statistical manual of mental disorders (5th ed.). Washington, DC: Author. Amso, D., Fitzgerald, M., Davidow, J., Gilhooly, T., & Tottenham, N. (2010). Visual exploration strategies and the development of infants' facial emotion discrimination. Frontiers in Psychology, 1, 180. https://doi.org/10.3389/fpsyg.2010.00180 Angeli, V. (2015). Infants' early representation of faces: the role of dynamic cues (Unpublished doctoral dissertation). University of Padova, Padova, Italy. Ankowski, A. A., Vlach, H. A., & Sandhofer, C. M. (2013). Comparison versus contrast:



97

Task specifics affect category acquisition. Infant and Child Development, 22(1), 123. https://doi.org/10.1002/icd.1764 Asghar, A. U. R., Chiu, Y. C., Hallam, G., Liu, S., Mole, H., Wright, H., & Young, A. W. (2008). An amygdala response to fearful faces with covered eyes. Neuropsychologia, 46(9), 2364Â­2370. https://doi.org/10.1016/j.neuropsychologia.2008.03.015 Bar-Haim, Y. (2010). Research review: Attention bias modification (ABM): A novel treatment for anxiety disorders. Journal of Child Psychology and Psychiatry, and Allied Disciplines, 51(8), 859Â­870. https://doi.org/10.1111/j.14697610.2010.02251.x Barrera, M. E., & Maurer, D. (1981). The perception of facial expressions by the threemonth-old. Child Dev, 52(1), 203Â­206. Batki, A., Baron-Cohen, S., Wheelwright, S., Connellan, J., & Ahluwalia, J. (2000). Is there an innate gaze module? Evidence from human neonates. Infant Behavior and Development, 23(2), 223Â­229. https://doi.org/10.1016/S0163-6383(01)00037-6 Belsky, J., Crnic, K., & Woodworth, S. (1995). Personality and parenting: Exploring the mediating role of transient mood and daily hassles. Journal of Personality, 63(4), 905Â­929. https://doi.org/10.1111/j.1467-6494.1995.tb00320.x Bergink, V., Kooistra, L., Lambregtse-van den Berg, M. P., Wijnen, H., Bunevicius, R., van Baar, A., & Pop, V. (2011). Validation of the edinburgh depression scale during pregnancy. Journal of Psychosomatic Research, 70(4), 385Â­389. https://doi.org/10.1016/j.jpsychores.2010.07.008 Biele, C., & Grabowska, A. (2006). Sex differences in perception of emotion intensity in



98

dynamic and static facial expressions. Experimental Brain Research, 171(1), 1Â­6. https://doi.org/10.1007/s00221-005-0254-0 Bornstein, M. H., & Arterberry, M. E. (2003). Recognition, discrimination and categorization of smiling by 5-month-old infants. Developmental Science, 6(5), 585Â­ 599. https://doi.org/10.1111/1467-7687.00314 Boucher, J., & Ekman, P. (1975a). Facial areas and emotional information. Journal of Communication, 25(2), 21Â­29. https://doi.org/10.1111/j.1460-2466.1975.tb00577.x Boucher, J., & Ekman, P. (1975b). Facial areas and emotional information. Journal of Communication, 25(2), 21Â­29. https://doi.org/10.1111/j.1460-2466.1975.tb00577.x Bould, E., & Morris, N. (2008). Role of motion signals in recognizing subtle facial expressions of emotion. British Journal of Psychology, 99(2), 167Â­189. https://doi.org/10.1348/000712607X206702 Boyd, R. C., Zayas, L. H., & McKee, M. D. (2006). Mother-infant interaction, life events and prenatal and postpartum depressive symptoms among urban minority women in primary care. Maternal and Child Health Journal, 10(2), 139Â­148. https://doi.org/10.1007/s10995-005-0042-2 Bradley, M. M. (2009). Natural selective attention: Orienting and emotion. Psychophysiology, 46(1), 1Â­11. https://doi.org/10.1111/j.1469-8986.2008.00702.x Brechet, C. (2017). Children's recognition of emotional facial expressions through photographs and drawings. Journal of Genetic Psychology, 178(2), 139Â­146. https://doi.org/10.1080/00221325.2017.1286630 Bruce, V., & Valentine, T. (1988). When a nod's as good as a wink: The role of dynamic information in facial recognition. In M. M. Gruneberg, P. E. Morris, & R. N. Sykes



99

(Eds.), Practical aspects of memory: Current research and issues, Vol. 1. Memory in everyday life (pp. 169-174). Oxford, England: John Wiley & Sons. Brugha, T., Bebbington, P., Tennant, C., & Hurry, J. (1985). The list of threatening experiences: A subset of 12 life event categories with considerable long-term contextual threat. Psychological Medicine, 15(1), 189Â­194. https://doi.org/10.1017/S003329170002105X Bylsma, L. M., Morris, B. H., & Rottenberg, J. (2008). A meta-analysis of emotional reactivity in major depressive disorder. Clinical Psychology Review, 28(4), 676Â­ 691. https://doi.org/10.1016/j.cpr.2007.10.001 Calvo, M. G., Avero, P., FernÃ¡ndez-MartÃ­n, A., & Recio, G. (2016). Recognition thresholds for static and dynamic emotional faces. Emotion, 16(8), 1186Â­1200. https://doi.org/10.1037/emo0000192 Caron, R. F. F., Caron, A. J. J., & Myers, R. S. S. (1985). Do infants see emotional expressions in static faces? Child Development, 56(6), 1552Â­1560. https://doi.org/10.2307/1130474 Christie, F., & Bruce, V. (1998). The role of dynamic information in the recognition of unfamiliar faces. Memory & Cognition, 26(4), 780Â­790. https://doi.org/10.3758/BF03211397 Cohn, J. F., Campbell, S. B., Matias, R., & Hopkins, J. (1990). Face-to-face interactions of postpartum depressed and nondepressed mother-infant pairs at 2 months. Developmental Psychology, 26(1), 15Â­23. https://doi.org/10.1037/00121649.26.1.15 Cohn, J. F., & Tronick, E. Z. (1983). Three-month-old infants' reaction to simulated



100

maternal depression. Child Development, 54(1), 185Â­193. https://doi.org/10.1111/j.1467-8624.1983.tb00348.x Cox, J. L., Holden, J. M., & Sagovsky, R. (1987). Detection of postnatal depression: Development of the 10-item edinburgh postnatal depression scale. British Journal of Psychiatry, 150, 782Â­786. https://doi.org/10.1192/bjp.150.6.782 Cunningham, D. W., & Wallraven, C. (2009). The interaction between motion and form in expression recognition. Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization (APGV 2009), 41Â­44. https://doi.org/10.1145/1620993.1621002 de Haan, M. (2013). Infant EEG and event-related potentials. Infant EEG and EventRelated Potentials. https://doi.org/10.4324/9780203759660 de Haan, M., Belsky, J., Reid, V., Volein, A., & Johnson, M. H. (2004). Maternal personality and infants' neural and visual responsivity to facial expressions of emotion. Journal of Child Psychology and Psychiatry and Allied Disciplines, 45(7), 1209Â­1218. https://doi.org/10.1111/j.1469-7610.2004.00320.x de Haan, M., & Nelson, C. A. (1998). Discrimination and categorisation of facial expressions of emotion during infancy. In A. M. Slater (Ed.), Perceptual development: Visual, auditory, and language development in infancy (pp. 287Â­309). London: University College: London Press. Dollion, N., Soussignan, R., Durand, K., Schaal, B., & Baudouin, J.-Y. (2015). Visual exploration and discrimination of emotional facial expressions in 3-, 7- and 12month-old infants. Journal of Vision, 15(12), e795. Downey, G., & Coyne, J. C. (1990). Children of depressed parents: An integrative



101

review. Psychological Bulletin, 108(1), 50Â­76. https://doi.org/10.1037/00332909.108.1.50 Durand, K., Gallay, M., Seigneuric, A., Robichon, F., & Baudouin, J. Y. (2007). The development of facial emotion recognition: The role of configural information. Journal of Experimental Child Psychology, 97(1), 14Â­27. https://doi.org/10.1016/j.jecp.2006.12.001 Ehrlich, S. M., Schiano, D. J., & Sheridan, K. (2000). Communicating facial affect: It's not the realism, it's the motion. Proceedings of ACM CHI 2000 Conference on Human Factors in Computing Systems, (April), 252Â­253. https://doi.org/citeulikearticle-id:210479 Eisenbarth, H., & Alpers, G. W. (2011). Happy mouth and sad eyes: Scanning emotional facial expressions. Emotion, 11(4), 860Â­865. https://doi.org/10.1037/a0022758 Ekman, P., & Friesen, W. V. (1978). Manual for the Facial action coding system. Palo Alto, California: Consulting Psychologists Press. Emery, N. J. (2000). The eyes have it: The neuroethology, function and evolution of social gaze. Neuroscience and Biobehavioral Reviews, 24(6), 581Â­604. https://doi.org/10.1016/S0149-7634(00)00025-7 Fantz, R. L. (1961). The origin of form perception. Scientific American, 204(5), 66Â­72. https://doi.org/10.1038/scientificamerican0561-66 Farroni, T., Menon, E., Rigato, S., & Johnson, M. H. (2007). The perception of facial expressions in newborns. The European Journal of Developmental Psychology, 4(1), 2Â­13. https://doi.org/10.1080/17405620601046832 Faul, F., ErdFelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3.1 manual.



102

Behavioral Research Methods, 39(2), 175Â­191. https://doi.org/10.3758/BF03193146 Field, T. (1984). Early interactions between infants and their postpartum depressed mothers. Infant Behavior and Development, 7(4), 517-522. https://doi.org/10.1016/S0163-6383(84)80010-7 Field, T. (1995). Infants of depressed mothers. Development and Psychopathology, 4(1), 49Â­66. https://doi.org/10.1017/S0954579400005551 Field, T., Woodson, R., Greenberg, R., & Cohen, D. (1982). Discrimination and imitation of facial expressions by neonates. Science, 218(4568), 179Â­181. https://doi.org/10.1126/science.7123230 Fleming, A. S., Ruble, D. N., Flett, G. L., & Shaul, D. L. (1988). Postpartum adjustment in first-time mothers: Relations between mood, maternal attitudes, and mother-infant interactions. Developmental Psychology, 24(1), 71-81. https://doi.org/10.1037/00121649.24.1.71 Forssman, L., Peltola, M. J., Yrttiaho, S., Puura, K., Mononen, N., LehtimÃ¤ki, T., & LeppÃ¤nen, J. M. (2014). Regulatory variant of the TPH2 gene and early life stress are associated with heightened attention to social signals of fear in infants. Journal of Child Psychology and Psychiatry and Allied Disciplines, 55(7), 793Â­801. https://doi.org/10.1111/jcpp.12181 Fries, A. B. W., & Pollak, S. D. (2004). Emotion understanding in postinstitutionalized Eastern European children. Development and Psychopathology, 16(2), 355Â­369. https://doi.org/10.1017/S0954579404044554 Gao, X., & Maurer, D. (2010). A happy story: Developmental changes in children's sensitivity to facial expressions of varying intensities. Journal of Experimental Child



103

Psychology, 107(2), 67Â­86. https://doi.org/10.1016/j.jecp.2010.05.003 Geangu, E., Hauf, P., Bhardwaj, R., & Bentz, W. (2011). Infant pupil diameter changes in response to others' positive and negative emotions. PLoS ONE, 6(11). https://doi.org/10.1371/journal.pone.0027132 Gentner, D., Anggoro, F. K., & Klibanoff, R. S. (2011). Structure mapping and relational language support children's learning of relational categories. Child Development, 82(4), 1173-1188. https://doi.org/10.1111/j.1467-8624.2011.01599.x Gentner, D., & Namy, L. L. (1999). Comparison in the development of categories. Cognitive Development, 14(4), 487-513. https://doi.org/10.1016/S08852014(99)00016-7 GredebÃ¤ck, G., Eriksson, M., Schmitow, C., Laeng, B., & Stenberg, G. (2012). Individual differences in face processing: Infants' scanning patterns and pupil dilations are influenced by the distribution of parental leave. Infancy, 17(1), 79Â­101. https://doi.org/10.1111/j.1532-7078.2011.00091.x Gross, J. J. (2014). Emotion regulation: Conceptual and empirical foundations. In J. J. Gross (Ed.), Handbook of emotion regulation (pp. 3-20). New York, NY, US: Guilford Press. Haith, M. M., Bergman, T., & Moore, M. (1977). Eye contact and face scanning in early infancy. Science, 198(4319), 853Â­855. https://doi.org/10.1126/science.918670 Hall, J. A. (1984). Nonverbal sex differences: Accuracy of communication and expressive style. Nonverbal sex differences. Baltimore, MD: John Hopkins University Press. Hanawalt, N. G. (1944). The role of the upper and the lower parts of the face as a basis for judging facial expressions: II. In posed expressions and "candid-camera"



104

pictures. The Journal of General Psychology, 31(1), 23Â­36. https://doi.org/10.1080/00221309.1944.10545217 Harrison, A. J., & Gibb, B. E. (2015). Attentional biases in currently depressed children: An eye-tracking study of biases in sustained attention to emotional stimuli. Journal of Clinical Child and Adolescent Psychology, 44(6), 1008Â­1014. https://doi.org/10.1080/15374416.2014.930688 Harwood, N. K., Hall, L. J., & Shinkfield, A. J. (1999). Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation. American Journal of Mental Retardation, 104(3), 270Â­278. Heck, A., Hock, A., White, H., Jubran, R., & Bhatt, R. S. (2016). The development of attention to dynamic facial emotions. Journal of Experimental Child Psychology, 147, 100Â­110. https://doi.org/10.1016/j.jecp.2016.03.005 Hepach, R., & Westermann, G. (2013). Infants' sensitivity to the congruence of others' emotions and actions. Journal of Experimental Child Psychology, 115(1), 16Â­29. https://doi.org/10.1016/j.jecp.2012.12.013 Hernandez-Reif, M., Field, T., Diego, M., Vera, Y., & Pickens, J. (2006). Happy faces are habituated more slowly by infants of depressed mothers. Infant Behavior and Development, 29(1), 131Â­135. https://doi.org/10.1016/j.infbeh.2005.07.003 Hill, H., & Johnston, A. (2001). Categorizing sex and identity from the biological motion of faces. Current Biology, 11(11), 880Â­885. https://doi.org/10.1016/S09609822(01)00243-3 Hoehl, S., & Striano, T. (2010). The development of emotional face and eye gaze processing. Developmental Science, 13(6), 813Â­825. https://doi.org/10.1111/j.1467-



105

7687.2009.00944.x Hunnius, S., de Wit, T. C. J., Vrins, S., & von Hofsten, C. (2011). Facing threat: Infants' and adults' visual scanning of faces with neutral, happy, sad, angry, and fearful emotional expressions. Cognition & Emotion, 25(2), 193Â­205. https://doi.org/10.1080/15298861003771189 Ichikawa, H., Kanazawa, S., & Yamaguchi, M. K. (2014). Infants recognize the subtle happiness expression. Perception, 43(4), 235Â­248. https://doi.org/10.1068/p7595 Ichikawa, H., & Yamaguchi, M. K. (2014). Infants' recognition of subtle anger facial expression. Japanese Psychological Research, 56(1), 15Â­23. https://doi.org/10.1111/jpr.12025 Jessen, S., Altvater-Mackensen, N., & Grossmann, T. (2016). Pupillary responses reveal infants' discrimination of facial emotions independent of conscious perception. Cognition, 150, 163Â­169. https://doi.org/10.1016/j.cognition.2016.02.010 Johnson, M. H., Dziurawiec, S., Ellis, H., & Morton, J. (1991). Newborns' preferential tracking of face-like stimuli and its subsequent decline. Cognition, 40(1Â­2), 1Â­19. https://doi.org/10.1016/0010-0277(91)90045-6 Joormann, J., Gilbert, K., & Gotlib, I. H. (2010). Emotion identification in girls at high risk for depression. Journal of Child Psychology and Psychiatry and Allied Disciplines, 51(5), 575Â­582. https://doi.org/10.1111/j.1469-7610.2009.02175.x KÃ¤tsyri, J., & Sams, M. (2008). The effect of dynamics on identifying basic emotions from synthetic and natural faces. International Journal of Human Computer Studies, 66(4), 233Â­242. https://doi.org/10.1016/j.ijhcs.2007.10.001 Kestenbaum, R., & Nelson, C. A. (1990). The recognition and categorization of upright



106

and inverted emotional expressions by 7-month-old infants. Infant Behavior and Development, 13(4), 497Â­511. https://doi.org/10.1016/0163-6383(90)90019-5 Kim, H. I., & Johnson, S. P. (2013). Do young infants prefer an infant-directed face or a happy face? International Journal of Behavioral Development, 37(2), 125Â­130. https://doi.org/10.1177/0165025413475972 Klinnert, M. D. (1984). The regulation of infant behavior by maternal facial expression. Infant Behavior and Development, 7(4), 447Â­465. https://doi.org/10.1016/S01636383(84)80005-3 Knight, B., & Johnston, A. (1997). The role of movement in face recognition. Visual Cognition, 4(3), 265Â­273. https://doi.org/10.1080/713756764 Kotsoni, E., de Haan, M., & Johnson, M. H. (2001). Categorical perception of facial expressions by 7-month-old infants. Perception, 30(9), 1115Â­1125. https://doi.org/10.1068/p3155 Krumhuber, E. G., Kappas, A., & Manstead, A. S. R. (2013). Effects of dynamic aspects of facial expressions: A review. Emotion Review, 5(1), 41Â­46. https://doi.org/10.1177/1754073912451349 Kuchuk, a, Vibbert, M., & Bornstein, M. H. (1986). The perception of smiling and its experiential correlates in three-month-old infants. Child Development, 57(4), 1054Â­ 1061. https://doi.org/http://dx.doi.org/10.2307/1130379 Kujawa, A. J., Torpey, D., Kim, J., Hajcak, G., Rose, S., Gotlib, I. H., & Klein, D. N. (2011). Attentional biases for emotional faces in young children of mothers with chronic or recurrent depression. Journal of Abnormal Child Psychology, 39(1), 125Â­ 135. https://doi.org/10.1007/s10802-010-9438-6



107

La Barbera, J. D., Izard, C. E., Vietze, P., & Parisi, S. A. (1976). Four- and six-month-old infants' visual responses to joy, anger, and neutral expressions. Child Development, 47, 535Â­538. https://doi.org/10.2307/1128816 LaFrance, M., Hecht, M. A., & Paluck, E. L. (2003). The contingent smile: A metaanalysis of sex differences in smiling. Psychological Bulletin, 129(2), 305Â­334. https://doi.org/10.1037/0033-2909.129.2.305 Lander, K., & Bruce, V. (2000). Recognizing famous faces: Exploring the benefits of facial motion. Ecological Psychology, 12(4), 259Â­272. https://doi.org/10.1207/S15326969ECO1204_01 Lander, K., Bruce, V., & Hill, H. (2001). Evaluating the effectiveness of pixelation and blurring on masking the identity of familiar faces. Applied Cognitive Psychology, 15(1), 101Â­116. https://doi.org/10.1002/1099-0720(200101/02)15:1<101::AIDACP697>3.0.CO;2-7 Lander, K., Christie, F., & Bruce, V. (1999). The role of movement in the recognition of famous faces. Memory and Cognition, 27(6), 974Â­985. https://doi.org/10.3758/BF03201228 Lanes, A., Kuk, J. L., & Tamim, H. (2011). Prevalence and characteristics of postpartum depression symptomatology among canadian women: A cross-sectional study. BMC Public Health, 11. https://doi.org/10.1186/1471-2458-11-302 Langten, S. R. H., Watt, R. J., & Bruce, V. (2000). Do the eyes have it? Cues to the direction of social attention. Trends in Cognitive Sciences, 4(2) 50-59. https://doi.org/10.1016/S1364-6613(99)01436-9 Legerstee, M. (2009). The role of dyadic communication in social cognitive



108

development. Advances in Child Development and Behavior, 37, 1Â­53. https://doi.org/10.1016/S0065-2407(09)03701-X Leo, I., Angeli, V., Lunghi, M., Dalla Barba, B., & Simion, F. (2018). Newborns' face recognition: The role of facial movement. Infancy, 23(1), 45Â­60. https://doi.org/10.1111/infa.12197 LeppÃ¤nen, J. M., Cataldo, J. K., Bosquet Enlow, M., & Nelson, C. A. (2018). Early development of attention to threat-related facial expressions. PLOS ONE, 13(5), e0197424. LeppÃ¤nen, J. M., Hietanen, J. K., & Koskinen, K. (2008). Differential early ERPs to fearful versus neutral facial expressions: A response to the salience of the eyes? Biological Psychology, 78(2), 150Â­158. https://doi.org/10.1016/j.biopsycho.2008.02.002 LeppÃ¤nen, J. M., Moulson, M. C., Vogel-Farley, V. K., & Nelson, C. A. (2007). An ERP study of emotional face processing in the adult and infant brain. Child Development, 78(1), 232Â­245. https://doi.org/10.1111/j.1467-8624.2007.00994.x LeppÃ¤nen, J. M., & Nelson, C. A. (2009). Tuning the developing brain to social signals of emotions. Nature Reviews Neuroscience, 10(1), 37Â­47. https://doi.org/10.1038/nrn2554 LeppÃ¤nen, J. M., & Nelson, C. A. (2012). Early development of fear processing. Current Directions in Psychological Science, 21, 200Â­204. https://doi.org/10.1177/0963721411435841 LeppÃ¤nen, J. M., Peltola, M. J., MÃ¤ntymaa, M., Koivuluoma, M., Salminen, A., & Puura, K. (2010). Cardiac and behavioral evidence for emotional influences on attention in



109

7-month-old infants. International Journal of Behavioral Development, 34(6), 547Â­ 553. https://doi.org/10.1177/0165025410365804 Libertus, K. Needham, A., & Pelphrey, K. (2007, March). An eye-tracking study of face perception: From infancy to adulthood. Poster presented at the Biennial Meeting of the Society for Research in Child Development, Boston, MA. Ludemann, P. M., & Nelson, C. A. (1988). Categorical representation of facial expressions by 7-month-old infants. Developmental Psychology, 24(4), 492Â­501. https://doi.org/10.1037/0012-1649.24.4.492 Lundy, B., Field, T., & Pickens, J. (1996). Newborns of mothers with depressive symptoms are less expressive. Infant Behavior and Development, 19(4), 419-424. https://doi.org/10.1016/S0163-6383(96)90003-X Malatesta, C. Z. (1985). Developmental course of emotion expression in the human infant. In G. Zivin (Ed.), The Development of Expressive Behavior: BiologyEnvironment Interactions (pp. 183Â­213). Academic Press. Malatesta, C. Z., & Haviland, J. M. (1982). Learning display rules: The socialization of emotion expression in infancy. Child Development, 53(4), 991Â­1003. https://doi.org/10.2307/1129139 Maurer, D. (1985). Infants' perception of facedness. In T. Field & N. Fox (Eds.), Social Perception in Infants (pp. 73Â­100). Norwood, NJ: Ablex. Montague, D. P. F., & Walker-Andrews, A. S. (2002). Mothers, fathers, and infants: The role of person familiarity and parental involvement in infants' perception of emotion expressions. Child Development, 73(5), 1339Â­1352. https://doi.org/Article Morales, S., Brown, K. M., Taber-Thomas, B. C., LoBue, V., Buss, K. A., & PÃ©rez-



110

Edgar, K. E. (2017). Maternal anxiety predicts attentional bias towards threat in infancy. Emotion, 17(5), 874Â­883. https://doi.org/10.1037/emo0000275 Morton, J., & Johnson, M. H. (1991). CONSPEC and CONCERN: A two-process theory of Infant face recognition. Psychological Review, 98(2), 164Â­181. Moulson, M. C., Fox, N. A., Zeanah, C. H., & Nelson, C. A. (2009). Early adverse experiences and the neurobiology of facial emotion processing. Developmental Psychology, 45(1), 17Â­30. https://doi.org/10.1037/a0014035 Nelson, C. A. (1987). The recognition of facial expressions in the first two years of life: mechanisms of development. Child Development, 58(4), 889Â­909. https://doi.org/10.2307/1130530 Nelson, C. A., & de Haan, M. (1996). Neural correlates of infants' visual responsiveness to facial expressions of emotion. Developmental Psychobiology, 29(7), 577Â­595. https://doi.org/10.1002/(SICI)1098-2302(199611)29:7&lt;577::AIDDEV3&gt;3.0.CO;2-R Nelson, C. A., & Dolgin, K. G. (1985). The generalized discrimination of facial expressions by seven-month-old infants. Child Development, 56(1), 58Â­61. https://doi.org/10.1111/j.1467-8624.1985.tb00085.x Nelson, C. A., Morse, P., & Leavitt, L. (1979). Recognition of facial expressions by seven-month-old infants. Child Development, 50(4), 1239Â­1242. https://doi.org/10.2307/1129358 Ogren, M., Burling, J. M., & Johnson, S. P. (2018). Family expressiveness relates to happy emotion matching among 9-month-old infants. Journal of Experimental Child Psychology, 174, 29Â­40.



111

Orme, J. G., Reis, J., & Herz, E. J. (1986). Factorial and discriminant validity of the Center for Epidemiological Studies Depression (CESD) scale. Journal of Clinical Psychology, 42(1), 28Â­33. https://doi.org/10.1002/10974679(198601)42:1<28::AID-JCLP2270420104>3.0.CO;2-T Otsuka, Y., Konishi, Y., Kanazawa, S., Yamaguchi, M. K., Abdi, H., & O'Toole, A. J. (2009). Recognition of moving and static faces by young infants. Child Development, 80(4), 1259Â­1271. https://doi.org/10.1111/j.1467-8624.2009.01330.x Owens, M., Harrison, A. J., Burkhouse, K. L., Mcgeary, J. E., Knopik, V. S., Palmer, R. H. C., & Gibb, B. E. (2016). Eye tracking indices of attentional bias in children of depressed mothers: Polygenic influences help to clarify previous mixed findings. Development and Psychopathology, 28(2), 385Â­397. https://doi.org/10.1017/S0954579415000462 Parker, S. W., & Nelson, C. A. (2005). An event-related potential study of the impact of institutional rearing on face recognition. Dev Psychopathol, 17(3), 621Â­639. https://doi.org/S0954579405050303 [pii]\r10.1017/S0954579405050303 Peckham, A. D., McHugh, R. K., & Otto, M. W. (2010). A meta-analysis of the magnitude of biased attention in depression. Depression and Anxiety, 27(12), 1135Â­ 1142. https://doi.org/10.1002/da.20755 Peltola, M. J., Forssman, L., Puura, K., van Ijzendoorn, M. H., & LeppÃ¤nen, J. M. (2015). Attention to faces expressing negative emotion at 7 months predicts attachment security at 14 months. Child Development, 86(5), 1321Â­1332. https://doi.org/10.1111/cdev.12380 Peltola, M. J., Hietanen, J. K., Forssman, L., & LeppÃ¤nen, J. M. (2013). The emergence



112

and stability of the attentional bias to fearful faces in infancy. Infancy, 18(6), 905Â­ 926. https://doi.org/10.1111/infa.12013 Peltola, M. J., LeppÃ¤nen, J. M., & Hietanen, J. K. (2011). Enhanced cardiac and attentional responding to fearful faces in 7-month-old infants. Psychophysiology, 48(9), 1291Â­1298. https://doi.org/10.1111/j.1469-8986.2011.01188.x Peltola, M. J., LeppÃ¤nen, J. M., MÃ¤ki, S., & Hietanen, J. K. (2009). Emergence of enhanced attention to fearful faces between 5 and 7 months of age. Social Cognitive and Affective Neuroscience, 4(2), 134Â­142. https://doi.org/10.1093/scan/nsn046 Peltola, M. J., LeppÃ¤nen, J. M., Palokangas, T., & Hietanen, J. K. (2008). Fearful faces modulate looking duration and attention disengagement in 7-month-old infants. Developmental Science, 11(1), 60Â­68. https://doi.org/10.1111/j.14677687.2007.00659.x Peltola, M. J., LeppÃ¤nen, J. M., Vogel-Farley, V. K., Hietanen, J. K., & Nelson, C. A. (2009). Fearful faces but not fearful eyes alone delay attention disengagement in 7month-old infants. Emotion, 9(4), 560Â­565. https://doi.org/10.1037/a0015806 Pike, G. E., Kemp, R. I., Towell, N. A., & Phillips, K. C. (1997). Recognizing moving faces: The relative contribution of motion and perspective view informatio. Visual Cognition, 4(4), 409Â­438. https://doi.org/10.1080/713756769 Pizzagalli, D. A., Holmes, A. J., Dillon, D. G., Goetz, E. L., Birk, J. L., Bogdan, R., ... Fava, M. (2009). Reduced caudate and nucleus accumbens response to rewards in unmedicated individuals with major depressive disorder. American Journal of Psychiatry, 166(6), 702-710. https://doi.org/10.1176/appi.ajp.2008.08081201 Pollak, S. D., Cicchetti, D., Hornung, K., & Reed, A. (2000). Recognizing emotion in



113

faces: Developmental effects of child abuse and neglect. Developmental Psychology, 36(5), 679Â­688. https://doi.org/10.1037/0012-1649.36.5.679 Pollak, S. D., Klorman, R., Thatcher, J. E., & Cicchetti, D. (2001). P3b reflects maltreated children's reactions to facial displays of emotion. Psychophysiology, 38(2), 267Â­274. https://doi.org/10.1017/S0048577201990808 Pollak, S. D., & Sinha, P. (2002). Effects of early experience on children's recognition of facial displays of emotion. Developmental Psychology, 38(5), 784Â­791. https://doi.org/10.1037/0012-1649.38.5.784 Quinn, P. C., Anzures, G., Izard, C. E., Lee, K., Pascalis, O., Slater, A. M., & Tanaka, J. W. (2011). Looking across domains to understand infant representation of emotion. Emotion Review, 3(2), 197Â­206. https://doi.org/10.1177/1754073910387941.Looking Radloff, L. S. (1977). The CES-D scale: A self-report depression scale for research in the general population. Applied Psychological Measurement, 1(3), 385Â­401. https://doi.org/10.1177/014662167700100306 Recio, G., Schacht, A., & Sommer, W. (2013). Classification of dynamic facial expressions of emotion presented briefly. Cognition and Emotion, 27(8), 1486Â­ 1494. https://doi.org/10.1080/02699931.2013.794128 Reynolds, G. D., & Richards, J. E. (2008). Infant heart rate: A developmental psychophysiological perspective. Developmental Psychophysiology: Theory, Systems, and Methods. https://doi.org/10.1017/CBO9780511499791.009 Roy, A. K., Dennis, T. A., & Warner, C. M. (2015). A critical review of attentional threat bias and its role in the treatment of pediatric anxiety disorders. Journal of Cognitive



114

Psychotherapy, 29(3), 171Â­184. https://doi.org/10.1891/0889-8391.29.3.171 Sadr, J., Jarudi, I., & Sinha, P. (2003). The role of eyebrows in face recognition. Perception, 32(3), 285Â­293. https://doi.org/10.1068/p5027 Scheller, E., Buchel, C., & Gamer, M. (2012). Diagnostic features of emotional expressions are processed preferentially. PloS One, 7(7), e41792. https://doi.org/10.1371/journal.pone.0041792 Schurgin, M. W., Nelson, J., Iida, S., Ohira, H., Chiao, J. Y., & Franconeri, S. L. (2014). Eye movements during emotion recognition in faces. Journal of Vision, 14(13), 1Â­ 16. https://doi.org/10.1167/14.13.14 Sherrod, L. R. (1979). Social cognition in infants: Attention to the human face. Infant Behavior and Development, 2, 279Â­294. https://doi.org/10.1016/S01636383(79)80037-5 Smith, M. L., Cottrell, G. W., Gosselin, F., & Schyns, P. G. (2005). Transmitting and decoding facial expressions. Psychological Science, 16(3), 184Â­189. https://doi.org/10.1111/j.0956-7976.2005.00801.x Striano, T., & Rochat, P. (2000). Emergence of selective social referencing in infancy. Infancy, 1(2), 253Â­264. https://doi.org/Doi 10.1207/S15327078in0102_7 Sugden, N. A., Mohamed-Ali, M. I., & Moulson, M. C. (2014). I spy with my little eye: Typical, daily exposure to faces documented from a first-person infant perspective. Developmental Psychobiology, 56(2), 249Â­261. https://doi.org/10.1002/dev.21183 Termine, N. T., & Izard, C. E. (1988). Infants' responses to their mothers' expressions of joy and sadness. Developmental Psychology, 24(2), 223Â­229. https://doi.org/10.1037/0012-1649.24.2.223



115

Thomas, J. C., Letourneau, N., Campbell, T. S., Tomfohr-Madsen, L., Giesbrecht, G. F., Kaplan, B. J., ... Singhal, N. (2017). Developmental origins of infant emotion regulation: Mediation by temperamental negativity and moderation by maternal sensitivity. Developmental Psychology, 53(4), 611Â­628. https://doi.org/10.1037/dev0000279 Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., ... Nelson, C. (2009). The NimStim set of facial expressions: Judgments from untrained research participants. Psychiatry Research, 168(3), 242Â­249. https://doi.org/10.1016/j.psychres.2008.05.006 van der Schalk, J., Hawk, S. T., Fischer, A. H., & Doosje, B. (2011). Moving faces, looking places: Validation of the Amsterdam Dynamic Facial Expression Set (ADFES). Emotion, 11(4), 907Â­920. https://doi.org/10.1037/a0023853 Vinette, C., Gosselin, F., & Schyns, P. G. (2004). Spatio-temporal dynamics of face recognition in a flash: It's in the eyes. Cognitive Science, 28(2), 289Â­301. https://doi.org/https://doi.org/10.1016/j.cogsci.2004.01.002 Wagner, J. B., Luyster, R. J., Yim, J. Y., Tager-Flusberg, H., & Nelson, C. A. (2013). The role of early visual attention in social development. International Journal of Behavioral Development, 37(2), 118Â­124. https://doi.org/10.1177/0165025412468064 Walker-Andrews, A. S. (1986). Intermodal perception of expressive behaviors: Relation of eye and voice? Developmental Psychology, 22(3), 373Â­377. https://doi.org/10.1037/0012-1649.22.3.373 Walker-Andrews, A. S. (1988). Infants' perception of the affordances of expressive



116

behaviors. In Advances in Infancy Research, 5, 173Â­221). Walker-Andrews, A. S. (1997). Infants' perception of expressive behaviors: differentiation of multimodal information. Psychological Bulletin, 121(3), 437Â­456. https://doi.org/10.1037/0033-2909.121.3.437 Walker-Smith, G. J., Gale, A. G., & Findlay, J. M. (1977). Eye movement strategies involved in face perception. Perception, 6(3), 313Â­326. https://doi.org/10.1068/p060313 Walker, A. S. (1982). Intermodal perception of expressive behaviors by human infants. Journal of Experimental Child Psychology, 33(3), 514Â­535. https://doi.org/10.1016/0022-0965(82)90063-7 Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063Â­1070. https://doi.org/10.1037/00223514.54.6.1063 Wehrle, T., Kaiser, S., Schmidt, S., & Scherer, K. R. (2000). Studying the dynamics of emotional expression using synthesized facial muscle movements. Journal of Personality and Social Psychology, 78(1), 105Â­119. https://doi.org/10.1037//00223514.78.1.105 Weinberg, M. K., & Tronick, E. Z. (1998). Emotional characteristics of infants associated with maternal depression and anxiety. Pediatrics. https://doi.org/10.1542/peds.102.5.SE1.1298 Weyers, P., MÃ¼hlberger, A., Hefele, C., & Pauli, P. (2006). Electromyographic responses to static and dynamic avatar emotional facial expressions. Psychophysiology, 43(5),



117

450Â­453. https://doi.org/10.1111/j.1469-8986.2006.00451.x Wilcox, B. M., & Clayton, F. L. (1968). Infant visual fixation on motion pictures of the human face. Journal of Experimental Child Psychology, 6(1), 22Â­32. https://doi.org/10.1016/0022-0965(68)90068-4 Xiao, N. G., Perrotta, S., Quinn, P. C., Wang, Z., Sun, Y. H. P., & Lee, K. (2014). On the facilitative effects of face motion on face recognition and its development. Frontiers in Psychology, 5, 633. https://doi.org/10.3389/fpsyg.2014.00633 Xiao, N. G., Quinn, P. C., Liu, S., Ge, L., Pascalis, O., & Lee, K. (2015). Eye tracking reveals a crucial role for facial motion in recognition of faces by infants. Developmental Psychology, 51(6), 744Â­757. https://doi.org/10.1037/dev0000019 Yoshikawa, S., & Sato, W. (2008). Dynamic facial expressions of emotion induce representational momentum. Cognitive, Affective and Behavioral Neuroscience, 8(1), 25Â­31. https://doi.org/10.3758/CABN.8.1.25



118


