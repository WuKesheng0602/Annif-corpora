Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2003

An optimized SVM kernel for texture classification and its application in microcalcification detection
Mahdi Sabri
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Electrical and Computer Engineering Commons Recommended Citation
Sabri, Mahdi, "An optimized SVM kernel for texture classification and its application in microcalcification detection" (2003). Theses and dissertations. Paper 196.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

AN OPTIMIZED SVM KERNEL FOR TEXTURE CLASSIFICATION AND ITS APPLICATION IN MICROCALCIFICATION DETECTION
By
Mahdi Sabri Bachelor of Electrical Engineering Sharif University of Thecnology Tehran, Iran

-

A thesis submitted to Ryerson University in Partial fulfilment of the requirement for the degree of Master of Applied Science in the program of Electrical and Computer Engineering.

Toronto, Ontario, Canada, 2003
~~ahdi

Sabri 2003

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

UMI Number: EC52894

INFORMATION TO USERS

The quality of this reproduction is dependent upon the quality of the copy submitted. Broken or indistinct print, colored or poor quality illustrations and photographs, print bleed-through, substandard margins, and improper alignment can adversely affect reproduction. In the unlikely event that the author did not send a complete manuscript and there are missing pages, these will be noted. Also, if unauthorized copyright material had to be removed, a note will indicate the deletion.

Â®

UMI
UMI Microform EC52894 Copyright 2008 by ProQuest LLC. All rights reserved. This microform edition is protected against unauthorized copying under Title 17, United States Code. ProQuest LLC 789 E. Eisenhower Parkway PO Box 1346 Ann Arbor, MI 48106-1346

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Acknowledgment
I would like to express my gratitude to my supervisor Prof. Javad Alirezaie. Not only this work, but also my higher education was not possible without his support. I am also thankful to Prof. Krishnan for introducing me to the field of statistical signal processing and helpful discussion during the course of my studiC's at Ryerson University. I would like to thallk the thesis exam committee members for their valuable COllllnellts and discussions. I would like to acknowledge Prof. }'laIlUel Dayy and Dr. Kim for useful discussion and provided codes. I \vould like to thank my parents who raised me in a way that I always believe that knowledge is a value that cannot be replaced eyen hy great wealth. Also to my wife who highly supported me and patiently accompanied me through all the difficulties we had during my fl.1ASc education.

iii

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Contents

1 Introduction
1.1 1.2 1.3 1.4 I\Iotiva tiOll . A CAD system based on texture features Summary of contributions Thesis outlines ..... .

1

1 2

3
4

2

SVM Learning Classifier
2.1 2.2 Linear SVI\1 Classifier .

5

5
8
12
12 13

Non-Linear SVI\1 Classifier.

3 Time series and Time-frequency features
3.1 Local discriminant basis . . . . . . . . . . 3.1.1 3.1.2 3.2 \Vavelet and \Vavelet Packet Transforms LDB Algorithm .

17

Linear predictive coding 3.2.1 3.2.2 The LPC Model LPC analysis equation . The Autocorrelation method .

20 20
22
24

3.2.3
3.3

Cohen's Space-Frequency signal representation.

28

IV

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4

A Novel SVM Kernel for Texture Classification
4.1 4.2 4.3 1vIotivation for texture classification Applications . . . . . . . . . . . . . Texture Classification Challenges and Previous works 4.3.1 4.3.2 4.4 4.5 Feature Extraction Classification . . .

30

30
30

31 31 32

Ext.enwl features to build a llew kernel for SVl\I Classification and segmentation Results . . . . . 4.5.1 4.5.2 4.5.3 Comparison with Original SVM method Comparison with Other Classification methods. Segmentation Results and Comparison

33
35

36
37 38
43

4.6
5

Gabor Filter Banks . . . . . . . . . . . . . . .

Detection of Abnormalities In Mammograms
5.1 5.2 .Motivation for a CAD system Previous work . . . . . 5.2.1 5.2.2 5.2.3 5.2.4 5.2.5 5.2.6 5.2.7 5.2.8 5.2.9 Shape Analysis Iterative feature extraction. Thresholding and morphology Multiscale matched filters ..

45

46

48 48 48
49

49
50

Statical feature of surrounding region Laplacian scale-space . . . . . . . . . Spatial statistical features alld wavelet features Support Vector l\'Iachine . . . . . . . . Multi-resolution Based Segmentation.

51 51 51 52

v

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

5.3

Proposed Algorithm 5.3.1 5.3.2 5.3.3 5.3.4 Image segmentation. Enhancement of the mamlllograms Preprocessing............ Reduction of False positives through supervised learning

55
56

57
59
65 66

5.4

Experimental Results

5.4.1
5.4.2 5.4.3

Database Selected Data Set Results

66

69
71

6 Conclusion and future work
6.1 6.2 SV:l\1 in texture classification and external texture features. Incorporating the Gabor filter Bank to Increase the performance of LPC-SVM texture classification algorithm.

75

75

76 76
78 78
89

6.3
6.4 6.5

~licroca1cification

detection

Research contributioll . Future work . . . . . .

Vita

VI

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

List of Figures
3.1
3.2

vVavelet and Sine wave . . . . . . . . . . . . . . . . . . . . Computing wavelet coefficients via subband coding method Signal Decomposition to wavelet coefficients . . Generalization of subband coding for 2D Signal vVavelet Packet transform: An extension to wavelet transform \Vavelet Packet decomposition (quad-tree for image signal) .. LDB best basis selection sclwme for a two class{A,B) problem Linear Prediction Coding IVlodel Signal spectra is compared with its estimated spectra using LPC of order a) 6 b) 12

14 15 15
16 16

3.3
3.4
r.: 3 .0

3.6 3.7 3.8 3.9

17
19

20

26
27

3.10 Spectral estimation of sample speech signal, "'ith a LPC of order a) 24 b) 48 4.1 Average VC-Dimension for three kernel fUllctions based on the three feature extraction reviewed in chapter 3 4.2 Two-texture images used in experiments (a) D4, D84 (b) D5, D92{ Brodatz album) 4.3 multitexture images used in experiments

36

38 39

vii

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4.4

multi texture images segmentation:(a) Five texture Brodatz image originally published by Jain and Farrokhnia(b) segmentation with LPC-SVl'.l before post processing (c) segmentation with LPC-SVM after post processing (d) segmentation using Optimized Gabor Filter (e)segmcntation using Optimized Gabor Filter (sigmoidal activation) . . . . . . . . . . . . . . . . . . . . . .. 42

5.1

Sample of lllicrocalcifieation in a mammogram. shown in the right after zooming and enhancement.

The lllierocakifieatioll is

47
55 60 61

5.2 5.3 5.4 5.5

Overall flow of the developed algorithm for detection of microcalcifieation
Preproces~ing

steps

Orthogonal subhands at different resolutions Mother wavelet for DAUB 4, a narrow and tall wavelet for analyzing highfrequency characteristics . . . . . . . . . . . . . . . .

63

5.6

1'.10ther wavelet for DAUB 20,(a) a wide and short wavelet for analyzing low-frequency characterbtics(b) a narrow and tall wavelet for analyzing highfrequency characteristics 64

5.7
5.8 5.9

Sample ics file Chain codes values and directions Sample Overlay File ...... .

67

68
G8
72

5.10 ROC curves show that the proposed algorithm outperforms SVM 5.11 ROC curve for the most recent published research on DDSM

73

Vlll

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

List of Tables
4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 I3rief comparison between SV:1\1 and NN Source of test images . . . . . . . . . . . Error Rates (percent) for two-texture image (Fig.4.2a). Error Rates (percent) for two-texture image(Fig.4.2b). Error Rates (percent) for llluititexture imagc(Fig4.3.a). Error Rates (percent) for multitexture image(Fig4.3.b). Comparison of LPC-SVM with SGLD1'.1,FPS,T\iVT,LAWS, GAI30R method Comparison of Correct Classification Rate in Logic Operator 1'.1ethod and LPC-SVM 4.9 Comparison of Correct Classification Rate in vVavelet Transform Method and LPC-SVM 4.10 Error Rates (percent) for t\vo-texture image (Fig.4.2a). 5.1 5.2 5.3 5.4 5.5 Rate of Breast Cancer mortality in some developed countries. Summary of related research in calcification detection Coefficients of the DAUB filters. ........... 41

33
38

39 39
40 40 40

41

44
45 54 62 70 74

Summary of ground truth information for the test images The number of false positives and correct true positives in each test image

IX

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 1 Introduction

1.1

Motivation

T

HE most frequently diagnosed cancer for Canadian women is by far breast cancer. This is applicable to the most of the developed countries. An average Canadian

\"oman has a 1 in 9 chance to develop breast cancer in her life time. Each day more than 10 women die in Canada from breast cancer. Because the Canada population is aging, the number of women who will die from breast cancer will increase [1]. There is no cure for breast cancer. The only effective way to save the patient life is early detection and removing cancerous cells. X-ray mammography is a very important tool and the most commonly used method for early detection. For women aged over 40 years old, screening mammography is recommended once a year. This result in huge number of mammograms that they need to be examined and interpreted by trained radiologists. Unfortunately, mammograms are one of the most difficult medical images to interpret. Specifically, visual assessment of microcalcification turns out to be a challenging task. Approximately 25% of the cancers that are visible on retrospective review are failed to detect by radiologists [2].
1

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

2 Thus, computer aided diagnosis (CAD) systems are motivated to help the radiologists with this tedious task. In fact, the goal of a CAD systems is not to create a high-tech radiologist or automate the detection procedure. CAD systems are intended to act as the first cut in the process and a second opinion for the radiologists. The term" second Opinion" means that the radiologists can use the results of a computer analysis of the mammogram in making a diagnosis. A CAD system tries to locate suspicious regions in the mammogram for more detailed examination by the radiologists.

1.2

A CAD system based on texture features

Ovcr last two decades several approaches have becn used by image processing scientists and researchers to develop a CAD system for microcalcification detection. A series of different methods and research results are briefly addressed in chapter five. Texture features are

important and commonly used. It is not only used in breast cancer detection, but also is extensively used in other applications which deal with abnormality detection in medical images. Any texture classification method involves two major steps:

Â· Feature Extraction Â· Classification
In this thesis both concepts are studied and a new method is proposed. Researchers have found the feature extraction more challenging and numerous different methods have been proposed and applied. Recently, Kim et. al. [3] showed the effectiveness of the Support

Vector I\1achine (SVM) in texture classification and its advancement over neural networks. Since SVM performs well in high dimensional space such as the space spanned by texture images, they successfully employed SVM without any external features. In fact, the kernel function in SVM algorithm, implicitly performs feature extraction. Since SVM is basically

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

3

suited for two-class classification problems, it is potentially a good choice for several different medical imaging which deal with abnormality detection. The main contribution of this thesis in the sense of texture classification is proposing a new texture classification algorithm by effectively employing ext.ernal features wit.hin SVI\l kernel and introducing a new feature extraction method for texture classification.

1.3

Summary of contributions

Â· Developing a new texture classification algorithm by proposing a new SV:i\1 kernel which incorporates external features.

Â·

Proposing an extension to aforementioned algorithm by using Gabor filter banks to en-

rich the external features, for the case of highly non-stationary signal and image classification.

Â·

Achieving promising microcalcification rate by applying the new texture classification

algori HUll.

Â·

Overcoming the asymmetry of training data

III

mammograms by applying a wavelet

based pre-process stage to detect sllspicious sites.

Â· Discussing t.he clinically assessed difficulty level of mammograms in interpreting of classification rates.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

4

1.4

Thesis outlines

SVI\I is reviewed in Chapter two. Three feature extraction nlC'thod are considered to be
used within SVl\1 kernel. These feature algorithm are presented in Chapter three. In Chapter four, texture classification challenges are discussed and previous works are cited. The three feature extraction algorithm presented in Chapter three, are compared by their estimated VC-dimeusioll. LPC has been fOllnd superior and is used within a new kernel for SVl\I. The new texture classification algorithm is compared \vith several traditional texture dassificatiom; and some recently reported novel methods. In Chapter five, previous works in micro calcification detection are addressed and the new algorithm is applied to mammograms following a pre-process stage. Chapter six is dedicated to conclusion aud intellded future works.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 2 SVM Learning Classifier

S

UPPORT vector machine is basically proposed for the case of binary classification. The main idea of support vector machine is to construct a hyperplane as the decision

surface in such a way that the margin of separation between positive imd negative examples is maximized. This goal is motivated by principle:,; of statifitical learning theory and the method of structural risk minimization [4], [5]. Indeed, the support vector machine ifi an approximate implementation of the structural risk minimization. According to the statistical learning theory, the error rate of a learning machine on the test data (i.e. the generalization error rate) is bounded by the sum of the training-error rate and a term that depends on Vapnick-Chervonenkis(VC) dimension [4]. In the case of separable patterns, SV11 produces a value of zero for the first term and minimize the second term. Accordingly, The optimal hyperplane which is sought by SVM is equivalent to minimum bound on the VC-dimension. In the following, we first review the formulation of SVM in the simple case of separable patterns and then we discuss the case of non-separable case.

2.1

Linear SVM Classifier

Let vector x E X denote a pattern to be classified, and let scalar y denotes its class label Y E {Â± 1}. In addition, let {(Xi, Yi), i

=

1, 2, ... , I} denote a given fiet of I training exam pIes.

5

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

6 The problem is how to construct a classifier [i.e., a decision function I(.?:)] that can correctly classify an input. pattern that. is not necessarily from the training set.. On the assumption of linearly separable case, there exist a linear function of the form

(2.1)
Such that for each training example
Xi,

the function yields f(x.i)

>

a for

:t}i

+1, and

f (Xi) < 0 for

J-Ii = -l. In ot.her words, training examples from the t.wo different classed are

s<'parated by the hyperplane f(x)

= w T X + b = O.

For a given training set, while there may exist many hyperplane that separate the two classes, the SV1\1 classifier is based on the hyperplane that maximizes the separating margin bet\vpen the two classes. In other words, SVI\i finds the hyperplane that causes the largest separation between the decision function values for the borderline members of the two classes referred as support vectors. This hyperplane can mathematically be found by minimizing the following cost function:

1 1 W(w) = -wTw = -II

2

2

W

112

(2.2)

Subject to separability constraints:

for Yi = +1
and

wTxÂ·l +b <-1 _ ,

for Yi

= -1

(2.3)

i=1,2, .. ,I.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

7

Equivalently, these constraints can be written more compactly as

Yi (W

T

Xi

+ b)

~ 1,

for 'i = 1,2, .. , I.

(2.4)

This specific problem forlllulation lllay not be useful in practice because the training data may not. be completely separable
b~T

a hyperplane. In this case, slack variables, denot.ed by

f;i, can be introduced to relax the sC'parability constraints in (2.4) as follows:

(2.5)
Accordingly, the cost function in (2.2) can be modified as follows:

(2.6) where C is a user-specified, positive, regularizat.ion parameter. In (2.6), the variable ~ is a vector containing all the slack variables
~i,

i = 1,2, ... , I.

The modified cost function in (2.6) constitutes the so-called structural risk, which balances empirical risk (i.e., the training error reflected by second term) with model

complexity (t.he first term) [6]. The regularization parameter C controls this trade-off.
The purpose of llsing model complexity to constrain the optimization of empirical risk is t.o avoid overfilling, a situation in which the decision boundary too precisely corresponds to the training data, and thereby fails to perform well on data outside the training set..

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

8

2.2

Non-Linear SVM Classifier
1110-

In practice, input patterns are unlikely to be linearly separable. Non-linear SVr'vl is

tivated by Cover's theorem [7] on the separability of patterns. The theorem states that snch a linearly non-separable pattern space can be transformed into a new feature space where patterns are liuf'arly separable with a high probability. According to the theorem, the transform must be non-linear and the dimellsion of feature space lllust be high enough. Let x denote a vector drawn from the input space X, assumed to be of dimension mo. Let
{cI>j(X)}j~]

denote a set of non-linear transformatiolls from the input space to the feature
mI.

space of dimension

It is assumed that {cI>j(x)} is defined a prior for all j. Then, a

hyperplane acting as decision surface is defined as:

L

111)

'Wj<l>j(x)

+b= 0

(2.7)

j=]

where {1Oj} j~l denotes a set of linear weights com1E'cting the feature space to output space, and b is the bias. The equation (2.7) can be simplified by writing:

L wjcI>j(x) = 0
j=O

112}

(2.8)

wllf're it is assumed that cJ>o(x) = 1 for all x, so that Wo denotes the bias b. Equation (2.8 ) df'fines the decision surface computed in the feature space in terms of the linear weights of the machine. The quantity cI>j(x) represent the input supplied to the weight
Wj

via the

feature space. In fact, the vector [<l>o(x), cI>l(X), ... , cI>m1 (X)]T can be considered the "image" induced in the feature space due to the iuput vector x. The problem of finding weight coefficients w can be formulated as an optimization problem with constraint. It can be shown that finding optimal hyperplane is equal to minimizing the cost function [G]:

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

9

(2.9)
The parameter C is regarded as the regularization parameter and is selected by user. A large C corresponds to assigning a higher penalty to the training errors. Given the training samples as {(Xi, di)}~l' the constraiuts which must be satisfied are:

(2.10)
This constrained optimization problem call be solved using the lagrange multiplier. The Lagrangian function is cOllstructed as:

(2.11) where the nonnegative variables
0:;

are called Lagrange multipliers. The solution to <.:on-

strained optimization problem is determined by the saddle point of the Lagrange function J(w, b, 0:), which has to be minimized with respect to wand b. It also has to be maximized with respect to
0:.

Applying the optimality conditions:

8J(w, o,~, b)

=

0

Dw
DJ(w, o:,~, b) = 0 8b DJ(w, o:,~, b) = 0
8~

(2.12) (2.13) (2.14)

to Lagrange function (2.11 ) yields:

W=

L O:idi <I> (Xi)
i=l

N

(2.15)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

10
Thell, the non-linear SV11 classifier is obtained as:

N

f(x) = LOidi<I>T(Xj)<I>(x)
i=1

+b=

N

LQidiK(Xj,x)
i=1

+b

(2.16)

where the function K(.,.) is defined as:

(2.17)

K is referred as kernel function.
Practically Lagrallge 11ultipliers arc solved from dual form of (2.9), which is expressed as:

N
" L 0 i=1
1

1
-

N

N

" 2" LL i=1 j=1

etaÂ·d ' J JÂ·K(xÂ· I, xÂ·) J

(2.18)

Subject to:

0::;

Qi::;

C,

i

= 1,2, ... ,N
N

(2.19)

LQidi = 0
i=l

(2.20)

The dual problem is solved numerically through quadratic programming. The Karush-KuhnTucker optimality conditions for (2.18) lead to the following three cases for each Q( a) This corresponds to d;J(Xi) > 1. In this case, the data element margin of the function f(x) and is correctly classified.
Xi

Qi

= O.

is outside the decision

b) 0 <

Qi

< C. In this case,

d;J(Xi) = 1. The data element
Xi

Xi

is strictly located on the decision margin of f(x). Hence,
Qi

is referred as a margin support vector for f(x). c)

= C. In this case, d;J(Xi) < 1.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

11

The data element Accordingly,
Xi

Xi

is inside decision margin (though it lIlay still be correctly classified).

is called an error support vector of f(x). The most of the training examples

in a typical problem are correctly classified by the trained classifier ( case a), i.e., only a few training exalllpies will be support vectors. For simplicity, let
Sj,

nj,j = 1,2, ... , N s , denotes

these support vectors and their corresponding nonzero Lagrange llluitipliers respectively. The decision function in (2.1G) can be simplifiC'd as:

(2.21)
As it can be seen in (2.18) and (2.21), The nonlinear lllapping
~(.)

never apears explicitly in

either dual form of SV11 training problem or the resulting decision function. The mapping
~(.)

only enters the problem implicitly through the kernel function K(., .), thus it is only
~(.).

necessary to define K(.,.) which implicitly defines

However, when choosing a kernel

function, it is necessary to check that it is associat(>d with the iuner product of some nonlinear mapping. 1Iercer's theorem [G] titates that such a mapping indeed underlies a kernel
K(.,.) provided that K(.,.) is a positive integral operator, that is, for every square-integrable

function g(.) defined on It' the kcmel K(., .) satisfies the following condition:

JJ

K(x,y)g(x)g(y)dxdy

~0

(2.22)

A new kernel is proposed in Chapt.er 4 hased on feat.ure extraction methods reviewed in Chapter 3.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 3 Time series and Time-frequency features

F

EATURE selection is a crucial step in sigual and image classification and pattern recognition. The goal of feature selection is having large" between class distance" and

small "within-class variance" in the feature vector space. SVJ\'I classification method basically does not incorporate any external feature. In fact the kernel function implicitly extract features within the learning scheme. The main focus of current and following chapter is investigating the effectiveness of employing external features \\'ithin SV:M kernel. In the following sections three feature

extraction methods are reviewed and they will be compared in the next chapter.

3.1

Local discriminant basis
[8] is a supervised scheme for feature extraction and is

Local discriminant basis (LDB)

the classification counterpart of the Best Basis algorithm developed by Coifman and Wickerhauser [9] for signal aud image compressioll. The appeal of LDB algorithm lies ill the

supervised selection of basis functions from redundant and structured basis that are well localized both in time and frequency. The tree structured dictionary of the basis, which is provided by wavelet packet transform, is pruned to find the most discriminative basis for the

12

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

13 purpose of the classification.

In the following subsections, wavelet, wavelet packet, and LDB algorithm are briefly
reviewed.

3.1.1

Wavelet and Wavelet Packet Transforms

Fourier transform has been the principle tool for signal analysis for a long time. It constructs a siuusoidal basis to describe the energy distribution of a signal in frequency domain. But Fourier transform has a drawback that the time (or spatial) information is lost due to the integration over entire real axis. Thus by looking at the Fourier Transform domain it. is impossible to say when (or where) a particular phenomena took place. However most of the practical signal and images contain quasi-periodic and quasi-stationary characteristics that capturing them is crucial for classification purpose. \Vindowing the signal and applying Fourier Transform to each window (Short Time Fourier Transform (STFT) ) was the first attempt to overcome this problem. STFT in turn, reduces the frequency resolution which is in compliance with Heisenberg uncertainty principal [10J. Wavelet transform, which is based on multiresolutional analysis, were proposed and developed to overcome fixed resolution problem in STFT. Similar to the Fourier transform, that describes a signal iu terms of the sinusoidal waves of various frequencies. wavelet analysis is an attempt to describe a signal according to shifted and scaled versions of a basis function. This basis function is referred as mother wavelet. By considering the wavelets and sine waves, depicted in Fig 3.1, we can see intuitively that signals with sharp changes might be better analyzed with an irregular wavelet than with a smooth sinusoid. It also makes sellse that local features can be described better with wavelets that have local extent. The discrete wavelet transform is defined as [10]:

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

14

(a) w3vckt

(IÂ» sine

WHYC

Figure 3.1: \Vavelct and Sine \>,rave

s(n)<h(n) vN 11 . 1 W 'll (), k) = IN ~ s(n)wj,d n )
I7\T

. W q,(), k) =

1

L

(3.1)
(3.2)

n = 0"" ,N-1

J = 0,"', J - 1

J

= log2 N

where N is signal length and IV<j, and IV'll are approximation and detailed coefficients

respectively. j and k denote scale and translation. wand <I> are wavelet and scale function.

:rvI all at [11] implemented an algorithm known as two-channel subband coder to obtain the
discrete wavelet coefficient which is described in Fig. 3.2. The expansion can be continued

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

15

J times (Fig. 5.4) for complete signal decompositiOll resulting in the complete set of wavelet
coefficients. For the sake of image processing applications the subband coding scheme of Fig. 3.2 is generalized as depicted ill Fig. 3.4.

low-pass filter

dowllsampie
F

approximatIon caeffici en is

s

-q

1.0_0

HLD
high-pass filter

G

:1

,2 I dow1/samp!c
s

l

2

..

cAl

..

CDl

detail ('O(! {fide nts

(a)

Figure 3_2: Computing wavelet coefficients via subband coding method

cAl

~
2

eD,

cA 2

~ cD

~
Â·
Â· Â·
Â· Â·

Â·
(a)

Figure 3.3: Signal Decomposition to wavelet coefficients

The wavelet transform is extended to wavelet packet decomposition which offers a richer range of pos~;;ibilities for signal analysis. In wavelet packet transform after splitting the signal to approximation and detail in first stage we continue the splitting over the detail part as

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

16
(:UI&#IIII.,5.

1\. J

cDj .. 1
\''''1,~al

.d,

CDj+l
diaSClla/

(a)

Figure 3.4: Generalization of subband coding for 2D Signal

well as approxilllatioll part. Thus, for

TI

level decomposition there will be n + 1 possibilities.

These different possibilities construct the redulldant library, which is used by LDB.

Â· Â·

Â·

Â·

Â·

Â·

Â· Â·

Â· Â·

Â· Â·
(a)

Â·

Â·

Â·

Â·

Â· Â·

Â·

Â· Â·

Â·
Â· Â·

Figure 3.5: \Vavclet Packet transform: An extension to wavelet trawiforIll

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

17

(a)

Figure 3.6: \Vavclct Packet decomposition (quad-tree for image signal)

3.1.2

LDB Algorithm

The graphical LDB scheme is shown in Fig. 3.7. In the following a formal statement of the algorithm is presented. Assume that set of N training signals from L different classes are given and Ni is the number of signals in class l. Let

s;l)

denote the collection of training signals in class l, in

which the superscript (I) indicates the class that the signal belongs to. Suppose that each s belongs to a unique class so that N = N]

+ N2 + ... + Nt.

LDB uses this training set of

signals to search for a best basis in available libraries of bases respect to its cost function.

Definition I Given a sequence of vectors {s(c)}~~l (C is number of classes), their Jdivergence is defined as in (3.3) where the summation is taken over all pairs of i and that are not equal.
(3.3)

Definition II Let {slc)}~] be a set of training signals belonging to class c. Thell the time-

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

18 frequency energy map of class c, denoted by

r

Cl

is a table of real numbers specified by triplet

(j, k, I):

r (. k
c), "

I) =

,,11/c (wT s(e))2 L..,=1 J.k,f I

,,'!c

L..,=1

II 8, ,(e) II

(3.4)
j -

where w is a basis function and j = 0,1, ... , J ,k = 0,1, ... ,2) -1 , and 1 = 0,1, .. " 211o In other word,

1.

rc

is computed by accumulating the squares of expansion coefficients of the

sigllals at each position in the binary tree followed by the normalization of the total energy of the signals belonging to class c. This normalization is important especially if there is significant difference in the lllunber of samples among classes. The following notation is used in LDB algorithm stated below,

~j,/.' = J( {reCi, k, ')}~=l =

L
f

J(rlCi, k, I), ... , reCi, k, l))

(3.5)

Here is an algorithm to select an orthonormal basis ( from the library), which maximizes the discriminant measure on the time-frequency energy distributions of classes. Let a set of basis vectors at the subspace 0. j ,k arranged as a matrix: denote

Bj,k

(3.6)

Let the

Aj,k

represent the LDB (which the algorithm is searching for) restricted to

Bj,k.

Algorithm (The Local Discriminant Basis Selection Algorithm) . Given a training
database of classes of signals:

Step

o.

Choose a library of orthonormal bases(i.e. specify Ql\fFs for wavelet packet).

Step 1 Construct time-frequency energy lllap Step 2 Set
Aj,k

re

for c = 1, .. , C

=

Bj,k

for k

= 0,1, ... , 2J

-

l.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

10

Step 3 DeterminE' the best subspace Aj,I,' for j = J - 1, .",0 and k = 0,1, "" 2j
following rule:

-

1 by the

then A j,k

=

Bj,k

Step 4. Order the basis functions by their power of discrimination (see Step 5), Step 5. Use k( < 11) most discriminant basis functions for constructing classifiers,

a

D
I S C

7

7

R I

A

N

T

Ba!s
_

Search the best discriminative basis

f;

7

2

v e

R
U E

-~~ ~
7

a
g

e

Z

I I

1

=--k=

I
I

(a)

Figure 3.7: LDB best basis selection scheme for a two class(A,B) problem

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

20

3.2

Linear predictive coding

Lincar predictive coding (LPC) is a popular and effective technique for signal compression [12], [13]. In particular, it has been employed successfully for speech coding [14], [15], [16],

[17] and image compression [18].
Usually, in a linear predictive coder, a sample of the image signal to be coded is first pn'dictC'd. If tIl(' sample is predicted as
a

weighted

Sl1m

of other samples of this signal, the

LPC is called the autoregressive (AR). If the sample is predicted from a known excitation, the LPC is referred to as the moving average (IvIA) model. if the sample is predicted using both the signal to be coded and an excitation signal, the LPC is known as the autoregressive moving average
(AR~lA)

model. In this thesis, the (AR) model has been used and it is

reviewed in the following subsections.

3.2.1

The LPC Model

u(o)

A(z)

I----.~

S(o)

G

(a)

Figure 3.8: Linear Prediction Coding I\lodcl

The basic idea behind the LPC model is that a given signal s(n), can be approximated as a linear combination of the previous p signal samples, such that

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

21

8('11)

~

018(n - 1)

+ 028(n - 2) + ... + aps(n - p)

(3.7)

The equation 3.7 can be converted to an equality by including an hypothetical input term,
Ou(n), which is referred as excitation in speech analysis literature. The ('quation can be

written as:
]J

8('11) = LOis(n - i)
;=1

+ ClI(n)

(3.8)

where 11(11) is a normalized hypothetical input and G is its gain. By expressing Eq. 3.8 in the z-domain we get the relation:

11

S(z) = LQ.iZ-iS(Z)
;=1

+ GU(z)

(3.9)

leading to the transfer function:

1 A(z)

(3.10)

Fig. 3.8 is presenting the transfer function in 3.10. The linear prediction model indicates that a signal 8(n) can be estimated by a system of order p with a scaled input u(n). While the signal is quasi-stationary the input. can be a random noise or arbitrary quasi-periodic signal. Accordingly, the A(z) coefficients can provide an efficient feature for preseuting the signal s(n).

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

22

3.2.2

LPC analysis equation

Based on the model of Fig. 3.8 the estimate s( n) is defined as:
l'

S= and the prediction error is:

L
/"=1

oks(n - k)

(3.11)

C(lI)

=

S(l1) -.5 = s(n) -

L
1.-=1

l'

(1k8(n - k)

(3.12)

Now a set of coefficients lllust be sought to minimize mean square error in a short segment of the signal. To set up the equatiolls that lllust be solved to dC'terminc the predictor coefficients, the short-term signal and error segments arc defincd at signal sample n as:

srlm)

=

s(n + 1/1); en(m) = e(n + m)

(3.13)

I

where m is the segment length. Then the mean square error which must be minimized can be written as:

I

I I
I
/'

En = L[Sn(m) , Tn

L
1.-=1

p

aJ.;Sn(m -

kW

(3.14)
OJ.:

To solve Eq. 3.14, for the predictor coefficients, we differentiate En with respect to each and set the results to zero,
k=1,2, ... ,p

I

(3.15)

giving
p

LSn(m -- i)sn(rn) =
m

L
J.:=1

QJ.:

L
rn

sn(m - i)sn(m - k)

(3.16)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

23
Knowing t hat terms of the form of s,Jrn) , i.e.,

I: STl enl - i) STl (m - k) are terms of the short-term covariance

rPlI(i, k) =

L
111

sTl(rn - i)sn(m - k)

(3.17)

The Eq. 3.16 can be expressed in the compact notation:

rPTl(i, 0) =

L
k=l

l'

OkrPll(i, k)

(3.18)

which describe a set of p equations in p unknowns. It is readily shown that the minimum mean-squared error,

En,

can be expressed as:
p

En =

L
'"

s;,(m) -

L
k= 1

Uk
p

L
111

SnCm)Sn(m - k)

(3.19)

rPn(O,O) -

L
k=1

O krPll(O,k)

(3.20)

Thus the minimum mean-squared error consists of a fixed term (rPn(O,O)) and terms that depend on the predictor coefficients. To solve Eq. 3.18 for the optimum predictor coefficients (

nd

the (/>r,(i, k) must be

computed for 1 ::; i ::; p and 0 ::; k ::; p, and then the resulting set of p simultaneous equations. Practically, the method of solving aforementioned equations is a strong function of the range of m used in defining both the section of signal for analysis and the region over
which the moan-squared error is computed. In this thesis tho autocorrelation mothod was

used and is reviewed in the next subsection.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

24

3.2.3

The Autocorrelation method

In this approach, it is assumed that the signal segment, 87/ (171), is idcntically zero outside
the interval 0 ::; m ::; N - 1. This is equivalent to assuming that the signal, s(m

+ 'II),

is multiplied by a finite length window, w( 111), which is identically zero outside the range

o ::; m ::; N

- 1. Thus the signal sample for minimization can be expressed as:

S7/(m) = 8(m. + n).w(n),
."in

0::; m :s; 1V - 1 otherwise

(3.21)

(rn) = 0,

According to this equation, for
'111

111

< 0, the error signal c,Jm) is zero since Sll(111) =

Â°

for all

< 0 and therefore there is no prediction error. Furthenllore, for m > N - 1 + ]J there is
871

again no prediction ('rIor because m = 0 (Le., fromm = () to m

(m) =

a for all m

> N - 1. However, in the region of

=]J -

1) the windowed signal sll(rn) is being predicted from

previous samples, some of which are arbitrarily zero. Hence the potential for relatively large prediction errors exists in this region. Furthermore, in the region of m = N - 1 (i.e., from m = N - 1 to m = N - 1 + p) the potential of large prediction errors again exists because the zero-valued (weighted) signal is being predicted from at least some nonzero previous samples. The purpose of the window of Eq. 3.21 is to taper the signal near m = () and near m = N - 1 so as to minimize the errors at section boundaries. Based on using the weighted signal of Eq. 3.21 the mean-squared error becomes:
N-l+p

En =

L

c;,(m)

(3.22)

m=O

and Â¢n(i, k) can be expressed as:

N-l+p

Â¢1l(i, k) =

L

sn{m - i)sn{m - k),

1::; i ::;

p,

()::; k ::;]J

(3.23)

m=O

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

25

or

1\'-1-(i-1.:)

cP,,(i,k)=

L

sn(rn)sn(m+i-k),

l~i~p,

O~k~p

(3.24)

111=0

Since Eq. 3.24 is only a function of i - k (ratl1('r than the two independent variables i and

k), the covariance fUllction, 1>n(i, k), reduces to the simple autocorrelation function, i.c.,
N-l-(i-k)

1>,,(i. k)

= T lI (i

- k) =

L

.'in(m)s,,(rn + i - k)
T lI
(

(3.25)

11l=()

Since the autocorrelation function is symmetric, i.e. be expressed as:
p

-k)

=

Tn(k),

the LPC equatiolls can

L7",,(1
1.:=1

i -

k I)ih-

= I",,(i),

1~ i ~

P

(3.2G)

this equation can be expressed in matrix form as:

7",,(0)

7"n(1) 7"n(2) Tn(P1)

7",,(1) 7"n (0)

7"n(1)

7"71(2) 7"n (1) 7"T/(0)

7"n(P - 1) Tn(P- 2) 1"n(P- 3)
T1/

01 02
03

rOn (1) rOn (2)
Tn

(3)

(3.27)

TrJp - 2) Tn(P- 3)

(0)

o'p

Tn(P)

The p x P matrix of autocorrelation values is a Toeplitz matrix (symmetric with all diagonal clements equal) and hence can be solved efficiently through several well-known procedures. Levinson-Durbin algorithm implemented in matlab toolbox have been used ill this thesis. The spectral estimation for a given speech sample is shown in Fig, 3.9 and Fig. 3.10 for LPC model of different orders.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

26
90~----~-------.------.-------,---~==~====~ estimated signal signal

80

~ :2 40

Â·c

2

Q)

50

:

30

20

10

OL-------~----

____

~

________L __ _ _ _ _ _
300 n

~

_ _ _ _ _ _ _ _~ _ _ _ _ _ _~

a

100

200

400

500

600

(a)

90~------,------,-------.-------.--~==~======~ estimated signal signal

30

I
)

20 10

I
,
I

OL-________L -_ _ _ _ _ _- J_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ __ L_ _ _ _ _ _ _ _ o 100 200 300 400 500
~

~~

______

~

600

n

(b)

Figure 3.9: Signal spectra is compared with its estimated spectra using LPC of order a) 6 b) 12

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

27
90r------,-------.------,-------,---~==c.=~==~

-

estimated signal signal

,

.

: ....

~

50

.3 'c

:::;; '" 40 30

'"

20

10

0

0

100

200

300
n

400

500

600

(a)
90

30 20 10
OL-------~

________

~

________L __ _ _ _ _ _
300
n

~

________

~

______

~

o

100

200

400

500

600

(b)

Figure 3.10: Spectral estimation of sample speech signal, with a LPC of order a) 24 b) 48

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

28

3.3

Cohen's Space-Frequency signal representation
ct(n, n)

The Cohen's group space-frequency [19] representation of signal s(n) is shown by and is parameterized by its SFR kernel Â¢. \Vhere n is discrete frequency.

I\fany parametric SFrr kernel shapes have been proposed in literature, and an efficient choice is the family of radially Gaussian kernels, defined in the ambiguity plane as:

(3.28)

\Vhere p and Â¢ are the polar coordinates given by:
p
if
=

=Vt,2 +

T2

(3.29) (3.30)

arct an -1 ( ~ )
T

and the contour function is:
Pmar

c(ip) =

00

+

2:= [01'cos(2p!,C) + b sin(2p'P)]
p

(3.31)

p=l

in which ao is chosen such that c( if) 2: a~ > 0 for all if. The SFrr kernel parameters are then
0,0', 0,1'

and b1" with P = 1, ... , Pmax. In the following, this set of parameters is denoted:

Given the training set of signals ON for a two-class classification problem, an optimization procedure is applied to obtain the best discriminative set of parameters procedure,

e.

In the optimization

(h and aT are considered to he subsets of approximately equal size obtained by

randomly partitioning training set ON in two parts, each containing elements from both classes. Thcre are T+1 samples in OT labelled

+ 1, and T-1

samples labelled -1 (It is assumed

that the two classes of data are labelled by -1, 1). Also, There is L+1 samples in OL labelled

+ 1,

and L-1 samples labelled -1. In order to optimally obtain

e, an optimization criterion

P( e I ON) is introduced which is minimized with respect to

e via a standard optimization

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

29
procedure. For a given training set ON and given kernel parameters (), P(() I ON) is calculated as follows: Step 1) Use the set 0 L to train the classifier. Stcp 2) Classify each element in OT. This results in two sets of values:

i=l, ... ,T+l
and
i = 1, ... , T-l

Step3) Compute the empirical mean and f~i{ where
,_

1l1+1,7n_1

and standard deviations+l,s_1 of f~i

1

Ttl
(i)

1
nl._
1

711+1 - - I : f + I ' T+I ;=1
111+1

= -

T

LlI. " ,.(!)

-I

LÂ·-l
1=1

(3.32) (3.33)

,2 _ 8+ 1 -

T

1

T+l

1
)2
,

T_I

.

" ( .(i) L j +1 +1 ;=1

52 -I -

-

T

"(1(1) _ TTl )2 L -1-1

- I 1=1

Step 4) Compute the criterion introduced in [20]: (3.34) where

Q(v) =

1,
v

+ac

v 27r

~exp( - - )du

1

'/1. 2

2

(3.35)

steps one to four are repeated R times with differen subsets OL and OT, and the final criterion P(()

I ON)

is obtained as the average of P(() lOT, OL) over all the subsets tested.
C311

Under Gaussian assumption over the distributions of f~? and f~t it

be shown [20]

P( () I ON) is an estimate of the classification probability of error for the classifier implemented

with parameter (). In practice, () is selected to minimize this estimated probability of error
P( ()

I ON)

using a standard numerical optimization procedure. When the optimal ()' =

argmino P(()

I ON)

is obtained, the classifier is trained over full learning set ON using the

optimal SFR kernel.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 4
A N Dvel SVM Kernel for Texture Classification

4.1

Motivation for texture classification

Researchers have been working for several years to implement image understanding algoritlnlls that can duplicate the excellent ability of human brain to recognize objects. In addition to shape of an object, it is believed that texture characteristics play an important role in human brain and visual system to recognize and interpret objects in perceived images. Consequently, texture analysis has been an active research field and large number of diverse algorithms have been proposed and tried. Texture classification currently is used as an essential part in a variety of image processing applications.

4.2

Applications

A wide range of different industrial applications have been successfully implemented based on texture analysis algorithms.
~:Iarti ct.

al. [21] used texture features in conjunction with

other modalities for object recognition in industrial environments. Kumar and Pang [22]

30

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

31 applied texture analysis for defect detection in textured materials. Li et. al. tough and tender beef by texture analysis. Texture characteristics have been also used extensively in medical imaging to detect abnormalities. Text.ure features and maximum likelihood classifier were used by Horng d.
al. [24] to classify ultrasonic liver images into three category of normal liver, liver hepatitis

[23] classified

and cirrhosis. A statistical model bast'd on autoregressive lwriodic random field model in

cOlljullctioll with conventional texture analysis parameters were llsed by Bleck ct. at. [25] to
det('ct microfocallesions in ultrasound
li\'(~r

images. \\Tang et. al. [26] used texture featur(\s

to ddeet infected tissues by ulcer and coli in endoscopic images. Texture features and analysis is also used for breast cancer detection. Gurcan ct. aI. [27] used higher order statistical texture features for detection of microcalcifications in mammograms. Tlwy used skewness and kurtosis to discriminate between normal and abnormal tissues. In allot her work, textural features was used by Kim and Park [28] to detect microcalcific:ation in digitized l\Iammography. A computer aided diagnosis (CAD) system for automatic abnormalities detection in chest radiographs was introduced by Ginneken et. aI. [29] using local texture analysis.

4.3

Texture Classification Challenges and Previous works

Texture classificatioll algorithms generally include two crucial steps: 1) feature extraction and 2) classification. In the following subsections the aforementioned and the previous work on each of them are addressed.

4.3.1

Feature Extraction

In feature extraction stage, a set of features are sought that can be efficiently computed and
embody as much discriminative information as possible about the textural characteristics.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

32 There are several methods for extracting textural feature which can be loosely classified as statistical, model-based and signal processing methods [3]. In statistical methods, textures are modelled using statistical measures. These methods are motivated by the fact that psychophysical experiments have demonstrated human visual system ability to extract some statistics of an order higher than two [30].The major drawback to this type of method is the enormous amount of data involved in higher order statistics. In l\lodel-based methods, textures are characterized based on probability distribution in random fields, such as l\Iarkov chains and l\larkov random fields (MRFs) [31], [32]. MRF based methods are ,,,Â·idely used but they are computationally expensive. Signal processing methods (also referred as multichannel filtering) are popular due to their simplicity of implementation. In these methods a textured input image is decolllposed into feature images using filter banks such as Gabor, wavelet or neural network based filters [33], [34], [35]. The main issue in these methods is selection of optimum set of filters for the problem in hanel.

4.3.2

Classification

In the second stage of a texture classification algorithm, a classification paradigm is constructed to distinguish between texture features corespondent to different texture classes. Several different classifiers have been reported. Minimum distance classifier based
011

Eu-

clidean or Mahalanohis distance was used in [36] and [37]. Manian et. al. used K-nearcst lleighbor in [38J. Fisher linear discriminant (FLD) was used in [39J by Clausi and Jernigan. Raghu and Yegnanarayana used artificial neural network (ANN) in [40J. Bayesian classifier was used in [41], [42J. Bayesian classifier is known as optimal classifier, but calculation of underlying probability distribution of the problem under study is not practically possible, specially in the absence of adequate number of training samples. In this thesis, the SVM has been chosen since it was shown that SVM outperforms other classification methods [6].

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

33 Superiority of SVl\I originates from its ability to generalize in high dimensional spaces, sHch as the space which is spanned by texture patterns. The generalization ability of SVM is based on its profound relation to the underlying statistical learning theory. In fact, SVM is an approximate implementation of Structural Risk l\1inimization method(SRM) since it imposes a theoretical bound on generalization error and sparseness of the solution. In SVM, instead of minimizing an objective function based on the training salllples ( sueh as mean square error), it is attempted to minimize a bound on generalization error ( i.e., the error made by the learning machine on test data not used during training). Therefore. an SVl\I tend to perform well when applied to data outside the training set. SVM achieves this advantage by focusing on the training examples that are most difficult to classify. These "borderline': training examples are called support vectors. Since SVM is a learning based classifier, it is mostly comparable with Neural Network. A brief comparison is given in Table 4.1. Table 4.1: Brief comparison between SVM and NN

SVM Minimize Structural Risk Less over fit Convex QP can always find a global optimum Challenge: choice of kernel functions Faster training

NN
l\1inimize Empirical Risk More over fit gradient descent. may stick at local optima Challenge: Structures of network Slower training

4.4

External features to build a new kernel for SVM

Unlike other texture classification methods, SVM based method dose not necessarily incorpOl'ate any external feature extraction method. Kim ct. ai. [3] showed the effectiveness of

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

34

SVl\l in texture classification problem. In fact, in an SVl\1, feature extraction is implicitly

performed by a kernel, which is defined as the dot product of two mapped patterns. There are some basic and popular kernels which are widely used ill different research including Kim et. al. 's work [3]. It is believed that the proper selection of SVM kernel significantly affect
t he overall performance of the algorithm.

In this thesis, we are seeking a tranSfOl"lllatioll within SVl\I kernel to transform input patterns to a llew space with emphasizing on difff'rf'llces hetween classes and deemphasizing on similarities. In practice, this is important when \\'e face small learning set and there i:-5 a need of model-free representation space. A variety of approaches has been llsed such as \vavelet packet based algorithms [43] (selection of the best discriminative basis) and methods based on time-frequency representations (TFR) [44], [45](selection of the best TFR within Coben's class [19]). Also, LPC has been widely used in speech recognition and coding

applications [46]. In this section, the effectiveness of the aforementioned features are studied and compared. The most effective feature is employed ill developing a new kernel for SVl\I. The main contribution of this thesis, in the sense of texture classification, is incorporating an optimized external feature into a new kernel function for the SVM algorithm. proposed kernel has the form of:
1
w2

The

'U)2

k(Xi,Xj) = e:rp- -2[2: 2: 1 NTx.(n,f2) - NTa;j(n,f2) 12] 20" n=1 n=1
_____ where
Xi

(4.1 )

and

Xj

are the vector forms of the subimages sampled by a windowing operator

of the size w x w. nand f2 are presenting spatial and frequency domaiu variables. The notation NTxJn, f2) emphasize the normalization of the transform T:

NT ( f2) _ 1 Tx(n, f2) 1 x n, - L~v2 Lf2 1 Tx(n, f2)

(4.2)
1

The transformation T in 4.1 can be any of the three feature extraction reviewed in

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

35
Chapter 3. To select the optimized feature extraction for texture classification the VCdimension is estimated [6] for each of the three feature extraction. The VC-dimensioll is principally indicates the largest number of points that can be separated in all possible ways using the functions of the given class [4]. The VC-dilllension of the kernel and its parameters can be estimated by the radius feature space minimize
T T

of the smallest ball containing all the data points in the

[47]. The proposed strategy is to select the kernel and parameters that

and eventually VC-dimension.

The VC-dimension is estimated for each possible two-texture classification among the four texture shown in Fig. 4.1. According to t his estimation LPC feature extraction with optimal feature I1l11uber is adopted to develop t he This algorithm is referred as optimized this thesis.
SV~l

with optimized kernel texture classificatioll.

SV~1

(OSVM) or LPC-SVM algorithm in the rest of

4.5

Classification and segmentation Results

To verify the effectiveness of the proposed method, experiments were performed on classification and segmentation of several test images. The test images were drawn from two different commonly used texture sources: the Brodatz album [48] and MIT vision texture (VisTex) database [49]. Table 4.2 summarize the source of the test images. All textures are gray-scale images with 256 levels. The classifiers were trained on randomly selected portions of subimages of texture images that are not included in the test subimages. Both the training Rnd test images
W(lfe

globally histogram equalized before being used. Gray scales were

linearly normalized into [-1, 1]. The results of classification are compared with some other texture classification methods including original SVl1 [3] as well as logic operators method [38], wavelct transform mcthod

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

36

c

Â·iii

o

c Q)

E 15

>

t)

number of features

o

*

LPC COHENTFR

+ LOB
120 140

o

20

40

60

80

100

Figure 4.1: Average VC-Dimcnsion for three kernel fUIlctions based on the three feature extraction reviewed in chapter 3

[37] and optimal Gabor filter method [39].

4.5.1

Comparison with Original SVM method

Images in Fig.4.2 are 256 x 256. Classifiers were trained by 1000 patterns from each texture. This corresponds to about 1.7 percent of the total available input patterns. The results are compared at different window sizes of 9 x 9,13 x 13,17 x 17,and 21 x 21. The original SVl\1 shows the optimal classification rate at window size 17 x 17. In the proposed LPC-SVl\1 the classification error rate decreases by increasing window size. Classification error rates are presented in Table 4.10 and 4.4. The proposed method outperforms the original SV:l\I specifically in larger window size. This is due to the ability of LPC kernel in inducing more

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

37
space-frequency characteristic into feature space at larger window size.

4.5.2

Comparison with Other Classification methods

In order to establish the superiority of the LPC-SV:t\i, its performance is compared with the most common techniques in texture classification. The spatial gray level dependence method (SGLDM) or co-occurrence matrix method. Fourier power spectrum method, tree- structured wavelet transform method, Laws texture features and Gabor method are used. The SGLDM estimate!"' the second-order joint conditional probability density functions, writtpn in matrix form and are called co-occurrence matrices. Haralick [50] proposed fourteen statistical

features that can be computed from these matrices. Although SGLD:t\1 has been proven to perform well for texture classification, the selection of the appropriate distance between pixels and angle for the co-occurrence matrix computation poses a problem and it is also computationally intensive. The Fourier spectrum method has not performed well even in earlier comparisons [51]. The statistical Fourier features of average magnitude, maximum magnitude, energy and features using the zonal masks used in [51] are computed. In the tree-structured wavelet transform (T\VT) method, the texture samples are decomposed into multiresolution hierarchy only at nodes where the energy of the decomposed subimages is not significantly smaller than the other subimages at that leveL The energy map of the channels is used as a feature vector for classification [52]. Energy features up to four level of decompositions are considered. This method has the drawback of becoming noisy at higher levels of decomposition. Four of the most powerful Laws mask are used to compute the texture energy measures [53]. Gaussian window function is used to compute Gabor transform and the Gabor coefficients are approximated using an optimization criteria [54]. Average energy and residual features are computed with this method. The feature selection process is applied with all methods in order to obtain the best feature set for each algorithm

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

38 . The Euclidean distance classifier used for all the aforementioned methods. Six differcnt experiment with six texture classes in each case are conducted. The results are giYen in Table 4.7. The proposed method is compared with two new method, Logical Operators [38] and wavelet co-occurrence features method [37]. Results are listed in Tables 4.8 and 4.9.

4.5.3

Segmentation Results and Comparison

One of the most important application of texture analysis in image processing is segmcntatiOll. The results of segmentation using proposed method are shown and compared with optimized Gabor filter method in Fig.4.4.

(a)

(b)

Figure 4.2: Two-texture images used in experiments (a) D4, D84 (b) D5, D92( Brodatz album)

Image Fig4.2a Fig4.2b Fig4.3a Fig4.3b Fig4.3c

Table 4.2: Source of test images Source D4 and D84 from [48] D5 and D92 from [48] D4,D9,D19,and D57 from [48] Fabric.0007, Fabric.0009, Leaves.0003, Ivlisc0002, and Sand.OOOO from [49] Fabric.OOOO, Fabric.0007, Flowers.0005, Food.0005, Grass.OOOl, 1vleta1.0002, Sand.OOOO, and Stone.0004 from [49]

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

30

(a)

(IÂ» .

(')

Figure 4.3: Illultitcxture images used in experimcnts

Table 4.3: Error R}ltes (pmwmt) for two-texture image (Fig.4.2a).

Parameter window size 9x9 13 x 13 17 x 17 21 x 21

Error Rate % Original SVM 12.7 9.4 8.6 13.0

LPC-SV}'l 9.6 7.6 4.1 1.2

Table 4.4: Error Rates (percent.) for two-texture illlage(Fig.4.2L).

ParClmetcr window size 9x 9 13 x 13 17 x 17 21 x 21

Error RClte % Original SVM 14.6 12.1 11.9 15.6

LPC-SVM 14.2 11.2 7.3 5.0

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

40

Table 4.5: Error Rates (percent) for lllllltitexture image(Fig4.3.a).

Parameter window size 9x9 13 x 13 17 x 17 21 x 21

Error Rate % Original SVM 22.3 17.3 16.1 21.8 LPC-SVM 15.2 11.9 8.7 7.1

Table 4.6: Error Rates (percent) for mllltitexture image(Fig4.3.b).

Paramet.er window size 9x9 13x 13 17 x 17 21 x 21

Error Rate % Original SVl\l 21.8 20.0 18.5 19.7 LPC-SVM 14.5 10.3 7.2 4.3

Table 4.7: Comparison of LPC-SVM with SGLDI\1,FPS,TWT,LA\VS, GABOR method

Texture Image r Mosaic of 6 textures D94,D101,D36,D84,D103,D56 D28,D20,D9,D38,D50,D57 D90,D74,D93,D34,D65,D53 D105,D79,D82,D52,D19,D78 D28,D9,D5 7,D24,D4,D38 DI03,D105,D12,D78,D79,D82

% Correct Classification with different methods LPC-SVM SGLDM FPS TWT Laws Gabor 52 62 54 63 67 97 62 84 70 75 84 98 47 62 67 96 64 56 40 54 53 55 99 59 77 99 65 63 56 67 58 58 59 97 56 77

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

41

Table 4.8: Comparison of Correct Classification Rate in Logic Operator Method and LPC-SVl\1

Texture D15 D19 D52 DG5 D74 D79 D82 D84

Logic Operator Method 89 97 81 84 73 92 86 72

LPC-SVl\1 89 100 100 100 81 98 98 95

Table 4.9: Comparison of Correct Classification Rate in Wavelet Transform 1'.lethod and LPC-

SVl\1 Texture Bark.0006 Clouds.OOOl Fabric.00l7 Grass.OOOl Leaves.0012 Misc.0002 Sand.OOO2 \Vavelet transform 1'.letllOd 92.86 94.0 97.6 78.6 91.7 97.7 96.4 LPC-SVl\J 89.0 100 100 96.1 94.2 100 100

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

42

(a)

(b)

(c)

(d)

(e)

Figure 4.4: multitextllre images segmentation: (a) Five texture Brodatz image originally published by .Jain and Farroklmia(h) segmentation with LPC-SVlI before post processing (c) scgmcntation with LPC-SVM after post processing (d) segmentation using Optimized Gabor Filter (e)segmentation using Optimized Gabor Filter (sigmoidal activation)

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

43

4.6

Gabor Filter Banks
filtering is an effective in texture classification and is widely used. Filter

~Iuliti-channel

banks have the ability of decompose an image into relevant texture features suitable for the purpose of classification. Multi-channel filtering is motivated by its ability to mimic Human Visual System (HVS). Hubel and \iViesel [55] showed that simple cells in retina are sensitive to specific orientations with approximate bandwidth of 30Â°. In addition to sensitiYity, tIl(' HVS has spatial frequency sensitivity [56]. This consequently led to a HVS lllodpl consist of independent detector mechanism each preceded by a relatively narrow band Filter tuned to a different frequency. Experiments indicate that the frequency bandwidth of simple cells ill the visual cortex is about one octave [57]. In this way, Gabor filters are motivated to be used in the filter bank clue to thpir ability to be tuned into various oriental and spatial, spatial frequency characteristic. Spatially, a Gabor function is a Guassian modulated by a sine or cosine:

(4.3)
And in the frequency domain:

We introduce Gabor Filter bank in this section to note that the ability of LPC-SVM algorithm in dealing with highly non-stationary and non-periodic signals, can be improved by adding gab or filter bank features. It is believed that gabor filter banks have an excellent ability in extracting non-stationary features. The main challenge in deploying Gabor filter

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

44
bank is proper and efficient feature selection. In the proposed method, LPC is used and the end of each filter for feature extraction. Such a feature extraction is not efficient for most of the classifiers and learning algorithms. Considering the ability of SVrvl in high dimensional feature spaces, the resulting algorithm i::i an excellent pattern recognition algorithm.

Table 4.10: Error Rates (percent) for two-texture image (Fig.4.2a).

Parameter window size Original SVI\1

Error Rate % LPC-SVM

9x9 13 x 13

12.7 9.4

9.6 7.6

LPC-SVM with Gabor 5

3.9

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 5 Detection of Abnormalities In Mammograms

B

REAST cancer is a major problem of public health in the western world, where it is by far the most common cancer among womcn. Each clay, breast caucer calls for

approximately oue hundred death in North America. As shown in Tablc 5.1 the rate of mortality is also considerably high in other developed countries.

Table 5.1: Rate of Breast Cancer mortality in some developed countries.

Country Canada France Germany Japan Korea Singapore

l\Iortality ( per 100 000) 65.1 35.5 44.5 10.7 3.9 13.8

On out of eight women over 40 years of age, develops one type of breast cancer. There is no way to prevent or cure breast cancer and the etiologies of breast cancer are unclear and no single dominant cause has emerged. The ouly way to save the life of patient is early detection and removing cancerous tissues before spreading to the other parts of body. The risk of 45

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

46
mortality increases if the cancerous tumors are not detected early. Thus, early diagnosis and treatment has a critical role in increasing the chance of survival. X-ray mammography is the most effective, low cost and highly sensitive method capable of detection of breast cancer in asymptomatic woman at its early stage [58]. There are two types of mammography: screening and diagnostic. The objective of screening mammography is to detect cancer when it is still small in asymptomatic woman. The diagnostic mammography is an X-ray examination of the breast of a woman who has either breast cancer symptoms or an abnormality in her screening mammography. An increasing number of developed countries have started screening mammography. "Vomen over 40 years of age are recommended to obtain mammograms regularly for screening purpose. This results to a huge number of mammogram that needs to be examined and interpreted by expert radiologists.

5.1

Motivation for a CAD system
clus~

Among the various types of hreast abnormalities that are visible in the mammograms,

tered micro calcification and mass lesions are the most important ones. Masses and clustered microcalcifications often characterize early breast cancer that can be detected in mammograms before a woman or physician can palp them. Among these two early signs, the cluster microc:alcific:ation is more difficult to be detected. A microc:alcification is a small granule-like calcium deposit that has accumulated in the breast tissue, alld it appears as a small bright spot embedded within an inhomogeneow:; background of the mammogram. Clustered micro calcification is defined by radiologist as the presence of three or more visible rnic:rocalcific:ations within a square of 1 cm 2 . It is very difficult to interpret X-ray mammograms because of the aforementioned tiny size of microcalcification and the very small differences in the image densities of various breast tissues, specially in dense breast. It is estimated that radiologists fail to detect approximately

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

47

Figure 5.1: Sample of microcalcification in a mammogram. The microcalcification is shown in the right after zooming and enhancement.

25% of the cancers that are visible on retrospective review [2]. This significant failure rate is due to visual fatigue, inexperienced radiologist and noise and lack of contrast in mammograms. Screening mammography also results in a huge number of mammograms which must be examined by experienced radiologist. Manual reading, the current exalllination procedure, is labor intensive, time consuming and demands great concentration. To help radiologist to overcome this huge hurden more accurately and in a reasonable time, several CAD (Computer Aided Diagnosis) systems have been proposed and tried. Indeed, the goal of CAD systems is not to create a high-tech radiologist or automate the

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

48 detection procedure. CAD systems are intended to act as the first cut in the process and a second opinion for the radiologists. The term" second Opinion" means that the radiologists can use the results of a computer analysis of the mammogram in making a diagnosis.\iVe try to locate suspicious regions in the mammogram for more detailed examination by the radiologists.

5.2

Previous work

Several CAD systems have been designed for extracting abnormalities in breast X-ray images. A few number of the most popular and recently proposed methods are briefly reviewed in the following. The reviewed methods and algorithm are summarized and listed in Table 5.2.

5.2.1

Shape Analysis
[59] developed a set of shape factors to measure the roughness of contours of

Shen ct. al.

calcifications in mammograms and for use in their classification as malignant or benign. The analysis of mammograms is performed in three stages. First, a region growing technique is used to obtain the contours of calcifications. Then, three measures of shape features, including compactness, moments, and Fourier descriptors are computed for each region. Finally, their applicability for classification is studied by using the three shape measures to form feature vectors. Classification of 143 calcifications from 18 biopsy-proven cases as benign or malignant using the three measures with the nearest-neighbor method was reported 100% accurate.

5.2.2

Iterative feature extraction

Karssemeijer [60] implemented an adaptive noise equalization and statistics-based model for the detect of microcalcification. As a pre-processing step, a robust rescaling was applied

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

49

as a filter adaptive noise equalization, which estimated noise characteristic from the image at hand. This system also took into account the impact of the variability in the tissue across t.he image. I\IicrocaIcifications were detected in these rescaled images using an initial segmentation and an iterative process used to update the pixel labels. The labelling process used filtered versions of the mammogram, each of which depicted a local image feature thought to be important in distinguishing a microcalcification. This work used 25 training images and another 40 for testing, from the Nijmegen database. These images all contained one or more known clusters of microcalcifications labelled by an expert radiologist. The images were digitized at 12 bits of gray-scale and to a size of 2048 x 2048 pixels.

5.2.3

Thresholding and morphology
[61] proposed an algorithm including three basic steps. First, the digital

Nishikawa et. al.

mammogram is enhanced using a spatial filter. This step tries to enhance the signals from micro calcification and suppress the signals from background structures of the breast. Second, global and local gray-level thresholding and a morphological crosin is used to extract potential microcalcifications from the image. Third, the feature analysis are used to reduce the number of false positive(FP) detections. The algorithm has been tested on a database (10bits/pixel, 0.1 mm/pixel) of 78 mammograms with size of 800 x 1000 pixels. Half of the images

without microcalcification and half of them with at least one microcalcification for a total of 41 microcalcifications. Visual interpretation was used for determining the true positive rate and the average number of false positives per image. A true positive rate of %85 were obtained at an average false positive detection of one per image.

5.2.4

Multiscale matched filters

Strickland and Hahn [62J developed a wavelet transform algorithm which acted as a bank of multiscale matched filters for the clustered microcalcification. By studying the cross

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

50
section of clustered microcalcifications, Strickland and Hahn concluded that it is reasonable to model a microcalcificatioll using a circularly-symmetric Gaussian function. Thus, the matched filters were designed for detecting Gaussian objects in correlated IVlarkov noise. The I\IaTkov noise were adopted as the model of background and normal tissues. The method was tested on a database of 40 mammograms digitized at 12 bits/pixel and 100 11m/pixel. The data based consisted of 40 mammograms with size 2048 x 2048 pixels. Each mammograms contained at Olle or more clusters microcalcifications. A duster was considered detected if two or more microcaleifications w('re found within the truth circle. An False Positive (FP) was counted if two or more erroneous detections were made within an empty, dosed region of 0.5 in width. The results showed that the method could achieve a True Positive(TP) rate of about 55% with 0.7 FP detection per image. Alternatively, a TP rate of about 85% with a corresponding 3.2 FP per image.

5.2.5

Statical feature of surrounding region

Kim and Park [28] proposed a method based on statistical textural features. In their method, four features named horizontal-weighted sum, vertical-weighted sum, diagonal-weighted sum and grid-weighted sum are computed based on the surrounding region matrix. Surrounding region matrix presents softness or coarseness for each Region Of Interest (ROI). The extracted features are feed into a three-layer back-propagation neural network for classificatioll. 120 X-ray mammograms were selected from the patient files based on visual crit.eria and biopsy results. The mammograms were digitized with pixel size of 100 Jim and 12 bits per pixel. 172 ROI's were selected to evaluate the method. 72 ROI's contained microcalcification and 100 ROI's without microcalcification. 86 ROIs ".'ere used for training including 36 ROI's with micro calcifications. A fraction of 0.8 TP were reported at 0.4 FP.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

51

5.2.6

Laplacian scale-space

Netsch and Peitgen [63] developed a method based on the Laplacian scale-space representation of the mamlllogram. Their motivation for applying Laplacian filtering originates on the fact that microcalcifications have an small bright circular appearance or slightly elongated spots in the mammogram. Bright spots corresponds to local maxima in Laplacian filtered images if the size of the filter kernel is chosen appropriat<'ly. Netsch and Peitgen applied their method on database of 40 mammograms digitized at 12 bits/pixel and 100 11m/pixel. The database consisted of 40 mammograms with size 2048 x 2048 pixels. Each mammograms contained one or more clusters microcalcifications. They achieved 85% TP detection at 1 FP per image.

5.2.7

Spatial statistical features and wavelet features

Yu and Guan [64] used a mixture of wavelet features and gray level statistical features. A set of 15 features were selected among 31 features by measuring the discriminatory power of the features using general regression neural networks via sequential forward and sequential backward selection method. The method were applied to a database of 40 digitized mammograms of 21 patients. The database included 105 microcalcific:ation and were digitized with 12 bit per pixels. The true positive of 90% were reported at 0.5 false positive per image. It mllst be noted that test data were used in feature extraction step. This must be considered, when the results of this work is compared with other above mentioned methods.

5.2.8

Support Vector Machine
[65] applied SVM for detection of clustered lllicrocalcification in digital

EI-Naqa ct al.

mammograms. In this approach SVM was used without deploying any external feature

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

52 extraction. In preprocess phase, a linear-phase finite impulse response high-pass filter with 3-dB cutoff frequency
We

= 0.125 was applied to digital mammograms. The gray level pixels

were directly applied to SVM after preprocessing on the digital mammograms. Using the gray level pixels without prior pre-process stage causes problem in training of SVM due to the very large number of training sample presenting normal tissue in comparison with the few nUlllber of twilling samples presentillg miCTocalcifications. El-Naqa ct ol. [65] proposed an algorithlll referred as succes::;ive cnhallCelllCl1t-learning(SEL) to reduce the training samples without microcalcification. The algorithm were evaluated using a database provided by the Department of Radiology at The University of Chicago. The data set contained 76 clinical mammograms, all containing multiple microcalcifications. Thc mammograms were of dimension 1000 x 700 pixels, with a spatial resolution of O.lmrn/pixel and 10 bits gray scale. The database included 1120 microcalcifications detected by expert radiologists at the Department of Radiology of Ulliversity of Chicago. A correct rate of 94% were reported at one false positive per image. The authors compared their algorithm with several other methods and claimed its superiority.

5.2.9

Multi-resolution Based Segmentation
[66] proposed a multi-resolution approach combined wavelet analysis to

S. Sentelle et al.

provide a segmentation of potential calcifications. An Initial multi-resolution approach to fuzzy c-means(FCl\I) scgmentation was employed. Some tissue areas were chosen in each image and were broken into multiple windows. \Vithin each window, wavelet analysis was used to generate a contrast image, and a local FCM segmcntation generated an estimate of local intensity. A sirnple two-rule fuzzy system thereafter combined intensity and contrast information to derive fuzzy memberships of pixels in the high-contrast, bright pixel class. A double threshold is applied at the end to this fuzzy membership to detect and segment

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

53 calcification. The algorithm was applied to 25 images obtained from the Digital Database for Screening .l\,iamlllography(DD.l\IS) provided by the University of Southern Florida.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

54 Table 5.2: Summary of related research in calcificat.ion detect.ion Relevant techniques Image sources

Authors L. Shen

et al. Shape Analysis
Iteratively segmenting filtered mammograms after denoising the original image Detection of potential calcification by global and local gray-level thresholding and a morphological erosin and reducing false positive rate by feature analysis. Designing matched filters for detecting Gaussian objects in correlated !\larkov noise

MIAS Database

[59]
Karssemeijer

65 image from Nijmegan database 78 mammograms from department of radiology, U niyersity of Michigan
40 images from Nijmegan database

[60]
Nishikawa

[G1]

Strickland and Hahn

[G2]
Kim and Park

[28]
Netsch and Peitgen [63]

Extracting four texture features from surrounding n'gion matrix and using three-layer backpropagation neural network for classification. Obtaining local maxima (bright spots) in the Laplacian scale-space representation of the mammogram. Selecting the best of 15 features among 31 features by measuring the discriminatory power of the features using general regression neural networks and using sequential forward and sequential backward classification method. Applying support vector machine to high-pass filtered mamograms and reducing the samples presenting normal tissues by successive enhancementlearning method.

Department of radiology, Asau !\1cdical centre, Korea 40 image from Nijmegan database

Yu and Guan

[64]

40 images of 21 patient from Nijmegan database.

EI-Naqil ct al.

[65]

Datahase of mammograms from the Department of Radiology, University of Michigan 25 images from Digital Database of Screening Mammography(DDSM) at the Uinversity of Southern Florida.

S. Sentelle et al. [66]

Using fuzzy c-mean segmentation on the multiresolution images followed by reconstructing the filtered image by bio-orthogonal wavelet.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G5

5.3

Proposed Algorithm

Image Segmentationt: Tissue & non-tissue (Using fuzzy c-mean)

Image Enhancement: Selective Median Filter & Contrast Enhancement

Preprocess: Detecting suspicious sites by reconstructing filtered mammograms using regular wavelet with compact support (i.e. Daubechies)

Reduction of false positives(FPs) : Applying Improved SVM to reduce false positives

Figure 5.2: Overall flow of the developed algorithm for detection of micro calcification The raw data in digital mammogram are not appropriate for learning algorithms such as SVM. The very high resolution and noisy images result in huge computation and ina<.:curate classification. Noise suppression and reduction methods are targeting image enhancement while trying to preserve microcalcification. On the other hand, while the image enhancement techniques try to ease detection of tiny calcification embodied in dense tissues, they are

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

56
potential of enhancing noise as well. To reduce the number of samples prior to classification, a highly sensitive algorithms can be used to detect suspicious sites in the mammogram. Our proposed methods includes three steps: ( as shown iu Fig. 5.2) a) Segmentation of image to tissue and non-tissue regions. b) Image enhancement c) Detection of suspicious sites d) Applyiug the improved SV"t\1 to sample of windows centerC'd at the suspicious sites.

In the following subsections the aforementioned steps are discllssed in more details.

5.3.1

Image segmentation

The proposed algorithm begins by separating the mammographic tissue from non-tissue regions in the image. The fuzzy c-mean (FCl\1) [67] is used in this thesis, which is basically an unsupervised least-squares clustering algorithm. The FCl\1 is similar to hard c-mean (HCM) approach but employs fuzzy membership when labelling data points rather than assigning crisp labels as in HCM. In FCl\1, a set of initial cluster centres are specified and an iterative process begins to adjust initial centres and calculate the fuzzy membership of all data points to the corresponding centres. At the end, maximal membership is employed for determination of crisp labelling from the fuzzy membership. FCM is initiated by assiguillg an initial class label to each data point determined from
the initialization routine. This assignment is stored in l1lcl1lbcrship l1latrix ( equation 5.1).

In this equation, k is an index into the image for data point

Xk,

and (i,j) are the class indices

for initial centres v. The number of classes is indicated by c. The m is the fuzzification value.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

57

'Ui,k'

=

II:ck' - Vi 112 "'-=-1 [2: (II . . 11 2 ) lVi, k j=l Xk - Vj
c

"

-1

(5.1 )

This U matrix is then used to calculate cluster centres of the data points for each class label, which are stored inlllatrix V(Eq. 5.2). In this equation, i is the cluster centre l1lunber, n is the number of pixels,
J;k

is the pixel value, and m is the fuzzification index.

",71

111 ..Â·

U'

1

= ' ( " ,71.

L.k=l Ilik.l'k)
In

\..I'

vi

(5.2)

L.k=l U ik

From the new duster ce11tres, the U membership matrix is recalculated. Once the U matrix has been recalculated, the duster centres are then recalculated. This process continnes until the measnre of the distance between old cluster centres and each set of Hew cluster centres meets a threshold criterion. At termination, the cluster centres for newly discovered data clusters as well as labels for the data points can be retrieved. however, this algorithm is computationally expensive due to its memory requirements, iterative nature, and slow convergence for large data sets sHch as high resolution lllamruograms. In this thesis, to reduce the data points processed by the algorithm, a multi-resolution approach is employed. The image is first down sampled by a factor of 16 and the algorithm is applied to the down sampled image. This decrease the convergence time of the procedure significantly, while the accuracy of the segmentation is still acceptable for the purpose of segmenting image to tissue and non-tissue. Local and global thresholding is finally applied to the image.

5.3.2

Enhancement of the mammograms

The fundamental enhancement needed in mammography is an increase in contrast, espedally for dense breast. Contrast between malignant tissues and normal dense tissue may be present on a mammogram but below the threshold of human perception. Similarly, micro-

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

58 calcifications ill a ::-;ufficiently dense mass may not. be readily visible because of low contrast. As a result, defining the characteristics of microcalcifications is difficult. Conventional image processing t.echniques do not perform well on mammographic images [68]. In this thesis, selective median filtering is used to remove background noise while preserving the edge information of suspicious areas. This approach was proposed by Lai ct
al. [69]. A selective median filter is defined as follows:

Given a window IF (i, j), centered at image coordinates (i, j), t.he output of the selective median filter is:

:ri,j = median{xl',s E

NO, j), and 11'1'.8 -

,ri.j

1<

T}

(5.3)

where

.Ti,j

is the image intensity at (i,j), N(i,j) is the area in the image covered by window

W (i, j) ,and T is the threshold. In computing the median, the set of pixels are restricted to
those with a differC'llee in gray level no greater than the threshold T, The amount of edge smearing can be controlled by adjusting the parameter T. If T is small, the edge preserving power of the filter is strong, but its smoothing effect is smalL If T is large, the filter behaves the other way around. In the llext step of enhancC'ment the image's contrast is improved by contrast-to-noise ratio method [70]:

xCi,j) = J:(i,j) - mean(y(r, s), T', s E Window) std(y(l, m), l, m. E Window)
where

(5.4)

xU, j) is the pixel value at the position (i, j), and "'Window'~

is an 9 x 9 square area

cent.ered at position (i, j), sid is the standard deviatioll of the pixel values in the "\Vindow" ,

xCi, j)

is the normalized gray level value at position (i, j),

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

59

5.3.3

Preprocessing

The preprocessing detection of suspicious area is based on the hypothesis that the microcalcification present in mammograms can be preserved under a transform which can localize the signal characteristics in the original and transform domain. In a time signal the harmonic frequency components are present but they are hidden, \vhereas in its frequency spectrum the time information is hidden. Therefore, transforms with basis functions other than the complex sinllsoids must be used. In addition, these basis functions must be able to localize the signal in both spatial and frequency domains. A suitable transform that satisfies t.he above requirements is the wavelet transform. The wavelet transform uses basis functions that can dilate in scale and translate in position according to the signal characteristics. Given that the microcalcifications correspond to high frequency components of the image spectrum and wavelets can localize signal characteristics in both frequency and scale, our hypothesis is that the resolution and scale of the rnicrocalcificatiolls in the spatial domaill can be preserved if we use wavelet filters to decompose the mammogram into different frequency subbands. Accordingly, microcalcifications can be extracted from mamlllograms by suppressing the subband of the wavelet-decomposed image that carries the lowest frequencies and contains smooth (background) information, before the reconstruction of the image. The proposed system is described in the block diagram shown in Fig. 5.3. The origillal mammogram is decomposed into a set of orthogonal subbands of different resolution and frequency content. The decomposition is based on wavelet analysis filtering and downsampling along the rows and colullllls of the illlage. Fig. 5.4 shows the seven subbands of resuItillg after two levels of wavelet decomposition of the image. The four subbauds at resolution 1 are produced by the decomposition scheme described in Section 3.1.1. The application of the same decomposition scheme to the upper-left subband that carries the lowest frequencies at resolution 1 results in the two level subband decomposition shown in Fig. 5.4. In the wavelet decomposed image shown in Fig. 5.4 the upper-left

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

GO
Image enhanced mammogram

Multi-resolution Decomposition (Wavelet analysis filtering)

Low frequency subband Elimination

Image Reconstruction (Wavelet synthesis filtering)

Suspicious area detected mammogram (Non-linear thresholding)

Figure 5.3: Preprocessing steps

subband at resolution level 2 contains the background intensity of the original image and, thus, carries the lowest frequencies of the image spectrum. The microcalcifications, which corresponds to the highest frequencies, are carried by other subbands. The detection of microcalcifications is accomplished by setting the wavelet coefficients of the upper-left subband to zero in order to suppress the image background information before the reconstruction of the image. The reconstructed mammogram is expected to contain only high-frequency components, including the microcalcifications. The reconstruction consists of wavelet synthesis filtering and up-sampling along the rows and columns of the image. The wavelet filters used in analysis and synthesis stages, are maximally flat wavelet filter constructed by Daubechies [71]. These wavelets are compactly supported and regular.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

61

Resolution level 2
N
Q)

Resolution level 1

> Q)

'p

o

c

:::l

o (.fJ

er::
Q)

Q)

> Q)
c

'p :::l

o o

en
Q)

er::
Figure 5.4: Orthogonal subLands at different resolutions \Vavelets are compactly supported if they have finite support with maxilllulll number of vanishing moments for their support width. The compact. support improves the time resolution of wavelets. Table 5.3 shows the filter coefficients of the two wavelets from Daubechies 4-coefficicnt (DAUB 4) filter and Daubechies 20-coefficient (DAUB 20) filter. In t.his table,
g(n) is the low-pass filter and h(n) is the high-pass filter as shown in Fig.3.4. The high-pass

filt.er can be obtained as h(n) = (-1)1!g(n)(-n

+ 2N -

1) where the N is the length of the

filter. Fig 5.5 shows the amplitude plot of the mother wavelet '1/) for the family of DAUB 4 filters. Fig.5.5(a) represent the long window used to analyze long term behavior of a sigllal, whereas Fig. 5.5 (b) is the scaled and translated version of the same wavelet used to analyze the instantaneous behavior of a signal. Fig. 5.6 shows the amplitude plot of the mother wavelet 'ljJ for the family of DAUB 20 filters. In each case, note that the stretched wavelets have higher amplitudes while the dilated wavelets have lower amplit.udes. The results of the preprocess st.age is shown in Fig. 5.6. All microcalcifications present in the original mammogram are visible in the images produced by the proposed preprocess. The performance of

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G2
the preprocess depends on the length of the wavelet filters used in the decomposition of the mammograms. This is a direct sequence of the different f-ihapes of the corresponding mother wavelets. According to Fig. 5.5 and 5.6, the mother wavelet of the DAUB4 filter is more spike-like compared with that of f-il1l00ther DAUB20 filter. It is clear from these images that the DAUB4 filter detects more pixels of high spatial frequency compared with the DAUB20 filter. These pixels may belong to microcalcifications, breast boundary, or background noise. Thus, short('r wavelet filters are more sensitive to existing micro calcifications but they tend to produce more false positives. Therefor, the DAUB4 is used in this thesis to have all pot('lltialmicrocalcifications in the next step( supervised learning).
Table 5 -3'. of the DAUB filters. - Coefficients

n 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

DAUB4 g(n) 0.482691 0.836516 0.224143 -0.129409

DAUB20 g(n) 0.026670 0.188177 0.527207 0.688459 0.281172 -0.249816 -0.195946 0.127369 0.093057 -0.071394 -0.029457 0.033212 0.003606 -0.010733 0.001395 0.001992 -0.000686 -0.000116 0.000093 -0.00013

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

63

0.15 r - - - , - - - - , - - - - , - - - - , - - - - r - - - . , - - - - - , - - - - - ,

0.1

005

o

-0.05

-0.1 '----_ _-'--_ _--'-_ _--'-_ _--'_ _ _'--_ _-'--_ _-'-_ _......J o 50 100 300 350 400 150 200 250

(a)

0.3 r---.,------,-----,---..,..-,----,------,------,----, 0.25

0.2 0.15 0.1
0.05

or----------.\
-0.05

-0.1
-0.15

-0.20L-----5-'-0---1--'O-0---15LO---2~0-0---25LO---3...L00----"35O'-----'400

(b)

Figure 5.5: Mother wavelet for DAUB 4, a narrow and tall wavelet for analyzing high-frequency characteristics

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

64

0.08 0.06 0.04 0.02

o
-0.02
-0.04

-0.06 -o.oa

-0.' 0L......--,.L00'---2.J...00--3..L00--4..L00--5-'00--6-'0-0--70'-0--aoLO--90Lo----..J, 000

(a)

0.'5 r - - - - , - - - - , . - - - , - - - , - - - - , - - . , . . - - - - - , , - - - , . . . - - - , - - - ,

0.'

0.05

o~------------/

-0.05

-0.'

-0.'5

-0.2 '--_--"-_ _-'-_--'-_ _--'--_--'_ _....L.._ _'--_---'--_ _.l.-_ _ o '00 200 300 400 500 600 700 800 900 1000

(b)

Figure 5.6: Mother wavelet for DAUB 20,(a) a wide and short wavelet for analyzing low-frequency characteristics(b) a narrow and tall wavelet for analyzing high-frequency characteristics

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

65

5.3.4

Reduction of False positives through supervised learning

The pre-processed images contain false positives which arc presenting noise and other highfrequency components rather than microcalcific<'ltion. In this step, SV11 in conjunction with

LPC is used to implement a supervised learning machine to reduce the number of false
positives. The detailed procedure is discussed in the following.

5.3.4.1 Input features
For every pixel in the pre-processed image. we define the input pattern to LPC-SVI\l to be all AI x .AJ window centered at the pixel detected in preprocess. These windows are selected from ellhallced mammogram. The 111 x 111 matrix is converted to a vector X of length 111'2 and then its length is reduced to N < .AJ'2 by using LPC feature extraction method. During the trainillg phase, each illput feature X is labelled with (y pres(,llt, or (y = -1) for mic:rocalcification absellt.

=

+1) for microcalcification

5.3.4.2 Model selection and SVM training
_Once the training samples are gathered, the llext step is to determine the SV11 decision function. In this process, the following parameters must be determined:

a) \Vindow size, 111

b) LPC features, N

c:) The type of kernel functioll

d) The regularization parallleter in the structural risk function, C

To optimize these parameters, rn-fold cross validation [72] is applied to training data set. This procedure consists of the following steps.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

GG First, divide randomly all the available training examples into m equal-sized subsets. Second, for each model-parameter setting, train the SVM classifier m times; during each time of the m subset.s is held out in turn while all the rest of the subsets are used to train the SVM. The trained SVl'vi classifier is then tested using the held-out subset, and its classification error is recorded. Third, the classification errors are averaged to obtain an estimate of the generalization error of the be adopted.
SV~l

classifier. In the end, the model with the smallest generalization error will

In the next section, the performance of the overall algorithm is evaluated using Receiver
Operating Characteristic (ROC) analysis.

5.4
5.4.1

Experimental Results
Database

The research results in this thesis is based on the data sets derived from Digital Database
for Screening Mammogmphy (DDSM), located at the University of South Florida [73]. The

DDSM contains approximately 2500 cases of fully annotated mammographic images. Each case contains two views of each breast along with patient information such as age at the time of study, American College of Radiology (ACR) breast density rating, subtlety rating for abnormalities, and ACR description of each abnormality. Information about how the mammograms were digitized is also included, such as relative spatial resolution and scanner used. The DDSJ'vl is organized into" cases" and " volumes" . A" case" is a collection of images and information corresponding to one mammography exam of one patient. A case consists of between G and 10 files. These are an "ies" file, an overview" 16-bit
PGM" file, four image files that are compressed with loss less JPEG encoding and zero to

four overlay files. Normal cases will not have any overlay files.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G7
Structure of ".ics" files:
The ".ics" file provides information about a case as a whole. In ASCII format, it lists
ics_version 1.Q filename 8-3024-1 DATE_OF_STUDY 2 71995 PATIENT_AGE 42 FILM FILM_TYPE REGULAR DENSITY 4 DATE_DIGITIZED 7 221997 DIGITIZER LUMISYS SELECTED LEFT CC LINES 4696 PIXELS PER LINE 3024 BITS PER PIXEL 12 RESOLUTION 50 OVERLAY LEFT-MLO LINES 4688 PIXELS PER LINE 3048 BITS PER PIXEL 12 RESOLUTION 50 OVERLAY RIGHT CC LINES 4624 PIXELS-PER -LINE 3056 81TS -PER -PIXEL 12 RESOLUTION 50 OVERLAY RIGHT=MLO LINES 4664 PIXELS_PER_LlNE 3120 BITS_PER_PIXEL 12 RESOLUTION 50 OVERLAY

Figure 5.7: Sample ics file
important information such as the date of the study, the patients age, the ACR breast tissue density, the date of digitization of the films, the type of digitizer used and a list of the image files. The" .ics" file also gives an rating of 1 to 4 as assessed by an expert radiologist. The size of each image file, number of bits per pixel, the scanning resolution (in microns) and information on the existence or lack of an overlay file for each image is provided. As it can be seen in Fig. 5.7, all four images have overlays. If the image description lines had "NON-OVERLAY" instead of "OVERLAY" then the images would not have overlay files.

Structure of Overlay files
Abnormal cases have between one and four overlay files depending on the number of images in which the radiologist marked any abnormalities. Each overlay file may specify n11l1tiple abnormalities, so the first line of the file gives the total number of abnormalities. In the case of multiple abnormalities, each abnormality is then listed one after another. Each

abnormality has information on the lesion type, the assessment, the subtlety, the pathology and at least one outline. The keywords that describe the lesion type are taken from the ACR Bi-RADS lexicon. The assessment code is a value from 1 to 5, and also comes from the ACR Bi-RADS standard. The outlines for the suspicious regions are derived from markings

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G8
made
011

the film by an experienced radiologist. Each houndary is specified as a chain code.

This chain code is found on the line after the key\\Tord "BOUNDARY" or "CORE". The first two values of each chain code are t he starting column and row of the chain code that order. Follmving these two numbers, the chain code is given and a "#" character indicates the end of the chain code. The numbers correspond to the directions as shown in Fig. 5.8.

IChain code value

IX Coordinate
IY coordinate
1

I"

r

Figure 5.8: Chain codes values and directions

TOTAL_ABNORMALITIES 1 ABNORMALITY 1 LESION_TYPE CALCIFICATION TYPE PLEOMORPHIC-FINE_LlNEAR_BRANCHING DISTRIBUTION REGIONAL ASSESSMENT 5 SUBTLETY 4 PATHOLOGY MALIGNANT TOTAL_OUTLINES 4 BOUNDARY 813684444444422222222 ... 0000000001 # CORE 1681824222222222222222 ... 1 011 011 011 # CORE 3841848222222221111111 ... 0000000000# CORE 3682192666666660000000 ... 0000000000 #

Figure 5.9: Sample Overlay File

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

G9

5.4.2

Selected Data Set

III this thesis 60 images are used. The Images are submitted to database from mammography clinic, the Ann L. Baraco Centre for \Vornen's Health, located at Sacred Heart Hospital in PCllsacola, Florida. The images are divided to two sets of 40 and 20 images for training and testing. The 20 images used for test are listed in Table 5.4. As described in Table 5.4, the 20 imagC's with malignant microcalcifications were divided into four sets according to density and subtlety ratings: (1) high-density breast tissue with obvious abnormalities, (2) high density breast tissue with subtle abnormalities, (3) lowdensity breast tissue with obvious abnormalities, (4) low-density breast tissue with subtle abnormalities. In Table 5.4, column 1 indicates the case category. Column 2 indicates the case number and view. The breast density rating is shown in column 3. This rating is according to Breast Imaging Reporting and Data systerns (BI-RADS) density ratings. A density rating in scale 1 to 4 is used as follows [74]: 1= The brea<;t is almost entirely fat. 2

=

There are scattered fibroglandular densities that could obscure a lesion on a mammogram. 3 = The breast is hetrogelleously dense. This lllay lower the sensitivity of mammogram. 4= the breast is extremely dense which lowers the sensitivity of mammogram. According to this metric a higher density rating implies that it is more difficult for radiologist ( and computer) to detect the abnormality. Colull1Il 4 indicates the BI-RADS assessment rating ranging from 1 to 5. 1

= Negative. 2

= Benign finding. 3 = Probably

benign finding. 4 = Suspicious abnormality (biopsy should be considered). 5 = Highly suggestive of malignancy (appropriate action should be taken). In column 5, subtlety rating is presented. The value of this measure range from 15, where 1 stands for "subtle" and 5 for "obyious". A higher subtlety rating indicates

easier interpretation task for both radiologist and computer. Columns 5 and 6 are showing pathology and lesion type as well as number of abnormalities.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

70
Table 5.4: SUlllmary of ground truth information for the test images

High density/ subtle

Case/view e0060LCC

Density Assessment Subtlety Pathology Lesion type/description 3 2 r-.1alignaut 1 calcification/amorphous 4 distribution: clustered
4 4

c00060LMLO 3 c150LCC 3

2 1 1 2 2 2

c0150Lr-.lLO 3 c0198RCC 3

.Â±
4 4

c0198Rr-.1LO 3 Low drnsity / subtle cOOO2LCC 2

4

r-.lalignant 1 calcification/amorphous distribution: clustered Malignant 2 calcification/amorphous distribution: clustered ~laligll<lllt 2 calcification/amorphous distribution: clustered r-.lalignant 1 calcification/amorphous distribution: segmental Malignant 1 calcification/amorphous distribution: segmental r-.lalignant 1 calcification/pleomorphic distribution: segmental r-.Ialignant 1 calcification/pleomorphic distribution: segmental Malignant 1 calcification/pleomorphic distribution: regional r-.lalignant 1 calcification/amorphous distribution: clustered Malignant 1 calcification/amorphous distribution: clustered Malignant 1 Calcification/punctate distribution: clustered Malignant 1 Calcification/punctate distribution: clustered Malignant 1 Calcification/ amorphus-pleomorphic distribution: segmental Malignant 1 Calcification/ amorphus-pleomorphic distribution: segmental Malignant 1 Calcification/pleomorphic distribution: clustered Malignant 3 Calcifications/pleomorphic distribution: clustered Malignant 3 Calcifications/pleomorphic distribution: clustered r-.lalignallt 1 Calcification/pleomorphic distribution: segmental i\laligllallt 1 Calcification/pleolllorphic
rl;,-.f-rlhl'ltirY1,"")
0

cOO02Lr-.lLO 2 c0020LMLO 2 c0169RCC 2

4 4

1 2 2 2

4 4

c0169RMLO 2 High density / obvious c0036RCC 4

4

4

cOO36RMLO 4 c0120LCC 4

4 5

4 5

c0120LMLO 4

5

5

Low density / obvious

c0045RMLO 2

4

5

c0087LCC

2

5 5
4 4

5 5 4
4

c0087LMLO 3 c0214RCC

2

c0214RMLO 2

LVlrrn"lontnl

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

71

5.4.3

Results

The training data set were used to train the proposed algorithm. 40 images including 51 micro calcification ,vere used in training phase. The preprocess algorithm on test data resulted in 5236 suspicious sites. The improved SVM then were applied to the test data. To detect the microcalcifications, the nUluber of pixels which are classified as ,. ~1icrocalcificat.ion Preseut" , are counted in each 1cm 2 of the mamlllogram. A threshold is set to dcterminc whether a site contains a microcalcification or not.
If the number of pixels which are classified

as "microcalcification present" is greater than the threshold, the site is considered as a microcalcification. The ROC curves in Fig. 5.10 were generated by varying the threshold from 3 to 14. The proposed algorithm outperforms the previously applied SV11 to the problem of microcaIcificatioll [65]. The El-Naqa ct al. algorithm were applied to current data set to obtain the ROC curve. The area under the two curves are calculated as 7.22 for LPC-SVM and 5.68 for SVM. This indicates a 26% improvement. The numerical results which were obtained here for the SVM [65] is different from original report. The reason is that the data set is different here. It is obvious that the data set can significantly affect the caIculated performance. i.e. if some difficult subtle images were excluded from the study, the performance was totally different. \Vhile the overall detection performance is very good, a morc detailed analysis of the individual cases reveals additional insight int.o the performance. The ROC for the most recent published research on the same database is given in Fig.5.11. The are under curve is 4.86 which 46% less than proposes LPC-SVM method. The detail results of the proposed algorithm are given in Table 5.5. This table is presenting the case of 1.5 false positives at 92% of correct true positive detection. As described before, the test data has been scIected for 4 classes mammograms: 1) low-density tissues with obvious calcifications, 2) low-density tiflsue with obvious calcifications, 3) high density tissues with obvious calcifications, 4) high-density tissue with subtle calcifications. The re-

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

72
8P01188

of the algorithm to these classes, are different, as ran he seen ill Tahle 5.5. The

algorithm is quite successful in detection of microcalcification in low/high density-obvious and low density subtle and more research is required for high-density subtle. The application of the proposed algorithm on the high density images are resulted in relatively high 11umher of false positives.

. . . ."
'.

. .

.

0.95

. .-. .' . . . . . .:. . . . . . .

....

0.9

Â§ 0.85
~

tt
a..
o

ctl
Q)

~ 'iii
OJ ::>

0.8

.= 0.75
0.7

0.65

..
Proposed Algorithm SVM

0.6 L_--L_ _- 1 - _ - - L_ _~_

_..l._ _.....l__

o

2

345

6

_._l_===::r::====:Jr:::===::J 10 9 7 8

Average Number of False Positives

Figure 5.10: ROC curves show that the proposed algorithm outperforms SVl\1

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

73

0.9

0.8
o
c

U
en Q) >

~ 0.7

.u:; "" o
Q)

a.. 0.6
:::J

.=
0.5

0.4

0.3~----~------~----~------~----~------~------L------L----~

o

2

4

6 12 8 10 Average Number of False Positives

14

16

18

Figure 5.11: ROC curve for the most recent published research on DDS},!

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

74

Table 5.5: The number of false positives and correct true positives in each test image

Case High density subtle c0060LCC cOO060U,ILO c150LCC
c0150L~lLO

True Positive Rate 1 1 1 0.5 0.5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

False Positives 2

Subtlety 2 2 1 1 2 2 2 1 2 2 2 4 4 5 5

4
3 1 1 2 1 0 1 1 1 6 2 1 1 1
0

c0198RCC
c0198R~ILO

Low density subtle

cOO02LCC cOO02LMLO c0020LMLO c0169RCC c0169RMLO cOO36RCC cOO36RMLO c0120LCC c0120LMLO cOO45RMLO cOO87LCC cOO87LMLO c0214RCC c0214RMLO

High density obvious

Low density obvious

5 5
5 4 4

1 1 0

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Chapter 6 Conclusion and future work
6.1 SVM in texture classification and external texture features

The SVl\I learning algorithm has been proven to outperform other texture dassification algorithllls \\'hich are based on snpC'rvised learning i.e. neural network.

In this thesis, the effectiveness of SVl\1 in texture classification problem was investigated.
It was shown that SVl\1 is a powerful algorithm not only for didlOtolllY ( two-dass) problems

but also for multi-texture. The effect of external features in performance of SVl\1 were studied. The estimation of the VC-dimension of the external features were used to chose the best feature set among suggested features. The comparison of the VC-dimension resulted in LPC-SVM texture classification algorithm. LPC which provides a high dimensional feature space were chosen
considering the excellent ability of SVl\1 to set a learning hyperplane in a high dimension

input features. In the study of VC-dimcnsion, LPC provides the minimum VC-dimension and training error. The discrimination ability of LPC features is reduced in high resolution classification

75

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

76

when the window size of the sampled textures and evelltually the input feature dilllension is reduced.

6.2

Incorporating the Gabor filter Bank to Increase the performance of LPC-SVM texture classification algorithm

\Vhen we need to detect tiny feature in an image or to increase the resolution of a textme classification-based segmentation, we come across the problem of low dimension input features with poor texture information. In the low dimension input texture feature space the VC-dimension and training errors of LPC is reduced. This is due t.o t.he poor discriminant features in a slllall t.cxtUl"{' window. The Gabor filter bank is extensively and successfully used in several texture classification algorithms. To enrich the texture discrimination ability of input features in case of small window size, the Gabor filter bank were employed. III the proposed algorithm, for each sample windO\v ill the original image, corresponding sample windows from filtered version of the image are extracted and concatenated to form an input feature prior to LPC feature selection. In this way the classification accuracy was improved slightly but the time consumption of the algorithm increased considerably.

6.3

Microcalcification detection

SVM algorithm is an ideallearnillg machine for a supervised learning approach in a two-class classification (dichotomy) problem. The dichotomy problem is concerned in a wide range of

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

77 medical imaging applications. Detectioll of abllormalities in medical imaging is a dichotomy problem in the sense of classifying an illlage to normal and abnormal areas. The high rate of breast cancer development in WOlllen over 40 years of age, and the mortality rate was the main motivation for applying the proposed algorithm to lllicrocalcification detection in screening mammography. j\Iicrocalcificatioll is the most difficult type of breast cancer to be detected by radiologist. The major problelll in application of LPC-SV1I texture classificatioll algorithm to microcalcification detection is the asymmetry of the training data. The huge number of pixels which arc presenting normal tissue ill compare with a very few number of pixels presenting the microcalcificatiolls. A preprocess stage was added to overall algorithm to detect suspicious areas and reduce the number of the pixels presenting normal areas. The preprocess were applied prior to the proposed algorithm to increase the accuracy and time efficiency. The test data for evaluating the proposed algorithm were evenly divided into four category of mammograms. The images were divided categorized according to tissue density and the difficulty of detection of microcalcification. The results were evaluated through ROC curve and were compared with SVM algorithm. A correct rate of 92% were archived at 1.5 false positives per image. Considering the number of very difficult images in the data set used during the test, the overall performance is significant.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

78

6.4

Research contribution

The main contribution of this work is in the field of medical imaging. The SVrvI algorithm were empowered by external features for t('xture classification. A number of external features were suggested and examined and through the study of VC-dimension the best were chosen. Tlw perfonnallce of LPC features in high resolution classification, \vere improved byemploying gabor filter bank features. The excellent ability of SV}'1 in dichotomy problems were used in detection of ahnormalities. The asymmetry problem of training data were solved with pre-process st.age based on wavelet transform decomposition and reconstruction of the mammograms. The performance of the algorithm were discussed for the images clinically categorized illto different levels of difficulty for assessment.

6.5

FUture work

More research is needed to increase the accuracy of the algorithm in subtle cases of cancer and to reduce the number of false positives in high density images. Incorporating information from other modalities such as ultra sound and }'IRI imaging must be considered to achieve a CAD system with acceptable performance for clinical usc.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

79

Acronyms
CAD: LDB: LPC: Computer Aided Diagnosis Local Discriminant Basis Linear Predictive Coding Human Visual System Receiver Op<'rating Characteristic Support Vector l\lachine Vapnic-Chcrvoncnkis

HVS:

ROC:
SVM: VC:

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

Bibliography
[1] C. C. Society, "Caucer stRtistics," in http://unlJ'llJ.cancer.r:a, 2002. [2] N. Karsscmeijer, "Computer assisted reading mammograms," Joumal of Eauropean
Radiology, vol. 7, pp. 743-748, 1997.

[3] K. 1. Kim, K. .lung, S. H. Park, and H. .1. Kim, "Support vector machine for texture
classification," IEEE Transaction on Pattcr'n Analysis and Alachine Intelligence, vol. 24, pp. 1542-1550, 2002. [4] V. Vapnik, The Natm'c of Statistical Learning Theory. New York: Springer-Verlag,

1995.
[5] C. Burges, "A tutorial on support \'ector machines for pattern recognition," Data Mining
and Knowledge Discovery, vol. 2, pp. 1-47, 1998.

[6] B. Scholkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Pogio, and V. Vapnik, "Comparing support vector machines with gaussian kernels to radial basis function classifiers," IEEE Tmnsaction on Signal Processing, yol. 45, pp. 2785-2765, 1997. [7] T. Cover, "Geometrical and statistical properties of systems of linear inequalities with application in pattern recognition," IEEE Transaction on Electronic Computers, vol. 14, pp. 326-334, 1965.

80

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

81 [8] N. Saito and R R Coifman, "Local discriminant bases and their applications," J.

A1athematical Imaging and Vision, vol. 4, pp. 337-358, 1995.
[9] RRcoifman and 1\1. \Vickerhauser, "Entropy-based algorithm for best basis selection,"

IEEE Trans. Inform. Theory, vol. 38, pp. 713-719, 1992.
[10] S. G. 1\la113t, A Wavelet Tour of Signal Pmcc8sing. San Diego: Acad('lllic Press, 1998.

[11] S. 1\1311at, "A theory for multiresollltion i'iignal decomp0i'iition: the wavelet representation," IEEE Pattern Anal. and A1ach:inc Intcll, vol. 11, pp. G74-693, 1989. [12] N. S..Jayant and P. Noll, Digital Coding of Waveforms. [13] A. Gresho, "Advances in speech and audio compression," in Pmc. IEEE, ser. pp. 900918, 1994. [14] J. D. Gibson, "Adaptive prediction for speech differential encoding system," in Pmc.

IEEE, ser. pp. 1789-1797, vol. G8, 1974.
[15] P. Kroon and E. F. Deprettere, "A class of analysis-by-synthesis predictive coders for high quality speech coding at rates between 4.8 and IG kbits/s," IEEE J. Select. Areas

Commun, vol. G, pp. 353-363, 1988.
[lG] B. S. Atal, "Predictive coding of speech at low bit rates," IEEE Trans. Commun., vol. COMM-30, pp. 600-614, 1982. [17] l\i. R Schroeder and B. S. Atal, "Code-excited linear prediction (celp): High quality speech at very low bit rates," in in Pmc. IEEE Int. ConI Acoustics, Speech, and Signal

Processing, ser. pp. 937-940, 1985.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

82 [18] .1. H. Hu, Y. W., and P. T. Cahill, ":Multispectral code excited linear prediction coding

and its application in magnctic rcsonance images," IEEE Trans. on Image Processing, vol. 6, pp. 1555-1566, 1997.
[19] F. Hlawatsch and G. Boudreaux-Bartels, "Linear and quadratic time-frequency signal

representations," IEEE Signal Processing Magazine, vol. April, pp. 21-07, 2001.
[20] 1\1.
Dav~r.

C. Doncarli, and G. F. Boudreaux-Bartels, "Improved optimization of tilllP-

frequency based signal classifier," IEEE Signal Pmccssing Letters, vol. 8, pp. 52-57,
2001. [21] .1. l\larti, .1. BatHe, and A. Casals, "Model-based objects recognition in industrial en-

vironments for autonomous vehicles control," in International Conference on Robotics and Automation, ser. Proc. IEEE, 1997.
[22] A. Kumar and G. Pang, "Defect detection in textured materials using gab or filters," in

Industry Applications Conference, ser. Proc. IEEE, 2000.
[23] .1. Li, .1. Tan, and P. Shatadal, "Classification of tough and tender beef by image texture

analysis," Meat Science, vol. 45, pp. 341-346, 2001.
[24] 1\1. H. Horng, Y. N. SUll, and X. Z. Lin, "Texture feature coding method for classification

of liver sonography," Computerized Medical Imaging and Graphics, vol. 26, pp. 33-42,
2002. [25] J. S. Bleck, U. Ranft, M. Gebel, H. Hecker, M. Westoff-Bleck, C. Thiesemann, S. \Vag-

ner, and M. Manns, "Random field models in the textural analysis of ultrasonic images of the liver," IEEE Transaction on Medical Imaging, vol. 15, pp. 796-801, 1996.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

83 [26] .1. Wang, S. Iv1. Krishnan, C. Kugean, and :t\1. P. Tjoa, "Classification of endoscopic

images based on texture and ncuraillctwork," in IEEE 23rd EBMS International ConfeTcncc, ser. Proc. IEEE, 2001.
[27] 1\1. N. Gurcan, Y. Yardmici, A. E. Cetin, and R. Ansari, "Detection of microcalcific:ation

in mammograms using higher order statistics," IEEE Signal Pmcessing Lettcrs, vol. 4, pp. 213-216, 1997.
[28] J. K. Kim and H. \V. Park, "Statistical texture features for detection of rnicrocalcifica-

tiOlls in digitized mammograms," IEEE Tmnsaciion on Medical Imaging, vol. 18, pp.
231-238, 1999. [29] B. V. Ginnekcn, S. Katsuragawa, B. 1\1. T. H. Romeny, K. Doi,and M. A. Viergcvcr,

"Automatic detectioll of abllormalitics in chest radiographs using local texture analysis," IEEE Tmnsaction on Medical Imaging, vol. 21, pp. 139-149, 2002.
[30] P. Diaconis and D. Freedman, "On the statistics of vision: The julesuz conjecture,"

Journal of Math. and Psycology, vol. 24, 1981.
[31] S. Z. Li, Markov Random Field Mdeling in Computer Vision.

New York: Springer-

Verlag, 1995.
[32] S. C. Zhu, Y. \\Tu,

and D. Mumford,

"Filters, random field and maXlll1Ulll

entropy(frame)- towards a unified theory for texture modeling," Intc77wtional Joumal of Compter' Vision, vol. 27, 1998.
[33] A. K. Jain and F. Farroklmia, "Unsuprvised texture segmentation using gabor filters,"

Pattern Recognition, vol. 24, pp. 1167-1186, 1991.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

84
[34] C. Lu, P. Chung, and C. Chen, "Unsupervised texture segmentation via wavelet trans-

form," Pattern Recognition, vol. 30, pp. 729-742, 1997.
[35] A. K. Jain and K. Karu, "Learning texture discrimination masks," IEEE Tmnsadion

Pattem Analysis and Machine Intelligence, vol. 18, pp. 195-205, 1996.
[36] J. "V. Wang, C. Chen, \V. M. Chien, and C. 1\1. Tsai, "Texture classification using nOll-

seprable two-dimensional wavelets," Pattern Recognition LetterÂ·s, vol. 19, pp. 1225-1234,
1998. [37] S. Arivazhagan and L. Ganesan, "Texture classification using wan'let transform," Pat-

tern Recognition Letters, vol. 24, pp. 1513-1521, 2003.
[38] V. 1\lanian, R. Vasquez, and P. Katiyar, "Texture classification using logical operators,"

IEEE Transaction on Imagc Process'ing, vol. 9, pp. 1G93-1703, 2000.
[39] D. A. Clausi and 11. E. Jerningan, "Designing gabor filters for optimal texture sepra-

bility," Pattern Recognition Letters, vol. 33, pp. 1835-1849, 2000.
[40] P. P. Raghu and B. Yegllanarayalla, "Supervised texture classification using a proba-

bilistic neural network and constraint satisfication model," IEEE Transaction on Neural
Netw07'ks, vol. 9, pp. 516-522, 1998.
[41] L. M. Kaplan, "Extended fractal analysis for texture classification and segmentation,"

IEEE Transaction on Image Pmcessing.

[42] N. T. M. N. Shirazi, H. Noda, "Texture classification based on markov modeling in wvelet feature space," Image and Vision Computing, vol. 18, pp. 9G7-973, 2003.
[43] N. Saito and R. Coifman, "Local discriminant bases and their applications," Journal of

Mathematical Imaging and Vision, vol. 5, pp. 337-358, 1996.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

85 [44] C.Heitz, "Optimum time-frequency representaions for the classification and detection of signals," J. of Applied Signal Processing, vol. 3, pp. 124-143, 1!)95. [45] B. Gillespie imd L. Atlas, "Optimizing tf kernels for classification," IEEE T'l'onsaciion

on Signal Processing, vol. 48, pp. 485-4!)6, 2001.
[46] L. Rabiner and B. H. Juang, Fundamentals of speech recognition. Englewood Cliffs,N.J: Prilltce Hall, 1993. [47] C. B. B. Scholkpof alld V. Vapnik, "Extracting support data for a given task" in Pmc.

Int'l ConI Knowledge Discover'y Data Mining, ser. pp. 252-257, 19%.
[48] P. Brodatz, Te:dures: A Photogmphic Albmn for' ArÂ·tists and Designers, Newyork, l!)GG. [49] 1\1. Vision and M. Group, 1998. [50] R. 1\'1. Haralick, K. Shunmugam, and 1. Dinstein, "Textureal features for image classification," IEEE Tmnsaciion on Syst. Man. Cybern., vol. SMC-18, pp. 610--621, 1973. [51] R. \V. Conners, "A theoritical comparison of texture algorithms," IEEE Tmnsaction

on Pattern Anal. Machine Intel., vol. PA1\1I-2, pp. 204-222, 1980.
[52] T. Chang and C. J. Kuo, ('Texture analysis and classification \\'ith tree-structured wavelet transform," IEEE Transaction on Image Processing, vol. 2, pp. 429-441, 19!)3. [G3] J. You and H. A. Cohen, "Classification and segmentation of rotated and scaled texture images using texture tuned masks," Pattern Recognition, vol. 36, pp. 245-258. [54] J. G. Daugman, "Complete discrete 2-d gabor transforms by neural networks for image analysis and compression," IEEE Transaction On Speech, Signal Processing, vol. 36, pp.1169-1179.

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

86
[55] D. Hubcl and T. \Viesel, "Receptive fields and fUllctional architecture in two nonstriate
vi~ual

areas 18 and 19 of the cat," J. NeurophysioI., vol. 28, pp. 229-289, 1965.

[56] F. Campbell and J. Kulikowski, "Orielltational selectivity of the human visual system," J. Physiol., vol. 187, pp. 437-445, 1966. [57] D. Pollen and S. ROllller, "Visual corticallwurolls as localized spatial frequency filters,"

IEEE Trans. Syst. Man. Cybero., vol. 5. pp. 907-916. 1983.
[58] S. Bruner and B. Langfeldt, Recent Results in Cancer RescaTch.

Berlin, Germany:

Springer-Verlag, 1990.
[59] L. Shen, R. M. Rangayyan, and J. E. L. Desautels, "Application of shape analysis

to n1allllllographic calcifications," IEEE Trans. Medical Imaging, vol. 13, pp. 263-274,
1994. [60] N. Karssemeijer, "Adaptive noise equalization and recognition of micro calcification clus-

ters in mammograms," In : Bowyer, K. W. and Astcly, S. (cds), State of the Art in
Digital Mammographic Image Analysis, vol. Singapore: World Scientific, pp. 148-166,
1994. [61] R.
~I.

Nishikawa,

~1.

L. giger, K. Doi, C. J. Vyborny, and R. A. Schmidt, "Computer

aaided detection of clustered micro calcification in digital mammograms," Med. BioI.
EnComput., vol. 33, pp. 174-178, 1995.
[62] R. N. Strickland and H. I. Hahn, "\Vavelet transform for detecting microcalcifications

in mammograms," IEEE Trans. In Med. Imag., vol. 15, pp. 218-229, 1996.
[63] T. Netsch and H. Peitgen, "Scale-space signature for the detection of clustered microcal-

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

87
cifications in digital mammograms," IEEE Trans. In Med. Imag., vol. 18, pp. 774-78G,

1999. [64] S. Yu and L. Guan, "A cad system for automatic detection of clustered micro calcific ations in digitized mammogram films," IEEE Trans On .Med. Imag., vol. 19, pp. 115-126,

2000.

[Go] 1. EI-Naqa, Y. Yang, Â£..1. N. \Vernick, N. P. Galatsallos, and R. Â£..1. Nishika\\"(l, "A
support vector machine approach for detection of microcalcifications," IEEE Trans On

Med. Imag., vol. 21, pp. 1552-1563,2002.

[GG] S. Sentelle, C.Sentelle, and 1\1. Sutton, "Multiresolution-based seglllClltation of calcificat.ion for the early detection of breast cancer," Jomnal of Real- Time Imaging, pp. 237-252, 2002.
\'01.

8,

[67] J. C. Bezdek, .J. Keller, R. Krisnapuram, and N. R. Pal, Fuzzy Models and Algorithms for'

Pattern Recognition and Image Processing. Norwell, MA: Kluwer Academic Publisher,
1999. [G8] T. C. \Vang and N. B. Karayiannis, "Detection of microcaleifications in digital mammograms using wavelets," IEEE Trans. on Medical Imaging, vol. 17, pp. 498-509,1998.

[69] S. Lai, X. Li, and Vl. Bischof, "On techniques for detecting cicumscribed masses in
mammograms," IEEE Trans. on Medical Imaging, vol. 8, pp. 377-386, 1989.

[70] P. F. Stetson, F. Sommer, and A. 1\lacovski, "Lesion contrast enhancement in medical
ultrasound imaging," IEEE Trans. on Medical Imaging, vol. 16, pp. 416-425, 1997.

[71] I. Daubechies, Ten Lecture on Waveletes.

Philadelphia,PA: SIA1\L

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

88
[72] K. R.
~Iuller,

S. Mika, G.Rat.ch, K. Tsuda, and B. Scholkopf, "An introduction to

kcrncl-hasC'd learning algorithms," IEEE Trans. on Neural Networks, vol. 12, pp. 181201, 2001. [73] H. Heath, K. Bowyer, D. Kopans, R Moore, and P. Kegelnwyer, "The digital database for screening mammography," in Yaffe, M.J.(ed.),IWDM 2000 5th International IVorkshop on Digital Mammography.,
SCI'. ~Iadisoll,

\VI:

~Iedical

Physics Publishing, 2001.

[74] A. C. of Radiology, BTeast Imaging Reporting and Data Systems(BIRADS)(2nd edn.).

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

VITA
NA1'.lE: PLACE OF BIRTH: YEAR OF BIRTH: POST-SECONDARY EDUCATION AND DEGREES: Mah(li Sabri Tehran, Iran 1972 Sharif Uniw'rsity of T('chnology Tehran, Iran 1990-1995, BSc Co-winner of PrcsidC'nt Award for Illdustrial ImlOYation 1998

HONORS AND A\VARDS:

RELATED WORK EXPERIENCE:

1995-1997 Software developer / designer, 1'.Iaharall Corp., Te hran: Iran 1997-2000 Senior software designer, Maharall Corp.,Tehran,Iran 2000-2002 Project Manager, 1'.laharan Corp.,Tehran,Iran

PUBLICATIONS
1. M. Sabri, .1. Alirezaie, "Support Vector Machine with Optimized Kernel for texture

classification" submitted to Journal of Electronic Imaging (SPIE) 2. M. Sabri, J. Alirezaie, "Support Vector Machine with Optimized Time Frequellcy Kernel for texture classification" Accepted in Electronic Imagillg 2003(SPIE) 3. M. Sabri, K. Hazaveh, .1. Alirezaie, K. Raahemifar, Two Dimellsional Local DiscTimination Basis Algorithm for Texture Classification, in proceeding of IEEE Canadian conference in Electrical and computer Engineering 2003. 4. M. Sabri, J. Alirezaie Texture Classification Using Improved Local Discrimination Basis Algorithm in proceeding of IEEE SSP (Statistical Signal Processing) 2003. 5. M. Sabri, .1. Alirezaie, S. Krishnan, Audio Noise Detection using Hidden 1'.larkov Model, to be published in proceeding of IEEE SSP2003. 89

Reproduced with permission of the copyright owner. Further reproduction prohibited without permission.

