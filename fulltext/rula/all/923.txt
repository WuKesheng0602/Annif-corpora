Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2009

Unsupervised learning for biomedical applications
Nasim Shams
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Electrical and Computer Engineering Commons Recommended Citation
Shams, Nasim, "Unsupervised learning for biomedical applications" (2009). Theses and dissertations. Paper 648.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

UNSUPERVISED LEARNING FOR BIOMEDICAL APPLICATIONS
Nasim Sharns, B.Sc Isfahan University of Technology, Isfahan, Iran, 2007

A thesis presented to Ryerson lJniversity in partial fulfilln1ent of the require1nents for the degree of 1\tiaster of 1\pplied Science in the progran1 of Electrical and Cornputer Engineering

R.yerson University Toronto, Ontario, Canada, 2009 @Nasin1 Sha1ns, 2009

Atlthor's Declaration
I hereby declare that I arn the sole author of this t hesis. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose · of scholarly research .

I further a.uthoFfz~,-R.ycrson University to reproduce this thesis by photocopying or by other rneans, in total or in part, at the request of otherinstitutions or individuals for the purpose of scholarly research.

Abstract
UNSUPERVISED LEARNING FOR BIOMEDICAL APPLICATIONS
@Nashu Shan1s, 2009 Master of Applied Science Electrical and Computer Engineering Ryerson University

With the growth of application of cornputcrs in the generation aud analysis of bio1ucdical data, a. variety of eomputerized 1nethods and algoritluns have been proposed to optim.izc the process of acquisition and analysis of the data. Although advanced computerized techniques have provided the n1eans for rnorc precise diagnosis, the intcrprcta.tion of the recorded data in son1e cases is an issue due to the large amount of the data or com.plexity of it. \Vhilc most of t.he existing work in. the literature consider supervised techniques for analysis of the collected data, the use of unsupervised techniques in the area of analysis and classification of bimncdical signals is relatively unexplored cmnpared to supervised approaches. In general, the investigation of application of unsupervised techniques for analysis of biornedical signals can be worthwhile frmn different view points. In son1e cases, bimnedical databases tend to contain a large anwunt of data. Genmnic da.tabases or pathological speech databases are exmnples of this kind. The devdopnwnt of any supervised nwthod for analysis of such databases requires predse n1anual labeling of the data, which ean be extrcnwly costly. However, t.:he use of an unsupervised classifier can be beneficial to accelerate the process tuld to acquire infonnation about the structure of the dataset. In addition, the characteristics of the collected bion1edical data can be affected by the recording process. In this work application of unsupervised learning in two biomedical signal processing problen;1s is investigated. In the first prohlmn, fuzzy C-rneans clustering has been used in design of a c01nputer aided diagnosis n1ethod for detection of abnorrnalities in s1nall bowel capsule endoscope images. The performance of the systCin shows an accuracy of 76which is an acceptable rate for an unsupervised 1nethod. In the second case, self organizing tree 1na.ps (SOT~1) has been applied to audio signal dassification for hearing aids. An accuracy of 96% was achieved for discrirnination of hmnan voice from the environmental noise, which is one the 1najor classification scenarios for hearing aid applications.

ii

Acknow ledgn'lent
It is a pleasure to thank those who rnade this thesis possible. First, I would like to sincerely thank rny supervisor Dr.Sridhar Krishnan for providing m.e the opportunity to pursue 1ny rvla.stcr's degree and for his continuous guidance and support throughout the two years of rny lVIasters. I am forever grateful to l'lirn. This thesis would not have ·been possible without the help of my friends a.nd colleagues Behnaz Ghoraani and Clair vVintcr who provided me with the features used in Chapter 4 and Chapter :3 of this thesis and Dr.Karthi lhnapathy for his invaluable assistance on rnany occasions. Also, I would like to show rny gratitude to Dr.l\IIatthcw Kyan for his enlightening and infonnative discussions he ha.d with rnc about the SOT1vL Last but not the least, I would like to thank rny family, \Vhorn without their constant encouragcrnent I would not be able to a.ccon1plish a.n y of this, and my unforgettable friends at Ryerson University, Elna.z shokrollahi, tv1ehrnaz shokrollahi, Pa.yman Shokrolla.hi and Sina Zarei for their priceless support and for rnaking the two years of rny 1Vlaster's at Ryerson so
nwm.orable.

Thank you ...

iii

Dedication
To rn:y par·ents, NasTin and Behr-ouz,. for f;heir love, suppor·t and encom·agemenL

iv

Contents
1 Introduction 1.1 Cmuputer n1ethods in n1edieal ilnaging 10101 Cmnputer aided diagnosis (CAD) 1.102 CAD for srnall bowel images 1. 2 Audio signal classification 1.3 Organization . . . . . . .
2
1

5 6 8 9 11

Unsupervised Learning and Clustering 201 Introduction and l\tiotivation 2.2 Steps of a Clustering Task .. 2.3 Clustering techniques . . . . . 2.3.1 Partitional Algorithrns 2.3.2 Fuzzy C-nwans Clustering 2.4 Neural Network Approaches . 2.4.1 Self-Organizing Architectures 2.4.2 'I'he Kohonen Self-Organizing Feature rnap (SOFIVI) 2.4.:3 Self-Organizing Tree iviap (SOTJ\!1) . . . . . . . . .
o ·

13 13

15
16 17 19 20 21 21

23
30
~30

3

Unsupervised Learning in Medical In1age Classification 3.1 Sn1all Intestine Inutges . . . 3.2 Fc~at urc Extraction . . . . . 0 . . . . . . . . .. 0 0 3.2.1 CIE Lab Color Space 0 0 0 .· . . . . 0 0 .. . 3.2.2 Shift Invariant Discrete Wavelet Transfonn . 3.2.3 Cross Co-occurrence 1natrices 30:3 Classification and Results 3.3.1 Future \Vork . 0 0 . . . . . . 0 Unsupervised Learning in Hearing Aids Signal Analysis 4.1 Audio classification for hearing aids 4. 2 Audio signal classification . . . . 4.2.1 Taxonorny of audio signals . 4.2.2 Audio signal classification . 4. 2.3 Review of the previous works
v

33
35

37
;:10 41 44:
50

4

50 51

51
52 56

4.. 3

4.4

4.2.4 The proposed rnethod Feature extraction . . . . . . 4.3.1 !\!latching pursuit TFD 4.3.2 Non-negative rnatrix factorization 4.3.:3 Feature selection . . . . . Classification and results . . . . . 4.4.1 classification rnethodology 4.4.2 Results . . . . . . . . . . .

59 60 60
62

64
67 67

68
74 75 75 76

5

Conclusion 5.1 Classification of small bowel in1ages 5.1.1 Results and discussion 5.1.2 Future work . . . . . . 5.2 Classification of audio signals .5.2.1 Results and discussion 5.2.2 Future work . . . . . .

77 77
78

vi

List of Figures
1.1

Irna.ges of the PillCa.m . . . . . . . . Clusters in different shapes and sizes Clustering block diagram . . . . . . . Hierarchy of unsupervised approaches hierarchical representation of self-organizing approaches .. l\t1apping sarnplcs frorn the input space onto the SOFl\ti lattice: The input x.;. is assigned to the winning node. rrhe neighbors that arc connected to the winning node in the lattice are updated according to the gaussian neighborhood function (courtesy of l\tLKyan) . . . . . . . . . . . . . . . . . . . . . . . . . . Clustering procedure in the SOTIVI(left) vs the SOFl\ti(right). The SOFl\1 uses <1 predefined lattice to span the input steps and assigns the san1plcs to the eloscst node, or the winning node. 'I'he input is used to update the winning node and its innncdiatc neighbors in the lattice. The SOTlM (:right), on the contrary, explores the input space by a growing structure in a top-down rna.nner. As it can be seen in the figure , unlike the SOFrvi, the SOTI\!1 does not suffer frmn the nodes begin trapped in low density areas. (courtesy of rv1 .Kyan) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . Different H( t) decay st.ratcgies illust r ated for period of generation of 10 neurons. (a) Pure H(t) decay; (b) Stepped fl(t;) with regular period; (c) Stepped Ii(t) with irregular period; (d) Stepped n·(l-) vvith irregular period and node inhibition. (courtesy of IVI.Kyan) . . . . . . . . . . . . . . . . . . . . . . . . Sa.n1ple sn1a1l bowel irnages collected by the PillCanr obtained frmn the Image Atlas of Given Irna.ging Ltd. (a) Healthy sn1all bowel , (b) norrnal pyloric: region , (c) nonnal jejunurn, (d) small bowel polyp, (c) snw.ll bowellyn1phon1a , (c) srnall bowellyn1phorna . . . . . . . . . . . . . . . . . . . . . . . . . . . . Block diagrarr1 of the feature extraction procedure . . . . . . . . . . . . . . . Top row: nonnal srnall bowel images. (a) normal srnall bowel, (b) nonna.l jeju.nun1, (c) nonnal jejunurn, (d) nonnal srnallbowel. Bottorn row: abnonual hnages. (e) small bowel polyp, (f) small bowcllyrnphoma, (g) polypoid n1ass, (h) GIST tun1or. . . . . . . . .. . . . .. . . . . . . . . . . . . . . . . Wavelet coeHicients for two level decornposition of a small bowel ilnage . . .

6 14 17 18

2. 1 2.2

2.3
2.4

21

22

2.6

25

2.7

28

3.1

3.2
3.3

35

3.4

39

vii

3.5 :3.6 30 7

SID\VT decomposition tree for three levels of decornposition with the best selection corresponding to the minilnum cost path . . . . . . . . T'he Receiver Operating Characteristics curve with an area of 0.76 . . . . . . Texture pairs with identical second-order statistics. (a) The upper half and lower half contain the same tcxtons. The visual systen1 can not discriminate the different textures without careful scrutiny. (b) The upper region contains textons different frmn the lower region. Htuna.ns ca.n differentiate the two textures effortlessly. . . . . . . . . .
0 0 · · · · · 0 0 · · ·

40 44

47 52 59

40 1 Taxonorny of audio signals used in this work 4.2 Block diagrmn of the feature extraction and classification

viii

List of Tables
:::t 1
:3.2 3.3 The definition of confusion matrix . . . . . . . . . . . Classification results for the fuzzy C-1neans classifier . Classification results for the k-means classifier . . . . Classification results for the SOTIVI classifier . . . . . Cornparison of the results of unsupervised classification rnethod with supervised classification for different feature sets and different color spaces. Typical features used for n1usic content retrieval . . . . . . . . . . . . Summary of the feature extraction and classification techniques used in the literature for audio classification . . . . . . . . . . . . . . . . . . . . . . . . . Different audio classes in the data set and the nun1ber of signals in each class Confusion 1natrix for classifying hunum vs non-h1nnan audio signals . . . . . Different audio classes in the data set and the nun1ber of signals in each class Confusion Inatrix for classifying hurnan speech vs n1usical instnunents . Confusion n1a.trix for classifying na.tural vs artificial sounds . . . . . . Confusion rnatrix for classifying hurnan vs nature sounds . . . . . . . Confusion rna.trix for classifying 1nusical instrmnent vs aircraft sounds 45 L15 46

3.4
3.5

46
49

4.1 4.2

55

4.3 4.4 4.5 4.6 4. 7 L1.8 4.9

58 68 70 71 71 72 72 73

ix

Chapter 1 Introduction

A

S the application of computers in the acquisition and generation of 1nedical data is growing, the use of con1putcrized analysis rnethods in processing the n1edical data is

increasing. Although the usc of advanced ilnaging and recording techniques has provided the physicians with n1ore precise diagnosis, the interpretation of the data is sornctilnes an issue due to the large arnount of data or cmnplexity of it. As a result, a of cmn-

putcr based rnachine learning rnethods have em.crged to assist the doctors to interpret the data and extract n1orc infonnation frmn the recorded signals. In general, 1nachinc learning techniques caJ1 be divided into three groups; Supervised learning, unsupervised leaxniug a.nd reinforccrnent learning [1], Supervised learning: In supervised learning, a teacher provides a category label or cost for each pattern in a. training set, the goal is to reduce the stun of the costs for these patterns. Unsupervised learning: In unsupervised learning or clustering the ca.tegory or label of the data is not known beforehand. There is no explicit teacher, and the system fonns dusters or natural groupings the input patterns,

Reinforce1nent learning The reinforcCinent learning n:wthod is analogous to learning \Vith a eritie. In this case no desired category is given for a datum; critic instead, only gives a binary feedback that states whether the tentative. category is right or wrong but docs 1

2

not say specifically how it is wrong. Unsupervised classification is a natural way to proceed tm,v ards cornputer-a.idecl diagnostic systerns and the 1nain motivation of using such scheme is to provide the automatic:. clustering
of the irnage features in the sanw way hurnan visual systern does. It. helps to get an insight about the structures and patterns that already exists in the data and hence enables us to

find n1ore robust features, which correspond to the natural characteristics of the da.ta. The usc of unsupervised nwthods n1ight seem unpromising at first. One might even
B ... <:ik

the

question whether or not it is possible to learn anything of value from unlabeled sarnples.
However, there arc ma,ny cases where unsupervised dassification could be very beneficial.

For exarnple, collecting labeled data is not

alwa~ys

an easy task. In fact, sometimes labeling a

large dataset can be surprisingly costly a.nd not feasible" lJnsupcrviscd classification can be used to discover the natural groupings that exist in the dataset and then use supervision only to label the clusters found . Furthennore, in smne cases the characteristics of the features change with tirne. Hence an unsupervised classifier can be used to track the changes and ·
n1ake the necessary corrections. Another application of unsupervised learning is to get smnc

insight about the structure of the dataset. The knowledge about the intrinsic characteristics of the dataset and the patterns that n1ight exist in the dataset, can help us to cmne up with
rnore efficient feature extraction and classifieation strategies.

There is a considerable mnount of work in the literature on the usc of unsupervised techniques for analysis a,nd classification of biomedical signals. Here 'live discuss the application of so1ne of the popular unsupervised techniques for biornedical s. ignals. e Independent component analysis (ICA): ICA is an en1erging field in bimnedical signal processing. The wide usage of ICA is 1notivated by the eom.n1on practical problcrn in biornedical signal processing. Recording bimnedical signals usua.lly involves several source signals and several sensors. Each sensor receives a rnixture of souree signals" The problen1 consists of recovering the source signals frorn the rnixture" In [2] and [3} a cornbination of wavelet transform. and ICA has been used to separate fetal ECG fron1 [4] n1other ECG" In [5] Bigan adopts ICA to detect chaothic cardia

:3 arrhythrnia. in EGG signals. Gao ct al. [4] usc a combination of ICA and Single value
decomposition (SVD) to extra.ct fetal ECG frmn the rnixturc signal. In [6} and [7], Joyce ct al. and Zhou et al. have used ICA to remove eye blink artifact and power line artifacts frorn EEG signal. The works done by Navarro ct al. [8] and .Joshua ct al. [9] arc 1nore examples of adaptation of ICA for EEG signals.
e

Principle con1ponent analysis (PCA): PCA is a widely used dirnensionality reduction technique in data analysis and popularity con.1es front three hnportant

properties: First, it is the optimal (in tenus of mean squared error) linear schmnc for cmnpressing a set of high dimension vectors into a set of lower dirnension vectors and then reconstructing. Secon.d, the n1odcl pararnctcrs can be computed dircctl:y frmn the data - for example by diagonalizing the sarnplc covariance. Third, com.pression and decmnprcssion arc easy to perfonn given the model paran1etcrs, and require only m.atrix rnultiplications. In [10] a PCA based 1nethod for ECG-QRS detection has been proposed. Once the QRS complex has been identified, a rnore detailed exmnination of ECG signal ean be perforrned. A c01nhination of wavelets and PCA is proposed in [11) for dccmnposing EiviG signals. In [12] and [13] PCA has been used along with neural network and self organizing rnaps (SOiVI) for pa.ttern recognition in EIVIG signals. In

[14] original PCA has been applied to the data for dassification of cardiac arrhythrnias. In [1.5} a n1ethod for clustering analysis of QRS cornplexes has been proposed that integrates PCA and SOIVL Another exan1ple of integrating n1ethods for ECG can be found in [16] where PCA and SVIVI have been used. The main goal is to classify nonnal fron1 abnonnal signals and then specify the kind of abnonuality for abnormal signals.
e

K-n1eans clustering: K-n1cans clustering is one the simplest and most basic clustering tedmiques, which will be described in Chapter 2. In [17] a k-rneans clustering technique ha.s been adopted to classify all discrete points fonning a heart rnodel with respect to their position vectors or source-to 1neasurement transfer rnatrices. [18] also

4

uses k-mcans clustering for EEG arousal detection.

· Fuzzy C-1neans clustering: Fuzzy C-rneans clustering is another popular clustering
technique that is used widely in pattern recognition problems. T'his method is very dose to K-1neans clustering and will be described inmore details in Chapter 2. In [19] a fuzzy d ustering n1ethod has been used to classi(y three types of abnormality. Average period and the pulse w-idth arc the features used for classification, and then fuzzy clustering was perfonncd for these two features. The work by Geva and Kcre1n [20] also utilizes wavelet transform for feature extraction and unsupervised. fuzzy clustering for classifying brain-states. In the work by Ajiboye and \iVeir [21] also fuzzy clustering is used for E:MG Pattern Recognition for JMultifunctional Prosthesis Control. Finally, in [22} Ajiboye and vVeir use fuzzy C-rneans clustering to cla."lsify six rnajor grasping patterns of the hmnan hand. The usc of unsupervised techniques in the area of biornedical signal analysis has been the topk of nutny research works. In general, the investigation of application of unsupervised techniques for analysis of bimnedical signals can be worthw hilc fron1 different view points. In smnc cases, bion1edical databases tend to contain a large amount of data. Genmnic

databases or pathological speech databases are exarnples of this kind. The develop1nent of any supervised n1ethod for analysis of such databases requires precise Inanuallabeling of the data, which can be ext.ren1ely costly. However, the usc of
tiD

unsupervised classifier can be

beneficial to accelerate the process and to acquire infonnat.ion about .the structure of the dataset. In addition 1 the characteristics of the collected bion1edical data can be affected by
n1any factors during the recording process. For instance, the recorded EEG sign~l can be

affected by .the stress level of the patient or rnovCinent artifacts. In the process of recording bimnedical data, sorne patients n1ight need special nwdieations (e.g sedative drugs) or the recording procedure needs to be perfonned in a modified way due to the special conditions of the patient. Another exarnple is the capsule endoscopy ·where preparation of the bowel for the experiment is one of the factors that affects the characteristics of the captured irnages. Hence, in1ages captured during different experilnents could posses rrwre or less different

5 characteristics and this could deteriorate the perfonnance of a supervised classifier. Finally,
unsupervised learning rnethods can be used to get sorne insight about the structure of the dataset and intrinsic characteristics if the data or can be cmnbincd as a preprocessing with a supervised approach to build a. robust classifier. In this work the application of fuzzy C-means and self organized tree maps (SOTIVI) for the biomedical signals will be exarnined. These two algorithms will be explained fully in Chapter 2. Fuzzy C-rneans has been previously applied to bimnedical signals such as EEG, ECG etc. It has also been u.scd for seg1ncntation of rnedieaJ irnagcs. However, its application for classification of biornedical irnages is unexplored. ln Chapter 3, fuzzy C-means has been used for classification of abnonnalities in the s1nall bowel images. Chapter 4 covers adaptation of SOTIVI for classification of audio signals for hearing aid application. SOTTh/1 has been used for seg1ncntation of biological ilua.ges but in this work the application of this algoritlnn for analysis of bi01nedical signals will be investigated for the first time" In Section 1.1 and Section 1.2 smne of the background infonnation required for Chapters 3 and Chapter 4 arc provided respectively.

lsl

Computer metl1ods in medical imaging

Iviedical ilnaging is one of the n1ost explosive devclop1ncnts that has taken place in the last two decades. The new findings in this area not only provide a better dia,gnosis, but also offer new hopes for trcatrnent of n1any critical diseases. Different imaging techniques such as X-ray, com.puted tom.ography (C'I') a.nd 111agnetic resonance in1aging (Th/IRI), provide the physicians with a more precise and non-invasive diagnostie tooL For example, for cancer or epilepsy, the predse identification of the lesion already facilitates the usc of surgery, the only therapeutic option for son1e patients. Also, they can provide rnorc accurate diagnosis for son1e parts of the body which arc not easy to evaluate using conventional methods. The srnall intestine is one of the parts that has been ahl\lays difficult to evaluate because of its shape and size" Traditional endoscopy used to be the only way the gastroenterologists to

get an insight fonu the s1nall bowel and detect abnonnalities. The procedure is extrmnely

6 inconvenient for the patients. During the operation the endoscopic tube, which is rather stiff,

is inserted from the mouth and moves around to navigate patient's gastrointestinal tract. The procedure causes a lot of discomfort and the patients are given anesthetics before the operation. In 2000, a new product was introduced by the Given imaging Ltd that attracted a large amount of interest from the gastroenterologists; the PillCam. PillCam is a tiny capsule endoscope with nearly the size of an ordinary capsule, which has a built in camera. The capsule is ingested from the mouth and as it goes down through the gastrointestinal tract (by the natural movements of the tract) it captures images and sends them wirelessly to a receiver that the patients wears around his/her chest. The capsule is exerted naturally and the patient lives normally during the procedure. Fig 1.1 shows images of the PillCam.

Figure 1.1: Images of the PillCam

1.1.1

Computer aided diagnosis {CAD)

The benefits of the imaging techniques to achieve reliable diagnosis however depends on the quality of image interpretation as well as image acquisition. Computer technology, has made a significant contribution in the quality of interpretation of medical images in the recent years. The use computer-aided diagnosis (CAD) in the area of medical imaging was initiated in the 1960s and has increasingly grown since then [23]. Nowadays, CAD is being widely used in detection and diagnosis of many different kinds of abnormalities in medical of the routine clinical procedure for detection images. For instance, CAD has become a part _ of breast cancer from the mammograms in many hospitals [23].

7 CAD is a diagnosis rnadc by a radiologist who uses the output frmn a cmnputer. The
cmnputerized analysis of nuxlical hnages is provided to the radiologist as a second opinion in detecting lesions, assessing extent of disea.'Je, and rna.king diagnostic dedsions. \Vhile the final diagnosis is rnade by the radiologist, the usc of CAD is expected to iinprove the interpretation cmnponent of medical ilna.ging [2-4]. These m:e son1e of the reasons that the use of CAD in the area. of 1ncdical irnaging is growing rapidly. In addition, interpretation of images by hun1ans can be affected by the presen.ce of structure noise in the hnage and the presentation of cmnplex disease states requiring the integration of vast arnounts of iinage data and clinical infonnation. Another benefit of using CAD in the analysis of medical i1nages is to deal with the large an1ount of data. T'he interpretation of screening i1nages is a repetitive and tedious task, which involves visual scanning of n1ostly healthy subjects for a specific abnormality. Screening of Inanunograins for early detection of breast cancer, the use of CT for detection of lung cancer in high risk individuals and the usc of colonogra.phy for detection of polyps that 1nay lead to colon cancer are cxmnples this kind.

It n1ight be useful to ernphasize sornc of the differences between con1puter-aided diagno-

sis and another similar concept in t.his area., a.utornated cornputer diagnosis (23]. In both approaches, rnedical iina.ges are analyzed by cornputer algorithrns. However there are 1uajor differences between the two n1ethods. In CAD, ra.diologists use the cornputer output as a second opinion, and 1nake the final decisions. The cmnputer output may accepted or rejected

by the radiologists based on their level of confidence. Furthermore, in this approach even if the performance of the con1putcr is not equal to or higher than that of radiologists, it can be still cmnbined with the radiologist's skills to achieve better diagnosis. \i\Tith autmna.ted cmnputer diagnosis, however, the decision is n1ade by the cmnputer. Thus the efficiency of the processing technique is required to be very high and cmnparable to that of radiologist's.

1~1$2

CAD for small bowel in1ages

8

The rnethod developed in this work for the analysis of the snw1l bowel hnages is designed as a CAD method. Although images captured by the PillCam provide the gastroenterologists with n1ore infonnation about the inside of small intestine, one nw.jor drawback of this technology is the large arnount of data that is generated in each ·expcriinent. During each exarnination, an average of 50000 irnages or an equivalent of 8 hours of video is captured.
~Ianual

evaluation of such a large nun1ber of images is a very tiine consmning and laborious

task and ilnportant dues might be rnisscd due to fatigue or repetitive nature of the task. Hence, a CAD n1cthod caJl be developed and used as a second opinion to point out the suspicious regions to the gastroenterologists. The first work on a CAD rncthod for detecting ahnornwlities in the sm.all bowel images captured by the PiUCarn was published in 2006 by Khaderni et al. [25]. In this work rnultiresolutional analysis is perfonned on the gray scale hnages to extract the texture infonna.tion and linear diserirninant analysis is used for classification of the hnages. In [26], Li and .Nieng use color inforrnation to detect bleeding in the small intestine. In [27] Bonnel et al. propose a feature extraction n1ethod based on wavelet analysis and cross co-occurrence matriees, where the extracted features contain both color and texture information. Canonical discrirninant analysis is then applied to the features for classification. In the work by Barbosa et al. [28}, the features are extracted fron1 wavelet coefficients and 1nulti layer perceptron (IviLP) is used as the classifier. All of the n:tentioned papers use supervised classification for detecting abnor:malities in the irnages. In this work however, the application of unsupervised classification will be investigated, Although the existing rnethods with supervised elassification typically report higher accuracy rates, the use of unsupervised classification can be advantageous in rnany ways. The perforrna.nce of a supervised classifiers depends on the train data. Hence, a wrongly labeled daturn, which is not ra.re in bimnedica.l databases, can a.ffect the overall performance of the classifier. Besides, in order to obtain sufficient reliability, the dataset needs to be large enough to overcom.e proqlerns such as overfi.tting and the curse of dirncnsionality [29]. In addition, characteristics of the ilnages captured by the PillCmn, are affected by the bowel preparation procedure.

g Colors of intra lurninal rua.teria.l rnay be significantly different between examinations. This

leads to different im.age characteristics and consequently different features for each experinlent. A supervised classifier could be biased by the characteristics of the in1ages in the train set. vVhereas, under such circun1stances, an unsupervised cla.ssifier does not suffer frmn the cha.nge of the image characteristics like a supervised classifier docs. Finally, the application of unsupervised techniques could be useful to discover clusters that might naturally exist in the data. and the features that are related to these groupings. In this work, a feature extraction schcnlC similar to the rnethod used in [27] is used, which extracts both color and texture infonnation. A fuzzy C-1neans classifier is applied to the dataset to find two clusters in the dataset, representing nonnal (healthy) abnorn1a.l

(diseased) irnages. The results of the unsupervised classification not only can be used as CAD, but also can be used to get rnorc insight about the structure of the da.ta and help find the features that best represent the characteristics of the data.

1. 2

At.tdio signal classification

Audio da.ssification for hearing aids is one the growing areas of application of signal processing and rnachine learning rnethods in biornedidne. Although there arc a wide variety of hearing aids available, studies show that hearing aid users arc not very satisfied with the perfonna.nce of their hearing aid in the noisy outdoor cnvironn1ents such as restaurants, workplace, street etc [30]. In fact, in a survey performed in [31], low perforrnance in the noisy environrnents is one of the major reasons that hearing irnpaired people are reluctant to use their hearing aid devices. Sirnilar studies show tha.t better perforrr1ance of the hearing aids in the appearance of the noise, is one the most desirable irnprove1nents arnong the hearing aid o\vners. In order to overcornc these problen1s, several audio processing and classification algorithms have been proposed for the hearing aids to discri1ninate different auditory classes and detection of the audio environrncnL In a survey obtained by Kochkin
[~32]

it was observed

that a hearing aid that can operate efficiently under different listening conditions is very desirable. Frmn 223 hearing aid users that took this survey, less than one third were satisfied

10 with their hearing aid if the deviee worked properly in only three or fewer environnwnts.
However, over 91% of the users were satisfied if the hearing aid could be adjusted according to the audio envirom: n ent. Thus, there is growing evidence that substantially better user satisfa.ctjon can be expected of the perfonna.ncc of the hearing aid can be in1proved. Audio classification is a one of the research areas that has attracted 1uany researchers in the recent years. Discrilnination of different audio classes .is one of the ta.':lks that htunans do effortlessly everyday. However, irnplmncnting such eapability in machines is a dernanding job and takes a la.rge am.ount of effort. A la.rge nurnbcr of papers in the literature is dedicated to various techniques for dassification of audio signals for .different applications. There is a wide range of applications . for the classification of audio signals. Speech processing for security applications and hmnan con1putcr interactionj n1ultirncdia data rnanagenwnt and distribution, security, bionwtries and bioacoustics arc sornc of the applications of audio signals classification [33]. Furthcnnore, with the growth of application of con1puterized

processing techniques in the area of bimnedical signals, the use of audio processing and classification algorithms for biornedical applications such as hearing a.ids and pathological voice reeognition is rapidly inereasing. Various nwthods ha.ve been proposed for discrimination of different audio classes. Howevcr1 nwst of the existing works use supervised classification schernes. The proposed solutions include hidden IVIarkov n1odel [:34], k-rneans clustering, histogra1n driven Bayes classifiers, rnultilaycr perceptrons [35] , Gaussian mixture rnodels ·[36], k nearest neighborhood (K-NN)[::>7], support vector 1naehinc (SVl\l)[:38J and linear discriminant analysis (LDA) [33]. The application of unsupervised n1ethods, on the other hand, is relatively unexplored. At this point, the results of the unsupervised nwthod can be either presented to the user or can be followed by a supervised approach for further processing, The works proposed by Sha.o et al. [39) and Rauber et al [t10) are two exmnples of application of clustering 1nethods for the n1usie databases. Using a clustering n1ethod has the advantage of avoiding the constraints of a fixed taxonomy, which rnay suffer frmn a1nbiguitics and inconsistencies. Considering the variety of the audio signals, son1e of the signals nw,y

11
sixnply not fit within a given category [41]. The classification 1ncthod used in this work is a fusion of supervised and unsupervised cla.'3sificr. The proposed method in this work is based on the self organizing tree n1aps followed by a fuzzy labeling of the data.. Another hnportant issue in the cla.'3sification of audio signals is the extra.cted features. There is a large anwunt of work in the literature on various feature extraction 1nethods for audio signal da.'3sifica.tion. The feature extraction strategy depends on the classification scenario and characteristics of the signals. In this work, however, the 1nain focus is on the classification part rather than feature extraction. A brief overview of the existing teehniques for audio feature extraction is provided in Chapter
4.

1. 3

Organization

In this thesis, the suitability of two unsupervised techniques for biomedical data will be explored. Chapter :3 is dedicated to the application of an unsupervised technique (fuzzy C-means clustering) for detection of abnorma1ities in the capsule endoscopy hnages while Chapter ,4 describes an unsupervised method (SO'I'!vi) for cla<ssification of audio signals for lwaring aid application. The organization of this thesis is deseribed here;
1. Introduction: In the first Chapter, background infonnation on CAD in niCdical inw,g-

ing and audio classification for hearing aid application is provided. An overview of the existing works in the literature on the application of unsupervised methods for bimuedical signals is also given in this Chapter. 2. Unsupervised learning and clustering: An overvie\v of the dustering techniques is provided in this chapter. In addition the two clustering n1ethod used in this work is explained in 1nore details.

:3. Unsupervised Learning in :1\tiedical In1age Classification: In this chapter the
application of fuzzy C-n1eans clustering n1ethod for detection of abnormalities in the small intestine irnages will be described. A feature extraction Inethod based on wavelet

12
coefficients and cross co-occurrence rnatrices is used to extract color and texture infor1nation of the irnages. Then fuzzy C-mcans is applied to the extracted features. 4. ·unsupervised Learning in IIearing Aids Signal Analysis: In Chapter 4 a classification nwthod based on the SOTlVI clustering algorithrn is used for discrin1ination of audio signals for hearing aiel. The feature extraction techniqueused in this work is based on thne-frequency clecornposition of the audio signal, which is n1ore suitable to handle the non-stationary audio sig. n.a.ls. A dassification technique, which is a fusion of supervised and unsupervised dassifieation is applied to the extracted feature and tested in different scenarios such as discrimination of hurnan/non-hurnan, natrual/ artificial and hurnanjn1usic. 5. Conclusion: The conclusion for this thesis and the discussion of future works is given in the last chapter.

Chapter 2 Unsupervised Learning and Clustering
2. 1 Introduction and Motivation
NSUPER~I~ED

U

classific-ation is a pattern recognition technique that aims to con-

struct decision boundancs based on unlabeled dataset. That is, we are interested

in exploring the dataset and sec what can be done when all we have is a. collection of unlabeled sa1uples. Unsupervised classification is also known as data clustering which is a generic label for a variety of procedures designed to natural groupings, or dusters, in

rnultidirnensional data, based on rnea.sured or perceived sirnilarities ~unong the patterns [42].

One example of clustering is the detection of a region eontaining a. high density of a specifie
pattern cornparcd to the rest of the background. Son1e of the functional definitions proposed for a cluster are: · Patterns within a duster are n1ore sin1ilar to each other than those belonging to dif-

fercnt dusters.
e

A cluster, which consists of an area with relatively high density points, is separated frmn other clusters an area of relatively low density.

Figure 2.1 shows exa:mples of dusters with different sizes and shapes [43). The problen1 of unsupervised classification or clustering is very challenging because data can contain

13

14

Figure 2.1: Clusters in different shapes and sizes

clusters with different shapes and sizes. Even the nurnbcr of dusters in the data depends on the resolution with which we view the data.. The question that n1ight cmne to the mind is that why anyone is
intcrc~sted

in using unlabeled sainples and whether or ·not it is possible

even in principle to learn anything valuable frorn an unlabeled dataset. There are at least five n1ain reasons for using unsupervised classification [1].
e First, in smne cases labeling a large dataset can be surprisingly costly. One exam-

ple could be the application of land-use classification in renwte sensing. In this case obtaining the "ground truth" infonnation for the samples, \Vhieh is the category for each pixel in an in1age, requires one to visit the specific site assoeiated with the pixeL Another exmnple is speech classification. Recorded speech is free but ctccurately labeling it (which is Inarking the word or phoncnlC uttered a.t each tinlC) is extrcnwly tirne consurning. If a classifier can be crudely designed on a small labeled dataset and then run without supervision on a large unlabeled dataset, n1uch tirne and trouble can be saved. · The second advantage of using unsupervised learning is that it rnakes it possible to proceed in the reverse direction; train with large arnounts of inexpensive unlabeled data, and then used supervision only to la.bel the groupings found. This is the case for

15
large data ruining a.pplieations where we are dealing with a large dataset with no prior knowledge about the contents of the data.
e

Third, in many applications the characteristic-s of the data can change over tim.e.
F<x cxarnple, in an autornatcd food classification prohlcn1, the extracted features may

change as the season changes. In such ca.<:;e, the perfonnance of the systen1 can be irnproved by running a classifier in unsupervised n1ode to track the changes. · Fourth, unsupervised n1ethods can be used prior to a supervised classifier to in1prove feature selection. vVe can usc these 1nethods to find more meaningful and discrinlinatory features that will be used for classification. 'I'here are unsupervised n1ethods that represent a fonn of"srnart preprocessing" or"srnart feature extraction". · Lastly, in the early stages of an investigation we ean use unsupervised rnethods to some insight into the nature or structure of the data. The discovery of distinet subclasses or sin1ilarities a.xnong patterns or of major departures front expected characteristics 1nay suggest we significantly alter our approach to design the classifieL

2. 2

Steps of a Clustering Task

A typical clustering task usually consists of following steps [44]:
1. Pattern :representation (including feature extraction and/or feature selec-

tion): This phase refers to representing the data to the clustering algoritlun. The
infonnation regarding the mnubcr of dasses, type and scale of the features are considered in this phase" In this step one can use either the original data.'let or usc a set of features extracted frorn the da.t&':let to represent the data. Feature extraction is the process of applying different transfonnations, decompositions and analysis on the dataset to obtain salient features. In many c&':les, feature extraction is followed by a feature selection step to identify and choose the nwst effective feature subset frmn the original feature set.

16 2. Defining .(or selecting) a proxhnity 1neasure: There are a variety of distance

measures defined for xneasuring the proxirnity of the points in the dataset e.g Euclidean distance,
~!Iahalanabis

distance, :Minkowski distance etc. 'I'he distance 1neasures will

be described in 1nore details in Section 2.4.
:3. Clustering: Grouping the smnplcs in the datasetcan be done in a nuinber of ways.

The result of the clustering depends on the type of clustering rnethod used to group the data. The output can be hard (each point belongs to only one cluster) or fuzzy (where each point has a xnembership value in different clusters) or a nested series of partitions when a hierarchical clustering approach is used. Various clustering techniques will be discussed in Section 2.5. 4. Data abstraction (optional): Typically data abstraction is a compact representation of each cluster, usually by using duster prototypes or cluster centroid. 5. Cluster validation (optional): Cluster validation is the asscssrnent of the output of the clustering algorithn1. It determines how "good'1 the clustering results a.re. All clustering algorithms, when represented with a dataset, produce clusters regardless of whether or not the data actually contains clusters. In those cases where the dataset actually contains clusters, son1e clustering nwthods return better results. In order to dctern1ine if the groupings found by a clustering algorithrn are actually rneaningful and evaluate ho\v good or how poor the clusters arc, different quantitative rneasures a.re developed. Figure 2.2 shows the block diagran1 of the first three steps, including a feedback loop wl1ere the feature extraction and selection rnethod.s can be adjusted based on the grouping results

[45].

2. 3

Clustering tecl1niques

Cluster analysis is a very useful technique in different areas of pattern recognition. The speed, reliability and consistency with which a clustering algorithrr1 can organize a. large dataset ha.'3

17

l'~

l,oop

Figure 2.2: Clustering block diagram

led to \Videspread use of elustering tcehniques in areas such as data 1nining [45], infonnation retrieval [46] [47] image segn1cntation [48], signal cmnpression and coding[49] and rnachinc learning. Consequently, nurnerous clustering algoritluns have been proposed in the literature and new ones continue to appear. Classification of the existing clustering nwthods can be done based on different points of view. Figure 2.3 shows a hierarchical representation of the clustering algoritluns [43] [50]. Based on this taxonmny, the algorithrns can be divided into two 1najor classes, paranwtric and non-parametric. Non-pararnetrie approaches, in turn, fall within two groups: Partitional clustering a.nd Hierarchical clustering. The techniques in the first category are n1ainly based on the popular iterative squa.re-error partitional clustering. These algoritluns ahn to obtain the partition which n1inilnizcs the within class scatter or maxin1izes the within class scatter. Hierarchical algorithrns in the second category are 1nostly based on the a.gglom.erative hierarchical clustering. These algoritluus attmnpt to organize data in a nested sequence of groups which ean be displayed in the form of a dendrogram or a tree.

Partitional Algorithms
Partitional clustering algorithms atte1npt to obtain a single partition of the data. These rncthods have the advantage in applications where a large an1ount of data is to be processed. In such eases 1 the usc of a dendrograrn is not cmuputationally feasible . The partitional techniques usually generate clusters optimizing a criterion function which is defined either

18

Gmph b~~s~d
(.&.RT .~30Fl\,·1 ,i>JG ,.

{C>.J\ b;3l~A3d.
PSC~ hi:~Sf.:d)

(MST, f(;!Mlmn
\~.f~.~n-~.~r · .,. -~

Unksge ba(J~d
(S~n~~~;~ .. Hn~; . C~>ii',plel!'J·Ill1k}

Muttml
nei~;hbors

5\}Tfv\ ..,)

Figure 2.3: Hierarchy of unsupervised approaches

locally or globally. The algorithm is run rnultiple tilnes with different starting points and the best configuration is then selected as the result of clustering. One of the rnost popular partitional clustering algorithrns is square-error clustering algoritlun. The general objective is to find the cluster configuration within the dataset, for which the squared-error is minimum
for a fixed nmnber of clusters. The squared-error for cluster Ck is defined as the sun1 of

Euclidean distances between each pattern in Ck and its cluster center m/'. This distance is also called the within-cluster variation.

(2.1) \Vhere :.r~ is the 'ith pattern belonging to cluster Ckl
cluster
nk

is the nun1ber of patterns in the
a,s

ch

and rnk is the Inean, or center of the

I< th cluster defined

(2.2)

19 The overall squared-error for a configuration is the sun1 of the square-error for all dusters
described as:
K

Eli:= ~ek
k=l

2

~

.2

(2.3)

objective of the squared-error a.lgoritlun is to find the cluster configuration that n1inirnizcs the total square-error for a fixed munber of clusters K. The resulting partition has also been referred to as the 1ninimurn variance partitkm. The k-n1ea.ns clustering is one the shnplcst and the n1ost popular square-error a.lgorithn1s. Th.e a.lgoritlnn is cmnputationally efficient and gives good results on a dataset that consists of cmnpact and well separated clusters with a hyperspherieal shape [43). The algorithrn is even able to detect hypcrellipsoidal clusters if the IVIahalanobis distance is used in 2.3 in defining the squared-error. The following briefly explains the algorithm steps [1]: 1. Begin with

I<

initial cluster centroid.

2. Classify the n san1ples according to the nearest distance. 3. Rccmnpute the cluster center for each cluster. If the new cluster centers are the sarne as previous ones, there is no need to recalculate the centers again. The current cluster centers arc the final ones. Otherwise, go back to step 2 and classify the points with the new duster centers. A big drawback of the algorithm., however, is the lack of a guideline to select the critical para1neters such as the number of clusters and the initial cluster centers [51]. Several variations have been proposed to hnprove the perforrn.ance of the ba.'Jic k:-rneans a1goritlun. One of the possible modifications is to introduce a fuzzy criterion function. This results in fuzzy c-means algorith1n, which will be described in the next subsection.

2e3 . 2

Fuzzy C-1neans Clustering

In the traditional clustering approaches, each pattern belongs to one and only one cluster. This type of clustering is called hard clustering. In contra.."t to hard clustering n1ethods,

20

fuzzy clustering rnethods assign a degree of mernbership in ea.c h cluster to each pattern. A fuzzy clustering algorithm can be converted to a hard algorithn1 by assigning a pattern to the cluster with the largest degree of Ineinbcrship. The steps involved in perfonning a fuzzy c-means algoritlnn is very close to that of k-rncans, except for the objective function, which is defined as

(2.4)
i=l j:::::l

vVhere rn is the fuzziness index, cluster , i., :rj (j

{lij

is the degree of rnernbership of observation

~ri

in the

= 1, 2, ... , N)

is the jth d-dilnensional data point and Ci is the d-dirnensional

center of the duster. The fuzzy set Theory ,;vas initially applied to data clustering by Ruspini [44]. Although the results of the algorithn1 is better than the hard k-nwans a.lgoritlun, FClVI can still converge to the localrninin1a of the squared-error criterion funetion.

2~4

Neural Network Approaches

Artificial Neural Networks (ANN) has been widely used in pattern recognition applications in both supervised and unsupervised ways. ANN approaches typically fall into two groups: · The first group are those based on emnpetitive learning or learning vector quantization [50]. In cornpetitivc learning sirnilar patterns are grouped together by the network represented by a neuron. This grouping is done based on correlation an1ong the data. In unsupervised context, well-known example of ANN are the Kohonen 's self-organizing rnap (801\!1) a.nd adaptive resonance theory proposed by Carpenter and Grossberg in 1990 (50]. The architecture of these networks are single-layered. Patterns m·e represented to the input layer and associate to the output layer. The weights between the input and output layers are updated iteratively until a tennina.tion criterion is fulfilled. This group of algorithnlS will be discussed in more details shortly. · The second group are techniques derived fron1 the principle cmnponent analysis (PCA), factor analysis and independent component anlysis (ICA)[52].

2.4.1

Self-Organizing Architectures

21

Self-Organizing methods are closely related to unsupervised learning. A number of selforganizing architectures are: the Kohonen self-organizing feature map, neural gas approaches , hierarchical feature map, dynamic hierarchical architectures, non-stationary architectures and hybrid architectures [50]. The self-organizing technique used in this work is self-organized tree mapping, which is a derivation of the Kohonen self-organizing map and will be the focus of this Section. Figure 2.4 shows the hierarchy of different self-organizing methods .

... __
I

.........
~

Fundlmelal
lr'CIIIIdLnl

.,..... ....... ........
-~
~

I

o,..niD

I
~-y

lr'CIIIIdLnl

...

-

·.
I
...
-

~

.........

I
Nil ..:Nell ......miiP
(tAl)

(80FM)

Figure 2.4: hierarchical representation of self-organizing approaches

2.4.2

The Kohonen Self-Organizing Feature map (SOFM)

In the basic SOFM algorithm, input samples from ad dimensional feature space, are mapped onto a grid with lower dimensions (usually two or tree dimensional) [50]. Each node on the grid acts like a memory element; it stores the prototype vector that describes commonly occurring vector patterns from the input space. The points that are close to each other in the input space are mapped onto the neurons that are nearby in the grid. Whenever a node is updated, the nearby nodes are also updated based on their distance from the original winning node. Figure 2.5 shows the mapping of samples onto an SOFM lattice. The steps

22

l
Figure 2.5: Mapping samples from the input space onto the SOFM lattice: The input xi is assigned to the winning node. The neighbors that are connected to the winning node in the lattice are updated according to the gaussian neighborhood function (courtesy of M.Kyan)

involved in the SOFl\'1 algorithrn are: 1. Initialize the weight vector WiJ of each neuron in the lattice using a random va1u.e. This randmn value can be a sample random.ly selected frmn the dataset -'Y.. 2. randmnly select an input vector
:J..~i

from the dataset and present to the network.

3. choose a winning node wi*i* based on the minimum Eudidean distance. 4. update the neurons on the lattice according to a Gaussian neighborhood function defined as: (2.5) where
1'ii

represents the position of the node at(i,j) on the lattice, a(t) represents the

leaTning rate, which decays from a sn1all initial value and a( t) controls the radius of the neighborhood, which also decays over tilne and H(t) is the neighborhood function defined as: (2.6)

5. update a{t) and o'(t)
6. repeat iteration frorn step 2 until there is no significant change in
Wij

Association between the nodes is an hnportant advantage in SOF:tv1 that helps the evolution of the network ea.n be useful for extracting inter-clusters relationships. This property is useful for visualization of rnultivariatc data, where data with high dinlCnsjon is rnappcd onto a two din1cnsional lattice. Since the 1na.pping preserves the topology, neighbor nodes in the lattice represent the sa.n1ples with related properties in the original data [50] .

2. 4 . 3

Self-Organizing Tree Map (SOTl\1)

SOTT\11 was originally introduced in [58] to rmnovc irnpulse noise from ilnagcs. The algorithm
is a hybrid of the traditional SOFivf ,which was explained in the previous chapter and the Adaptive H.esonance Theory (ART) [54]. Like ART, the growth of the network is controlled

24 by a vigilance test, which essentially watches for an input that is in contrary to the current

knowledge about the input feature spaee. If sueh an input is found the resonance occurs a,nd results in refinernent of the winning node or generation of a new node. On the other hand, like the SOFlVI the generated network is 1norc topologically aware and the refinen1ent of the
existing prototypes is guided by a Kohencn style learning rule. Like its counterpart SOFJ\11,

the SOT:N1 algoritlun uses con1petitive learning approach to find clusters within the data while rnaintaining the general topology of the feature space. However, unlike the SOF1\t1, SOTIVI does not suffer from the disadvantage of nodes being trapped in the low density areas
[50] and the network has a dynarnic structure and grows front a single node. Generation of

a new node is guided by a hierarchical control function H.(t), which acts as an ellipsoid of significant siinilarity. H.(t) can be assun1cd as a global vigih.u1ce threshold that is used for n1casuring the proxirnity of a new input sample to the nearest existing node in the network. Smnples that fall outside the scope of the nearest existing node, result in generation of a new node as child of the winning node. By initializing ll(t) to start frmn a large value, the clusters discovered at the early st.a.ges of the clustering will be far frmn each other. Decay of H (t) over thnc results in partitioning the data space in low resolution at the early stages of the clustering, while favoring partitioning at higher resolutions later. Figure 2.6 depicts the clustering process in SOTl\1 and SOFIVI.

The SOTM Algorithm
The steps involved in the basic SOTNI algoritlnn are:
l. Initialization: ra.ndmnly select a training vector frmn the feature space .-Y. Initialize

t.he network parameters H(O) and a(O) 2. randornly select an input :v frorn the feature space and calculate the distance di frorn
::c to all cu.rren.tly existing neurons 'l.Vj(j

= 1, ... , 1VcJ when Nc is the total munber of

currently existing neurons. 3. seleet the node with the m.inilnmn distance as the winning node ·w 1.~ sueh that di(x, wk) =

rwinidJ(x, ·t.ui)

25

·
·

..

·

Figure 2.6: Clustering procedure in the SOTM(left) vs the SOFM(right). The SOFM uses a predefined lattice to span the input steps and assigns the samples to the closest node, or the winning node. The input is used to update the winning node and its immediate neighbors in the lattice. The SOTM (right), on the contrary, explores the input space by a growing structure in a top-down manner. As it can be seen in the figure, unlike the SOFM, the SOTM does not suffer from the nodes begin trapped in low density areas. (courtesy of M.Kyan)

26
4. if dj(: r, 1o 1z) : _:; H(t:) , then: update the weight vector of the winning node using the reinforced learning rule: wk(t + 1)

= wk(t) + o:(t)[~r-

wi] where a(t) is the lea.rning

rate and H(t) is the hierarchical control function. 5. alternatively if dj (~r, tv k) > H (t), then spawn a new node at the position x frorn the
winning node
'IBk·

6. update network parmneters

· a(t;): decays with tin1e, lies on [0,1}, resets periodically. · H(t): decays with time, controls the hierarchical level of the tree.
7. repeat fn.:nn step 2 until either: · there is no significant change in the network. · all neurons are allocated and there is no significant change in the network. · nutxinuun number of epochs is reached.

The Hierarchical Control Function
In general we assunw that the date samples arc presented to the network randomly. The only constrained hnposed on H (t) is that it should be nwnotonieally decreasing over time as the samples are represented to the network [50]. Besides, H(t) should be ideally initialized to a large enough value to cover the span of the da.ta" There arc two standard hierarchical control function proposed for the original SOT11 algoritlun: linear and exponential decay.

H('t)

=

H(O)- [(1- e-fJrH)I/(0)/f,]t
H(t) = I-I(O)e-tfTii

(2.7)
(2o8)

where

T

H is a thne constant) which is bound to the projected size of the input data . .Y,

f/(0) is the initial value, t is the nurnber of iterations (or sa1nple presentation) and f, is the nurnber of iterations over which the linear · version of H ( t) would decay to the same level

as the exponential version. One benefit of initializing H(t)

27 a large value, possibly larger

than the maxirnum variation within the data, is that all levels of resolution across the data can be explored. There arc two natural choke proposed for H(O) [50]:

Range-based H(O}: :E (all ranges across the dimensions of X) SD-based II(O): > 6ax a. distance beyond twice the n1axinnnn deviation (T = :3crx) of 99%
of the smnples frmn the 1nean. In addition, II (t) can operate in different nwdes. Figure 2.7 shows alternative strategies for decay of H(t) [50). 1. Pure H(t) decay: In this case in transition to a new I-I(t) value, only one single randmn san1ple is considered. This is the typical approach for decay of H(t). However, it results a rather lilnited search to he conducted in each hierarchical level. In fact, in the lower levels of resolution, the slower the decay of H(t) means the data is being assessed more thoroughly.

2. Stepped Ii ( t) with regular period T H step: A stepped fonn of decay is introduced
in this approa,ch. This allows at least T Ii step san1ples to be explored before narrowing

the search to a finer resolution. Due to the random nature of sa.n1ple representation, it is assurned tha.t a.t least sorrw smnplcs frorn all parts of the data are explored in this period.

3. Stepped I-I(t) with irregular period: This mode is in fact an extension of xnodc
2. In this mode the counter is begin reset every tin1e a new node is generated. This

guarantees tha.t the search will continue for at least another

T

Hste]J sa.1uples after a

new node is generated. This allows a ncvv node to have a chance to adjust itself.

4. Stepped H (t) with irregular period and node inhibition: This n1odc adds an
additional constrain to 1nocle 2 by forcing the network adaptations only for a period of
T

H stqJ before inserting a new node. This allows nodes to organize and have sufficient

28 time to adjust themselves before new (and possibly unnecessary) nodes are allocated.

This process repeats every time a new node is spawned and gives the network a period of time to settle before generating a new node.

.
'
(a)

:.~f '· \

(c)

(d)

Figure 2.7: Different H(t) decay strategies illustrated for period of generation of 10 neurons. (a) Pure H(t) decay; (b) Stepped H(t) with regular period; (c) Stepped H(t) with irregular period; (d) Stepped H(t) with irregular period and node inhibition. (courtesy of M.Kyan)

Learning Rate

The learning rate a(t) is an important factor in organizing the network. Like the hierarchical control function , H(t), a(t) can also operate in number of different global or local modes. In global modes a single learning rate is applied to all node, whereas in local modes an individual rate operates for each node a set of nodes. There are a few modalities proposed for the operation of the learning rate. Some of these modes are discussed below. The first mode is the original periodic reset strategy proposed for the SOTM. Modes 2-4 are the new appr?aches suggested in [50]. However, it has been mentioned in [50] that Modes 1 and 2 are noticed to have better results for an SOTM process.

29 o Global periodic reset: In this traditional approach the network rnmnory is refreshed

with regard to the underlying density.
e

Global reset upon node generatioin: This approach is a n1odification of the first nwde based on the idea that a network needs to reorganize its nwn1ory only \lilhen a new node is generated.

o

Local rest of winner and child upon node generation: This rnodification restricts the plasticity only to the region of the rnap which is recently grown. based on the assurnption that the adjustlnent of the nodes that are distant from the growing region is not necessary.

e Local reset of winner, child and siblings upon node

generation~

This nwde is

very sirnilar to 1node :3, with the exception that children of the winning node arc also considered to be plastic within the updating region. As n1entioned in [50], the global reset n1odes (1,2) tend to outperform the local reset n1odcs. In addition, it is suggested that rnodc 2 is preferred because the reset is justified when new infonnation is to induced to the network after node generation.

Chapter 3 Unsupervised Learning in Medical Image Classification
3.1 Small Intestine Images
EDICAL ilnaging is certainly one of the most explosive devdopn1ents that has t<llien place in the last two decades. The new findings in this area not only provide a better diagnoses: but also offer new hopes for treat1nent of 1nany critical diseases. Different
irnaging techniques such a.s lVIRI and x-ray provide the physicians with a nwrc precise non-

invasive diagnostic tool. Different rnedical imaging techniques arc complen1cntary and their progress has in1mcdiatc reperct1ssion on the devcloprnent of treatn1ents as t.hey provide a
nmch less invasive diagnosis cmnpared to previous n:tcthods.

For exan1ple 1 for cancer or

epilepsy, the predse identifica tion of the lesion already facilitates the usc of surgery; the only therapeutic option for son1e patients. Also, ima.ging techniques can provide 1nore aecurate

diagnosis for son1c parts of the body which arc not easy to evaluate using conventional
methods. The sn1all bowel for exa.n1ple has always been difficult to eval uatc because of its shape and size. Traditional endoscopy used to be the only way to gather actual images frmn inside the patients intestine. The operation needs to be perfonned by highly skilled doctors and is inconvenient for the patients. The endoscopy's tube, whieh is inserted fron1

the n1outh, is rather stiff and causes son1e discmnfort as the doctor navigates the patient's
gastrointestinal tract. In addition, since the ca1ncra cannot reach all parts of the small

30

:31 intestine, diagnosing diseases of the s1nall intestine was a Inajor problern for doctors[55]. The
appm.u:ance of capsule endoscopy in 2000 has generated a large
~uuount

of interest a1nong

gastroenterologists. PillCa.n1 is a tiny capsule (lOnun x 27mn1)[56], which was introduced by Given Imaging Ltd. The capsule is digested frorn the n1outh and Inoves slowly through the gastrointestinal tract (including the sina.ll intestine) by a dint of natural contractions. As the capsule moves through the gastrointestinal tract, it captu.res color irnagcs and trans1nits thmn wirelessly to a receiver that the patient wears around his or her waist [25). The capsule is exerted naturally with the natural bowel movmnent.s [25]. The da.ta collected through the exan1ination is an 8-hour-long video that provides visualization of the 21 foot. long sn1all bowel, which used to be a "bla.ck-box" to doctors [25]. The procedure is a1nbulatory and enables the patient to live norn1ally during the endoscopic exan1ination. Clinical results show that PillCam is a superior diagnostic rnethod for detecting the diseases in the small intestine

[55). Four n1ain types of cancer, which are usually found in the s1nall intestine are listed and
described below [25). Adenocarcinon1.a: This type of cancer originates in the epithelial lining of the n1ucosa and is rnainly found in the duodenurn. Sa:rcoina: This cancer originates in the n1usde wall of the sn1all intestine and is mostly found in the ileum. Carcinoid: This type of cancer originates in the specialized neuroendocrine cells are found in the srnall intestine, the ileurn and sornetimes in the appendix.

Lymphoma: This type of rnalignancy is usually fonned within the ly1nphoid tissue of the
small bowel. They are comrnonly found in t:he jejunurn or ileunL The PillCam provides gastroenterologists with a new n1cthod for detection of the srnall bowel diseases through a live video representation, which was not available with the traditional endoscopy. However, the drawback of this technology is the large amount of data which is collected in each experiment. An average of 50000 irnages or 8 hours of video is

32
recorded during an examination. Manual evaluation of these images is a an extremely laborious and time consuming task and important clues might be missed due to fatigue or repetitive nature of the job [57]. Therefore, a computer aided diagnostic method can be developed and used as a secondary opinion, that views and points out the suspicious areas to the doctor. Figure 3.1 shows sample images taken by the PillCam, which includes three normal and three abnormal images.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3.1: Sample small bowel images collected by the PillCam obtained from the Image Atlas of Given Imaging Ltd. (a) Healthy small bowel, (b) normal pyloric region, (c) normal jejunum, (d) small bowel polyp, (e) small bowel lymphoma, (e) small bowel lymphoma

In addition, a computer aided system can be used to confirm and compliment the doctor's diagnosis. It can help to decrease the number of required biopsies, detect cancer in an early stage, and in general improve the quality of diagnosis [25]. The first work on the automatic

;33 detection of abnonnalities in the capsule endoscope irnages was proposed by Khademi et a1.
[25), where linear discriminant analysis has been uses for classification. Other classifica.tion techniques used in this area include canonical discrirninant analysis [27] and :tvfLP [28] [26]. vVhile all the previous works have used supervised techniques, in this work the application of an unsupervised rnethod will be explored. The investigation of application of unsupervised approaches for n1edical in1ages can be useful frmu different viewpoints. One of the reasons for considering an unsupervised n1ethod is that i1nage characteristics might vary in different experirnents. One of the rnost irr.1.portarrt features of the capsule endoscopy proecdurc is the bowel preparation. Colors of intra luminal 1naterial n1ay be significantly different between exmninations. Therefore, the charactcristies of the hnages used for training the supervised classifier rnight be different fron1 those of hna.ges captured in the test experhnent and this could affect the pcrfonnance of a. supervised classifier. However, the perfonna.nee of an unsupervised classifier does not depend on the characteristies of the training and test data. Another important factor to be considered is the size of dataset. Although the ground truth for the hnage dataset is given in this case, in order to build a robust supervised classifier the dataset has to be large enough to guarantee good genera.lization. In addition, unsupervised teehniques can be used to get smne insight about the structure of the data and existence of the natural patterns, discovery of distinct subclasses or similarities an1ong patterns and to find 1ueaningful and discrin1inatory features that best represent natural groupings in the data.

3$2

Feature Extractio11

Like alrnost any other classifieation problcn1, the first step in the classifieation of s1nall bowel irna.ges is extracting a set of descriptors frmn t.he iinages that ean efficiently represent eharactcristies of the images and have high discrhninatory power. The extracted features are then fed to the classifier, which is unsupervised in this ease, to rnake the decision. The outcome of the classifier is related to the diagnosis of the images, which can be either a. nonnal (healthy) or an ahnorn1al (diseased) ii:nage. In addition, since the input space consists of images the

34 input data is expected to have a very high dimensionality (256x256 in this case). Performing

any classification method on data with such high dimensions would be extremely costly and computationally intensive. Hence, the need for a feature extraction scheme becomes more significant. Figure 3.2 shows the feature extraction procedure performed in this work. The Images are first converted into CIE lab color space, then shift invariant wavelet transform is performed on the images, and then cross co-occurrence matrices are calculated on the wavelet coefficients. Each of the blocks will be described in more details shortly.

Input ir!wge in

RGB

Labelno

(normal/abnormal)

)

Figure 3.2: Block diagram of the feature extraction procedure

The two main features used in this work are color and texture.

These features are

directly related to the clues used by the doctors to evaluate the images. Texture is one of the important clues in analyzing both color and gray scale medical images. The human visual system can discriminate different textured areas in an image effortlessly. However, implementing this task on computers has been the subject of research in the area of machine vision for a long time. Texture: The images captured by the PillCam are from different organs, structures and anatomical objects along the gastrointestinal tract. It can be noticed from the experimental dataset that normal images contain mostly smooth and homogeneous texture ·with very little disruptions in uniformity except for folds and crevices [25]. On the other hand, abnormal images tend to contain different textures at the same time and

35 more heterogenous textured areas. This can be seen in Figure 3.3.
Color: Color contents of an image also provides discriminatory information about the re-

gion or objects in the image. Normal regions usually exhibit pinkish colors, whereas abnormal regions show some difference in color compared to the surrounding area. Malignant tumors are usually inflated, more reddish and severe in color compared to normal areas while benign tumors show less intense hues. Redness may specify bleeding, blackness could be treated as deposits due to laxative, green may be the presence of fecal materials and yellow relates to pus information of the image [58].

(a)

(b)

(c.)

(d)

Figure 3.3: Top row: normal small bowel images. (a) normal small bowel, (b) normal jejunum,

(c) normal jejunum, (d) normal small bowel. Bottom row: abnormal images. (e) small bowel polyp, (f) small bowel lymphoma, (g) polypoid mass, (h) GIST tumor.

3.2.1

CIE Lab Color Space

As explained in [59], abnormal regions are observed to show more or less differences in color compared to the surrounding regions. In fact, malignant tumors are usually inflamed, reddish and more severe in color. Hence, color information plays an important role in the

36
detection of abnormalities in srnall bowel hnages. The im.ages taken by the PillCa.rn are eom.pressed in .JPEG and coded in R.GB color space. Hmvever, in this work the feature extraction is perfonned in the CIE lab color space. Unlike the RGB, the La,b color space is designed to approxin1ate the hun1an vision. The n1.ain advantage of using lab color space is tha,t this color space is perceptually unifonn, which rneans a change in the color value results in a change of about the same visual ilnportancc. In addition, Eudidea.n distance Ineasurc has a better perfonnancc in this color space. The L cornponcnt defines the huninance, a. is red/blue chrmninancc, and b·is yellow /blue chron1inancc. The equations for converting the RGB color space to the Lab color space are given below [59]:
r Y

x_

]

=

[

[

Z

0.41. 2 . 0 .3_._ · _ 57 .· .·._·· 0.072 0. 180 .]. [ G R _ ] _. 0.212 0.715 0.019 0.119 0.950 B

(3.1)

The first step is to transfonn the color form. RGB color space into XYZ color space using Equation 3.1. Next, values in the ..Yl'" Z color space are converted into the Lab color space using the following equations
L

y 1/3 y = 116(. · ) - 16for-. > 0.008856

Yrt

Yn

(3.2)

L

= 903.3( Y. ) fory: ::; 0.008856
n n

y

y

(3.3)

a=

500(J(J~·)-

f< )

(3.4)

y' !(-z 0 }-) - )) b =50 ( '( Yn' Zn ··
where

(:3.5)

f(t)

=

ifi for t > 00008856
16

= 7.7787t + 116 for t < 0.00885 _ 6 J~(t) . where AYn, 1/:l, and Zn correspond to the white color in the XYZ color space and L, a and b are the huuinance a.n d chrmninance in the Lab color space respectively.

3o2o2

Shift Invariant Discrete Wavelet Transforn1

37

In the previous works on the classification of the srnall bowel ilnagcs [27] [25], the application of the shift invariant discrete wavelet transfonn has been investigated and has been proven to be efficient for extracting the texture inforn1ation inparticular. :rv1ultiresolutional ana1ysis of the ilnagcs is a natural way to highlight the features of interest, such as texture, in a.n in1age.
It provides a. representation of the image in which the textural infonnation can be retrieved

easily. This rnethod is basically a projection of the images onto a set of finite-length and fast-decaying oscillating functions known as wavelets. Wavelet transforn1s can be classified into discrete wavelet transforrns (D\iVT) and ·continuous wavelet transfonns (C\VT) . The latter operates over every possible scale and translation whereas the forn1er uses a specific subset of scale and translation values or representation grid. The DvVT is a scale-invariant transfonn since a decornposition of the hnagc contains all the
b~1..sic

functions needed to

decmnpose different scales of the image. This feature of the DvVT is of in1portance since pathological areas in the sn1all bowel ilnages may occur in different sizes. However, as the Pillcam travels freely through the bowels, the orientation of the images is not always the sarne and the location of the suspicious areas is unpredictable. However, the D\VT is a shif-invariant transfonn, which means di1Terent translations of an input irnage results in different set of D\VT cocflicicnts [60]. In order to extract a consistent feature set, one solution is to use the shift-invariant discrete wavelet transfonn (SIDWT). Several solutions have been proposed to overcmne the shift-invariant property of DVVT. The method suggested by :rviallat et al is based on selecting the local exterrna frmn the filtered and fully sampled version of the ilna.ge. These local cxtcnna.s are used to detect and translate the shift since a shift in the signal results in a shift in the local exterrnas. However, due to lack of decin1ation there is a large mnount of redundancy and each level of decomposition has as many smnplcs as the original input irnage, which makes the algorithn1 to be costly overall. One solution for the eases where the dictionary contains Inany redundant wavelet basis functions is the Ivia.tching Pursuit (I\1P) algorithn1. However, this algorithn1 is cOinputationally intensive itself and can slow down the the syste1n. Bradley proposes a rnethod

38 whieh is a trade off between the sparsity of representation and tirne invariance where critical

san1pling is perfornwd for certain subbands only and the rest arc fully sarnplcd. The result of this rnethod is an approxintate SID\lVT. The n1entioned algorit.luns either suffer forrn high cornputational cost or achieve only an approxirnation of SIDWT. The SID\V1' algorithrn proposed by Beylkin does not have the discussed shortcon1ings. It calculates the D\VT for all circular shifts in a cornputationa.lly efficient way. In addition, since this transforrr1 uses orthogonal basis, it results in less redundancy. An extension of Beylkin's a.lgorit.lun to 2-D signals is developed by Lian et al. The application of this algorithn1 to the bio1nedieal irnagcs is shown to give prornising results in the previous works [25][27]. The algorithrn proposed by Liang and Parks in [61] is used in this work to decon1pose the i1nagcs in the wavelet. dmnain. In fact, this algorithm is a.n easy and fast i1nplemcntation of nmltiresolutional analysis using filtcrbanks. It makes for a good localization for high frequencies and a good frequency precision for low frequencies. The 2 - D filterbank schcrne used for an _N x lV irnage applies a high pass filter on the in1agc followed by a low pass filter. Applying the low pass filter l10 (z) and then the high pass filter H 1 (z) to each row of the image X creates two inutgcs: one conta,lning the low frequencies of X , ..\ (L) and the other one containing the high frequencies X (H). The rows, and . Y(L) . and X(H) axe subsarnpled by a factor of 2, then the sante filters H0 and H 1 arc applied to the columns of each image. Finally another subsarnpling by 2 is perforrned on the colurnns. The result, as depicted in Figure :3.5, is four irna.gcs LL, H L, LH. and H H for two levels of decornposition. The smnc procedure is repeated for further decornposition. The high pass filters applied in the horizontal and vertical directions in this seherne cn1phasize the high frequency contents of the hna.ge and give oriented: The H H-, H L, andLH sub bands represent the diagonal, horizontal and vertical edges respectively. The 5/3 Gull wavelet has been used in this work as used in [62) because the filter lengths ru:e sn1all and can wa.rrant an efficient irnplementation. In order to be inva.riant to translations, the algoritlnn should look at all translations of the input ilnage and select the best set of wavelet coefficients. The procedure consists of two parts, first, an efficient algoritlnn for cmnputing the wavelet

39

Figure 3.4: Wavelet coefficients for two level decomposition of a small bowel image

transform for all the translations and second a fast quadtree search algorithm. The wavelet decomposition is performed for different shift values. There are four elementary shifts in this algorithm: (0, 0), (0, 1), ( 1, 0) and ( 1, 1) where the first index corresponds to the row and second index corresponds to the column·. Every shift can be represented as a combination of these elementary shifts. So the obtained by [25]
ith

level of decomposition for the input shift (a, b) can be

LLi(a, b)=

L L ho(m- 2a)h (n- 2b)LLJ0

1

(m, n)

(3.6) (3.7)

m

n

m

n

(3.8)
m n

(3.9)
m n

The result of this decomposition is a tree shown in Figure

3.5 [27], which contains all

the DWT coefficients for N 2 translates of the image X, where the size of the image is

N x N. In this work, since the images are represented in the lab color space, three trees are

40 generated, each corresponding to a color channel. The total entror)y of the color image is

found by calculating the entropy ofthe corresponding color subba.nds and then adding them. up. Since the rniddle wavelet detailed channels contain n1ost textural inforrna.tion [63] and in order to lirnit the nun1bcr ofcoefficients being generated, only two levels of dccmnposition was perforrned on the huages in this work.

Figure 3.5: SID,JVT decomposition tree for three levels of decomposition with the best selection corresponding to the minimum cost path

3o2.3

Cross Co-occurrence matrices

1'he principle of cross co-occurrence Inatriccs is based on the gray scale co-occurrence rna-

trices(GCl\!1). The GCrvi for a gray scale ilnage shows the distribution of co-occurring values at a given offset. Each entry in a GCivi, J.lf ('i, j), indicates how often a pixel with gray-level
value i occurs at the distanced to a pixel with the value j, where dis the given offset vector.

A cross co-occurrence 1natrix (CC1V1) is the counterpart of GCrvi for color ilnagcs. Let I he
an iV
X

JV Slnall bowel inu"tgc and bl, b2, and b3 the three color subbands. c~l,b 2 is the CCl\11

rnatrix for the color subbands bl and b2 for the offset d. Hence, each entry of the 1natrix,

'-11 c~l ,b2 ( i,j), represents the probability of the intensity level i in the color subband bl and

intensity level j in the color subband b2 to occur at two locations seperated by the distance vector d. As shown in [27], since the subbands are oriented, only some particular CCJVIs are calculated on each subband. The displaeen1ent vectors are grouped according to the orientation of the subbands: vertical, diagonal and horizontal. Six rnatrices a.re generated for · ·1 · · ""b an d cac 1 :::;uo.
01·

· Is · · t nccs ·· · 1n · t.o t·l · an lffid.ge. · , F. . 11 y, s1nce .· . · 111a a for 1na
bl b2

2 cbl,b 'd
2

·d an

2 1 cb .·d ,b

rep1csen t
0 · ·

the s;Jme infonnation, the average of these two 1natriccs J\1d '

=

cbl,b2+cb2,!Jt 'd 't

is used in this

work. The use of CCMs has the advantage of extracting color and texture information at the sarne thne. As proposed in [27][64], four principal features can be derived frmn each 1natrix: contrast, energy, homogeneity and entropy. In this work however, based on the efficiency of the features only two features are kept: energy and hmnogencity. The fanner is calculated as the stun of the squared elements. If J\,1 is a cross co-occurrence rnatrix, the energy for the matrix is calculated as
IV= EAJ(i,j) 2
i,j

(3.10)

Hon1ogcneity is another feature used to describe the textural characteristics in the hnage. This fea.ture n1easures the closeness of the distribution of elen1ents in the eo-occurrence 1natrix to the matrix diagonal and is defined by
(3.11)

Two sets of features arc extracted from each ilna.gc based on the energy and hmnogcneity rneasures. As rnentioned earlier the CC:tvis are calculated for three groups of offsets, vertical, horizontal and diagonal. Hence, there arc 6 1natrices for each subband or 18 m.atriees per image. Finally, t\vo sets of features arc extracted from each CCivi based on energy and hon1ogencity measures, which rnakcs for a total nurnber of 36 features for each hnagc.

3. 3

Classification and Results

To evaluate the perfonnancc of an unsupervised elassification schcn1e on the small bowel dataset, two sets of experinmnts were conducted using k.-xneans and fuzzy C-1neans clustering

42 algoritluns. The ·algorithn1s were applied to the extracted features. The database contains

75 iinages, including 41 healthy (normal) iinages and 34 diseased (abnor:rnal) irnagcs. using the feature extraction techniques in the previous sections, each hnage in the database is represented with a feature vector of 36 features. Since there is a considerable difference in the range of the values for different features, the features are norrnalized prior to further analysis. In both classification scenarios ( using k-1neans and fuzzy C-nwans) the rmtnbcr of clusters is necd.ed to be known beforehand. Since in this work we aim to detect the existence of abnonnalities in the irnages, and not detennine the type of abnonnalities, the nun1bcr of dusters is defined to be 2 to represent norn1al and abnorn1al irnages. The Fuzzy C-rneans algorithn1 calculates 1 for each hnage X, the degree of n1en1bership for the healthy duster and
the diseased duster. Then the hna.ges are separated into two clusters based on the criterion of

n1a..xinuun rnembership. For the k-nwans algorithrn, it is the sarne rnethod; the same rnatrix of extracted features F is used. The a.lgoritlun calculates the squared Euclidian distance between each row of F (which represents one srnall bowel irnage) and the centroid. The centroids are then recalculated and these steps arc repeated until the a.lgoritlnn converges. The result of the two algorithrns is a 75 x 1 n1atrix. Each row of the n1a.trix corresponds to one im.age in the dataset and indicates ·whether the in1age belongs to group one or group two. Finally it is the physician who labels one group as the healthy bowels and the other as
the diseased bowels.

The eHiciency of the algoritlnn is provided in the confusion matrix (or the matching rna.trix) given in Table 3.2. Table 3.1 shows the definition of the confusion matrix where the specificity and sensitivity are defined as:
Sens'ii:ivity = · .

1Vurnber of cor-rect positive predictions · 1btal n·umber· of abnor'm al cases

~

----

TP TP + F N

(3.12) (3.13) (3.14)

iVurnber- of correct negative p·red-ict·ions T IV SrJec-ificity = . = - -- Total n:wnber of nor·mal cases T JV + F P . . !vr·wmbeT of corrY::ctly classified i·mages r>.f f'tczenc:y = Total numbeT of iTnages

As it can be seen in Table 3.2 that an accuracy rate of 76% is achieved which is a rather satisfactory result for an unsupervised classifier.The results of other da.ssifica.tion rnethods

4:3 (k-n1cans and SOTivi) using energy and h01nogeneity features are provided in Tables 3.3 and
3.4 for con1parison. Another n1easure to evaluate the performance of the classification 1ucthod is the Receiver Operating Characteristic (ROC) curve. The ROC curve represents the fraction of true positive (TP) vs the false positive (FP). The TP corresponds to the sensitivity and is the proportion of diseased bowel hnages cla..':{sified as abnorn1al while the FP represents the portion of nonnal hnages classified as abnormal. An ideal classifier would yield a point in the upper left corner or coordinate (0,1) of the ROC space, where all ilnagcs have been correctly classified. This point represents 100% sensitivity (no false negatives) and 100% specificity (no false positives). The classification accuracy is also rneasured by calculating the area under the ROC curve. An area of 1 corresponds to perfect classification, whereas an inefficient cla.-.;sification is represented by a horizontal straight line going from the point (0, 0) to the point (1 , 1). In order to have an efficient classifier, the curve ha..o;; to be above this line. The ROC curve for the unsupervised classification .techniques used in this ·wodc is given in Figure 3.6, where the area under the ROC curve was calculated to be 0.76. Table 3.5 shows the results of using different feature sets along with supervised and unsupervised classification 1nethods. In the supervised classifi.ca.tion, LDA has been used in conjunction with leave one out rnethod (LOOIV!) to cmnbat the proble1n of small sarnple size. In the unsupervised colmnn, the results of applying fuzzy C-1neans is provided. Both techniques arc used on the same database of 75 ilnagcs (including 41 norrnal and 34 abnorn1al im.agcs). As it can be seen fron1 the table, the extracted feature for a supervised classifier are not necessarily optimal for an unsupervised classifier. However, a feature set that yields a good results with an unsupervised classifier n1ay naturally lead to better results if a supervised cla..c;sifier is used. This sho\vs how an unsupervised classification can be used as a first step in dassifica,tion to select the naturally rnost discrin1ina.nt features. l<'tmn Table 3.5 it can be observed that using the SIDvVT along with cross co-occurrence 1natrices in the RGB color spa.ce returns an accuracy rate of 52% for the k-1neans or fuzzy C-n1cru1s clustering while a relatively high accuracy rate is achieved using a supervised classifier. Nevertheless, using

44
the feature set that is extracted in the Lab colQr space for unsupervised classification results in an accuracy of 76%. In an attcrnpt to test the 1nethods with more iinages, all the in1ages were rotated by 180 degrees to obtain a database of 150 images. The classiflcation accuracy for the enlarged database is 70.7% which shows the 1nethod could be applicable to larger databases.

o.s
0.8
~

"

_______.--""~------

w 0.7
0.6
0.5
0.4 /

~~-

..,...,..

.,.,.,--------

;

/
;··

~
0

~ ~ ~<-

0.3

1
/,/

I

02

0.1

/ /
FALSE POSITI'<IE RA1E

0 o~-O~j--~0.~2--0~.3 --~ Q~4--o~s--~o.s~~--o~.7--~o.s---o~.9~

Figure 3.6: The Receiver Operating Characteristics curve with an area of 0. 76

Future work
Although ·wavelets are shown to be effective as texture feature extraction tools, the adaptation of other texture descriptors for the Inedical hnages is growing. Arnong the new textural features, textons have shown promising results in extracting texture features for classification. Textons are used to describe the fundarnental n1iero-structure elcnwnts in natural
hna.ges. The appearance of the textons has a root in the psychological study of the texture

recognition process in hurnan. The theory of textons was first proposed by .Jluesz [65] to explain the ''preattentive discrirnination" of the texture pairs. To discuss Julesz pioneering

45

Predietive positive

Predicted negative

Actually positive

TP(true positive)

FN (false negative)

Actually negative

FP(false positive)

TN(true negative) . .
·

Table 3.1: The definition of confusion matrix

Normal

Abnonnal

Norn1a.l

32

10

Abnormal

8

25

Table 3.2: Classification results for the fuzzy C-:means elassifier

work on textons, '~'e need to describe these t-vvo concepts: [66)

First order statistics refers to the probability of oecurrcnee of a gray value at a randmn
location in an in1age. These statistical nwasures can be calculated fron1 the histogran1 of gray level intensity of the im.age. First order statistics depen.d only on individual pixel values and not on the co-occurrence of the neighbor pixels. The rnean gray level value in an inmgc is an exan1plc of first order characteristics.

Second order statistics rneasure the likelihood of gray level intensities occurring separated with a. displaeCincnt vector d where the lengt.h and orienta.tion of the vector dis random.

46

Nonnal

Abnorn1al

Nonnal

29

10

Abnonnal

11

23

Table 3.3: Cla.'lsification . results for the k-means classifier

Norn1al

Abnonna.l

Norrnal

25

16

Abnornutl

8

26

Table 3.4: Classification results for the SOTI\rl classifier

These two attributes were used by .Julesz to detennine whether two textures are prcattentively discrirninable. The theory of textons was proposed to address this problern. Textons can be considered as visual events in an irnage such as collinearity, t.ern1ination and closure. Using the theory of textons, the two different textures in Figure 3. 7 can be described as followso The two regions in Figure 3. 7(a) have identical second order statistics and the nmnbcr of tenninations (Le tcxton infonnation) in both the upper ·and lower tegions is the sarncl therefore the htnnan visual
syst(~rn

is not able to discrirninate the two textures preat-

tentivclyo On the other hand, in Figure 3.7(b), the munbcr of tenninations in the upper and lower region is different (three in the upper half and four in the lower half). Because of

the difference in this texton, the two textures are d.iserhninable.

47

,L
.~:i

/ ·:~

;:::;;

;:::: ru ru ~::1 ;:::.' nJ ru :::'? ;::~' ;:;] ,::::=

~~:l

n,; ;u ;:::

u Ul

:;_;

e mU '

(a)

Figure 3. 7: Texture pairs with identical second-order statistics. (a.) The upper half and lower half contain the same textons. The visual system can not discriminate the different textures without careful scrutiny. (b) The upper region contains textons different from the lower· region. Humans

can differentiate the two textures effortlessly.

48 The application of tcxtons in the area of Ultxlica1 irnage processing is growing recently.

in [67] H;..ums ct. a.l. extract texture 1nicro-edges and textons between these rnicro edges to diagnose leukmnic Inaligna.ncy in samples of stained blood eells. In [68] a. texture feature extraction based on textons is used to classify the breast density pattern to dctcnninc the breast cancer risk. In [69] Tuzcl et al. use texton histognuns to distinguish among hernatology cases directly frorn rnicroscopic spcdrnens. The in1ages contain nonnal in1ages and for groups of four different hcn1atologic rnalignancies. Initially, the basic texture elcrnents (textons) for the nuclei and cytoplasn1 are learned, the cells are represented through texton histogran1s and finally a SVM classifier is applied to the extracted features. The work proposed by Adjeroh et a.l. in [70] is one cxarn.ple of using the textons for segn1entation of retinal hnagcs. The application of tcxtons in the area of nwdica.l iinage analysis for extracting texture infonnation appears to be increasing among the researchers and the results are prornising. Hence., as the future work a new set of features based on textons can be developed for the s1nall bowel i:mages to extract the texture infonnation and in1prove the accuracy.

4:9

Color space

Extracted featu.res

Unsupervised classification

Supervised classification

RGB

Contrast Energy Ho1nogeneity Entropy

52%

94.

Lab

Contrast Energy Hon1ogeneity Entropy

Lab+RGB

Contrast Energy Homogeneity Entropy

56%

79%

Lab

Energy Hornogeneity (normalized features)

76%

76%

Lab

Energy (third subband, normalized features)

72%

84%

RGB

Energy Honwgeneity

61%

78%

Lab+ RG B

Energy Hon10geneity

65%

88%

Table 3.5: Comparison of the result.s of l.HL"mpervised classification rnethod with supervised classification for different featu.re sets and different color spaces.

Chapter 4 Unsupervised Learning in Hearing Aids ignal Analysis
4*1 A·udio classificatio11 for hearing a.ids
PEECH and envirmunental audio signals are irnportant sources of infonna.tion in our everyday corn1nunication, and can provide .information about the location or envi.ron-

S

nwnt of the captured scene or event. Having approxiinately 10% of the world population suffering fron1 son1e sort of hearing loss, one of the ilnportant applications of audio classification is in hearing aids for hearing irnpaired people. U scrs of hca.ring aids are forced to listen under a variety of noise conditions and in rnost ca..,es sirnple an1plification cannot help hearing-impaired listeners. Such devices a.Inplify the noise as well as the desired signal. Consequently, nurncrous signal enhancernent algorithn1s have been proposed for digital hearing aids. To overconw this problem, the hearing aid should be able to detect the audio classes which the incorning signals belong to, and then change the hea.ring aid parmneters
aceordinglyo The first step to achieve this goal is the ability to quickly and correctly classify

the audio signals in the environment. There is a growing body of evidence that different hearing aid characteristics tha.t can operate efficiently under different listening conditions are desirable [71). In a survey obtained
by Kochkin [:32) from. 2323 hearing a.id users it was observed that less than one third of the

hearing aid users were satisfied with their hearing aid if the device worked properly in

50

51
only three or fewer environments while over 91% of the users were satisfied if the hearing aid worked wherever it was needed. Thus if the hearing aid can be a.utmna.tically adjusted according to the listening conditions substantially better user satisfaction would be expected.

4*2

Audio signal classification

Audio signal classification is one the tasks that humans perfo.nn etiortlessly all the tin1e. Differentiating the voice of a singer frorn the nmsie, understanding heavily accented speech, recognizing a voice on the telephone, telling the difference between a helicopter sound and a car sound, discrhninating the speakers voice from the background noise are sorne of the auditory tasks that we do every day without even considering then1. However, duplicating this capability on machines takes a.n intensive effort. In the area. of n1achine learning and artificial intelligence, analysis and discri1nination of the audio signals is one the research areas that has been active for a long time and is not con1pletcly solved yet. There is a wide range of applications for the classification of audio signals. Speech processing for security applications and human computer interaction, nulltirncdia data rnanagmnent and distribution, security, bion1etrics a.nd bioaeoustics are smne of the applications of audio signals classification [:3:3].

Taxonon1y of audio signals
Before discussing different existing classification and analyzing techniques, it is irnportant to define a taxonmny of auditory signals. Audio signals can be sorted into classes from different viewpoints. However, the taxonmny presented here is based on the origin of the signal. Figure 4.1 shows the taxonomy of the audio classes used in this work as a reference. The audio signals used in this work can be divided into two Inain groups, signals that have a natural origin and those which are htunan 1nadc or artificial sounds. Natural signals are then subdivided into human signals (or speeeh) 1 which in turn consists of 1nalc and fcrnale speakers and non-lnunan sounds, whic:h include bird, anilnal and insects. On the other hand, lunua:n 1nade sounds consist of two rnain categories: 1nachine sounds, which in turn are divided into helicopter and
aircraft~

and musical instn1n1ents such as piano? flute and

52
drun1. Other taxonomies with higher resolution can be obtained in rnany ways for exarnple
by subdividing the hurnan speech into pathological or norrnal or by dividinginusical sounds

into difl'erent nmsical genres such as pop, rock, etc. In this work however, we confine our attention to the taxonon1y given in Figure 4.1.
t"·'-=''·o"~·w>·<···.-.-="~ ···~~«<.>~» .>i

i., ,..'\udio Sig·nals ! i
f .
~

~~--··-]·-. .......:!

I M.<~.r.Nm:w'l J
r~ir~l<1n~l [ ______ ]

_L~"ll
[ He1k:optel' ·· ···· · ···. .... .. J

Figure 4.1: Taxonomy of audio signals used in this work

Audio signal classification
Audio signal classification consists of extracting physical and perceptual features frorn an audio signal (or one seg1ncnt of the signal) and categorizing the signal into one the given audio classes. Audio classification in general is a wide area of research and a large amount

of research has been done on it in the last decade.

~1ost

of the resca,rch works in the area
analysis~

can be divided into three main categories: speech, n1usic and audio scene
these topics will be discussed in more detail below:

Each of

Speech analysis
A considerable part of research in the area of audio signals has been devoted to speech analysis and classification. Speech analysis is a wide area of research itself. The following
areas are some of the rnajor branches of the speech analysis in the literature.

5:3
Speech recognition: Speech recognition is one of the oldest and the rnost fundarnental

speech classification problcn1s. The goal is to convert the words frmn the hurna.n speech into a readable text. Speech recognition har; a wide range of applications in the areas such as health care, n1ilitary, security, telephony and enabling people with disabilities. References [72) [73] and [74] are son1e of the cmnprehensive works in the area of speech processing history and proposed rnethods and solutions.
Pathological speech analysis: Can be used for recognition of selected types of vocal tract

pathologies [75]. Various pathological conditicms affect the vocal functions, which result in speech disorders. The aim of pathological speech analysis is to assess the speech disorders by using acoustic characteristics of the speech. It can be also helpful in n1onitoring the progress of the patient over the course of therapy [33). Further n1ore,
it is valuable to provide the physician with a qum1tita.tive guideline for a dcfonnation

degree assessment of speech signal [76].

Speaker recognition: Speaker recognition (or smnetimes called speaker verification [77])
is the identification or verification a user based on the characteristics of their voice.

Cmnpared to the speech recognition problem, where the main goal is to detern1inc what word is uttered, the goal is to find out who the speaker is. Son1c of the applications of spea.ker recognition can be speaker authentication, identification or bion1ctrics.
Music

As the an1ount of n1ultinwdia and n1usic files is growing every day, automatic extraction
of 1nusic information is gaining n1ore irnportance as a way to structure and organize the increasingly large nmnbers of 1nusic files available digitally on the Web. Today a large portion of the audio classifi.cation literature is related to n1usic and n1usic infonnation retrieval. However, rrwst of the research in this area, fall within one of these categories:

]\tlusic content analysis: vVith the creation of huge music databases, the dem.and for fast
and reliable tools for content analysis and description is growing. These analysis tools

54 ean be used for searches, content queries, and interactive access. An1ongst all possible

descriptors, 1nusic genres are crucial since they have been widely used for years to organize music ea.talogues, libraries, and rnusic stores [·:11]. A rnusical genre is typically characterized by the .cmnmon attributes related to
instrurnentation~

rhythn1ie

structure, and harrnonic content of the rnusic. The music genre classification rnaps a taxonomy of genres, i.e., a hierarchical set of categories onto a rnusic collection. Sirnilar to any other classification problcrn, a set of features is used to decide on the rnusic genre. Table 4.1 shows a sunun.ary of the features being used in 1nusic content retrieval today [41]. As for the classification, a number of supervised and unsupervised rnethods have been proposed. Shao et a.l. [39] usc agglmnerativc hierarchical clustering on their rnusic dataset. In the work by Rauber et al. ['10] the growing hierarchical self-organizing rnap is applied to cluster data and organize thern on a two-dirnensional space. References [78] and [79] arc exa.1nples of application of supervised dassifiers where K-nearest neighbor are used in the context of genre classification. The hidden rnarkov n1odels (HIVI:~~Is) have been used in [80] and [81]. In [82] vVest and Cox show the applications of linear discrhninant analysis in genre classification of audio content. In [83] support vector machines are used for the classification purpose and finally [84) is an instance of the usc of artificial neural networks. Musical instrument recognition: :Niusical instrument recognition is another aspect of rnusic inforrna.tion retrieval. Such a capability rnay be extremely helpful in the frarnework of autmnatic rnusical transcription systems as \vell
~1..c;

in content-based search

applications. One of the practical applications of rnuslcal instnunent recognition is autornatic rnusic transcription. A typical task of classification of musical instnuncnts consists of three phases [85] the first step is the preprocessing, which can be also referred to as pitch extraction. The next stage is the extraction of frequency infonnation, funda1nental frequencies and hannonics. These .infonnation will then be used in the third stage which is the pattern recognition and classification stage. Some of the works use the ten1poral inforrnation as well [86]. References [87] [88] and [89] are son1e of the

55
other existing techniques in the literature on the recognition of musical instrun1ents.

Speech/music discrin1ination: Another aspect of content based audio classification that
has attracted many researchers is discrin1ination of hun1an speech frmn the music. In this process, smnetilnes we are rnore interested in extracting the speech inforrnation fonn the background 1nusic, for exa1nple for the purpose of perforn1ing autmnatic speech recognition on the soundtrack data. On the other hand, smnetirnes the music content is of
1non:~

hnportancc e.g. rnany listeners are n1ore interested in the rnusic

on broadcast radio rather than the conunercial and talk progran1ming. The works by
Hawley et a.l. [90] and Saunders et al. [91) are some of the previous works on this topic in the literature. Several feature sets have also been suggested for this purpose. In

[92] a cornparison of the proposed feature sets for speech/rnusic discrimination (such
as cepstral coefficients, amplitude features and pitch features) is presented.

Tin1bre texture n1odel: rnodel of features over texture window:

1v1elody/Harmony
pitch function: n1easure of the energy in function of nrusic notes

Rhytlun periodicity function: m.easure of the periodicities of features

1) Sirnple modeling with low order statistics 2) rnodeling with auto regressive m.odel 3) n1odeling with distribution estiination algoritluns( e.g. EI'vl estirnation of a G 1\!IIVI of fra1ue)

1) Unfolded function: describes pitch content and pitch range 2) folded function: describes hannonic content

1) Te·mpo: periodicities typically in the range 0.31 1 58 (i.e., 20040 BPI\1) 2) rnusical pattern: pcriodicites between 2 and 6 s (corresponding to the length of one or Inore rneasure bar)

Table 4.1: Typical features used for music content retriev·al

56

Audio scene classification Audio scene a.na1ysis is the process of extracting infonnation about the environment based on the characteristics of the received signal, and has nurncrous applications in nmltiincdia processing. Hence, cornpared to the previously rnentioned classification categories (rnusic and speech) audio scene analysis is a more general and cmnprehensivc task. The idea of audio scene analysis was first proposed by Brcgrna11 in [93], which is the cornerstone of this area. In his work Bregrnan presented a new perspective in human sound perception. The concept of audio scene analysis con1es frmn the way that htunan brain works to use the sounds to build a picture from the surrounding cnvironrnent, whieh is also called an auditory scene. There are ntunerous applications for audio scene analysis. Arnongst all, one of the Inost popular applications of audio scene ana.lysis is in the developrnent of smart hearing aids, which will be discussed in n1ore details in the future sections.

4o2o3

Review of the previous works

Iviany rnethods have been proposed in the area of audio signal classification with the application to hearing aids. In [71] Kates proposes the selection of processing algoritlnn based on the audio infonnation from. the scene. N ordqvist and Leijon [34] introduced a hidden IVIarkov n1odel (HJ\!Il\i1) based classifier for hearing aids using features derived fronr ccpstral coefficients. In the work done by Buchler et. al [35] a va.riety of rna,chine lea.rning techniques (k-n10ans, histograrn driven Bayes classifiers, n1ultilaycr perceptrons, and Hl\fMs) were tested and the ergodic Hl\1!\,1s were shown to outperform the rest of the xnethods. Audio content analysis at IVIicrosoft research conunonly en1ploys Gaussian rnixture nwdels (Gl\Il\/1)[36], k nearest neighborhood (K-NN)[37] and support vector rnachine (SV:rvi)[:38] for audio classification. Other popular classifiers for audio classification include linear d.iscrirninant analysis (LDA) [33], hidden l\!Iarkov 1nodels (HlVII\11)[:39} and artificial neural networks (ANN)[94]. \Vhile there is a large anwunt of research in the literature on the application of supervised classifiers, the use of unsupervised classifiers for audio classification is relatively unexplored.

57 Clustering (or unsupervised) approaches are nwst beneficial in cases where precise rnanual

labeling of the data is tiine consurning and laborious or when the feature characteristics rn.ight change over time. As rnentioned earlier, the hearing aid is expected to operate in a wide range of audio envirmuncnts. Therefore, the nurnber of audio classes and nature of the classes in the received audio signa.l is not predictable. In this case, a clustering approach can be beneficial to discover different audio cla.r;ses that exist in the received audio signal. This step ca.n be followed by supervision. to select and arnplifythe desired audio class. In addition, using a clustering nwthod has the ad vantage of avoiding the constraints of a fixed tax:onmny, which may suffer frorn ambiguities and inconsistencies. In addition, considering the vc.u:iety of the audio signals, sorne of the signals rnay sirnply not fit within a given category [41]. The usc of a clustering technique makes it possible to take into account the overlap that n1ight exist between different classes. In [39), Sha.o et al. use
~:u1

aggloinerative hierarchical

clustering on the audio data. set for music genre classification. Rauber ct al. [40] use the growing hierarchical self organizing 1nap to create a. 2-D output for visual representation of the n1usic data set. The classification method proposed in this work is based on the self organizing tree 1na.ps, whieh was explained in Chapter 2, followed by a fuzzy· labeling of the data. approach allows for extraction of underlying characteristics of the data and then supervised labeling is used to interpret the discovered clusters. The proposed 1nethods can also be discussed front the point of feature extraction. IVIost of the existing nwthod extract either temporal or spectral features for classification. A wide range of feature sets have been proposed for this purpose. In [92) a comparison of different feature sets proposed for audio classification is given. Smne of the suggested features include signal energy, pitch, zero crossing ra.te [92] [91], Entropy rnodulation [95], 4 Hz rnodulation energy, percentage of low-energy fraJnes, spectral rolloff point, spectral centroid, rnean frequency, ccpstral coefficients [96), [97} and high and low frequency slopes [71]. All the rnent;ioned features arc extracted only fron1 tirne or frequency dornain; however, the ternporal or spectral features are not enough for representation and localization of nonstationary aspects of audio signals, such as trends, discontinuities, and repeated patterns.

58

Thus the features used in this work are based on joint tilne and frequency analysis of the signals, which is effective for revealing non-stationa.ry characteristics of audio signals.

\Vork

Classification technique

Nordquist et al. [:34) Behler et al. [35]

HlVIl\11
k-means, J\:ILP

Delta features fron1 cepstral coefficients Tonality, width, pitch variance, 1neasures of tiine offset
4Hz rnodulation, low energy frarnes, spectral ·roll off, spectral centroid, cepstral residual, pulse n1etric, spectral flux, zero crossing rate, variance of the low band energy

hayes classifier, HIV111.
Abu-El-Quran et al. [36) Adaptive thresholding of feature values

Lu et al. [37)

K-NN

High zero crossing ratio, low short tirne energy ratio, spectrun1 flmc, LSP divergence,band periodicity, noise f.ra1ne ratio Total power, subband powers, brightness bandwidth, pitch, 1nel frequency cepstral coefficients ( l\!1FCC)
l\;1FCC, linear prediction coeiiidents derived frmn cepstrum coefficients, delta and acceleration
!vlean frequency, high and low frequency slopes, envelope 1nodulat.ion

Guo et al. [38]

SVl\ti

Shao et al. [39]

HfviiYI

J:.reen1an et al. [94]

ANN

Table 4.2: Summary of the feature extraction and classification techniques used in the literature for audio classification

59
Figure 4.2 shows the block dia.gran1 of the ilnplernented syst.mn, where the blue lines show the
flow of the train data and. the red lines show thef1ow of the test data. In the training phase
ea,ch input audio seg1nent ..Y is passed through the adaptive timc-ii·cquency decomposition

(TFD) block. The TFD matrix V is then d.ecmuposcd b.Y the use of Non-negative n1atrix
Factorization (NivlF) nwthods into base and coefficient 1natriccs ~V and l{. Then the features a.re processed and the desired nurnbcr of featu.res arc extracted frcnn each ba..'3e vector and its corresponding coeffident vector to fonn the feature set

f. Once this

procedure is run for

all the segments in the training set, the SOTN1 clustering technique is applied to the data
to discover the clusters aJJd cornputer the du.ster centers C. Then a n1cn1bership degree is

calculated for each duster, a, which will be used for the labeling of the test data. Eaeh segment in the test da.taset, after passing through the feature extraction block, is fed to the data labeling block, where the deeision is n1a.dc about whieh class the segment. belongs to. All of these blocks will be described in Inore details in the future Sections.

Input s~eg{ ment ·. ·· · A ··· · daptive

.... ~-- ~ -=

~ --~

TFD ..- ·· · ··

Figure 4.2: Block diagram of the feature extraction and classification

Feature extractio11

60

The features used in this \Vork arc captured by applying the 1natching pursuit algorit.lun on
the signals followed by the non-negative rua.trix decomposition. The concept of these two algorithms are briefiy described in Sections 4.3.1 and 4.3.2. Then a feature set is created fron1 the results of these two algoritluns which is described in Section 4.8.3.

Matching pursuit TFD
In every day conversations, we connnunieate a wide range of ideas with precision. By adding or ornitting a few words, we
c~m

cornmunica.tc subtle differences in close meanings. This is

possible due to the fact .that. natural hmnan languages have large vocabularies that include words with close rnea.nings. In the area of infonna.tion proeessing, a low level represcnta,...

tion of the signal nmst include infonnation about distinct properties and minor differences
si1nultancously. However, nwst of the signals we deal with in real life applications (such as audio signa.ls) are cmnplex sigrw.ls that consist of a wide scope of patterns. Precise representation of these signals with few basic functions is not an easy task [98]. This is the nwtivation behind the idea of projection of the signals onto large and redundant dictionaries
of wavefonns, which was proposed by :M allat et al. in [98]. According to this work, linear

transfonns (such as Fourier and wavelet) do not have the flexibility required for representing
wide range of signals. Fourier transfonn elin1inates temporal properties and hence provides

a poor representation of the signals that are well localized in tiinc. vVavelet bases also arc not opthnal for those signals whose Fourier transforn1 has a narrow high frequency support. Hence, decornposing a signal on such basis, is like writing a text using a small vocabulary. Although it might be possible to express the idea, it takes extra effort and extra. words to describe the unavailable words. Flexible deeornpositions arc particularly irnportant for those signals whose local ternporal and spectral properties vary widely.

In the rnatching pursuit algorithn1, the signal is decomposed into a. linear expansion of
waveforms . These waveforms belong to a redundant dictionary and are selected in order to best rnatch the signal structure. These wavefonns are called tirne-frequency atmns. for

61 example, impulses need to be decon1posed using atorns that are well concentrated in tiine, while spectral lines are better represented by waveforms which have a narrow frequency bandwidth. Although the n1atching pursuit decornposition is a nonlinear algorithm, it maintains the energy conservation property like an orthogonal expansion. When using a dictionary of tilne-frequency atorns, applying the rnatching pursuit algorithrn yields an adaptive tiine-frequency transfornl. It decon1poses the function f(t) into a sum of con1plex time-frequency atoms that best match its residues. A general fmnily of timefrequency atoms can be obtained by scaling, translating and modulating a single window function g(t). By denoting'"'(= (s, tl, ~),the function g(t) can be defined
t - u- if,t , g1 (t ) - 1I v ~r. sg ( --)e
8

&'3

(4.1)

\rVhere s > 0 is the scale and

~

and u represent frequency modulation and translation

respectively. The Fourier transfonn of g-y(t) can be written as [99]

.9(w)

=

y'Sg(s(w-

e))e-i(w-f.)u

(4.2)

In this work, a dictionary of Gabor time-frequency atoms has been used. The discrete Gabor time-frequency atom can be written as

(4.3)
where
= K-8

(n ) 9s ·

r;, L

~ ?. 1; 4 -1r(n- pJV) 2
....,

e

.

(4.4)

V S p=O

S

The constant J(., is used for normalizing the function g8 , p (0:::; p < N) is the tin1e shift,

¢ (0 :::; ¢ < 2n) the phase shift, and 0 :::; k < N. The decon1position of the signal f can be
written as a linear expansion of the signal over a set of atorns selected frorn the dictionary. In order to find the atorns that best match the structure of the signal a successive approximation

of
n

f with orthogonal projections on the elernents of the dictionary is performed [99]. After iterations, the decon1position of the signal f is given by:
f )'n -1

62

L....li=O

(Rif ' 9-ri ) 9~yi

+ Rif,

(4.5)

where Rif is the decon1position residue after n iterations and (, ) denotes the inner product of the two functions. At each stage of iteration, the algorithm selects the atmn
g.h for which the inner product (Ri f, 9-ri) is rnaximized [99). The energy distribution of the

decomposition can be written as
n-1

E f (t' w) =

L

I(

Ri f' 9~ti (t' w)) 1 \V 9-ri ( t' w)
2

(4.6)

·i=O

where l'Vg1i(t,w) is the Wigner distribution of the aton1 9Art(-t,w) which does not include cross terrns [99] .

4e3.2

Non-negative matrix factorization

Non.. negative rnatrix factorization (NMF) is a decmnposition technique proposed by Lee and Seung in [100]. The interpretation of Nlv1F for the application of statistical analysis of the multivariate data can be described as follows: Assume V is an m x n non-negative data rnatrix, where n is the dimension of the data and rn. is the number of vectors or the nun1ber of san1ples in the data set. The goal is to find non-negative matrix factors l¥mxT and H.,.xn to approximate the n1atrix l/, such that
(4.7)

and also

v ~ VVh,

(4.8)

where v and h are the corresponding colun1ns of V and H respectively. This rneans each data vector v can be approximated by a linear cornbination of the columns of

w· weighted

63
by the con1ponents of h. Therefore lV can be considered as a set of basis vectors that are optimized for the linear approxin1ation of the data in V. Usually r is selected to be smaller than nor m, so the result is a compressed version of the original matrix, V and the data vectors can be represented using fewer basis vectors. On the other hand, in order to obtain a good approxhnation, the basis vectors should discover the structure that is latent in the data. In this work, the NJ\;IF technique has been performed on the tirne-frequency 1natrix. Therefore n is the length of the signal, rn is the frequency resolution of the constructed tin1e-frequency matrix, and r is the decomposition order. After decomposition, lV and H carry spectral and ten1poral characteristics of the original matrix respectively. HI contains spectral structures and H contains the corresponding location of each spectral structure in the original matrix. The problem of finding TV and H can be considered as a minin1ization of the function

f = IIV- vVHII2

(4.9)

There is a variety of strategies in the literature to find tV and H [101][102]. In this work a gradient-based 1nethod proposed by Lin in [103], which uses bond-constrained optin1ization technique. The standard form of bound-constrained optimization problem can be expressed as [103]: n1in f (::t)

:rER

subject to l.i :::; xi :::; ·ui, i = 1, ... , n

(4.10) (4.11)

(4.12)
where
Xi

P[;J;]

=
{

ni

li

for for for

li < ~ri < ui
xi
Xi

2: ui :S li

(4.13)

In [103] this technique is applied to the NJ\1F problen1. This n1ethod is computationally efficient and offers better convergence properties than the standard approach [10:3].

64

493.3

Feature selection

As shown in Figure 4.2, once the TFD rnatrix (V) is decmnposed into base and coefficient n1atrices (T!V and H), a feature set is extracted from each base vector and its corresponding coefficient vector. The features are derived from coefficient vectors, base vectors, and frorn
1\tiP decomposition. A brief description of the features used in this work is provided here: 1. Sparsity: The sparsity feature is calculated for each coefficient vector,{hih xN, as
8
=
hi

VJV- (I:~=l hi(n))/JL:;:=l hr

JN - 1

(4.14)

The value of this feature is 1, if and only if hi contains a single non-zero con1ponent, and is zero if and only if the components are equal. 2. Sum of derivatives: This feature is calculated on the base vector and represents discontinuities and abrupt changes in the signal. The equation for derivation of this feature is given by
N-1

Dhi

=

L

h~(n) ,

2

(4.15)

n=l

where
(4.16)

n = 1, ... ,N -1

(4.17)

The value of this feature is a measure of discontinuities. If there are discontinuities in the coefficient vector, the value is large, otherwise it is sn1all. 3. Moments: The first mmnent of the base and coefficient vectors are also extracted. The spectral and te1nporal moments, 1\10wi and Jv!Ohi, are obtained using the following equations
A!Ow. i=

I: rnwi(m)
rn=1

M

(4.18)

l\10hi =

I: nhi(n)
n=l

N

(4.19)

65 where hi and wi are the base and coefficient vectors and A1 is the frequency resolution

of the TFD. 4. Sparsity I: In addition to the sparsity of the coefficient vectors, the sparsity of the base vectors is also extracted. This feature represents the noisy structure of the signal and is calculated as

s
Wi

=

VM - (I:~~=l wi(n))/VL..~;,=l wr

VM -1

(4.20)

5. Sparsity II: This feature is defined as the nu1nber of samples whose value is smaller than a threshold
E

to the total nun1ber of samples in the base vector:

SP. =
Wl

Wi

A1 '

<

E

(4.21)

where wi <

E

is the number of base sa1nples less than a small threshold and NI is the

total nu1nber of smnples in each coefficient vector. This function is unity if and only if all the components in wi are greater than the threshold, and is zero if and only if all the sa1nples are less than the threshold. 6. Periodicity: \iVhile the previous feature measures the scattering of the components in frequency, we still need another feature to represent the presence of harn1onicity of the energy in frequency. For each base vector, the Fourier transform of the vector is calculated as
H1i(v)
=

I

L

l\J

e-j rr~;w wi(1n) j
2

(4.22)

1n=l

where AI is the length of the base vector, and H'i(v) is the Fourier transfonn of the base vector wi . Next a second Fourier transform is perfonned on the base vector to obtain
Hli ( K)

as

vVi(K)
Finally we sum up all the values of

=I L e-j~~;;l¥i(v)l
v=l
I~V(K}I

M/2

(4.23)

for K > rn 0, where 1n0 is a small number.
(4.24)

4 Pw,. = EA~ K-rno j Hl·(K)·i ~

66 The value of Pwi is large for bases whose cornponents show strong periodic behavior,
such as vowels in speech. However, for non-periodic sounds such as aircraft, the feature has lower values. 7. Su1n of derivatives: This feature is calculated on the coefficient vectors and captures discontinuities and abrupt changes in the signal.
M-1

Dw;
where
w~(rn)

=

L...
1n=l

'\.---..

wiI ( m.)2

(4.25)

= wi(m, + 1)- wi(rn)rn = 1, ... , l'vf- 1
rn = 1, ... , AJ- 1

(4.26) (4.27)

where

w~

is the first derivative of the coefficient vector. The value of this feature is

large if the coefficient vector contains discontinuities. 8. Projection features: As shown in Eq 4.5, I\1P decomposition projects the signal onto a set of time-frequency atoms. The an1ount of signal energy that is projected in each iteration depends on the structure of the signal. Signals with coherent structures need less nun1ber of iterations, while signals with a non-coherent structure tend to take rnore iterations to get decomposed. This property is used as feature to discrin1inate coherent audio signals form non-coherent signals. To extract this class of features, first we calculate the difference in the projection energy between iteration i and i

+ 1:
(4.28)

i = 0, ... , I - 2

(4.29)

where
a~ri

a

=

Total energy of the decomposed signal

(4.30)

is the ratio of the projection energy at each iteration. Next, we define Li as the sun1 of the energy differences:

Li

=

do

+ d1 + ... + di

(4.31)

i=O, ... ,I~2

67 (4.32)

Li keeps the trend of the energy coefficients (ai) but it is normalized and it is independent of the signal's energy. Finally, norn1alized coefficients (Li) are used to calculate 1\!IP feature:
J-2

A1P =ELi
i=O

(4.33)

4.4
4.4.1

Classification and results
classification methodology

The classification rnethod used in this work is based on the SOTJ\;1 clustering algorithm. The proposed method, which is a fusion of supervised and unsupervised classification, consists of two stages. In the first stage the SOT1V1 clustering algoritlun is applied to the training dataset. Since the data is represented to the SOTM in a randon1 n1anner, the fonnation of the clusters might be slightly different for each run. In fact, sorne of the discovered clusters include one or very lilnited nmnber of sarnples. Therefore, those clusters in which the number of smnples is smaller than a threshold will be elin1inated. The value of this threshold in this work was adjusted to be 5% of the total number of samples in the train data set. Next a mernbership matrix, 1VLmxn, is calculated based on the distribution of each class in different clusters, where rn is the nun1ber of clusters and n is the nu1nber of classes. Each entry in the rnembership rnatrix , rniJ, (which we call men1bership coefficient) indicates the probability of a vector in the cluster ·i to belong to the
mu Af= [
rn21

)th

class.

m~,

(4.34)

where

(4.35)

68
These coefficients will be used in the calculation of the fuzzy rnembership degree for each of the test vectors. Each segn1ent is represented using 15 feature vectors. By using this approach less weight is associated with the vectors that are in the overlap regions. In the second stage, each of the feature vectors representing a test signal is assigned to one of the cluster centers found in the previous stage based ·on the n1inimum Euclidean distance criterion. For each test signal, the scatter vector S is defined as
(4.36)

where si is the nun1ber of the representing vectors for a test signal that fall within the

ith

cluster and C is the nurnber of clusters. Finally the probability of a signal belonging to the
)th

class is calculated according to the distribution of its representing feature vectors in

different clusters and can be written as:

<I>(j)
4.4~2

=

S.Af(j)

(4.37)

Results

The audio data set used in this work consists of 192 signals of about :3s duration, with a sarnpling rate of 22.05 KHz and a resolution of 16 bits per sample. Table 4.3 shows different sound classes in the data set and the nurnber of signals in each class.

Airplane

Anirnal

Bird

Drum

Fen1ale

Flute

Helicopter

Insect

I\!Iale

Piano

20

20

20

20

20

15

17

20

20

20

Table 4.3: Different audio classes in the data set and the number of signals in each class

J\1P- TFD with the frequency resolution of Af = 250 is constructed for each audio signal.
Once the time-frequency matrix (TFM) is extracted, NI\!IF with decomposition order of 15

69
(r = 15) is performed on each TFl\1. Next, a feature vector comprised of nine features is extracted from each base and coefficient vector.

(4.38) Finally, SOTJ\II is applied on the training dataset and the nurnber of valid dusters is calculated for each classification scenario. One of the advantages of using SOTM is that unlike other clustering approaches such as fuzzy C-rneans, the exact nurnber of clusters is not needed to be determined beforehand. The clusters are formed as the data is presented to the network and the number and size of the clusters is determined by the paran1eters such as the hierarchial control function (H(t)) and the learning rate ( a(t) ). The initial values of these functions are appointed according to the dataset. In the next stage, the rnernbership coefficients are calculated for each cluster based on the distribution of the train signals. In the test stage, each of the test signals are assigned to one of these cluster centers based on the minilnum Euclidean distance measure. Finally, the class label of each signal is detenuined by the weighted stun of the feature vectors falling within each duster rnultiplied by the rnernbership coefficients. Another point to be discussed here is that since the data is represented to the SOTl\1[ in a randmn rnanner, the nurnber and the shape and size of the clusters might vary each time the clustering algoritlun is run on the data. However, since there is not a one to one correspondence between the clusters and the audio classes, this fact has no considerable impact on the total performance of the classifier. In addition, the results of the several are averaged to further elirninate this effect. One of the rnost irnportant classification tasks for a hearing aid system is to discrirninate hun1an speech fonn environmental noise. Therefore, in the first scenario the data set consists of signals from hurnan speech and envirmunental sounds. The hun1an category includes 20 signals from male speakers and 20 signals fronr fernale speakers and environn1ental sounds include 10 bird, 10 aircraft, 10 piano and 10 anin1al signalso Table 4.4 shows the results for this classification task where an accuracy of 96% has been achieved. As it can be seen from the confusion 1natrices, the systern dernonstrates high accuracy in discrimination of hun1an

70 voice fron1 other audio signals. The achieved true positive rate shows that all hun1an voice signals have been classified correctly. In addition, the overall accuracy rate for classification scenarios that include discrirnination of human voice is very high. Furtherrr10re, in order to evaluate the efficiency of the systen1 to discriminate humanvoicein particular environments, two other classification tasks have been defined. In the first case, an accuracy of 98% has been achieved in discrirnination of human voice from the rnusical instnunents. This capability could be useful in recognizing and separation of hurnan voice fron1 the background n1usic in a song or at the concert. The second classification task was defined as discrimination of hun1an voice from natural sounds, where an accuracy of 96% has been achieved. Furthermore, the proposed method was applied to other classification scenarios such as natural vs artificial sounds and 1nusical instruments vs aircraft. The results of these classification tasks are provided in Tables 4.7 and 4.9. Table 4.5 shows the overall obtained accuracy rate and the data set used for each classification scenario.

Human

Non-human

Total

Hun1an

40 (100%)

0

(0%)

40 (100%)

Non-hun1an

3 (7.5%)

37 (92.5%)

40 (100%)

Table 4.4: Confusion matrix for classifying human vs non-human audio signals

71

Classification scenario

Data set

Accuracy rate

Hurnanjnon-hurnan

Non-hurnan:aircraft, piano, animal, bird Human: rnale, female

96%

Hurnanj1\1usic

Hurnan:male, fen1ale 1v1usic:piano,flute,drum

98%

Nat ural/ Artificial

Natural:rnale, female, bird, anirnal, insect Artificial: helicopter, airplane, piano, flute, drum

81%

Hurnan/Nature

Hun1an:rnale, female Nature:anilnal, insect, bird

96%

Iviusic/ Aircraft

1v1usic:piano, flute, drurn Aircraft:helicopter, airplane,

92%

Table 4.5: Different audio classes in the data set and the number of signals in each class

Human

1\1 usical instrurnents

Total

Hurnan

40 (100%)

0 (0%)

40 (100%)

.1\1 usical instrurnents

1

(2%)

39 (98%)

40 (100%)

Table 4.6: Confusion matrix for classifying human speech vs musical instruments

72

Natural

Artificial

Total

Natural

50 (100%)

0

(0%)

50 (100%)

Artificial

19 (34%)

36

55
(100%)

(66%)

Table 4. 7: Confusion matrix for classifying natural vs artificial sounds

Hun1an

Nature

Total

Hurnan

20 (100%)

0

(0%)

20 (100%)

Nature

3

17

(15%)

(75%)

20 (100%)

Table 4.8: Confusion matrix for classifying human vs nature sounds

73

l\!1 usical instruments

Aircraft

Total

l\!1 usical instruments

34

(75%)

6 (15%)

40 (100%)

Aircraft

0

37

50

(0%)

(100%)

(100%)

Table 4.9: Confusion matrix for classifying musical instrument vs aircraft sounds

Chapter 5
Conclusion

I

N this work the application of unsupervised learning for analysis and classification of bion1edical signals was investigated. Although there are rnany works on the applica-

tion of supervised learning techniques for classification of bimnedical data, exploring the application of unsupervised learning rnethods can be beneficial in Inany ways. Building a reliable supervised classifier requires a large enough, precisely labeled dataset. However, some bimnedical datasets are very large and n1anual labeling of the data can be extren1ely costly and tirne consun1ing. In such cases, unsupervised learning methods can be used to find the natural groupings (e.g in audio classification) that exist in the dataset and then a physician can label the discovered groups. Furthermore, unsupervised techniques posses 1nore flexibility in situations where the characteristics of the data change over tirne or the the number of classes is not known beforehand. For example, consider the audio classification task in a hearing aid device. The audio signals that are received by the device contain different audio classes depending on the audio environment. Audio classes that exist in an indoor environ1nent can be different frmn those that are found in an outdoor envirorunent or at the concert or at a lecture. In such situations where the nun1ber and the nature of the classes are not known, a clustering method rnight perfonn better than a supervised classifier that is tuned to detect specific classes. In addition, unsupervised classifiers can be used to get some insight about the structure of the data and select n1ore efficient feature extraction methods. Two classification methods based on clustering techniques was applied to two separate

74

75 bimnedical signal classification problern. In Chapter 3, fuzzy C-rneans clustering was applied
for classification of sn1all bowel capsule endoscope in1ages and in the Chapter 4 classification of audio signals for hearing aids was investigated. Despite the different classification tasks in the Chapter 3 and Chapter 4, there are conunonalities for the two databases. First, the signals in both databases are non-stationary. Second, in both scenarios we are dealingwith a large volurne of data and lastly in both cases the real-time performance of the algorithrns is in1portant. For the hearing aid application, the need for real-time performance is more obvious. No hearing aid user would be interested in a device that amplifies the audio signals with delay. In the case of capsule endoscopy, the real-time performance becomes more critical in the design of the next generation of capsule endoscopes, or the "smart'' capsule endoscopy, where the capsule itself contains the drugs and can release the drug wherever it is required in the gastrointestinal tract. Based on the nature of the classification task in Chapter 4, where the nun1ber of audio classes is not known, a classification rnethod based on SOTIVI clustering algoritlun was used to discriminate different audio classes. The advantage of SOTM over other clustering techniques such as fuzz C-means is that in this approach the number of clusters is not required beforehand and this n1akes the SOTI\!I more suitable for this audio classification task. The discussion and conclusion for each of the chapters is provided in following sections.

5ol
5.1.1

Classification of sn'lall bowel images
Results and discussion

In Chapter 3, fuzzy C-n1eans clustering was applied to the problem of detecting abnormalities in the small bowel capsule endoscopy images. Initially the iinages were converted to Lab color space. The Lab color space is a perceptually uniforn1 color space and the Euclidean distance measure perfonns better in this color space. The results provided in Table 3.3 show that the classification accuracy in this color space is better than the rates obtained in the RG B space. A feature extraction method based on wavelet coefficients and cross co-occurrence Ina-

76

trices was applied to the ilnages. Since the abnorrnalities rnight occur at random locations in the image, SIDvVT was used for the wavelet decon1position to extract shift-invariant features. The combination of wavelet coefficients and cross co-occurrence nratrices was shown to be efficient in the previous works. Four types of features were extracted frmn the CCl\11 to represent texture characteristics, including energy, homogeneity, texture and contrast. Since the feature extraction process was performed on the three color planes of the hnage, the extracted features contain color infonnation as well. Difl'erent combinations of features were evaluated and the results was provided in Table 3.3. The results for a supervised classifier, which is LDA in this case, is also provided for the same feature set. As it can be observed from the table, the best performance for unsupervised classification was achieved with energy and hon1ogeneity features in the Lab color space. The confusion matrix and receiver operating curve for this feature set is provided in Fig 2.6 and Table 2.2. An accuracy rate of 76% was achieved for with fuzzy C-rneans algorithrn. Although the results show higher accuracy rates for the supervised classifier, one should bare in n1ind that the perfonnance of the supervised classifier can be biased by the dataset to son1e extent. In order for a supervised classifier to be reliable and provide good generalization, it has to be trained on a large enough dataset. However, the number ofirnages in the srnall bowel data base is 75. Hence, despite the higher accuracy rate the reliability of the supervised classifier yet has to be investigated.

5.1.2

Future work

Although the accuracy rate obtained in this work is acceptable for an unsupervised classifier, other alternatives and n1odifications can be sought to in1prove the perforn1ance of the system. In the feature extraction stage, wavelet decomposition followed by the CCiv1 was used to extract color and texture information. Although CCl\1s have been used successfully in the previous works, they rr1ight not be the best solution for sn1all datasets since a large a1nount of data is generated after the calculations. Hence, a large amount of averaging and down sarnpling has to be done to decrease the number of features to a reasonable nurnber and this

77 could cause the loss of inforn1ation. Among other texture analysis rnethods, textons are shown to be effective in representing textural infonnation. Textons have already been used in for texture analysis in biological and bion1edical irnages and have shown promising results. Thus, one of the subjects of the future research work would be to exan1ine alternative feature extraction n1ethods such as textons.

5. 2
5.2.1

Classification of audio signals
Results and discussion

In Chapter 4 a classification rnethod based on SOTl\1 clustering algorithm was applied to the classification of audio signals for the hearing aid application. The SOTl\!1 is a newly emerged clustering n1ethod, which has been already used for segn1entation of biological images. In this work however, the classification method is a fusion of supervised and unsupervised classification. Unlike most of the previous works in this area, the features extracted in this work were based on time-frequency analysis of the signals followed by the matching pursuit TFD. Due to the non-stationary nature of the audio signals, ten1poral or spectral features can not effectively represent localized features of the audio signals such as trends, discontinuities and repeated patterns. TF features on the other hand, are rnore suitable to capture and represent characteristics of the audio signals. The proposed method was tested under different classification scenarios such as human/non-human, human/music , natural/artificial, lnnnan/nature etc. The classification was perforn1ed on a database of 10 different audio classes including 20 aircraft, 20 animal, 20 bird, 20 drum, 20 female, 15 flute, 17 helicopter, 20 insect, 20 rnale and 20 piano signals. The classification results provided in Table 2.5 show high accuracy rates for rnost classification scenarios. An accuracy of 96% was achieved for discrin1ination of human vs nonhunlan sounds, which is the n1ost common classification scenario considered for the hearing aid. l\4any rnethods have been proposed for audio classification for hearing aid. However, most

78

of the existing papers in the literature address the problem of discrin1ination of the hurnan voice from the background noise. Although this would be desired capability in a hearing aid, it is not enough for other listening situations such as outdoor, lecture , concert etc. The problmn of audio scene analysis is rather a general problenr that can be the ultirnate goal for the hearing aids. The classification rnethod used in this work is based on SOT:NI clustering algorithrn. Hence, the nun1ber of audio classes is not needed to be known beforehand. This rnakes the proposed rnethod suitable for the problern of audio scene analyis for hearing aid where the nun1ber of audio classes vary under different listening situations. An efficient classification algorithm that can perforrn effectively in different audio environnlents could have a definite application in the hearing aids. According to several surveys, a considerable number of hearing aid users are not satisfied with the perfonnance of their hearing aid since it an1plifies the background noise as well as the desired signal. In addition, it has been observed in sirnilar studies that if the quality of the hearing aids can be improved, substantially better user satisfaction can be expected.

5.2.2

Future work

The proposed classification n1ethod was tested in different classification scenarios and high accuracy rates were achieved. Nevertheless, the following suggestions can be applied to improve the perforn1ance and reliability of the systern.
s Although the nurnber of audio classes is not needed beforehand in the classification

process, the nun1ber of discovered clusters is .detennined by the parameters in the SOTl\1 algorithm such as H(t) ( the hierarchical control function) and o:(t) ( the reset pararneter). The initial values for these
paranl(~ters

affect the number of the

discovered clusters and the variance of the san1ples within each cluster. In this work, these values were adjusted according to the perforrnance of the classifier. Thus,a future · improvement for this systmn would be to find a way to autornatically calculate the optimal value of these parameters fron1 the statistical characteristics of the data and

79
with regard to the classification results. · The number of clusters found by the SOTI\1,or any other clustering algorithm in general, does not always represent the actual nurnber of groupings that exist within the dataset. Therefore, a cluster validation technique ha.s to· be perforrned on the results of the clustering to evaluate the validity of the discovered clusters. In this work after the clustering stage, the clusters whose number of sarnples were srnaller than 5% of the total number of samples in the dataset, were recognized as invalid dusters and were elirninatecl. This threshold was deternrined based on the perfonnance of the classifier. However, there are more advanced cluster validity techniques that can be adapted for this purpose. So, another area for future work could be to find the best cluster validity n1ea.sure that optirnizes the perfonnance of the classifier. · In the SOTI\!1 algorithm, the representation of the data to the network is in a random nranner. Therefore, the results of the clustering rnight be slightly different for each time the algorithm is run on the dataset. In this work the result of the several runs are averaged to calculate the final results. However, a more robust solution would be to n1ake n1odifications to the SOTlVI ·algorithm or data representation so that the clustering results do not depend on the order in which the data is fed to the SOTM. · In Chapter 4 different classification scenarios were proposed and tested. The proposed scenarios are based on the taxonomy provided in Fig 4.1 and comrnon listening situations. Another topic for further research in this area would be to design more classification tasks that are tailored for the hearing aid application.

Bibliography
[1] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. Wiley New York, 2001. [2] A. Bousbia-Salah, A. Belouchrani, and A. Cichocki. Application of tirne-frequency distributions to the independentcomponent analysis of ECG signals. In IEEE International 2001. [3] W. Zhou and J. Gotlnan. Removal of El\IIG and ECG artifacts frorn EEG based on wavelet transfonn and ICA. In 26th IEEE Ann·ual Internat·ional Conference of the Engineering in Medicine and Biology 8oC'iety,IElv!BS'04., volun1e 1, 2004. [4] P. Gao, E. C. Chang, and L. Wyse. Blind separation of fetal ECG from single rnixture using SVD and ICA. In Pr-oc. the Joint
Confe?~ence Sy·mposi~tm

on Signal Processing and its Applications (IS8PIT'01), volurne 1,

of the 4th International Confer-

ence on Inforrnation, Com.1n'unications and Signal ProcessingJ and the 4th Pacific Rirn Confe·rence on Multimedia (ICICS-PCJI;f'03), volume 3, pages 1418- 1422. [5] C. Bigan. Chaotic cardiac arrhythmia detection by ICA and nonlinear dynan1ic processing of ECG signal. In IEEE International SympoBiurn on Intelligent Signal Processing, pages 117- 120, 2003. [6] C.A. Joyce, I.F. Gorodnitsky, and 1\1. Kutas. Automatic removal of eye rnovement and blink artifacts fron1 EEG data using blind cmnponent separation. Psychophysiology, 41(2):313-325, 2004. [7] W. Zhou, J. Zhou, H. Zhao, and L. Ju. Re1noving eye rnovement and power line 80

81

artifacts from the EEG based on ICA. In 27th Ann·nal Internat·ional Conference of
the Engineering in IV!edicine and Biology Soc·iety. IEEE-Elv1BS'05., pages 6017---6020,

2005.
[8} I. Navarro, B. Hubais, and F. Sepulveda. A comparison of time, frequency and ICA based features and five classifiers for wrist n1ovement cla.-,sification in EEG signals. In 21th Annnal International Conference of the Engineering in Medic·ine and Biology
Society. IEEE-ElvfBS'05., pages 2118--2121, 2005.

[9) L.K.L Joshua and J.C Rajapakse. Extraction of event-related potentials from EEG
signals using ICA with reference. In Proc. IEEE International Joint Conference on
Nenral Networks. IJCNNj05., volume 4, 2005.

[10] NIPS Chawla, HK Venna, and V. Kumar. ECG modeling and QRS detection using
principal cmnponent analysis. In 8rd International Conference On Advances in Medical
Signal and Inform.ation PTocessing. JVIEDSIP '06. JET, pages 1--4, 2006.
[11] R.. Yamada, .J. Ushiba, Y. Tomita, andY. I\1asakado. Decon1position of electrornyo-

graphic signal by principal con1ponent analysis of wavelet coefficients. In IEEE EMBS
Asian-Pacific Conference on Biomed·ical Engineering., pages 118-119, 2003.

[12] J. U. Chu, I. I\1oon, S.K. Kirn, and JV1.S. I\1un. Control of multifunction myoelectric
hand using a real-time El\1G pattern recognition.

[13] J. U. Chu, L I\1oon, and 1\tl.S. 1\tiun. A real-time EJ\IIG pattern recognition system based
on linear-nonlinear feature projection for. a multifunction myoelectric hand.
Transactions on Biom.edical Engineer·ing,
53(11):2232~2239,

IEEE

2006.

[14} J. Nadal and R.B Panerai. Classification Of Cardiac Arrhythmias Using Principal
Component Analysis Of The ECG. In Proc. the Annual IEEE International Conference
of the Engineering in Aifedicine and Biology. Soc·iety., volume 13.

82 [15] Y. Wenyu, L. Gang, L. Ling, and Y. Qilian. ECG analysis based on PCA and 80~1.

In Proc. the International Conference on Neural Networks and Signal Processing., volunle 1, pages 37-40.
[16] H. Zhang and L.Q. Zhang. ECG analysis based on PCA and support vector rnachines.

In Proc. the International Conference on Neu.ral Network.s and Brain. ICNNf!_1B'05, volun1e 2, pages 743-747. [17] N. Takano, H.G. Puurtinen, M. Rautiainen, J. Hyttinenl and J. Malmivuo. ECG source location clustering based on position vectors and forward transfer rnatrices. Cornputers
in CaTdiology, pages 3L3-316, 2002 .

[18) O.R Pacheco and F. Vaz. Integrated system for analysis and autmnatic classification of sleep EEG. In Proc. the 20th Ann·uallnternational Conference of the IEEE Engineering
in A1edicine and Biology Society, volume 4, pages 2062-2065, 1998.

[19] D. Wu and Vv. Wan Tan. Genetic learning and perforn1ance evaluation of interval type2 fuzzy logic controllers. Engineering Applications of Artificial Intelligence, 19(8):829-841, 2006. [20] A.B Geva and D.H Kerem. Forecasting generalized epileptic seizures frOin the EEG sig-

nal by wavelet analysis and dynamic unsupervised fuzzy clustering. IEEE Transactions
on Biomedical Enginee-ring, 45(10):1205--1216, 1998.

[21] AB Ajiboye and R.F. \IVeir. A heuristic fuzzy logic approach to E!v1G pattern recognition for n1ultifunctional prosthesis control. iEEE Transact-ions on Neural Systems and
Rehabilitation Engineering, 13(:3):280-291, 2005.
[22] A.B Ajiboye and R.F \Neir. Fuzzy c-rneans clustering analysis of the
E~1G

patterns

of six n1ajor hand grasps. In Proc. the 9th lnte·rnatio·nal Conference on Rehabilitation
Robotics. ICORR '05., pages 49- 52.

83 [23] K. Doi. Cornputer-aided diagnosis in n1edical irnaging: historical review, current status and future potential. Computerized fll!edical Imaging and GraphicB, 2007. [24] lVLL Giger, N. Karssen1eijer, and S.G Armato. Guest editorial con1puter-aided diagnosis in medical imaging. IEEE Transactions on Medical Imaging, 20(12):120.5-1208, 2001. [25] A. Khademi and S. Krishnan. l\1ultiresolution Analysis and Classification of Small Bowel I\/[edical Images. In Proc. 29th Annual IEEE International Conference of the
Engineering in Medicine and Biology Society. Ek1BS'01., pages 4524-4527, 2007.
31(4-5):198~211,

[26] B. Li and I'vLQ.H. I\11eng. Analysis of the gastrointestinal status fron1 wireless capsule endoscopy irnages using local color feature. In Proc. IEEE International Conference
on lnfonnation Acq'tti8ition. !CIA '07., pages 553-557, 2007.

[27] .J. Bonnel, A. Khaderni, S. Krishnan, and C. Ioana. Small bowel image classification using cross-co-occurrence rnatrices on wavelet dornain. Biomedical Signal Processing
and Control, 4(1):7-15, 2009.

[28] D.J.C. Barbosa, J. Ramos, and C.S .. Lima. Detection of small bowel tun1ors in capsule endoscopy frames using texture analysis based on the discrete wavelet transforn1. In 30th IEEE Annual International Conference of the Engineering in flr1edicine and
Biology Society. Ell1B8'08., pages 3012--3015, 2008.

[29] G. Hughes. On the n1ean accuracy of statistical pattern recognizers. IEEE Transactions
on Infonnation Theory, 14(1):55---63, 1968.

[30] S. Kochkin. 10-year custon1er satisfactiontrend.s in the US hearing instrurnent n1arket.
Iiearing Re·uiew, 9.

[31] S. Kochkin. " Why my hearing
Hear·ing Journal, 53(2):34-42, 2000.

are in the drawer": The consun1ers' perspective.

84 [32] S. Kochkin. MarkeTrak III identifies key factors in determining consumer satisfaction.

Hearing Journal, 45:39-39, 1992.
[33] K. Umapathy and S. Krishnan. Feature analysis of pathological speech signals using

local discrirninant bases technique. Nledical and Biological Engineering and Cmnputing,
43( 4) :457-464, 2005. [34] P. Nordqvist and A. Leijon.

An efficient robust sound classification algorithm for

hearing aids. The JmtTnal of the Acottstical Society of A·merica, 115(6).
[35] 1t1. Behler, S. Allegro, S. Launer, and N. Dillier. Sound classification in hearing aids

inspired by auditory scene analysis. EURASIP Jo·urnal on Applied Signal Processing,
18:2991--3002, 2005. [36] Adaptive Feature Selection for Speech/IV1usic Classification. IEEE 8th Workshop on

Multimedia Signal PTocessing.
[37] L. Lu, H.J. Zhang, and H. Jiang. Content analysis for audio classification and segrnen-

tation. IEEE transactions on 8peech and audio process·ing, 10(7):504- 516, 2002.
[38] G. Guo and S.Z. Li. Content-based audio cla..'3sification and retrieval by support vector

machines. IEEE Transactions on Neural Networks, 14(1):209--21 5, 2003.
[39] X. Shao, C. Xu, and l\II.S. Kankanhalli. Unsupervised classification of n1usic genre

using hidden Markov model. In Proc. IEEE International Conference on J\;Jultirnedia
and
E~rpo,

ICME'04., volurrw 3.

[40] A. Rauber, E. Pan1palk, and D. l\!Ierkl.

Using psycho-acoustic n1odels and self-

organizing maps to create a hierarchical structuring of music by sound sirnilarity. In
Proc. International Society for 1\Iu,sic Infor·mation Retrieval Conference ISMIR, pages
71-80~

2002.

[41] N. Searingella, G. Zoia, and D.l\1lynek. Automatic genre classification of n1usic con-

tent: a survey. IEEE Signal Processing Magazine, 23(2):133-141, 2006.

85

[42] A.K. Jain and R.C. Dubes. Algo·rithm,s for clu.stering data. Printice Hall, 1988.
[43] AK Jain, RPW Duin, and J. JVIao. "Statistical pattern recognition: A review''. IEEE
Transactions on paUern analysis and rnachine intelligence, 22(1):4--37, 2000.

[44] A.K Jain, J\;LN lV1urty, and P.J Flynn. Data clustering: a review. ACM computing
sun;eys, 31(3), 1999. [45] D. Judd, PK l\1cKinley, and AK Jain. "Large-scale parallel data clustering". IEEE Transact·ions on Pattern Analysis and A1achine Intelligence, 20(8):871-876, 1998.

[46] S.K Bhatia and J .S Deogun. "Conceptual clustering in inforn1ation retrieval". IEEE
Transactions on System.s, A1an, and Cybernetics, Part B, 28(3):427--436, 1998.

[47] C. Carpineto and G. Rmnano. "A lattice conceptual clustering system and its application to browsing retrieval". JI;Jachine Learning, 24(2):9.5---122, 1996.

[48] H. Frigui and R. Krishnapuran1.

"A robust con1petitive clustering algorithn1 with

applications incornputer vision". IEEE Trnnsactions on Pattern Analysis and Machine
Intelligence, 21(5):450--465, 1999. [49] H.l\1. Abbas and 1\/I.l\1. Fahmy. "Neural networks for maximun1 likelihood clustering". Signal Processing, 36(1):111-126, 1994.

[50] lVLJ. Kyan. Unsupervised learning through dynmnic self-organization: Implications
for microbiological image analysis. In PhD thesis, School of Electrical and Information
Engineering University of Sydney, 2007.

[51] E. Backer. Computer-assisted reasoning in cluster analysis. Prentice Hall International
Ltd. Hertfordshire, UK, 199.5.

[52] A. Hyva.rinen and E. Oja. Independent con1ponent analysis: algorithms and applications. Neural networks, 13(4-5):411-430, 2000.

86
[53] H. Kong and L. Guan. Detection and re1noval of i1npulse noise by a neural network guidedadaptive n1edian filter. In IEEE International Conference on Neural Networks,
1995. Proceeding.s., volu1ne 2, 199.5.

[54) GA Carpenter and S. Grossberg. The ART of adaptive pattern recognition by a selforganizing neuralnetwork. Computer, 21(3):77-88, 1988. [55] Given Imaging Ltd. formation guide. In PillCamTM SB Capsule Endoscopy - product in-~Vorld

Wide

w·eb~

http:/jwww.givenirnaging.com/en-

usjHealthcarePr'Ofessionals/ProductsjPages/PillCamSB.asp:r, 2009.

[56) B. Kiln, S. Park, C.Y. Jee, and S.J. Yoon. An earthworn1-like locomotive rnechanis1n
for capsule endoscopes. In IEEE/RSJ International Conference on Intelligent Robots
and Systern,8. (IROS 2005), pages 299T---3002, 2005 .
[57] D.G. Adler and C.J. Gostout.

Wireless capsule endoscopy.

Hospital Physician,

39(5):14-22, 2003.
(58) B. Li and IVI.Q.H. 1\tleng. Analysis of the gastrointestinal status frorn wireless capsule

endoscopy in1ages using local color feature. In Information Acquisition, 2007. !CIA '07.
International ConfeTence on, pages 553-557, 2007.

[59) B. Li and JVI.Q.H. I\1eng. Analysis of the gastrointestinal status from wireless capsule
endoscopy irnages using local color feature. In Information Acquis-ition, 2007. 1CIA '07.
InteTnational Conference on, pages .553--.557, 2007.

[60) S. l\1allat. A wavelet to·ur of 8'ignal processing. Acade1nic press, 1999. [61] J. Liang and T\V Parks. In1age coding using translation invariant wavelet transforms withsyrnrnetric extensions. 1998. [62] A. Khaderni. Multiresolutional analysis for classification and COlllJJfession of n1edical images. l\1aster's thesis, Ryerson University, Canada.
IEEE Transactions on Irnage Process-ing, 7(5):762- 769,

87 [63] S.A. Karkanis, DJC Iakovidis, D.E. Niaroulis, D.A. Karras, and M. Tzivras. Computeraided tumor detection in endoscopic video using color wavelet features. IEEE Transactions on Inforrnat·ion Technology in Biom.edicine,
7(3):141~152,

2003.

[64} V. Arvis, C. Debain, JVL Berducat, and A. Benassi. Generalization of the cooccurrence

rnatrix for colour in1ages: application to colour texture classification. Image Analysis and Stereology, 23(1):63--72, 2004.
[65) B. Julesz. Textons, the elernents of texture perception, and their interactions. 1981.

[66] lv1. Tuceryan and A.K. Jain. Handbook of pattern recognition 1!:1 cornputer vision. vVorld Scientific Pub Co Inc, 1999.

[67] H. Harms, U. Gunzer, and HJ\1 Aus. Combined local color and texture analysis of
stained cells. Comp·uter vision, graphics, and image processing,
[68] S. Petroudi, T. Kadir, and lvi. Brady.
33(3):364~376,

1986.

Autornatic classification of n1anunographic

parenchyrnal patterns: A statistical approach. In Proc. the 25th Annual IEEE International Conference of the Engineering in A!edicine and Biology Society., volume 1,
2003. [69] 0. Tuzel, L. Yang, P. JVieer, and D ..J. Foran. Classification of hematologic rnalignancies

using texton signatures. Pattern Analysis

ef Applications,

10(4):277-290, 2007.

[70] D.A. Adjeroh, U. Kandaswamy, and .J.V. Odom. Te.x:ton-based segrnentation of retinal
vessels. Journal of the Optical Society of Arnerica A, 24(5):1384-1393, 2007.

[71] J Jv1. Kates. Classification of background noises for hearing-aid applications.
Journal of the Acoustical Society of America, 97:461, 1995.
[72] L. Rabiner and B.H. Juang. Fundamental8 of speech recognition. 1993.

The

[73) J.C. Junqua and J.P. Haton. Robus.tness in autmnatic speech recognition: fundarnentals

and applications. Kluwer Aca.dernic Publishers Norwell, !viA, USA, 1995.

88
[74] B. Gold and N. 2006. [75] A. Izworski, R. Tadeusiewicz, and W. Wszolek. Artificial Intelligence l\t1ethods in
Diagnostics of the Pathological Speech Signals. Lecture notes in cornputer science, pages 740-748, 2004.
l\~1organ.

Speech 1 audio signal processing. Wiley India Pvt. Ltd.,

[76] Z. Han, X. Wang, and J. Vvang. Pathological Speech Defonnation Degree Assess1nent
Based on Dynamic and Static Feature Integration. In The 2n,d International Conference
on Bioinformatics and Bimnedical Eng'ineering, 2008. ICBBE 2008., pages 2036- 20:39,

2008. [77] l\1.A. Lund and C.C. Lee. A robust sequential test for text-independent speaker verification. The Journal of the Acoustical Society of America, 99:609, 1996.

[78] G. Tzanetakis and P. Cook. l\1usical genre classification of audio signals. IEEE Transactions on speech and audio processing, 10(5):293--302, 2002.
[79] E. Pan1palk, A. Flexer, and G. Widmer. In1proven1ents of audio-based n1usic sin1ilarity

and genre classification. In Proc. International Society joT lvf-usic lnforrnation Retrieval
Conference. JSMIR '05, volu1ne 5, 2005.

[80} N. Scaringella and G. Zoia. On the modeling of tinw information for auton1atic genre recognition systems in audio signals. In Proc., pages 666-671.

[81] H. Soltau, T. Schultz, l\rL Westphal, and A. Waibel. Recognition of n1usic types. In
Proc. the IEEE International Conference on Acoust·ics, Speech and Signal Processing, ICASSP'98, volun1e 2, pages 1137-1140.

[82] K. West and S. Cox. Finding an optimal segn1entation for audio genre classification. In
Proc. 6th International Symposiv,·m on Afusic Information Retrieval, ISMJR '05, pages

680-685.

89
[83] T. Lidy and A. Rauber. forn1ations for music genre
~,J.ckoOH.LI._,

and psycho-acoustic tra.nsrnat·ional Conference on

lv!usic Information Retrieval

[84] A. Berenzweig, D.P.W. Ellis,

rce seJ~In.en1~s to in1prove artist
aL
,()o· n tF::rP1U'P

on "Virtual, Syn-

[85] B. Kostek. l\11usical instrument mation retrieval techniques.

e1nploying music infor-

and Language Processing,

[87] L Kaminsky and A. l\!Iaterka. cal instrumentsounds. In Proc volume 1, pages 189-194. [88] K.D. l\!Iartin. Toward autornatic n1ents. NATO Co·mputational
1-12~

hHJ.Gtt.LlU'll

of monophonic rnusion Neural
Networks~

"-n l'.o,.'"'Y'U'D

identifying musical instru'tute, Il Ciocca, Italy, pages

1998.

[89) A. Eronen and A. Klapuri. andte1nporal features. In Proc.
and Signal Proce8sing, ICASS

[90] Ivi.J. Hawley. StructuTe out of bridge, i\1A, USA, 1993. [91] J. Saunders, L.l\1. Co, speech/music. In Proc. IEEE
Signal P-rocessing,ICASSP'96., on Acoust·ics,

90

[92] l\1..1 . Carey, E.S. Parris, and H. Lloyd-Thomas. A con1parison of features for speech, rnusic discrin1ination. In Proc. IEEE International Conference on
and Signal Pr·ocessing, ICASSP'99, volurne 1, pages 149·--152, 1999.
[93] A.S. Bregrnan. Auditory scene analysis: The perceptual
01~ganization

Aco·ustics~

Speech

of sound. The

l\!IIT Press, 1994.
[94] G. Freeman, R.D Dony, and S.J\11 Areibi. Audio Environment Classification for Hearing

Aids using Artificial Neural Networks with Windowed Input. In Proc. IEEE Syrnposiurn
on Cornputational Intelligence in Image and Signal Processing, CIISP '07, pages 183188, 2007.

(95) .J. Pinquier, J.L. Rouas, and R. Andre-Obrecht. Robust speech/rnusic classification

in audio docurnents. In Proc. the 7th Seventh International Conference on Spoken
Lang'uage Processing, volurne :3.

[96] E. Scheirer and l\1. Slaney.

Construction and evaluation of a robust rnultifeature

speech/music discrirninator. In PToc. IEEE International Conference on Acoustics
Speech and Signal Processing, ICASSP'97, volume 2, pages 13:31-1334, 1997.
[97] N. Niesgarani, M. Slaney, and SA Shanuna. Discrirnination of speech frcnn nonspeech

based on rnultiscale spectro-ternporal modulations.
Speech, and Language Processing, 14(3):920-930, 200G.

IEEE Transactions on A·uclio,

[98} SG IV1a.llat and Z. Zhang. J\!Iatching pursuits with tirne-frequeney dictionaries. IEEE
Transactions on Signal Processing, 41(12):3397--3415, 1993.
[99) P.J. Franaszczuk, G.K. Bergey, P.J. Durka, and H.l\!1. Eisenberg.

Time- frequency

analysis using the nwtehing pursuit a.lgorithrn applied to seizures originating frorn the mesial temporal lobe. Electroencephalography and clinical nevxophysiology, 106(6):513-521, 1998.

91

[100) D.O. Lee and H.S. Seung.
torization. Nature, 401
(101] I. Buciu. Non-negative matrix

'"'"' .. "'·, .···c· " .. n.n.··a.n,r-c

by non-negative matrix fac-

feature extraction: Theory
ZTIA'],

and Applications. In Proc. the
Communications and
Control~

LD£rl?.r Jn1~er:na1~w1

Conference on C(nnputer-s,

[102) M.vV. Berry, IVI. Browne, A.N.
and applications for
UUFI!J ...

and R.J. Ple1nn1ons. Algorithms
n-·..... t- .... ,--v

~'...._J. ....!.A._.,,_ ,,_, n~=>nrteg;:ttl'\re

factorization.

Computational

[103) C.J. Lin. Projected gradient ............,,,_uv\.

.1.0

Co'm putation, 19(10):2756-2779, 2007.

