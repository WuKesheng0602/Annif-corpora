Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2010

A Laser Scanner And Void Visualizer For Use In A Search And Rescue Environment
Vijay Somers
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Computer Sciences Commons Recommended Citation
Somers, Vijay, "A Laser Scanner And Void Visualizer For Use In A Search And Rescue Environment" (2010). Theses and dissertations. Paper 1642.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

A LASER SCANNER AND VOID VISUALIZER FOR USE IN A SEARCH AND RESCUE ENVIRONMENT

By Vijay Somers Bachelor of Applied Science in the Program of Electrical Engineering, University of Waterloo, 2005 Masters of Engineering in the Program of Electrical Engineering Ryerson University, 2007

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Science in the Program of Computer Science

Toronto, Ontario, Canada, 2010 © Vijay Somers 2010

I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research. VIJAY SOMERS

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. VIJAY SOMERS

ii

ABSTRACT
A LASER SCANNER AND VOID VISUALIZER FOR USE IN A SEARCH AND RESCUE ENVIRONMENT

Vijay Somers MSc, Computer Science, Ryerson University, 2010

Urban Search and Rescue (USAR) environments present many risks to emergency first responders. Technologies that can allow people to explore dangerous locations in great detail while being physically separate from them are of great value. This thesis provides an intuitive 3D viewing application called Voidviz for just that purpose, with features specifically designed for USAR and bomb identification. It is tested using 3D data gathered by two devices: a computerized theodolite, and a custom built laser scanner. The theodolite was found to be impractical for scanning dangerous locations due to its low resolution and slow speed, but the custom laser scanner was able to gather high resolution data at a useful speed. This thesis shows that useful data can be derived from sufficiently detailed simulations of voids within building collapses and unexploded explosive devices. This data can be used to increase the situational awareness of first responders.

iii

ACKNOWLEDMENTS

I could not have finished this thesis without the moral and physical support provided by my family and friends. Their gentle encouragement and occasional deliveries of supplies made this immeasurably less stressful. Most importantly though, I would like to thank my supervisor, Dr. Alex Ferworn, for his patience and assistance. His passion for developing practical and useful life saving technology is a credit to the University and has shaped my interests. I would like to express my gratitude to Jimmy Tran and Martin Gerdzhev for travelling with me to perform field tests and for spending hours lying on cold, wet concrete while being scanned. Sergeant Dave Moffat provided a key piece of technology that would have been impossible to get a hold of any other way, as well as his time on a very cold day. For that he is greatly appreciated.

iv

TABLE OF CONTENTS ABSTRACT ................................................................................................................................. iii ACKNOWLEDMENTS ............................................................................................................... iv LIST OF FIGURES .....................................................................................................................vii ABBREVIATIONS ................................................................................................................... viii CHAPTER 1
1.1 1.2 1.3 1.4 1.5 1.6

INTRODUCTION ...................................................................................... 1

Introduction ........................................................................................................................... 1 Problem Definition .............................................................................................................. 3 Objectives ................................................................................................................................ 4 Thesis Statement .................................................................................................................. 5 Contributions ......................................................................................................................... 5 Thesis Organization............................................................................................................. 6

CHAPTER 2 BACKGROUND - Literature Review of 3D visualization systems used in Disaster Scenarios.................................................................................... 7
2.1 Introduction ........................................................................................................................... 7 2.2 Emergency Management ................................................................................................... 7 2.3 Urban Search and Rescue ............................................................................................... 10 2.3.1 Task Force Makeup..................................................................................................................... 11 2.3.2 Ease of Use ..................................................................................................................................... 13 2.3.3 Rubble Piles ................................................................................................................................... 13 2.4 Situational Awareness ..................................................................................................... 14 2.4.1 Temporal Aspects........................................................................................................................ 15 2.4.2 SA and Decision Making............................................................................................................ 15 2.4.3 Who Needs SA? ............................................................................................................................. 16 2.4.4 How is SA Acquired? .................................................................................................................. 16 2.5 Computational Public Safety ......................................................................................... 17 2.6 3D Scanning Technologies ............................................................................................. 17 2.7 Game Engines and Visualization .................................................................................. 20 2.8 Viewing 3D Datasets ......................................................................................................... 22 2.8.1 Raster Data ..................................................................................................................................... 22 2.8.2 Vector Data..................................................................................................................................... 23 2.9 Scan Matching ..................................................................................................................... 25 2.10 Surface Reconstruction ................................................................................................... 27 2.11 Summary .............................................................................................................................. 28

CHAPTER 3

MATERIALS AND METHODS ............................................................ 29

3.1 Overview and System Architecture ............................................................................ 29 3.2 Materials Overview........................................................................................................... 30 3.2.1 The Total Station ......................................................................................................................... 30 3.2.2 The Voidscanner .......................................................................................................................... 32
3.2.2.1 3.2.2.2 3.2.2.3 Voidscanner Construction .............................................................................................................. 32 Frames of Reference and Coordinate Transforms ............................................................... 33 Error Mitigation .................................................................................................................................. 35

3.2.3

Void Scanner Visualizer ............................................................................................................ 36

v

3.2.3.1

Voidviz Functions ............................................................................................................................... 37

3.3 Methodology........................................................................................................................ 41 3.3.1 Practicalities .................................................................................................................................. 42

CHAPTER 4

EXPERIMENTS AND RESULTS ......................................................... 44

4.1 Experiment 1 Victim Recognition Using Total Station Data .............................. 44 4.1.1 Facility.............................................................................................................................................. 44 4.1.2 Setup ................................................................................................................................................. 45
4.1.2.1 4.1.2.2 4.1.2.3 Virtual Environment Construction Using Google Sketchup ............................................ 46 Environment Construction Results ............................................................................................ 47 Victim Visualization .......................................................................................................................... 47

4.2 Experiment 2 UCRT Facility Scans with Voidscanner .......................................... 49 4.2.1 Victim Visualization.................................................................................................................... 50
4.2.1.1 4.2.1.2 Experimental Results........................................................................................................................ 50 Discussion .............................................................................................................................................. 54 Procedure .............................................................................................................................................. 56 Results and Discussion .................................................................................................................... 57

4.2.2

Modeling an Access Path .......................................................................................................... 55

4.2.2.1 4.2.2.2

4.3 Experiment 3 Bomb Component Identification...................................................... 60 4.3.1 Experiment Setup and Device Description ....................................................................... 61 4.3.2 Procedure ....................................................................................................................................... 62 4.3.3 Results and Discussion.............................................................................................................. 62 4.4 Conclusion ............................................................................................................................ 64

CHAPTER 5

CONCLUSION AND FUTURE WORK ................................................ 66

5.1 Summary of Findings and Conclusion ........................................................................ 66 5.2 Future Research ................................................................................................................. 68 5.2.1 Scan Matching ............................................................................................................................... 68 5.2.2 Scanner Mobility .......................................................................................................................... 68 5.2.3 Laser Shadow Detection and Marking ................................................................................ 68 5.3 Concluding Remarks ........................................................................................................ 69

BIBLIOGRAPHY ....................................................................................................................... 70 GLOSSARY ................................................................................................................................. 73

vi

LIST OF FIGURES Figure 2-1 Emergency Management Cycle [6] ................................................................... 9 Figure 2-2 Pancake style building collapse ...................................................................... 14 Figure 2-3 Void interior with rescue canine ­ Photo at UCRT exercise by Dr. Alex Ferworn ..................................................................................................................... 14 Figure 2-4 SICK Laser S3000 model rangefinder[23] ..................................................... 19 Figure 2-5 Hokuyo URG-04LX-UG01 laser scanner[24] ................................................ 20 Figure 2-6 Phoxel-Space interface showing an MR scan [29] ......................................... 23 Figure 2-7 Comparison between Raster and Vector approximations of a map ................ 25 Figure 3-1 Voidviz Architecture ....................................................................................... 30 Figure 3-2 Sokkia 350R Total Station .............................................................................. 31 Figure 3-3 Voidscanner with the USB cable unplugged .................................................. 33 Figure 3-4 Laser Scanner Alignment Correction .............................................................. 35 Figure 3-5 Markup indicating victims confirmed to be alive ........................................... 37 Figure 3-6 Scene without Surfacing applied ..................................................................... 39 Figure 3-7 Scene with Surfacing applied .......................................................................... 40 Figure 3-8 Surface of UCRT training facility ................................................................... 42 Figure 4-1 Test Site Overview (Before addition of debris) .............................................. 45 Figure 4-2 Simulation Overview ...................................................................................... 47 Figure 4-3 Simulated Overhead View of Victim .............................................................. 48 Figure 4-4 Actual Overhead View of Victim ................................................................... 48 Figure 4-5 Simulated View of Victim through Tunnel ..................................................... 49 Figure 4-6 Actual View of Victim through Tunnel .......................................................... 49 Figure 4-7 Testing facility entrance, post-burial by debris ............................................... 50 Figure 4-8 Example Victim .............................................................................................. 53 Figure 4-9 Victim rendered in Points only ....................................................................... 53 Figure 4-10 Victim rendered using 25% of points............................................................ 53 Figure 4-11 Victim rendered using 33% of points............................................................ 53 Figure 4-12 Victim rendered using 50% of points............................................................ 53 Figure 4-13 Victim rendered using 100% of points.......................................................... 53 Figure 4-14 Reassembled Scans ....................................................................................... 56 Figure 4-15 Training Facility Entrance way ..................................................................... 59 Figure 4-16 Simulation Training Facility Entrance way .................................................. 59 Figure 4-17 View down Tunnel 1 to Room 2 ................................................................... 59 Figure 4-18 Simulation View down Tunnel 1 to Room 2 ................................................ 59 Figure 4-19 Victim ............................................................................................................ 59 Figure 4-20 Simulation Victim ......................................................................................... 59 Figure 4-21 Oklahoma City Federal Building, Post ­bombing, pre-demolition .............. 60 Figure 4-22 Bomb Identification Configuration ............................................................... 62 Figure 4-23 Bomb Mockups ............................................................................................. 64 Figure 4-24 Simulated Bomb Mockups ............................................................................ 64

vii

ABBREVIATIONS USAR CBRNE UCRT Urban Search and Rescue Chemical, Biological, Radiological, Nuclear Explosive Urban Search and Rescue, Chemical, Biological, Radiological, Nuclear Explosive Response Team MRPT ICP SLAM SA TF TS OPP Mobile Robot Programming Toolkit Iterative Closest Point Simultaneous Localization And Mapping Situational Awareness Task Force Total Station Ontario Provincial Police

viii

CHAPTER 1
1.1 Introduction

INTRODUCTION

Given the generally unpredictable nature of large-scale disasters, a well trained and well equipped team of first responders is required to limit casualties. The tools they have available have a large impact on their effectiveness, thus there is always a push to improve existing tools and to develop innovative new ones as well. This thesis describes a new tool in the area of Urban Search and Rescue (USAR). Due to the higher population densities in urban centres, the potential for loss of life during a disaster is always greater. A large population can magnify the effects of even moderate disasters and make them much more difficult to deal with in their immediate effects and their longer-term aftermath. This has been demonstrated by various incidents including the China earthquake (2008) and the World Trade Center collapses (2001) to name but two. This magnified threat is what makes USAR response so important. In dense urban centres, building collapses following a disaster event are common. A consistent feature that is produced after a collapse is what is known as a void. Voids are empty spaces that are formed by chance within a collapsed structure, and have varying degrees of stability. Any survivors to be found are usually located within voids--others, not so lucky, are crushed by building debris. The rescue process involves locating the survivors, stabilizing the voids physically and the victims medically, and then extracting the victims to safety and further medical attention.

1

Two factors that are important in the decision making process for emergency managers and first responders are the passing of time and the rapid acquisition of situational awareness--defined as the understanding that an individual has of what is actually happening in a given situation [1]. The sequence of events that unfold as emergency responders find casualties, develop rescue plans, and implement them all take time. Anything that can safely decrease the time required for any of these steps can lead to an increase in saved lives as delay reduces survival rates [2]. It is important that any technology or tool that increases speed also decreases the risk for either the casualties or the responders. In fact, anything that decreases the risk to the responders ultimately assists the casualties as well, as the responders are less likely to be injured in the harsh and chaotic USAR environment. When a building collapse occurs, the resulting rubble will typically contain voids of varying sizes. These voids are formed when partially intact pieces of debris are able to resist the crushing effect of the debris above them and shelter the area below them. One of the first actions taken by first responders with a rubble pile is for a Structural Engineer to perform a cautious examination of the potential and accessible voids where live victims are likely to be found to decide how they should be reinforced to stave off further collapse. This must be done before victims within voids can be assisted. This is a risky period during search operations because the stability of the rubble pile is largely unknown and the collapse is usually still active as debris settles. Further rapid collapses could occur while first responders are on or in the pile, making any rescue more complicated and dangerous.

2

When performing the examination of the pile, there may be voids that are not easily accessible in a safe fashion, so a proper reconnaissance may not be possible. The points of access to the voids may be too small to provide an adequate view, or the void itself may be too deep in the rubble. So while the most common tools for the first responder to use to examine voids are their eyes, this isn't always possible. Camera probes can be inserted into voids, but these can only produce 2D images and video. Three dimensional scanning technologies are currently uncommon but are

becoming more prevalent due to the maturing of the technology. They can produce extremely detailed and accurate data about a collapse, but the applications to manipulate and deal with the data are still primitive. A point of sensing is the location where information about an area is sensed. The point of attention is the location where thought is applied to that sensed information. For a person not using any technological aides, those are the same points. We propose that it is possible to use techniques broadly within the field of Computational Public Safety (CPS) to move the point of sensing of a first responder away from the point of attention. In essence, we think that it is possible to allow a first responder to perform some activities in a virtual environment. This would remove them from danger while at the same time not negatively impacting their trained assessment skills, and possibly improving their chances of success by giving them abilities they would not otherwise possess.

1.2 Problem Definition
First responders are often presented with environments that pose them significant risk if entered, but that may contain injured people. Remote sensing tools

3

are often used to allow the rescuers to gather information without exposing them to significant risk. Many of the tools used today, while useful, could be improved. A camera inserted into a void can provide 2D imaging of the space, but that can be confusing due the generally chaotic nature of voids which are typically devoid of visual contrast. In addition, cameras cannot provide accurate metrics concerning the physical dimensions and location of what is being sensed. A void in a collapsed concrete structure is typically uniformly matte grey on the inside even when lit, and could have confusing directional indicators. For instance, walls could have fallen to the floor. It would be difficult to use a 2D image to gather useful information about such a scene. Dimensional metrics are also important in the related field of Chemical, Biological, Radiological and Nuclear explosive (CBRNE) mitigation. When attempting to gather information about possible explosive devices, accurate information about physical dimensions is important for the proper aiming of disruptors. 3D scanning devices are becoming more prevalent, but there are no visualization tools specifically for USAR or CBRNE work. Such a tool that could allow a first responder to intuitively explore a 3D environment to gather accurate physical data about a hazardous environment without placing them in extreme danger would be extremely useful.

1.3 Objectives
There were several objectives for this thesis. One goal is to show that it is possible to recognize human shapes in the environment presented within a void and to determine what level of detail is required. Also in a USAR related capacity, it will

4

be determined if a laser scan visualizer can be used to help plan a route through the voids of a collapsed building to access a victim. Using the same premise, we have the related objective of determining the feasibility of employing a scan visualization tool to provide accurate physical information about unexploded bombs and similar hazardous situations.

1.4 Thesis Statement
It is possible to increase situational awareness safely in hazardous environments by using Voidviz to visualize 3D data points collected by a scanner on site.

1.5 Contributions
There are three main research contributions made by this thesis. The Total Station, detailed in section 3.2.1, is normally only used for reconstructing car accidents. In this thesis it was instead used to gather data about voids inside

collapsed buildings. Although it became apparent that the Total Station had an impractically low resolution and slow scanning rate, it could still be taken into voids and used to generate 3D models of the surroundings. This is an environment and task that Total Stations have not been applied to before. The notion of using such a low-resolution device for modeling an environment that typically is very chaotic and detail-rich is not only novel but a potentially useful way of measuring and, ultimately, depicting scenes within rubble. The second contribution made by the thesis is the Voidscanner, which is detailed in section 3.2.2. The Voidscanner is a relatively inexpensive scanning

5

device that is capable of relatively fast operation and high scanning resolution. The components of the Voidscanner itself cost less than $1500--costing much less than other competing devices [3]. This makes it both useful, inexpensive, and potentially ubiquitous--allowing such devices to be deployed in quantities large enough to make the visualization of voids a potential operational standard. The third and most important contribution made by this thesis is Voidviz--a 3D visualization program made specifically for first responders. It allows the user to explore accurate simulations generated by data from laser scanners, and provides the user with virtual equivalents of some of the tools normally available to first responders in real disaster environments. Voidviz provides a simulation that is meant to support normal Search and Rescue operations while requiring only minimal training.

1.6 Thesis Organization
This thesis is organized into 5 chapters. This chapter has been the introduction, which explains the motivations behind the thesis, describes its goals and outlines its contributions. Chapter 2 provides some of the related background information on the Search and Rescue discipline and related technologies. Chapter 3 details what

technologies were developed for this thesis. Chapter 4 describes the experiments that were conducted to test the thesis statement and what results were obtained. Finally, chapter 5 summarizes the results and outlines some future areas of research.

6

CHAPTER 2

BACKGROUND - Literature Review of

3D visualization systems used in Disaster Scenarios

2.1 Introduction
This chapter describes and reviews three broad areas related to the thesis: Urban Search and Rescue, imaging technology, and data visualization technology. While the details on USAR and emergency management are not immediately important to understand the Voidviz system, they are vital to understand the context within which the technology is applied. Section 2.6 reviews the range of 3D

scanning technologies available, and sections 2.7-2.9 reviews available and related data viewing and manipulation techniques.

2.2 Emergency Management
The terms disaster, emergency, hazard are clearly related and are commonly used as synonyms by lay people. However, in the context of Search and Rescue, their meanings are very specific. A hazard is a thing or condition that is capable of causing damage to someone or something under certain conditions. For instance, a cut is not a hazard, however

7

the knife that does the cutting is. That is because the knife has the potential to cause harm, whereas the cut is the harm itself. Depending on the source, the term emergency has two meanings. According to McEntire in [4], an emergency is synonymous with disaster, a term that is defined below. However, in [5], an emergency is defined in as having two possible

meanings. The first is that an emergency is a small event that causes few casualties and a minor amount of property damage. This includes events like house fires or traffic accidents. The second definition is that of an impending event. For example, when a tsunami has been detected, but has not made landfall, this creates an emergency situation for which there is a limited amount of time to respond in order to limit what are likely to be major consequences. When the term emergency is used in this paper, the second definition will be used. A disaster is a deadly, destructive, and disruptive event that occurs when a hazard or multiple hazards manifest their potential harm when interacting with vulnerabilities of human life. For something to be considered a disaster, it must be so destructive that it cannot be handled by the resources of a single community. That community must reach out to larger organizations for assistance, usually government agencies of some kind that operate on a regional or national scale. A disaster usually causes large number of deaths and major property damage, whilst straining whatever local resources are available to mitigate the disaster [4, 5]. Properly dealing with an emergency is a complex job and requires specialized knowledge, skill, and expertise. Professional Emergency managers are employed

8

by the government to help reduce damages and to manage the response to disasters as they occur [4]. The emergency management process occurs in a four phase cycle; respond, recover, mitigate, prepare which can be seen in Figure 2-1. Emergencies occur between the prepare and the respond phases of the cycle. For large disasters, this can be a multi-year cycle, for example the damage from Hurricane Katrina in 2005 was so massive that the rebuilding still continues today, although it has merged partially with the prepare phase.

Figure 2-1 Emergency Management Cycle [6] The emergency cycle is broken down into four parts, starting with the Response phase. The Response phase is the phase which begins immediately after a disaster and first responders are taking action to protect the population and limit damage. First responders are typically police, fire and Emergency Medical Services. The Recovery phase, also known as the rebuilding phase, lasts up until the services and functions of the community affected by the event have returned to

9

normal. If the disaster is particularly large, this phase can take years, as evidenced by the long rebuilding times required in New Orleans after Hurricane Katrina. The time during which attempts are made to ensure that disaster does not occur again, or has a lower possibility of occuring is the Mitigation phase. To extend the New Orleans example, increasing the flood drainage pumping capacity and raising the levees would be an example of mitigation. Weakeness that allowed the disaster to occur in the first place are identified and remedied here. Preparation is the final phase. During this time, plans are made to dicate behavior and activities in the event of future disasters. These plans should help save lives and reduce property damage by telling people how to respond appropriately during emergency situations. While it may seem that this phase should come first in the cycle, in truth it does not matter. At any given point in time, the cycle is already occuring and which phase comes first is simply a matter of perspective.

2.3 Urban Search and Rescue
USAR involves the detection, extrication, and the initial medical stabilization of victims who cannot be freed immediately. Building collapses are the most

common cause of trapped victims, but people could potentially be trapped in any kind of failed man-made or even natural structure. This includes events such as mine collapses, car accidents, building collapses, fissures caused by Earthquakes, etc. Given the rarity of disasters within a Canadian context, maintaining a completely separate response team for every type of significant hazard would be impractical. It follows that any USAR team must be sufficiently adaptable to respond

10

to earthquakes, technological accidents, radioactive materials release, and many other types of disaster. Because of this necessary adaptability, USAR is considered a multi-hazard discipline.

2.3.1 Task Force Makeup
In most parts of the world capable of responding to a disaster, USAR activities are performed by teams called Task Forces (TF) which are self-contained units of up to 140 trained individuals. While the members of the task forces must be

available to perform in case of disasters, the force itself is not their primary employment. The TF will only gather when needed in an actual emergency or for training exercises. The members are drawn from a wide range of governmental and private agencies, such as the fire department, the police, or even private industry. Skills such as building collapse specialists, technical rescue specialists and heavy equipment operators are common. Their USAR skills are not necessarily related to their day jobs, but they must possess some form of emergency related certification to join a TF. In the United States, Federal Emergency Management Agency (FEMA) mandated TFs, each member of the team must specialize in one of 4 areas: Search: Responsible for locating victims. Rescue: Responsible for extracting victims from where they are trapped. Technical: Those who deal with structural reinforcement issues Medical: Provides any medical treatment required, for both victims and rescuers. USAR Operational Phases

11

There are three potentially overlapping phases to a USAR operation. The first phase is the Sizeup phase. This is where building collapses and structures are investigated to assess weaknesses and damage, and to see if there are any remaining hazards that are not structural in nature. While this is the first phase, it should continue during the other two phases, so that the rescuers are always aware of any changes in the safety of their environment. The second phase is the Search phase, where search techniques are focused not on locating individual victims themselves, but on locating possible areas of entrapment like voids. Voids can occur in a variety of ways, for example as a lean-to formation where a partially collapsed ceiling is supported by one wall, creating a triangular shaped void. They can also form from non-structural elements, like the spaces under desks, or inside bathtubs. Once the suspected voids have been locating, the rescuers will attempt to ascertain which of them contain victims by several methods. By having periods of silence during the search, they listen for the victims' cries for help. The rescuers will also employ canine teams to locate victims who cannot use sound to indicate their presence in a void. Buildings that have been searched are marked on the outside using a standardized marking system described in section 3.2.3.1.1. This prevents duplication of search efforts by summarizing the results of a search on the outside of a building. The final phase of USAR is the Rescue phase. This is where victims are removed from the voids, and medically stabilized should they require it. The

technical specialists stabilize the debris around victims so that a clear path can be made to them without disturbing the remainder of the collapse.

12

2.3.2 Ease of Use
USAR environments are, by definition, high stress environments where time is a critical resource. Any first responder will have a variety of activities competing for their attention. Because human error rates increase during stressful situations, it is important that critical tools are as easy to use as possible [7]. Where possible, tool interfaces should be made as human-centered as possible [8]. This means that interface should not be what is most convenient to construct, but what most closely matches how the user imagines the tool works [9].

2.3.3 Rubble Piles
Rubble piles can form in a variety of ways, depending on the type of building and on the way it collapses. A common type of pile is the pancake, whereby the floors from a multilevel building all collapse downwards while shattering the loadbearing walls [10]. The name derives from the visual similarity between the pile of floor/roof elements and a stack of pancakes. An example of this can be seen in Figure 2-2. Voids can form between the floor/roof elements. One of the larger problems with navigating through voids in large rubble piles is how dark and disorienting they are, which is illustrated in Figure 2-3. The hallway is strewn with debris from a partially collapsed ceiling. The dark shape with a point of light on it in the lower right-hand quadrant of the image is a rescue canine wearing an early model Canine Augmentation Technology harness [11]. Voids in concrete structures can be even more disorienting due to the lack of distinguishing features among broken concrete pieces and layers of dust generated during a collapse.

13

Figure 2-2 Pancake style building collapse

Figure 2-3 Void interior with rescue canine ­ Photo at UCRT exercise by Dr. Alex Ferworn

2.4 Situational Awareness
An important concept to consider in any high stress environment is situational awareness (SA). It can be defined as the perception of the elements in the

environment within a volume of time and space, the comprehension of their meaning and the projection of their status in the near future[12]. The definition clearly delineates the three levels that SA occurs on. The first is the correct Perception of

14

the cues in an environment. Without this fundamental ability, it becomes extremely difficult to form a correct picture of what is going on in a situation. The second level is the Comprehension of the cues that have been perceived. This encompasses the act of integrating multiple cues together and determining their relevance to the events at hand, if any. A reasonable analogy that compares level 1 to level 2 is the difference between reading words and understanding sentences. The third level is Projection, or the ability to forecast events based on what has been comprehended in level 2. This ability marks the people who have the highest level of knowledge of a situation. They have the ability to anticipate future events and make timely and appropriate decisions. This level of SA is the mark of highly trained operator [12].

2.4.1 Temporal Aspects
Given that one of the important aspects of SA is the ability to predict future events, gaining SA in a timely manner becomes very important. Predictions based on a particular situation that take too long to deduce might be made irrelevant by the passage of time. Being able to project quickly is especially important in Search and Rescue because there are lives in the balance.

2.4.2 SA and Decision Making
There is a significant distinction that must be made between SA and decision making. SA merely represents the mental model a human agent possesses that depicts all the important factors of an environment. What those factors are depends on what the agent is attempting to accomplish. Even with good SA, bad decisions can still be made. This might be due to lack of experience, training, or equipment

15

limitations, among other factors. For example, a 1995 study of human error in aircraft accidents indicates that 26.6% of incidents involved bad decisions, even when the flight crew appeared to have good grasp of the situation [13].

2.4.3 Who Needs SA?
The importance of SA first became noticed during World War I, and was derived from fighter pilots' view of combat. Since then, the concept has been expanded to many other fields. Anywhere that the volume of data exceeds the useful information in a system, SA is an important issue to consider. A building collapse is a particularly relevant example, as it is an environment that was once orderly but has been reduced to chaos. Any once-famliar cues about structural stability can become meaningless amongst the debris. Rescuers would need to be well trained in how to properly perceive a collapse to avoid making dangerous mistakes, thus it is vital that they have good SA.

2.4.4 How is SA Acquired?
The perception stage of SA can be derived from any sensory information, not just visual. Even other technological sensor systems can be brought to bear to gather information. These systems may include anything from service animal cues such as barking that indicates the presence of a live human in the pile, to infrared laser scanners that allow distances to be measured within a collapse. USAR training programs based on video game engines have been developed that are based on the assumption that learning to increase SA in a game leads to increased SA in the real environment [14].

16

2.5 Computational Public Safety
Computational Public Safety (CPS) is the application of computational resources, theory, and practice to support safety processes with the aim of enhancing them. Other CPS projects include CAT (Canine Augmentation technology), CRDS (Canine Remote Deployment System), Canine Brain Function, and response robotics [11, 15, 16, 17]. The CAT project seeks to increase the amount of useful information that rescue canines can return to their handlers by equipping the canines with wide angle cameras and wearable computers. The CRDS project seeks to equip rescue canines with pouches that can be automatically or remotely dropped to victims trapped inside building collapses. All these systems seek to improve the SA of first responders so that better decisions can be made during rescue operations.

2.6 3D Scanning Technologies
Before 3D structural data can be visualized, it must be collected. There are a variety of ways to gather distance information from some kind of ranging scanner. The most obvious one is to use binocular vision in the same fashion that humans do. However, the algorithms required to extract the 3D information from two 2D images is complex and subject to some limitations. While the images can be collected as fast as a camera can make them, extracting the 3D information is computational intensive. Also, smooth surfaced objects with uniform textures can be difficult for binocular computer vision systems to detect which can result in gaps in the scan

17

information [18]. The quality of the data collected can also be adversely affected by the lighting available when the images are taken. Structured light scanners are a way of gathering 3D information about a scene by using a projector to manipulate the lighting. A pattern of light is flashed over the object being scanned, and an image is taken for each pattern. Based on whether a given point on each image is illuminated or not for each of the images makes it possible to calculate its location relative to the projector and the camera. The location of the camera relative to the projector must be known with high accuracy, or else the depth map will not be calculated accurately. Ideally the projector would be the only source of light in the environment while scanning takes place, although infrared variations exist [19, 20]. The SICK [21] laser rangefinders were originally designed to produce a 2D scan along a plane. They use a laser and a rotating mirror to make a series of distance measurements along a plane. The entire scanner can be fitted to a servo, allowing the scanning plane to be rotated. Rotating the scanner permits 3 dimensional data to be captured. The higher the resolution of the rotating servo, the higher the resolution of the scan, but the longer it takes. The SICK rangefinder is LIDAR technology, which means that it utilizes the time of flight of the laser beam to make range calculations. For any scanning technology that uses light, the reflectivity of the object being scanned effects how well it is resolved. For instance, in any of the techniques mentioned above, a mirror would appear as a hole leading to the reflected image in the final scan [22]. This is because a mirror has high specular reflectivity, meaning

18

the reflection angle is the same as the incident angle. A surface with a high diffuse reflectivity shows up well under light based scanning technology. Conversely, a surface with no reflectivity at all could not be registered by the same technology.

Figure 2-4 SICK Laser S3000 model rangefinder[23]

The Hokuyo laser scanner uses a rotating mirror to sweep a low-power infrared laser beam in a circle. Due to the design of the scanner, parts of the device obscure the laser, resulting in an effective scan of only 682 measurements across 240º. Because using time of flight information to calculate distances would require expensive hardware capable of gigahertz level timing, the laser is subject to amplitude modulation and the resulting phase difference of the reflected beam is used to calculate the distance. This decreases the cost of the device significantly. The device itself can be seen in Figure 2-5. The centre of the 240º arc is

perpendicular to the visible right hand side of the device in the figure. The scanner is not typically suitable to outdoor work due to infrared interference from other light

19

sources. However, the voids in building collapses are typically very dark, which makes the Hokuyo scanner quite suitable, unlike a binocular system which requires good lighting.

Figure 2-5 Hokuyo URG-04LX-UG01 laser scanner[24]

The crime scene scanner in [3] performs similar activities compared to the Voidscanner and Voidviz. They both produce simulations of environments so they can be analyzed and assessed by people not physically present in the real environments. There are several differences though. The crime scene scanner

utilizes a pair of stereoscopic cameras and specialized light projector to provide texture to matte surfaces, whereas the Voidscanner only uses a single laser scanner. Because of this, the crime scene scanner is a much bulkier and more expensive device. This bulk would make it difficult to maneuver and use inside a void.

2.7 Game Engines and Visualization
The maturity of the video game industry has made a plethora of powerful game engines available for modification and use. A game engine of particular versatility is the Unreal engine by Epic Games [25], which was developed for the video game Unreal-- a first-person shooter game. Its active modification community and low

20

price make it ideal for many simulations [14]. Wang et al [14] used it to create a virtual representation of the NIST robot test facility at the University of Texas. Making this simulation widely available allows robot builders to test their machines at times when they are not physically able to visit the facility. However, the

effectiveness of this simulation is dependent on how accurately the dynamics of the robots can be modeled. Besides just testing robots, some first responders are being trained to handle uncommon threats like a fire in a nuclear facility using the same engine [25, 26]. Essentially, a first-person rescue game is built around a model of a nuclear facility. They are given realistic tasks and can cooperate in a multiplayer setting. Despite the versatility of the Unreal engine, it is still a proprietary engine that is released in its binary form only and so allows for very limited customization. The Delta3D engine is a modular and completely open source engine [27]. It integrates a series of other open source components together under a unified Application Programming Interface (API). The user has access to all of the low-level

functionality of each module to allow effective customization, while providing a high-level API to simplify simulation creation. It has a very active user base and has been applied to USAR scenarios in the past, although with different aims than this thesis [28].

21

2.8 Viewing 3D Datasets
2.8.1 Raster Data
Raster data is any type of data that is represented by grids. For instance, common image files like bitmaps are raster files because they contain a grid of pixels. While in a black and white image file, the pixel represents the intensity of the light at that point, it need not always be so. The pixel could represent something not visible, like temperature. It could even represent an abstract quality, like if the raster file were of a map; the pixel could represent the way the land at that point is being used. This can be seen in Figure 2-7A, where the value of each cell indicates the terrain type. The concept of raster representation can also be extended to 3

dimensions, creating volumetric data. This kind of data could be acquired from magnetic resonance imaging. Viewing this kind of dense 3D data can be problematic though, as it is impossible to view in detail by simply reconstructing it in 3D and rendering it on a 2D screen. Some data will always be occluded by other data being situated between it and the viewer. This is usually resolved by displaying slices made through the data, or by not displaying data that match some undesired attribute. Ratti [29] created a novel interface to view dense data, called the Phoxel-Space [29]. Phoxel is a portmanteau of Physical Voxel. The term voxel means volumetric pixel and is an extension of the 2D pixel concept to 3 dimensions. Where a pixel on a bitmap defines the attributes of a square area, the voxel defines the attributes of a cube in space. A voxel map can be used to store the information about a field of non-homogenous data in a way that is uniformly accurate to the resolution of the voxels. Storing the equivalent information as a series of 3D polygons could require a

22

prohibitive amount of memory. As described earlier, viewing the inside of a voxel map can be problematic, as one is left with the problem of how to sensibly view the inside of a solid object. The Phoxel-Space user is presented with some kind of malleable surface, possibly made of sand or clay, which the voxel data is projected on to. Some kind of infrared laser scanner like a Vivid 900 is used to generate a height map of the surface, which is used to determine what data from the voxel map should be projected. This height map is updated frequently. As the user digs deeper into the surface, data from deeper inside the voxel map is projected. In this fashion, one could literally dig through a magnetic resonance scan of a brain to find nonplanar cross-sections. This can be seen in Figure 2-6.

Figure 2-6 Phoxel-Space interface showing an MR scan [29]

2.8.2 Vector Data
Not every 3D dataset is dense enough to necessitate the use of a voxel map. Some kinds of data lend themselves well to be stored as Vector data. Vector data is

23

stored as a series of points, lines, and polygons, so anything whose shape can be described as such would be a good candidate to be stored as Vector data. In Figure 2-7B, the terrain types are divided into separate polygons. The lines themselves do not have any attributes in this case, but the regions they encircle do. Because design data generally deals with regular shapes, not arbitrarily detailed solids, it can be stored efficiently in vector format. Vector data has the advantage that it can be scaled easily. This kind of storage is commonly found in programs like AutoCAD and Google Sketchup [30]. Forte et al [31] built a viewer to display a variety of vector data derived from archaeological sites at Pompeii [31]. Architectural data gathered from computerized theodolites called total stations (see sec 3.1 for further details on total stations) is used to model the gross features of the buildings. The modeled walls are then overlaid with site specific local raster data, like high resolution photographs and construction details. This combination of vector and raster is combined so that the vector data can give a coarse model of the site and the raster data can provide a more accurate visual representation of areas of interest.

24

A

B

Figure 2-7 Comparison between Raster and Vector approximations of a map

2.9 Scan Matching
Scan matching is when two laser scans made from separate locations are combined together into one coherent map. This can be done by hand or by using one of the scan matching algorithms. Iterative Closest Point matching is the most

popular form of scan matching algorithm and it operates by attempting to minimize the equation below [32]. For the purposes of the explanation, assume the two scans being matched are called scan A and scan B. In the equation, xi represents point i from scan A, and point yi represents the point i in scan B that is closest to point xi. R and t are a given rotation and translation being applied to scan A. The goal is to find an R and t such that F(R,t) is minimized.

25

Equation 1
F ( R, t )  1 N  Rxi  t  yi N i 1
2

The minimization is performed through the following procedure: i. The xi,yi matching closest pairs are calculated. ii. R and t are found such that F is minimized. iii. R and t are applied to all points of x. iv. If the current minimum of F is not below some maximum threshold, return to step i. Else, the algorithm is complete.

Step ii can be performed using optimization methods such as steepest descent, or conjugate gradient, although there are more efficient ways for this particular equation [33]. ICP can be applied to data with any dimensionality, including 2D and 3D, as long as they can be converted to point format. There is a substantial increase in processing time requirements for every increment in the number of dimensions. Like many iterative optimization algorithms, it can be prone to becoming trapped in local minima that are not acceptable solutions. There are techniques to reduce the possibility of this problem, but they are beyond the scope of this work. ICP can be used in SLAM (Simultaneous Localization And Mapping) implementations to derive the relative motion of the scanner between scans [34]. It can serve as a replacement or be combined with the Kalman filters that are also commonly used in SLAM algorithms. With the difficulty of using a gyro and

26

odometry on rubble piles, ICP could be used as a useful replacement to provide SLAM [35].

2.10 Surface Reconstruction
Laser scan data derived from a rotating SICK range finder takes the form of a cloud of points in three-dimensional space. The points themselves lie approximately on the surfaces of the scanned environment, but at this point they have no relationship to each other. Surface reconstruction is performed to turn the point cloud into a complex polygonal surface called a triangular mesh that better approximates the scanned surfaces. The mesh consists of the points from the point cloud connected together by edges that were derived through a surface reconstruction algorithm. While viewing a surface-less cloud of points can still be useful, a proper mesh representation of the scan results in more intuitive viewing, as well as providing useful characteristics like allowing additions to be made in the form of markups. The ball and pivot algorithm [36] is a simple and robust method of calculating where an edge should be placed between the points, also called vertices. This creates triangular polygons between selected triads of points. In a naïve

implementation, a radius is selected which reflects that average distances between the points. Then all the triples of points are examined. If all the points of a triple can lie on the surface of a sphere of the chosen radius with no points lying inside the sphere, then they are connected to each other with edges [36]. While other

algorithms are also available for surface reconstruction, like the marching cubes algorithm [37], ball and pivot was used because of the availability of the implementation.

27

2.11 Summary
In order to provide a better understanding about the incentive for this thesis, background information on Emergency management, USAR, Situational awareness, and computational public safety were presented. In addition, information about the technological basis for the thesis was also presented; including 3D scanning devices and visualization engines. Much of that technological literature relates to their applications in Search and Rescue work. Some key computer science principles that were addressed were scan matching, data storage formats, and surfacing algorithms.

28

CHAPTER 3

MATERIALS AND METHODS

3.1 Overview and System Architecture
This thesis is predominantly about visualizing the voids inside rubble piles, and the Voidviz application was developed to that end. However, in order to test this application, some way of scanning the interior of voids in rubble piles was required, separate from what was needed to visualize the data. Because of the confined nature of voids, the technology must be both small and portable. This chapter describes two such devices that meet those requirements. Section 3.2.1 describes the Total Station, a surveying tool, and section 3.2.2 describes the Voidscanner, laser scanner built specifically for this thesis. Both of the devices are based around infrared ranging laser technology and can produce accurate location information about the contents and walls of a void. Once the data has been collected, the Voidviz program,

described in section 3.2.3 was used to view and manipulate the data in ways that would be useful to first responders. The key to this system, operates based on the architecture seen in Figure 3-1. Scan data, consisting of either of a data file of ASCII encoded 3D points or a 3DS model file, is fed into the SMC (Surfacing and Model Creation) algorithm to be converted in to a renderable format. If required, a surface is applied to the scan data and the resultant model is passed on the simulation engine. Several sets of scan data can be passed to the simulation engine to be rendered simultaneously. Once all sets of scan data have been processed by the surfacing algorithm, the simulation engine then renders the models and the user proceeds with their virtual exploration.

29

Scan Data

Surfacing and Model Creation Algorithm

Simulation Controls Simulation Engine Simulation Visuals

User

asdfsdf

Figure 3-1 Voidviz Architecture

3.2 Materials Overview
3.2.1 The Total Station
One of the devices used in this experiment to scan the interior of a rubble pile was the Sokkia 530R Total Station, a computerized theodolite. A theodolite is a

surveying device used to measure both horizontal and vertical angles to a very high degree of accuracy. Given two sightings of a point of interest, trigonometry can be used to determine the coordinates of that point, relative to the sighting locations. With the addition of an infrared based ranging laser, a theodolite no longer requires a second sighting location to calculate the relative coordinates of a point of interest and becomes known as a tacheometer. All three coordinates of a distant point can be measured at one time, although in a spherical coordinate system and relative to the device. The Sokkia Total station is an intelligent self-registering tacheometer that is equipped with a computer capable of converting the spherical coordinates to a Euclidean reference and storing them on a memory card for later use. Currently, the device is used by the OPP to chart all the elements of car accident aftermaths so there 30

is an accurate record of their locations when required. Typically, this data may be required during court proceedings to present accurate information concerning motor vehicle accidents in question. This device was used in this thesis because of its availability. The police officer in charge of its operation agreed to operate the unit during experiment 1 to gather the scan data. This Total Station is a reflectorless unit, which means it does not require a reflector at the point it is trying to register. It relies on the innate reflectivity of the target. This limits its range to 100m, which more than meets the 10m distance requirements of this experiment.

Figure 3-2 Sokkia 350R Total Station

31

3.2.2 The Voidscanner

3.2.2.1 Voidscanner Construction
The scanner consists of a Hokuyo URG-04LX-UG01 laser scanner attached to a stepper motor via a gearing system. The stepper motor is a 4 wire 6V system and is capable of a maximum of 0.9º of resolution. Using a gearing system with a ratio of 15:72, the resolution of the stepper motor is increased to 0.1875º per step. The scanner is fitted to the larger gear via a 90º L bracket so that it is in approximate alignment with axis of rotation. Ideally, the L bracket would be exactly 90º and would place the axis of rotation directly through centre of the scanner, however there were small deviations. These deviations were measured and included in the

calculations used to transform the coordinates from a local frame of reference of the scanner to a global one. The Hokuyo laser is a USB device that communicates via a USB port emulating a serial port. While there is no physical reason it could not interface with a USB equipped microcontroller, this would be problematic in practice due to the copious amounts of data generated and a very high transfer rate. With a scan rate of 10Hz and a data transfer rate of 38.4k baud or greater, it works best when communicating directly with a laptop or PC [38]. The low level serial

communication between the laser and the PC is handled by an off-the-shelf piece of software from the MRPT (Mobile Robot Programming Toolkit), which contains a specialized driver for communicating with the laser [39]. When requested to by the main body of the Voidscanner program, this driver passes a list of 682 distances from the Hokuyo laser to the scanning program. These distances are then transformed into

32

their 3D coordinates in a global frame of reference and stored in a data file. Further details of the coordinate transform can be found in section 3.2.2.2. The coordinates are stored as vector data. Because each coordinate is located to 6 significant digits, storing a scan within only a cubic meter would require on the order of 1011 megabytes, most of which would consist of empty space. That is a ridiculous and impractical memory requirement for an application like this. The scan data from the Voidscanner at high resolution requires less than 10 megabytes when stored as vector data, which is a much more reasonable value. The stepper motor is controlled via an Arduino microcontroller that is in turn controlled by a Serial port emulating USB port on the same laptop that controls the Hokuyo scanner.

Figure 3-3 Voidscanner with the USB cable unplugged

3.2.2.2 Frames of Reference and Coordinate Transforms
The driver software for the Hokuyo scanner outputs data in a 2D frame of reference that is relative to the centre of the scanner and must be transformed into a global frame of reference. In the VoidScanner software, the set of coordinates from

33

a scan is stored as a C++ Standard Template Library vector. Figure 3-4 shows the initial relationship between an untransformed scan and its local frame of reference. The three transformations that are applied to the scan are as follows: 1. Rotate about local Z axis by 2.635º 2. Translate along Z axis by 5mm 3. Rotate about local X axis by the current stepper motor rotation

The rotation in step 1 is performed to correct a misalignment in the scanning device itself [22]. The 682 scanned points are spread evenly along a 240º arc around the Hokuyo. Ideally, the 120º position of the arc should be facing straight out from the front of the scanner, but this is not the case. All the scans must be rotated by 2.635º clockwise about the Z axis to ensure that the true midpoint of the arc is in the correct position. This is illustrated in Figure 3-4. If this correction is not made, then the final 3D simulation of the data shows a distinctive twisting effect that causes the first and last scans to misalign. The translation in step 2 is necessary because the plane of the scanner does not coincide with the axis of rotation of the stepper motor. The final transformation in step 3 is the rotating the scans about the X axis by an amount equal to the current rotation of the stepper motor. This value changes each time the stepper motor rotates another increment and will have values from 0º to 180º.

34

Figure 3-4 Laser Scanner Alignment Correction

3.2.2.3 Error Mitigation
It has been shown in [22] that the orientation of the Hokuyo scanner relative to the direction of gravity causes errors of up to 2cm in the measured distances. Given that the Hokuyo rotates on a stepper motor, compensating for the error would be very complicated if the force of gravity changed relative to the scanner. The measurement error was minimized by positioning the Hokuyo laser such that its front always faced upwards. This can be seen in Figure 3-3. The front is considered to be the direction of the centre measurement from the 240º arc of measurements, allowing for the deviations noted in 3.2.2.2. As a result of this, the 120º that are not scanned create a cone shaped blind spot that would obscure whatever tripod or mounting device the scanner was attached to.

35

3.2.3 Void Scanner Visualizer
The void scan visualizer, or Voidviz, is built on the Delta3D video game engine. The visualizer can take in a variety of data files as inputs, including common file formats for 3D models, like flt, 3ds, obj, and render them appropriately. Once the scans have been rendered, the user can perform virtual walkthroughs of the data and leave visible mark-ups on the surfaces, similar to the way first responders use spray paint to mark the interiors rubble piles when they perform their actual walkthrough. However, with the Voidviz, they are in complete safety. Given that the visualizer is meant to be used in high stress search and rescue situations, it is meant to run on basic and robust computing systems. A ruggedized laptop was used for the experiments in this thesis. The Delta3D engine was selected for this project over other more frequently used engines for several reasons. More commonly used software like the Unreal engine would require any 3D models it uses be pre-prepared by a separate program [25]. This over complicates the model generation procedure. With Delta3D,

Voidviz has been programmed to generate models based on data files of 3D points. The Delta3D engine is maintained by the United States Navy, meaning that it is under continuous improvement. There are also advantages to using Delta3D when viewed from a financial standpoint. When Voidviz is sold as a product, the fact that it is built on an open source engine means no licensing fees need to be paid to its programmers.

36

3.2.3.1 Voidviz Functions
3.2.3.1.1 Surface Markup
During the course of their work, first responders in a rubble pile would periodically make marks on the walls to indicate areas of danger and what needs reinforcing. Typically, this is done using either spray paint or lumber crayon [10]. Voidviz replicates this with virtual spray paint, allowing coloured markers to be placed on the walls. Different symbols can be drawn, depending on the requirements of the user. The symbol shown in Figure 3-5 indicates that a nearby victim has been confirmed to be alive according to FEMA marking systems [10]. The scan was taken from the interior of a simulated rubble pile using a similar scanning technology to the Voidscanner, but visualized using Voidviz.

Figure 3-5 Markup indicating victims confirmed to be alive

3.2.3.1.2 Measuring
Another useful tool for a first responder in a rubble pile is a tape measure. It is important to be able to accurately describe to other responders the dimensions of

37

important void characteristics, for instance the ceiling height or the distance between joists. This information is useful when determining the appropriate shoring and reinforcing equipment. The Voidviz comes equipped with a virtual tape measure that can be used to measure the distance between any two points that are located on the surfaces of rendered objects.

3.2.3.1.3 View Modes
There are two different scan viewing modes: First person, and orbital. In the first person mode, the user can explore the virtual environment in a manner similar to a first person video game. Yaw and Pitch can be controlled with the mouse, and x, y, and z translations relative to the user's viewpoint can be controlled with the keyboard. The view can be toggled to the orbital mode by pressing the appropriate key. In this mode, the viewpoint can be moved around a sphere that is centered on the position that the viewpoint was at during the first person mode. The radius of the sphere can be changed in order to provide the most effective position for observation in the circumstances. A first person view alone can, on occasion, be too confining a viewpoint. The orbital mode allows user to get an outside viewpoint without having to shift the location of the first person view. The ability for a first responder to step out of their own centre of attention enables them to observe details they might otherwise not be able to see.

3.2.3.1.4 Surfacing
Two techniques were used to apply surfaces to 3D points collected by the Voidscanner. The program Meshlab, by the Visual Computing Laboratory at the

38

Istituto di Scienza e Tecnologie dell'Informazione used the ball and pivot surfacing algorithm that was referenced in section 2.10. No guarantees can be made about the accuracy of the edges, but the results were generally effective. This algorithm did suffer from one problem which resulted in blurred details on edges and points near the scanner when the bulk of the points were further away. The second surfacing algorithm leveraged knowledge of the number of points per scan and number of scans to deduce which points were adjacent to each other and how to connect them in a way that was consistent with the actual scanned surface. It is important that these surfaces are generated because the virtual spray paint only works when there are virtual surfaces to apply it to. Figure 3-7 and Figure 3-6 show the same scene, both without and with surfaces. There is a much greater sense of immersion with the surfaced image than the image with only points.

Figure 3-6 Scene without Surfacing applied

39

Figure 3-7 Scene with Surfacing applied

3.2.3.1.5 Simulated Lighting
Using the existing lighting functionality of the game engine, the user is equipped with a simulated flash light that can be used to make surface details stand out more easily. The user can also place a fixed omni-directional light that simulates a powerful work light. Every generated surface has a designated normal which determines how the light reflects off it. With the Delta3D engine, the normal doesn't necessarily have to be perpendicular to the surface, but for this application it is.

3.2.3.1.6 Map Component Moving
The same first-person control scheme that applies to the user can also be cycled through any of the rendered scans. For instance, a series of different scans can be displayed simultaneously, and then maneuvered into their correct positions using the keyboard and mouse. Theoretically, ICP could be used to position the scans

automatically, but in practice this would have been too unreliable for a USAR application.

40

3.3 Methodology
Thanks to the cooperation of the OPP, a UCRT training facility was used as a test site for much of the scanning equipment. The training facility is a simulated rubble pile constructed from concrete sewer culverts connected to form a single network. This network of tunnels was then buried under a variety of debris,

including a bus, various kinds of concrete, rebar, gravel, and cars. Figure 3-8 shows a portion of the surface of this facility. The scanners were used in various locations within the concrete pipe network below the debris. For experiment 1, the Total Station was operated by a by an OPP officer who had volunteered to help test the capabilities of the unit beyond what it was normally used for. The Total Station was placed in several locations around the interior of the training facility and used to scan pertinent physical features of the environment. This data was then assembled into a 3D model using a CAD (Computer Aided Design) program and loaded into Voidviz, where its fidelity and usefulness were tested. The Total Station was selected because it was a scanner that was available as a loan at zero cost due to a friendly collaborative relationship with the OPP. The Voidscanner was constructed when it became apparent through experimentation that the Total Station had limitations that made it impractical. Experiments 2 and 3 involved similar procedures to Experiment 1, but with expanded aims. The Voidscanner was used inside the UCRT training facility to make several scans. Rather than being assembled together using a CAD program, they were assembled using functions built into Voidviz.

41

In all the experiments, photographs were taken of features of interest around the scan sites. The detail of the Voidviz simulations were tested by

comparing the photographs to the digital recreations of the same perspectives.

Figure 3-8 Surface of UCRT training facility

3.3.1 Practicalities
When Voidviz was used with the Total Station, it required an impractically long period to generate the scans. During the scanning period of experiment 1, approximately 100 points were registered by the Total Station over three hours. It should be noted that the Total Station was operated by an individual trained and experienced with the device, so it should be assumed that the length of time required was not fault of the operator. In addition, the operator of the Total Station was required to spend much of this time crouched in an uncomfortable position in a cold and wet environment. This is a stark contrast to the Voidscanner, which could scan 327360 points in approximately two minutes. The operator of the Voidscanner

42

would have to vacate the area while scanning was in progress. A time delay was added to the start of the scanning sequence to allow for this. Once the scans were obtained, the simulation could be used approximately five minutes later, even at high surfacing resolutions. This gives the system a very fast turnaround time, which is an important consideration when delay costs lives. The surfacing algorithm is limited to scans where the actual kinematics of the scanners are well known. Because the algorithm exploits this knowledge to calculate which points are adjacent to each other, it cannot deal with surface scans that were made with an unknown scanning device.

43

CHAPTER 4

EXPERIMENTS AND RESULTS

While chapter 3 details the materials and software used in this thesis, chapter 4 explains the experiments and the results obtained by using Voidviz with different sets of data scans and different kinds of scanners. With each new set of scans, the accumulated experience is used to guide the development of Voidviz. The

experiments are classified according to the target of the scan set and the type of scanner. The first experiment uses a very sparse data set created by the Total Station. The second experiment uses the Voidscanner in the same environment that experiment 1 occurred in. The third and final experiment shifts from victim

recognition to bomb recognition.

4.1 Experiment 1 Victim Recognition Using Total Station Data
This experiment was conducted in a training facility at UCRT headquarters in Bolton. It is meant to serve as a proof of concept to investigate the difficulty in recognizing a victim in a laser scan of a void using Voidviz.

4.1.1 Facility
This experiment was performed in the UCRT facility described in section chapter 1. Figure 4-1 shows an image of the tunnel network before it was covered in debris. It is constructed from common concrete sewer pipes, arranged in a square pattern. The main entry point is the opening at the bottom centre of the image. This tunnel network is used by UCRT during their confined space rescue training

44

exercises and is often used to test new techniques and technologies. When the manholes visible in Figure 4-1 are covered, the interior is sufficiently dark that navigating without a light source is difficult and moderately hazardous. It is

designed to be similar enough to a building collapse for training purposes while still being structurally sound and safe to work in given that approved personal protective equipment is worn1.

Figure 4-1 Test Site Overview (Before addition of debris)

4.1.2 Setup
The physical parameters of the void were collected using the Total Station (TS) described in section 3.2.1. Because the TS requires each point to be registered manually, the simulation had to be created using a very sparse number of points due to the time required. The approximately 100 registered points required roughly 3

1

Personal Protective Equipment within this space is defined as CSA approved safety helmet and steel-toed boots, gloves and knee pads. 45

hours to collect. Three locations within the void were selected that maximized the possible coverage of the TS and allowed the void to be recreated. Scan set one detailed the circumference of the floor of room one, along with the length and shape of the tunnel connecting room one to room two. The TS was placed in the approximate centre of room 1 for this scan. Scan two detailed the circumference of the floor of room two, along with the connecting edge of the connecting tunnel. The TS was placed in the approximate centre of room two for this scan. Scan three traced a rough skeleton of a simulated victim in room two along with some points of reference that allowed it be localized relative to the points from scan two. The TS was placed just inside the mouth of where tunnel two connects to room two for this scan.

4.1.2.1 Virtual Environment Construction Using Google Sketchup
Based on the sparse sets of data gathered by the TS, a more detailed model of the void was created that took advantage of the simple geometric shapes that were used to construct the void. Because the void was a training facility created from concrete sewer culverts, most of the shapes involved were near-cylinders. Therefore, in order to determine their dimensions, only points on the edges of the cylinders needed to be captured. Google Sketchup was used to merge the three scans and to add surfaces to the model [30].

46

4.1.2.2 Environment Construction Results
Once the three scans described in section 4.1.2 are merged and surfaced, they result in the model seen in Figure 4-2. The blue, red and green lines indicate the local axis of the model. The modeled area itself represents the area outlined by the red line in Figure 4-1, although the area is being viewed from roughly opposite directions. Only half the wall of room one was modeled. It should be noted that the simulation shows only the internal dimensions of the tubes from Figure 4-1 and does not take in to account the thickness of the concrete. This explains the apparent difference in length between the simulated tunnel one and the real tunnel one.

Figure 4-2 Simulation Overview

4.1.2.3 Victim Visualization
This experiment was also an attempt to see if a human could be recognized in a simulated void as, ultimately, this will be one of the tasks of this form of technology. Figure 4-4 shows an overhead view a human shape (the victim) slumped against a

47

wall at the bottom of room two. Figure 4-3 shows the resulting simulation of the victim after it has been scanned by the TS, processed, and displayed in Voidviz. The simulated victim can be observed as a red stick figure. Tunnel one can also be seen leading away from the left side of both figures. Figure 4-3 clearly shows a

representation of the victim in what is recognizably the same position as in the photograph next to it. Figure 4-5 and Figure 4-6 show the same victim in the same position, but from a different perspective. In this case, the viewpoint is inside tunnel two and pointing towards room two. As in the other viewpoint, the simulated victim can be observed clearly as a red stick figure. This demonstrates that under certain circumstances a victim can be identified in a simulation of a void by using laser scanning technology, thus potentially increasing the situational awareness of the rescuers.

Figure 4-3 Simulated Overhead View of Victim

Figure 4-4 Actual Overhead View of Victim

48

Figure 4-5 Simulated View of Victim through Tunnel

Figure 4-6 Actual View of Victim through Tunnel

4.2 Experiment 2 UCRT Facility Scans with Voidscanner
This experiment uses three extremely detailed scans of the UCRT facility that were taken using the Voidscanner. The rotational resolution of the Voidscanner was set to 480 steps across 180º, with 682 points per step. This resulted in scans of 327360 points each. These scans will be loaded into Voidviz in order to see what kind of useful information can be extracted. This experiment was performed at the same facility as described in section 4.1.1, with the test victim in the same room, although not in the same position. The scans were taken from the following positions: Scan 1 was taken from the centre of the floor of room one. Scan two was taken from the centre of the floor of room two. Scan three was taken from just outside the entrance of room one, approximately below the car in Figure 4-7. This figure depicts the entrance to the tunnel network shown in bottom centre of Figure 4-1 after it has been covered by debris.

49

Figure 4-7 Testing facility entrance, post-burial by debris

4.2.1 Victim Visualization
This experiment is an attempt to discover how changes in resolution alter what details can be derived from a scan of a victim. The victim is photographed while they are positioned as they were for the scan, and then simulated from the same perspective at different resolutions. Observations are made to determine what effect the different resolutions have on the image features.

4.2.1.1 Experimental Results
Figure 4-8 shows the image of the test victim slumped against the inner wall of room two. The Voidscanner was positioned on the floor in front of them at Figure 4-9 to Figure 4-13 show the various different

approximately 1m away.

50

simulations of the victim to be compared against reality. Figure 4-9 displays the victim rendered using only the specific points registered by the Voidscanner. The grey object at the top of the image is a different piece of the simulation that is visible through the transparent wall of points and should be ignored for this experiment. The blank circle on the floor in front of the victim in all the simulated images is the blind spot of the Voidscanner mentioned in section 3.2.2.3. Figure 4-10 shows the victim in a simulation where only 25% of the points are used for the surface rendering. This means that out of the total 327360 points, approximately 78724 were used to render 157448 faces. Specifically how many were used to render the victim cannot be determined because there is no way to distinguish individual points on the victim from the surrounding rubble. The pose of the victim is still quite clear, but much of the surface detail has been lost. Figure 4-11 shows the victim rendered using 33% of the points, creating a simulation using 210566 faces. Surface details on the chest of the victim have become a little smoother and less blocky, though there is little useful difference from the 25% simulation. Figure 4-12 shows the victim rendered using half the available points, creating a surface of 315866 facets. The surface details of the victim continue to become more apparent, for example the wrinkles in the right shirtsleeve of the victim have become distinguishable. Figure 4-13 shows the victim rendered using 100% of the points. Higher surfacing resolution using the existing data is not possible and creates a model using 633670 faces. The resulting surface is extremely detailed, with wrinkles in the

51

victims clothing becoming detailed. Detail about the position of their right hand also becomes discernible, as well as the radio harness on their chest. At this level of detail, large wounds may become visible. In all the surfaced images, the simulated work light was positioned to enhance the shadows cast by smaller features and to bring out their detail.

52

Figure 4-8 Example Victim

Figure 4-9 Victim rendered in Points only

Figure 4-10 Victim rendered using 25% of points

Figure 4-11 Victim rendered using 33% of points

Figure 4-12 Victim rendered using 50% of points

Figure 4-13 Victim rendered using 100% of points

53

4.2.1.2 Discussion
While the maximum surfacing resolution shows a great deal of detail, it also uses a prohibitively large number of faces. More surfaces in fact, than many

graphically intensive video games. Using this level of detail for displaying large, mutli-scan simulations is currently impractical. However, reducing the level of

surfacing detail to 33% leaves the victim clearly recognizable without overburdening the simulation computer.

4.2.1.2.1 Laser Shadow Effect
It can be seen in Figure 4-9 that the victim's knee seems to extend far enough to obscure the ladder on the left side of the image, while it does not in Figure 4-8. This is an example of a laser shadow, where the space between the victim's leg and the ladder are obscured from the Voidscanner due to its perspective. The surfacing algorithm assumes that pairs of points within a certain maximum distance of each other are part of the same object being scanned and should therefore be connected by an edge. The distance between the victim's leg and the ladder is within that distance, so they have surfaces drawn between them. Incorrect edge placement like this can be reduced by decreasing the maximum linking distance in the surfacing algorithm, although this leads to more anomalous holes in the completed surface.

4.2.1.2.2 Errors Caused By Oblique Reflection Angles
It can be seen that the rounded upper surface of the simulated helmet in Figure 4-9 does not match the corresponding areas of Figure 4-8. This is because the curve of the surface of the helmet and its high specular reflectivity. The more oblique the part of the helmet the laser hits, the more likely that it will not reflect back with

54

sufficient intensity for the ranging data to be registered. This causes the areas of the helmet that curve away from perpendicular to the laser to seem to fade away in the scan data.

4.2.2 Modeling an Access Path
Another important feature of the Voidviz to test is whether or not it can be used to reassemble scan data and then model an access path to a potential victim. It would be of benefit to first responders if they could plan a potential rescue route through a detailed virtual representation that does not require them to be in the collapsed structure themselves. We argue that this could reduce the amount of time required to plan a rescue, help determine a feasible path without endangering humans. It should be acknowledged that with a real collapsed structure, there would be difficulty getting the Voidscanner or Voidscanner-like device into the cramped cavities of a void. Solving this problem is beyond the scope of this thesis. The scan manipulation functions of Voidviz from section 3.2.3.1.6 were used to reassemble the three scans described in section 4.2. The resulting map is seen in Figure 4-14. The scans themselves were taken from three locations. One scan was taken from just inside the entrance to the training facility, at the location marked by the green and red axis indicator in Figure 4-14. The second scan was taken with the Voidscanner in approximately the centre of room one. The final scan was taken in the centre of room two. In all three scans, the Voidscanner was mounted on a small tripod, approximately 27cm tall. This scanner setup can be seen in Figure 3-3.

55

Figure 4-14 Reassembled Scans Room one and room two in Figure 4-14 can be easily identified when the map is compared to Figure 4-1, an actual photograph of the training facility. The irregular shapes in the lower right of the figure above show a pile of debris that conceals the entry to the network from casual viewing. That debris was added after the initial tunnels system was constructed and cannot be seen in Figure 4-1. The tunnel on the right side of the image is designated tunnel three, and connects to room three. The small unattached piece at the far right of the image above represents one side of the tunnel that traverses room three from front to back.

4.2.2.1 Procedure
A test victim was placed inside room two of the training void, slumped against a wall. A series of photographs was taken which simulated the progress of a rescuer as they proceed through void to the victim. Using the Voidviz, an attempt was made

56

to create an identical trip through the simulated version of the training void seen in Figure 4-14.

4.2.2.2 Results and Discussion
A rescuer attempting to penetrate the training facility would first have to travel through the entrance to room one shown in Figure 4-15. The circular portal is clearly visible in Figure 4-16, the simulated version of the same area. A piece of debris wedged in the left side of the doorway is visible in both images and shows an example of laser shadowing. The visibility of this block indicates that further debris might also be clearly identifiable. The circular arc of blue at the bottom of the simulation image is the edge of the blindspot of the Voidscanner. Once through the portal and inside room one, the rescuer could turn and look to the left and see the victim at the other end of the tunnel. Figure 4-17 shows the view from inside room one, down tunnel one, and towards room two. It should be noted that the photograph of tunnel one was taken during experiment one when the victim was in a different position, while the scan shown in Figure 4-18 was taken explicitly for this experiment. This accounts for the difference in victim poses between the two images. Besides the differences in victim pose, the two images are recognizable of the same location. Tunnel rim detail is visible in both images and the ladder on the far wall can also be seen. A large hole is visible in the simulation of tunnel one. This is because both scans that make up this section were taken with the Voidscanner only slightly higher than the bottom of the tunnel. As a result, the reflection angle becomes too oblique for the scanner to receive a return and a gap appears in the scan

57

data.

This shows that even a relatively rough and non-reflective material like

concrete can't be detected when the reflection angle is oblique enough. After a rescuer proceeds through the tunnel into room two, they are faced with the victim seen in Figure 4-19 and simulated in Figure 4-20. The full experiment detailing victim recognition can be found in section 4.2.1. This experiment shows that based on a set of reassembled scans, first responders could use the system to model an access path through a debris pile to a victim.

58

Figure 4-15 Training Facility Entrance way

Figure 4-16 Simulation Training Facility Entrance way

Figure 4-17 View down Tunnel 1 to Room 2

Figure 4-18 Simulation View down Tunnel 1 to Room 2

Figure 4-19 Victim

Figure 4-20 Simulation Victim

59

4.3 Experiment 3 Bomb Component Identification
Not all building collapses have causes as impersonal as earthquakes or fires. Bombs have been used in the past too great effect to demolish buildings. A notable example is the Oklahoma city bombing in 1995, caused by an explosive filled truck, the aftermath of which is visible in Figure 4-21. This experiment is to test the Voidviz's ability to recognize the components of two different kinds of bombs scanned by a detector placed in an inverted position.

Figure 4-21 Oklahoma City Federal Building as seen in 1995, Post­bombing, predemolition

60

4.3.1 Experiment Setup and Device Description
The Voidscanner is attached to a 41cm by 21cm platform in an orientation upside down compared to that shown in Figure 3-3. The platform is suspended from the ceiling by pulley system that is attached to the platform by three points. This arrangement can be seen in Figure 4-22. The platform is stabilized by two ropes that are fixed to weights on the floor. This may seem like a rather odd setup but it was selected as it has stability characteristics that are similar to those that might be found on devices bringing a Voidscanner to the scene of such an incident--like a robot, small Unmanned Aerial Vehicle or similar device. Placed beneath the Voidscanner is a bucket 28cm across that is half filled with gravel. On the gravel are placed two simulated bombs. One bomb is a simulated pipe bomb, and the second bomb is mock up of three sticks of dynamite with a cell phone activated detonator. Road flares are used as a stand in for the dynamite, but the simulated pipe bomb uses typical materials found in such devices (besides the lack of actual explosive). Both simulated bombs were constructed by the OPP and are representative examples of common bombs.

61

Figure 4-22 Bomb Identification Configuration

4.3.2 Procedure
A bucket containing sand and two simulated devices is placed on the floor. The Voidscanner is suspended upside down from a platform at a distance of 23cm from the rim of the bucket. A scan at maximum resolution is taken of the bucket. The resultant simulation is then compared to photographic images in order to discern what bomb characteristics are recognizable in the simulation, and what are more difficult. The simulated work light is used to create favorable shadow effects that enhance detail.

4.3.3 Results and Discussion
Figure 4-23 shows photograph of the two fake bombs in the bucket, and Figure 4-24 shows their simulated counterparts. The profile of the cellphone on the lower

62

bomb is clearly visible in the simulated version and it is also possible to see the outlines of all three sticks of dynamite. The pipe bomb is also clearly visible, although it exhibits the consequences of scanning materials with a high specular reflectivity, much like the helmet from section 4.2.1.2.2. On the cylindrical body of the bomb, the laser from the scanner simply reflects away from the bomb at an oblique angle and does not return to the scanner. This creates two parallel blank areas down the side of the bomb, creating the illusion of holes that go down through the bucket and out the floor. It is possible that these parallel holes in the simulation could be used as an indicator to determine where there are reflective cylinders in the scanned region.

The thick blue line that protrudes from the left centre side of Figure 4-24 is an artifact resulting from the construction of the Voidscanner. Because the axis of rotation of the stepper motor does not coincide with the axis of rotation of the Hokuyo scanner itself, a small strip approximately 1cm wide is not covered by the Voidscanner. Where the thick blue lines stops, and continuing along to the right side of the image is a strip of equal dimensions that has been scanned twice. This effect occurs in all scans, although it is more obvious in this experiment due to the close proximity of the viewpoint to the virtual surface. The tape measure functionality of Voidviz was used to determine that the pipe bomb measured approximately 19cm long, and that the cell phone bomb was approximately 20cm long. Both these measurements proved to be accurate.

63

Figure 4-23 Bomb Mockups

Figure 4-24 Simulated Bomb Mockups

4.4 Conclusion
Experiment 1 effectively proved that a victim could be recognized in a simulation based on laser scan gathered from inside a void. While the scanning

64

method used was prohibitively time consuming, it served as an adequate proof of concept that data from within a void can be modeled in a simulation. Experiment 2 used a much faster and more detailed scanning system to show how the scan data can be used to recognize victims and to plan routes into the void to rescue them. This increase of information available to first responders increases their situational awareness. Experiment 3 shows that a laser scan of an explosive device can be used to provide information about the physical dimensions of a device in a way that a camera cannot do accurately due to its two dimensional nature. Again, this increased

information to the responders increases their situational awareness. Modeling the interiors of voids and possible explosive devices are uses that devices such as the Voidscanner or TS have not been put to before. These

experiments have shown that the thesis contribution of Voidviz has made it possible to derive useful information from them.

65

CHAPTER 5

CONCLUSION AND FUTURE WORK

5.1 Summary of Findings and Conclusion
This was a proof-of-concept experiment to determine important features of search and rescue environments could be recognized using a laser scanner. The TS used to gather the data for this experiment was a computerized theodolite that required each point of data to be registered manually. The time-consuming nature of this makes it far too inefficient to be used as a data source in USAR situations. Also, in order for descriptive highlights to be applied to the victim simulation, the points that made up the victim outline had to be identified beforehand. It was determined, based on the data gathered by the TS, that it is possible to recognize a victim inside a void from multiple perspectives. Due to the sparse data, fine details of the victim were not visible, such as facial expressions, clothing, and their general build. However, the general posture of the victim was visible and it could be determined if the victim was standing up or sitting down. Experiment two introduced the Voidscanner, a custom-made 3D laser scanner that operated much faster than the Total Station and at a higher resolution. It lacked the long range of the Total Station, but given that the Voidscanner is meant to be used inside of voids and confined spaces, this is not a significant disadvantage. Due to the increased resolution of the scanner, the resulting simulations had a much larger number of faces. At maximum simulation resolution, the Voidviz controls were sufficiently slow to be almost useless. Rendering in Voidviz using 33% of the scanned points resulted in a simulation that is a useful compromise between visual

66

detail and smooth simulation control. At this level of detail, small surface details could still be resolved, for instance folds in their clothing and the kind of headgear they were wearing. This experiment also showed how materials with a high specular reflectivity could produce deceptive scan results. The shiny safety helmet worn by the simulated victim seemed smaller in simulation due to the way it reflects light away at oblique angles. This is similar to the way stealth aircraft reflect radar signals away at angles that ensure it is unlikely to be redirected back to the radar receiver. This experiment also made it apparent that the scanner would frequently produce what were called laser shadows. They are a deceptive phenomenon that gives the appearance of surfaces where there are none, which is never a desirable trait where accuracy is required. This experiment also showed that three scans assembled together using the scan manipulation tools in Voidviz could be used to model the access path to a victim trapped inside a void. This would help rescuers to decide what would be the most effective way of getting to the victim and providing aid. Experiment three determined that, based on data gathered by the Voidscanner, it is possible to identify components of at least two common explosive devices. The number of sticks of dynamite used in the construction of a cell phone-activated device could be found and the profile of the cell phone used could also be seen in Voidviz. The simulation of the pipe bomb produced some characteristic blank

parallel lines down the side of the pipe body due to oblique specular reflection in those areas. Physical measurements of the devices could be made using the tape

measure functionality of Voidviz. These measurements eliminate the errors made

67

when determining bomb dimensions using 2D images because distances between points in the simulation do not change when the perspective of the viewer changes.

5.2 Future Research
5.2.1 Scan Matching
While there are currently no scan matching algorithms that can merge detailed scans of the size used in this thesis in a timely and accurate manner, advancement in that field proceeds in a rapid pace. In addition, the decreasing cost and increasing accuracy of inertial measurement units would also make it simpler to localize different scans relative to each other without using processer intensive and difficult to tune matching algorithms.

5.2.2 Scanner Mobility
The laser scanners used for this thesis needed to be placed by hand, meaning a first responder would first have to gain entry to a void before they can use the scanner. Future research should include methods of introducing 3D scanners to a void while keeping any rescuers at a safe distance. Recently reported work on the DEX [40] robot may provide a means for delivering scanners to voids without involving humans.

5.2.3 Laser Shadow Detection and Marking
Laser shadows occurred in most of the scans in this thesis. Rather than the current technique of simply limiting maximum edge length of a face, a more discerning tactic could be employed. Given that the long faces produced by laser

68

shadows must be very close to parallel with the path of the laser in the scanner, it should be possible to use that information to determine if they should be omitted from the simulation.

5.3 Concluding Remarks
The intent of this thesis is to show that a 3D visualization tool can be a boon to first responders in various situations related to CPS. This work provides a basis for how visualization tools could be further customized to work for first responders and bomb technicians in a specialized fashion. The technology could be further

expanded to include other circumstances where it would not be prudent for a human to physically interact with an environment. While disasters cannot be completely avoided, preparation can make a large difference. First responders need to have access to the correct tools in order to more effectively perform rescues and to keep themselves safe. Our hope is that research continues on this technology until first responders no longer have to enter dangerous rubble piles or interact with explosive devices, but can enjoy all the benefits of virtual presence without any of the dangers of their real counterparts.

69

BIBLIOGRAPHY
[1] J. Tran. A telepresence system for canine search in an urban search and rescue environment. Master's thesis, Ryerson University, 2009. [2] J. A. Smith E. R. Macintyre, A. G. Barbera. Surviving collasped structure entrapment after earthquakes: A "time-to-rescue" analysis. PREHOSPITAL AND DISASTER MEDICINE, 21:pages 4­19, 2006. [3] A. Topol, M. Jenkin, J. Gryz, S. Wilson, M. Kwietniewski, P. Jasiobedzki, H.-K. Ng, and M. Bondy. Generating semantic information from 3d scans of crime scenes. In Computer and Robot Vision, 2008. CRV '08. Canadian Conference on, pages 333 ­340, may. 2008. [4] David A. McEntire. Disaster Response and Recovery. Wiley, 2007. [5] C. Prater M. K. Lindell and R. W. Perry. Introduction to Emergency Management. Wiley, 2007. [6] Winston-Salem/Forsyth County Office of Emergency Management. The emergency management cycle. Winston-Salem/Forsyth County Office of Emergency Management website, 2010. [7] A. B. Brown and D. A. Patterson. To err is human. In Proceedings of the First Workshop on Evaluating and Architecting System dependability (EASY '01), 2001. [8] P. Booth. An Introduction to Human-Computer Interaction. BPCC, 1989 Wheatons Ltd. [9] Donald A. Norman. The Psychology of Everyday Things. Published by Basic Books, 1989. [10] U.S. Army Corps of Engineers. US&R Structures Specialist: Field Operations Guide. U.S. Army corps of Engineers Readiness Support Centre, 2005. [11] J. Tran, A. Ferworn, C. Ribeiro, and M. Denko. Enhancing canine disaster search. In System of Systems Engineering, 2008. SoSE '08. IEEE International Conference on, pages 1 ­5, jun. 2008. [12] D. J. Garland and M. R. Endsley. Situation awareness: analysis and measurement. Lawrence Erlbaum Associates, 2000. [13] Mica R Endsley. A taxonomy of situation awareness errors. Human factors in aviation operations; Proceedings of the 21st Conference of the European Association for Aviation Psychology (EAAP), 3 (A95-40158 11-53):pp. 287­292, 1995. [14] J. Wang, M. Lewis, and J. Gennari. A game engine based simulation of the nist urban search and rescue arenas. In Proceedings of the 2003 Winter Simulation Conference, volume 1, pages 1039­1045 o.1, Dec. 2003. [15] A. Ferworn, D. Ostrom, K. Barnum, M. Dallaire, D. Harkness, and M. Dolderman. Canine remote deployment system for urban search and rescue. Journal of Homeland Security and Emergency Management, 5: Iss. 1, Article 9, 2008. [16] S. Sharieh, A. Ferworn, and V. Toronov. A gsm mobile system to monitor brain function using a near-infrared light sensor. In Electrical and Computer Engineering, 2008. CCECE 2008. Canadian Conference on, pages 000665 ­000668, may. 2008. [17] A. Ferworn, and G. Hough. Expedients for marsupial operations of usar robots. In IEEE International Workshop on Safety, Security and Rescue Robotics (SSRR06.

70

[18] F. Pozzi. Comparison of 3d measurement techniques in cultural heritage application: user point of view. In 3D Data Processing Visualization and Transmission, 2002. Proceedings. First International Symposium on, pages 762 ­ 765, 2002. [19] Z. Zhang, H. Guo, G. Nejat, and P. Huang. Finding disaster victims: A sensory system for robot-assisted 3d mapping of urban search and rescue environments. In Robotics and Automation, 2007 IEEE International Conference on, pages 3889­3894, April 2007. [20] Z. Zhang and G. Nejat. Robot-assisted intelligent 3d mapping of unknown cluttered search and rescue environments. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pages 2115­2120, Sept. 2008. [21] J. Pascoal, L. Marques, and A.T. de Almeida. Assessment of laser range finders in risky environments. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pages 3533 ­3538, sep. 2008. [22] L. Kneip, F. Tache, G. Caprari, and R. Siegwart. Characterization of the compact hokuyo urg-04lx 2d laser range scanner. In Robotics and Automation, 2009. ICRA '09. IEEE International Conference on, pages 1447 ­1454, may 2009. [23] SICK Sensor Intelligence Press Release. S3000 laser rangefinder picture. SICK website: http://www.sick.com/group/EN/home/pr/press_releases/Pages/laser_scanner_s3000_prof inet.aspx, Retrieved November 2009. [24] Picture of urg-04lx-ug01. Sentek Europe website: http://www.sentekeurope.com/urg-04lx-ug01.html, Retrieved April 2010. [25] Z. Kriz, R. Prochaska, C.A. Morrow, C. Vasquez, Hsingtzu Wu, and Rizwanuddin. Unreal iii based 3-d virtual models for training at nuclear power plants. In Nuclear Renewable Energy Conference (INREC), 2010 1st International, pages 1 ­5, 21-24 2010. [26] R. L. Sanders and J. E. Lake. Training first responders to nuclear facilities using 3-d visualization technology. In Simulation Conference, 2005 Proceedings of the Winter, page 5 pp., Dec. 2005. [27] R. Darken, P. McDowell, and E. Johnson. Projects in vr: the delta3d open source game engine. Computer Graphics and Applications, IEEE, 25(3):10­12, May-June 2005. [28] J. Park, T. Kim, and Y. Kim. Applying open source game engine for building visual simulation training system of fire fighting. In AsiaSim Communications in Computer and Information Science. Springer Berlin Heidelberg, 2007. [29] C. Ratti, Y. Wang, B. Piper, H. Ishii, and A. Biderman. Phoxel-space: an interface for exploring volumetric data with physical voxels. In DIS '04: Proceedings of the 5th conference on Designing interactive systems, pages 289­296, New York, NY, USA, 2004. ACM. [30] A. Chopra and L. Town. Introduction to google sketchup. Wiley, 2007. [31] Maurizio Forte, Eva Pietroni, Claudio Rufa, Angela Bizzarro, Alessandro Tilia, and Stefano Tilia. Dvr-pompei: a 3d information system for the house of the vettii in opengl environment. In VAST '01: Proceedings of the 2001 conference on Virtual reality, archeology, and cultural heritage, pages 307­314, New York, NY, USA, 2001. ACM. [32] P. Tokekar, V. Bhatawadekar, D. Fehr, and N. Papanikolopoulos. Experiments in object reconstruction using a robot-mounted laser range-finder. In Control and Automation, 2009. MED '09. 17th Mediterranean Conference on, pages 946 ­951, june 2009.

71

[33] Z. Zhang. Iterative point matching for registration of free-form curves and surfaces. International Journal of Computer Vision, 13(2):119­152, 1994. [34] K. Ohno, S. Tadokoro, K. Nagatani, E. Koyanagi, and T. Yoshida. 3-d mapping of an underground mall using a tracked vehicle with four sub-tracks. In Safety, Security Rescue Robotics (SSRR), 2009 IEEE International Workshop on, pages 1 ­6, nov. 2009. [35] K. Ohno and S. Tadokoro. Dense 3d map building based on lrf data and color image fusion. In Intelligent Robots and Systems, 2005. (IROS 2005). 2005 IEEE/RSJ International Conference on, pages 2792 ­ 2797, aug. 2005. [36] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and G. Taubin. The ballpivoting algorithm for surface reconstruction. Visualization and Computer Graphics, IEEE Transactions on, 5(4):349 ­359, oct-dec 1999. [37] Y. Kenmochi, K. Kotani, and A. Imiya. Marching cubes method with connectivity. In Image Processing, 1999. ICIP 99. Proceedings. 1999 International Conference on, volume 4, pages 361 ­365 vol.4, 1999. [38] Acroname corporate author. Laser rangefinder overview. Acroname website: http://www.acroname.com/robotics/info/articles/laser/laser.html, Retrieved January 2010. [39] University of Malaga. Hokuyo interface tutorial. MRPT website: http://www.mrpt.org/Example:HOKUYO_URG/UTM_Laser_Scanner, Retrieved May 2009. [40] A. Ferworn D. Ostrom M. Gerdzhev, J.Tran. Dex ­ a design for canine-delivered marsupial robot. In IEEE International Workshop on Safety, Security & Rescue Robotics (SSRR), 2010.

72

GLOSSARY  Disruptor: A type of device used by bomb disposal teams to destroy explosive devices before they can detonate. An explosive is used to drive a volume of water through a bomb in such a way that all the pieces of the bomb are either separated from each other or destroyed in a fraction of a second.  Faces: A flat, triangular plane in a computer simulation defined by three points. It may be placed in any orientation. A complex object in a computer simulation must be constructed from many of these. Faces consisting of more than three points are possible, but are not used in this thesis.   Facets:See Faces First Person Shooter: a genre of video game that centres on a first person perspective and gun oriented gameplay. through the protagonist's eyes.  Specular Reflection: The reflection of light from a surface that is mirror like. Light from a single incoming direction is reflected in a single The player plays the game

outgoing direction. Materials with specular reflection are characterized by being glossy, in contrast to matte surfaces.

73

