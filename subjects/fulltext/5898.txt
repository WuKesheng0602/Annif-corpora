Social Media Screening of Job Applicants

You're Hired: Examining Acceptance of Social Media Screening of Job Applicants
Full Paper

Anatoliy Gruzd Ryerson University gruzd@ryerson.ca Abstract

Jenna Jacobson Ryerson University jenna.jacobson @ryerson.ca

Elizabeth Dubois University of Ottawa elizabeth.dubois @uottawa.ca

The paper examines attitudes towards employers using social media to screen job applicants. In an online survey of 454 participants, we compare the comfort level with this practice in relation to different types of information that can be gathered from publicly accessible social media. The results revealed a nuanced nature of people's information privacy expectations in the context of hiring practices. People's perceptions of employers using social media to screen job applicants depends on (1) whether or not they are currently seeking employment (or plan to), (2) the type of information that is being accessed by a prospective employer (if there are on the job market), and (3) their cultural background, but not gender. The findings emphasize the need for employers and recruiters who are relying on social media to screen job applicants to be aware of the types of information that may be perceived to be more sensitive by applicants, such as social network-related information. Keywords Social media, information privacy, job screening; hiring practices.

Introduction
As more people are joining and contributing to social media sites, these sites and the data they hold are becoming increasingly attractive to organizations for a wide variety of purposes, such as marketing and human resources. Organizations are beginning to turn to social media to attract, recruit, and screen qualified job applicants. According to a recent survey of hiring managers by CareerBuilder (2016), the number of organizations in the U.S. that rely on social media to screen job applicants has increased from 11% in 2006 to 60% in 2016. When researching job applicants on social media, organizations seek to validate candidates' qualifications, identify whether candidates present themselves professionally, and ensure that candidates are not posting any abusive or harassing content. It is not only the availability of social media data that makes it so attractive to organizations; there is also early evidence that such data can be a good proxy to assess job candidates' personality traits (Stoughton et al. 2013), as well as predict key performance metrics, including hirability, academic ability, and job performance (Kluemper et al. 2012). To date, much of the research in this area has approached social media screening from the perspective of employers. This paper examines the ever-evolving practice of using social media to screen job applicants from job seekers' perspectives. Past research in this area has studied job seekers' awareness level of this hiring practice, showing that as many as 80% of respondents in one study (Root and McKay 2014) believe that employers are checking job applicants' social media activities. We expand this line of research to go beyond merely investigating whether people are aware of this hiring practice, but rather measure how comfortable job applicants are with this practice. Our goal is to compare the comfort level of this practice between likely job seekers and non-job seekers in relation to different types of information that can be gathered about a job applicant from their publicly accessible social media. The findings will help to develop a more granular understanding of job seekers' attitudes towards social media screening and will help hiring managers develop best practices that are more aligned with job applicants' expectations.

Twenty-third Americas Conference on Information Systems, Boston, 2017

1

Social Media Screening of Job Applicants

Driven by the rapid emergence of so-called Communication-Minded Visualizations (CMV)--which are visualizations "designed to support communication and collaborative analysis" (Viégas and Wattenberg 2006)--we used information visualizations to represent different information types and displayed them to participants in our online survey. The nine sample visualizations were designed to solicit participants' reactions about the use of a particular piece of information from social media. For example, a "word cloud" represented frequently used words, a bar chart showed the percentage of positive and negative messages, a pie chart listed the top 10 posters, a network diagram identified who communicated with whom, and another bar chart displayed the number of postings over time. The visualizations were used to illustrate the types of data that can be collected and analyzed from social media. The use of visualizations differentiates our research from previous work in this area that only use text-based questions to ask people about their attitudes related to information types from social media (Root and McKay 2014; Wang et al. 2011). To test the potential effect of using visualizations in the survey, the study begins with an examination of the following methodological question: Question 1: In the context of screening job applicants using social media data, is there a difference in how people react when presented with a visualization-based versus text-based question? Next, we turn to identifying potential differences in people's responses to different information types presented to them--regardless of whether they were shown a visualization or in a textonly format question. To build on the previous work that largely focused on examining people's attitudes towards either Raw Data or Metadata associated with their social media profiles, we included two additional information types: (1) Analytics--information derived from raw data or metadata by using some form of automated analysis, such as a sentiment analysis; and (2) Social Networks--information related to social connections formed with others on social media. Question 2: In the context of screening job applicants using social media data, is there a difference in how people react to various information types: raw data/metadata versus analytics versus social network-related information? As most literature has either focused on people who are actively seeking employment or students who presumably will soon seek employment, we want to contribute to this research area by comparing attitudes between likely job seekers and those who are not seeking employment. Thus, we ask: Question 3: Do job seekers react to the practice of screening job applicants on social media differently than non-job seekers? Because our survey included participants primarily from two countries: U.S. (319 people) and India (113 people) and because some privacy literature shows that cultural and regulatory differences (Bellman et al. 2004) influence information privacy concerns, we ask, Question 4: Do people living in the U.S. react to the practice of screening job applicants on social media differently than those in India? Finally, as previous work suggests that women have a higher level of privacy concerns in comparison to men in the context of Internet and social media use (Sheehan 1999; Cho and Hung 2011; Youn and Hall 2008), we ask: Question 5: Do women react to the practice of screening job applicants on social media differently than men?

Literature Review
Social Media Screening Practices
Research in this area has primarily focused on whether and how organizations and recruiters use social media for hiring purposes. In a small-scale study with a focus on LinkedIn use, Zide et al. (2014) found that recruiters usually used the site to look for applicants' "employment history, education, years of experience, and how the applicant presents him or herself on the site." 60% of the interviewees in their study indicated that they consider how well-connected the applicants are. Another study examined how recruiters use online social networks to attract and screen job seekers (Ollington et al. 2013). Through a series of interviews, researchers identified four distinct mechanisms of how recruiters use various social networking sites: (1) connecting job seekers via secondary connections and establishing themselves as highly connected nodes in online social networks, (2) building their own online brand, (3) offering job seekers access to specialized online resources that are normally only available to recruiters, and finally, (4) acquiring accurate data about job seekers (Ollington et al. 2013). Sivertzen et al. (2013) showed how employers' use of social media can positively influence corporate reputation and indirectly increase job applicants' intention to apply for a job with the company. In a recently published edited book, "Social Media in Employee Selection and Recruitment" (Landers and Schmidt 2016), a number of chapters identified potential risks and challenges associated with using social media for attracting, recruiting, and screening job applicants, in-

Twenty-third Americas Conference on Information Systems, Boston, 2017

2

Social Media Screening of Job Applicants

cluding potential negative biases towards minorities (Ruggs et al. 2016), legal concerns (Schmidt and O'Connor 2016), misrepresentation, and misattribution of job applicants (Frantz et al. 2016).

Studies of Prospective Employees
Particularly relevant to the current study is research that examines job applicants. Previous research has explored job seekers' self-disclosure practices on social media by examining factors that influence what job seekers share, delete, or decide not to share on social media. Inappropriate self-disclosure on social media--defined as posting informal or controversial selfies or tagged photos, commenting on controversial topics, or "participation in activities which are in violation of university or workplace policy" (Peluchette and Karl 2008)--is often the focus of research in this area. El Ouirdi et al. (2015) examined the relationship between self-disclosure and the following factors: (1) the motivation to maintain a professional online image, (2) social media self-efficacy, and (3) the perceived effectiveness of social media in the job search context. One particularly interesting finding was that social media self-efficacy was found to be positively associated with job seekers' self-disclosure of both inappropriate and career-oriented content, which the authors attributed to the increased use of social media and loss of inhibition. Zide et al, (2014) studied the relationship between what information LinkedIn users share and their occupation for three occupation groups: HR, sales/marketing, and industrial-organizational psychologists (I-O psychologists). Unsurprisingly, the authors found sales/marketing professionals were the most "networking-savvy" of the three groups. The paper concluded that "[i]n job search activities, these [HR and I-O] professionals may suffer a disadvantage over other applicants who provided more information" (p. 599). Another line of research examines prospective employees' awareness of social media screening practices by organizations and recruiters. Root and McKay (2014) found that students are becoming increasing aware of this practice--80% of respondents felt that prospective employers were likely to check their social media profiles. The authors found that students were generally comfortable with the practice as they tended to disagree with the statement: "it is wrong for anyone to consider what they have posted on the Internet when applying for a job." Stoughton et al. (2015) came to a somewhat different conclusion in their study: the practice of screening applicants on social media increased students' sense of invasion of privacy (regardless of whether a student was offered a job in the experiment or not). To investigate possible reasons behind these somewhat divergent findings, our paper extends the previous literature in the following ways. Similar to Root and McKay (2014), we examine the relationship between different types of social media information and the practice of job applicant screening, but instead of asking what information prospective employees think is important to employers, we ask participants what information would be acceptable for potential employers to access. We also go beyond studying readyaccessible data and metadata to include analytics and social network-related information that can and are used by companies. Related work (Wang et al. 2011, Osatuyi 2015) examined social media users' privacy attitudes, but without providing any particular context in terms of who is accessing users' social media data and for what purposes. We address this limitation by focusing on the use of social media data by organizations in the context of screening job applicants. Finally, as most previous studies surveyed job seekers or students, we use a more diverse population of social media users sampling from: those who are likely to seek employment and non-job seekers.

Method
Recruitment via Amazon Mechanical Turk
Data was collected using Amazon Mechanical Turk (AMT). AMT is a crowdsourcing service organized like a marketplace, where requesters can post an online task, such as a survey, and volunteers anywhere in the world (called "workers" or "turkers") can complete this task for a small compensation (Mason and Suri 2012). Turkers perform tasks primarily to supplement their income (61%) and for personal enjoyment (41%) (Paolacci et al. 2010). AMT's pool of potential subjects has been relatively stable over time and includes a sample comparable to standard Internet samples (Buhrmester et al. 2011) and other more traditional samples (Goodman et al. 2013). In their study of evaluating internal and external validity of AMTbased samples, Berinsky et al. (2012) found that AMT samples were more representative of the U.S. population than in-person "convenience" samples, but less representative than internet-based panels or national probability samples. The researchers also concluded that turkers tend to be older (M:32.3; SE:0.5)

Twenty-third Americas Conference on Information Systems, Boston, 2017

3

Social Media Screening of Job Applicants

than student samples (M:20.3; SE:8.2), but younger than adult samples (M: 45.5; SE:0.9). AMT has implemented a number of mechanisms to ensure the high-quality pool of respondents. For example, every time a turker completes a given task, a requester has an opportunity to review their work and either approve or reject it. Thus, each turker is assigned an overall approval rating of their tasks, which, in turn, can be used to exclude turkers whose approval rate is less than a specified threshold. Notably, previous work has evaluated the quality of data collected via AMT and confirmed comparable quality to collecting data by more traditional means (Buhrmester et al. 2011). A recent study also revealed that turkers were more attentive to the instructions than a comparable sample of college students (Hauser and Schwarz 2016). Rand (2012) also found that turkers complete the demographic questions truthfully. Prior to beginning our study, the Research Board Ethics approval was obtained from two universities. The survey was open for three days between December 22­25, 2016. Turkers were paid $1 USD per response. Since the average time completing the survey was just under 10 minutes, it represents the compensation of $6/hour, which is higher than what was reported in other studies using AMT (Berinsky et al. 2012; Wang et al. 2011). To ensure the quality of our data, we specified that only turkers with the overall approval rate of 75% or higher could participate in the survey. We also inserted a trap question to ensure that participants were carefully reading the questions. In total, we received 506 responses. We further cleaned our dataset by excluding 33 responses that did not answer a "trap" question correctly, 18 responses that were completed in less than 3 minutes, and 1 response where the participant did not have any social media accounts (an outlier). In total, we ended with 454 responses.

Survey Design and Information Visualizations
The survey consisted of three sections. Section 1 asked 17 questions to measure users' Concern for Social Media Information Privacy (CFSMIP) (Osatuyi 2015) and built on a widely accepted construct of Concern for Information Privacy (CFIP) (Stewart and Segars 2002). These questions were general in nature and were not limited to the hiring context. This section also included the trap question "Please select `strongly agree' as your answer choice." Before proceeding to Section 2, all participants were shown a textual brief informing them about the practice of using social media data to screen prospective employees and some common ways companies may use such information. In Section 2, participants were asked to score their comfort level (on a scale from 1­7; 1-"very uncomfortable" to 7-"very comfortable") with a potential employer viewing their publicly accessible information from social media. This question was repeated nine times, each with one of the nine sample information types, selected from the three broad categories: Raw Data & Metadata--readily available information; Analytics--information that requires some processing; Social Networks--information related to users' online social network (see Table 1). The "information type" questions were presented in random order. Raw Data & Metadata User's posts User's photos Locations where the user posted from (city and street level) Analytics Words used frequently by the user User's posting frequency Sentiments of posts (positive, negative or neutral) Social Networks Top posters in a public group User's followers/friends Who is connected to whom in a public group

Table 1. Categories of Information Types The survey instrument was developed and evaluated over a one-year period to ensure that all text and visual elements are clearly visible and properly interpreted by participants. For example, to avoid a potential colour bias in visualizations, we used neutral colours whenever possible. Furthermore, to emphasize that shown visualizations are only for demonstration purposes, we displayed two versions of each visualization depicting different possible outcomes. For example, when showing a hypothetical result of sentiment analysis (Figure 1), one visualization showed the majority of positive posts (displayed to the left), while the other visualization showed the majority of neutral posts and a significant proportion of negative posts (displayed on the right). In the final section, participants were asked about their overall social media use (what platforms and frequency of use) as well as general demographic data (age, country, education level, etc.). We also asked questions about participants' awareness of cases of social media misuse and whether they have been a victim of data privacy violation.

Twenty-third Americas Conference on Information Systems, Boston, 2017

4

Social Media Screening of Job Applicants

Figure 1. Sample Visualization of Sentiment Analysis

Statistical Tests
The current paper only focuses on the analysis of the survey sections 2 and 3. To analyze data, we used SPSS, which is a popular software for statistical analysis. To analyze differences between various groups of participants (Research Questions 1, 3­5), we used a non-parametric Mann-Whitney U Test. A nonparametric test is recommended when working with Likert scale items since they should not be treated as continuous variables. To answer Question 2, we used Cronbach's Alpha to check internal consistency, Spearman's rho to conduct a correlation analysis, and ran Principle Components Factor Analysis to examine the dimensionality of the Likert scale questions.

Results
Participants' Demographics
Our sample is well balanced in terms of gender and job seeking status. Consistent with other AMT-based samples (Berinsky et al. 2012; Paolacci et al. 2010; Wang et al. 2011), our sample is skewed towards younger (but older than student samples), more educated, social media users (age 25­34), who use at least one of the eleven popular social media sites presented to them. Most of the users are from the U.S. and India (3:1 ratio). The only major difference between our sample and those reported in other AMTbased studies is that we had more male participants (especially in the sample from India). See Table 2. N=454 Gender Women Men Prefer not to say Age Under 25 25­34 35­44 45­54 55­64 65 or older Job Seeker* Likely (scores 5­7) Not likely (scores 1­4) 193 258 3 61 212 116 43 19 3 241 213 Percent 42.51% 56.83% 0.66% 13.44% 46.70% 25.55% 9.47% 4.19% 0.66% 53.08% 46.92% Education High school diploma College diploma Some college, no degree Bachelor's degree Master's degree Doctorate degree Professional degree (J.D., M.D., D.O., etc.) Some school, no degree Country USA India Other 29 53 101 168 81 11 8 3 319 113 22 6.39% 11.67% 22.25% 37.00% 17.84% 2.42% 1.76% 0.66% 70.26% 24.89% 4.85%

*Transformed variable from a 7-point Likert scale

Table 2. Participant Background Question 1: When asking about the comfort level in the context of screening job applicants on social media, is there a difference in how people react when presented with a visualization-based versus text-based version of the question? Using a random assignment, 233 people were presented with the visualization-based questions and 221 with the text-based questions. Based on the Independent-Samples Median Test and Mann-Whitney U Test, we found that only two out of the nine visualizations, "Word Cloud" and "Posting Frequency," showed a difference in the median responses and
Twenty-third Americas Conference on Information Systems, Boston, 2017 5

Social Media Screening of Job Applicants

distributions. In particular, participants tended to be slightly less comfortable (by a single point) when shown the "Word Cloud" and "Posting Frequency" visualizations as opposed to being asked the same questions without showing the visual representation of the implied data. There was no difference in the medians and distributions for the other questions. One of the possible reasons that the "Word Cloud" question solicited a "less comfortable" response than its text-based equivalent could be because some words in the visualization might have solicited a stronger response. In our case, one visual of the "Word Cloud" contained politically motivated words related to the 2016 U.S. Presidential election including "Trump," "Clinton," "America;" the second visual included words that were related to Pokémon Go, such as "egg," "park," "lol." Both visuals were shown side-by-side. As for the "Posting Frequency" question, its visual representation in the form of frequency bar charts (one showing posting frequency over a 24-hour period and another by days of the week) might have revealed more information than what people perceived when reading the text-based version of the question. For example, a person might have been less comfortable if a potential employer sees that they are active on social media during work days and/or work hours. This result partially supports our original supposition that analytical information based on some form of data analysis would receive a stronger reaction when accompanied by representative visualizations. This is potentially useful because such information might be difficult to mentally visualize without seeing it. Interestingly, however, other examples of analytical information, such as those based on a sentiment analysis, have not shown a statistically significant difference in the way the questions were answered. Based on this finding, we excluded the "Word Cloud" and "Posting Frequency" questions for all subsequent tests since the two questions have shown a slight difference in responses when shown with and without their accompanying visual representations. Question 2: In the context of screening job applicants, is there a difference in how people react to different information types: raw data/metadata versus analytics versus social network-related information? In this question, we sought to discern whether certain information types solicited a stronger or weaker comfort response. To answer this question, we started with an inter-coder reliability check. The Cronbach's alpha (based on standardized items) for the remaining seven questions about different information types is 0.917, suggesting that the questions have relatively high internal consistency. Next, we turned to a correlation analysis. Since the variables in Section 2 were measured using a Likert scale and are not treated here as a single construct, we used Spearman's rho. Based on the results, all seven variables were strongly correlated with each other (statistically significant at the 0.01 level, 2tailed), suggesting that all information types solicited a similar response (See Table 3). 1 2 3 4 5 6 7 Top Posters Followers/ Friends Network Sentiments Geolocation Photos Posts 1 1 .61 .65 .69 .56 .61 .66 2 .61 1 .70 .59 .64 .58 .62 3 .65 .70 1 .64 .57 .66 .66 4 .69 .53 .64 1 .55 .62 .69 5 .56 .64 .57 .55 1 .49 .51 6 .61 .58 .66 .62 .49 1 .65 7 .66 .62 .66 .69 .51 .65 1

Table 3. Spearman's rho Furthermore, we conducted a factor analysis to examine the dimensionality of the seven questions. Even though the questions correlated with one another, they could still form distinct groups. KMO and Bartlett's Test confirmed the suitability of our data to perform a factor analysis (See Table 4). Kaiser-Meyer-Olkin Measure of Sampling Adequacy Bartlett's Test of Approx. Chi-Square Sphericity df: 21 Table 4. KMO and Bartlett's Test Based on the Principle Components Factor Analysis, the eigen value for the first factor is much larger than the eigen value for the second factor (4.67 versus 0.59), and the first factor accounts for 66.72% of the to.924 1952.73 Sig: .000

Twenty-third Americas Conference on Information Systems, Boston, 2017

6

Social Media Screening of Job Applicants

tal variance (See Table 5). This suggests that the seven information types are unidimensional; in other words, there are no natural groupings among different information types that may be considered as more or less sensitive by the participants. Initial Eigenvalues Total % of Variance 4.67 66.72 .59 8.46 .45 6.47 .37 5.25 .34 4.79 .30 4.35 .28 3.96 Cumulative % 66.72 75.18 81.65 86.91 91.69 96.04 100 Extraction Sums of Squared Loadings Total % of Variance Cumulative % 4.67 66.72 66.72 Com ponent 1 2 3 4 5 6 7

Table 5. Principle Components Factor Analysis Question 3: Do job seekers react to the practice of screening job applicants on social media differently than non-job seekers? This question asks whether there is a difference in how job seekers versus those who are not currently or planning to seek employment in the near future react to the practice of screening job applicants. To answer this question, we again used the Independent-Samples Median Test and Mann-Whitney U Test. The question "How likely are you to seek employment within the next 6 months?" (7-point Likert scale; from 1-very unlikely to 7-very likely) was used to group the respondents: job seekers (those who scored between 5­7) and non-job seekers (with scores 1­4). The results show that there is a difference between how job seekers and non-job seekers answered, either in terms of the resulting medians and/or distributions. Based on the difference in medians, job seekers were more comfortable with some information types being accessed by a potential employer including "Sentiment Analysis," "Geolocation," and "Posts" (median=4). When comparing the resulting response distributions, six out of seven were different between the two groups. Only the distribution for the "Communication Network" remained consistent (median=3) for both groups. This result reveals a more complexity of people's attitudes towards their public social media data being accessed by prospective employers. Even though their overall feeling towards this practice was less comfortable, job seekers showed more willingness to lower their concerns for some information types, but not all. For example, job seekers were more comfortable with an employer viewing the places they visited based on their public social media activities than they were with an employer accessing their social network-related information. The latter could be due to the fact that social network-information can reveal how influential someone is. This might have been perceived by both groups of respondents as something that can affect an employer's hiring decision, for example in case such information shows a lack of influence on social media. Question 4: Do people who live in the U.S. react to the practice of screening job applicants on social media differently than those in India? Based on the Independent-Samples Median Test and Mann-Whitney U Test, we found that the participants from the U.S. responded differently than those from India. The respondents from the U.S. were less comfortable with all seven information types (median=3) than those from India, (median=5) . This is an interesting result as it suggests a significant difference in the acceptance of the practice between people in the two countries. This could be because of cultural and/or regulatory differences in both countries (Bellman et al. 2004) and also because in India "the social and family structures place much less importance on privacy" (Ion et al. 2011). Our finding is in line with the previous research that showed privacy attitudes of knowledge workers (Patil et al. 2010) and social media users (Wang et al. 2011) in India tend to be lower than those in the U.S. Question 5: Do women react to the practice of screening job applicants on social media differently than men? Even though some previous work has suggested that women have a higher level of privacy concerns than men in the context of Internet and social media use (Sheehan 1999; Cho and Hung 2011; Youn and Hall 2008), our research showed no difference in how men versus woman responded to the seven "information type" questions. This is potentially because we investigated the comfort level in a
Twenty-third Americas Conference on Information Systems, Boston, 2017 7

Social Media Screening of Job Applicants

specific context (screening job applicants on social media). In this context, there may be some "universal" expectations and concerns regardless of one's gender. In a related study of Italian employed and nonemployed job seekers, the researchers also found no gender-based difference between professional online image concerns and inappropriate self-disclosure (El Ouirdi et al. 2015). They proposed that this could be because career-oriented items in their study "included an implicit minimum disclosure threshold on social media, and that it is widely common to disclose this amount of information by default without necessarily being on the job market" (El Ouirdi et al. 2015).

Conclusions
The study investigated how job seekers and non-job seekers perceive the practice of prospective employers using publicly accessible social media data to screen job applicants. The results can be broadly divided into two areas: (1) those that are related to the use of visualizations in surveys and (2) those that are more directly related to the human resources area.

Visualization as a Survey Tool
Following recent calls to Internet researchers to "engage critically with data visualization [...] by focusing on how people engage with them" (n.p.) (Kennedy et al. 2016), we began this research by investigating the role of visualizations in studying users' social media privacy concerns. As the amount and complexity of data that can be accessed from social media have increased exponentially, we were interested in examining the utility of using information visualizations as a tool to study people's attitudes and perceptions regarding their social media data being used by third parties. At their core, visualizations are designed to summarize large volumes of data, combine multiple dimensions, and reveal invisible patterns. Accordingly, visualizations are especially useful in demonstrating the types of data and metadata that can be stored, aggregated, and analyzed about users' online engagement by third parties, such as prospective employers. Despite our initial expectation, the result showed that in the majority of cases (seven out of nine), displaying a visualization alongside the question did not influence participants' responses (at least not statistically significantly). Only two questions about frequently used words/topics (visualized as a word cloud) and posting frequency (visualized as frequency charts by time and by day) showed a statistically significant difference. When these two information type questions were shown with the corresponding visualizations, the participants were less comfortable, which suggests that visualizations might have helped to better explain the types of information in question, but only in these two cases.

Implications for Hiring Managers
When comparing participants' reactions across different information types, there was no variation in individual responses. This is despite our initial expectation that people will be more comfortable with raw data, such as posts or photos, (and perhaps with metadata) being accessed by prospective employers, but less comfortable with more analytical and social network-type information being accessed for job screening purposes. Overall, we found that our participants were somewhat uncomfortable with prospective employers accessing their social media data regardless of the presented information type. There were, however, some notable exceptions; specifically, when we compared job seekers and non-job seekers as there were some differences between how the two groups responded to different information types. Job seekers were slightly more comfortable with potential employers accessing their posts, geolocation information, and sentiment analysis of their posts (median=4) than non-job seekers. This suggests that job seekers are not only aware of the practice, but might be trying to use social media to their advantage as a reputation management tool--while still having some reservations about the appropriateness of prospective employers accessing their social network-related information and photos. One of the strongest results in our study was that respondents from the U.S. were significantly less comfortable with all information types presented to them than those from India. The finding is in line with some previous work on cultural and regulatory differences between the two countries. Finally, we did not find any differences in responses based on gender, suggesting that in the context of employment seeking, there might be some consistent set of norms and expectations irrespective of one's gender.

Twenty-third Americas Conference on Information Systems, Boston, 2017

8

Social Media Screening of Job Applicants

The results emphasize the need for employers and recruiters who rely on social media to screen job applicants to be aware of the types of information that may be perceived to be more sensitive by applicants as this may "reduce the attractiveness of an organization during various phases of the selection process, especially if the applicant pool at large knows or suspects that the organization engages in such screening" (Stoughton et al. 2015). The results of the study can help organizations develop best practices when relying on social media for job screening purposes. For example, companies hiring in different countries or recruiting people with different cultural or ethnic backgrounds should recognize that some cultures may be more or less comfortable with such practice. Also, even though recruiters may check how connected job applicants are and who they are connected to (Ollington et al. 2013), our study showed that social network-related information was consistently marked as one of the most intrusive types of information.

Summary and Future Work
The results revealed a more complex and nuanced nature of people's information privacy expectations in the context of job screening and hiring practices. Specifically, the way people feel about employers using social media to screen job applicants depends on whether (1) they are seeking employment, (2) the type of information that is being accessed, and (3) their cultural background. In our future work, we will continue testing the effects of information visualizations on people's mental images. Our initial list of nine information types was used as a starting point in this line of inquiry, and we will continue to expand the information types in future work. We will specifically focus on adding and testing analytical-level information visualizations as such information might be harder for respondents to visualize and thus give an informed response. To avoid the possibility of the content of sample visualizations influencing participants' judgment, our future research will analyze people's reactions to their own personal data automatically extracted and visualized by our survey instrument. Future work will also explore relationships between the information types, additional demographic attributes, and people's concerns for social media information privacy.

References
Bellman, S., Lecturer, S., Johnson, E. J., Kobrin, S. J., Wurster, W. H., Management, P. M., and Lohse, G. L. 2004. "International Differences in Information Privacy Concerns: A Global Survey of Consumers," The Information Society (20:5), pp. 313­324. Berinsky, A. J., Huber, G. A., and Lenz, G. S. 2012. "Evaluating Online Labor Markets for Experimental Research: Amazon.com's Mechanical Turk," Political Analysis, (20:3), pp. 351­368. Buhrmester, M., Kwang, T., and Gosling, S. D. 2011. "Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?" Perspectives on Psychological Science: A Journal of the Association for Psychological Science, (6:1), pp. 3­5. CareerBuilder. 2016. "Number of Employers Using Social Media to Screen Candidates Has Increased 500 Percent over the Last Decade," Available at http://bit.ly/2pfxAnR. Cho, V., and Hung, H. 2011. "The Effectiveness of Short Message Service for Communication with Concerns of Privacy Protection and Conflict Avoidance," Journal of Computer-Mediated Communication, (16:2), pp. 250­270. El Ouirdi, M., Segers, J., El Ouirdi, A., and Pais, I. 2015. "Predictors of Job Seekers' Self-Disclosure on Social Media," Computers in Human Behavior, (53), pp. 1­12. Frantz, N. B., Pears, E. S., Vaughn, E. D., Ferrell, J. Z., and Dudley, N. M. 2016. "Is John Smith Really John Smith? Misrepresentations and Misattributions of Candidates Using Social Media and Social Networking Sites," in Social Media in Employee Selection and Recruitment, R. N. Landers and G. B. Schmidt (eds.), Springer, pp. 307­339. Goodman, J. K., Cryder, C. E., and Cheema, A. 2013. "Data Collection in a Flat World: The Strengths and Weaknesses of Mechanical Turk Samples," J. of Behavioral Decision Making, (26:3), pp. 213­224. Hauser, D. J., and Schwarz, N. 2016. "Attentive Turkers: MTurk Participants Perform Better on Online Attention Checks than do Subject Pool Participants," Behavior Research Methods, (48:1), pp. 400­ 407.

Twenty-third Americas Conference on Information Systems, Boston, 2017

9

Social Media Screening of Job Applicants

Ion, I., Sachdeva, N., Kumaraguru, P., and Capkun, S. 2011. "Home is Safer than the Cloud!: Privacy Concerns for Consumer Cloud Storage," in Proceedings of the Seventh Symposium on Usable Privacy and Security, SOUPS '11, New York, NY: ACM, pp. 13:1­13:20. Kennedy, H., Hill, R. L., Allen, W., and Kirk, A. 2016. "Engaging with (Big) Data Visualizations: Factors that Affect Engagement and Resulting New Definitions of Effectiveness," First Monday, (21:11). Kluemper, D. H., Rosen, P. A., and Mossholder, K. W. 2012. "Social Networking Websites, Personality Ratings, and the Organizational Context: More than Meets the Eye?" Journal of Applied Social Psychology, (42:5), pp. 1143­1172. Landers, R. N., and Schmidt, G. B. 2016. "Social Media in Employee Selection and Recruitment: An Overview," in Social Media in Employee Selection and Recruitment, R. N. Landers and G. B. Schmidt (eds.), Springer, pp. 3­11. Mason, W., and Suri, S. 2012. "Conducting Behavioral Research on Amazon's Mechanical Turk," Behavior Research Methods, (44:1), pp. 1­23. Ollington, N., Gibb, J., and Harcourt, M. 2013. "Online Social Networks: An Emergent Recruiter Tool for Attracting and Screening," Personnel Review, (42:3), pp. 248­265. Osatuyi, B. 2015. "Empirical Examination of Information Privacy Concerns Instrument in the Social Media Context," AIS Transactions on Replication Research, (1:3), pp. 1­14. Paolacci, G., Chandler, J., and Ipeirotis, P. G. 2010. "Running Experiments on Amazon Mechanical Turk," Judgment and Decision Making, (5:5), pp. 411­419. Patil, S., Kobsa, A., John, A., and Seligmann, D. 2010. "Comparing Privacy Attitudes of Knowledge Workers in the U.S. and India," in Proceedings of the 3rd International Conference on Intercultural Collaboration, ICIC '10, New York, NY: ACM, pp. 141­150. Peluchette, J., and Karl, K. 2008. "Social Networking Profiles: An Examination of Student Attitudes Regarding Use and Appropriateness of Content," CyberPsychology & Behavior, (11:1), pp. 95­97. Rand, D. G. 2012. "The Promise of Mechanical Turk: How Online Labor Markets Can Help Theorists Run Behavioral Experiments," Journal of Theoretical Biology, (299), pp. 172­179. Root, T., and McKay, S. 2014. "Student Awareness of the Use of Social Media Screening by Prospective Employers," Journal of Education for Business, (89:4), pp. 202­206. Ruggs, E. N., Walker, S. S., Blanchard, A., and Gur, S. 2016. "Online Exclusion: Biases that May Arise when Using Social Media in Talent Acquisition," in Social Media in Employee Selection and Recruitment, R. N. Landers and G. B. Schmidt (eds.), Springer, pp. 289­305. Schmidt, G. B., and O'Connor, K. W. 2016. "Legal Concerns when Considering Social Media Data in Selection," in Social Media in Employee Selection and Recruitment, R. N. Landers and G. B. Schmidt (eds.), Springer, pp. 265­287. Sheehan, K. B. 1999. "An Investigation of Gender Differences in On-Line Privacy Concerns and Resultant Behaviors," Journal of Interactive Marketing, (13:4), pp. 24­38. Sivertzen, A.-M., Nilsen, E. R., and Olafsen, A. H. 2013. "Employer Branding: Employer Attractiveness and the Use of Social Media," Journal of Product & Brand Management, (22:7), pp. 473­483. Stewart, K. A., and Segars, A. H. 2002. "An Empirical Examination of the Concern for Information Privacy Instrument," Information Systems Research, (13:1), pp. 36­49. Stoughton, J. W., Thompson, L. F., and Meade, A. W. 2013. "Big Five Personality Traits Reflected in Job Applicants' Social Media Postings," Cyberpsychology, Behavior, and Social Networking, (16:11), pp. 800­805. Stoughton, J. W., Thompson, L. F., and Meade, A. W. 2015. "Examining Applicant Reactions to the Use of Social Networking Websites in Pre-Employment Screening," Journal of Business and Psychology, (30:1), pp. 73­88. Viégas, F. B., and Wattenberg, M. 2006. "Communication-Minded Visualization: A Call to Action," IBM Systems Journal, (45:4), p. 801­812. Wang, Y., Norice, G., and Cranor, L. F. 2011. "Who Is Concerned about What? A Study of American, Chinese and Indian Users' Privacy Concerns on Social Network Sites," in Trust and Trustworthy Computing, J. M. McCune, B. Balacheff, A. Perrig, A.-R. Sadeghi, A. Sasse, and Y. Beres (eds.), pp. 146­153. Youn, S., and Hall, K. 2008. "Gender and Online Privacy among Teens: Risk Perception, Privacy Concerns, and Protection Behaviors," CyberPsychology & Behavior, (11:6), pp. 763­765. Zide, J., Elman, B., and Shahani-Denning, C. 2014. "LinkedIn and Recruitment: How Profiles Differ across Occupations," Employee Relations, (36:5), pp. 583­604.

Twenty-third Americas Conference on Information Systems, Boston, 2017

10

